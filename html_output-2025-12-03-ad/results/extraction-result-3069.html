<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3069 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3069</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3069</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-272145972</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.03028v3.pdf" target="_blank">An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning. How do these different capabilities relate? Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3069.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3069.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive large language model from the OpenAI GPT family used in this paper to evaluate deductive (instruction following), inductive (few-shot/in-context), and abductive (instruction inference) prompting strategies across multiple tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer model from the OpenAI GPT family; used both to generate predictions for base tasks and to generate/rerank hypotheses during instruction inference experiments. Hypotheses and predictions were generated with varying temperatures (hypotheses T=1; base-task T in {0,1} for synthetic domains; lower T for Kalamang due to cost).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['instruction following (deductive)', 'few-shot prompting / in-context learning (inductive)', 'instruction inference / hypothesis generation and reranking (abductive)', 'zero-shot chain-of-thought (zs-cot) as a comparator in some experiments']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Instruction following: model is given an explicit natural-language instruction (e.g., 'y = 5x + 3') and applies it deductively at test time. Few-shot prompting: model is given k=5 input-output examples in-context and must generalize (inductive). Instruction inference: model generates n=5 natural-language hypotheses about the task (T=1), then hypotheses are reranked using one of several scores (verbalized confidence from the LM, P(data) or P(answer) via log-probabilities, and an external validator for structured hypotheses in the functions domain), the best hypothesis is fed back as an instruction and used deductively. Zero-shot chain-of-thought: used as a baseline prompting method in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse — the model was tested with multiple distinct reasoning styles (deduction via explicit instructions, induction via few-shot examples, and abduction via explicit hypothesis generation + reranking). The paper implements these via prompt design and multi-hypothesis sampling+reranking; differences in behavior across settings are measured empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Linear function learning; Colours (artificial language) translation; Kalamang low-resource translation (MTOB)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Linear functions: predict y = ax + b given 5 in-context examples drawn from one of 40 randomly sampled linear functions (integer a,b in [-20,20]). Colours: synthetic compositional artificial-language mapping nonce words to colors and repetition operations. Kalamang (MTOB): realistic low-resource machine translation using a 400-sentence training set and a 100-sentence test set; tasks include English<->Kalamang translation and induction of vocabulary and grammar features.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Linear functions: few-shot baseline (inductive) for GPT-3.5-turbo improved when using true instruction; model-proposed hypothesis coefficients show positive Spearman correlations with ground truth: coefficient 'b' rho = 0.275 (p ≈ 5.0e-15), coefficient 'a' rho = 0.562 (p ≈ 2.7e-67). However, point-biserial correlation between correctness of final hypothesis and in-context prediction accuracy was essentially chance (≈0.0013), indicating dissociation. Colours: hypothesis accuracy varies by nonce word; GPT-3.5-turbo had low accuracy for repeat terms (e.g., ~15% mean for 'bluf'); instruction-inference often improved over few-shot baseline but less consistently than in functions. Kalamang: vocabulary induction accuracy for GPT models generally low (10–20% for many settings); GPT-3.5-turbo grammar sketch accuracy ≈ 27.78% (Table K), and instruction-inference did not reliably improve translation chrF. Exact few-shot and instruction-inference accuracy numbers across all settings are reported graphically (Figures 3–5) rather than as full numeric tables in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Explicit experimental contrasts: few-shot (induction) vs instruction-following (deduction with ground-truth instruction) vs instruction-inference (abduction with multi-hypothesis reranking). Key observations for GPT-3.5-turbo: (1) ground-truth instructions give large gains in synthetic tasks; (2) self-induced instructions sometimes improve over few-shot baselines in synthetic tasks but are imperfect; (3) correctness of generated hypotheses does not reliably correlate with few-shot performance (dissociation shown quantitatively, e.g., near-zero point-biserial correlation in functions domain). In Kalamang, instruction-inference often failed or produced low-quality hypotheses and did not improve translation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5-turbo can benefit from different prompting styles, but success at one reasoning style (e.g., generating accurate natural-language hypotheses) does not reliably predict success at another (e.g., accurate in-context predictions); abductive instruction inference is imperfect (hypotheses are noisy), can improve performance in synthetic domains, but is generally weaker/less reliable than deductive instruction following when a correct instruction is available.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Kalamang translation: instruction inference and even providing ground-truth grammar/wordlist did not reliably improve translation chrF; vocabulary and grammar induction accuracy for GPT-3.5-turbo were low (vocab 10–20%, grammar ~27.78% accuracy). In linear functions the model could predict outputs correctly without inducing an accurate hypothesis (dissociation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3069.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3069.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A higher-capability OpenAI GPT-4 family model evaluated for its ability to follow instructions, learn from examples, and generate hypotheses for instruction inference across synthetic and low-resource translation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Higher-capability autoregressive transformer (OpenAI GPT-4 family) used as both the base predictive model and the hypothesis proposer/evaluator in experiments. Hypotheses sampled at higher temperature (T=1) and base-task generations often at T in {0,1} for synthetic domains and lower T for Kalamang.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['instruction following (deductive)', 'few-shot prompting / in-context learning (inductive)', 'instruction inference / multi-hypothesis abduction + reranking', 'zero-shot chain-of-thought as baseline comparator']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same implementation as for GPT-3.5: explicit natural-language instruction application (deduction), k=5 in-context examples (induction), n=5 sampled hypotheses (abduction) reranked by verbalized confidence or log-prob-based scores (P(data), P(answer)), and in the functions domain an external validator that parses and measures MSE to score hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse — GPT-4-turbo was tested with and exhibited distinct behaviors under all three core reasoning styles; the paper measures which style/combination yields better base-task performance across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Linear function learning; Colours artificial language; Kalamang low-resource translation (MTOB).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-3.5-turbo entry; identical tasks used to compare models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Linear functions: few-shot baseline accuracy for GPT-4-turbo reported around 30% (aggregated baseline), while using the true instruction (deductive) increases accuracy to ≈96% (Figure 3A). GPT-4-turbo's proposed coefficients are highly correlated with true coefficients: Spearman rho for 'b' ≈ 0.852 (p ≈ 2.2e-29) and for 'a' ≈ 0.91 (p-value not clearly printed but highly significant), indicating accurate hypothesis induction in the functions domain. Colours: GPT-4-turbo proposed word hypotheses with relatively high accuracy on many tokens (e.g., ~87% mean accuracy for the nonce 'lug'), and instruction inference improved performance in the colours domain; in some cases GPT-4's self-proposed hypotheses produced better results than the provided ground-truth grammar for certain models. Kalamang: GPT-4-turbo grammar-sketch accuracy ≈ 22.22% (Table K); instruction-inference did not substantially improve chrF scores in MTOB translation; vocabulary induction accuracy remained low (10–20% typical).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparisons show that deductive instruction-following with the ground-truth instruction gives the largest gains in linear functions (true-instruction 96% vs few-shot ~30%). Abductive instruction inference (multi-hypothesis + reranking) produced strong gains in the synthetic functions domain and improvements in colours, largely because GPT-4-turbo can generate accurate hypotheses there; but in Kalamang the abductive pipeline often produced incorrect hypotheses and failed to improve translation quality. Thus GPT-4-turbo demonstrates that a model can use diverse reasoning methods, but success depends on task complexity and faithfulness of induced hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4-turbo is able to induce high-quality natural-language hypotheses for simple, structured synthetic tasks (high Spearman correlation of induced coefficients; large accuracy gains when using inferred hypotheses), demonstrating that when abduction is successful it can outperform pure induction; however, in realistic low-resource MT (Kalamang) abduction is brittle and often fails to yield gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Kalamang: even for GPT-4-turbo, grammar and vocabulary induction accuracy were poor (~22.22% grammar accuracy), and instruction-inference did not raise chrF in the translation task — showing that generating plausible natural-language hypotheses does not always translate to better downstream task performance in complex, low-resource domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3069.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3069.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 7B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter Llama-2 chat model (Meta) evaluated in the paper; included to probe whether open-weight Llama family models behave similarly under deductive, inductive, and abductive prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-2-7b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight Llama-2 chat model (7B parameters) used for the same experimental pipeline; hypothesis generation used a slightly raised temperature (T ≈ 1.0625) because at T=1 the model would often return identical hypotheses, and base-task generation used T in {0.1,1}.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['instruction following (deductive)', 'few-shot prompting / in-context learning (inductive)', 'instruction inference (abductive) — attempted but often failed to yield parseable/accurate hypotheses']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same prompting paradigms as other models; for Llama-2 models, hypothesis generation temperatures and parsing heuristics were tuned because the model frequently produced unparseable or repeated hypotheses; final hypotheses were reranked using LM-based scores where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Technically multiple methods were attempted, but in practice Llama-2-7b displayed limited successful use of abductive methods — i.e., behaviors were less diverse in quality (few-shot and instruction-following were more reliable than hypothesis induction). Determination was via empirical comparison across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Linear functions; Colours; Kalamang translation (MTOB)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same tasks as above; used identical evaluation metrics (0-1 accuracy, median squared error for functions; chrF for translation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Linear functions: Llama-2-7b produced poor hypothesis quality (Spearman correlations near zero for some coefficients: e.g., b rho ≈ -0.0069, p≈0.90; a rho≈0.14, p≈0.06) and did not enjoy the same gains from instruction inference as GPT models. Colours: Llama-2-7b did not benefit as much from ground-truth or self-induced instructions in some settings (Figure 3B); in some cases it failed to generate usable hypotheses at all. Kalamang: Llama-2-7b achieved 0% grammar-sketch accuracy (Table K) and did not benefit from instruction-inference; vocabulary induction accuracy also low.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to GPT-family models, Llama-2-7b struggled more with abductive hypothesis generation and reranking; few-shot and ground-truth instruction following sometimes helped but overall gains from instruction-inference were limited or nonexistent. The paper attributes some failures to incoherent/unparseable hypotheses and model-specific generation behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama-2-7b is less capable of producing accurate natural-language hypotheses in the instruction-inference pipeline; this reduces the utility of abduction for that model, leaving it reliant on induction or ground-truth instructions. Thus similar prompting procedures can evoke very different internal reasoning outcomes across architectures/sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Llama-2-7b often produced incoherent or unparsable hypotheses that had to be assigned score -∞ in the reranking step; grammar induction for Kalamang was 0% accuracy, showing clear negative results for abductive methods on this model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3069.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3069.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 70B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger Llama-2 chat model (70B parameters) evaluated for the same set of reasoning paradigms; included to test whether scale within the Llama family improves abductive/in-context reasoning behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 chat model at ~70B parameters; hypothesis generation used slightly higher temperature than 1 (T ≈ 1.0625) to avoid repetitive outputs; base-task T in {0.1,1}. Used same prompting and reranking pipeline as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['instruction following (deductive)', 'few-shot prompting / in-context learning (inductive)', 'instruction inference (abductive) attempted but with limited success']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Identical prompting paradigms as other models; hypothesis generation and reranking implemented the same multi-hypothesis pipeline (n=5 hypotheses) and used LM-based reranking scores when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Although multiple methods were attempted, the model frequently failed to produce high-quality hypotheses; therefore the effective use of diverse reasoning methods was limited compared to GPT-4, and performance patterns resembled those of the smaller Llama model more than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Linear functions; Colours; Kalamang translation (MTOB)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same tasks as used for other models; metrics include 0-1 accuracy and chrF.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Linear functions: Spearman correlations for predicted coefficients were low/insignificant (e.g., reported values a/b correlations near zero or weak); instruction-inference did not yield the large improvements seen for GPT-4. Colours: limited or no consistent gains from instruction inference; models often returned malformed hypotheses. Kalamang: grammar-sketch accuracy 0% (Table K), vocabulary induction low (10–20%), and instruction-inference did not improve chrF.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Scale (70B) within Llama family did not reliably close the gap with GPT models for abductive hypothesis generation or for leveraging instruction inference; few-shot or ground-truth instruction following remained more reliable strategies for this model but without the dramatic gains observed in GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even at 70B, the Llama-2 model family had difficulty with producing parseable, accurate hypotheses for abduction; this limits the diversity of effective reasoning styles the model can employ in practice, compared to GPT-4-turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Grammar induction for Kalamang failed (0% accuracy); many generated hypotheses were unparsable and had to be given lowest possible scores, producing negative results for the instruction-inference pipeline on this model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Instruction induction: From few examples to natural language task descriptions <em>(Rating: 2)</em></li>
                <li>Learning with latent language <em>(Rating: 2)</em></li>
                <li>Hypothesis search: Inductive reasoning with language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3069",
    "paper_id": "paper-272145972",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "An autoregressive large language model from the OpenAI GPT family used in this paper to evaluate deductive (instruction following), inductive (few-shot/in-context), and abductive (instruction inference) prompting strategies across multiple tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Autoregressive transformer model from the OpenAI GPT family; used both to generate predictions for base tasks and to generate/rerank hypotheses during instruction inference experiments. Hypotheses and predictions were generated with varying temperatures (hypotheses T=1; base-task T in {0,1} for synthetic domains; lower T for Kalamang due to cost).",
            "model_size": null,
            "reasoning_methods": [
                "instruction following (deductive)",
                "few-shot prompting / in-context learning (inductive)",
                "instruction inference / hypothesis generation and reranking (abductive)",
                "zero-shot chain-of-thought (zs-cot) as a comparator in some experiments"
            ],
            "reasoning_methods_description": "Instruction following: model is given an explicit natural-language instruction (e.g., 'y = 5x + 3') and applies it deductively at test time. Few-shot prompting: model is given k=5 input-output examples in-context and must generalize (inductive). Instruction inference: model generates n=5 natural-language hypotheses about the task (T=1), then hypotheses are reranked using one of several scores (verbalized confidence from the LM, P(data) or P(answer) via log-probabilities, and an external validator for structured hypotheses in the functions domain), the best hypothesis is fed back as an instruction and used deductively. Zero-shot chain-of-thought: used as a baseline prompting method in some settings.",
            "diversity_of_methods": "Diverse — the model was tested with multiple distinct reasoning styles (deduction via explicit instructions, induction via few-shot examples, and abduction via explicit hypothesis generation + reranking). The paper implements these via prompt design and multi-hypothesis sampling+reranking; differences in behavior across settings are measured empirically.",
            "reasoning_task_name": "Linear function learning; Colours (artificial language) translation; Kalamang low-resource translation (MTOB)",
            "reasoning_task_description": "Linear functions: predict y = ax + b given 5 in-context examples drawn from one of 40 randomly sampled linear functions (integer a,b in [-20,20]). Colours: synthetic compositional artificial-language mapping nonce words to colors and repetition operations. Kalamang (MTOB): realistic low-resource machine translation using a 400-sentence training set and a 100-sentence test set; tasks include English&lt;-&gt;Kalamang translation and induction of vocabulary and grammar features.",
            "performance_by_method": "Linear functions: few-shot baseline (inductive) for GPT-3.5-turbo improved when using true instruction; model-proposed hypothesis coefficients show positive Spearman correlations with ground truth: coefficient 'b' rho = 0.275 (p ≈ 5.0e-15), coefficient 'a' rho = 0.562 (p ≈ 2.7e-67). However, point-biserial correlation between correctness of final hypothesis and in-context prediction accuracy was essentially chance (≈0.0013), indicating dissociation. Colours: hypothesis accuracy varies by nonce word; GPT-3.5-turbo had low accuracy for repeat terms (e.g., ~15% mean for 'bluf'); instruction-inference often improved over few-shot baseline but less consistently than in functions. Kalamang: vocabulary induction accuracy for GPT models generally low (10–20% for many settings); GPT-3.5-turbo grammar sketch accuracy ≈ 27.78% (Table K), and instruction-inference did not reliably improve translation chrF. Exact few-shot and instruction-inference accuracy numbers across all settings are reported graphically (Figures 3–5) rather than as full numeric tables in the text.",
            "comparison_of_methods": "Explicit experimental contrasts: few-shot (induction) vs instruction-following (deduction with ground-truth instruction) vs instruction-inference (abduction with multi-hypothesis reranking). Key observations for GPT-3.5-turbo: (1) ground-truth instructions give large gains in synthetic tasks; (2) self-induced instructions sometimes improve over few-shot baselines in synthetic tasks but are imperfect; (3) correctness of generated hypotheses does not reliably correlate with few-shot performance (dissociation shown quantitatively, e.g., near-zero point-biserial correlation in functions domain). In Kalamang, instruction-inference often failed or produced low-quality hypotheses and did not improve translation metrics.",
            "key_findings": "GPT-3.5-turbo can benefit from different prompting styles, but success at one reasoning style (e.g., generating accurate natural-language hypotheses) does not reliably predict success at another (e.g., accurate in-context predictions); abductive instruction inference is imperfect (hypotheses are noisy), can improve performance in synthetic domains, but is generally weaker/less reliable than deductive instruction following when a correct instruction is available.",
            "counter_examples_or_negative_results": "Kalamang translation: instruction inference and even providing ground-truth grammar/wordlist did not reliably improve translation chrF; vocabulary and grammar induction accuracy for GPT-3.5-turbo were low (vocab 10–20%, grammar ~27.78% accuracy). In linear functions the model could predict outputs correctly without inducing an accurate hypothesis (dissociation).",
            "uuid": "e3069.0",
            "source_info": {
                "paper_title": "An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4-turbo",
            "name_full": "GPT-4-turbo",
            "brief_description": "A higher-capability OpenAI GPT-4 family model evaluated for its ability to follow instructions, learn from examples, and generate hypotheses for instruction inference across synthetic and low-resource translation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4-turbo",
            "model_description": "Higher-capability autoregressive transformer (OpenAI GPT-4 family) used as both the base predictive model and the hypothesis proposer/evaluator in experiments. Hypotheses sampled at higher temperature (T=1) and base-task generations often at T in {0,1} for synthetic domains and lower T for Kalamang.",
            "model_size": null,
            "reasoning_methods": [
                "instruction following (deductive)",
                "few-shot prompting / in-context learning (inductive)",
                "instruction inference / multi-hypothesis abduction + reranking",
                "zero-shot chain-of-thought as baseline comparator"
            ],
            "reasoning_methods_description": "Same implementation as for GPT-3.5: explicit natural-language instruction application (deduction), k=5 in-context examples (induction), n=5 sampled hypotheses (abduction) reranked by verbalized confidence or log-prob-based scores (P(data), P(answer)), and in the functions domain an external validator that parses and measures MSE to score hypotheses.",
            "diversity_of_methods": "Diverse — GPT-4-turbo was tested with and exhibited distinct behaviors under all three core reasoning styles; the paper measures which style/combination yields better base-task performance across domains.",
            "reasoning_task_name": "Linear function learning; Colours artificial language; Kalamang low-resource translation (MTOB).",
            "reasoning_task_description": "See GPT-3.5-turbo entry; identical tasks used to compare models.",
            "performance_by_method": "Linear functions: few-shot baseline accuracy for GPT-4-turbo reported around 30% (aggregated baseline), while using the true instruction (deductive) increases accuracy to ≈96% (Figure 3A). GPT-4-turbo's proposed coefficients are highly correlated with true coefficients: Spearman rho for 'b' ≈ 0.852 (p ≈ 2.2e-29) and for 'a' ≈ 0.91 (p-value not clearly printed but highly significant), indicating accurate hypothesis induction in the functions domain. Colours: GPT-4-turbo proposed word hypotheses with relatively high accuracy on many tokens (e.g., ~87% mean accuracy for the nonce 'lug'), and instruction inference improved performance in the colours domain; in some cases GPT-4's self-proposed hypotheses produced better results than the provided ground-truth grammar for certain models. Kalamang: GPT-4-turbo grammar-sketch accuracy ≈ 22.22% (Table K); instruction-inference did not substantially improve chrF scores in MTOB translation; vocabulary induction accuracy remained low (10–20% typical).",
            "comparison_of_methods": "Direct comparisons show that deductive instruction-following with the ground-truth instruction gives the largest gains in linear functions (true-instruction 96% vs few-shot ~30%). Abductive instruction inference (multi-hypothesis + reranking) produced strong gains in the synthetic functions domain and improvements in colours, largely because GPT-4-turbo can generate accurate hypotheses there; but in Kalamang the abductive pipeline often produced incorrect hypotheses and failed to improve translation quality. Thus GPT-4-turbo demonstrates that a model can use diverse reasoning methods, but success depends on task complexity and faithfulness of induced hypotheses.",
            "key_findings": "GPT-4-turbo is able to induce high-quality natural-language hypotheses for simple, structured synthetic tasks (high Spearman correlation of induced coefficients; large accuracy gains when using inferred hypotheses), demonstrating that when abduction is successful it can outperform pure induction; however, in realistic low-resource MT (Kalamang) abduction is brittle and often fails to yield gains.",
            "counter_examples_or_negative_results": "Kalamang: even for GPT-4-turbo, grammar and vocabulary induction accuracy were poor (~22.22% grammar accuracy), and instruction-inference did not raise chrF in the translation task — showing that generating plausible natural-language hypotheses does not always translate to better downstream task performance in complex, low-resource domains.",
            "uuid": "e3069.1",
            "source_info": {
                "paper_title": "An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Llama-2-7b-chat",
            "name_full": "Llama 2 7B Chat",
            "brief_description": "A 7B-parameter Llama-2 chat model (Meta) evaluated in the paper; included to probe whether open-weight Llama family models behave similarly under deductive, inductive, and abductive prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-2-7b-chat",
            "model_description": "Open-weight Llama-2 chat model (7B parameters) used for the same experimental pipeline; hypothesis generation used a slightly raised temperature (T ≈ 1.0625) because at T=1 the model would often return identical hypotheses, and base-task generation used T in {0.1,1}.",
            "model_size": "7B",
            "reasoning_methods": [
                "instruction following (deductive)",
                "few-shot prompting / in-context learning (inductive)",
                "instruction inference (abductive) — attempted but often failed to yield parseable/accurate hypotheses"
            ],
            "reasoning_methods_description": "Same prompting paradigms as other models; for Llama-2 models, hypothesis generation temperatures and parsing heuristics were tuned because the model frequently produced unparseable or repeated hypotheses; final hypotheses were reranked using LM-based scores where possible.",
            "diversity_of_methods": "Technically multiple methods were attempted, but in practice Llama-2-7b displayed limited successful use of abductive methods — i.e., behaviors were less diverse in quality (few-shot and instruction-following were more reliable than hypothesis induction). Determination was via empirical comparison across methods.",
            "reasoning_task_name": "Linear functions; Colours; Kalamang translation (MTOB)",
            "reasoning_task_description": "Same tasks as above; used identical evaluation metrics (0-1 accuracy, median squared error for functions; chrF for translation).",
            "performance_by_method": "Linear functions: Llama-2-7b produced poor hypothesis quality (Spearman correlations near zero for some coefficients: e.g., b rho ≈ -0.0069, p≈0.90; a rho≈0.14, p≈0.06) and did not enjoy the same gains from instruction inference as GPT models. Colours: Llama-2-7b did not benefit as much from ground-truth or self-induced instructions in some settings (Figure 3B); in some cases it failed to generate usable hypotheses at all. Kalamang: Llama-2-7b achieved 0% grammar-sketch accuracy (Table K) and did not benefit from instruction-inference; vocabulary induction accuracy also low.",
            "comparison_of_methods": "Compared to GPT-family models, Llama-2-7b struggled more with abductive hypothesis generation and reranking; few-shot and ground-truth instruction following sometimes helped but overall gains from instruction-inference were limited or nonexistent. The paper attributes some failures to incoherent/unparseable hypotheses and model-specific generation behaviors.",
            "key_findings": "Llama-2-7b is less capable of producing accurate natural-language hypotheses in the instruction-inference pipeline; this reduces the utility of abduction for that model, leaving it reliant on induction or ground-truth instructions. Thus similar prompting procedures can evoke very different internal reasoning outcomes across architectures/sizes.",
            "counter_examples_or_negative_results": "Llama-2-7b often produced incoherent or unparsable hypotheses that had to be assigned score -∞ in the reranking step; grammar induction for Kalamang was 0% accuracy, showing clear negative results for abductive methods on this model.",
            "uuid": "e3069.2",
            "source_info": {
                "paper_title": "An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Llama-2-70b-chat",
            "name_full": "Llama 2 70B Chat",
            "brief_description": "A larger Llama-2 chat model (70B parameters) evaluated for the same set of reasoning paradigms; included to test whether scale within the Llama family improves abductive/in-context reasoning behaviors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-2-70b-chat",
            "model_description": "Llama-2 chat model at ~70B parameters; hypothesis generation used slightly higher temperature than 1 (T ≈ 1.0625) to avoid repetitive outputs; base-task T in {0.1,1}. Used same prompting and reranking pipeline as other models.",
            "model_size": "70B",
            "reasoning_methods": [
                "instruction following (deductive)",
                "few-shot prompting / in-context learning (inductive)",
                "instruction inference (abductive) attempted but with limited success"
            ],
            "reasoning_methods_description": "Identical prompting paradigms as other models; hypothesis generation and reranking implemented the same multi-hypothesis pipeline (n=5 hypotheses) and used LM-based reranking scores when possible.",
            "diversity_of_methods": "Although multiple methods were attempted, the model frequently failed to produce high-quality hypotheses; therefore the effective use of diverse reasoning methods was limited compared to GPT-4, and performance patterns resembled those of the smaller Llama model more than GPT-4.",
            "reasoning_task_name": "Linear functions; Colours; Kalamang translation (MTOB)",
            "reasoning_task_description": "Same tasks as used for other models; metrics include 0-1 accuracy and chrF.",
            "performance_by_method": "Linear functions: Spearman correlations for predicted coefficients were low/insignificant (e.g., reported values a/b correlations near zero or weak); instruction-inference did not yield the large improvements seen for GPT-4. Colours: limited or no consistent gains from instruction inference; models often returned malformed hypotheses. Kalamang: grammar-sketch accuracy 0% (Table K), vocabulary induction low (10–20%), and instruction-inference did not improve chrF.",
            "comparison_of_methods": "Scale (70B) within Llama family did not reliably close the gap with GPT models for abductive hypothesis generation or for leveraging instruction inference; few-shot or ground-truth instruction following remained more reliable strategies for this model but without the dramatic gains observed in GPT-4.",
            "key_findings": "Even at 70B, the Llama-2 model family had difficulty with producing parseable, accurate hypotheses for abduction; this limits the diversity of effective reasoning styles the model can employ in practice, compared to GPT-4-turbo.",
            "counter_examples_or_negative_results": "Grammar induction for Kalamang failed (0% accuracy); many generated hypotheses were unparsable and had to be given lowest possible scores, producing negative results for the instruction-inference pipeline on this model.",
            "uuid": "e3069.3",
            "source_info": {
                "paper_title": "An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Instruction induction: From few examples to natural language task descriptions",
            "rating": 2,
            "sanitized_title": "instruction_induction_from_few_examples_to_natural_language_task_descriptions"
        },
        {
            "paper_title": "Learning with latent language",
            "rating": 2,
            "sanitized_title": "learning_with_latent_language"
        },
        {
            "paper_title": "Hypothesis search: Inductive reasoning with language models",
            "rating": 2,
            "sanitized_title": "hypothesis_search_inductive_reasoning_with_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.01539525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models
20 Aug 2024</p>
<p>Emmy Liu 
Language Technologies Institute Carnegie Mellon University</p>
<p>Graham Neubig gneubig@cs.cmu.edu 
Language Technologies Institute Carnegie Mellon University</p>
<p>Jacob Andreas 
CSAIL Massachusetts Institute of Technology</p>
<p>An Incomplete Loop: Instruction Inference, Instruction Following, and In-context Learning in Language Models
20 Aug 202460A23D92BBD9F2DB9405EFB29DFEF55DarXiv:2404.03028v3[cs.CL]
Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions.Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning.How do these different capabilities relate?Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task.Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures. 1 .</p>
<p>Suppose a friend is teaching you to cook.You watch them place a pan on the stove and heat olive oil at low heat, adding minced garlic and chili flakes to the olive oil once it gets hot.Later, you decide to make the recipe yourself, but you are out of olive oil.You hypothesize that the olive oil served to cook the garlic without burning it, and so substitute butter for olive oil, as it should fulfill the same function.Here, you learned a generalizable cooking procedure by reasoning abductively-finding the rule that best explains your experience (Frankfurt, 1958;Peirce, 1965).This is only one way of learning: the friend could have instead provided a recipe, from which you could have reasoned deductively about how to apply it in your kitchen.If you had remembered other times when you put but-Table 1: Summary of reasoning types and analogues in language models.Citations to instruction following, in-context learning, and chain-of-thought are limited to the original paper due to the high number of papers on these topics, while we have tried to list all currently available papers on instruction inference (for abductive reasoning) with LMs.</p>
<p>ter on a hot pan before cooking, without explicitly reasoning about why, you could have cooked the same meal inductively.</p>
<p>Each form of reasoning has a close analogue in current procedures for steering language models (LMs).In order to induce LMs to perform new tasks, we may condition them on explicit commands (instruction following; Wei et al., 2022a;Sanh et al., 2022), or a collection of examples from the task of interest (few-shot prompting; Brown et al., 2020); recently, several methods have been proposed that prompt LMs to generate textual commands from examples before conditioning on commands during prediction (instruction inference; Andreas et al., 2018;Honovich et al., 2023).But it is often unclear when to prefer one of these procedures over the other, and more generally how these different capabilities relate in current LMs.Does the ability to learn effectively from few-shot prompts imply the ability to perform instruction inference, or vice-versa?Are there tasks that can be learned from few-shot prompts, but not instructions?In this paper, we examine these questions through two tasks: learning numerical functions ( §4.1) and translation models ( §4.2, §4.3).We ask two questions: RQ1 When does (abductive) instruction inference improve LM performance over ordinary (inductive) few-shot prompting?</p>
<p>Finding: Instruction inference improves over few-shot prompting in simple cases (linear function learning and simple artificial language learning), but suffers from incorrect hypotheses in a more complex case (low-resource machine translation).</p>
<p>RQ2 How does the ability to learn from instructions (deductively) relate to the ability to learn from in-context examples (inductively or abductively)?</p>
<p>Finding: The ability to learn through abduction (proposing hypotheses) is generally not related to learning through induction (few-shot learning).Deductive reasoning is generally strong, with large performance gains over using inductive reasoning alone when the provided hypothesis is correct.</p>
<p>Our results highlight the diverse capabilities (and diversity of different reasoning mechanisms) triggered by different prompts and examples in current LMs; future work may investigate how these reasoning types can be combined or made consistent to enhance problem solving in LMs.</p>
<p>Three Types of Reasoning in Language Models</p>
<p>We first give a formal definition of instruction following, in-context learning, and instruction inference, relating these processes to deductive reasoning, inductive reasoning, and abductive reasoning respectively.(Other ways of interacting with LMs may also evoke one or more of these forms of reasoning, but we focus on important and widely used prompting strategies.) Figure 1 gives a schematic of the reasoning loop, while Table 1 gives examples of work falling in each category.2Throughout this section, we assume that we have an input</p>
<p>x, and we want to produce an output y with an autoregressive LM conditioned on some additional piece of data D that specifies the target task: p LM (y | x, D).We use in-context learning of linear functions as a running example.</p>
<p>Instruction Following</p>
<p>In instruction following, we want to map each input x to a y according to a general instruction or prediction rule D that is specified in the input.This may be viewed as a kind of deductive reasoning, which in begins with one or more premises, and applies logically valid rules to reach a conclusion (Shapiro &amp; Kouri Kissel, 2024).For instance, the instruction D might name a general function, such as Apply this function to the input: y = 5x + 3, followed by a specific query: Input: -3.This is illustrated in Figure 1.</p>
<p>Few-Shot Prompting</p>
<p>Few-shot prompting, by contrast, specifies the target task implicitly through examples.</p>
<p>For each input x, the task specification D consists of a set of k training examples D = {(x 1 , y 1 ), ..., (x k , y k )}.We then sample from p LM (y | x, D).Few-shot learning requires LMs to perform inductive reasoning (Hawthorne, 2021).Unlike deductive reasoning, there is no explicit premise stated, but the model must complete the task in a similar way to the examples.In Figure 1, this is pairs of inputs and outputs, i.e.Input: 5, Output: 28.These examples help specify the task as a numeric prediction task, as well as the identity of the specific target function.</p>
<p>Instruction Inference</p>
<p>Instruction inference connects instruction following and in-context learning: given examples D = {(x i , y i )} k i=1 and input x, we can instruct the model to generate a hypothesis h about the identity of the task, i.e. an instruction describing the task associated with D. We may sample one hypothesis and immediately condition on it (a form of chain-of-thought prompting), or sample several and select the most promising one.</p>
<p>Compared to chain-of-thought, fewer studies have explored multi-hypothesis instruction induction; those that do typically rely on an external validation model rather than using the LM itself to evaluate.Our approach (in §3) has the following high-level form: after sampling n hypotheses h 1 , ..., h n from p LM (h | t, x), we evaluate each hypothesis by assigning a score score(h i , t) based on the hypothesis and in-context examples.We choose the best one h * = arg max score(h i , D), and feed it back into the context as an instruction. 3bductive reasoning is often called "inference to the best explanation" (Lipton, 2001;Douven, 2021).Suppose that the model generates the hypotheses shown in Figure 1.We may parse hypotheses and apply them back to the in-context examples, then ranking them by prediction error.Then the model simply has to follow the instruction as in Section 2.1.</p>
<p>Methods</p>
<p>We describe the implementation of instruction inference with multiple hypotheses in this section.As the other settings (few-shot, zero-shot chain-of-thought, etc) are already well understood, we do not explain them further, but include the exact prompts used to implement methods in Appendix B, F, and J.
h * = arg max h i ∈H score(h i , D i ) .
Settings with instruction inference are referred to collectively as instruction inference.</p>
<p>We detail the score functions below:
Verbalized</p>
<p>Domains and Evaluation</p>
<p>We investigate LM behavior in three domains: linear function inference, an artificial language learning task, and vocabulary + typological feature learning in the Kalamang language.We refer to the underlying task we would like to improve (output prediction, translation) as the base task, and the task of inferring an explicit natural language hypothesis from few-shot examples as the abductive task.Examples of each task are in Table 2.We also evaluate hypotheses themselves in each domain.For the exact prompts used in each domain, refer to Appendix B, F and J. Models used across all domains consisted of gpt family models (gpt-3.5-turbo;Brown et al., 2020, gpt-4-turbo, OpenAI et al., 2024) and llama family models (llama-2-7b-chat, llama-2-70b-chat, Touvron et al., 2023).</p>
<p>Linear Functions</p>
<p>Following investigations of language models' in-context learning abilities (Garg et al.  [−20, 20].We refer to this as the functions domain.Prompts used are given in Appendix B.</p>
<p>Hypotheses In this domain, hypotheses are proposed using all in-context examples for each test example.The model was presented with functions (and instructed to write them) in the form y = ax + b.For external validator reranking, we parse the generated hypothesis, and score it by its negative mean-squared error (MSE) when applied to the in-context examples. 5f h i (x ik ) represent executing the parsed hypothesis represented by h i on example x ik :
s ground truth (h i , D i ) = k ∑ j=1 (h i (x ij ) − f (x ij )) 2
Evaluation To evaluate the base task, we use 0-1 accuracy, as well as median squared errors.</p>
<p>To evaluate hypotheses, we examine the accuracy of model-proposed a and b coefficients, as well as the Spearman correlation between proposed coefficients and the ground truth.</p>
<p>Simple Artificial Languages</p>
<p>Inspired by compositional instruction-following datasets, we generate a simple dataset where the inputs are nonce words such as lug, and the outputs are colour terms such as blue (Lake &amp; Baroni, 2018;Lake et al., 2019).We call the expanded dataset the colours domain.</p>
<p>The ruleset for this domain can be found in Appendix E. We generate 200 test examples and 800 training examples. 6Prompts used are in Appendix F.</p>
<p>Hypotheses Following the original miniscan, we create a fixed minimal set of 5 in-context examples that contains each nonce word at least once, and from which the meaning of each nonce word can be reasonably inferred.During instruction inference, we isolate one nonce word at a time from the sentence, and have the model try to induce the vocabulary mapping for that word from 5 retrieved in-context examples containing that word.The ground truth grammar was written by the author, and consisted of production rules for each nonce term.Prompts used in the abductive and base tasks can be found in Appendix F.</p>
<p>Evaluation</p>
<p>We also use 0-1 accuracy to evaluate the base task of nonce word translation, as well as corpus-level chrF (Popović, 2015).To evaluate the quality of the hypotheses themselves, we extract the production rules proposed by models and compare 0-1 accuracy against the ground truth.As a lenient evaluation on the "repeat" terms (see Appendix E for nonce terms and meanings), we marked a hypothesis for "repeat" terms as correct if it contained the term repeat or the numerals 2 and 3 respectively.</p>
<p>Kalamang Translation</p>
<p>For low-resource translation, we use the Machine Translation with One Book (MTOB) dataset, an English-Kalamang dataset with a grammar book (Tanzer et al., 2024).Kalamang is an extremely low-resource language with fewer than 200 speakers, and virtually no text on the web.The base task is to perform sentence-level translation in both directions, while the abductive task is to infer correct grammar features and vocabulary mappings.The dataset consisted of 100 test sentences (50 in each direction) and 400 train sentences.A ground-truth bilingual dictionary was provided (Visser, 2020).A grammar book was included as well, but to check correctness of high-level grammar inferences, we compiled a high level grammar sketch instead from WALS and GramBank features (Dryer &amp; Haspelmath, 2013;Skirgård et al., 2023).More details about the grammar sketch can be found in Appendix K. Otherwise, we use the same experimental settings as the baseline, summarized in Appendix I.
✓ ✓ ✗ ✗ Colours ✓ ✓ ✗ Kalamang ✗ ✗ ✗
Table 3: Summary of results.A checkmark indicates that the property held for all or almost all language models, a half-checkmark indicates a partial success for all or almost all language models, while an X-mark represents lack of success for most language models.</p>
<p>Hypotheses We split hypotheses into vocabulary and grammar.The ground truth instruction included retrieved examples from the wordlist and the grammar sketch.To induce the grammar sketch, we posed each grammar feature as a question, for instance: What is the order of subject and verb in Kalamang?, and sampled 5 sentence pairs at a time with the requisite parts of speech until the model proposed an answer to the question.If the model responded that it was unclear, this would repeat up to a maximum of 10 iterations.We only performed this process once, for GPT-3.5-turbo and GPT-4-turbo respectively.Each model used its self-induced grammar sketch at test time. 7For vocabulary induction, we followed a similar process as in the colours domain.8</p>
<p>Evaluation We use corpus-level chrF.To evaluate the grammar sketch and vocabulary induction, we respectively compare to the ground truth wordlist and grammar sketch.</p>
<p>Results</p>
<p>In this section, we first evaluate models' concrete performance across different domains (RQ1).We highlight the significant improvement instruction inference offers in some cases in synthetic tasks, yet despite this, improvements are not uniform across tasks, and not attested in challenging domains like Kalamang.Our analysis also highlights an inconsistent correlation between the quality of generated hypotheses and few-shot learning success (RQ2), meaning that the ability to generate or follow instructions doesn't reliably predict task mastery or vice versa.These results (Table 3) suggest that while structured instructions can boost performance in simpler scenarios, their impact is less predictable in complex settings.Furthermore, the relationship between instruction induction, instruction following, and in-context learning is complex, and each capability may rely on separate unknown aspects of model architecture, training procedures, or data.</p>
<p>When Does Instruction Inference Improve Over In-Context Learning?</p>
<p>How effective is using the true instruction?In the domains we study, the ground-truth instruction tends to yield accurate results.Figure 3 displays mean accuracy in the linear functions and colours domains over six trials, sampled at different temperatures (see Appendix A) for details.Notably, in linear functions (3A), GPT-4-turbo's accuracy increases to 96% from a baseline of 30%, with GPT-3.5 also notably improving.In the colours domain, (3B), we see the true instruction also helps all models except for Llama-2-7b.However, this trend does not extend to the Kalamang task, where most models struggled to leverage the provided wordlist and grammar sketch effectively, indicated by chrF scores in Figure 4.</p>
<p>How effective are models' induced instructions?Self-generated instructions also improve on the baseline in many cases, with variations by domain.In linear functions, models' hypothesis induction markedly surpasses the few-shot baseline, with both the verbalized confidence and log-probability based reranking methods yielding comparable improvements (see Figure 3A again).For the colours domain (3B), instruction inference benefits performance, though not as strongly as in linear functions.chrF scores follow a similar trend, and are depicted in Appendix G. Interestingly, for gpt-4-turbo and Llama-2-7b, using the models' self-proposed hypotheses benefits performance more than using the ground-truth grammar for the colours language, despite the fact that self-proposed are not always correct.</p>
<p>Unlike the two synthetic domains, inducing grammar and vocabulary items for Kalamang does not improve translation metrics in most cases.</p>
<p>How Does the Ability to Induce Instructions Relate to In-Context Learning?</p>
<p>GPT-3.5-turbo</p>
<p>GPT-4-turbo</p>
<p>Figure 2: Real coefficients of linear functions and relationship to hypothesized coefficients for GPT-3.5-turbo and GPT-4-turbo.Remaining models can be found in Appendix D. The x-axis has been truncated for visualization purposes (as there are some large outlier hypotheses).GPT-4-turbo is able to induce a reasonable function in-context, but other models struggle.</p>
<p>How accurate are model-generated hypotheses?</p>
<p>In assessing accuracy of models' self-generated hypotheses across different domains, our findings reveal significant variations in accuracy.</p>
<p>In the linear functions domain, we plot hypotheses generated by two models in Figure 2, and list the Spearman ρ (Spearman, 1904) as well as p-values of model-predicted coefficients in Table 4. GPT-3.5-turbo and GPT-4-turbo's proposed coefficients are positively correlated with the real coefficients, and this is statistically significant.However, this level of accuracy is not observed with Llama-2 models, indicating a disparity in model capabilities.</p>
<p>In the colours domain, most models, except GPT-4-turbo, tend to generate inaccurate hypotheses.The exact accuracy of proposed hypotheses for colours is shown in Appendix H. Mappings of simple vocabulary items such as lug tend to more accurate, with GPT-4-turbo achieving an 87% mean accuracy for hypotheses about this word.On the other hand, the difficulty increases with the terms which involved repeats, with GPT-3.5-turboonly achieving a 15% mean accuracy on bluf, the "repeat twice" term.However, when extending these analyses to Kalamang, all models' performance in predicting grammar features is relatively poor, with GPT-3.5-turbopredicting 5/18 and 4/18 features correct respectively.Appendix L shows the correctness of each grammar feature.Vocabulary induction accuracy is also generally low, falling between 10-20% for most models in both the English to Kalamang as well as Kalamang to English directions.See Appendix M for details, as well as averaged segment-level chrF for the vocabulary hypotheses.</p>
<p>Is abductive reasoning related to in-context learning ability?</p>
<p>In our final analysis, we examine whether the ability to induce correct instructions correlates with success in incontext learning.Specifically, we focus on the final hypotheses selected by the gpt model family, given that many hypotheses generated by llama models were incoherent or not formatted correctly.We use the point-biserial correlation (Pearson, 1895) to assess the relationship between the accuracy of a model's final hypothesis and its success in in-context learning.</p>
<p>In linear functions, we examine GPT-3.5-turbo because of GPT-4-turbo's consistent accuracy in selecting correct hypotheses. 9The analysis reveals a chance-level agreement (0.0013), suggesting GPT-3.5-turbo may be able to predict outputs without identifying the underlying function well, or vice versa.This reveals a dissociation between prediction accuracy and instruction induction.</p>
<p>In the colours domain, we examine both gpt models and conduct a similar analysis for each nonce word.We divide the test examples into examples containing each word, and examine the point-biserial correlation between accuracy of induced word meanings and correct translations in the few-shot context.This correlation is generally low, and few p-values are significant after correcting for multiple comparisons with false discovery rate (FDR) (Benjamini &amp; Hochberg, 1995).See Appendix H for details.</p>
<p>In Kalamang, we repeat the process of computing point-biserial correlation between vocabulary induction correctness with segment-level chrF in the few-shot translations.Unlike the other domains, there is a small positive correlation between correct vocabulary hypotheses and chrF in gpt models.See Appendix M for details.We note that chrF is a more finegrained measure than accuracy, and the initial scores were low enough that copying some correct vocabulary items may have had a slight impact on otherwise completely incorrect translations.</p>
<p>Related Work</p>
<p>Hypothesis Proposal with Language Models Recent work explores hypothesis proposalto improve language model performance in synthetic tasks, namely ACRE, the original MiniSCAN, ListOps, and versions of the ARC dataset (Qiu et al., 2024;Wang et al., 2024).These methods often rely on domain-specific interpreters or code generation by LMs, akin to our functions domain's ground-truth reranker.We additionally explores probabilitybased reranking for rule selection across different domains and assesses the accuracy of model-induced rules prior to reranking.</p>
<p>Language models have also been used to automate hypothesis discovery as an end in itself, to discover distributional differences in text (Zhong et al., 2022;2023).In this case, the hypothesis proposer LM is paired with a validator trained to filter out irrelevant hypotheses.</p>
<p>We similarly find that hypotheses generated by language models themselves may not be very accurate inherently.Abductive Reasoning in Language Models Datasets for abductive commonsense reasoning (Wang et al., 2019;Bhagavatula et al., 2020;Zhang et al., 2020) and logical reasoning (Sinha et al., 2019;Yang et al., 2024) have been proposed.Bhagavatula et al. (2020) focuses on the ability for a model to select the more plausible hypothesis from pairs of hypotheses, while Yang et al. (2024) focuses on proposing natural language rules and evaluating them against ground-truth human rules.Zhao et al. (2023) use contrastive explanations to tune a model to recognize fluent ones.Comparatively, we do not focus on commonsense reasoning, and examine relationships between reasoning types on the same task.</p>
<p>Program Synthesis and Library</p>
<p>Learning The general approach we outline for abductive reasoning can be considered a soft form of program synthesis with language models.Library learning, in which a set of reusable tools is learned from examples, is related (Ellis et al., 2023).A similar approach to ours using LLMs is proposed by Zhu et al. (2023), who use a similar two-step process to learn a library of rules for an arithmetic domain as well as a previous synthetic dataset (Sinha et al., 2019).We find similar gains on synthetic domains, and further examine challenges in applying abductive reasoning in complex domains.</p>
<p>LM-generated Instructions</p>
<p>Instruction backtranslation is a related concept (Honovich et al., 2023;Li et al., 2024), in which an LM generates instructions for textual data.However, it differs in that the proposed instructions are not directly used at the same time that they are proposed, and it does not focus on generating rules.</p>
<p>Conclusion</p>
<p>We have examined the interplay between deductive, inductive, and abductive reasoning in LMs through the tasks of hypothesis proposal, in-context learning, and self-generated instruction following.Across three domains (linear function learning, artificial language translation, and Kalamang translation), we show that instruction inference is able to improve over few-shot prompting in simple synthetic domains, but that the relationship between these types of reasoning is complex, and they may not work together as expected presently when models are solving complex tasks.As abductive reasoning seems to be a relatively weaker capability in current language models as compared to instruction following, future work could develop more advanced mechanisms for natural-language hypothesis verification and correction.The use of hypothesis proposal during training remains underexplored, and joint training of models on question answering and hypothesis proposal with enforced consistency may help models display more consistent behaviour.Enhancements in these areas could accelerate progress towards models capable of autonomous learning and self-improvement.</p>
<p>B Prompts for Linear Functions Domain</p>
<p>Table 5: Prompts for the functions domain.Newlines are depicted visually for ease of reading.Variables that are substituted depending on the question are marked like {this}.The wording for the "prompt with self-induced hypothesis" and "zero-shot chain-of-thought prompt" were slightly changed from the few-shot examples prompt because models were sometimes confused by long-winded hypotheses, and responded with a long-winded answer in return, causing failures to parse their answers.In comparison, models mostly returned outputs in correct formats in other settings.</p>
<p>Prompt Type</p>
<p>Usage Prompt Text</p>
<p>Base system prompt</p>
<p>For reasoning with in-context examples</p>
<p>You are a problem solving system.Your job is to use the input-output pairs to solve the problem as well as you can.</p>
<p>Hypothesis proposal system prompt</p>
<p>For proposing hypotheses based on in-context examples</p>
<p>You are a pattern recognition system.Your job is to come up with a function that describes the data as well as you can.</p>
<p>Instruction following system prompt</p>
<p>For applying a proposed hypothesis or ground-truth hypothesis to the input You are a problem solving system.Your job is to apply the function to the data in order to produce an answer.... Function (please write explicitly in the exact form" 'Output: y = axˆ0 + bxˆ1)':</p>
<p>Few</p>
<p>Table 5: Prompts for the functions domain.Newlines are depicted visually for ease of reading.</p>
<p>Variables that are substituted depending on the question are marked like {this}.The wording for the "prompt with self-induced hypothesis" and "zero-shot chain-of-thought prompt" were slightly changed from the few-shot examples prompt because models were sometimes confused by long-winded hypotheses, and responded with a long-winded answer in return, causing failures to parse their answers.In comparison, models mostly returned outputs in correct formats in other settings.</p>
<p>Prompt with a self-induced hypothesis</p>
<p>Used similarly to the "prompt with ground-truth hypothesis", except with a self-generated hypothesis.The wording is slightly changed.</p>
<p>Use this function to apply to the input example to get the correct output.
{</p>
<p>C Prediction Fit of Other Models on Linear Functions</p>
<p>Figure 5 shows predictions made by each model when using a few-shot prompt, true instruction, or self-induced instruction.</p>
<p>D Predicted Coefficients Compared to Real Coefficients in Linear Functions</p>
<p>E Grammar and Details of Colours Domain</p>
<p>The rules of the colours domain are expressed through production rules below.Models were found to respond better to verbal instructions than formal ones in initial testing (e.g. a verbal statement "repeat twice" rather than
[[x]]bluf → [[x]][[x]])
Listing 1: Grammar of the colours language, as presented to LMs. lug -&gt; blue dax -&gt; green wif -&gt; red zup -&gt; yellow bluf -&gt; repeat the last action twice walm -&gt; repeat the last action three times Training and test data was automatically generated by generating sentences of up to 5 nonce words on the source side, with shorter sentences being more likely (the respective probabilities for sentence lengths from 1 to 5 are [0.4, 0.3, 0.15, 0.1, 0.05]).Each colour could also be repeated with the repeat actions, and whether repeat nonce words were inserted was also random, though skewed toward no repetition (the probabilities were respectively [0.8, 0.1, 0.1] for no repeats, one repeat, or two repeats).A repeat term never followed another repeat term, and the same colour word never appeared twice consecutively to make it easier to learn the repeat terms.</p>
<p>Additionally, we provided a fixed set of few-shot examples that covered all the nonce terms:</p>
<p>F Prompts for Colours Domain</p>
<p>Table 6: Prompts for the colours domain.Newlines are depicted visually for ease of reading.Variables that are substituted depending on the question are marked like {this}.The "prompt with a self-induced hypothesis" was slightly modified from the base prompt in order to encourage models to follow the correct formatting, while the "prompt for zero-shot chain-of-thought" was slightly modified to encourage models to generate a concrete chain of thought, and also to follow the correct formatting.</p>
<p>Prompt Type</p>
<p>Usage Prompt Text</p>
<p>Base system prompt</p>
<p>For reasoning with in-context examples</p>
<p>You are a problem solving system.Your job is to use the input-output pairs to solve the problem as well as you can.''6: Prompts for the colours domain.Newlines are depicted visually for ease of reading.Variables that are substituted depending on the question are marked like {this}.The "prompt with a self-induced hypothesis" was slightly modified from the base prompt in order to encourage models to follow the correct formatting, while the "prompt for zero-shot chain-of-thought" was slightly modified to encourage models to generate a concrete chain of thought, and also to follow the correct formatting.</p>
<p>Hypothesis</p>
<p>Prompt with ground-truth hypothesis</p>
<p>Used when prompting the model to directly apply the correct hypothesis to the input.In-context examples are also included.</p>
<p>Use this grammar to parse the input example to get the correct output.6: Prompts for the colours domain.Newlines are depicted visually for ease of reading.</p>
<p>Variables that are substituted depending on the question are marked like {this}.The "prompt with a self-induced hypothesis" was slightly modified from the base prompt in order to encourage models to follow the correct formatting, while the "prompt for zero-shot chain-of-thought" was slightly modified to encourage models to generate a concrete chain of thought, and also to follow the correct formatting.</p>
<p>G chrF for Colours Domain</p>
<p>H Vocabulary Induction Accuracy for Colours Domain</p>
<p>We show in Table 7 the mean accuracy for each colour term for gpt models, computed over all selected (highest-ranked) word hypotheses.Point-biserial correlation between hypothesis correctness (in the instruction-induction case) and few-shot correctness is also shown.P-values are corrected with FDR in the trials for each model.We performed the correction per model rather than across all models because we were separately interested in the behaviour of each model.We parsed the production rule generated by the model in order to determine hypothesis correctness.</p>
<p>I MTOB Experimental Settings</p>
<p>We generally used the same experimental setup as in mtob.For the few-shot condition, we used two reference sentences for each word on the source side, selected via longest subsequence.For the true-instruction setting, we used reference sentences, in addition to the wordlist and true grammar sketch.For each word in the source sentence, the most similar word from the wordlist was also retrieved based on the longest common substring.In the instruction-inference settings, the model's self-induced grammar sketch was substituted for the true grammar sketch, and the model was also prompted to first create hypotheses about the translation of each word in the source sentence.</p>
<p>At the time that we conducted experiments, the Kalamang to English training set was not available, so we created this training set by reversing the source and target in the English to Kalamang training set.This may have slightly harmed translations in this direction, as this caused the training set to be different from the one reported in the benchmark results.</p>
<p>J MTOB Prompts</p>
<p>Figure 1 :
1
Figure 1: Diagram of abductive reasoning for an LM.Red arrows show data flow in inductive reasoning (few-shot prompting), while blue arrows show data flow in deductive reasoning (instruction following).Black arrows indicate data flow unique to abductive reasoning (instruction induction).Instruction inference generally improves on few-shot prompting and zero-shot chain of thought.However, success at inductive reasoning and success at instruction inference are not related.</p>
<p>, 2022; Aky ürek et al., 2023), we construct a dataset of 40 linear functions f (x) = ax + b, where a and b are uniformly sampled from the integers [−20, 20], along with 5 test examples for each function, yielding 200 test examples.For each test example, we randomly generate 5 in-context examples of the function (x i , y i ) 5 i=1 and one test example.The test example and inputs for in-context examples are also uniformly sampled from the integers</p>
<p>Figure 3 :
3
Figure 3: Accuracy of models in synthetic domains with and without hypothesis generation.Error bars indicate standard error.The top row shows results for the functions domain, while the bottom row shows results for the colours domain.Results are aggregated across 6 runs, and zero values are marked with '0'.</p>
<p>Figure 4 :
4
Figure 4: chrF scores for Kalamang under different methods, in English to Kalamang direction (top row) and Kalamang to English direction (bottom row)</p>
<p>Figure 5 :
5
Figure 5: Model predictions plotted against true function output for all models.Range is restricted to the [-400, 400] range for visualization purposes, although there are large outlier values for all models.</p>
<p>Figure 6 Figure 6 :
66
Figure 6 plots model hypotheses about the coefficients of x 0 and x 1 in a linear function against the actual coefficients</p>
<p>walm Output: red red red Input: lug walm dax bluf Output: blue blue blue green green</p>
<p>Figure 7 Figure 7 :
77
Figure 7 depicts the mean chrF score over 6 trials for each model in each setting.Reranking methods are averaged for instruction-induction.</p>
<p>Table 2 :
2
Examples of in-context examples, queries, and hypotheses in each domain.</p>
<p>Instruction inference [Abduction with many hypotheses] After</p>
<p>generating n hypotheses H = {h 1 , ..., h n }, we explore methods for reranking them.For all experiments, n = 5.
Gen-erally, these reranking methods capture "fit" to training data. External validator rerankingwas used only in the functions domain (see Section 4.2). The other reranking methods useda language model. Given scores of each hypothesis, we choose:</p>
<p>confidence We directly prompt the model to estimate the probability of the hypothesis given D i .score Verbal (h i , D i ) is set to the model's probability estimation.
P(data) We use a separate LM, 4 to generate log probabilities for in-context examples giventhe hypothesis. score P(data) (h i , D i ) is the sum of log-probabilities of tokens of D i .P(answer) This is similar to P(data) and uses the same template, but score P(answer) (h i , D i )only sums log-probabilities of tokens in answers y i in the in-context examples.External validator For structured hypotheses, it is possible to parse them and apply themback to in-context examples, with the score as the negative error. It may not be possible toparse all hypotheses, if they are in natural language or inconsistent formats.</p>
<p>Table 4 :
4
Spearman correlation and p-values of true vs predicted coefficients for each model in the functions domain.
Modelb corr.b p-vala corra p-valGPT-3.5-turbo0.275.0 × 10 −150.562.7 × 10 −67GPT-4-turbo0.852.2 × 10 −2920.910.0Llama-2-7b-0.00690.900.140.0081Llama-2-70b0.140.060-0.0180.82</p>
<p>model's hypothesis }
However, just write the output likewhat's shown in these examples.Input: {input1}Output: {output1}Input: {input2}Output: {output2}...Return the output preceded by 'Output:'Input: {query input}Return the output preceded by 'Final Output:'Input: {input1}Output: {output1}Input: {input2}Prompt forUsed to encourageOutput: {output2}zero-shotthe model to...chain-of-thoughtgenerate a chainLet's think step by step about what the functionof thought.could be. Remember to write down 'Final Output:'before your final answer.Input: {query input}How likely is this hypothesisabout the function to be true given the data?Examples:Input: {input1}Output: {output1}Input: {input2}Prompt forUsed to prompt aOutput: {output2}hypothesislanguage model...probabilitydirectly forFunction explanation: {model's hypothesis}estimatererankinghypothesesPlease give a probability between 0 and 1 inclusive,and only answer with a number.Probability:These are examples of applying this function:{model's hypothesis}Examples:Prompt for data logprobs givenUsed to rerank hypotheses basedInput: {input1} Output: {output1}hyp estimateon logprobs.Input: {input2}Output: {output2}...</p>
<p>Table
For proposingYou are a rule induction system. Your job is to figure out theproposal systemhypotheses basedrules underlying a problem and report on them. Use the examplesprompton in-contextto guide your thinking.examplesInstructionFor applying aYou are a parser. Carefully use the grammar to parse inputs tofollowing systemproposeddetermine the correct output.prompthypothesis orground-truthhypothesis to theinputReturn the output preceded by 'Output:'Input: {input1}Output: {output1}Few-shotFor reasoningInput: {input2}examples promptwith in-contextOutput: {output2}examples only...Input: {query input}Remember to start your answer with 'Output:'</p>
<p>The below examples contain the nonce word {word}.
TableGrammar:{colours domain grammar}Examples:Input: {input1}Output: {output1}Input: {input2}Output: {output2}...Return the output preceded by 'Output:'Input: {query input}Prompt forUsed to have thehypothesismodel propose ainductionsingle hypothesisfor the translationUsing the examples, deduce what {word} means.of a word.Input: {input1}Output: {output1}Input: {input2}Output: {output2}...Write your answer like this: word -&gt; meaning.Meaning can be a word or a general rule dependent on the context.Rule:Use this grammar to parse the input exampleto get the correct output.{ model's hypothesis grammar }However, just write the output likePrompt with aUsed similarly towhat's shown in these examples.self-inducedthe "prompt withInput: {input1}hypothesisground-truthOutput: {output1}hypothesis",Input: {input2}except with aOutput: {output2}self-generated...hypothesis. TheReturn the output preceded by 'Output:'wording is slightlyInput: {query input}changed.</p>
<p>Table 8 :
8
Tanzer et al. (2024) translation with MTOB.We follow the same prompt formatting as inTanzer et al. (2024), with the exception of replacing retrieved grammar book passages with the grammar sketch in Appendix K. Newlines are depicted visually for ease of reading.Variables that are substituted depending on the question are marked like {this}.The English to Kalamang direction is depicted in the examples.'''' indicates that the prompt is the same as in Appendix F.
Prompt TypeUsagePrompt TextBase systemFor reasoning''''promptwith in-contextexamplesHypothesisFor proposing''''proposal systemgrammar featureprompthypotheses based(Grammar)on in-contextexamplesHypothesisFor proposing''''proposal systemvocabularyprompt (Vocab)mappings basedon in-contextexamplesInstructionFor applying a''''following systemproposedprompthypothesis orground-truthhypothesis to theinput
Code and data, including generated hypotheses may be found at https://github.com/ nightingal3/rule induction
We note that there is some disagreement in the philosophy literature about the exact distinction about inductive and abductive reasoning. Here we use commonly-cited definitions, which happen to distinguish in-context learning from the more explicit process of verbalizing and testing hypotheses.
Several recent papers have recently proposed similar processes, but called this inductive reasoning. We believe that abductive reasoning may be a more apt term in any process that includes hypothesis evaluation and selection. See Section 6 for more discussion.
text-davinci-002 was used as the logprobs-generating LM for all models.
If a model produced an unparsable hypothesis (for instance, I don't know, or a generic answer like y = ax + b), that hypothesis was assigned a score of −∞.
We generate this dataset instead of using the original miniscan to mitigate memorization of the original nonce words, as well as to gain more test examples, as the original dataset has only 10.
Llama-2 models were found to be unable to propose grammar features with our prompts, so we used the GPT-3.5-turbo induced grammar sketch for these models.
For Kalamang, due to computational costs, we cache the first parseable hypothesis proposed for each word and reuse it on subsequent sentences containing that word.
This means agreement cannot be computed with the point-biserial correlation.
AcknowledgementsThank you to Kiril Gashteovski, Ekin Aky ürek, Shuyan Zhou, and Linlu Qiu for helpful discussions on this project.Additional thanks to David Mortensen for suggestions on grammar features and linguistic consultation.Thank you to NEC Labs Europe for supporting this project through the Student Research Fellowship program.We also acknowledge the support of the National Sciences and Engineering Research Council of Canada (NSERC) through a postgraduate fellowship given to EL (PGSD).We also appreciate support from Intel and the National Science Foundation through grants CCF-2217064 and IIS-2238240.AppendixA Model Inference SettingsGeneration settings, as well as other details, used in the three domains are detailed here.Each model was tested 6 times for each setting (few-shot, zs-cot...) at different temperatures, and the aggregate results are shown in figures and tables throughout the paper.The exact model versions used for gpt models were gpt-3.5-turbo-0613and gpt-4-1106-preview.Functions When answering questions in the base task, gpt models were tested 3 times each at temperature T = {0, 1} (note that T = 0 is nondeterministic in gpt models because of hardware).No max number of tokens was set for the generation.For llama models, T = {0.1,1} was used, also with no max number of tokens.When generating hypotheses, we always used the same model as performed the base task.That is to say, gpt-3.5-turbowould generate hypotheses used by gpt-3.5-turbo,and so on.Hypotheses were always generated with a temperature above 0 to encourage generation of diverse hypotheses.For gpt models, hypotheses were generated with T = 1 and for llama models, with T = 1.0625.The specific value for llama models was because llama would usually generate the same hypothesis 5 times at T = 1, but higher values greatly increased the number of nonsensical and badly formatted hypotheses.When self-evaluating hypotheses, the verbalized confidence score was generated at T = 0.When using the log-probabilities from text-davinci-002 to rerank hypotheses, the model was also set to T = 0.Colours As in the functions domain, gpt models and llama models were respectively tested 3 times each at temperature T = {0, 1} and T = {0.1,1} with no max number of tokens.When generating hypotheses, T = 1 was used for all models.When self-evaluating hypotheses, settings were the same as in the functions domain.Kalamang For the base translation task, the same settings were used for generation as inTanzer et al. (2024).Due to cost constraints, we ran only once in each translation direction on each setting with T = 0.05.Vocabulary hypotheses for all models were generated with T = 1.Grammar feature hypotheses were generated independently of translations, and the first non-null hypothesis was chosen due to cost constraints.T = 0.7 was used for gpt grammar hypotheses, and T = 1 was used for llama grammar hypotheses.When self-evaluating hypotheses, T = 0 was once again used for all models.Published as a conference paper at COLM 2024   Published as a conference paper at COLM 2024K Kalamang Grammar SketchModelGrammar feature accuracy GPT-3.5-turbo27.78%GPT-4-turbo 22.22%Llama-2-7B 0%Llama-2-70B 0%Table9: Overall grammar sketch accuracy of models.Unsure answers were marked as incorrect.M Vocabulary Induction Accuracy for MTOBThe overall vocabulary induction accuracy for gpt models is shown in Table10.Accuracy and chrF were calculated overall when exclude morphology is marked No, and morphologically rich words in the dictionary (marked with * or with -to represent suffixes/prefixes) were excluded when this value was marked Yes.For words with multiple translations in the dictionary, the translation was marked correct if it matched any of the possible translations.Words without an entry in the dictionary were skipped.If a null hypothesis such as I don't know was proposed, it was marked as incorrect.Due to hypotheses for this task being more difficult to automatically evaluate, an author also annotated the correctness of results for a single setting (instruction-inference:verbalized confidence) in order to calculate the association between translation quality and hypothesis correctness more accurately.Results can be found in Table11.Published as a conference paper at COLM 2024N Example Kalamang TranslationsTable12shows example translations made by GPT-3.5-turbo in both directions.
What learning algorithm is in-context learning? investigations with linear models. Ekin Aky Ürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou, Proceedings of the Eleventh International Conference on Learning Representations (ICLR). the Eleventh International Conference on Learning Representations (ICLR)2023</p>
<p>Learning with latent language. Jacob Andreas, Dan Klein, Sergey Levine, 10.18653/v1/N18-1197Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Controlling the false discovery rate: a practical and powerful approach to multiple testing. Yoav Benjamini, Yosef Hochberg, Journal of the Royal Statistical Society. Series B (Methodological). 5711995</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih, Choi, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Igor Douven, The Stanford Encyclopedia of Philosophy. Edward N Zalta, 2021Metaphysics Research Lab, Stanford UniversitySummer 2021 edition</p>
<p>10.5281/zenodo.7385533WALS Online (v2020.3). Matthew S Dryer, Martin Haspelmath, 2013</p>
<p>Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sablé-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, Joshua B Tenenbaum, 10.1098/rsta.2022.0050Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 381202320220050</p>
<p>What can transformers learn in-context? a case study of simple function classes. H Frankfurt ; Shivam, Dimitris Garg, Percy Tsipras, Gregory Liang, Valiant, Advances in Neural Information Processing Systems. 1958. NeurIPS 2022552022Peirce's notion of abduction</p>
<p>Inductive Logic. James Hawthorne, The Stanford Encyclopedia of Philosophy. Edward N Zalta, Spring 2021 edition, 2021Metaphysics Research Lab, Stanford University</p>
<p>Instruction induction: From few examples to natural language task descriptions. Or Honovich, Uri Shaham, R Samuel, Omer Bowman, Levy, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. M Brenden, Marco Lake, Baroni, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. Jennifer G Dy, Andreas Krause, the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLRJuly 10-15, 2018. 201880of Proceedings of Machine Learning Research</p>
<p>Human few-shot learning of compositional instructions. M Brenden, Tal Lake, Marco Linzen, Baroni, Annual Meeting of the Cognitive Science Society. 201958006558</p>
<p>Self-alignment with instruction backtranslation. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, Mike Lewis, Proceedings of the Twelfth International Conference on Learning Representations (ICLR). the Twelfth International Conference on Learning Representations (ICLR)2024</p>
<p>Inference to the Best Explanation. Peter Lipton, 2001Routledge</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Felix, Juston Sim Ón Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Jan Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Ashvin Mishkin ; Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Daniel Perelman ; John Schulman, Kyla Selsam, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Wojciech Wong ; Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Barret Zhuk, Zoph, Felipe Petroski Such. Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,2024Amin TootoonchianFilipe de Avila Belbute Peres ; Juan Felipe Cer ón Uribe, Andrea Vallone, Arun VijayvergiyaSherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu. Gpt-4 technical report</p>
<p>Notes on regression and inheritance in the case of two parents. Karl Pearson, Proceedings of the Royal Society of London. 581895</p>
<p>. Charles Sanders, Peirce , Collected Papers of Charles Sanders Peirce. 51965Harvard University Press</p>
<p>chrF: character n-gram F-score for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren, Proceedings of the Twelfth International Conference on Learning Representations (ICLR). the Twelfth International Conference on Learning Representations (ICLR)2024</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Proceedings of the Tenth International Conference on Learning Representations (ICLR). Edward N Zalta, Uri Nodelman, the Tenth International Conference on Learning Representations (ICLR)2022. 2024Metaphysics Research Lab, Stanford UniversityThe Stanford Encyclopedia of Philosophy. Spring 2024 edition</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>. Hedvig Skirgård, Hannah J Haynie, Damián E Blasi, Harald Hammarstr Öm, Jeremy Collins, Jay J Latarche, Jakob Lesage, Tobias Weber, Alena Witzlack-Makarevich, Sam Passmore, Angela Chira, Luke Maurits, Russell Dinnage, Michael Dunn, Ger Reesink, Ruth Singer, Claire Bowern, Patience Epps, Jane Hill, Outi Vesakoski, Martine Robbeets, Daniel Noor Karolin Abbas, Nancy A Auer, Giulia Bakker, Robert D Barbos, Swintha Borges, Luise Danielsen, Ella Dorenbusch, John Dorn, Giada Elliott, Jana Falcone, Yustinus Fischer, Hannah Ghanggo Ate, Hans-Philipp G Gibson, Jemima A Öbel, Victoria Goodall, Andrew Gruner, Rebekah Harvey, Leonard Hayes, Roberto E Herrera Heer, Miranda, Biu Nataliia H Übler, Jessica K Huntington-Rainey, Marilen Ivani, Erika Johns, Eri Just, Carolina Kashima, Janina V Kipf, Nikita K Klingenberg, Aikaterina Önig, Koti, G A Richard, Olga Kowalik, Krasnoukhova, L M Nora, Mandy Lindvall, Hannah Lorenzen, T Lutzenberger, R A Martins, Celia Mata German, Suzanne Van Der Meer, Jaime Montoya Samamé, Saliha Michael M Üller, Kelsey Murado Glu, Johanna Neely, Miina Nickel, Cheryl Akinyi Norvik, Jesse Oluoch, India O C Peacock, Naomi Pearey, Stephanie Peck, S Petit, Mariana Ören Pieper, Daniel Poblete, Javier Prestipino, Judith Vera, Tim Voß, Henry Witte, Stephanie Wu, Jingting Yam, Maisie Ye, Tessa Yong, Roberto Yuditha, Zariquiey, 10.1126/sciadv.adg6175Linda Raabe, Amna Raja, Janis Reimringer, Sydney C. Rey, Julia Rizaew, Eloisa Ruppert, Kim K. Salmon, Jill Sammet, Rhiannon Schembri, Lars Schlabbach, Frederick W.P. Schmidt, Amalia Skilton, Wikaliler Daniel Smith, Hilário de Sousa, Kristin Sverredal, Daniel Valle,Martin Haspelmath2023Robert Forkel, Nicholas Evans, Stephen C. Levinson; Simon J. Greenhill, Quentin D. Atkinsonand Russell D. Gray. Grambank reveals global patterns in the structural diversity of the world's languages. Science Advances, 9</p>
<p>The proof and measurement of association between two things. Charles Spearman, American Journal of Psychology. 151904</p>
<p>A benchmark for learning to translate a new language from one grammar book. Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, Luke Melas-Kyriazi, 2024</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Does it make sense? and why? a pilot study for sense making and explanation. Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, Tian Gao, 10.18653/v1/P19-1393Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Hypothesis search: Inductive reasoning with language models. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D Goodman, Proceedings of the Twelfth International Conference on Learning Representations (ICLR). the Twelfth International Conference on Learning Representations (ICLR)2024</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, Proceedings of the Tenth International Conference on Learning Representations (ICLR). the Tenth International Conference on Learning Representations (ICLR)2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS). the 36th Conference on Neural Information Processing Systems (NeurIPS)2022b</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 20241</p>
<p>WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge. Hongming Zhang, Xinran Zhao, Yangqiu Song, 10.18653/v1/2020.acl-main.508Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Abductive commonsense reasoning exploiting mutually exclusive explanations. Wenting Zhao, Justin Chiu, Claire Cardie, Alexander Rush, 10.18653/v1/2023.acl-long.831Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Describing differences between text distributions with natural language. Ruiqi Zhong, Charlie Snell, Dan Klein, Jacob Steinhardt, International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, Baltimore, Maryland, USAPMLR17-23 July 2022. 2022162of Proceedings of Machine Learning Research</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Advances in Neural Information Processing Systems. NeurIPS 2023362023</p>
<p>Large language models can learn rules. Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
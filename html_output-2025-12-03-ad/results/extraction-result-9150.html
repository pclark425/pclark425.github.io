<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9150 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9150</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9150</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-267412874</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03244v2.pdf" target="_blank">Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9150.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9150.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (ScienceWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-0613) acting in the ScienceWorld text-based simulator via Skill Set Optimization (SSO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used as an LLM actor in the ScienceWorld text-based scientific simulator to perform sequential, interactive commonsense science tasks (e.g., measuring melting temperature, heating substances, mixing ingredients) while augmented in-context with skills constructed and refined by the SSO method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 model variant used for ScienceWorld experiments (gpt-4-0613). The paper does not report model parameter counts or detailed training-data composition; embedding model text-embedding-ada-002 was used for state/action similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Commonsense scientific reasoning across multiple elementary science topics (ScienceWorld): tasks covering chemistry/thermodynamics (melting temperature, heating), basic laboratory-style procedures (measurements, mixing), color mixing, plant/animal/growth questions, and genetics-style questions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Interactive sequential decision-making in the ScienceWorld text simulator: the LLM actor must choose actions in-text (e.g., focus, move, activate, heat, mix) to accomplish tasks such as measuring melting temperature, heating substances, filling/using containers, mixing reagents, and answering multi-step science questions; tasks provide intermediate subgoal rewards and a final score (successful completion = 100).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Environment cumulative task score (0–100 for a fully successful task), intermediate/subgoal rewards (discounted future returns), action/subgoal accuracy from example trajectories (proportion), and correctness of LLM self-reporting when claiming execution of a provided skill (~proportion). Comparisons versus baselines (ReAct, Reflexion, CLIN) use average task scores under adaptation and transfer protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported performance improvements: SSO achieved state-of-the-art on ScienceWorld, outperforming the previous state-of-the-art CLIN by an average of ~35% (paper-wide summary). Action-accuracy improvements from using skills (Table 1): Fill container improved from 0.07 to 0.37; Heat substance 0.04 to 0.22; Mix ingredients 0.30 to 0.36. A completed task has a score of 100; SSO attains substantially higher average task scores than baselines on many tasks (paper reports per-task scores and aggregate improvements but does not provide a single overall accuracy percent beyond the cited ~35% advantage).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Factors identified in the paper include: (1) presence of intermediate subgoal rewards (SSO performs best when environment provides intermediate rewards); (2) similarity metric quality for extracting subtrajectories (state/action embedding cosine similarity using text-embedding-ada-002); (3) granularity and length of subtrajectories (they limited subtrajectory lengths to [2,5]); (4) skill retrieval relevance (cosine similarity between current state and skill initial states); (5) quality of skill generation (the GENERATE step uses a generative LLM and is sensitive to generation reliability); (6) self-reporting accuracy (LLM self-reports execution of skills correctly about 70% of the time, which affects observed-value estimates); (7) domain complexity and distracting state information (similarity-based extraction can be less effective with noisy/distracting states); (8) low-level action spaces (domains with character-level or unintuitive actions, e.g., NetHack, reduce similarity effectiveness and make extraction harder); and (9) hyperparameters such as number of retrieved skills, sampling temperature, and score-weighting used in skill scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against ReAct and Reflexion baselines and the prior SOTA CLIN agent; SSO outperforms CLIN on ScienceWorld (average ~35% improvement reported) and outperforms ReAct and Reflexion in adaptation and transfer evaluations described in the paper. The paper also compares skill-based in-context examples vs. few-shot full trajectories and reports that skills provide better performance with substantially shorter contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations: (1) the similarity metric (state/action embedding cosine similarity) can be ineffective in environments with distracting state information or low-level actions; (2) SSO is most effective when intermediate rewards are available—while it can operate without them, designers should include intermediate subgoal rewards for best performance; (3) self-reporting by the LLM to identify skill execution is imperfect (~70% correct), which can bias skill observed-values; (4) the GENERATE step depends on a generative-LLM and may increase cost or vary in reliability; (5) standard-error reporting was limited for ScienceWorld due to compute cost, so per-task variability is not fully quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend: (1) include intermediate subgoal rewards in environment design for best SSO performance; (2) improve or replace the current similarity metric for extracting transferable subtrajectories in domains with noisy states or low-level actions; (3) combine SSO with complementary techniques (e.g., Reflexion) to leverage negative feedback and failure reflection; (4) tune hyperparameters (skill length, number of retrieved skills) to domain granularity; (5) rely on ongoing environment feedback and skill-refinement (pruning skills with non-positive observed returns) to maintain a useful skill set; and (6) prefer creating skills from multiple similar subtrajectories to encourage abstraction and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>All information above is drawn directly from the paper's reported experiments on ScienceWorld (GPT-4 gpt-4-0613) and the SSO method; where the paper did not specify model internals (parameter counts, training data), those details are not asserted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is your agent smarter than a 5th grader? <em>(Rating: 2)</em></li>
                <li>Clin: A continually learning language agent for rapid task adaptation and generalization <em>(Rating: 2)</em></li>
                <li>Expel: Llm agents are experiential learners <em>(Rating: 1)</em></li>
                <li>Voyager: An openended embodied agent with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9150",
    "paper_id": "paper-267412874",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4 (ScienceWorld)",
            "name_full": "OpenAI GPT-4 (gpt-4-0613) acting in the ScienceWorld text-based simulator via Skill Set Optimization (SSO)",
            "brief_description": "GPT-4 was used as an LLM actor in the ScienceWorld text-based scientific simulator to perform sequential, interactive commonsense science tasks (e.g., measuring melting temperature, heating substances, mixing ingredients) while augmented in-context with skills constructed and refined by the SSO method.",
            "citation_title": "Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills",
            "mention_or_use": "use",
            "model_name": "gpt-4-0613",
            "model_description": "OpenAI GPT-4 model variant used for ScienceWorld experiments (gpt-4-0613). The paper does not report model parameter counts or detailed training-data composition; embedding model text-embedding-ada-002 was used for state/action similarity.",
            "scientific_subdomain": "Commonsense scientific reasoning across multiple elementary science topics (ScienceWorld): tasks covering chemistry/thermodynamics (melting temperature, heating), basic laboratory-style procedures (measurements, mixing), color mixing, plant/animal/growth questions, and genetics-style questions.",
            "simulation_task": "Interactive sequential decision-making in the ScienceWorld text simulator: the LLM actor must choose actions in-text (e.g., focus, move, activate, heat, mix) to accomplish tasks such as measuring melting temperature, heating substances, filling/using containers, mixing reagents, and answering multi-step science questions; tasks provide intermediate subgoal rewards and a final score (successful completion = 100).",
            "evaluation_metric": "Environment cumulative task score (0–100 for a fully successful task), intermediate/subgoal rewards (discounted future returns), action/subgoal accuracy from example trajectories (proportion), and correctness of LLM self-reporting when claiming execution of a provided skill (~proportion). Comparisons versus baselines (ReAct, Reflexion, CLIN) use average task scores under adaptation and transfer protocols.",
            "simulation_accuracy": "Reported performance improvements: SSO achieved state-of-the-art on ScienceWorld, outperforming the previous state-of-the-art CLIN by an average of ~35% (paper-wide summary). Action-accuracy improvements from using skills (Table 1): Fill container improved from 0.07 to 0.37; Heat substance 0.04 to 0.22; Mix ingredients 0.30 to 0.36. A completed task has a score of 100; SSO attains substantially higher average task scores than baselines on many tasks (paper reports per-task scores and aggregate improvements but does not provide a single overall accuracy percent beyond the cited ~35% advantage).",
            "factors_affecting_accuracy": "Factors identified in the paper include: (1) presence of intermediate subgoal rewards (SSO performs best when environment provides intermediate rewards); (2) similarity metric quality for extracting subtrajectories (state/action embedding cosine similarity using text-embedding-ada-002); (3) granularity and length of subtrajectories (they limited subtrajectory lengths to [2,5]); (4) skill retrieval relevance (cosine similarity between current state and skill initial states); (5) quality of skill generation (the GENERATE step uses a generative LLM and is sensitive to generation reliability); (6) self-reporting accuracy (LLM self-reports execution of skills correctly about 70% of the time, which affects observed-value estimates); (7) domain complexity and distracting state information (similarity-based extraction can be less effective with noisy/distracting states); (8) low-level action spaces (domains with character-level or unintuitive actions, e.g., NetHack, reduce similarity effectiveness and make extraction harder); and (9) hyperparameters such as number of retrieved skills, sampling temperature, and score-weighting used in skill scoring.",
            "comparison_baseline": "Compared against ReAct and Reflexion baselines and the prior SOTA CLIN agent; SSO outperforms CLIN on ScienceWorld (average ~35% improvement reported) and outperforms ReAct and Reflexion in adaptation and transfer evaluations described in the paper. The paper also compares skill-based in-context examples vs. few-shot full trajectories and reports that skills provide better performance with substantially shorter contexts.",
            "limitations_or_failure_cases": "Reported limitations: (1) the similarity metric (state/action embedding cosine similarity) can be ineffective in environments with distracting state information or low-level actions; (2) SSO is most effective when intermediate rewards are available—while it can operate without them, designers should include intermediate subgoal rewards for best performance; (3) self-reporting by the LLM to identify skill execution is imperfect (~70% correct), which can bias skill observed-values; (4) the GENERATE step depends on a generative-LLM and may increase cost or vary in reliability; (5) standard-error reporting was limited for ScienceWorld due to compute cost, so per-task variability is not fully quantified in the paper.",
            "author_recommendations_or_insights": "Authors recommend: (1) include intermediate subgoal rewards in environment design for best SSO performance; (2) improve or replace the current similarity metric for extracting transferable subtrajectories in domains with noisy states or low-level actions; (3) combine SSO with complementary techniques (e.g., Reflexion) to leverage negative feedback and failure reflection; (4) tune hyperparameters (skill length, number of retrieved skills) to domain granularity; (5) rely on ongoing environment feedback and skill-refinement (pruning skills with non-positive observed returns) to maintain a useful skill set; and (6) prefer creating skills from multiple similar subtrajectories to encourage abstraction and transfer.",
            "notes": "All information above is drawn directly from the paper's reported experiments on ScienceWorld (GPT-4 gpt-4-0613) and the SSO method; where the paper did not specify model internals (parameter counts, training data), those details are not asserted.",
            "uuid": "e9150.0",
            "source_info": {
                "paper_title": "Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is your agent smarter than a 5th grader?",
            "rating": 2,
            "sanitized_title": "is_your_agent_smarter_than_a_5th_grader"
        },
        {
            "paper_title": "Clin: A continually learning language agent for rapid task adaptation and generalization",
            "rating": 2,
            "sanitized_title": "clin_a_continually_learning_language_agent_for_rapid_task_adaptation_and_generalization"
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners",
            "rating": 1,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "Voyager: An openended embodied agent with large language models",
            "rating": 1,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        }
    ],
    "cost": 0.011154749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills
22 Jun 2024</p>
<p>Kolby Nottingham 
Bodhisattwa Prasad Majumder 
Dalvi Bhavana 
Mishra 
Sameer Singh 
Peter Clark 
Roy Fox 
Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills
22 Jun 20244AB3D762FE99BF9CBB21C11045CDC3BFarXiv:2402.03244v2[cs.LG]
Large language models (LLMs) have recently been used for sequential decision making in interactive environments.However, leveraging environment reward signals for continual LLM actor improvement is not straightforward.We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills.SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill.These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards.Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards.We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement.SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.</p>
<p>Introduction</p>
<p>Large Language Model (LLM) actors have been deployed in interactive domains such as robotics (Ichter et al., 2022;Huang et al., 2022a;b), games (Nottingham et al., 2023a;b), and programming (Chen et al., 2022).Similar to the reinforcement learning (RL) setting, these domains often provide a reinforcement signal in the form of reward, task success, or user feedback.For example, the task in Figure 1   provide a success signal upon measuring the substance's temperature and intermediate rewards for completing subgoals such as activating the stove.However, finetuning an LLM actor directly using a traditional RL policy gradient is often impractical with contemporary LLMs and impossible with black-box closed-source LLMs.Instead, we explore a new paradigm of in-context policy improvement.</p>
<p>In natural language processing (NLP) tasks, in-context learning improves task performance by editing LLM inputs with instructions (Brown et al., 2020), task examples (Wei et al., 2021), or auxiliary tasks (Wei et al., 2022).However, interactive domains require sequential decision making with long trajectories and complex credit assignment (see Section 3), so naively applying in-context learning techniques generalizes poorly and does not scale well.Instead, recent approaches for improving LLM actors construct a "memory" of knowledge about the world (Majumder et al., 2023), skills (Wang et al., 2023a), or task insights (Shinn et al., 2023) to use in-context for policy improvement.Current approaches to constructing "memory" for LLM actors have shortcomings such as a lack of continual memory evaluation, intermediate subgoals, and memory retrieval.</p>
<p>To address these shortcomings, we propose Skill Set Optimization (SSO)1 , a method for automatically constructing skills for in-context policy improvement, where a skill is composed of an initial state, a list of language instructions, and a language subgoal.SSO takes inspiration from both in-context learning and policy optimization to optimize a set of skills for in-context policy improvement.Figure 2 shows how skills are constructed from past trajectories by extracting similar subtrajectories, scoring and sampling sets of subtrajectories that do not overlap, and finally generating an abstract subgoal and instructions for each set of subtrajectories.The constructed skill set is later refined by filtering skills that do not lead to high future rewards when executed.By prioritizing skills according to environment reward at the construction and refinement steps, we identify subgoals that have high impact on policy improvement.Additionally, the subtrajectories' initial states provide a reference for skill retrieval allowing SSO to retrieve skills that are immediately relevant.Finally, because each skill subgoal and instructions are generated from multiple subtrajectories, the resulting subgoals and instructions are often abstract (as in Figure 1) and further facilitate task transfer.</p>
<p>We empirically demonstrate the advantage of using skills for in-context policy improvement and evaluate SSO's ability to rapidly adapt and transfer skill sets in two sequential decision making domains.First, we evaluate SSO on the text-based environment ScienceWorld (Wang et al., 2022) and on the grid-based game NetHack (Küttler et al., 2020).SSO achieves state-of-the-art performance on ScienceWorld, outperforming the previously top performing CLIN agent (Majumder et al., 2023) by an average of 35%.SSO also outperforms baselines on our custom NetHack task by 40% despite its low-level action space.Our analysis and ablations show that SSO continually optimizes an LLM actor's policy by extracting increasingly helpful skills that maximize the task's reinforcement signal.</p>
<p>Related Work</p>
<p>In-Context Learning</p>
<p>Finetuning state-of-the-art LLMs can be prohibitively expensive.In-context learning utilizes an LLM's ability to recognize patterns by augmenting prompts to modify LLM behavior (Brown et al., 2020;Wei et al., 2021;2022).Instruction tuning facilitates this by finetuning LLMs to follow custom instructions (Brown et al., 2020).Previous work has attempted to learn prompts that maximize performance on a dataset (Deng et al., 2022;Zhang et al., 2023;Fernando et al., 2023).However, to maintain interpretability, these methods typically merely edit existing prompts.Also, they require constant evaluation on a downstream task which is often impractical for sequential tasks with delayed rewards.</p>
<p>LLMs for Sequential Decision Making</p>
<p>LLMs are popular tools for planning and high-level decision making in robotic applications (Ichter et al., 2022;Huang et al., 2022a;b) games (Nottingham et al., 2023a;b), programming (Chen et al., 2022), and computer tasks (Kim et al., 2023;Liang et al., 2023).Previous work focuses on improved prompting and evaluation methods for LLM actors without considering approaches for continual learning.This is partly because continual learning for LLMs via finetuning is very expensive and often impractical, especially in sequential decision making domains with long contexts and noisy credit assignment.However, in this paper, we leverage in-context learning for sequential decision making to develop a powerful method for quickly adapting to and generalizing between tasks.</p>
<p>LLMs with Environment Feedback</p>
<p>Some recent work has leveraged task success signals from an environment for in-context policy improvement.The Reflexion (Shinn et al., 2023), Voyager (Wang et al., 2023a), DEPS (Wang et al., 2023b), and ADAPT (Prasad et al., 2023) agents attempt to retry tasks or subgoals after a failure.These methods work by prompting an LLM to reflect on the failed attempt and suggest improvements to make on the next attempt.However, none of these methods, with the exception of Voyager (see below), leverage successful task completions for learning and are not compatible with task transfer or generalization.</p>
<p>This paper focuses on the problem of building a memorylike collection of helpful information for adaptation to new tasks and transfer between tasks.Previous work that has pursued this research direction include the Voyager (Wang et al., 2023a), ExpeL (Zhao et al., 2023), andCLIN (Majumder et al., 2023) agents.Voyager generates javascript skills, self-corrects mistakes with a reflexion-like process, and then stores all successful skills in-context.Similar to SSO, Voyager's skills store instructions for reaching a subgoal.However, Voyager's skills are executed as code and explore future subgoals that are yet to be reached rather than extracting subgoals from past experience to maximize reward.ExpeL prompts an LLM to generate or edit freeform insights based on successful trajectories.Unlike SSO, ExpeL does not leverage partially successful trajectories when generating insights.CLIN generates a memory of insights in the form "action X [may/should] be necessary to do action Y" by prompting an LLM with past trajectories.Like SSO, CLIN does not require a past trajectory to be successful in learning useful information, but CLIN does not take task performance or policy improvement into account when constructing its memory.Additionally, no previous method continually evaluates memories based on environment feedback like in SSO's skill refinement step.</p>
<p>Most of the methods for in-context policy improvement in this section are complementary.For example, the ExpeL agent utilizes reflexion and fewshot trajectories in addition to its memory of insights.However, for the sake of making a direct comparison, our agent only uses SSO for policy improvement, and we choose to compare to one self-correcting method (Reflexion), one insight method (CLIN), and fewshot example trajectories.</p>
<p>In-Context Policy Improvement</p>
<p>With the increased dominance of LLMs in NLP, in-context learning has become an essential tool for improving performance on NLP tasks.Rather than doing expensive finetuning on downstream tasks, in-context learning enables significant increases in task performance just by adding supplementary inputs to the LLM.In-context changes to an LLM actor may be an efficient way to make continual changes to the actor's policy.However, previous work has done little to explore effective methods of leveraging environment feedback for LLM actor in-context learning.</p>
<p>Unlike other NLP tasks, sequential decision making in an interactive environment requires multiple outputs from an LLM actor in the correct sequence to generate a successful trajectory.Sequential decision making is often represented as a Markov Decision Process (MDP) with states s ∈ S, actions a ∈ A, a transition propability function T : S × A × S ′ → [0, 1], and a reward function R : S × A → R.</p>
<p>In the case of an LLM actor, environment states are the text inputs to the LLM, and the output of the LLM is executed as an action in the environment.The objective of an LLM actor is to model an optimal policy π(a|s) that maximizes rewards in a trajectory τ = s 0 , a 0 , ..., s T , a T :
J(π θ ) = E τ ∼π θ ,T |τ | t=0 R(s t , a t )
To avoid traditional gradient-based policy optimization techniques, which can be computationally prohibitive with LLMs or impossible with black-box LLMs, we assume that the policy π is parameterized by text inputs θ that are provided to the agent alongside the state s.A successful in-context policy improvement method will identify a θ that increases the objective J(π θ ).</p>
<p>A straightforward adaptation of in-context learning for incontext policy improvement would be to provide an LLM actor with examples of successful trajectories, thus providing information to the LLM actor about the optimal policy.However, this approach does not scale well as it results in long context lengths and may include redundant information or information that does not transfer well between tasks.Instead, we develop a scalable and transferable mem- ory structure for in-context policy improvement that we call skills.Skills leverage the sequential information from trajectory examples while keeping information brief and modular.A skill is composed of an initial state for used for retrieval, a final state used as a natural language subgoal, and a sequence of actions used as natural language instructions for reaching that subgoal.</p>
<p>To demonstrate the difficultly of using fewshot trajectory examples for in-context policy improvement, we compare the use of fewshot trajectories vs. skills on ScienceWorld (Wang et al., 2022) and NetHack (Küttler et al., 2020) domains (see Section 5).For the fewshot LLM actor, we gather 30 trajectories using an LLM actor and select the three best trajectories to provide in-context at each step.For the skill-based LLM actor, we use the same 30 trajectories to create skills using SSO, and we retrieve three skills to provide in-context at each step.Figure 3 shows how well skills perform vs. fewshot examples for in-context policy improvement.This is especially the case for the NetHack domain where trajectories are longer and actions are expressed characters instead of language actions.Also, the context for the fewshot actor was on average 5x longer than the context for the skill-based actor.The following section ellaborates on how we use SSO to learn and optimize a set of skills for continual in-context policy improvement.</p>
<p>Skill Set Optimization</p>
<p>We propose optimizing LLM actors by providing transferable skills in-context.First, we construct new skills for our skill set from subtrajectories with high rewards to reinforce successful behaviors.Second, we further refine the constructed skill set by filtering skills that do not result in high rewards when used in subsequent trajectories.When using skills in-context, we retrieve relevant skills to include in context based on cosine similarity of skill initial states and the current environment state.Each iteration of SSO includes rolling out a single trajectory with the current LLM actor and skills, constructing new skills, and filtering out skills that did not result in positive rewards in the last trajectory.The updated skill set is then used by the LLM actor in the following iteration.This process is illustrated in Figure 2 and described in the following sections.</p>
<p>Skill Set Construction</p>
<p>A skill is expressed to the actor as an abstract LLMgenerated subgoal and list of instructions for reaching that subgoal.We define a unique skill using one or more subtrajectories where the final states in each of the subtrajectories are used to generate the subgoal and the actions in each subtrajectory are used to generate the instructions for reaching that subgoal.Given the LLM actor's previous trajectories, we extract potential subtrajectories, score them using several heuristics, sample a skill set using beam search, and generate subgoals and instructions for each skill.</p>
<p>EXTRACT</p>
<p>To identify transferable skills, we extract multiple similar subtrajectories to be used to generate each skill.The subtrajectories of a skill must be similar enough that a common abstract subgoal and instructions can be generated.We estimate the similarity of two subtrajectories using the average cosine similarity of each of their state and action embeddings.Using multiple subtrajectories for each skill has two important benefits: (1) the resulting skills are abstract and more transferable, and (2) repeated similar subtrajectories is a strong signal for identifying useful subgoals.</p>
<p>Enumerating every possible set of subtrajectories from an LLM actor's experience would be infeasible, so we only consider pairs of subtrajectories of certain lengths ([2,5] in our experiments) that come from different trajectories.After each completed trajectory, we enumerate its subtrajectories and extract the most similar subtrajectory from each of the N previous completed trajectories, as shown in Algorithm 1.Each of these subtrajectory pairs are considered for potential skills, although most would not result in a useful skill.</p>
<p>SCORE</p>
<p>The majority of extracted subtrajectory pairs must be removed from canidacy for the skill set.Pairs should be similar enough to have common subgoal and instructions, lead to high rewards, and have high coverage of past experience.To accomplish this, we calculate the average state and action similarity, discounted future reward, and length of each pair.We compute a score using a weighted sum of these values to identify which subtrajectory pairs would make useful skills.In our experiments, we set score weights to w 1 = 1, w 2 = 0.1, and w 3 = 0.01 to prioritize first similarity, then reward, and finally length.We utilize subtrajectory pair scores to conduct a beamsearch over pairs to maximize the sum of scores.Many of the extracted potential skills have significant overlap with each other, so we impose a constraint on the beamsearch that there can be no overlap of skill subtrajectories in a skill set.We also include all subtrajectory pairs in the search that were sampled during previous iterations of SSO.The final sampled skill set includes unique skills that prioritize similar, high reward, multi-step subtrajectory pairs.4.1.4.GENERATE Finally, the subtrajectories in each pair are summarized into a single subgoal and list of instructions.We prompt the actor LLM with the pair of subtrajectories and ask it to generate "a numbered list of instructions for completing the skill" and "a single target observation that would indicate the success of the skill".We also ask the LLM to remove any semantically identical skills that may remain.Note that this is the only step in SSO that utilizes a generative-LLM.We found that depending on generative-LLMs for extraction and scoring was less reliable in addition to increasing costs.</p>
<p>Skill Set Refinement</p>
<p>The previously described skill set construction prioritizes including transferable high-reward skills the skill set.Experimentally, including constructed skills in context improves action accuracy across each subgoal in Table 1.We further propose the following method for refining the skill set by evaluating environment rewards after executing a skill.</p>
<p>To evaluate skill performance, we record the discounted future return after executing a particular skill.Since it is not straightforward to identify which skill is being executed at each step, we ask the LLM actor to self-report when it is executing a skill with the prompt "output which of the given subgoals you are targeting next and then output the next action to execute".Table 1 reports correctly self-reports skill execution about 70% of the time.Each time the LLM actor self-reports using a skill, we compute the discounted sum of future rewards and add this to the skill's "observed value".If a skill's observed value is ever below or equal to a threshold ϵ (zero in our experiments), we filter that skill out of the skill set.This process refines our skill set to include skills that demonstrably result in high rewards.Example code for self-refinement can be found in Appendix C.</p>
<p>Experimental Setup</p>
<p>Science World</p>
<p>We evaluate SSO's ability to quickly adapt to and transfer knowledge between tasks in the ScienceWorld domain (Wang et al., 2022).ScienceWorld is a text-based simulator that tests common sense and reasoning.It is organized into various tasks with train and test variants of each.For each of the 18 task classes listed in Figure 4, we test SSO and baselines on 7 to 10 test variants.For example, the Melting Temp task requires the LLM actor to measure the melting temperature of a substance, but what that substance is and the environment setup will be different in each variation.</p>
<p>We evaluate on two training modes: adaptation and transfer.</p>
<p>When evaluating adaptation, we allow 5 attempts on each test variant with the ability to learn between each trial.When evaluating transfer, we train the LLM actor on 10 training variants for 30 episodes (3 trajectories each).After training, we evaluate on the same test variants that we used to evaluate adaptation.The environment provides intermediate rewards for completing subtasks.A successfully completed task will have a final cumulative reward, or score, of 100.</p>
<p>We compare with the following GPT-4 based methods:</p>
<p>ReAct (Yao et al., 2022) prompts an LLM actor to reason about the task before outputting an action.All other methods also incorporate this in their prompts.</p>
<p>Reflexion (Shinn et al., 2023) prompts an LLM actor to reflect on failed task attempts and then retry the task.</p>
<p>CLIN (Majumder et al., 2023) reflects on past experience to learn transition information of the form "A [may/should] be necessary to A".</p>
<p>NetHack</p>
<p>NetHack is a grid-based videogame that requires challenging exploration and problem solving (Küttler et al., 2020).Unlike ScienceWorld, NetHack requires low-level navigation actions.We choose to include NetHack in our evaluation because it poses a potential challenge for our method.SSO requires aligning common sequences of states and actions when extracting skills.However, in environments with lower-level actions such as NetHack, dissimilar action sequences can be used to achieve the same subgoal.Additionally, NetHack uses character-based actions instead of natural language ("k" moves the player north and "," picks up an item).Despite this, SSO is able to successfully learn helpful skills in NetHack.</p>
<p>We utilize the MiniHack library (Samvelyan et al., 2021) to design a custom level that tests an LLM actor's ability to explore and learn several skills to complete a task.Figure 4 shows the layout of our custom task.The LLM actor must pick up and use a key to unlock a door, pick up and use an item to begin levitating, and safely cross the lava to the goal.The task provides intermediate rewards for achieving each subgoal.As in ScienceWorld, task completion results in a score of 100.Starting locations of the actor, items, and staircase are randomized in each episode so that SSO cannot learn memorized paths between subgoals.Also, the item that is used to levitate may be a potion or a magic ring, each of which requires different actions to activate.</p>
<p>In our custom NetHack task, we compare SSO with ReAct and Reflexion baselines.We omit CLIN for this task because CLIN was specifically designed for ScienceWorld and reimplementing CLIN for a new domain would be non-trivial.Similar to adaptation on ScienceWorld, we allow Reflexion and SSO to adapt to our NetHack task.However, unlike the adaptation setup in ScienceWorld, we run these methods for 30 iterations and evaluate every 10 iterations by attempting the task 10 times with a frozen set of skills/reflections.All LLM actors for NetHack utilize GPT-4-Turbo instead of GPT-4 to save on costs, but we found performance between the two LLMs to be similar.</p>
<p>Experimental Results</p>
<p>In both ScienceWorld and NetHack domains, SSO provides large performance gains by informing the LLM actor about potential subgoals and how to accomplish them.Figure 4 reports SSO's state-of-the-art performance for both task adaptation and transfer in ScienceWorld as well as SSO's superior performance and continual learning ability when compared with ReAct and Reflexion baselines in NetHack.</p>
<p>Error bars for NetHack show standard error across five training runs.We do not provide standard error for ScienceWorld because the large number of tasks and expensive GPT-4 actor limited our computational budget.In the following sections, we analyze what makes SSO successful.Skill set size measures the number of skills created minus those that were pruned during refinement.Executed skills is the average number of unique skills executed in a trajectory as reported by the LLM actor.Finally, the average task score is reported throughout training.</p>
<p>Skill Analysis</p>
<p>Figure 5 reports several skill statistics throughout SSO training.SSO continuously increases the size of its skill set during training, and the rate of constructing new skills decreases as the skill set increases in experience coverage.In our experiments, we retrieve up to three skills to include in-context at each step, but much fewer than this are selfreported as "executed" during the trajectory.Despite this, our experience suggests that even unreported skills improve LLM actor performance.Also, the number of skills being executed per trajectory steadily increases, suggesting that learned skills become more useful throughout training.</p>
<p>Finally, the subtrajectories used to create skills have an average length of 2.64 and 2.95 for ScienceWorld and NetHack respectively.Remember that we limit subtrajectory length to [2,5] in our experiments.The length of generated instructions is slightly longer at 3.18 and 3.27 respectively.</p>
<p>ScienceWorld Melting Temp Task</p>
<p>Subgoal: The stove is turned on.on the stove is: a substance called liquid [substance].1. Focus on the thermometer 2. Focus on the substance you want to heat 3. Move the focused substance to the stove 4. Activate the stove</p>
<p>NetHack Task</p>
<p>Subgoal: You succeed in unlocking the door.1. Stand adjacent to the closed door that needs to be unlocked 2. Use the action 'a' to apply the relevant key or tool that can unlock the door 3. Confirm the unlock action by responding affirmatively when prompted, typically by using the action 'y' Qualitatively, we found that SSO was especially helpful with less intuitive aspects of the action space.For example, the ScienceWorld Melting Temperature task requires using the focus action on the thermometer and substance before attempting to melt it, and NetHack requires using the apply action on a key to unlock a nearby door.Humans can quickly adapt to unintuitive domain specific requirements after a few tries, but we found that the unaltered LLM actors often got stuck in situations where actions were unintuitive outputting text such as "I apologize for the confusion.I am trying to..." before attempting the invalid action again.However, skills generated by SSO, such as those in Table 2, allow the actor to reliably pass bottlenecks and continue to explore the next steps of the task.</p>
<p>We hypothesize that SSO continually optimizes the LLM actor's policy by iteratively creating better skills and gathering better data.We look for evidence of this by visualizing the skill lifecycle in Figure 6.Each row of the figure represents the skills that were created in the corresponding iteration of SSO and when those skills were executed during training.The densities in the figure are normalized by the total number of skills, so the the density width indicates both how many skills were created in that iteration and in which iterations they were executed.</p>
<p>Figure 6 shows that skill set refinement prunes most skills shortly after they are created.Also, while skills from the initial iterations are used for longer, the LLM actor tends to use skills that were created more recently, suggesting that more useful skills are discovered later in training.This is further supported by visualizing when certain actions first appear in skills, as shown by the icons in Figure 6.Note that the icons indicate when a skill first appears, but improved versions of that same skill may be created in later iterations.</p>
<p>In general, actions that are required later in a trajectory are included in skills later in training.</p>
<p>Ablations</p>
<p>We compare SSO to several ablations on ScienceWorld's Melting Temperature task and the NetHack task.First, we ablate the skill refinement method described in Section 4.2 and do not ever prune the skill set.Next, we ablate the use of task reward in scoring skills for sampling by reducing the corresponding score weight to zero.Finally, we ablate similarity-based extraction and instead use the three steps before every environment reward to generate skills.</p>
<p>Figure 7 shows that each of these ablations decreases the performance of SSO but still regularly outperform the baseline.</p>
<p>Skill set refinement appears to be slightly more impactful than environment rewards and extracting pairs of similar subtrajectories.Using similarity as an extraction method appears less impactful than simply using environment rewards.However, including similarity-based extraction still contributes to SSO by automatically determining the start and end states for high-reward subtrajectories.</p>
<p>Discussion &amp; Conclusion</p>
<p>Skill Set Optimization (SSO) is a new in-context policy optimization method that allows LLM actors to quickly adapt to and transfer between tasks.SSO achieves state-of-the-art results in the ScienceWorld domain and outperforms all experimented baselines on our custom NetHack task.Every iteration, SSO constructs commonly executed high-reward skills to add to the skill set and filters out poorly performing skills.Unlike previous work, SSO utilizes ongoing environment feedback to evaluate and prune skills, leverages intermediate rewards to identify subgoals, facilitates skill retrieval, and learns abstract transferable skills.</p>
<p>One limitation of SSO is the current similarity metric used for extracting potential skills.State and action embedding similarity may be less effective in environments with distracting state information or low-level actions.Also, although SSO is capable of operating in environments without intermediate rewards, for best performance task designers must include intermediate subgoal rewards in the environment feedback.Fortunately, this is a common paradigm for sequential decision making.Finally, SSO on its own does not include a method for leveraging negative environment feedback outside skill set refinement, but a method such as Reflexion (Shinn et al., 2023) can easily be used in addition to SSO to provide this feature.We provide these limitations as potential inspiration for future work.</p>
<p>We believe that SSO takes a significant step towards reliable in-context policy optimization methods for LLM actors.SSO manages this while being a general purpose solution, not limited by specific output formats (Wang et al., 2023a), and capable of operating in scenarios with (Zhao et al., 2023) or without (Majumder et al., 2023) environment reinforcement signals.We hope SSO inspires continued research in improving the effectiveness and learning efficiency of continual learning for LLM actors.</p>
<p>Impact Statement</p>
<p>This work aims to improve the ability of deployed LLMs to continually adapt and transfer knowledge between tasks without retraining.This research direction has the potential to improve many state-of-the-art LLM applications, but does not alter any of their fundamental limitations.</p>
<p>A. Models and Hyperparameters</p>
<p>All of our ScienceWorld experiments were completed with OpenAI's gpt-4-0613, and our NetHack experiments were completed with gpt-4-1106-preview.We used text-embedding-ada-002 as the embedding model for all similarity metrics.Table 3 shows the hyperparameters we used in all of our experiments.We found that these hyperparameters we robust and worked well in all of our experiments on both domains.However, we expect that the parameters regulating skill length and number of retrieved skills may need to be adjusted in domains with skills that operate at a different granularity.</p>
<p>B. Prompts and Examples</p>
<p>Below we detail the prompts used for our LLM actor and generating skills.All prompts are the same across domains besides task, state, and admissible action descriptions.Text marked with &lt;&gt; indicate where variable text is inserted.We also include example skills in Tables 4 and 5 and corresponding subtrajectories in Tables 6 and 7.</p>
<p>B.1. LLM Actor Prompt</p>
<p>You are playing a text-based game in which you must interact with your surroundings to complete a task.You will occasionally be given posisible subgoals.You may choose to target one of these subgoals or ignore them.</p>
<p><task description></p>
<p>Given the state, reflect on what has happened so far, explain your plan to accomplish the task, output which of the given subgoals you are targeting next (match one of the subgoals in the prompt word for word or output "none"), and then output the next action to execute (use one of the action templates below).</p>
<p>For example:</p>
<p>The last action had the effect of... To accomplish the task, I will need to.Table 7: Subtrajectories used to generate the "you have a key named the master key of thievery" skill described in Table 5.</p>
<p>may</p>
<p>Figure 1 :
1
Figure 1: Example of a interactive text task and skill.</p>
<p>Figure 2 :
2
Figure 2: Each iteration of SSO collects a trajectory of interactions with the current LLM actor, uses this trajectory to construct new skills and filter poorly performing skills, and updates the skill set for use in the next iteration.New skills are constructed by extracting, scoring, and sampling sets of similar subtrajectories that are then used to generate subgoals and instructions for skills.Skills are filtered based on the discounted future rewards observed after executing a skill.</p>
<p>Figure 3 :
3
Figure 3: Comparison between in-context skills, fewshot trajectory examples, and no in-context information on the Melting Temperature ScienceWorld and NetHack tasks.</p>
<p>Figure 5 :
5
Figure 5: Skill Set statistics for ScienceWorld and NetHack during training.Skill set size measures the number of skills created minus those that were pruned during refinement.Executed skills is the average number of unique skills executed in a trajectory as reported by the LLM actor.Finally, the average task score is reported throughout training.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Visualization of the lifecycle of SSO skills.For every skill created in an iteration of SSO, the corresponding row shows when that iteration's skills were executed throughout training.Most skills are pruned soon after creation and replaced with improved skills.The icons in the figure indicate when skills with the corresponding actions were first created.</p>
<p>Table 1 :
1
Action accuracy from example trajectories with and without learned skills in-context, and LLM actor success rate self-reporting that it is executing the provided skill.
Subgoalw/o Skill w/ Skill Self-ReportingFill container0.070.370.69Heat substance0.040.220.68Mix ingredients0.300.360.71</p>
<p>We compare SSO with ReAct and Reflexion baselines in ScienceWorld and NetHack domains.We also compare with the previous state-of-the-art for ScienceWorld, CLIN.In ScienceWorld we evaluate adaptation-attempting a single variant five times-and transfer-learning on 10 train variants for 30 iterations before testing on the heldout test variants.In NetHack we test task adaptation across 30 iterations.
ScienceWorldAdaptationTransferTaskReAct Reflexion CLIN SSO CLIN SSOTemperature7.25.914.310015.771.6Melting Temp6.128.651.897.349.769.2Find Plant26.764.910010059.2100Find Living53.316.410096.710090Chemistry5170.444.482.642.248Color Mixing58.970.756.781.185.671.1Lifespan, Longest611001001006590NetHackLifespan, Shortest Life Stages, Plant Life Stages, Animal Boil Freeze Grow Plant Grow Fruit67.5 8 27.7 3.5 7.8 9.1 18.684.4 8 2.6 4.2 7.8 7.3 1390 8 81 15.2 10 11 71.6100 6.2 100 81.7 74.3 86.6 7875 32 42.8 4.4 8.9 10.9 70.880 3.4 77 38.9 61.2 28.3 48.7Task Score40 45 50 55SSO Reflexion ReActGravity Friction40.5 4450.6 100100 72.5100 9470 7074 67.535Genetics, Known25.750.910078.584.542.530Genetics, Unknown Average16.8 29.623.7 39.492.6 62.248.7 83.761.4 52.720.3 60.105 10 15 20 25 30 IterationFigure 4:</p>
<p>Table 2 :
2
Example generated skills.</p>
<p>Table 3 :
3
Skill Set Optimization hyperparameters.
ParameterValueMax skill length5Min skill length2Adaptation training episodes5Transfer training episodes30Sampling temp (train)0.7Sampling temp (test)0.0Max retrieved skills3Skill refinement threshold0Skill length score weight0.01Reward score weight0.1State similarity score weight1.0Action similarity score weight1.0</p>
<p>Table 6 :
6
You are an expert planning system.You are creating reusable skills to execute when completing various tasks.You create skills by looking at successful examples of task completions.A skill is composed of a list of instructions and a target state.After creating a skill, it will be used to execute actions in an environment.The environment will return a set of observations that summarize the new environment state.These observations will be used in conjunction with the skill's target state to determine whether the last skill was successful.Consider the example trajectories of states and actions below.You'll be asked to analyze the similarities between each.Pay attention to the wording of the state observations and actions.Then you'll be asked to generate the common instructions, and target state for them.Generate a single target observation that would indicate the success of the skill.The target should be similar to one of the observations in the final states.Create a generic target that would be valid for every example.Do not mention the examples in the target.Use the output format: Skill [skill name] target: [target observation] Subtrajectories used to generate the "you move to the kitchen" skill described in Table4.Initial State: You have a +0 short sword (weapon in hand).You have 15 +0 daggers (alternate weapon; not wielded).You have an uncursed +1 leather armor (being worn).You have an uncursed potion of sickness.You have an uncursed lock pick.You have an empty uncursed sack.You see a vertical wall far east.You see a horizontal wall near north and northeast.You see a area of lava near northeast.You see a stairs down near northeast.You see a vertical wall near west.You see a horizontal closed door near northwest.You see a dark area near northwest.You see a lava very near northeast, northeast, and east.You see a horizontal wall adjacent southeast, south, and southwest.You see a key adjacent northwest.Hello Agent, welcome to NetHack!You are a chaotic male human Rogue.You have a +0 short sword (weapon in hand).You have 15 +0 daggers (alternate weapon; not wielded).You have an uncursed +1 leather armor (being worn).You have an uncursed potion of sickness.You have an uncursed lock pick.You have an empty uncursed sack.You have a key named The Master Key of Thievery.You see a vertical wall far east.You see a horizontal wall near north and northeast.You see a lava near northeast, east, and southeast.You see a area of lava near northeast.You see a stairs down near east.You see a vertical wall near west.You see a dark area near northwest.You see a horizontal wall very near southeast, south, and southwest.You see a horizontal closed door very near northwest.You see a stairs up adjacent southeast.g -a key named The Master Key of Thievery.Initial State: You have a +0 short sword (weapon in hand).You have 8 +0 daggers (alternate weapon; not wielded).You have an uncursed +1 leather armor (being worn).You have an uncursed potion of sickness.You have an uncursed lock pick.You have an empty uncursed sack.You have an uncursed blindfold.You see a vertical wall far east.You see a horizontal wall near north and northeast.You see a area of lava near northeast.You see a stairs down near northeast.You see a vertical wall near west.You see a horizontal closed door near northwest.You see a dark area near northwest.You see a lava very near northeast, northeast, and east.You see a horizontal wall adjacent southeast, south, and southwest.You see a key adjacent northwest.Hello Agent, welcome to NetHack!You are a chaotic male human Rogue.You have a +0 short sword (weapon in hand).You have 8 +0 daggers (alternate weapon; not wielded).You have an uncursed +1 leather armor (being worn).You have an uncursed potion of sickness.You have an uncursed lock pick.You have an empty uncursed sack.You have an uncursed blindfold.You have a key named The Master Key of Thievery.You see a stairs down far northeast.You see a vertical wall far east.You see a horizontal wall near north and northeast.You see a lava near northeast, east, and southeast.You see a area of lava near northeast.You see a vertical wall near west.You see a dark area near northwest.You see a horizontal wall very near southeast, south, and southwest.You see a horizontal closed door very near northwest.You see a stairs up adjacent southeast.h -a key named The Master Key of Thievery.
..
Department of Computer Science, University of California Irvine, Irvine CA, United States
Allen Institute for AI, Seattle Washington, United States. * Equal contribution. Correspondence to: Kolby Nottingham <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#107;&#110;&#111;&#116;&#116;&#105;&#110;&#103;&#64;&#117;&#99;&#105;&#46;&#101;&#100;&#117;">&#107;&#110;&#111;&#116;&#116;&#105;&#110;&#103;&#64;&#117;&#99;&#105;&#46;&#101;&#100;&#117;</a>.
https://allenai.github.io/sso/
AcknowledgementsWe would like to thank the Aristo team at the Allen Institute for Artificial Intelligence for their discussions and feedback.This work was funded in part by the DARPA ANSR program under award FA8750-23-2-0004.C. SSO CodeWe include the following python code as a high-level overview of SSO.However, the complete codebase is available at https://github.com/allenai/sso.def train(env, agent, skillset, iterations=30)
Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Larochelle, H.</p>
<p>Codet: Code generation with generated tests. M Ranzato, R Hadsell, M Balcan, H Lin, B Chen, F Zhang, A Nguyen, D Zan, Z Lin, J.-G Lou, W Chen, The Eleventh International Conference on Learning Representations. Curran Associates, Inc2020. 202233Advances in Neural Information Processing Systems</p>
<p>Optimizing discrete text prompts with reinforcement learning. M Deng, J Wang, C.-P Hsieh, Y Wang, H Guo, T Shu, M Song, E Xing, Z Hu, Rlprompt, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. C Fernando, D Banarse, H Michalewski, S Osindero, T Rocktäschel, arXiv:2309.167972023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, 6th Annual Conference on Robot Learning. 2022b</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. B Ichter, A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, D Kalashnikov, S Levine, Y Lu, C Parada, K Rao, P Sermanet, A T Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, M Yan, N Brown, M Ahn, O Cortes, N Sievers, C Tan, S Xu, D Reyes, J Rettinghouse, J Quiambao, P Pastor, L Luu, K.-H Lee, Y Kuang, S Jesmonth, K Jeffrey, R J Ruano, J Hsu, K Gopalakrishnan, B David, A Zeng, C K Fu, 6th Annual Conference on Robot Learning. 2022</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>The nethack learning environment. H Küttler, N Nardelli, A Miller, R Raileanu, M Selvatici, E Grefenstette, T Rocktäschel, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. Y Liang, C Wu, T Song, W Wu, Y Xia, Y Liu, Y Ou, S Lu, L Ji, S Mao, arXiv:2303.164342023arXiv preprint</p>
<p>B P Majumder, B D Mishra, P Jansen, O Tafjord, N Tandon, L Zhang, C Callison-Burch, P Clark, Clin, arXiv:2310.10134A continually learning language agent for rapid task adaptation and generalization. 2023arXiv preprint</p>
<p>Do embodied agents dream of pixelated sheep? embodied decision making using language guided world modelling. K Nottingham, P Ammanabrolu, A Suhr, Y Choi, H Hajishirzi, S Singh, R Fox, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023a</p>
<p>Selective perception: Learning concise state descriptions for language model actors. K Nottingham, Y Razeghi, K Kim, J Lanier, P Baldi, R Fox, S Singh, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023b</p>
<p>A Prasad, A Koller, M Hartmann, P Clark, A Sabharwal, M Bansal, T Khot, arXiv:2311.05772Adapt: As-needed decomposition and planning with language models. 2023arXiv preprint</p>
<p>Minihack the planet: A sandbox for open-ended reinforcement learning research. M Samvelyan, R Kirk, V Kurin, J Parker-Holder, M Jiang, E Hambro, F Petroni, H Kuttler, E Grefenstette, T Rocktäschel, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021Round 1</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Voyager: An openended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Is your agent smarter than a 5th grader?. R Wang, P Jansen, M.-A Côté, P Ammanabrolu, Scienceworld, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents. Z Wang, S Cai, G Chen, A Liu, X Ma, Y Liang, Thirtyseventh Conference on Neural Information Processing Systems. 2023b</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations. 2021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Test-time prompt editing via reinforcement learning. T Zhang, X Wang, D Zhou, D Schuurmans, J E Gonzalez, Tempera, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Expel: Llm agents are experiential learners. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, arXiv:2308.101442023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
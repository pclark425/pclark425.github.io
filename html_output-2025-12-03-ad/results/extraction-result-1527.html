<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1527 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1527</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1527</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-89c8aad71433f7638d2e2c009e1ea20e039f832d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/89c8aad71433f7638d2e2c009e1ea20e039f832d" target="_blank">AI2-THOR: An Interactive 3D Environment for Visual AI</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks and facilitate building visually intelligent models.</p>
                <p><strong>Paper Abstract:</strong> We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at this http URL AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1527.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1527.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The House Of inteRactions (AI2-THOR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale, near photo-realistic Unity-based interactive 3D environment for embodied AI research that supports navigation, arm-based manipulation, causal interactions, multiple image modalities, and rich environment metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AI2-THOR (Unity backend)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A Unity 3D based simulator providing near photo-realistic indoor scenes with thousands of interactive objects, physics modeling, multiple agents/embodiments, manipulable object states, and renderable image modalities (RGB, depth, segmentation, normals).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics / physical interaction</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Near photo-realistic visual fidelity and medium-to-high fidelity for object interactions (visuals highly realistic; physics approximated by Unity physics engine).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes detailed object models and materials, object state changes (open/close, fill, break, cook), arm-based manipulation, causal interactions (e.g., turning on a coffee machine fills a mug), randomizable materials/lighting, multiple rendering modalities; physics provided by Unity (approximate rigid-body physics, collision detection, articulated arm kinematics). Does not claim full high-precision physical simulation of e.g. fluid dynamics or thermodynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LoCoBot (navigation), ManipulaTHOR agent and StretchRE1 (arm manipulation), Abstract agent, Drone</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embodied agents trained with reinforcement learning / actor-critic policies for navigation (ObjectNav/ImageNav) and agents with control of arms for manipulation; used with standard RL actor-critic networks in benchmarking/profiling.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Visual navigation (ObjectNav, ImageNav), interactive manipulation, vision-and-language instruction following, affordance learning, sim2real transfer studies.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Profiling run: ObjectNav agent trained for 1M steps on 2-GPU machine giving training FPS in the range 145.5–179.4 (average 167.7) for the described setup.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Other simulators and real-world scenes (e.g., RoboTHOR real-world reconstructions, iTHOR, ArchitecTHOR held-out houses).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: pre-training on ProcTHOR (within AI2-THOR) produced state-of-the-art ObjectNav performance on RoboTHOR, iTHOR, and ArchitecTHOR in a 0-shot setting (no numeric scores reported in this paper). No numeric sim->real transfer scores are provided here for RoboTHOR experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper argues that near photo-realistic visuals and interactive objects help transfer to the real world and that increasing procedural diversity (ProcTHOR) improves generalization; it does not specify quantitative minimal-fidelity thresholds required for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reports overfitting when training on small handcrafted scene sets (iTHOR, RoboTHOR) and notes discrepancies between simulation and reality when evaluating sim2real — motivating ProcTHOR and RoboTHOR real-world reconstructions to study such gaps. No specific low-fidelity failure experiments (e.g., thermodynamics/circuits failures) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1527.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1527.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProcTHOR-10K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProcTHOR-10K (procedurally generated ProcTHOR dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale procedurally generated set of 10,000 semantically plausible indoor houses within the AI2-THOR framework used to massively increase training diversity and improve generalization of embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Procthor: Large-scale embodied ai using procedural generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>ProcTHOR (procedural generation within AI2-THOR / Unity)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A procedural scene-generation pipeline that produces large numbers of diverse, semantically plausible indoor houses (floorplans + object placements) rendered in AI2-THOR, preserving the same interaction and physics capabilities of AI2-THOR.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-to-high visual fidelity (inherits AI2-THOR rendering); fidelity emphasis is on diversity/scale rather than per-scene ultra-high physical fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Procedurally sampled floorplans and object placements, supports the same object interactions and physical approximations as AI2-THOR (arm manipulation, object states, rendering modalities); randomness and scale intended to reduce overfitting to narrow scene distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ObjectNav agents pretrained on ProcTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agents (actor-critic policy network used in ProcTHOR experiments) trained on large-scale procedurally generated scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Visual navigation (ObjectNav) and generalization to unseen real-world-like layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>RoboTHOR, iTHOR, ArchitecTHOR (held-out evaluation and real-world-like test houses)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: pre-training on ProcTHOR alone achieved state-of-the-art ObjectNav performance on RoboTHOR, iTHOR, and ArchitecTHOR in a 0-shot setting as reported in the paper; numerical transfer metrics are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Suggests that high scene diversity/scale (procedural generation) can substantially improve generalization beyond simply more of the same handcrafted scenes; does not state a minimum visual/physics fidelity requirement.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Overfitting observed when training only on limited hand-built scenes (iTHOR/RoboTHOR); ProcTHOR mitigates that overfitting. No other explicit failure cases described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1527.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1527.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboTHOR: An open simulation-to-real embodied AI platform</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of hand-modeled dorm-style apartment scenes within the AI2-THOR ecosystem developed to study simulation-to-real (sim2real) transfer by providing simulators and matching real-world reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robothor: An open simulation-to-real embodied ai platform.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>RoboTHOR (subset of AI2-THOR scenes focused on sim2real)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Hand-built maze/dorm-sized apartments modeled by professional artists with the intent to reproduce distributions similar to real indoor environments; some scenes are recreated in the real world for direct sim2real evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics / sim2real</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Near photo-realistic visual fidelity intended to match real scenes; physics and interactions supported through AI2-THOR/Unity approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Hand-modeled scene geometry and object placements designed to be similar to corresponding real-world rooms; supports object interactions, arm manipulation abstraction, and metadata; intended to reduce sim-real discrepancy by careful modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Agents trained on simulation for ObjectNav (trained on 75 simulation scenes in sim2real studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agents for navigation (ObjectNav) using actor-critic policies as in the profiling experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>ObjectNav and sim2real transfer evaluation (deploying simulation-trained agents to real-world scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Agents were trained in simulation on 75 scenes for sim2real experiments; no numeric learning or transfer metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Unseen real-world scenes reconstructed to match the simulated scenes (real-world evaluation in Seattle reconstructions).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Paper states sim2real transfer was evaluated (agents trained in sim, tested in real) but does not report numeric transfer performance here.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper highlights recreating many scenes in reality to study sim-vs-real discrepancies and implies that closer visual/layout fidelity helps evaluate transfer; no explicit minimal-fidelity prescription.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Notes discrepancies between simulation and reality can affect evaluation; does not enumerate quantified failure cases in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1527.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1527.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>iTHOR (original AI2-THOR scene set)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The original collection of 120 hand-modeled indoor room-scale scenes (bedrooms, bathrooms, kitchens, living rooms) used within AI2-THOR for training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>iTHOR (AI2-THOR scene subset)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>120 hand-built near photo-realistic room-sized scenes modeled by professional 3D artists and used as primary dataset in early AI2-THOR experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Near photo-realistic visual fidelity for indoor rooms; supports interactive objects and object state changes.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Hand-modeled scenes with many interactive objects, object states (open/close, clean/dirty, etc.), arm-manipulation support and multiple image modalities; limited in scale (120 scenes) compared to procedural datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Agents trained for navigation, manipulation, and vision-and-language (e.g., ObjectNav, manipulation policies).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning and imitation learning agents; used with metadata and actor-critic policies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>ObjectNav, ImageNav, manipulation, vision-and-language instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Other held-out simulated houses and sim2real evaluation sets (RoboTHOR, ArchitecTHOR comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Not quantified in this paper; authors report that training on small handcrafted sets like iTHOR can lead to overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper uses iTHOR as a motivating example of limited-scale training data that can induce overfitting; suggests scale/diversity (ProcTHOR) is required for better generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Overfitting to the small set of iTHOR scenes when used alone for training agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1527.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1527.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArchitecTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ArchitecTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small set of larger, hand-built houses (10 houses: 5 validation, 5 test) developed to act as an evaluation set for models trained on procedurally generated houses (ProcTHOR).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Procthor: Large-scale embodied ai using procedural generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>ArchitecTHOR (evaluation scenes within AI2-THOR)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Hand-crafted large single-story houses designed to reflect real-world house distributions and used to test whether models trained on procedural scenes generalize to realistic floorplans and object placements.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity with larger, realistic scene scale; hand-modeled layouts intended to reflect real-world distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Larger-scale scenes, manually designed by professional artists, realistic object placements; used for held-out evaluation to detect memorization of procedural biases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ObjectNav agents (evaluated 0-shot after ProcTHOR pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning navigation agents (actor-critic networks used in ProcTHOR experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Generalization evaluation for ObjectNav (held-out house evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Models trained on ProcTHOR -> ArchitecTHOR held-out evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: ProcTHOR pretraining generalized to ArchitecTHOR in a 0-shot setting and yielded state-of-the-art ObjectNav performance according to the paper, but no numeric metrics are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Used specifically to test whether procedural training merely memorizes biases vs generalizes to real-world-like house layouts; no explicit minimal-fidelity claim.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1527.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1527.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat / HM3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat: A platform for embodied AI research (with HM3D dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Habitat is a platform for embodied AI; HM3D is a large dataset of 1000 Matterport-derived 3D environments used for training navigation agents. The paper uses Habitat+HM3D as a baseline in a profiling comparison against ProcTHOR/AI2-THOR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Habitat: A platform for embodied ai research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Habitat (with HM3D dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A simulation platform (Magnum renderer) for embodied AI research focusing on large-scale navigation datasets; historically less interactive in early versions but optimized for high-throughput simulation of navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Variable: Habitat 1.0 emphasized fast large-scale rendering with limited object-level interactions (lower interaction fidelity); Habitat 2.0 reintroduced more interaction capabilities (medium fidelity).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>HM3D provides large-scale diverse scenes; Habitat instances used less GPU memory in profiling; earlier Habitat versions lacked detailed object state interactions compared to AI2-THOR.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LoCoBot agent (same action space used in profiling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning actor-critic policy network used for ObjectNav in the profiling comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>ObjectNav (visual navigation) benchmarking and throughput profiling.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Profiling run: training FPS in the range 119.7–264.3 (average 230.5) for the same ObjectNav setup used for comparison with ProcTHOR/AI2-THOR.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes Habitat instances consumed less GPU memory and that Habitat 1.0 lacked interaction support; does not provide a quantitative minimal-fidelity requirement for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1527.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1527.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iGibson 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>iGibson 2.0: Object-centric simulation for robot learning of everyday household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PyBullet-based object-centric simulation platform with an emphasis on interaction and realistic object physics, listed in the paper's comparison table as an alternative simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>igibson 2.0: Object-centric simulation for robot learning of everyday household tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>iGibson 2.0 (PyBullet backend)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulator focusing on object-centric physics using PyBullet, supports arm manipulation, multi-agent, VR, and detailed object state interactions across a modest number of scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium fidelity physics via PyBullet; supports object states and arm manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Uses PyBullet physics, supports arm manipulation and multi-agent, VR support; reported as 15 scenes and 1217 objects in the comparison table in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Embodied interaction and manipulation tasks (general mention).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1527.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1527.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unity (engine)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unity 3D game engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose real-time game engine used as the backend renderer/physics engine for AI2-THOR, providing rendering, physics, shaders, and extensibility for simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Unity (rendering & physics backend)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Real-time game engine used to store scenes, execute actions, render image modalities via shaders, and perform approximate physics modelling for AI2-THOR and other Unity-based simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>simulation / graphics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Engine can support high visual fidelity and approximate physics; actual fidelity depends on assets, physics configuration, and implemented interactions (in AI2-THOR configured for near photo-realistic visuals and approximate rigid-body physics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides rendering pipelines (RGB, depth, segmentation), shader support for modalities, approximate physics and collision, headless rendering and WebGL support; physics are approximate rigid-body and joint kinematics rather than high-precision continuous physics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Procthor: Large-scale embodied ai using procedural generation. <em>(Rating: 2)</em></li>
                <li>Robothor: An open simulation-to-real embodied ai platform. <em>(Rating: 2)</em></li>
                <li>Habitat: A platform for embodied ai research. <em>(Rating: 2)</em></li>
                <li>Habitat 2.0: Training home assistants to rearrange their habitat. <em>(Rating: 2)</em></li>
                <li>Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. <em>(Rating: 2)</em></li>
                <li>igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. <em>(Rating: 2)</em></li>
                <li>Manipulathor: A framework for visual object manipulation. <em>(Rating: 2)</em></li>
                <li>ThreeDWorld: A platform for interactive multi-modal physical simulation. <em>(Rating: 1)</em></li>
                <li>Sapien: A simulated part-based interactive environment. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1527",
    "paper_id": "paper-89c8aad71433f7638d2e2c009e1ea20e039f832d",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "AI2-THOR",
            "name_full": "The House Of inteRactions (AI2-THOR)",
            "brief_description": "A large-scale, near photo-realistic Unity-based interactive 3D environment for embodied AI research that supports navigation, arm-based manipulation, causal interactions, multiple image modalities, and rich environment metadata.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "AI2-THOR (Unity backend)",
            "simulator_description": "A Unity 3D based simulator providing near photo-realistic indoor scenes with thousands of interactive objects, physics modeling, multiple agents/embodiments, manipulable object states, and renderable image modalities (RGB, depth, segmentation, normals).",
            "scientific_domain": "embodied AI / robotics / physical interaction",
            "fidelity_level": "Near photo-realistic visual fidelity and medium-to-high fidelity for object interactions (visuals highly realistic; physics approximated by Unity physics engine).",
            "fidelity_characteristics": "Includes detailed object models and materials, object state changes (open/close, fill, break, cook), arm-based manipulation, causal interactions (e.g., turning on a coffee machine fills a mug), randomizable materials/lighting, multiple rendering modalities; physics provided by Unity (approximate rigid-body physics, collision detection, articulated arm kinematics). Does not claim full high-precision physical simulation of e.g. fluid dynamics or thermodynamics.",
            "model_or_agent_name": "LoCoBot (navigation), ManipulaTHOR agent and StretchRE1 (arm manipulation), Abstract agent, Drone",
            "model_description": "Embodied agents trained with reinforcement learning / actor-critic policies for navigation (ObjectNav/ImageNav) and agents with control of arms for manipulation; used with standard RL actor-critic networks in benchmarking/profiling.",
            "reasoning_task": "Visual navigation (ObjectNav, ImageNav), interactive manipulation, vision-and-language instruction following, affordance learning, sim2real transfer studies.",
            "training_performance": "Profiling run: ObjectNav agent trained for 1M steps on 2-GPU machine giving training FPS in the range 145.5–179.4 (average 167.7) for the described setup.",
            "transfer_target": "Other simulators and real-world scenes (e.g., RoboTHOR real-world reconstructions, iTHOR, ArchitecTHOR held-out houses).",
            "transfer_performance": "Qualitative: pre-training on ProcTHOR (within AI2-THOR) produced state-of-the-art ObjectNav performance on RoboTHOR, iTHOR, and ArchitecTHOR in a 0-shot setting (no numeric scores reported in this paper). No numeric sim-&gt;real transfer scores are provided here for RoboTHOR experiments.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Paper argues that near photo-realistic visuals and interactive objects help transfer to the real world and that increasing procedural diversity (ProcTHOR) improves generalization; it does not specify quantitative minimal-fidelity thresholds required for transfer.",
            "failure_cases": "Reports overfitting when training on small handcrafted scene sets (iTHOR, RoboTHOR) and notes discrepancies between simulation and reality when evaluating sim2real — motivating ProcTHOR and RoboTHOR real-world reconstructions to study such gaps. No specific low-fidelity failure experiments (e.g., thermodynamics/circuits failures) are reported.",
            "uuid": "e1527.0",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "ProcTHOR-10K",
            "name_full": "ProcTHOR-10K (procedurally generated ProcTHOR dataset)",
            "brief_description": "A large-scale procedurally generated set of 10,000 semantically plausible indoor houses within the AI2-THOR framework used to massively increase training diversity and improve generalization of embodied agents.",
            "citation_title": "Procthor: Large-scale embodied ai using procedural generation.",
            "mention_or_use": "use",
            "simulator_name": "ProcTHOR (procedural generation within AI2-THOR / Unity)",
            "simulator_description": "A procedural scene-generation pipeline that produces large numbers of diverse, semantically plausible indoor houses (floorplans + object placements) rendered in AI2-THOR, preserving the same interaction and physics capabilities of AI2-THOR.",
            "scientific_domain": "embodied AI / robotics",
            "fidelity_level": "Medium-to-high visual fidelity (inherits AI2-THOR rendering); fidelity emphasis is on diversity/scale rather than per-scene ultra-high physical fidelity.",
            "fidelity_characteristics": "Procedurally sampled floorplans and object placements, supports the same object interactions and physical approximations as AI2-THOR (arm manipulation, object states, rendering modalities); randomness and scale intended to reduce overfitting to narrow scene distributions.",
            "model_or_agent_name": "ObjectNav agents pretrained on ProcTHOR",
            "model_description": "Reinforcement learning agents (actor-critic policy network used in ProcTHOR experiments) trained on large-scale procedurally generated scenes.",
            "reasoning_task": "Visual navigation (ObjectNav) and generalization to unseen real-world-like layouts.",
            "training_performance": null,
            "transfer_target": "RoboTHOR, iTHOR, ArchitecTHOR (held-out evaluation and real-world-like test houses)",
            "transfer_performance": "Qualitative: pre-training on ProcTHOR alone achieved state-of-the-art ObjectNav performance on RoboTHOR, iTHOR, and ArchitecTHOR in a 0-shot setting as reported in the paper; numerical transfer metrics are not provided in this paper.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Suggests that high scene diversity/scale (procedural generation) can substantially improve generalization beyond simply more of the same handcrafted scenes; does not state a minimum visual/physics fidelity requirement.",
            "failure_cases": "Overfitting observed when training only on limited hand-built scenes (iTHOR/RoboTHOR); ProcTHOR mitigates that overfitting. No other explicit failure cases described.",
            "uuid": "e1527.1",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "RoboTHOR",
            "name_full": "RoboTHOR: An open simulation-to-real embodied AI platform",
            "brief_description": "A set of hand-modeled dorm-style apartment scenes within the AI2-THOR ecosystem developed to study simulation-to-real (sim2real) transfer by providing simulators and matching real-world reconstructions.",
            "citation_title": "Robothor: An open simulation-to-real embodied ai platform.",
            "mention_or_use": "use",
            "simulator_name": "RoboTHOR (subset of AI2-THOR scenes focused on sim2real)",
            "simulator_description": "Hand-built maze/dorm-sized apartments modeled by professional artists with the intent to reproduce distributions similar to real indoor environments; some scenes are recreated in the real world for direct sim2real evaluation.",
            "scientific_domain": "embodied AI / robotics / sim2real",
            "fidelity_level": "Near photo-realistic visual fidelity intended to match real scenes; physics and interactions supported through AI2-THOR/Unity approximations.",
            "fidelity_characteristics": "Hand-modeled scene geometry and object placements designed to be similar to corresponding real-world rooms; supports object interactions, arm manipulation abstraction, and metadata; intended to reduce sim-real discrepancy by careful modeling.",
            "model_or_agent_name": "Agents trained on simulation for ObjectNav (trained on 75 simulation scenes in sim2real studies)",
            "model_description": "Reinforcement learning agents for navigation (ObjectNav) using actor-critic policies as in the profiling experiments.",
            "reasoning_task": "ObjectNav and sim2real transfer evaluation (deploying simulation-trained agents to real-world scenes)",
            "training_performance": "Agents were trained in simulation on 75 scenes for sim2real experiments; no numeric learning or transfer metrics provided in this paper.",
            "transfer_target": "Unseen real-world scenes reconstructed to match the simulated scenes (real-world evaluation in Seattle reconstructions).",
            "transfer_performance": "Paper states sim2real transfer was evaluated (agents trained in sim, tested in real) but does not report numeric transfer performance here.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Paper highlights recreating many scenes in reality to study sim-vs-real discrepancies and implies that closer visual/layout fidelity helps evaluate transfer; no explicit minimal-fidelity prescription.",
            "failure_cases": "Notes discrepancies between simulation and reality can affect evaluation; does not enumerate quantified failure cases in this paper.",
            "uuid": "e1527.2",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "iTHOR",
            "name_full": "iTHOR (original AI2-THOR scene set)",
            "brief_description": "The original collection of 120 hand-modeled indoor room-scale scenes (bedrooms, bathrooms, kitchens, living rooms) used within AI2-THOR for training and evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "iTHOR (AI2-THOR scene subset)",
            "simulator_description": "120 hand-built near photo-realistic room-sized scenes modeled by professional 3D artists and used as primary dataset in early AI2-THOR experiments.",
            "scientific_domain": "embodied AI / robotics",
            "fidelity_level": "Near photo-realistic visual fidelity for indoor rooms; supports interactive objects and object state changes.",
            "fidelity_characteristics": "Hand-modeled scenes with many interactive objects, object states (open/close, clean/dirty, etc.), arm-manipulation support and multiple image modalities; limited in scale (120 scenes) compared to procedural datasets.",
            "model_or_agent_name": "Agents trained for navigation, manipulation, and vision-and-language (e.g., ObjectNav, manipulation policies).",
            "model_description": "Reinforcement learning and imitation learning agents; used with metadata and actor-critic policies.",
            "reasoning_task": "ObjectNav, ImageNav, manipulation, vision-and-language instruction following.",
            "training_performance": null,
            "transfer_target": "Other held-out simulated houses and sim2real evaluation sets (RoboTHOR, ArchitecTHOR comparisons).",
            "transfer_performance": "Not quantified in this paper; authors report that training on small handcrafted sets like iTHOR can lead to overfitting.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Paper uses iTHOR as a motivating example of limited-scale training data that can induce overfitting; suggests scale/diversity (ProcTHOR) is required for better generalization.",
            "failure_cases": "Overfitting to the small set of iTHOR scenes when used alone for training agents.",
            "uuid": "e1527.3",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "ArchitecTHOR",
            "name_full": "ArchitecTHOR",
            "brief_description": "A small set of larger, hand-built houses (10 houses: 5 validation, 5 test) developed to act as an evaluation set for models trained on procedurally generated houses (ProcTHOR).",
            "citation_title": "Procthor: Large-scale embodied ai using procedural generation.",
            "mention_or_use": "use",
            "simulator_name": "ArchitecTHOR (evaluation scenes within AI2-THOR)",
            "simulator_description": "Hand-crafted large single-story houses designed to reflect real-world house distributions and used to test whether models trained on procedural scenes generalize to realistic floorplans and object placements.",
            "scientific_domain": "embodied AI / robotics",
            "fidelity_level": "High visual fidelity with larger, realistic scene scale; hand-modeled layouts intended to reflect real-world distributions.",
            "fidelity_characteristics": "Larger-scale scenes, manually designed by professional artists, realistic object placements; used for held-out evaluation to detect memorization of procedural biases.",
            "model_or_agent_name": "ObjectNav agents (evaluated 0-shot after ProcTHOR pretraining).",
            "model_description": "Reinforcement learning navigation agents (actor-critic networks used in ProcTHOR experiments).",
            "reasoning_task": "Generalization evaluation for ObjectNav (held-out house evaluation).",
            "training_performance": null,
            "transfer_target": "Models trained on ProcTHOR -&gt; ArchitecTHOR held-out evaluation.",
            "transfer_performance": "Qualitative: ProcTHOR pretraining generalized to ArchitecTHOR in a 0-shot setting and yielded state-of-the-art ObjectNav performance according to the paper, but no numeric metrics are given here.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Used specifically to test whether procedural training merely memorizes biases vs generalizes to real-world-like house layouts; no explicit minimal-fidelity claim.",
            "failure_cases": null,
            "uuid": "e1527.4",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "Habitat / HM3D",
            "name_full": "Habitat: A platform for embodied AI research (with HM3D dataset)",
            "brief_description": "Habitat is a platform for embodied AI; HM3D is a large dataset of 1000 Matterport-derived 3D environments used for training navigation agents. The paper uses Habitat+HM3D as a baseline in a profiling comparison against ProcTHOR/AI2-THOR.",
            "citation_title": "Habitat: A platform for embodied ai research.",
            "mention_or_use": "use",
            "simulator_name": "Habitat (with HM3D dataset)",
            "simulator_description": "A simulation platform (Magnum renderer) for embodied AI research focusing on large-scale navigation datasets; historically less interactive in early versions but optimized for high-throughput simulation of navigation tasks.",
            "scientific_domain": "embodied AI / robotics",
            "fidelity_level": "Variable: Habitat 1.0 emphasized fast large-scale rendering with limited object-level interactions (lower interaction fidelity); Habitat 2.0 reintroduced more interaction capabilities (medium fidelity).",
            "fidelity_characteristics": "HM3D provides large-scale diverse scenes; Habitat instances used less GPU memory in profiling; earlier Habitat versions lacked detailed object state interactions compared to AI2-THOR.",
            "model_or_agent_name": "LoCoBot agent (same action space used in profiling)",
            "model_description": "Reinforcement learning actor-critic policy network used for ObjectNav in the profiling comparison.",
            "reasoning_task": "ObjectNav (visual navigation) benchmarking and throughput profiling.",
            "training_performance": "Profiling run: training FPS in the range 119.7–264.3 (average 230.5) for the same ObjectNav setup used for comparison with ProcTHOR/AI2-THOR.",
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "Paper notes Habitat instances consumed less GPU memory and that Habitat 1.0 lacked interaction support; does not provide a quantitative minimal-fidelity requirement for transfer.",
            "failure_cases": null,
            "uuid": "e1527.5",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "iGibson 2.0",
            "name_full": "iGibson 2.0: Object-centric simulation for robot learning of everyday household tasks",
            "brief_description": "A PyBullet-based object-centric simulation platform with an emphasis on interaction and realistic object physics, listed in the paper's comparison table as an alternative simulator.",
            "citation_title": "igibson 2.0: Object-centric simulation for robot learning of everyday household tasks.",
            "mention_or_use": "mention",
            "simulator_name": "iGibson 2.0 (PyBullet backend)",
            "simulator_description": "Simulator focusing on object-centric physics using PyBullet, supports arm manipulation, multi-agent, VR, and detailed object state interactions across a modest number of scenes.",
            "scientific_domain": "embodied AI / robotics",
            "fidelity_level": "Medium fidelity physics via PyBullet; supports object states and arm manipulation.",
            "fidelity_characteristics": "Uses PyBullet physics, supports arm manipulation and multi-agent, VR support; reported as 15 scenes and 1217 objects in the comparison table in this paper.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "Embodied interaction and manipulation tasks (general mention).",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1527.6",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "Unity (engine)",
            "name_full": "Unity 3D game engine",
            "brief_description": "A general-purpose real-time game engine used as the backend renderer/physics engine for AI2-THOR, providing rendering, physics, shaders, and extensibility for simulation.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Unity (rendering & physics backend)",
            "simulator_description": "Real-time game engine used to store scenes, execute actions, render image modalities via shaders, and perform approximate physics modelling for AI2-THOR and other Unity-based simulators.",
            "scientific_domain": "simulation / graphics / robotics",
            "fidelity_level": "Engine can support high visual fidelity and approximate physics; actual fidelity depends on assets, physics configuration, and implemented interactions (in AI2-THOR configured for near photo-realistic visuals and approximate rigid-body physics).",
            "fidelity_characteristics": "Provides rendering pipelines (RGB, depth, segmentation), shader support for modalities, approximate physics and collision, headless rendering and WebGL support; physics are approximate rigid-body and joint kinematics rather than high-precision continuous physics.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1527.7",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Procthor: Large-scale embodied ai using procedural generation.",
            "rating": 2
        },
        {
            "paper_title": "Robothor: An open simulation-to-real embodied ai platform.",
            "rating": 2
        },
        {
            "paper_title": "Habitat: A platform for embodied ai research.",
            "rating": 2
        },
        {
            "paper_title": "Habitat 2.0: Training home assistants to rearrange their habitat.",
            "rating": 2
        },
        {
            "paper_title": "Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI.",
            "rating": 2
        },
        {
            "paper_title": "igibson 2.0: Object-centric simulation for robot learning of everyday household tasks.",
            "rating": 2
        },
        {
            "paper_title": "Manipulathor: A framework for visual object manipulation.",
            "rating": 2
        },
        {
            "paper_title": "ThreeDWorld: A platform for interactive multi-modal physical simulation.",
            "rating": 1
        },
        {
            "paper_title": "Sapien: A simulated part-based interactive environment.",
            "rating": 1
        }
    ],
    "cost": 0.0221465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AI2-THOR: An Interactive 3D Environment for Visual AI</h1>
<p>Eric Kolve ${ }^{1}$, Roozbeh Mottaghi ${ }^{1,2}$, Winson Han ${ }^{1}$, Eli VanderBilt ${ }^{1}$, Luca Weihs ${ }^{1}$, Alvaro Herrasti ${ }^{1}$, Matt Deitke ${ }^{1,2}$, Kiana Ehsani ${ }^{1}$, Daniel Gordon ${ }^{2}$, Yuke Zhu ${ }^{3}$, Aniruddha Kembhavi ${ }^{1,2}$, Abhinav Gupta ${ }^{1,4}$, Ali Farhadi ${ }^{1,2}$<br>${ }^{1}$ Allen Institute for AI, ${ }^{2}$ University of Washington, ${ }^{3}$ Stanford University, ${ }^{4}$ Carnegie Mellon University</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AI2-THOR consists of interactive 3D environments that can be used with embodied agents.</p>
<h4>Abstract</h4>
<p>We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.</p>
<h2>1 What is AI2-THOR?</h2>
<p>Humans demonstrate levels of visual understanding that go well beyond current formulations of mainstream vision tasks (e.g. object detection, scene recognition, image segmentation). A key element to visual intelligence is the ability to interact with the environment and learn from those interactions. Current state-of-the-art models in computer vision are trained by using still images or videos. This is different from how humans learn. We introduce AI2-THOR as a step towards human-like learning based on visual input.
There are several key factors that distinguish AI2-THOR from other simulated environments:</p>
<ol>
<li>Interactions. AI2-THOR supports many types of interactions, including object state changes, arm-based manipulation, and causal interactions. For example, a microwave can be opened or closed, a loaf of bread</li>
</ol>
<p>can be sliced and toasted in the toaster, and a faucet can be turned on to fill a mug with water. Figure 6 shows some examples of interactions supported in AI2-THOR.
2. Scenes. AI2-THOR provides substantially more interactive objects and scenes for training than other platforms [18, 32, 7] by using procedural generation [2]. We also provide support for many scenes designed manually by professional 3D artists, with 120 stand-alone rooms in iTHOR, 89 scenes in RoboTHOR [1], and 10 evaluation houses ArchitecTHOR [2].
3. Quality. The objects and scenes in AI2-THOR are near photo-realistic. This allows better transfer of the learned models to the real world. In contrast, ATARI games or board games such as GO, which are typically used to demonstrate the performance of AI models, are very different from the real world and lack much of the visual complexity of natural environments.
4. API. AI2-THOR provides a Python API to interact with the Unity 3D game engine that provides many different functionalities such as navigation, applying forces, object interaction, and physics modeling.</p>
<p>Real robot experiments are typically performed in lab settings or constrained scenes since deploying robots in various indoor and outdoor scenes is not scalable. This makes training models that generalize to various situations difficult. Additionally, due to mechanical constraints of robot actuators, using learning algorithms that require thousands of iterations is infeasible. Furthermore, training real robots might be costly or unsafe as they might damage the surrounding environment or the robots themselves during training. AI2-THOR provides a scalable, fast and cheap proxy for real world experiments in different types of scenarios.</p>
<p>In the following sections, we discuss more of the features included in AI2-THOR, how it compares to other simulators, and work that has been conducted in it since the initial release.</p>
<h1>2 What does AI2-THOR feature?</h1>
<p>AI2-THOR is used for a wide range of tasks in Embodied AI, robotics, and computer vision. It encompasses many different types of scenes; different types of agents, each with its own set of actions to interact with objects; support for many image modalities; and functions to provide metadata about the state of the environment.</p>
<h3>2.1 API</h3>
<p>Figure 2 shows AI2-THOR's agent-simulator loop, which shows the front-end Python API that interacts with the Unity back-end. Here, actions are called from the Python API, which are sent through a local server to Unity. Unity is a powerful real-time game engine, which stores our scenes, code pertaining to how actions should be executed, 3D objects with their properties, and shaders to render different image modalities. Unity then returns an Event, which contains images from the cameras in the scene and the environment metadata.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: AI2-THOR's agent-simulator loop, where users control an agent from the Python side that interacts with the backend Unity simulator.</p>
<h3>2.2 Scene Datasets</h3>
<p>Many scene datasets have been built as part of AI2-THOR, including iTHOR, RoboTHOR [1], ProcTHOR10K [2], and ArchitecTHOR [2]. Each of these scene datasets is interactive and can be used from the same API with any of the agents.
iTHOR is the original set of scenes used for all experiments, which includes 120 room-sized scenes, covering bedrooms, bathrooms, kitchens, and living rooms. The scenes are modeled by hand by professional 3D artists.
RoboTHOR [1] was later developed, which consists of 89 maze-styled dorm-sized apartments to study sim2real transfer. The scenes are also developed by professional 3D artists. Many of the scenes are recreated in Seattle, near</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: AI2-THOR includes many scene datasets, including iTHOR, RoboTHOR [1], ProcTHOR [2], and ArchitecTHOR [2].
the Allen Institute for AI's offices, to study the discrepancies when evaluating models in the same environments in simulation compared to reality.
ProcTHOR [2] aims to use procedural generation to massively scale up the number and diversity of training scenes to improve generalization in Embodied AI. Overfitting to the training scenes is a severe problem that is often observed when training on iTHOR and RoboTHOR scenes, and it was hypothesized that merely improving the training data could help solve this problem. ProcTHOR-10K, the initial dataset released with the paper and used for experimentation, procedurally generates 10 K diverse and semantically plausible houses for training. Using ProcTHOR for training led to remarkable generalization results, and we expect it to be used as a starting point for training most projects in AI2-THOR moving forward.
ArchitecTHOR [2] is a set of 10 evaluation houses ( 5 for validation, 5 for testing) that was developed in conjunction with ProcTHOR. With ProcTHOR being procedurally generated, a test set of houses that comes from a real-world distribution are needed to evaluate if models training on ProcTHOR merely memorize biases from the procedural generation, or if they are capable of generalizing to real-world floorplans and object placements. Similar to iTHOR and RoboTHOR, the scenes are hand-built by professional 3D artists, although ArchitecTHOR houses are much larger and styled as single story houses.</p>
<h1>2.3 Agents</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The current agents available in AI2-THOR include the ManipulaTHOR and StretchRE1 agents, which support arm manipulation, and the LoCoBot, Abstract, and Drone agents, which support navigation and abstracted interaction.</p>
<p>AI2-THOR comes equipped with many agents that support a range of embodiments, including the ManipulaTHOR [5] agent, StretchRE1 [14], LoCoBot [24], Abstract agent, and Drone [42] agent. Each of these agents is embodied with a different physical robot and has its own set of actions that it can execute in the environment.
All of the agents are able to navigate around the scenes and perform environment queries and state changes. The ManipulaTHOR and StetchRE1 agents are able to use their arm to grasp and open objects. The LoCoBot, Abstract, and Drone agents interact with objects in a more abstract way, where a high-level OPEN or PICKUP command is executed if the agent is looking at the object, the high-level action is called, and the agent within a certain distance of the object.</p>
<h1>2.4 Actions</h1>
<p>Agents in AI2-THOR support a wide range of actions, which we can break down into navigation actions, interaction actions, environment queries, and environment state changes.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Examples of actions supported in AI2-THOR, including navigation actions (e.g. movement), interactive actions (e.g. object state changes and grasping), environment queries (e.g. finding the shortest path), and environment state changes (e.g. randomizing materials).</p>
<p>Navigation Actions. Each agent comes with some ability to navigate in a given scene. Navigation actions may be discrete or continuous move (e.g. MoveAhead by 0.25 m ), rotate (e.g. RotateRight by $30^{\circ}$ ), look (e.g. LookUP by $30^{\circ}$ ), or teleport actions. Agents with an arm have more actions to control how the arm is positioned.</p>
<p>Interactive Actions. There are many types of interactions supported in AI2-THOR, including abstracted interactions, arm-based manipulation, object state changes, and causal interactions.
Abstracted interactions are often a key component of research in Embodied AI, where one may be interested in studying high-level planning rather than low-level control. Here, an agent can execute an abstracted action, such as open, pickup, push, throw, drop, or place, where as long as the agent can see the object in its frame and it is within a certain distance away from it, the action can execute successfully. Abstracted actions can be used to change an object's state, such as cooking it, breaking it, slicing it, toggling it, filling it with liquid, or using it up.
Arm-based interactions are lower-level than abstracted actions, and require interacting with objects by moving an arm to grip them. They can be used to open an object incrementally in a continuous manner (Figure 5c) or grasping an object to move it from one position to another (Figure 5d).
Causal interactions result as a consequence of interacting with another object. For instance, turning a coffee machine on, which has a mug placed in it, will fill the mug with coffee; throwing a breakable an object hard enough may cause it and the surface it is thrown at to shatter; and pushing a table over will cause objects on top of the table to fall and potentially break.</p>
<p>Environment Queries. Environment queries are used to obtain information about the state of the environment that is not provided with each EVENT because it is often unnecessary to compute at each time step for every use case. Examples include obtaining the shortest path from the agent to a given target object in the scene, querying which object appears at a pixel in the agent's current frame, or obtaining the convex hull of a given object.</p>
<p>Environment State Changes. Environment state changes involve actions that modify the environment or its properties. For example, some environment state changes include randomizing the materials in the scene (Figure 5f), randomizing the lighting in the scene, updating the rendering quality, updating the resolution of the images from the cameras, and changing the skybox in the scene.</p>
<h1>2.5 Image Modalities</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Examples of image modalities supported in AI2-THOR, including RGB, depth, semantic segmentation, instance segmentation, and normals.</p>
<p>Figure 7 shows a suite of different image modalities that can be rendered from each of the cameras in the scene, including RGB, depth, semantic segmentation, instance segmentation, and normals. Each agent comes with a camera attached to it, but more cameras can also be added, such as one to capture a top-down view of the scene. More image modalities can be added by modifying the Unity back-end (often by adding shaders).</p>
<h3>2.6 Objects</h3>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Examples of objects in AI2-THOR's object database.</p>
<p>AI2-THOR includes 3,578 interactive objects in its object database, which is rapidly growing. Each of these objects has been hand-modeled to support our set of interactive actions and state changes, such as opening, breaking, or cooking. Figure 8 shows samples of objects from 4 categories, including alarm clocks, side tables, plants, and chairs.</p>
<h3>2.7 Environment Metadata</h3>
<p>Environment metadata is returned after each action is executed. It includes information such as the pose of each agent; the pose and state of each object in the scene (e.g., whether the object is moving, if it is visible to the agent, how far open it is, if it is clean or dirty); metadata about the scene, such as its size; and if the most recent action executed successfully (e.g., the agent did not collide with an object while trying to move). Metadata is often not provided to the agent for most tasks, as it would make the tasks too simple and easily solvable with a heuristic. Instead, many tasks use metadata to build a reward function with access to "expert-level" information that is hidden from the agent, build an imitation learning expert, and construct training and evaluation datasets.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Examples of environment metadata, including the dimensions of the scene, the 3D bounding box of each object, and the reachable grid positions, which may be used to randomize the agent's starting position or to build a heuristic search agent.</p>
<h1>3 What has AI2-THOR been used for?</h1>
<p>Since the initial release of AI2-THOR in 2017, it has been used for experimentation in over 150 publications and downloaded over 500k times. Some areas of work that we found particularly interesting include:</p>
<ul>
<li>Visual Navigation. Visual navigation was the first use case of AI2-THOR [45], which trains an agent to perform ImageNav (i.e. navigating to an image where the target object is described with a picture of it). Here, the agent executes a sequence of move or rotate commands to reach the target from egocentric camera inputs at each time step. ObjectNav is another common navigation task, where the agent is tasked with navigating to a given semantic category, such as a bed. Follow-up work from [40, 3, 44] uses semantic priors about where objects typically occur to improve navigation efficiency; [37] used meta-learning to try and better adapt to unseen scenes; [22] uses a Markov network to build a map of the environment; [15] found that using CLIP as a pre-trained visual encoder helps significantly boost generalization performance; and [2] found that training on many procedurally generated scenes strongly generalizes to RoboTHOR, iTHOR, and ArchitecTHOR in a 0 -shot setting.</li>
<li>Audio-Visual Navigation. [8] proposes the task of audio-visual navigation in which the agent is tasked with navigating to find where the sound is coming from in the scene.</li>
<li>Vision-and-Language. AI2-THOR has been used extensively for embodied vision-and-language research. Noteable datasets include ALFRED [31], for interactive instruction following from natural language; TEACh [27], for interactive instruction following from human-robot dialog; and DialFRED [9] and IQA [10] for interactive question-answering. Some other interesting work includes [28], which proposes the Episodic Transformer to encode the full history of vision and language inputs with each ALFRED task; [13], which uses grammar-based methods to learn high-level abstractions through decompositions of tasks; FILM [23], which builds a semantic map to perform exploration for instruction following; and PIGLeT [41], which learns natural language grounding through interaction.</li>
<li>Human-Robot Interaction. [38] inserts a human into AI2-THOR and uses virtual reality to control its gestures in simulation. By controlling the human's gestures, it can communicate different tasks it wants the robot to achieve, such as pointing to an object to encode moving to that object.</li>
<li>Sim2Real Transfer. RoboTHOR [1] studies sim2real transfer for robotics. Here, the goal is to train in simulation because it is faster, cheaper, and more scalable, and then to deploy the trained agent in the realworld. Agents train on 75 scenes in simulation and evaluate on unseen real-world scenes that come from a similar distribution. Initial work analyzed sim2real transfer for agents trained to perform ObjectNav.</li>
<li>Multi-Agent Interaction. [12] proposes the collaborative task of having 2 agents move to lift up furniture in a scene. For example, both agents might have to navigate to find the television in the scene, and work together to lift it up. Follow-up work from [11] takes the task a step further, where the agents not only have to lift up the furniture, but also work together to move it. Both tasks require visual navigation from the agents, and for them to communicate and coordinate together. Some other notable multi-agent work includes [35], which tasks agents with playing Cache, a variant of hide-and-seek where one agent hides an object and the other agent is tasked with finding that object; [33], which uses multiple agents for interactive question answering; [20], which proposes using multiple agents to more efficiently find multiple target objects in a scene; and</li>
</ul>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: AI2-THOR has enabled research in a wide range of fields. Here, we highlight some examples of how it has been used.</p>
<p>TEACh [27], which uses a commander agent and a follower agent to mimic human-robot dialog to solve interactive tasks.</p>
<ul>
<li>Learning Object Relationships. [19] proposes an approach to learn priors about inter-object functional relationships, such as which knobs on the stove control each burner, that the light switches controls a given light, and that the remote may control a television. [26] proposes using egocentric videos to learn which objects are used together to complete certain activities. They then use the priors to help guide agents towards achieving different activities in AI2-THOR.</li>
<li>Learning Affordances. [25] train an agent to interact with the environment to learn object affordances, which encode which objects may be interacted with and how. For instance, it learns that drawers or fridges may be opened, that the stove can turn on, and that an apple may be sliced. A model with an affordance landscape would make it easier to adapt to downstream tasks, such as learning to cut a tomato with a knife.</li>
<li>Scene Synthesis. ProcTHOR [2] uses procedural generation to synthesize training houses at scale to improve the generalization abilities of embodied agents. It procedurally generated and trained on 10 K houses by first sampling floorplans and then plausibly placing objects within each of the rooms in the floorplan. Remark-</li>
</ul>
<p>ably, pre-training on ProcTHOR alone was able to achieve state-of-the-art performance for ObjectNav on RoboTHOR, iTHOR, and ArchitecTHOR, without leveraging any additional training data. LUMINOUS [43] also uses scene synthesis techniques to train embodied agents, where it focuses on placing objects in iTHOR rooms.</p>
<ul>
<li>Learning with Interaction. AI2-THOR supports a wide range of interactions that can be used to train agents, including for rearranging objects in a scene with RoomR [34], arm-based manipulation with ManipulaTHOR [5], learning about objects by interacting with them [21], and playing hide-and-seek with objects to learn visual representations [35], among many others.</li>
<li>Computer Vision. The rich annotations available in simulation make it easy to use AI2-THOR for pure computer vision tasks. Notable work includes SeGAN [6], which used a GAN to generate occluded parts of an object from images in scene; Interactron [17], which performs object detection with embodied agents that are able to move around in the environment; and [16], which performs depth estimation and action prediction to evaluate contrastive learning approaches.</li>
<li>Interpretability. iSEE [4] uses probing to discover what information is in the hidden representations of Embodied AI models. It focuses on probing ObjectNav and PointNav agents to answer interpretability questions, such as how far the agent thinks it is from the target.</li>
</ul>
<p>AI2-THOR is rapidly updating to build out features and functionality. For the latest published papers, please visit the publication tracker on our website: https://ai2thor.allenai.org/publications.</p>
<h1>4 Why use AI2-THOR?</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Simulator</th>
<th style="text-align: center;">Scale</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Interaction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Simulator</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"># of <br> Scenes</td>
<td style="text-align: center;"># of Objects</td>
<td style="text-align: center;">Object <br> States</td>
<td style="text-align: center;">Arm Manipulation</td>
<td style="text-align: center;">Multi-Agent</td>
<td style="text-align: center;">Sound</td>
<td style="text-align: center;">VR</td>
<td style="text-align: center;">Engine</td>
<td style="text-align: center;">Interactive <br> Editor</td>
</tr>
<tr>
<td style="text-align: center;">AI2-THOR</td>
<td style="text-align: center;">$\infty$ [2]</td>
<td style="text-align: center;">3578</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Unity</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">iGibson 2.0</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">1217</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">PyBullet</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Habitat 1.0</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Magnum</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Habitat 2.0</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Magnum</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">ThreeDWorld</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Unity</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">SAPIEN</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2346</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">PhysX</td>
<td style="text-align: center;">$\times$</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison table between Embodied AI simulators.</p>
<p>Following AI2-THOR's first release in 2017, a number of simulators have been developed, including iGibson 2.0 [18], Habitat 1.0 [30], Habitat 2.0 [32], ThreeDWorld [7], and SAPIEN [39]. Table 1 shows a comparison table between the simulators. AI2-THOR is significantly larger in scale than other simulators, while providing first-class support for interaction, and, by leveraging Unity, makes it easy to add new capabilities.</p>
<p>Performance Benchmark. To benchmark performance, we trained an ObjectNav agent for 1 million steps on a 2-GPU machine. Here, GPU-0 stores and performs updates to the model while GPU-1 renders a batch of parallel instances of the simulator. We obtain a training FPS ranging between 145.5-179.4 (167.7 average). For comparison, we ran the same setup with Habitat 1.0 and obtained a training FPS ranging between 119.7-264.3 (230.5 average). More details are described in Appendix B.</p>
<h2>5 Conclusion</h2>
<p>We present AI2-THOR, a large-scale interactive simulation platform for Embodied AI. It has been used for experimentation in over 150 publications, spanning a wide variety of tasks and research areas. It is highly customizable, and provides first-class support for many different types of scenes, agent embodiments, actions, and metadata. The capabilities of AI2-THOR are rapidly evolving, and we are excited to support new improvements and use cases to come. For the latest information, please visit our website: https://ai2thor.allenai.org/.</p>
<h1>References</h1>
<p>[1] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, Luca Weihs, Mark Yatskar, and Ali Farhadi. Robothor: An open simulation-to-real embodied ai platform. In CVPR, 2020. 2, 3, 6, 7
[2] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. arXiv, 2022. 2, 3, 6, 7, 8, 12
[3] Heming Du, Xin Yu, and Liang Zheng. Learning object relation graph and tentative policy for visual navigation. In ECCV, 2020. 6
[4] Kshitij Dwivedi, Gemma Roig, Aniruddha Kembhavi, and Roozbeh Mottaghi. What do navigation agents learn about their environment? In CVPR, 2022. 7, 8
[5] Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: A framework for visual object manipulation. In CVPR, 2021. 3, 8
[6] Kiana Ehsani, Roozbeh Mottaghi, and Ali Farhadi. Segan: Segmenting and generating the invisible. In CVPR, 2018. 8
[7] Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threedworld: A platform for interactive multi-modal physical simulation. In Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2020. 2, 8
[8] Chuang Gan, Yiwei Zhang, Jiajun Wu, Boqing Gong, and Joshua B Tenenbaum. Look, listen, and act: Towards audiovisual embodied navigation. In ICRA, 2020. 6, 7
[9] Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, and Gaurav S Sukhatme. Dialfred: Dialogueenabled agents for embodied instruction following. IEEE Robotics and Automation Letters, 2022. 6
[10] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In CVPR, 2018. 6
[11] Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, and Alexander G. Schwing. A cordial sync: Going beyond marginal policies for multi-agent embodied tasks. In ECCV, 2020. 6
[12] Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander G. Schwing, and Aniruddha Kembhavi. Two body problem: Collaborative visual task completion. In CVPR, 2019. 6, 7
[13] Siddharth Karamcheti, Dorsa Sadigh, and Percy Liang. Learning adaptive language interfaces through decomposition. arXiv, 2020. 6
[14] Charles C Kemp, Aaron Edsinger, Henry M Clever, and Blaine Matulevich. The design of stretch: A compact, lightweight mobile manipulator for indoor human environments. In ICRA, 2022. 3
[15] Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective: Clip embeddings for embodied ai. In CVPR, 2022. 6
[16] Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting contrastive selfsupervised representation learning pipelines. In ICCV, 2021. 8
[17] Klemen Kotar and Roozbeh Mottaghi. Interactron: Embodied adaptive object detection. In CVPR, 2022. 7, 8
[18] Chengshu Li, Fei Xia, Roberto Martín-Martín, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Elliott Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, Karen Liu, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, and Silvio Savarese. igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. In CoRL, 2021. 2,8
[19] Qi Li, Kaichun Mo, Yanchao Yang, Hang Zhao, and Leonidas Guibas. Ifr-explore: Learning inter-object functional relationships in 3d indoor scenes. In $I C L R, 2022.7$
[20] Xinzhu Liu, Di Guo, Huaping Liu, and Fuchun Sun. Multi-agent embodied visual semantic navigation with scene prior knowledge. IEEE Robotics and Automation Letters, 2022. 6
[21] Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, and Roozbeh Mottaghi. Learning about objects by learning to interact with them. In NeurIPS, 2020. 8
[22] Yi Lu, Yaran Chen, Dongbin Zhao, and Dong Li. Mgrl: Graph neural network based inference in a markov network with reinforcement learning for visual navigation. Neurocomputing, 2021. 6
[23] So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. Film: Following instructions in language with modular methods. In $I C L R, 2022.6$
[24] Adithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, and Abhinav Gupta. Pyrobot: An open-source robotics framework for research and benchmarking. arXiv, 2019. 3
[25] Tushar Nagarajan and Kristen Grauman. Learning affordance landscapes for interaction exploration in 3d environments. In NeurIPS, 2020. 7
[26] Tushar Nagarajan and Kristen Grauman. Shaping embodied agent behavior with activity-context priors from egocentric video. In NeurIPS, 2021. 7
[27] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. In AAAI, 2022. 6, 7
[28] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language navigation. In ICCV, 2021. 6</p>
<p>[29] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 11
[30] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A platform for embodied ai research. In ICCV, 2019. 8
[31] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In CVPR, 2020. 6, 7
[32] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel X. Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In NeurIPS, 2021. 2, 8
[33] Sinan Tan, Weilai Xiang, Huaping Liu, Di Guo, and Fuchun Sun. Multi-agent embodied question answering in interactive environments. In ECCV, 2020. 6
[34] Luca Weihs, Matt Deitke, Aniruddha Kembhavi, and Roozbeh Mottaghi. Visual room rearrangement. In CVPR, 2021. 7,8
[35] Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, and Ali Farhadi. Learning generalizable visual representations via interactive gameplay. In $I C L R, 2021.6,8$
[36] Luca Weihs, Jordi Salvador, Klemen Kotar, Unnat Jain, Kuo-Hao Zeng, Roozbeh Mottaghi, and Aniruddha Kembhavi. Allenact: A framework for embodied AI research. arXiv, 2020. 11
[37] Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. In CVPR, 2019. 6
[38] Qi Wu, Cheng-Ju Wu, Yixin Zhu, and Jungseock Joo. Communicative learning with natural gestures for embodied navigation agents with human-in-the-scene. In IROS, 2021. 6, 7
[39] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. Sapien: A simulated part-based interactive environment. In CVPR, 2020. 8
[40] Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, and Roozbeh Mottaghi. Visual semantic navigation using scene priors. In $I C L R, 2019.6$
[41] Rowan Zellers, Ari Holtzman, Matthew E. Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, and Yejin Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d world. In ACL, 2021. 6
[42] Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, and Ali Farhadi. Visual reaction: Learning to play catch with your drone. In CVPR, 2020. 3
[43] Yizhou Zhao, Kaixiang Lin, Zhiwei Jia, Qiaozi Gao, Govind Thattai, Jesse Thomason, and Gaurav S Sukhatme. Luminous: Indoor scene generation for embodied ai challenges. arXiv, 2021. 8
[44] Kaiyu Zheng, Rohan Chitnis, Yoonchang Sung, George Konidaris, and Stefanie Tellex. Towards optimal correlational object search. In ICRA, 2022. 6
[45] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017. 6, 7, 11</p>
<h1>A Contributions</h1>
<p>Eric Kolve was the lead engineer and built the API that connects Python and Unity, setup the infrastructure for maintenance and development, heavily optimized AI2-THOR to run faster, added support for headless rendering, contributed to the Unity backend, and contributed to RoboTHOR, ProcTHOR, and ManipulaTHOR.</p>
<p>Roozbeh Mottaghi managed the AI2-THOR project and its constituents and made decisions about the technical and artistic features of the framework and set priorities for the team.</p>
<p>Winson Han contributed to the Unity backend logic for features and functionality across AI2-THOR; oversaw the design and functionality of the agents; led the development of logic to support physics-based object interactions, state changes, visibility, repositioning, and the annotation pipeline; set up default object placement in scenes; contributed to the documentation; managed community feature requests and issues; and created many promotional graphics.</p>
<p>Eli VanderBilt built all of the 3D scenes for iTHOR, RoboTHOR, ArchitecTHOR; created thousands of interactive assets; modeled the agents; and designed and implemented various features, including arm-based manipulation.</p>
<p>Luca Weihs contributed to the AI2-THOR frontend and backend through the creation of new actions, tests, and processes; led the development of the AllenAct framework, a library used to train agents on AI2-THOR and Embodied AI tasks [36].</p>
<p>Alvaro Herrasti developed features and infrastructure for the Unity backend and Python API; graphics and shader work; built the WebGL infrastructure and demo integration; built the continuous action physics system for arm-based agents; led the Unity development of ProcTHOR; and contributed to RoboTHOR and ManipulaTHOR.</p>
<p>Matt Deitke led the development of ProcTHOR; built the AI2-THOR website, demo, and wrote documentation; contributed to building RoboTHOR; built infrastructure to make AI2-THOR more accessible; contributed to the Unity backend and Python API; and wrote the revised paper.</p>
<p>Kiana Ehsani led the ManipulaTHOR project and the direction of adding arm-based manipulation with the StretchRE1 and ManipulaTHOR agents.</p>
<p>Daniel Gordon developed some planning and rendering features for the early versions of AI2-THOR.
Yuke Zhu created the very first version of AI2-THOR (mentioned in [45]) with the help of EK and RM.
Aniruddha Kembhavi was involved in decision making for various features of ProcTHOR, ManipulaTHOR, ArchitecTHOR, and RoboTHOR.</p>
<p>Abhinav Gupta provided advice and guidance throughout the course of the project.
Ali Farhadi provided advice and guidance throughout the course of the project.</p>
<h1>B Performance Comparison</h1>
<p>Comparing performance between Embodied AI simulators is a surprisingly difficult question for many reasons:</p>
<ol>
<li>Different simulators support different agents, each with their own action spaces and capabilities, with little standardization across simulators. AI2-THOR supports many different types of agents, including the ManipulaTHOR, Abstract, and LoCoBot agents. The ManipulaTHOR agent is often slower to simulate than a navigation-only LoCoBot agent as it is more complex to physically model a 6 DoF arm as it interacts with objects. This is made even more complex when noting that random action sampling, the simplest policy with which to benchmark, is a poor profiling strategy as some actions are only computationally expensive in rare, but important, settings; for instance, computing arm movements is most expensive when the arm is interacting with many objects, these interactions are rare when randomly sampling but we'd expect them to dominate when using a well-trained agent.</li>
<li>Some simulators are relatively slow when run on a single process but can be easily parallelized with many processes running on a single GPU, e.g. AI2-THOR. Thus single-process simulation speeds may be highly deceptive as they do not capture the ease of scalability.</li>
<li>When training agents via reinforcement learning, there are a large number of factors that bottleneck training speed and so the value of raw simulator speed is substantially reduced. These factors include:
(a) Model forward pass when computing agent rollouts.
(b) Model backward pass when computing gradients for RL losses.
(c) Environment resets - for many simulators (e.g. AI2-THOR, Habitat, iGibson) it is orders of magnitude more expensive to change a scene than it is to take a single agent step. This can be extremely problematic when using synchronous RL algorithms as all simulators will need to wait for a single simulator when that simulator is resetting. When training this means that, in practice, important "tricks" are employed to ensure that scene changes are infrequent or synchronized, without these tricks, performance may be dramatically lower.</li>
</ol>
<p>To attempt to control for the above factors, we set up two profiling experiments, one in Habitat with HM3D [29] and one using ProcTHOR-10K, where we:</p>
<ul>
<li>Use a 2-GPU machine (GeForce RTX 2080 GPUs) where GPU-0 is reserved for the agent's actor-critic policy network and GPU-1 is reserved for simulator instances.</li>
<li>Train agents for the ObjectNav task (using the same LoCoBot agent with the same action space).</li>
<li>For both agents, use the same actor-critic policy network, the same used in the ProcTHOR paper [2].</li>
<li>Remove the "End" action so that agents always take the maximum 500 steps, this minimizes dependence on the learned policy.</li>
<li>Use a rollout length of 128 with the same set of training hyperparameters across both models.</li>
<li>Use a total of 28 parallel simulator processes, this approximately saturates GPU-1 memory. We found that Habitat instances used slightly less GPU memory than ProcTHOR instances and so we could likely increase the number instances for Habitat slightly, but we kept these equal for more direct comparison.</li>
<li>Use a scene update "trick" which forces all simulators to advance to the next scene in a synchronous fashion after every 10 rollouts (e.g. after every $10 \times 128 \times 28=35,840$ total steps across all simulators).</li>
</ul>            </div>
        </div>

    </div>
</body>
</html>
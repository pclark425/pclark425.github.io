<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8253 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8253</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8253</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-270560836</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.11698v1.pdf" target="_blank">Meta Reasoning for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Meta-Reasoning Prompting (MRP), a novel and efficient system prompting method for large language models (LLMs) inspired by human meta-reasoning. Traditional in-context learning-based reasoning techniques, such as Tree-of-Thoughts, show promise but lack consistent state-of-the-art performance across diverse tasks due to their specialized nature. MRP addresses this limitation by guiding LLMs to dynamically select and apply different reasoning methods based on the specific requirements of each task, optimizing both performance and computational efficiency. With MRP, LLM reasoning operates in two phases. Initially, the LLM identifies the most appropriate reasoning method using task input cues and objective descriptions of available methods. Subsequently, it applies the chosen method to complete the task. This dynamic strategy mirrors human meta-reasoning, allowing the model to excel in a wide range of problem domains. We evaluate the effectiveness of MRP through comprehensive benchmarks. The results demonstrate that MRP achieves or approaches state-of-the-art performance across diverse tasks. MRP represents a significant advancement in enabling LLMs to identify cognitive challenges across problems and leverage benefits across different reasoning approaches, enhancing their ability to handle diverse and complex problem domains efficiently. Every LLM deserves a Meta-Reasoning Prompting to unlock its full potential and ensure adaptability in an ever-evolving landscape of challenges and applications.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8253.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8253.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRP (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Reasoning Prompting (with GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system prompt that steers an LLM to evaluate a pool of reasoning-method descriptions per input, score each, choose the highest-scoring method, and then apply that method to solve the task; evaluated using GPT-4 (gpt-4-turbo) in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI GPT-4 family model (gpt-4-turbo) accessed via Azure OpenAI; the paper does not provide internal architecture or parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thoughts (CoT)', 'Tree-of-Thoughts (ToT)', 'Analogical prompting', 'Self-Refine', 'Solo Performance Prompting (SPP)', 'Step-Back Prompting', 'SimToM (perspective-taking)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MRP holds a Reasoning Pool of prompt descriptions (p_i) for each method (extracted from abstracts); for each input x0 the model evaluates M(p_i ∥ p_MR ∥ x0) to produce a score s_i, selects k = argmax_i s_i, then runs the chosen method prompt p_k on x0 to produce the final answer. Each method corresponds to a distinct prompting strategy (e.g., CoT: sequential stepwise decomposition; ToT: exploring multiple reasoning branches; Analogical: self-generated few-shot analogies; Self-Refine: iterative self-evaluation and refinement; SPP: multi-persona collaboration; Step-Back: abstraction and high-level guidance; SimToM: perspective-taking).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>MRP dynamically selects one method per problem from a predefined pool of seven distinct in-context reasoning methods and is compared against applying each single method independently (each baseline method executed alone with the same prompts). The selection mechanism scores combined prompts and picks the top-scoring method (no Top-K ensembling was used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Seven diverse benchmarks: GSM8K (arithmetic word problems), Game of 24 (complex arithmetic game), Trivia Creative Writing (Trivia CW; creative assimilation across domains), HotpotQA (multi-hop QA), BigToM (social reasoning / theory of mind), Code Readability (code quality/readability), and MMLU (high-school STEM subjects).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On GPT-4, MRP macro average = 0.772. Per-task accuracies (GPT-4, MRP row): GSM8K 0.921, Gameof24 0.310, Trivia CW 0.796, HotpotQA 0.797, BigToM 0.570, Code 0.867, MMLU 0.854. MRP achieved second-best on 4 of 7 tasks and the highest overall average across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>MRP shows strong robustness across heterogeneous tasks and particularly improves on more complex/differentiated tasks where single methods diverge; it is less clearly advantageous on simpler tasks (e.g., GSM8K where all methods scored >90%). The paper reports that larger models (GPT-4) exhibit superior meta-reasoning capacity, enabling reliable method selection.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Dynamically selecting reasoning strategies per-problem (diverse-method routing) enables better overall performance and generality than applying a single static reasoning method; MRP attains the highest overall average on the evaluated benchmarks with GPT-4 and excels especially on multi-faceted tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8253.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8253.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRP (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Reasoning Prompting (with GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same MRP selection framework evaluated using GPT-3.5 (gpt-3.5-turbo); the paper reports notably weaker meta-reasoning effectiveness on the smaller model and diagnoses specific error modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI GPT-3.5 family model (gpt-3.5-turbo) accessed via Azure OpenAI; internal architecture and parameter counts not given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thoughts (CoT)', 'Tree-of-Thoughts (ToT)', 'Analogical prompting', 'Self-Refine', 'Solo Performance Prompting (SPP)', 'Step-Back Prompting', 'SimToM (perspective-taking)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Identical Reasoning Pool of seven method descriptions; the model is prompted to score each method per input using M(p_i ∥ p_MR ∥ x0), choose the top method, then execute p_k on x0. Implementation is via system-level prompts and in-context descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (but limited by base model capability)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Same selection-vs-single-method comparison as GPT-4 experiments: MRP (dynamic single-method selection) compared to each baseline method executed alone. No ensembling (Top-K) was used; authors note Top-K/Top-P ensembles as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same seven benchmarks (GSM8K, Game of 24, Trivia CW, HotpotQA, BigToM, Code Readability, MMLU).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On GPT-3.5, MRP macro average = 0.433. Per-task accuracies (GPT-3.5, MRP row): GSM8K 0.781, Gameof24 0.050, Trivia CW 0.346, HotpotQA 0.187, BigToM 0.600, Code 0.759, MMLU 0.722. These results are lower than GPT-4 counterparts and often comparable to or worse than the best single-method baselines on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Authors report error analysis for GPT-3.5 MRP showing Scoring Error (wrong method selection), Self-opinion bias, Factual Error, and Reasoning Error — indicating insufficient meta-awareness in smaller models to reliably select the optimal method. MRP's advantage diminishes when base model lacks meta-reasoning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>MRP's benefits depend on the underlying model's meta-reasoning capability; larger models (GPT-4) can effectively leverage MRP, whereas smaller models (GPT-3.5) show reduced gains due to selection and reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8253.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8253.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-method baselines (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Individual in-context reasoning methods evaluated independently with GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Seven distinct prompting-based reasoning techniques (CoT, ToT, Analogical, Self-Refine, SPP, Step-Back, SimToM) each applied alone as baselines to measure performance without meta-selection; used with GPT-4 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI GPT-4 family model (gpt-4-turbo) accessed via Azure OpenAI; paper evaluates each prompting method on the same tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thoughts', 'Tree-of-Thoughts', 'Analogical prompting', 'Self-Refine', 'Solo Performance Prompting', 'Step-Back Prompting', 'SimToM']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each method implemented via an explicit prompt/description (p_i) and applied directly to input x0; no meta-level scoring/selection is performed. Examples: CoT = stepwise decomposition; ToT = multi-branch search and self-evaluation; Analogical = generate analogous problems as few-shots; Self-Refine = iterative output refinement; SPP = multi-persona collaboration; Step-Back = high-level abstraction guidance; SimToM = perspective-taking for social reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (each experiment uses a single repeated method)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Each of the seven methods was executed independently (i.e., single-method condition) on the same seven benchmarks; results reported per-method per-task and macro averages to compare to MRP.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same seven benchmarks used for MRP evaluation (GSM8K, Game of 24, Trivia CW, HotpotQA, BigToM, Code, MMLU).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Per-method macro averages on GPT-4 (from Table 1): Chain-of-Thoughts 0.654, Tree-of-Thoughts 0.725, Analogical 0.648, Self-Refine 0.677, Solo Performance Prompting 0.688, Step-Back 0.670, SimToM 0.640. Example per-task: ToT achieved 0.942 on GSM8K and 0.410 on GameOf24; CoT achieved 0.914 on GSM8K but lower on other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Single-method approaches can excel on particular benchmarks (e.g., ToT and CoT on GSM8K) but show inconsistent cross-task performance and fail to generalize across diverse problem types; this motivates a meta-selection strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Applying a single static reasoning method yields high performance on some tasks but inconsistent results across heterogeneous benchmarks; MRP's dynamic selection outperforms the average single-method baseline across the evaluated tasks on GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8253.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8253.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-method baselines (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Individual in-context reasoning methods evaluated independently with GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same seven prompting-based reasoning techniques applied alone but run with GPT-3.5; used as baselines to compare robustness of single-method usage against MRP on a weaker base model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI GPT-3.5 family model (gpt-3.5-turbo) accessed via Azure OpenAI; paper evaluates each prompting method on the same tasks with this smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thoughts', 'Tree-of-Thoughts', 'Analogical prompting', 'Self-Refine', 'Solo Performance Prompting', 'Step-Back Prompting', 'SimToM']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Implemented via the same method-specific prompts but executed on GPT-3.5 without meta-selection; used to assess how single-method prompting scales with a smaller LLM's capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Each method executed independently on the same dataset splits as in the GPT-4 experiments; comparisons made between per-method performance and MRP performance under GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same seven benchmarks (GSM8K, Game of 24, Trivia CW, HotpotQA, BigToM, Code, MMLU).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Per-method macro averages on GPT-3.5 (from Table 2): Chain-of-Thoughts 0.416, Tree-of-Thoughts 0.352, Analogical 0.433, Self-Refine 0.372, Solo Performance Prompting 0.469, Step-Back 0.452, SimToM 0.315. Individual task performance varied widely, e.g., some methods performed well on Code (TOT 0.797) but poorly on other benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Smaller model exhibits larger variability and more frequent method-specific failures; for GPT-3.5 the advantage of complex single methods is less reliable and MRP's selection accuracy suffers, leading to reduced benefit from meta-reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>On weaker LLMs, single-method prompting and MRP both suffer; MRP's advantage is curtailed because the model cannot reliably evaluate and choose the optimal method, manifesting in scoring and reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta Reasoning for Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Mixture of prompts for llm task adaptation <em>(Rating: 1)</em></li>
                <li>X-of-Thoughts improves the success rate of LLM on arithmetic problems by integrating three methods <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8253",
    "paper_id": "paper-270560836",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "MRP (GPT-4)",
            "name_full": "Meta-Reasoning Prompting (with GPT-4)",
            "brief_description": "A system prompt that steers an LLM to evaluate a pool of reasoning-method descriptions per input, score each, choose the highest-scoring method, and then apply that method to solve the task; evaluated using GPT-4 (gpt-4-turbo) in the paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-turbo",
            "model_description": "Closed-source OpenAI GPT-4 family model (gpt-4-turbo) accessed via Azure OpenAI; the paper does not provide internal architecture or parameter counts.",
            "reasoning_methods": [
                "Chain-of-Thoughts (CoT)",
                "Tree-of-Thoughts (ToT)",
                "Analogical prompting",
                "Self-Refine",
                "Solo Performance Prompting (SPP)",
                "Step-Back Prompting",
                "SimToM (perspective-taking)"
            ],
            "reasoning_methods_description": "MRP holds a Reasoning Pool of prompt descriptions (p_i) for each method (extracted from abstracts); for each input x0 the model evaluates M(p_i ∥ p_MR ∥ x0) to produce a score s_i, selects k = argmax_i s_i, then runs the chosen method prompt p_k on x0 to produce the final answer. Each method corresponds to a distinct prompting strategy (e.g., CoT: sequential stepwise decomposition; ToT: exploring multiple reasoning branches; Analogical: self-generated few-shot analogies; Self-Refine: iterative self-evaluation and refinement; SPP: multi-persona collaboration; Step-Back: abstraction and high-level guidance; SimToM: perspective-taking).",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "MRP dynamically selects one method per problem from a predefined pool of seven distinct in-context reasoning methods and is compared against applying each single method independently (each baseline method executed alone with the same prompts). The selection mechanism scores combined prompts and picks the top-scoring method (no Top-K ensembling was used in experiments).",
            "task_or_benchmark": "Seven diverse benchmarks: GSM8K (arithmetic word problems), Game of 24 (complex arithmetic game), Trivia Creative Writing (Trivia CW; creative assimilation across domains), HotpotQA (multi-hop QA), BigToM (social reasoning / theory of mind), Code Readability (code quality/readability), and MMLU (high-school STEM subjects).",
            "performance_results": "On GPT-4, MRP macro average = 0.772. Per-task accuracies (GPT-4, MRP row): GSM8K 0.921, Gameof24 0.310, Trivia CW 0.796, HotpotQA 0.797, BigToM 0.570, Code 0.867, MMLU 0.854. MRP achieved second-best on 4 of 7 tasks and the highest overall average across methods.",
            "qualitative_findings": "MRP shows strong robustness across heterogeneous tasks and particularly improves on more complex/differentiated tasks where single methods diverge; it is less clearly advantageous on simpler tasks (e.g., GSM8K where all methods scored &gt;90%). The paper reports that larger models (GPT-4) exhibit superior meta-reasoning capacity, enabling reliable method selection.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Dynamically selecting reasoning strategies per-problem (diverse-method routing) enables better overall performance and generality than applying a single static reasoning method; MRP attains the highest overall average on the evaluated benchmarks with GPT-4 and excels especially on multi-faceted tasks.",
            "uuid": "e8253.0",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MRP (GPT-3.5)",
            "name_full": "Meta-Reasoning Prompting (with GPT-3.5)",
            "brief_description": "Same MRP selection framework evaluated using GPT-3.5 (gpt-3.5-turbo); the paper reports notably weaker meta-reasoning effectiveness on the smaller model and diagnoses specific error modes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Closed-source OpenAI GPT-3.5 family model (gpt-3.5-turbo) accessed via Azure OpenAI; internal architecture and parameter counts not given in the paper.",
            "reasoning_methods": [
                "Chain-of-Thoughts (CoT)",
                "Tree-of-Thoughts (ToT)",
                "Analogical prompting",
                "Self-Refine",
                "Solo Performance Prompting (SPP)",
                "Step-Back Prompting",
                "SimToM (perspective-taking)"
            ],
            "reasoning_methods_description": "Identical Reasoning Pool of seven method descriptions; the model is prompted to score each method per input using M(p_i ∥ p_MR ∥ x0), choose the top method, then execute p_k on x0. Implementation is via system-level prompts and in-context descriptions.",
            "reasoning_diversity": "diverse (but limited by base model capability)",
            "reasoning_diversity_experimental_setup": "Same selection-vs-single-method comparison as GPT-4 experiments: MRP (dynamic single-method selection) compared to each baseline method executed alone. No ensembling (Top-K) was used; authors note Top-K/Top-P ensembles as future work.",
            "task_or_benchmark": "Same seven benchmarks (GSM8K, Game of 24, Trivia CW, HotpotQA, BigToM, Code Readability, MMLU).",
            "performance_results": "On GPT-3.5, MRP macro average = 0.433. Per-task accuracies (GPT-3.5, MRP row): GSM8K 0.781, Gameof24 0.050, Trivia CW 0.346, HotpotQA 0.187, BigToM 0.600, Code 0.759, MMLU 0.722. These results are lower than GPT-4 counterparts and often comparable to or worse than the best single-method baselines on some tasks.",
            "qualitative_findings": "Authors report error analysis for GPT-3.5 MRP showing Scoring Error (wrong method selection), Self-opinion bias, Factual Error, and Reasoning Error — indicating insufficient meta-awareness in smaller models to reliably select the optimal method. MRP's advantage diminishes when base model lacks meta-reasoning capacity.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "MRP's benefits depend on the underlying model's meta-reasoning capability; larger models (GPT-4) can effectively leverage MRP, whereas smaller models (GPT-3.5) show reduced gains due to selection and reasoning errors.",
            "uuid": "e8253.1",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Single-method baselines (GPT-4)",
            "name_full": "Individual in-context reasoning methods evaluated independently with GPT-4",
            "brief_description": "Seven distinct prompting-based reasoning techniques (CoT, ToT, Analogical, Self-Refine, SPP, Step-Back, SimToM) each applied alone as baselines to measure performance without meta-selection; used with GPT-4 in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4-turbo",
            "model_description": "Closed-source OpenAI GPT-4 family model (gpt-4-turbo) accessed via Azure OpenAI; paper evaluates each prompting method on the same tasks.",
            "reasoning_methods": [
                "Chain-of-Thoughts",
                "Tree-of-Thoughts",
                "Analogical prompting",
                "Self-Refine",
                "Solo Performance Prompting",
                "Step-Back Prompting",
                "SimToM"
            ],
            "reasoning_methods_description": "Each method implemented via an explicit prompt/description (p_i) and applied directly to input x0; no meta-level scoring/selection is performed. Examples: CoT = stepwise decomposition; ToT = multi-branch search and self-evaluation; Analogical = generate analogous problems as few-shots; Self-Refine = iterative output refinement; SPP = multi-persona collaboration; Step-Back = high-level abstraction guidance; SimToM = perspective-taking for social reasoning.",
            "reasoning_diversity": "similar (each experiment uses a single repeated method)",
            "reasoning_diversity_experimental_setup": "Each of the seven methods was executed independently (i.e., single-method condition) on the same seven benchmarks; results reported per-method per-task and macro averages to compare to MRP.",
            "task_or_benchmark": "Same seven benchmarks used for MRP evaluation (GSM8K, Game of 24, Trivia CW, HotpotQA, BigToM, Code, MMLU).",
            "performance_results": "Per-method macro averages on GPT-4 (from Table 1): Chain-of-Thoughts 0.654, Tree-of-Thoughts 0.725, Analogical 0.648, Self-Refine 0.677, Solo Performance Prompting 0.688, Step-Back 0.670, SimToM 0.640. Example per-task: ToT achieved 0.942 on GSM8K and 0.410 on GameOf24; CoT achieved 0.914 on GSM8K but lower on other tasks.",
            "qualitative_findings": "Single-method approaches can excel on particular benchmarks (e.g., ToT and CoT on GSM8K) but show inconsistent cross-task performance and fail to generalize across diverse problem types; this motivates a meta-selection strategy.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Applying a single static reasoning method yields high performance on some tasks but inconsistent results across heterogeneous benchmarks; MRP's dynamic selection outperforms the average single-method baseline across the evaluated tasks on GPT-4.",
            "uuid": "e8253.2",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Single-method baselines (GPT-3.5)",
            "name_full": "Individual in-context reasoning methods evaluated independently with GPT-3.5",
            "brief_description": "Same seven prompting-based reasoning techniques applied alone but run with GPT-3.5; used as baselines to compare robustness of single-method usage against MRP on a weaker base model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Closed-source OpenAI GPT-3.5 family model (gpt-3.5-turbo) accessed via Azure OpenAI; paper evaluates each prompting method on the same tasks with this smaller model.",
            "reasoning_methods": [
                "Chain-of-Thoughts",
                "Tree-of-Thoughts",
                "Analogical prompting",
                "Self-Refine",
                "Solo Performance Prompting",
                "Step-Back Prompting",
                "SimToM"
            ],
            "reasoning_methods_description": "Implemented via the same method-specific prompts but executed on GPT-3.5 without meta-selection; used to assess how single-method prompting scales with a smaller LLM's capabilities.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Each method executed independently on the same dataset splits as in the GPT-4 experiments; comparisons made between per-method performance and MRP performance under GPT-3.5.",
            "task_or_benchmark": "Same seven benchmarks (GSM8K, Game of 24, Trivia CW, HotpotQA, BigToM, Code, MMLU).",
            "performance_results": "Per-method macro averages on GPT-3.5 (from Table 2): Chain-of-Thoughts 0.416, Tree-of-Thoughts 0.352, Analogical 0.433, Self-Refine 0.372, Solo Performance Prompting 0.469, Step-Back 0.452, SimToM 0.315. Individual task performance varied widely, e.g., some methods performed well on Code (TOT 0.797) but poorly on other benchmarks.",
            "qualitative_findings": "Smaller model exhibits larger variability and more frequent method-specific failures; for GPT-3.5 the advantage of complex single methods is less reliable and MRP's selection accuracy suffers, leading to reduced benefit from meta-reasoning.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "On weaker LLMs, single-method prompting and MRP both suffer; MRP's advantage is curtailed because the model cannot reliably evaluate and choose the optimal method, manifesting in scoring and reasoning errors.",
            "uuid": "e8253.3",
            "source_info": {
                "paper_title": "Meta Reasoning for Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts",
            "rating": 2,
            "sanitized_title": "plan_verify_and_switch_integrated_reasoning_with_diverse_xofthoughts"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Mixture of prompts for llm task adaptation",
            "rating": 1,
            "sanitized_title": "mixture_of_prompts_for_llm_task_adaptation"
        },
        {
            "paper_title": "X-of-Thoughts improves the success rate of LLM on arithmetic problems by integrating three methods",
            "rating": 1,
            "sanitized_title": "xofthoughts_improves_the_success_rate_of_llm_on_arithmetic_problems_by_integrating_three_methods"
        }
    ],
    "cost": 0.0118345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Meta Reasoning for Large Language Models
17 Jun 2024</p>
<p>Peizhong Gao 
Tsinghua University</p>
<p>Ao Xie 
Tsinghua University</p>
<p>Shaoguang Mao 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Wenshan Wu 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Yan Xia 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Haipeng Mi 
Tsinghua University</p>
<p>Furu Wei 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Microsoft Research Asia</p>
<p>Meta Reasoning for Large Language Models
17 Jun 202442DD103A538B1115489F7101D858B0EDarXiv:2406.11698v1[cs.CL]
We introduce Meta-Reasoning Prompting (MRP), a novel and efficient system prompting method for large language models (LLMs) inspired by human metareasoning.Traditional in-context learning-based reasoning techniques, such as Tree-of-Thoughts, show promise but lack consistent state-of-the-art performance across diverse tasks due to their specialized nature.MRP addresses this limitation by guiding LLMs to dynamically select and apply different reasoning methods based on the specific requirements of each task, optimizing both performance and computational efficiency.With MRP, LLM reasoning operates in two phases.Initially, the LLM identifies the most appropriate reasoning method using task input cues and objective descriptions of available methods.Subsequently, it applies the chosen method to complete the task.This dynamic strategy mirrors human meta-reasoning, allowing the model to excel in a wide range of problem domains.We evaluate the effectiveness of MRP through comprehensive benchmarks.The results demonstrate that MRP achieves or approaches state-of-the-art performance across diverse tasks.MRP represents a significant advancement in enabling LLMs to identify cognitive challenges across problems and leverage benefits across different reasoning approaches, enhancing their ability to handle diverse and complex problem domains efficiently.Every LLM deserves a Meta-Reasoning Prompting to unlock its full potential and ensure adaptability in an ever-evolving landscape of challenges and applications.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown remarkable capabilities in natural language understanding and generation, making significant strides in various reasoning tasks.However, the diversity and complexity of real-world problems require advanced reasoning methods that surpass the capabilities of a single, static approach.While existing reasoning techniques, such as Chain-of-Thoughts [27], Tree-of-Thoughts [32], Analogical Prompting [33], and Solo Performance Prompting [26], offer valuable tools for enhancing reasoning, they often fall short in consistently achieving state-of-the-art performance across different tasks.</p>
<p>These challenges highlight the need for a more adaptive and flexible approach to reasoning in LLMs.In human cognition, meta-reasoning involves monitoring and regulating reasoning and problemsolving activities, adjusting strategies based on the context and specific task requirements [5,4].This adaptive capability allows humans to efficiently allocate cognitive resources, balancing trade-offs between accuracy, complexity, and computational cost.Inspired by this, we propose Meta-Reasoning Prompting (MRP) to endow LLMs with similar adaptive reasoning capabilities.</p>
<p>Meta-Reasoning Prompting (MRP) is a simple yet effective system prompt designed to guide LLMs in dynamically selecting and applying the most suitable reasoning method for a specific task.By incorporating meta-reasoning principles, MRP transforms task-specific prompt engineering into a more general and flexible approach.Under the guidance of MRP, the LLM evaluates the task input and selects an appropriate reasoning method from a predefined set (Reasoning Pool).This selection is informed by objective descriptions and evaluations of the available methods.The chosen method is then applied to complete the task, ensuring the model uses the most effective strategy for the given problem.</p>
<p>Recent advances in reasoning techniques, such as those described in [30,24], introduce a meta-buffer for storing high-level thoughts or use ensemble mechanisms to improve model generalizability.While some of these approaches align with the inherent logic of meta-reasoning, our proposed MRP achieves simple and efficient meta-cognitive effects by directly leveraging the meta-reasoning capabilities of LLMs through prompts, without introducing complex mechanisms.</p>
<p>To evaluate the effectiveness of MRP, we conducted experiments using multiple widely used benchmarks.These benchmarks cover different knowledge and reasoning abilities, providing a comprehensive test of the LLM's performance across various reasoning tasks.Our findings demonstrate that MRP not only approaches state-of-the-art performance across these benchmarks but also excels in tasks requiring a blend of different reasoning strategies.Additionally, we observe that larger models, such as GPT-4, exhibit superior meta-reasoning capabilities compared to smaller models like GPT-3.5.</p>
<p>As models improve, their understanding of problems and methods-i.e., their meta-reasoning abilities-also enhances.MRP utilizes the inherent meta-cognitive abilities of LLMs, providing a straightforward and effective method that enhances their generality across different tasks.Experimental and analytical results indicate the significant potential of MRP in boosting LLM performance.Future work could explore the broader application of MRP, such as constructing training data to enhance the meta-cognitive and general reasoning abilities of LLMs during the training process.</p>
<p>Our key contributions are as follows:</p>
<ol>
<li>
<p>We propose Meta-Reasoning Prompting (MRP), a system prompt that enables LLMs to dynamically select the most suitable reasoning method for specific tasks, enhancing their flexibility and effectiveness.2. Experiments on multiple benchmarks show that MRP approaches state-of-the-art performance and excels in tasks requiring diverse reasoning strategies, particularly in larger models like GPT-4.</p>
</li>
<li>
<p>MRP leverages LLMs' inherent meta-cognitive abilities, improving their generality and performance across tasks.Future work could further enhance these abilities through targeted training data.</p>
</li>
</ol>
<p>Meta-Reasoning Prompting</p>
<p>The Meta-Reasoning Prompting (MRP) is designed to guide a Language Learning Model (LLM) in selecting the most suitable reasoning method from a pool of available methods, thereby enhancing the overall reasoning performance of the model.Detailed prompts can be found in Fig. 2.</p>
<p>Guided by the Meta-Reasoning Prompting, the LLM (M ) begins with an input x 0 and a set of available reasoning methods α 1 , α 2 , . . ., α n .A reasoning pool contains descriptions of each reasoning method in the form of prompts p 1 , p 2 , . . ., p n , with these descriptions extracted from the abstracts of corresponding papers.A Meta-Reasoning Prompting p M R is defined to guide the selection process.For each reasoning method α i (i ranging from 1 to n), the model M evaluates the combined prompt (p i |p M R |x 0 ).This evaluation yields a score s i indicating the effectiveness of method α i for the given input x 0 .
s i = M (p i ∥p M R ∥x 0 ) for i = 1, 2, . . . , n.(1)
The algorithm identifies the reasoning method α k that receives the highest score s i by finding the index k that maximizes the set s 1 , s 2 , . . ., s n .
k = arg max i {s 1 , s 2 , . . . , s n }(2)
Once the best reasoning method α k is determined, it is executed on the input x 0 .The model M generates the final output y 0 using the prompt (p k |x 0 ), which combines the description of the chosen reasoning method with the original input.
y 0 = α k (x 0 )(3)s i = M (p i ∥p M R ∥x 0 ) 3: end for 4: k = arg max i {s 1 , s 2 , . . . , s n } 5:
Determine k for which α k is executed and reason with the chosen method.6: y 0 = α k (x 0 ) Return y 0 3 Experiments</p>
<p>Setup</p>
<p>Implementation of Meta-Reasoning Prompting We implement MRP with seven popular and distinct in-context learning reasoning methods, which also serve as our baseline for comparison.We prompt descriptions for each method, allowing the LLM to understand.</p>
<p>Tasks We experiment with seven diverse tasks, Details about the dataset and its construction are provided in Appendix A.1:</p>
<ol>
<li>Arithmetic Reasoning: GSM8K [3], 1319 basic math questions. 2. Complex Mathematical Reasoning: Game of 24 [32], a game using 4 numbers and basic arithmetic four operations to obtain 24. 3. Creative Writing: Trivia Creative Writing (Trivia CW) [26,14], necessitating the model to assimilate and combine heterogeneous information from multiple domains internally. 4. Multi-Hop Reasoning: HotpotQA, [31], requiring models to connect pieces of information from multiple documents to answer a question.5. Social Reasoning: BigToM [8], to evaluate social situations understanding and the theory of mind.6.Computer Code: Code Readability (Code) [19], to enhance the readability of given code snippets.7. STEM: MMLU [11], Physics, Chemistry, Biology, and Math problems of high school domain.Metrics To prevent any method from skewing the results due to exceptional performance on a specific task, we reported both the arithmetic mean accuracy and the harmonic mean accuracy of each method across all benchmarks.</li>
</ol>
<p>Models We used gpt-3.5-turbo2and gpt-4-turbo 3 with identical prompts to compare the effect of model size on meta-reasoning ability.</p>
<p>Baselines We select seven popular reasoning methods as baselines.These methods include:</p>
<ol>
<li>
<p>Chain-of-Thoughts: breaking down problems into a series of coherent reasoning steps [27].</p>
</li>
<li>
<p>Tree-of-Thoughts: exploring multiple reasoning paths and self-evaluating choices to solve complex problems [32].</p>
</li>
<li>
<p>Analogical prompting: self-generating few-shots based on past experiences and related problems [33].</p>
</li>
<li>
<p>Self-Refine: self-evaluating for refinement and continuously improving the output [17].</p>
</li>
<li>
<p>Solo Performance Prompting: simulating multiple personas to collaboratively solve complex tasks [26].</p>
</li>
</ol>
<p>6.</p>
<p>Step-Back Prompting: abstract high-level concepts and principles to guide the reasoning process [38].</p>
<ol>
<li>SimToM: enabling perspective-taking to understand the character's beliefs and goals [28]</li>
</ol>
<p>Main Results</p>
<p>Meta-Reasoning Prompting performs best on comprehensive tasks As shown in table 1, MRP consistently exhibits robust performance across multiple benchmarks.MRP achieves the second-best in 4 of 7 tasks, including Gameof24, TriviaQA, BigToM and Code.This impressive performance across a wide range of tasks demonstrates MRP's ability to effectively select and apply appropriate reasoning methods tailored to the specific requirements of each task.In terms of overall performance, MRP attains the highest across the 7 tasks, with an average of 0.772.In contrast, although TOT excels in certain tasks such as GSM8K and Gameof24, it performs less impressively in others.We observe noticeable performance gaps compared with MRP in tasks such as BigToM (0.43 VS 0.57) and Code (0.765 VS 0.867).This consistent excellence across all benchmarks underscores MRP's advantages, demonstrating its ability to maintain impressive performance across diverse task domains (as shown in figure 4).</p>
<p>Meta-reasoning capability is influenced by the base model capability As illustrated in table 2, while the performance with GPT-4 is satisfactory, the experimental results with GPT-3.5 indicate that the effectiveness of MRP is suboptimal.Error analysis revealed the main issues: Scoring Error, Self-opinion, Factual Error, and Reasoning Error.This indicates that when the model's capabilities are limited, it cannot have sufficient awareness of its own reasoning abilities and the meta-issues behind the reasoning problems.This performance drop also appears in other reasoning methods, which also indicates that the capability of meta-reasoning, like other reasoning abilities, improves as the model becomes more powerful.</p>
<p>Figure 5: Performance of methods on GSM8K benchmark</p>
<p>Meta-Reasoning Prompting is less effective for simple tasks but significantly improved for more differentiated tasks From the experimental results (see figure 5), it can be seen that MRP and other methods show equal competitiveness on GSM8K, the accuracy of all the reasoning methods is above 90%, but the differentiation between the accuracy of each method is not very high, it can be seen that when the task is simpler, it is harder for MRP to reflect its own advantages, but MRP method is better than each method on the more difficult and comprehensive But the MRP method is significantly better than the other methods in the more difficult and comprehensive tasks.</p>
<p>4 Related Works</p>
<p>Reasoning with LLMs</p>
<p>Prompt-based reasoning methods have become a key technology for enhancing the capabilities of pretrained large language models (LLMs).The Chain-of-Thought (CoT) prompting [27], and its Figure 4: The inference process of large language models (LLMs) under meta-reasoning prompting.</p>
<p>variants [37,39,2,12,25], such as Tree of thoughts (TOT) [32], Graph of thoughts (GOT) [1], enhances LLMs' ability to decompose complex tasks into smaller, manageable tasks, utilizing structured approaches to explore problem-solving pathways.Numerous studies have demonstrated the exceptional performance of prompt-based reasoning methods across various domains and benchmarks.[17,28,38,20,23] Some researchers have even employed analogical reasoning [34,7,33], enabling large models to generate similar questions based on user queries and subsequently summarize solutions based on the answers to these questions.While independent reasoning methods have been proven to improve LLM performance from different perspectives, they still fail to meet integrated problems.</p>
<p>There are also some methods to enhance LLM reasoning through ensemble mechanisms or tuning.X-of-Thoughts improves the success rate of LLM on arithmetic problems by integrating three methods [15].It proposes a trial-and-error iterative mechanism that allows LLM to autonomously repeat attempts to find a final solution.Ni et al.blending off-the-shelf benchmarks to create a comprehensive, integrated LLM assessment [18].Mixtural-Of-Prompts (MoP) dynamically manage and optimize prompt tuning across heterogeneous tasks and data distributions, significantly reducing perplexity and mitigating interference in multi-task scenarios [6].Some researchers fine-tune smaller models with a well-prepared dataset inspired by preference learning to achieve reasoning power comparable to a larger model [35,22,29].They present problem-method coupled datasets and show how to improve the model's grasp of inference skills at the data level.However, there is still a lack of research to explore the meta-reasoning ability of LLMs to choose reasoning methods.</p>
<p>Meta Reasoning</p>
<p>Meta-reasoning is a crucial cognitive process in human intelligence, involving the recognition and interpretation of reasoning to select optimal methods based on past experiences [9].In artificial intelligence, it refers to efficiently deploying computational resources for informed decision-making in specific situations [4,5].Recently, some works develop routing or buffer systems to improve performance, using supervised learning algorithms [21], reward model-based techniques, and other methods [10,16,24].Hu et al. created a benchmark to evaluate these methods' effectiveness [13].</p>
<p>Zeng et al. noted the neglect of meta-reasoning in independent LLMs and proposed a benchmark to evaluate reasoning rationality [36].In [30], the authors introduce a meta-buffer to store a series of high-level thoughts distilled from problem-solving processes across various tasks.This approach aligns with the inherent logic of meta reasoning.However, MRP achieves simple and efficient metacognitive effects by directly unleashing the meta reasoning capabilities of LLM through prompts, without introducing complicated mechanisms.</p>
<p>Conclusions and Outlook</p>
<p>This paper introduces Meta-Reasoning Prompting (MRP), a novel and efficient approach inspired by human meta-reasoning, designed to enhance the adaptability and efficiency of large language models (LLMs).By dynamically selecting and applying the most suitable reasoning method for each task, MRP enables LLMs to optimize performance across diverse problem domains, achieving near state-of-the-art results in comprehensive benchmarks.</p>
<p>Our experiments demonstrate that MRP significantly improves LLMs' ability to handle tasks requiring a blend of different reasoning strategies, particularly in larger models like GPT-4.This dynamic adaptability highlights MRP's potential to address the limitations of traditional reasoning techniques, offering a more flexible and effective solution for varied and complex tasks.</p>
<p>Looking ahead, future research could explore the integration of MRP into training datasets to further enhance LLMs' general reasoning abilities.Additionally, combining MRP with other advanced reasoning techniques could yield further improvements in model performance.Overall, MRP represents a significant step forward in developing more intelligent, efficient, and adaptable AI systems, capable of meeting the diverse demands of real-world problem-solving.</p>
<p>Limitations</p>
<p>Our study investigates the meta-reasoning mechanisms of LLMs by dynamically selecting suitable methods to enhance their performance across various reasoning tasks without introducing new knowledge or training efforts.Currently, Meta-Reasoning Prompting (MRP) selects the highestscoring method for each task.However, drawing from human cognitive processes, tackling complex problems often involves combining multiple reasoning methods.Future research will explore mechanisms such as Top-Probability (Top-P) or Top-K to allow models to ensemble relevant methods, potentially achieving better performance.</p>
<p>Our experimental results indicate that the meta-reasoning ability of LLMs is influenced by the capabilities of the models themselves.For instance, GPT-4's Meta-Reasoning Prompting shows significantly greater improvement compared to GPT-3.5, which aligns with our expectations.Nonetheless, we can further enhance the smaller model's meta-reasoning capabilities through instruction tuning in future works.</p>
<p>Due to space constraints and limited resources, our experiments primarily tested the most representative LLMs (GPT-4 and GPT-3.5).We did not fully cover the performance of other open-source or closed-source models.However, we believe that the experimental results on these representative LLMs provide sufficient insights and implications.</p>
<p>Figure 1 :
1
Figure 1: Illustration of Meta-Reasoning Prompting (MRP) and the difference compared to standard reasoning and traditional reasoning methods.</p>
<p>Figure 2 :
2
Figure 2: Meta-Reasoning Prompt.</p>
<p>Figure 3 :
3
Figure3: (a) Comparison of methods on different benchmarks reveals that guiding LLM to dynamically choose the appropriate reasoning method enables MRP to achieve consistently better performance across all tasks.(b) The arithmetic and harmonic average performances of applying a specific reasoning approach to all benchmarks demonstrate that MRP consistently excels in overall evaluation.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Prompt of COT</p>
<p>Figure 8 :Figure 9 : 13 Figure 10 : 14 Figure 11 :
8913101411
Figure 8: Prompt of TOT</p>
<p>Table 1 :
1
Experiments with GPT4: Comparison of performance on benchmarks using Meta-Reasoning Prompting versus using other methods independently.Bold represents the best performance, and underline represents the second-best performance.
MethodGSM8K Gameof24 Trivia CW HotpotQA BigToMCodeMMLU Macro Avg.COT0.9140.0500.7620.8000.4700.6850.8940.654TOT0.9420.4100.7860.7160.4300.7650.8150.725Analogical0.9240.0400.7350.7770.5000.6140.9470.648Self-Refine0.9290.0800.7640.7630.4700.8720.8610.677SPP0.9290.1700.8610.7630.5500.6720.8740.688STEP-BACK0.9330.0900.7870.8100.4200.8090.8410.670SimTom0.9380.0400.7390.6670.5900.6940.8150.640MRP (our)0.9210.3100.7960.7970.5700.8670.8540.772</p>
<p>Table 2 :
2
Experiments with GPT3.5:Comparison of performance on benchmarks using Meta-Reasoning Prompting versus using other methods independently.Bold represents the best performance, and underline represents the second-best performance.
MethodGSM8K Gameof24 Trivia CW HotpotQA BigToMCodeMMLU Macro Avg.COT0.8310.0300.4140.1870.6100.5780.6750.416TOT0.8100.1000.1550.3600.4300.7970.7350.352Analogical0.8250.0600.3240.1970.6600.7290.7210.433Self-Refine0.7160.0300.2130.1670.6500.7960.5430.372SPP0.8230.1600.5360.2170.5400.6840.6890.469STEP-BACK0.8170.0100.5360.1900.5700.6420.7880.452SimTom0.5860.0400.2400.1770.4600.5990.5030.315MRP (our)0.7810.0500.3460.1870.6000.7590.7220.433
Azure OpenAI, Model Name: gpt-35-turbo, API Version: 0301
Azure OpenAI, Model Name: gpt-4, API Version: 1106-Preview
A Implementation DetailsA.1 Dataset Details Table3shows the split and number of examples used for evaluations in GSM8K, Game of 24, Trivia Creative Writing, HotpotQA, BigTOM, Code Readability and MMLU.The dataset sizes of GSM8K, Gameof24, Trivia Creative Writing are consistent with the size used in the references.To control cost, we randomly tested 100-300 sample of data from HotpotQA, BigTOM, and Code Readability and MMLU.Despite of the economic consideration, we found that on this data scale, MRP has achieved significant results.
Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Boosting of thoughts: Trial-and-error problem solving with large language models. Sijia Chen, Baochun Li, Di Niu, arXiv:2402.111402024arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Metareasoning: An Introduction. Metareasoning. T Michael, Anita Cox, Raja, 2011</p>
<p>Metareasoning: Thinking about thinking. T Michael, Anita Cox, Raja, 2011MIT Press</p>
<p>Sweeping heterogeneity with smart mops: Mixture of prompts for llm task adaptation. Chen Dun, Mirian Del Carmen, Hipolito Garcia, Guoqing Zheng, Ahmed Hassan Awadallah, Anastasios Kyrillidis, Robert Sim, arXiv:2310.028422023arXiv preprint</p>
<p>Thought-retriever: Don't just retrieve raw data, retrieve thoughts. Tao Feng, Pengrui Han, Guanyu Lin, Ge Liu, Jiaxuan You, ICLR 2024 Workshop: How Far Are We From AGI. 2024</p>
<p>Understanding social reasoning in language models with language models. Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, Noah Goodman, Advances in Neural Information Processing Systems. 362024</p>
<p>Doing more with less: meta-reasoning and meta-learning in humans and machines. Frederick Thomas L Griffiths, Michael B Callaway, Erin Chang, Paul M Grant, Falk Krueger, Lieder, Current Opinion in Behavioral Sciences. 292019</p>
<p>Surya Narayanan, Hari , Matt Thomson, arXiv:2308.11601Tryage: Real-time, intelligent routing of user prompts to large language model. 2023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Chain-ofsymbol prompting elicits planning in large langauge models. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang, arXiv:2305.102762023arXiv preprint</p>
<p>Mars: A benchmark for multi-llm algorithmic routing system. Jason Qitian, Jacob Hu, Xiuyu Bieker, Nan Li, Benjamin Jiang, Gaurav Keigwin, Kurt Ranganath, Shriyash Keutzer, Upadhyay Kaustubh, ICLR 2024 Workshop: How Far Are We From AGI. 2024</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017arXiv preprint</p>
<p>Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts. Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Blending is all you need: Cheaper, better alternative to trillion-parameters llm. Xiaoding Lu, Adian Liusie, Raina Vyas, Yuwen Zhang, William Beauchamp, arXiv:2401.029942024arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, Yang You, arXiv:2406.06565Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. 2024arXiv preprint</p>
<p>Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, arXiv:2105.126552021arXiv preprint</p>
<p>A systematic survey of prompt engineering in large language models. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.079272024Techniques and applications. arXiv preprint</p>
<p>Tal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, Mikhail Yurochkin, arXiv:2309.15789Large language model routing with benchmark datasets. 2023arXiv preprint</p>
<p>Branch-train-mix: Mixing expert llms into a mixture-of-experts llm. Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-Tau Yih, Jason Weston, arXiv:2403.078162024arXiv preprint</p>
<p>Meta-prompting: Enhancing language models with task-agnostic scaffolding. Mirac Suzgun, Adam Tauman, Kalai , arXiv:2401.129542024arXiv preprint</p>
<p>Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou, arXiv:2406.04692Mixture-of-agents enhances large language model capabilities. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.05300202313arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Alex Wilf, Shawn Sihyun, Paul Pu Lee, Louis-Philippe Liang, Morency, arXiv:2311.10227Think twice: Perspective-taking improves large language models' theory-of-mind capabilities. 2023arXiv preprint</p>
<p>Mixture-of-instructions: Comprehensive alignment of a large language model through the mixture of diverse system prompting instructions. Bowen Xu, Shaoyu Wu, Kai Liu, Lulu Hu, arXiv:2404.184102024arXiv preprint</p>
<p>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, arXiv:2406.04271Buffer of thoughts: Thought-augmented reasoning with large language models. 2024arXiv preprint</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, Denny Zhou, arXiv:2310.01714Large language models as analogical reasoners. 2023arXiv preprint</p>
<p>Thought propagation: An analogical approach to complex reasoning with large language models. Junchi Yu, Ran He, Rex Ying, arXiv:2310.039652023arXiv preprint</p>
<p>Advancing llm reasoning generalists with preference trees. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, arXiv:2404.020782024arXiv preprint</p>
<p>Mr-gsm8k: A meta-reasoning revolution in large language model evaluation. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia, 2024</p>
<p>Enhancing zero-shot chain-of-thought reasoning in large language models through logic. Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter, arXiv:2309.133392023arXiv preprint</p>
<p>Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Quoc V Chi, Denny Le, Zhou, arXiv:2310.06117Take a step back: Evoking reasoning via abstraction in large language models. 2023arXiv preprint</p>
<p>Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen, arXiv:2311.08734Thread of thought unraveling chaotic contexts. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
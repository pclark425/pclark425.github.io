<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6144 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6144</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6144</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-a9e155fda1d97baa2b8712f580cc61887cc64e9b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a9e155fda1d97baa2b8712f580cc61887cc64e9b" target="_blank">ChatGPT outperforms crowd workers for text-annotation tasks</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the National Academy of Sciences of the United States of America</p>
                <p><strong>Paper TL;DR:</strong> It is shown thatChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection, and the per-annotation cost of ChatGPT is less than $0.003—about thirty times cheaper than MTurk.</p>
                <p><strong>Paper Abstract:</strong> Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6144.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6144.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accuracy_vs_gold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy (agreement with trained annotators as gold standard)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation method computing percentage of correct LLM annotations by comparing LLM labels to a gold standard produced by trained human annotators; accuracy computed only on texts where human annotators agreed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare LLM-produced labels to a human-created gold standard (trained annotators); compute percentage of correctly classified instances over total (accuracy). Only cases where human annotators agreed are counted for accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy = (number of LLM labels matching gold-standard labels) / (number of evaluated instances); reported per task (e.g., relevance, topic, stance, frames).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Computational social science / political communication (annotation of tweets and news articles about content moderation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory per se; LLM-generated categorical annotations (labels) for texts used as the object of evaluation (e.g., relevance, stance, topic, frame).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across four datasets (n=6,183), ChatGPT zero-shot accuracy exceeded MTurk by ~25 percentage points on average. Example accuracy rates for relevance tasks: 70% (content-moderation tweets), 81% (content-moderation news), 83% (US Congress tweets), 59% (2023 content-moderation tweets).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Four datasets: (i) 2,382 random content-moderation tweets (Jan 2020–Apr 2021); (ii) 1,856 US Congress tweets (2017–2022); (iii) 1,606 newspaper articles on content moderation (Jan 2020–Apr 2021); (iv) 500 tweets from Jan 2023 (339 English). Trained annotators' labels used as gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct comparison to MTurk crowd-workers (two crowd-workers per item) and to trained human annotators (gold standard). ChatGPT outperformed MTurk on most tasks in accuracy and matched/contributed to comparisons with trained annotators through the gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Accuracy is measured relative to human gold standard; paper does not evaluate LLM-generated scientific theories per se but LLM labeling performance. Accuracy computed only on items where human annotators agreed (reduces evaluated set). Performance sensitive to prompt examples (lack of examples caused errors in 2023 relevance task).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT outperforms crowd workers for text-annotation tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6144.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6144.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intercoder_agreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intercoder agreement (consistency between annotators/runs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measure of consistency: percentage of instances for which two annotators (or two LLM runs) assign the identical label; used to assess reliability and stability of annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute percentage of instances where both annotators in a given group (trained humans, MTurk workers, or two independent LLM runs) assign the same class to an item.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Intercoder agreement = (number of instances with identical labels by both annotators) / (total instances). Reported per group and per temperature/run configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Computational social science / annotation reliability</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Intercoder agreement is applied to LLM outputs by obtaining multiple independent responses (multiple runs / temperature settings) and measuring label consistency across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Average intercoder agreement: MTurk 56%, trained annotators 79%, ChatGPT temperature=1 -> 91%, ChatGPT temperature=0.2 -> 97%. Correlation between intercoder agreement and accuracy: Pearson r = 0.36 (positive). Lower temperature increased consistency without decreasing accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Same four datasets (n=6,183). For ChatGPT, two responses collected per temperature (temperatures 1 and 0.2) yielding two-run agreement per temperature; total of four ChatGPT responses per tweet.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ChatGPT's intercoder agreement substantially exceeded that of both MTurk crowd-workers and trained annotators, indicating higher internal consistency of LLM annotations than human annotators in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High intercoder agreement does not guarantee correctness relative to domain ground truth; extreme determinism (low temperature) may increase agreement but could conceal systematic bias. Intercoder agreement for LLM was computed across independent runs rather than independent annotators, which may not perfectly mirror human annotation variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT outperforms crowd workers for text-annotation tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6144.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6144.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gold_standard_annotators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trained human annotators as gold standard</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three trained research assistants produced the benchmark labels; their annotations (cases with agreement) were used as the gold standard for evaluating LLM and crowd-worker accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use independently trained human coders following detailed codebooks to create gold-standard labels; evaluate automated labels by measuring agreement with this gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Gold standard defined as items where human coders agreed; accuracy measured relative to these labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT) evaluated against this gold standard</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Social science annotation / content-moderation discourse analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Gold-standard labels capture researchers' conceptual categories (relevance, stance, topics, frames) and serve as the reference for evaluating LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to compute LLM and MTurk accuracy; trained annotators' intercoder agreement reported at 79%. ChatGPT accuracy reported as agreement with this gold standard and found higher than MTurk for most tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Gold-standard applied to the same four datasets; codebooks and instruction texts provided in SI Appendix and used identically for humans, MTurk, and ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Gold standard is human; comparisons are LLM vs MTurk vs trained annotators. The trained annotators both create the benchmark and provide a comparator via their own intercoder agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Gold standard depends on the training, interpretation, and agreement of a small set of human coders and may not capture all valid label interpretations; some tasks had low human agreement, reducing gold-standard coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT outperforms crowd workers for text-annotation tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6144.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6144.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero_shot_and_temperature_protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot prompting and temperature-based stability testing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Protocol: supply the same codebook prompt in zero-shot format to ChatGPT for each item; vary temperature (1.0 and 0.2) and collect multiple independent runs to evaluate accuracy and consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Zero-shot prompts (no few-shot examples) using identical instruction wording as used for human annotators; two temperature settings (1.0 and 0.2); for each temperature collect two independent responses per item by starting a fresh chat session for each item and run.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Measure accuracy relative to gold standard and intercoder agreement across runs and temperatures to assess consistency and sensitivity to randomness parameter.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP evaluation / annotation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a theory but an experimental protocol testing zero-shot LLM annotation behavior and the effect of sampling randomness (temperature) on labeling consistency and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Lower temperature (0.2) produced higher intercoder agreement (97%) versus temperature=1 (91%) while not decreasing accuracy; indicates temperature tuning can increase deterministic consistency of annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied across the four datasets; four ChatGPT responses per tweet (two temps × two runs) used to measure internal consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Protocol contrasts multiple LLM independent runs with independent human annotators and with MTurk workers (two annotators per item) to compare consistency and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Zero-shot setting may be sensitive to prompt wording and lack of in-prompt examples; paper intentionally avoided ChatGPT-specific prompt engineering to keep comparability with MTurk, which may understate best-case LLM performance (e.g., with few-shot or chain-of-thought methods).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT outperforms crowd workers for text-annotation tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6144.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6144.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cost_per_annotation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-annotation cost benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Monetary comparison of per-annotation cost between ChatGPT and MTurk crowd-workers, used to assess cost-effectiveness of LLM-based annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute monetary cost per annotation for ChatGPT (API usage) and for MTurk (payments to crowd-workers) and compare cost ratios alongside accuracy/quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Cost per annotation in USD; cost-effectiveness judged by cost relative to annotation quality (accuracy/intercoder agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Research methodology / annotation economics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Economic benchmark assessing feasibility of using LLMs to produce large labeled datasets by comparing per-item costs with crowd-sourcing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT per-annotation cost ≈ $0.003 (0.3 cents) — about thirty times cheaper than MTurk per-annotation cost in this study, while delivering higher quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to the same annotation tasks and datasets described above; cost comparisons use observed API costs and MTurk payments under the study's protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ChatGPT is substantially cheaper than MTurk while achieving higher accuracy and intercoder agreement in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported ChatGPT cost depends on API pricing at the time and on prompt length; does not factor in human-in-the-loop verification costs or costs of prompt engineering/few-shot design; privacy or data-sharing constraints may change practical costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT outperforms crowd workers for text-annotation tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6144.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6144.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets_and_sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four datasets and sampling strategy (benchmark datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study used four distinct datasets (two content-moderation tweet samples, one 2023 tweets sample, one US Congress tweets sample, and one newspaper articles sample) to benchmark LLM vs human annotation across time and text types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluate LLM performance across multiple real-world datasets varying by domain (social media vs news), time (2020–21 vs 2023), and labelability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Performance measured across tasks and datasets to assess robustness and potential memorization (2023 sample included to test for memorization of training data).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Computational social science / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Datasets serve as testbeds for evaluating LLM outputs for annotation tasks (relevance, stance, topics, frames) rather than for evaluating scientific theories.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT outperformed MTurk on most tasks across datasets; weaker performance on 2023 sample relevance task for certain tweets (misclassification of tweets about user suspensions) highlighted prompt-example sensitivity rather than memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Datasets: (i) random sample 2,382 content-moderation tweets (from 2.6M); (ii) 1,856 US Congress tweets (from 20M); (iii) 1,606 newspaper articles (from 980k); (iv) 500 tweets (Jan 2023) sampled from 1.3M (339 English). Total n=6,183.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Datasets previously annotated by trained annotators (gold standard) for most samples; MTurk and ChatGPT annotations were directly compared on these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not all annotation tasks were applicable to all datasets; for some datasets human annotations for certain tasks were not successful. The 2023 dataset inclusion aimed to probe memorization but results point more to prompt quality than clear evidence of memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT outperforms crowd workers for text-annotation tasks', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Out of One, Many: Using Language Models to Simulate Human Samples <em>(Rating: 2)</em></li>
                <li>Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech <em>(Rating: 2)</em></li>
                <li>ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification <em>(Rating: 1)</em></li>
                <li>Semi-automated data labeling <em>(Rating: 2)</em></li>
                <li>What Determines Inter-Coder Agreement in Manual Annotations? A Meta-Analytic Investigation. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6144",
    "paper_id": "paper-a9e155fda1d97baa2b8712f580cc61887cc64e9b",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "Accuracy_vs_gold",
            "name_full": "Accuracy (agreement with trained annotators as gold standard)",
            "brief_description": "Evaluation method computing percentage of correct LLM annotations by comparing LLM labels to a gold standard produced by trained human annotators; accuracy computed only on texts where human annotators agreed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compare LLM-produced labels to a human-created gold standard (trained annotators); compute percentage of correctly classified instances over total (accuracy). Only cases where human annotators agreed are counted for accuracy.",
            "evaluation_criteria": "Accuracy = (number of LLM labels matching gold-standard labels) / (number of evaluated instances); reported per task (e.g., relevance, topic, stance, frames).",
            "llm_model_name": "gpt-3.5-turbo (ChatGPT)",
            "theory_domain": "Computational social science / political communication (annotation of tweets and news articles about content moderation)",
            "theory_description": "Not a scientific theory per se; LLM-generated categorical annotations (labels) for texts used as the object of evaluation (e.g., relevance, stance, topic, frame).",
            "evaluation_results": "Across four datasets (n=6,183), ChatGPT zero-shot accuracy exceeded MTurk by ~25 percentage points on average. Example accuracy rates for relevance tasks: 70% (content-moderation tweets), 81% (content-moderation news), 83% (US Congress tweets), 59% (2023 content-moderation tweets).",
            "benchmarks_or_datasets": "Four datasets: (i) 2,382 random content-moderation tweets (Jan 2020–Apr 2021); (ii) 1,856 US Congress tweets (2017–2022); (iii) 1,606 newspaper articles on content moderation (Jan 2020–Apr 2021); (iv) 500 tweets from Jan 2023 (339 English). Trained annotators' labels used as gold standard.",
            "comparison_to_human": "Direct comparison to MTurk crowd-workers (two crowd-workers per item) and to trained human annotators (gold standard). ChatGPT outperformed MTurk on most tasks in accuracy and matched/contributed to comparisons with trained annotators through the gold standard.",
            "limitations_or_challenges": "Accuracy is measured relative to human gold standard; paper does not evaluate LLM-generated scientific theories per se but LLM labeling performance. Accuracy computed only on items where human annotators agreed (reduces evaluated set). Performance sensitive to prompt examples (lack of examples caused errors in 2023 relevance task).",
            "uuid": "e6144.0",
            "source_info": {
                "paper_title": "ChatGPT outperforms crowd workers for text-annotation tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Intercoder_agreement",
            "name_full": "Intercoder agreement (consistency between annotators/runs)",
            "brief_description": "Measure of consistency: percentage of instances for which two annotators (or two LLM runs) assign the identical label; used to assess reliability and stability of annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compute percentage of instances where both annotators in a given group (trained humans, MTurk workers, or two independent LLM runs) assign the same class to an item.",
            "evaluation_criteria": "Intercoder agreement = (number of instances with identical labels by both annotators) / (total instances). Reported per group and per temperature/run configuration.",
            "llm_model_name": "gpt-3.5-turbo (ChatGPT)",
            "theory_domain": "Computational social science / annotation reliability",
            "theory_description": "Intercoder agreement is applied to LLM outputs by obtaining multiple independent responses (multiple runs / temperature settings) and measuring label consistency across runs.",
            "evaluation_results": "Average intercoder agreement: MTurk 56%, trained annotators 79%, ChatGPT temperature=1 -&gt; 91%, ChatGPT temperature=0.2 -&gt; 97%. Correlation between intercoder agreement and accuracy: Pearson r = 0.36 (positive). Lower temperature increased consistency without decreasing accuracy.",
            "benchmarks_or_datasets": "Same four datasets (n=6,183). For ChatGPT, two responses collected per temperature (temperatures 1 and 0.2) yielding two-run agreement per temperature; total of four ChatGPT responses per tweet.",
            "comparison_to_human": "ChatGPT's intercoder agreement substantially exceeded that of both MTurk crowd-workers and trained annotators, indicating higher internal consistency of LLM annotations than human annotators in these tasks.",
            "limitations_or_challenges": "High intercoder agreement does not guarantee correctness relative to domain ground truth; extreme determinism (low temperature) may increase agreement but could conceal systematic bias. Intercoder agreement for LLM was computed across independent runs rather than independent annotators, which may not perfectly mirror human annotation variability.",
            "uuid": "e6144.1",
            "source_info": {
                "paper_title": "ChatGPT outperforms crowd workers for text-annotation tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Gold_standard_annotators",
            "name_full": "Trained human annotators as gold standard",
            "brief_description": "Three trained research assistants produced the benchmark labels; their annotations (cases with agreement) were used as the gold standard for evaluating LLM and crowd-worker accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Use independently trained human coders following detailed codebooks to create gold-standard labels; evaluate automated labels by measuring agreement with this gold standard.",
            "evaluation_criteria": "Gold standard defined as items where human coders agreed; accuracy measured relative to these labels.",
            "llm_model_name": "gpt-3.5-turbo (ChatGPT) evaluated against this gold standard",
            "theory_domain": "Social science annotation / content-moderation discourse analysis",
            "theory_description": "Gold-standard labels capture researchers' conceptual categories (relevance, stance, topics, frames) and serve as the reference for evaluating LLM outputs.",
            "evaluation_results": "Used to compute LLM and MTurk accuracy; trained annotators' intercoder agreement reported at 79%. ChatGPT accuracy reported as agreement with this gold standard and found higher than MTurk for most tasks.",
            "benchmarks_or_datasets": "Gold-standard applied to the same four datasets; codebooks and instruction texts provided in SI Appendix and used identically for humans, MTurk, and ChatGPT.",
            "comparison_to_human": "Gold standard is human; comparisons are LLM vs MTurk vs trained annotators. The trained annotators both create the benchmark and provide a comparator via their own intercoder agreement.",
            "limitations_or_challenges": "Gold standard depends on the training, interpretation, and agreement of a small set of human coders and may not capture all valid label interpretations; some tasks had low human agreement, reducing gold-standard coverage.",
            "uuid": "e6144.2",
            "source_info": {
                "paper_title": "ChatGPT outperforms crowd workers for text-annotation tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Zero_shot_and_temperature_protocol",
            "name_full": "Zero-shot prompting and temperature-based stability testing",
            "brief_description": "Protocol: supply the same codebook prompt in zero-shot format to ChatGPT for each item; vary temperature (1.0 and 0.2) and collect multiple independent runs to evaluate accuracy and consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Zero-shot prompts (no few-shot examples) using identical instruction wording as used for human annotators; two temperature settings (1.0 and 0.2); for each temperature collect two independent responses per item by starting a fresh chat session for each item and run.",
            "evaluation_criteria": "Measure accuracy relative to gold standard and intercoder agreement across runs and temperatures to assess consistency and sensitivity to randomness parameter.",
            "llm_model_name": "gpt-3.5-turbo (ChatGPT)",
            "theory_domain": "NLP evaluation / annotation protocol",
            "theory_description": "Not a theory but an experimental protocol testing zero-shot LLM annotation behavior and the effect of sampling randomness (temperature) on labeling consistency and accuracy.",
            "evaluation_results": "Lower temperature (0.2) produced higher intercoder agreement (97%) versus temperature=1 (91%) while not decreasing accuracy; indicates temperature tuning can increase deterministic consistency of annotations.",
            "benchmarks_or_datasets": "Applied across the four datasets; four ChatGPT responses per tweet (two temps × two runs) used to measure internal consistency.",
            "comparison_to_human": "Protocol contrasts multiple LLM independent runs with independent human annotators and with MTurk workers (two annotators per item) to compare consistency and accuracy.",
            "limitations_or_challenges": "Zero-shot setting may be sensitive to prompt wording and lack of in-prompt examples; paper intentionally avoided ChatGPT-specific prompt engineering to keep comparability with MTurk, which may understate best-case LLM performance (e.g., with few-shot or chain-of-thought methods).",
            "uuid": "e6144.3",
            "source_info": {
                "paper_title": "ChatGPT outperforms crowd workers for text-annotation tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Cost_per_annotation",
            "name_full": "Per-annotation cost benchmark",
            "brief_description": "Monetary comparison of per-annotation cost between ChatGPT and MTurk crowd-workers, used to assess cost-effectiveness of LLM-based annotation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compute monetary cost per annotation for ChatGPT (API usage) and for MTurk (payments to crowd-workers) and compare cost ratios alongside accuracy/quality metrics.",
            "evaluation_criteria": "Cost per annotation in USD; cost-effectiveness judged by cost relative to annotation quality (accuracy/intercoder agreement).",
            "llm_model_name": "gpt-3.5-turbo (ChatGPT)",
            "theory_domain": "Research methodology / annotation economics",
            "theory_description": "Economic benchmark assessing feasibility of using LLMs to produce large labeled datasets by comparing per-item costs with crowd-sourcing.",
            "evaluation_results": "ChatGPT per-annotation cost ≈ $0.003 (0.3 cents) — about thirty times cheaper than MTurk per-annotation cost in this study, while delivering higher quality.",
            "benchmarks_or_datasets": "Applied to the same annotation tasks and datasets described above; cost comparisons use observed API costs and MTurk payments under the study's protocol.",
            "comparison_to_human": "ChatGPT is substantially cheaper than MTurk while achieving higher accuracy and intercoder agreement in these experiments.",
            "limitations_or_challenges": "Reported ChatGPT cost depends on API pricing at the time and on prompt length; does not factor in human-in-the-loop verification costs or costs of prompt engineering/few-shot design; privacy or data-sharing constraints may change practical costs.",
            "uuid": "e6144.4",
            "source_info": {
                "paper_title": "ChatGPT outperforms crowd workers for text-annotation tasks",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Datasets_and_sampling",
            "name_full": "Four datasets and sampling strategy (benchmark datasets)",
            "brief_description": "The study used four distinct datasets (two content-moderation tweet samples, one 2023 tweets sample, one US Congress tweets sample, and one newspaper articles sample) to benchmark LLM vs human annotation across time and text types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Evaluate LLM performance across multiple real-world datasets varying by domain (social media vs news), time (2020–21 vs 2023), and labelability.",
            "evaluation_criteria": "Performance measured across tasks and datasets to assess robustness and potential memorization (2023 sample included to test for memorization of training data).",
            "llm_model_name": "gpt-3.5-turbo (ChatGPT)",
            "theory_domain": "Computational social science / NLP evaluation",
            "theory_description": "Datasets serve as testbeds for evaluating LLM outputs for annotation tasks (relevance, stance, topics, frames) rather than for evaluating scientific theories.",
            "evaluation_results": "ChatGPT outperformed MTurk on most tasks across datasets; weaker performance on 2023 sample relevance task for certain tweets (misclassification of tweets about user suspensions) highlighted prompt-example sensitivity rather than memorization.",
            "benchmarks_or_datasets": "Datasets: (i) random sample 2,382 content-moderation tweets (from 2.6M); (ii) 1,856 US Congress tweets (from 20M); (iii) 1,606 newspaper articles (from 980k); (iv) 500 tweets (Jan 2023) sampled from 1.3M (339 English). Total n=6,183.",
            "comparison_to_human": "Datasets previously annotated by trained annotators (gold standard) for most samples; MTurk and ChatGPT annotations were directly compared on these datasets.",
            "limitations_or_challenges": "Not all annotation tasks were applicable to all datasets; for some datasets human annotations for certain tasks were not successful. The 2023 dataset inclusion aimed to probe memorization but results point more to prompt quality than clear evidence of memorization.",
            "uuid": "e6144.5",
            "source_info": {
                "paper_title": "ChatGPT outperforms crowd workers for text-annotation tasks",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Out of One, Many: Using Language Models to Simulate Human Samples",
            "rating": 2
        },
        {
            "paper_title": "Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech",
            "rating": 2
        },
        {
            "paper_title": "ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification",
            "rating": 1
        },
        {
            "paper_title": "Semi-automated data labeling",
            "rating": 2
        },
        {
            "paper_title": "What Determines Inter-Coder Agreement in Manual Annotations? A Meta-Analytic Investigation.",
            "rating": 2
        }
    ],
    "cost": 0.01096775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CHATGPT OUTPERFORMS CROWD-WORKERS FOR TEXT-ANNOTATION TASKS</h1>
<p>Fabrizio Gilardi*<br>University of Zurich Zurich, Switzerland</p>
<p>Meysam Alizadeh<br>University of Zurich<br>Zurich, Switzerland</p>
<p>Maël Kubli<br>University of Zurich<br>Zurich, Switzerland</p>
<p>Published in the Proceedings of the National Academy of Sciences
https://www.pnas.org/doi/10.1073/pnas. 2305016120</p>
<h4>Abstract</h4>
<p>Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles ( $\mathrm{n}=6,183$ ), we show that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers by about 25 percentage points on average, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $\$ 0.003$-about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.</p>
<h2>1 Introduction</h2>
<p>Many NLP applications require high-quality labeled data, notably to train classifiers or evaluate the performance of unsupervised models. For example, researchers often aim to filter noisy social media data for relevance, assign texts to different topics or conceptual categories, or measure their sentiment or stance. Regardless of the specific approach used for these tasks (supervised, semi-supervised, or unsupervised), labeled data are needed to build a training set or a gold standard against which performance can be assessed. Such data may be available for high-level tasks such as semantic evaluation (Emerson et al., 2022). More typically, however, researchers have to conduct original annotations to ensure that the labels match their conceptual categories (Benoit et al., 2016). Until recently, two main strategies were available. First, researchers can recruit and train coders, such as research assistants. Second, they can rely on crowd-workers on platforms</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>such as Amazon Mechanical Turk (MTurk). Often, these two strategies are used in combination: trained annotators create a relatively small gold-standard dataset, and crowd-workers are employed to increase the volume of labeled data. Trained annotators tend to produce high-quality data, but involve significant costs. Crowd workers are a much cheaper and more flexible option, but the quality may be insufficient, particularly for complex tasks and languages other than English. Moreover, there have been concerns that MTurk data quality has decreased (Chmielewski and Kucker, 2020), while alternative platforms such as CrowdFlower and FigureEight are no longer practicable options for academic research since they were acquired by Appen, a company that is focused on a business market.</p>
<p>This paper explores the potential of large language models (LLMs) for text annotation tasks, with a focus on ChatGPT, which was released in November 2022. It demonstrates that zero-shot ChatGPT classifications (that is, without any additional training) outperform MTurk annotations, at a fraction of the cost. LLMs have been shown to perform very well for a wide range of purposes, including ideological scaling (Wu et al., 2023), the classification of legislative proposals (Nay, 2023), the resolution of cognitive psychology tasks (Binz and Schulz, 2023), and the simulation of human samples for survey research (Argyle et al., 2023). While a few studies suggested that ChatGPT might perform text annotation tasks of the kinds we have described (Kuzman, Mozetič and Ljubešić, 2023; Huang, Kwak and An, 2023), to the best of our knowledge our work is the first systematic evaluation. Our analysis relies on a sample of 6,183 documents, including tweets and news articles that we collected for a previous study (Alizadeh et al., 2022) as well as a new sample of tweets posted in 2023. In our previous study, the texts were labeled by trained annotators (research assistants) for five different tasks: relevance, stance, topics, and two kinds of frame detection. Using the same codebooks that we developed to instruct our research assistants, we submitted the tasks to ChatGPT as zero-shot classifications, as well as to crowd-workers on MTurk. We then evaluated the performance of ChatGPT against two benchmarks: (i) its accuracy, relative to that of crowd-workers, and (ii) its intercoder agreement, relative to that of crowd workers as well as of our trained annotators. We find that across the four datasets, ChatGPT's zero-shot accuracy is higher than that of MTurk for most tasks. For all tasks, ChatGPT's intercoder agreement exceeds that of both MTurk and trained annotators. Moreover, ChatGPT is significantly cheaper than MTurk. ChatGPT's per-annotation cost is about $\$ 0.003$, or a third of a cent-about thirty times cheaper than MTurk, with higher quality. At this cost, it might potentially be possible to annotate entire samples, or to create large training sets for supervised learning. While further research is needed to better understand how ChatGPT and other LLMs perform in a broader range of contexts, these results demonstrate their potential to transform how researchers conduct data annotations, and to disrupt parts of the business model of platforms such as MTurk.</p>
<h1>2 Results</h1>
<p>We use four datasets $(n=6,183)$ including tweets and news articles that we collected and annotated manually for a previous study on the discourse around content moderation (Alizadeh et al., 2022), as well as a new sample of tweets posted in 2023 to address the concern that ChatGPT might be relying on memorization for texts potentially included in the model's training dataset. We relied on trained annotators (research assistants) to construct a gold standard for six conceptual categories: relevance of tweets for the content moderation issue (relevant/irrelevant); relevance of tweets for political issues (relevant/irrelevant); stance regarding Section 230, a key part of US internet legislation</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ChatGPT zero-shot text annotation performance in four datasets, compared to MTurk and trained annotators. ChatGPT's accuracy outperforms that of MTurk for most tasks. ChatGPT's intercoder agreement outperforms that of both MTurk and trained annotators in all tasks. Accuracy means agreement with the trained annotators.
(keep/repeal/neutral); topic identification (six classes); a first set of frames (content moderation as a problem, as a solution, or neutral); and a second set of frames (fourteen classes). We then performed these exact same classifications with ChatGPT and with crowd-workers recruited on MTurk, using the same codebook we developed for our research assistants (see SI Appendix). For ChatGPT, we conducted four sets of annotations. To explore the effect of ChatGPT's temperature parameter, which controls the degree of randomness of the output, we conducted the annotations with the default value of 1 as well as with a value of 0.2 , which implies less randomness. For each temperature value, we conducted two sets of annotations to compute ChatGPT's intercoder agreement. For MTurk, we aimed to select high-quality crowd-workers, notably by filtering for workers who are classified as "MTurk Masters" by Amazon, who have an approval rate of over $90 \%$, and who are located in the US. Our procedures are described more in detail in the Materials and Methods section.</p>
<p>Across the four datasets, we report ChatGPT's zero-shot performance for two different metrics: accuracy and intercoder agreement (Figure 1). Accuracy is measured as the percentage of correct annotations (using our trained annotators as a benchmark), while intercoder agreement is computed as the percentage of tweets that were assigned the same label by two different annotators (research assistant, crowd-workers, or ChatGPT runs). Regarding accuracy, Figure 1 shows that ChatGPT outperforms MTurk for most tasks across the four datasets. On average, ChatGPT's accuracy exceeds that of MTurk by about 25 percentage points. Moreover, ChatGPT demonstrates adequate accuracy overall, considering the challenging tasks, number of classes, and zero-shot annotations. Accuracy rates for relevance tasks, with two classes (relevant/irrelevant) are $70 \%$ for content</p>
<p>moderation tweets, $81 \%$ for content moderation news articles, $83 \%$ for US Congress tweets, and $59 \%$ for 2023 content moderation tweets. In the 2023 sample, ChatGPT performed much better than MTurk in the second task but struggled with misclassifying tweets about specific user suspensions in the relevance task due to a lack of examples in the prompt. While these findings do not suggest that memorization is a major issue, they underscore the importance of high-quality prompts.
Regarding intercoder agreement, Figure 1 shows that ChatGPT's performance is very high. On average, intercoder agreement is about $56 \%$ for MTurk, $79 \%$ for trained annotators, $91 \%$ for ChatGPT with temperature $=1$, and $97 \%$ for ChatGPT with temperature $=0.2$. The correlation between intercoder agreement and accuracy is positive (Pearson's $\mathrm{r}=0.36$ ). This suggests that a lower temperature value may be preferable for annotation tasks, as it seems to increase consistency without decreasing accuracy.
We underscore that the test to which we subjected ChatGPT is hard. Our tasks were originally conducted in the context of a previous study (Alizadeh et al., 2022), and required considerable resources. We developed most of the conceptual categories for our particular research purposes. Moreover, some of the tasks involve a large number of classes and exhibit lower levels of intercoder agreement, which indicates a higher degree of annotation difficulty (Bayerl and Paul, 2011). ChatGPT's accuracy is positively correlated with the intercoder agreement of trained annotators (Pearson's $\mathrm{r}=0.46$ ), suggesting better performance for easier tasks. Conversely, ChatGPT's outperformance of MTurk is negatively correlated with the intercoder agreement of trained annotators (Pearson's $\mathrm{r}=-0.37$ ), potentially indicating stronger overperformance for more complex tasks.
We conclude that ChatGPT's performance is impressive, particularly considering that its annotations are zero-shot.</p>
<h1>3 Discussion</h1>
<p>This paper demonstrates the potential of LLMs to transform text-annotation procedures for a variety of tasks common to many research projects. The evidence is consistent across different types of texts and time periods. It strongly suggests that ChatGPT may already be a superior approach compared to crowd-annotations on platforms such as MTurk. At the very least, the findings demonstrate the importance of studying the text-annotation properties and capabilities of LLMs more in depth. The following questions seem particularly promising: (i) performance across multiple languages; (ii) implementation of few-shot learning; (iii) construction of semi-automated data labeling systems in which a model learns from human annotations and then recommends labeling procedures (Desmond et al., 2021); (iv) using chain of thought prompting and other strategies to increase the performance of zero-shot reasoning (Kojima et al., 2022); and (v) comparison across different types of LLMs.</p>
<h2>4 Materials and Methods</h2>
<h3>4.1 Datasets</h3>
<p>The analysis relies on four datasets: (i) a random sample of 2,382 tweets drawn from a dataset of 2.6 million tweets on content moderation posted from January 2020 to April 2021; (ii) a random sample of 1,856 tweets posted by members of the US Congress from 2017 to 2022, drawn from a dataset of 20 million tweets; (iii) a random sample of 1,606 articles newspaper articles on content moderation</p>
<p>published from January 2020 to April 2021, drawn from a dataset of 980k articles collected via LexisNexis. Sample size was determined by the number of texts needed to build a training set for a machine learning classifier. The fourth dataset (iv) replicated the data collection for (i), but for January 2023. It includes a random sample of 500 tweets (of which 339 were in English) drawn from a dataset of 1.3 million tweets.</p>
<h1>4.2 Annotation Tasks</h1>
<p>We implemented several annotation tasks: (1) relevance: whether a tweet is about content moderation or, in a separate task, about politics; (2) topic detection: whether a tweet is about a set of six pre-defined topics (i.e. Section 230, Trump Ban, Complaint, Platform Policies, Twitter Support, and others); (3) stance detection: whether a tweet is in favor of, against, or neutral about repealing Section 230 (a piece of US legislation central to content moderation); (4) general frame detection: whether a tweet contains a set of two opposing frames ("problem' and "solution"). The solution frame describes tweets framing content moderation as a solution to other issues (e.g., hate speech). The problem frame describes tweets framing content moderation as a problem on its own as well as to other issues (e.g., free speech); (5) policy frame detection: whether a tweet contains a set of fourteen policy frames proposed in (Card et al., 2015). The full text of instruction for the five annotation tasks is presented in SI Appendix. We used the exact same wordings for ChatGPT and MTurk.</p>
<h3>4.3 Trained annotators</h3>
<p>We trained three political science students to conduct the annotation tasks. For each task, they were given the same set of instructions described above and detailed in SI Appendix. The coders annotated the tweets independently task by task.</p>
<h3>4.4 Crowd-workers</h3>
<p>We employed MTurk workers to perform the same set of tasks as trained annotators and ChatGPT, using the same set of instructions (SI Appendix). To ensure annotation quality, we restricted access to the tasks to workers who are classified as "MTurk Masters" by Amazon, who have a HIT (Human Intelligence Task) approval rate greater than $90 \%$ with at least 50 approved HITs, and who are located in the US. Moreover, we ensured that no worker could annotate more than $20 \%$ of the tweets for a given task. As with the trained human annotators, each tweet was annotated by two different crowd-workers.</p>
<h3>4.5 ChatGPT</h3>
<p>We used the ChatGPT API with the 'gpt-3.5-turbo'. The annotations were conducted between March 9-20 and April 27-May 4, 2023. For each task, we prompted ChatGPT with the corresponding annotation instruction text (SI Appendix). We intentionally avoided adding any ChatGPT-specific prompts to ensure comparability between ChatGPT and MTurk crowd-workers. After testing several variations, we decided to feed tweets one by one to ChatGPT using the following prompt: "Here's the tweet I picked, please label it as [Task Specific Instruction (e.g. 'one of the topics in the instruction')]." We set the temperature parameter at 1 (default value) and 0.2 (which makes the</p>
<p>output more deterministic; higher values make the output more random). For each temperature setting, we collected two responses from ChatGPT to compute the intercoder agreement. That is, we collected four ChatGPT responses for each tweet. We created a new chat session for every tweet to ensure that the ChatGPT results are not influenced by the history of annotations.</p>
<h1>4.6 Evaluation Metrics</h1>
<p>First, we computed average accuracy (i.e. percentage of correct predictions), that is, the number of correctly classified instances over the total number of cases to be classified, using trained human annotations as our gold standard and considering only texts that both annotators agreed upon. Second, intercoder agreement refers to the percentage of instances for which both annotators in a given group report the same class.</p>
<h3>4.7 Data Availability</h3>
<p>Replication materials are available at the Harvard Dataverse, https://doi.org/10.7910/DVN/ PQYF6M.</p>
<h2>Acknowledgments</h2>
<p>This project received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement nr. 883121). We thank Fabio Melliger, Paula Moser, and Sophie van IJzendoorn for excellent research assistance.</p>
<h2>References</h2>
<p>Alizadeh, Meysam, Fabrizio Gilardi, Emma Hoes, K Jonathan Klüser, Mael Kubli and Nahema Marchal. 2022. "Content Moderation As a Political Issue: The Twitter Discourse Around Trump’s Ban." Journal of Quantitative Description: Digital Media 2.
Argyle, Lisa P., Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting and David Wingate. 2023. "Out of One, Many: Using Language Models to Simulate Human Samples." Political Analysis pp. 1-15.
Bayerl, Petra Saskia and Karsten Ingmar Paul. 2011. "What Determines Inter-Coder Agreement in Manual Annotations? A Meta-Analytic Investigation." Computational Linguistics 37(4):699-725.
Benoit, Kenneth, Drew Conway, Benjamin E. Lauderdale, Michael Laver and Slava Mikhaylov. 2016. "Crowd-Sourced Text Analysis: Reproducible and Agile Production of Political Data." American Political Science Review 116(2):278-295.
Binz, Marcel and Eric Schulz. 2023. "Using cognitive psychology to understand GPT-3." Proceedings of the National Academy of Sciences 120(6):e2218523120.
Card, Dallas, Amber Boydstun, Justin H Gross, Philip Resnik and Noah A Smith. 2015. The media frames corpus: Annotations of frames across issues. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). pp. 438-444.
Chmielewski, Michael and Sarah C. Kucker. 2020. "An MTurk Crisis? Shifts in Data Quality and the Impact on Study Results." Social Psychological and Personality Science 11(4):464-473.</p>
<p>Desmond, Michael, Evelyn Duesterwald, Kristina Brimijoin, Michelle Brachman and Qian Pan. 2021. Semi-automated data labeling. In NeurIPS 2020 Competition and Demonstration Track. PMLR pp. 156-169.
Emerson, Guy, Natalie Schluter, Gabriel Stanovsky, Ritesh Kumar, Alexis Palmer, Nathan Schneider, Siddharth Singh and Shyam Ratan, eds. 2022. Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022). Seattle: Association for Computational Linguistics.
Huang, Fan, Haewoon Kwak and Jisun An. 2023. "Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech." arXiv preprint arXiv:2302.07736 .
Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo and Yusuke Iwasawa. 2022. "Large language models are zero-shot reasoners." arXiv preprint arXiv:2205.11916 .
Kuzman, Taja, Igor Mozetič and Nikola Ljubešić. 2023. "ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification." arXiv e-prints pp. arXiv-2303.
Nay, John J. 2023. "Large Language Models as Corporate Lobbyists.".
Wu, Patrick Y., Joshua A. Tucker, Jonathan Nagler and Solomon Messing. 2023. "Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting.".</p>
<h1>S1 Annotation Codebooks</h1>
<p>Not all of the annotations described in these codebooks were conducted for every dataset in our study. First, the manual annotations we use as a benchmark were performed in a previous study, except for the new 2023 sample, which was specifically annotated for this current study. Second, certain annotation tasks are not applicable to all datasets. For instance, stance analysis, problem/solution, and topic modeling were not suitable for analyzing tweets from US Congress members. This is because these tweets cover a wide range of issues and topics, unlike content moderation topics, which are more focused. For news articles, our attempts at human annotation for stance, topic, and policy frames were not successful. This was because the articles primarily revolved around platform policies, actions, and criticisms thereof.</p>
<h2>S1.1 Background on content moderation (to be used for all tasks except the tweets from US Congressmembers)</h2>
<p>For this task, you will be asked to annotate a sample of tweets about content moderation. Before describing the task, we explain what we mean by "content moderation".
"Content moderation" refers to the practice of screening and monitoring content posted by users on social media sites to determine if the content should be published or not, based on specific rules and guidelines. Every time someone posts something on a platform like Facebook or Twitter, that piece of content goes through a review process ("content moderation") to ensure that it is not illegal, hateful or inappropriate and that it complies with the rules of the site. When that is not the case, that piece of content can be removed, flagged, labeled as or 'disputed.'</p>
<p>Deciding what should be allowed on social media is not always easy. For example, many sites ban child pornography and terrorist content as it is illegal. However, things are less clear when it comes to content about the safety of vaccines or politics, for example. Even when people agree that some content should be blocked, they do not always agree about the best way to do so, how effective it is, and who should do it (the government or private companies, human moderators, or artificial intelligence).</p>
<h2>S1.2 Background on political tweets (to be used for tweets by the US Congress members)</h2>
<p>For this task, you will be asked to annotate a sample of tweets to determine if they include political content or not. For the purposes of this task, tweets are "relevant" if they include political content, and "irrelevant" if they do not. Before describing the task, we explain what we mean by "political content".
"Political content" refers to any tweets that pertain to politics or government policies at the local, national, or international level. This can include tweets that discuss political figures, events, or issues, as well as tweets that use political language or hashtags. To determine if tweets include political content or not, consider several factors, such as the use of political keywords or hashtags, the mention of political figures or events, the inclusion of links to news articles or other political sources, and the overall tone and sentiment of the tweet, which may indicate whether it is conveying a political message or viewpoint.</p>
<h1>S1.3 Task 1: Relevance (Content Moderation)</h1>
<p>For each tweet in the sample, follow these instructions:</p>
<ol>
<li>Carefully read the text of the tweet, paying close attention to details.</li>
<li>Classify the tweet as either relevant (1) or irrelevant (0)</li>
</ol>
<p>Tweets should be coded as RELEVANT when they directly relate to content moderation, as defined above. This includes tweets that discuss: social media platforms' content moderation rules and practices, governments' regulation of online content moderation, and/or mild forms of content moderation like flagging.
Tweets should be coded as IRRELEVANT if they do not refer to content moderation, as defined above, or if they are themselves examples of moderated content. This would include, for example, a Tweet by Donald Trump that Twitter has labeled as "disputed", a tweet claiming that something is false, or a tweet containing sensitive content. Such tweets might be subject to content moderation, but are not discussing content moderation. Therefore, they should be coded as irrelevant for our purposes.</p>
<h2>S1.4 Task 2: Relevance (Political Content)</h2>
<p>For each tweet in the sample, follow these instructions:</p>
<ol>
<li>Carefully read the text of the tweet, paying close attention to details.</li>
<li>Classify the tweet as either relevant (1) or irrelevant (0)</li>
</ol>
<p>Tweets should be coded as RELEVANT if they include POLITICAL CONTENT, as defined above. Tweets should be coded as IRRELEVANT if they do NOT include POLITICAL CONTENT, as defined above.</p>
<h2>S1.5 Task 3: Problem/Solution Frames</h2>
<p>Content moderation can be seen from two different perspectives:</p>
<ul>
<li>Content moderation can be seen as a PROBLEM; for example, as a restriction of free speech</li>
<li>Content moderation can be seen as a SOLUTION; for example, as a protection from harmful speech</li>
</ul>
<p>For each tweet in the sample, follow these instructions:</p>
<ol>
<li>Carefully read the text of the tweet, paying close attention to details.</li>
<li>Classify the tweet as describing content moderation as a problem, as a solution, or neither.</li>
</ol>
<p>Tweets should be classified as describing content moderation as a PROBLEM if they emphasize negative effects of content moderation, such as restrictions to free speech, or the biases that can emerge from decisions regarding what users are allowed to post.
Tweets should be classified as describing content moderation as a SOLUTION if they emphasize positive effects of content moderation, such as protecting users from various kinds of harmful content, including hate speech, misinformation, illegal adult content, or spam.</p>
<p>Tweets should be classified as describing content moderation as NEUTRAL if they do not emphasize possible negative or positive effects of content moderation, for example if they simply report on the content moderation activity of social media platforms without linking them to potential advantages or disadvantages for users or stakeholders.</p>
<h1>S1.6 Task 4: Policy Frames (Content Moderation)</h1>
<p>Content moderation, as described above, can be linked to various other topics, such as health, crime, or equality.</p>
<p>For each tweet in the sample, follow these instructions:</p>
<ol>
<li>Carefully read the text of the tweet, paying close attention to details.</li>
<li>Classify the tweet into one of the topics defined below.</li>
</ol>
<p>The topics are defined as follows:</p>
<ul>
<li>ECONOMY: The costs, benefits, or monetary/financial implications of the issue (to an individual, family, community, or to the economy as a whole).</li>
<li>Capacity and resources: The lack of or availability of physical, geographical, spatial, human, and financial resources, or the capacity of existing systems and resources to implement or carry out policy goals.</li>
<li>MORALITY: Any perspective-or policy objective or action (including proposed action)that is compelled by religious doctrine or interpretation, duty, honor, righteousness or any other sense of ethics or social responsibility.</li>
<li>FAIRNESS AND EQUALITY: Equality or inequality with which laws, punishment, rewards, and resources are applied or distributed among individuals or groups. Also the balance between the rights or interests of one individual or group compared to another individual or group.</li>
<li>CONSTITUTIONALITY AND JURISPRUDENCE: The constraints imposed on or freedoms granted to individuals, government, and corporations via the Constitution, Bill of Rights and other amendments, or judicial interpretation. This deals specifically with the authority of government to regulate, and the authority of individuals/corporations to act independently of government.</li>
<li>POLICY PRESCRIPTION AND EVALUATION: Particular policies proposed for addressing an identified problem, and figuring out if certain policies will work, or if existing policies are effective.</li>
<li>LAW AND ORDER, CRIME AND JUSTICE: Specific policies in practice and their enforcement, incentives, and implications. Includes stories about enforcement and interpretation of laws by individuals and law enforcement, breaking laws, loopholes, fines, sentencing and punishment. Increases or reductions in crime.</li>
<li>
<p>SECURITY AND DEFENSE: Security, threats to security, and protection of one's person, family, in-group, nation, etc. Generally an action or a call to action that can be taken to protect the welfare of a person, group, nation sometimes from a not yet manifested threat.</p>
</li>
<li>
<p>HEALTH AND SAFETY: Health care access and effectiveness, illness, disease, sanitation, obesity, mental health effects, prevention of or perpetuation of gun violence, infrastructure and building safety.</p>
</li>
<li>QUALITY OF LIFE: The effects of a policy on individuals' wealth, mobility, access to resources, happiness, social structures, ease of day-to-day routines, quality of community life, etc.</li>
<li>CULTURAL IDENTITY: The social norms, trends, values and customs constituting culture(s), as they relate to a specific policy issue.</li>
<li>PUBLIC OPINION: References to general social attitudes, polling and demographic information, as well as implied or actual consequences of diverging from or "getting ahead of" public opinion or polls.</li>
<li>POLITICAL: Any political considerations surrounding an issue. Issue actions or efforts or stances that are political, such as partisan filibusters, lobbyist involvement, bipartisan efforts, deal-making and vote trading, appealing to one's base, mentions of political maneuvering. Explicit statements that a policy issue is good or bad for a particular political party.</li>
<li>EXTERNAL REGULATION AND REPUTATION: The United States' external relations with another nation; the external relations of one state with another; or relations between groups. This includes trade agreements and outcomes, comparisons of policy outcomes or desired policy outcomes.</li>
<li>OTHER: Any topic that does not fit into the above categories.</li>
</ul>
<h1>S1.7 Task 5: Policy Frames (Political Content)</h1>
<p>Political content, as described above, can be linked to various other topics, such as health, crime, or equality.</p>
<p>For each tweet in the sample, follow these instructions:</p>
<ol>
<li>Carefully read the text of the tweet, paying close attention to details.</li>
<li>Classify the tweet into one of the topics defined below.</li>
</ol>
<p>The topics are defined as follows:</p>
<ul>
<li>ECONOMY: The costs, benefits, or monetary/financial implications of the issue (to an individual, family, community, or to the economy as a whole).</li>
<li>Capacity and resources: The lack of or availability of physical, geographical, spatial, human, and financial resources, or the capacity of existing systems and resources to implement or carry out policy goals.</li>
<li>MORALITY: Any perspective-or policy objective or action (including proposed action)that is compelled by religious doctrine or interpretation, duty, honor, righteousness or any other sense of ethics or social responsibility.</li>
<li>
<p>FAIRNESS AND EQUALITY: Equality or inequality with which laws, punishment, rewards, and resources are applied or distributed among individuals or groups. Also the balance between the rights or interests of one individual or group compared to another individual or group.</p>
</li>
<li>
<p>CONSTITUTIONALITY AND JURISPRUDENCE: The constraints imposed on or freedoms granted to individuals, government, and corporations via the Constitution, Bill of Rights and other amendments, or judicial interpretation. This deals specifically with the authority of government to regulate, and the authority of individuals/corporations to act independently of government.</p>
</li>
<li>POLICY PRESCRIPTION AND EVALUATION: Particular policies proposed for addressing an identified problem, and figuring out if certain policies will work, or if existing policies are effective.</li>
<li>LAW AND ORDER, CRIME AND JUSTICE: Specific policies in practice and their enforcement, incentives, and implications. Includes stories about enforcement and interpretation of laws by individuals and law enforcement, breaking laws, loopholes, fines, sentencing and punishment. Increases or reductions in crime.</li>
<li>SECURITY AND DEFENSE: Security, threats to security, and protection of one's person, family, in-group, nation, etc. Generally an action or a call to action that can be taken to protect the welfare of a person, group, nation sometimes from a not yet manifested threat.</li>
<li>HEALTH AND SAFETY: Health care access and effectiveness, illness, disease, sanitation, obesity, mental health effects, prevention of or perpetuation of gun violence, infrastructure and building safety.</li>
<li>QUALITY OF LIFE: The effects of a policy on individuals' wealth, mobility, access to resources, happiness, social structures, ease of day-to-day routines, quality of community life, etc.</li>
<li>CULTURAL IDENTITY: The social norms, trends, values and customs constituting culture(s), as they relate to a specific policy issue.</li>
<li>PUBLIC OPINION: References to general social attitudes, polling and demographic information, as well as implied or actual consequences of diverging from or "getting ahead of" public opinion or polls.</li>
<li>POLITICAL: Any political considerations surrounding an issue. Issue actions or efforts or stances that are political, such as partisan filibusters, lobbyist involvement, bipartisan efforts, deal-making and vote trading, appealing to one's base, mentions of political maneuvering. Explicit statements that a policy issue is good or bad for a particular political party.</li>
<li>EXTERNAL REGULATION AND REPUTATION: The United States' external relations with another nation; the external relations of one state with another; or relations between groups. This includes trade agreements and outcomes, comparisons of policy outcomes or desired policy outcomes.</li>
<li>OTHER: Any topic that does not fit into the above categories.</li>
</ul>
<h1>S1.8 Task 6: Stance Detection</h1>
<p>In the context of content moderation, Section 230 is a law in the United States that protects websites and other online platforms from being held legally responsible for the content posted by their users. This means that if someone posts something illegal or harmful on a website, the website itself cannot be sued for allowing it to be posted. However, websites can still choose to moderate content and remove anything that violates their own policies.</p>
<p>For each tweet in the sample, follow these instructions:</p>
<ol>
<li>Carefully read the text of the tweet, paying close attention to details.</li>
<li>Classify the tweet as having a positive stance towards Section 230, a negative stance, or a neutral stance.</li>
</ol>
<h1>S1.9 Task 7: Topic Detection</h1>
<p>Tweets about content moderation may also discuss other related topics, such as:</p>
<ol>
<li>Section 230, which is a law in the United States that protects websites and other online platforms from being held legally responsible for the content posted by their users (SECTION 230).</li>
<li>The decision by many social media platforms, such as Twitter and Facebook, to suspend Donald Trump's account (TRUMP BAN).</li>
<li>Requests directed to Twitter's support account or help center (TWITTER SUPPORT).</li>
<li>Social media platforms' policies and practices, such as community guidelines or terms of service (PLATFORM POLICIES).</li>
<li>Complaints about platform's policy and practices in deplatforming and content moderation or suggestions to suspend particular accounts, or complaints about accounts being suspended or reported (COMPLAINTS).</li>
<li>If a text is not about the SECTION 230, COMPLAINTS, TRUMP BAN, TWITTER SUPPORT, and PLATFORM POLICIES, then it should be classified in OTHER class (OTHER).</li>
</ol>
<p>For each tweet in the sample, follow these instructions:</p>
<ol>
<li>Carefully read the text of the tweet, paying close attention to details.</li>
<li>Please classify the following text according to topic (defined by function of the text, author's purpose and form of the text). You can choose from the following classes: SECTION 230, TRUMP BAN, COMPLAINTS, TWITTER SUPPORT, PLATFORM POLICIES, and OTHER</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author (https://fabriziogilardi.org/).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9191 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9191</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9191</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-276465071</p>
                <p><strong>Paper Title:</strong> Benchmarking of Large Language Models for the Dental Admission Test</p>
                <p><strong>Paper Abstract:</strong> Background: Large language models (LLMs) have shown promise in educational applications, but their performance on high-stakes admissions tests, such as the Dental Admission Test (DAT), remains unclear. Understanding the capabilities and limitations of these models is critical for determining their suitability in test preparation. Methods: This study evaluated the ability of 16 LLMs, including general-purpose models (e.g., GPT-3.5, GPT-4, GPT-4o, GPT-o1, Googleâ€™s Bard, mistral-large, and Claude), domain-specific fine-tuned models (e.g., DentalGPT, MedGPT, and BioGPT), and open-source models (e.g., Llama2-7B, Llama2-13B, Llama2-70B, Llama3-8B, and Llama3-70B), to answer questions from a sample DAT. Quantitative analysis was performed to assess model accuracy in different sections, and qualitative thematic analysis by subject matter experts examined specific challenges encountered by the models. Results: GPT-4o and GPT-o1 outperformed others in text-based questions assessing knowledge and comprehension, with GPT-o1 achieving perfect scores in the natural sciences (NS) and reading comprehension (RC) sections. Open-source models such as Llama3-70B also performed competitively in RC tasks. However, all models, including GPT-4o, struggled substantially with perceptual ability (PA) items, highlighting a persistent limitation in handling image-based tasks requiring visual-spatial reasoning. Fine-tuned medical models (e.g., DentalGPT, MedGPT, and BioGPT) demonstrated moderate success in text-based tasks but underperformed in areas requiring critical thinking and reasoning. Thematic analysis identified key challenges, including difficulties with stepwise problem-solving, transferring knowledge, comprehending intricate questions, and hallucinations, particularly on advanced items. Conclusions: While LLMs show potential for reinforcing factual knowledge and supporting learners, their limitations in handling higher-order cognitive tasks and image-based reasoning underscore the need for judicious integration with instructor-led guidance and targeted practice. This study provides valuable insights into the capabilities and limitations of current LLMs in preparing prospective dental students and highlights pathways for future innovations to improve performance across all cognitive skills assessed by the DAT.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9191.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9191.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial general-purpose large language model evaluated in this study; used to answer DAT text-based questions and (for some experiments) image-based questions via screenshots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial general-purpose LLM (OpenAI family) referenced among general-purpose models in the paper; specific parameter count or training corpora not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences (biology, general chemistry, organic chemistry); Reading comprehension; Quantitative reasoning (educational assessment domain)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulating a test-taker by answering text-based DAT questions (NS, RC, QR sections) and providing short explanations; for some items screenshots of image-based questions were provided to multimodal-capable variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) on section-specific multiple-choice DAT items; qualitative thematic evaluation of explanations by subject-matter experts (SMEs) with inter-rater Kappa reported for theme assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: NS 100%, RC 100%, QR 95% (percentage correct on DAT sample test text-based items).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Presence of higher-order reasoning requirements (critical thinking, stepwise multi-step problems), complexity/intricacy of questions, tendency to hallucinate on complex items, inability to consistently apply elimination strategies on QR/PA-type problems, multimodal/image-processing capability for image-based items.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to other LLMs in the study (GPT-o1, GPT-4, GPT-3.5, Bard, Claude, fine-tuned medical models, open-source Llama models); outperformed most models on text-based sections (highest reported text-based accuracy). No human-test-taker numerical baseline reported; SMEs used for qualitative assessment of explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite near-perfect text-based performance, struggled on image-based visual-spatial (PA) items (PA accuracy 24% reported elsewhere in the paper) and on items requiring stepwise reasoning, deeper critical thinking, or transfer; hallucinations and occasional incorrect final answer despite correct intermediate steps were observed.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend targeted fine-tuning, improved multimodal integration for image-based reasoning, methods to mitigate hallucinations (e.g., retrieval-augmentation, biomedical knowledge graphs), and combining LLMs with instructor guidance for higher-order skill development.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9191.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-o1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-o1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial general-purpose LLM evaluated as a simulated DAT test-taker; showed top performance on text-based DAT sections in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-o1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial general-purpose LLM (OpenAI family referenced alongside GPT-4 variants); paper does not provide parameter counts or training-data specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning (educational assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering text-based DAT multiple-choice questions (NS, RC, QR) with short justifications, simulating student responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) by DAT section; qualitative SME thematic coding of explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: NS 100%, RC 100%, QR 95% (percentage correct on DAT sample test text-based items).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Same as other models: question cognitive level (higher-order reasoning reduces accuracy), stepwise/multi-step problems, prompt framing, and modality (image vs text); limited image-processing when screenshots were provided produced lower PA accuracy (PA 21% reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against other commercial, fine-tuned, and open-source LLMs in this work; matched or exceeded most models on text-based sections; no traditional simulator or human score baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low performance on PA/image-based items, struggles with stepwise and critical-thinking questions, occasional hallucinations and incorrect final answer reporting despite correct intermediate calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Recommend domain-targeted fine-tuning, improved multimodal capabilities, integration with retrieval (RAG) and knowledge graphs to reduce hallucinations and improve reasoning on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9191.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing commercial LLM included in the benchmark; demonstrated strong text-based performance on DAT sections although exact per-section numbers beyond qualitative description are not all enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial general-purpose LLM (OpenAI); the paper cites it among top-performing general-purpose models but does not report model size or training set specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning (educational assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answer DAT text-based multiple-choice questions and provide short explanations, used to assess capabilities for test-prep simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) on DAT sections; SME qualitative thematic analysis of generated explanations (Kappa reported for theme assignment).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported qualitatively as strong performance on NS and RC with accuracy generally exceeding 80% for text-based sections (exact per-section percentages not specified beyond 'generally exceeding 80%').</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Question cognitive complexity (higher-order tasks reduce accuracy), need for stepwise reasoning, potential training-data overlap with test items (not fully excluded), and multimodal/image capability limitations for image-based items.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to other LLMs in the study (GPT-4o, GPT-o1, GPT-3.5, Bard, Claude, fine-tuned and open-source models); performed well but specific numeric comparisons are provided elsewhere for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Underperforms on image-based PA items (models capable of images nonetheless had low PA accuracy) and on problems requiring multi-step logical reasoning; hallucinations and mis-interpretation of intricate questions noted.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest improving multimodal reasoning, targeted domain fine-tuning, and RAG/BKG integration to address hallucinations and reasoning limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9191.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used general-purpose LLM included in the benchmark; demonstrated generally strong text-based performance but lower than the top-performing GPT-4o and GPT-o1 in some respects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial general-purpose LLM (OpenAI family) referenced among tested models; the paper does not provide parameter counts or detailed training corpus information.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulated DAT taker answering text-based multiple-choice DAT items and providing short explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) on DAT sections; SME thematic qualitative evaluation of explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported qualitatively as strong on NS and RC, with accuracy generally exceeding 80% on text-based sections (no precise per-section percentages provided in the paper for GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Affected by higher-order cognitive task requirements (critical thinking, stepwise reasoning), prompt framing, and absence of multimodal image processing for many items.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across the set of 16 LLMs; trailed top performers (GPT-4o/GPT-o1) on some text-based metrics but remained a strong generalist.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same as other LLMs: problems with multi-step reasoning, intricate questions, and image-based PA items (couldn't process images directly in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Targeted fine-tuning and multimodal capability improvements recommended; use with instructor guidance for higher-order skill development.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9191.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google Bard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial conversational LLM evaluated on the DAT sample; performed strongly on text-based sections but poorly on perceptual/image-based items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's conversational general-purpose LLM referenced among tested models; the paper does not supply size or training-data specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering text-based DAT multiple-choice items and providing brief explanations; when applicable, processed screenshots of image-based items for multimodal-capable variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) on DAT sections; SME qualitative thematic coding.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: reported generally >80% accuracy on NS and RC; Image-based PA accuracy reported at 19% (poor performance on visual-spatial tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Cognitive demand of questions (critical thinking, multi-step reasoning), multimodal capability limits, inconsistency in applying elimination strategies across sections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to other commercial and open-source LLMs; performed well on text but markedly worse on PA than top text-performing models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low PA/image accuracy, inconsistent elimination strategy application, struggles with stepwise and intricate questions, occasional hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Improving multimodal reasoning, targeted fine-tuning, and grounding via RAG/BKGs recommended to address shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9191.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial general-purpose LLM evaluated for DAT question-answering; demonstrated strong text-based performance but limited image-based/perceptual ability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial LLM (Anthropic) included in the benchmark; paper does not report parameter count or training-data details.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answer DAT text-based multiple-choice questions and provide short explanations; some image-based items processed where supported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) by DAT section; SME qualitative thematic analysis of explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: generally >80% accuracy on NS and RC; Image-based PA accuracy reported at 36% (still low relative to text performance).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Question cognitive level (higher-order reasoning reduces accuracy), need for stepwise solution chains, modality (image vs text), and domain-specific content coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against other LLMs in the study; stronger on text-based tasks than many fine-tuned/open-source models, but inferior on PA relative to some.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low image-based PA accuracy, issues with stepwise problems and intricate reasoning, hallucinations and occasional incorrect reasoning noted.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors urge multimodal advances, fine-tuning for domain-specific content, and integration of retrieval and knowledge-graph techniques to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9191.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DentalGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DentalGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific fine-tuned model intended for dental/medical tasks; evaluated on DAT items and showed moderate success on text-based natural science questions but poor QR performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DentalGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-specific fine-tuned LLM for dental/medical content (citation in paper); parameter counts/training data specifics not provided in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education / medical knowledge (natural sciences within DAT: biology, general chemistry, organic chemistry) and some image-based processing where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answer DAT text-based multiple-choice items (NS, RC, QR) and image-based items when screenshots were provided; provide short explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) by DAT section; SME qualitative thematic evaluation of justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: NS 79% accuracy; QR 24% (poor on quantitative reasoning); RC performance reported as weaker (exact RC % not enumerated for DentalGPT in the paper summary).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Domain fine-tuning on general medical datasets (possible mismatch with DAT-specific content), limited capability on higher-order reasoning and stepwise QR problems, image-processing when applicable but still weak.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to general-purpose commercial models and open-source Llama models; performed moderately on NS but substantially worse on QR compared to GPT-4o/GPT-o1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Underperformed on QR and on tasks requiring critical thinking, stepwise analysis, and transfer; moderate image-based performance (PA ~28% reported) and hallucinations in complex scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note need for more targeted fine-tuning using DAT-specific or dental-focused datasets, better multimodal training for image tasks, and use of RAG/BKGs to ground responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9191.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medical-domain fine-tuned LLM included in the benchmark; achieved moderate accuracy on natural science DAT items but low performance on quantitative reasoning and image-based PA items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-specific fine-tuned medical LLM (referenced in paper); the study does not provide model size or training corpus details.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Medical/dental education (natural sciences in DAT), reading comprehension, quantitative reasoning; limited image-item processing in study.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering DAT text-based multiple-choice questions (NS, RC, QR) and short justifications; processed screenshots for image-based questions when supported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) by DAT section; SME thematic coding for explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: NS 79% accuracy; QR 16% (very low); PA (image-based) ~26% reported for fine-tuned models collectively (MedGPT specifically ~26%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Domain fine-tuning on general medical corpora that may not match DAT specificity, deficits in stepwise and critical reasoning, and multimodal/image limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to general-purpose models and open-source Llama variants; general-purpose models outperformed MedGPT on QR and many RC tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor QR performance, struggles with complex and stepwise items, hallucinations, and low PA/image accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors advocate for targeted fine-tuning on task-specific datasets, improved multimodal training, and grounding approaches (RAG/BKG) to reduce hallucinations and boost reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9191.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical/domain-specific fine-tuned model evaluated on the DAT; showed modest text-based natural science performance but weak QR and challenges with reasoning and images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-specific fine-tuned LLM for biomedical content (referenced in the paper); parameter/training details not provided in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Biomedical/dental education (natural sciences), reading comprehension, quantitative reasoning within DAT context.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answer DAT text-based multiple-choice questions (NS, RC, QR) and provide short justifications; processed image-based screenshots when supported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) on DAT sections; SME qualitative thematic evaluation of explanation concordance and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: NS 81% accuracy; QR 18% (low); RC performance weaker compared to top general-purpose models (exact RC % not enumerated for BioGPT in summary).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Training data focused on biomedical corpora leading to moderate NS performance but insufficient QR/RC coverage; higher-order reasoning deficits, hallucinations, and limited multimodal/image reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against general-purpose models (GPT variants, Bard, Claude) and open-source Llama models; underperformed on QR and some RC tasks relative to top general-purpose models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weak QR performance, trouble with stepwise and critical-thinking items, hallucinations on complex problems, limited image-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Targeted domain-specific fine-tuning on DAT-style content, multimodal capability improvements, and use of RAG/BKG approaches recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9191.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large language model (70B parameter class) evaluated in the study; competitive on reading comprehension and strong on NS text-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama3 series model with 70B parameter configuration (as listed in the paper's model roster); specific training corpora not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences; Reading comprehension (educational assessment domain)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answer DAT text-based multiple-choice questions and provide brief explanations (open-source model evaluated on text-only items; not tested on some image-based items due to architectural constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) per DAT section (NS, RC, QR).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: NS 89% accuracy; RC 100% accuracy on the DAT sample text-based items (competitive with top commercial models).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size (70B), architecture improvements (Llama3 generation), training data coverage for reading comprehension and natural science content; inability to process image-based items in this study limited multimodal assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared favorably to many other open-source and fine-tuned models and approached commercial model performance on some text-based sections; outperformed some Llama2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated on image-based tasks (architectural constraints noted), potential deficits on QR and higher-order reasoning tasks similar to other models, and occasional explanation inaccuracies as identified by SMEs.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Open-source architectures can be competitive for text-based educational tasks; authors recommend multimodal extensions and targeted fine-tuning for domain-specific assessment tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9191.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source smaller Llama2 variant (7B parameters) included in the benchmark; showed lower accuracy on NS and RC text-based DAT items relative to larger/open-source or commercial models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama2 series model with 7B parameter configuration (listed among tested models); training-data specifics not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences; Reading comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answer DAT text-based multiple-choice questions and produce short explanations (text-only evaluation; not used for image-based items).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) on DAT sections (NS, RC primarily).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: NS 40% accuracy; RC 47% accuracy on the sample DAT text-based items (substantially lower than larger models).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Smaller model size (7B), likely reduced training data/representational capacity for domain-specific factual knowledge and reasoning, and lack of multimodal capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Underperformed relative to larger open-source (Llama3-70B) and commercial LLMs (GPT-4o, GPT-o1); demonstrates model-size/architecture effect in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low performance on both NS and RC text-based tasks, not tested on image-based items, struggles with higher-order reasoning and complex question interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Scaling (larger models) and more targeted fine-tuning recommended to boost performance for educational assessment simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9191.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9191.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intermediate-size open-source Llama2 model (13B parameters) tested as a DAT question-answering simulator; achieved moderate text-based performance better than the 7B variant but below top models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking of Large Language Models for the Dental Admission Test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama2 family model with 13B parameter configuration; the paper lists it among open-source models without additional training detail.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Dental education testing / Natural sciences; Reading comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answer DAT text-based multiple-choice questions and provide brief explanations (text-only evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage correct (accuracy) on DAT sections.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Text-based: NS 60% accuracy; RC 76% accuracy on the sample DAT text-based items (intermediate performance).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model capacity (13B vs 70B), training-data coverage, absence of multimodal support in testing setup, and limited higher-order reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performed better than the 7B Llama2 variant but worse than Llama3-70B and commercial GPT variants on text-based sections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Moderate performance with notable deficits on complex, stepwise, and higher-order cognitive questions; not evaluated on image-based items.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Increasing model size, domain-specific fine-tuning, and multimodal extensions recommended to close gap with top performers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking of Large Language Models for the Dental Admission Test', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How does chatGPT perform on the medical licensing exams? The implications of large language models for medical education and knowledge assessment. <em>(Rating: 2)</em></li>
                <li>Performance of chatGPT on USMLE: Potential for ai-assisted medical education using large language models. <em>(Rating: 2)</em></li>
                <li>Reasoning limitations of multimodal large language models: A case study of bongard problems. <em>(Rating: 2)</em></li>
                <li>Hallucination is inevitable: An innate limitation of large language models. <em>(Rating: 2)</em></li>
                <li>Enhancing dietary supplement question answer via retrieval-augmented generation (RAG) with LLM. <em>(Rating: 1)</em></li>
                <li>Biomedrag: A retrieval augmented large language model for biomedicine. <em>(Rating: 2)</em></li>
                <li>One LLM is not enough: Harnessing the power of ensemble learning for medical question answering. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9191",
    "paper_id": "paper-276465071",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A commercial general-purpose large language model evaluated in this study; used to answer DAT text-based questions and (for some experiments) image-based questions via screenshots.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Commercial general-purpose LLM (OpenAI family) referenced among general-purpose models in the paper; specific parameter count or training corpora not provided in the paper.",
            "scientific_subdomain": "Dental education testing / Natural sciences (biology, general chemistry, organic chemistry); Reading comprehension; Quantitative reasoning (educational assessment domain)",
            "simulation_task": "Simulating a test-taker by answering text-based DAT questions (NS, RC, QR sections) and providing short explanations; for some items screenshots of image-based questions were provided to multimodal-capable variants.",
            "evaluation_metric": "Percentage correct (accuracy) on section-specific multiple-choice DAT items; qualitative thematic evaluation of explanations by subject-matter experts (SMEs) with inter-rater Kappa reported for theme assignment.",
            "simulation_accuracy": "Text-based: NS 100%, RC 100%, QR 95% (percentage correct on DAT sample test text-based items).",
            "factors_affecting_accuracy": "Presence of higher-order reasoning requirements (critical thinking, stepwise multi-step problems), complexity/intricacy of questions, tendency to hallucinate on complex items, inability to consistently apply elimination strategies on QR/PA-type problems, multimodal/image-processing capability for image-based items.",
            "comparison_baseline": "Compared directly to other LLMs in the study (GPT-o1, GPT-4, GPT-3.5, Bard, Claude, fine-tuned medical models, open-source Llama models); outperformed most models on text-based sections (highest reported text-based accuracy). No human-test-taker numerical baseline reported; SMEs used for qualitative assessment of explanations.",
            "limitations_or_failure_cases": "Despite near-perfect text-based performance, struggled on image-based visual-spatial (PA) items (PA accuracy 24% reported elsewhere in the paper) and on items requiring stepwise reasoning, deeper critical thinking, or transfer; hallucinations and occasional incorrect final answer despite correct intermediate steps were observed.",
            "author_recommendations_or_insights": "Authors recommend targeted fine-tuning, improved multimodal integration for image-based reasoning, methods to mitigate hallucinations (e.g., retrieval-augmentation, biomedical knowledge graphs), and combining LLMs with instructor guidance for higher-order skill development.",
            "uuid": "e9191.0",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-o1",
            "name_full": "GPT-o1",
            "brief_description": "A commercial general-purpose LLM evaluated as a simulated DAT test-taker; showed top performance on text-based DAT sections in this benchmark.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "GPT-o1",
            "model_description": "Commercial general-purpose LLM (OpenAI family referenced alongside GPT-4 variants); paper does not provide parameter counts or training-data specifics.",
            "scientific_subdomain": "Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning (educational assessment)",
            "simulation_task": "Answering text-based DAT multiple-choice questions (NS, RC, QR) with short justifications, simulating student responses.",
            "evaluation_metric": "Percentage correct (accuracy) by DAT section; qualitative SME thematic coding of explanations.",
            "simulation_accuracy": "Text-based: NS 100%, RC 100%, QR 95% (percentage correct on DAT sample test text-based items).",
            "factors_affecting_accuracy": "Same as other models: question cognitive level (higher-order reasoning reduces accuracy), stepwise/multi-step problems, prompt framing, and modality (image vs text); limited image-processing when screenshots were provided produced lower PA accuracy (PA 21% reported).",
            "comparison_baseline": "Compared against other commercial, fine-tuned, and open-source LLMs in this work; matched or exceeded most models on text-based sections; no traditional simulator or human score baseline provided.",
            "limitations_or_failure_cases": "Low performance on PA/image-based items, struggles with stepwise and critical-thinking questions, occasional hallucinations and incorrect final answer reporting despite correct intermediate calculations.",
            "author_recommendations_or_insights": "Recommend domain-targeted fine-tuning, improved multimodal capabilities, integration with retrieval (RAG) and knowledge graphs to reduce hallucinations and improve reasoning on complex tasks.",
            "uuid": "e9191.1",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A high-performing commercial LLM included in the benchmark; demonstrated strong text-based performance on DAT sections although exact per-section numbers beyond qualitative description are not all enumerated.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Commercial general-purpose LLM (OpenAI); the paper cites it among top-performing general-purpose models but does not report model size or training set specifics.",
            "scientific_subdomain": "Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning (educational assessment)",
            "simulation_task": "Answer DAT text-based multiple-choice questions and provide short explanations, used to assess capabilities for test-prep simulation.",
            "evaluation_metric": "Percentage correct (accuracy) on DAT sections; SME qualitative thematic analysis of generated explanations (Kappa reported for theme assignment).",
            "simulation_accuracy": "Reported qualitatively as strong performance on NS and RC with accuracy generally exceeding 80% for text-based sections (exact per-section percentages not specified beyond 'generally exceeding 80%').",
            "factors_affecting_accuracy": "Question cognitive complexity (higher-order tasks reduce accuracy), need for stepwise reasoning, potential training-data overlap with test items (not fully excluded), and multimodal/image capability limitations for image-based items.",
            "comparison_baseline": "Compared to other LLMs in the study (GPT-4o, GPT-o1, GPT-3.5, Bard, Claude, fine-tuned and open-source models); performed well but specific numeric comparisons are provided elsewhere for other models.",
            "limitations_or_failure_cases": "Underperforms on image-based PA items (models capable of images nonetheless had low PA accuracy) and on problems requiring multi-step logical reasoning; hallucinations and mis-interpretation of intricate questions noted.",
            "author_recommendations_or_insights": "Authors suggest improving multimodal reasoning, targeted domain fine-tuning, and RAG/BKG integration to address hallucinations and reasoning limitations.",
            "uuid": "e9191.2",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5",
            "brief_description": "A widely used general-purpose LLM included in the benchmark; demonstrated generally strong text-based performance but lower than the top-performing GPT-4o and GPT-o1 in some respects.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Commercial general-purpose LLM (OpenAI family) referenced among tested models; the paper does not provide parameter counts or detailed training corpus information.",
            "scientific_subdomain": "Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning",
            "simulation_task": "Simulated DAT taker answering text-based multiple-choice DAT items and providing short explanations.",
            "evaluation_metric": "Percentage correct (accuracy) on DAT sections; SME thematic qualitative evaluation of explanations.",
            "simulation_accuracy": "Reported qualitatively as strong on NS and RC, with accuracy generally exceeding 80% on text-based sections (no precise per-section percentages provided in the paper for GPT-3.5).",
            "factors_affecting_accuracy": "Affected by higher-order cognitive task requirements (critical thinking, stepwise reasoning), prompt framing, and absence of multimodal image processing for many items.",
            "comparison_baseline": "Compared across the set of 16 LLMs; trailed top performers (GPT-4o/GPT-o1) on some text-based metrics but remained a strong generalist.",
            "limitations_or_failure_cases": "Same as other LLMs: problems with multi-step reasoning, intricate questions, and image-based PA items (couldn't process images directly in this study).",
            "author_recommendations_or_insights": "Targeted fine-tuning and multimodal capability improvements recommended; use with instructor guidance for higher-order skill development.",
            "uuid": "e9191.3",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Google Bard",
            "name_full": "Google Bard",
            "brief_description": "A commercial conversational LLM evaluated on the DAT sample; performed strongly on text-based sections but poorly on perceptual/image-based items.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "Bard",
            "model_description": "Google's conversational general-purpose LLM referenced among tested models; the paper does not supply size or training-data specifics.",
            "scientific_subdomain": "Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning",
            "simulation_task": "Answering text-based DAT multiple-choice items and providing brief explanations; when applicable, processed screenshots of image-based items for multimodal-capable variants.",
            "evaluation_metric": "Percentage correct (accuracy) on DAT sections; SME qualitative thematic coding.",
            "simulation_accuracy": "Text-based: reported generally &gt;80% accuracy on NS and RC; Image-based PA accuracy reported at 19% (poor performance on visual-spatial tasks).",
            "factors_affecting_accuracy": "Cognitive demand of questions (critical thinking, multi-step reasoning), multimodal capability limits, inconsistency in applying elimination strategies across sections.",
            "comparison_baseline": "Compared to other commercial and open-source LLMs; performed well on text but markedly worse on PA than top text-performing models.",
            "limitations_or_failure_cases": "Low PA/image accuracy, inconsistent elimination strategy application, struggles with stepwise and intricate questions, occasional hallucinations.",
            "author_recommendations_or_insights": "Improving multimodal reasoning, targeted fine-tuning, and grounding via RAG/BKGs recommended to address shortcomings.",
            "uuid": "e9191.4",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Claude",
            "name_full": "Claude",
            "brief_description": "A commercial general-purpose LLM evaluated for DAT question-answering; demonstrated strong text-based performance but limited image-based/perceptual ability.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "Claude",
            "model_description": "Commercial LLM (Anthropic) included in the benchmark; paper does not report parameter count or training-data details.",
            "scientific_subdomain": "Dental education testing / Natural sciences; Reading comprehension; Quantitative reasoning",
            "simulation_task": "Answer DAT text-based multiple-choice questions and provide short explanations; some image-based items processed where supported.",
            "evaluation_metric": "Percentage correct (accuracy) by DAT section; SME qualitative thematic analysis of explanations.",
            "simulation_accuracy": "Text-based: generally &gt;80% accuracy on NS and RC; Image-based PA accuracy reported at 36% (still low relative to text performance).",
            "factors_affecting_accuracy": "Question cognitive level (higher-order reasoning reduces accuracy), need for stepwise solution chains, modality (image vs text), and domain-specific content coverage.",
            "comparison_baseline": "Benchmarked against other LLMs in the study; stronger on text-based tasks than many fine-tuned/open-source models, but inferior on PA relative to some.",
            "limitations_or_failure_cases": "Low image-based PA accuracy, issues with stepwise problems and intricate reasoning, hallucinations and occasional incorrect reasoning noted.",
            "author_recommendations_or_insights": "Authors urge multimodal advances, fine-tuning for domain-specific content, and integration of retrieval and knowledge-graph techniques to reduce hallucinations.",
            "uuid": "e9191.5",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "DentalGPT",
            "name_full": "DentalGPT",
            "brief_description": "A domain-specific fine-tuned model intended for dental/medical tasks; evaluated on DAT items and showed moderate success on text-based natural science questions but poor QR performance.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "DentalGPT",
            "model_description": "Domain-specific fine-tuned LLM for dental/medical content (citation in paper); parameter counts/training data specifics not provided in this study.",
            "scientific_subdomain": "Dental education / medical knowledge (natural sciences within DAT: biology, general chemistry, organic chemistry) and some image-based processing where applicable.",
            "simulation_task": "Answer DAT text-based multiple-choice items (NS, RC, QR) and image-based items when screenshots were provided; provide short explanations.",
            "evaluation_metric": "Percentage correct (accuracy) by DAT section; SME qualitative thematic evaluation of justifications.",
            "simulation_accuracy": "Text-based: NS 79% accuracy; QR 24% (poor on quantitative reasoning); RC performance reported as weaker (exact RC % not enumerated for DentalGPT in the paper summary).",
            "factors_affecting_accuracy": "Domain fine-tuning on general medical datasets (possible mismatch with DAT-specific content), limited capability on higher-order reasoning and stepwise QR problems, image-processing when applicable but still weak.",
            "comparison_baseline": "Compared to general-purpose commercial models and open-source Llama models; performed moderately on NS but substantially worse on QR compared to GPT-4o/GPT-o1.",
            "limitations_or_failure_cases": "Underperformed on QR and on tasks requiring critical thinking, stepwise analysis, and transfer; moderate image-based performance (PA ~28% reported) and hallucinations in complex scenarios.",
            "author_recommendations_or_insights": "Authors note need for more targeted fine-tuning using DAT-specific or dental-focused datasets, better multimodal training for image tasks, and use of RAG/BKGs to ground responses.",
            "uuid": "e9191.6",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MedGPT",
            "name_full": "MedGPT",
            "brief_description": "A medical-domain fine-tuned LLM included in the benchmark; achieved moderate accuracy on natural science DAT items but low performance on quantitative reasoning and image-based PA items.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "MedGPT",
            "model_description": "Domain-specific fine-tuned medical LLM (referenced in paper); the study does not provide model size or training corpus details.",
            "scientific_subdomain": "Medical/dental education (natural sciences in DAT), reading comprehension, quantitative reasoning; limited image-item processing in study.",
            "simulation_task": "Answering DAT text-based multiple-choice questions (NS, RC, QR) and short justifications; processed screenshots for image-based questions when supported.",
            "evaluation_metric": "Accuracy (percentage correct) by DAT section; SME thematic coding for explanations.",
            "simulation_accuracy": "Text-based: NS 79% accuracy; QR 16% (very low); PA (image-based) ~26% reported for fine-tuned models collectively (MedGPT specifically ~26%).",
            "factors_affecting_accuracy": "Domain fine-tuning on general medical corpora that may not match DAT specificity, deficits in stepwise and critical reasoning, and multimodal/image limitations.",
            "comparison_baseline": "Compared to general-purpose models and open-source Llama variants; general-purpose models outperformed MedGPT on QR and many RC tasks.",
            "limitations_or_failure_cases": "Poor QR performance, struggles with complex and stepwise items, hallucinations, and low PA/image accuracy.",
            "author_recommendations_or_insights": "Authors advocate for targeted fine-tuning on task-specific datasets, improved multimodal training, and grounding approaches (RAG/BKG) to reduce hallucinations and boost reasoning.",
            "uuid": "e9191.7",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT",
            "brief_description": "A biomedical/domain-specific fine-tuned model evaluated on the DAT; showed modest text-based natural science performance but weak QR and challenges with reasoning and images.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "BioGPT",
            "model_description": "Domain-specific fine-tuned LLM for biomedical content (referenced in the paper); parameter/training details not provided in this work.",
            "scientific_subdomain": "Biomedical/dental education (natural sciences), reading comprehension, quantitative reasoning within DAT context.",
            "simulation_task": "Answer DAT text-based multiple-choice questions (NS, RC, QR) and provide short justifications; processed image-based screenshots when supported.",
            "evaluation_metric": "Percentage correct (accuracy) on DAT sections; SME qualitative thematic evaluation of explanation concordance and accuracy.",
            "simulation_accuracy": "Text-based: NS 81% accuracy; QR 18% (low); RC performance weaker compared to top general-purpose models (exact RC % not enumerated for BioGPT in summary).",
            "factors_affecting_accuracy": "Training data focused on biomedical corpora leading to moderate NS performance but insufficient QR/RC coverage; higher-order reasoning deficits, hallucinations, and limited multimodal/image reasoning.",
            "comparison_baseline": "Benchmarked against general-purpose models (GPT variants, Bard, Claude) and open-source Llama models; underperformed on QR and some RC tasks relative to top general-purpose models.",
            "limitations_or_failure_cases": "Weak QR performance, trouble with stepwise and critical-thinking items, hallucinations on complex problems, limited image-based reasoning.",
            "author_recommendations_or_insights": "Targeted domain-specific fine-tuning on DAT-style content, multimodal capability improvements, and use of RAG/BKG approaches recommended.",
            "uuid": "e9191.8",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Llama3-70B",
            "brief_description": "An open-source large language model (70B parameter class) evaluated in the study; competitive on reading comprehension and strong on NS text-based tasks.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "Llama3-70B",
            "model_description": "Open-source Llama3 series model with 70B parameter configuration (as listed in the paper's model roster); specific training corpora not detailed in this paper.",
            "scientific_subdomain": "Dental education testing / Natural sciences; Reading comprehension (educational assessment domain)",
            "simulation_task": "Answer DAT text-based multiple-choice questions and provide brief explanations (open-source model evaluated on text-only items; not tested on some image-based items due to architectural constraints).",
            "evaluation_metric": "Percentage correct (accuracy) per DAT section (NS, RC, QR).",
            "simulation_accuracy": "Text-based: NS 89% accuracy; RC 100% accuracy on the DAT sample text-based items (competitive with top commercial models).",
            "factors_affecting_accuracy": "Model size (70B), architecture improvements (Llama3 generation), training data coverage for reading comprehension and natural science content; inability to process image-based items in this study limited multimodal assessment.",
            "comparison_baseline": "Compared favorably to many other open-source and fine-tuned models and approached commercial model performance on some text-based sections; outperformed some Llama2 variants.",
            "limitations_or_failure_cases": "Not evaluated on image-based tasks (architectural constraints noted), potential deficits on QR and higher-order reasoning tasks similar to other models, and occasional explanation inaccuracies as identified by SMEs.",
            "author_recommendations_or_insights": "Open-source architectures can be competitive for text-based educational tasks; authors recommend multimodal extensions and targeted fine-tuning for domain-specific assessment tasks.",
            "uuid": "e9191.9",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama2-7B",
            "name_full": "Llama2-7B",
            "brief_description": "An open-source smaller Llama2 variant (7B parameters) included in the benchmark; showed lower accuracy on NS and RC text-based DAT items relative to larger/open-source or commercial models.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "Llama2-7B",
            "model_description": "Open-source Llama2 series model with 7B parameter configuration (listed among tested models); training-data specifics not provided in the paper.",
            "scientific_subdomain": "Dental education testing / Natural sciences; Reading comprehension",
            "simulation_task": "Answer DAT text-based multiple-choice questions and produce short explanations (text-only evaluation; not used for image-based items).",
            "evaluation_metric": "Percentage correct (accuracy) on DAT sections (NS, RC primarily).",
            "simulation_accuracy": "Text-based: NS 40% accuracy; RC 47% accuracy on the sample DAT text-based items (substantially lower than larger models).",
            "factors_affecting_accuracy": "Smaller model size (7B), likely reduced training data/representational capacity for domain-specific factual knowledge and reasoning, and lack of multimodal capability.",
            "comparison_baseline": "Underperformed relative to larger open-source (Llama3-70B) and commercial LLMs (GPT-4o, GPT-o1); demonstrates model-size/architecture effect in this benchmark.",
            "limitations_or_failure_cases": "Low performance on both NS and RC text-based tasks, not tested on image-based items, struggles with higher-order reasoning and complex question interpretation.",
            "author_recommendations_or_insights": "Scaling (larger models) and more targeted fine-tuning recommended to boost performance for educational assessment simulations.",
            "uuid": "e9191.10",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama2-13B",
            "name_full": "Llama2-13B",
            "brief_description": "An intermediate-size open-source Llama2 model (13B parameters) tested as a DAT question-answering simulator; achieved moderate text-based performance better than the 7B variant but below top models.",
            "citation_title": "Benchmarking of Large Language Models for the Dental Admission Test",
            "mention_or_use": "use",
            "model_name": "Llama2-13B",
            "model_description": "Open-source Llama2 family model with 13B parameter configuration; the paper lists it among open-source models without additional training detail.",
            "scientific_subdomain": "Dental education testing / Natural sciences; Reading comprehension",
            "simulation_task": "Answer DAT text-based multiple-choice questions and provide brief explanations (text-only evaluation).",
            "evaluation_metric": "Percentage correct (accuracy) on DAT sections.",
            "simulation_accuracy": "Text-based: NS 60% accuracy; RC 76% accuracy on the sample DAT text-based items (intermediate performance).",
            "factors_affecting_accuracy": "Model capacity (13B vs 70B), training-data coverage, absence of multimodal support in testing setup, and limited higher-order reasoning capability.",
            "comparison_baseline": "Performed better than the 7B Llama2 variant but worse than Llama3-70B and commercial GPT variants on text-based sections.",
            "limitations_or_failure_cases": "Moderate performance with notable deficits on complex, stepwise, and higher-order cognitive questions; not evaluated on image-based items.",
            "author_recommendations_or_insights": "Increasing model size, domain-specific fine-tuning, and multimodal extensions recommended to close gap with top performers.",
            "uuid": "e9191.11",
            "source_info": {
                "paper_title": "Benchmarking of Large Language Models for the Dental Admission Test",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How does chatGPT perform on the medical licensing exams? The implications of large language models for medical education and knowledge assessment.",
            "rating": 2,
            "sanitized_title": "how_does_chatgpt_perform_on_the_medical_licensing_exams_the_implications_of_large_language_models_for_medical_education_and_knowledge_assessment"
        },
        {
            "paper_title": "Performance of chatGPT on USMLE: Potential for ai-assisted medical education using large language models.",
            "rating": 2,
            "sanitized_title": "performance_of_chatgpt_on_usmle_potential_for_aiassisted_medical_education_using_large_language_models"
        },
        {
            "paper_title": "Reasoning limitations of multimodal large language models: A case study of bongard problems.",
            "rating": 2,
            "sanitized_title": "reasoning_limitations_of_multimodal_large_language_models_a_case_study_of_bongard_problems"
        },
        {
            "paper_title": "Hallucination is inevitable: An innate limitation of large language models.",
            "rating": 2,
            "sanitized_title": "hallucination_is_inevitable_an_innate_limitation_of_large_language_models"
        },
        {
            "paper_title": "Enhancing dietary supplement question answer via retrieval-augmented generation (RAG) with LLM.",
            "rating": 1,
            "sanitized_title": "enhancing_dietary_supplement_question_answer_via_retrievalaugmented_generation_rag_with_llm"
        },
        {
            "paper_title": "Biomedrag: A retrieval augmented large language model for biomedicine.",
            "rating": 2,
            "sanitized_title": "biomedrag_a_retrieval_augmented_large_language_model_for_biomedicine"
        },
        {
            "paper_title": "One LLM is not enough: Harnessing the power of ensemble learning for medical question answering.",
            "rating": 1,
            "sanitized_title": "one_llm_is_not_enough_harnessing_the_power_of_ensemble_learning_for_medical_question_answering"
        }
    ],
    "cost": 0.01774875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmarking of Large Language Models for the Dental Admission Test
1 April 2025</p>
<p>Yu Hou 
Division of Computational Health Sciences
Department of Surgery
University of Minnesota
55455MinneapolisMNUSA</p>
<p>Center for Learning Health System Sciences
University of Minnesota
55455MinneapolisMNUSA</p>
<p>Jay Patel 
Oral Health Sciences Department
Kornberg School of Dentistry
Temple University
19140PhiladelphiaPAUSA</p>
<p>Liya Dai 
Wayzata High School
55446PlymouthMNUSA</p>
<p>Emily Zhang 
Wayzata High School
55446PlymouthMNUSA</p>
<p>Yang Liu 
School of Education
Syracuse University
13244SyracuseNYUSA</p>
<p>Zaifu Zhan 
Department of Electrical and Computer Engineering
University of Minnesota
MinneapolisMNUSA</p>
<p>Pooja Gangwani 
Oral Health Sciences Department
Kornberg School of Dentistry
Temple University
19140PhiladelphiaPAUSA</p>
<p>Rui Zhang ruizhang@umn.edu 
Division of Computational Health Sciences
Department of Surgery
University of Minnesota
55455MinneapolisMNUSA</p>
<p>Center for Learning Health System Sciences
University of Minnesota
55455MinneapolisMNUSA</p>
<p>Benchmarking of Large Language Models for the Dental Admission Test
1 April 2025881F79122F8B8131B1C434CF066DDA8810.34133/hds.0250Submitted 2 August 2024 Revised 30 January 2025 Accepted 14 February 2025
Background: Large language models (LLMs) have shown promise in educational applications, but their performance on high-stakes admissions tests, such as the Dental Admission Test (DAT), remains unclear.Understanding the capabilities and limitations of these models is critical for determining their suitability in test preparation.Methods: This study evaluated the ability of 16 LLMs, including general-purpose models (e.g., GPT-3.5, GPT-4, GPT-4o, GPT-o1, Google's Bard, mistral-large, and Claude), domain-specific finetuned models (e.g., DentalGPT, MedGPT, and BioGPT), and open-source models (e.g., Llama2-7B, Llama2-13B, Llama2-70B, Llama3-8B, and Llama3-70B), to answer questions from a sample DAT.Quantitative analysis was performed to assess model accuracy in different sections, and qualitative thematic analysis by subject matter experts examined specific challenges encountered by the models.Results: GPT-4o and GPT-o1 outperformed others in text-based questions assessing knowledge and comprehension, with GPT-o1 achieving perfect scores in the natural sciences (NS) and reading comprehension (RC) sections.Open-source models such as Llama3-70B also performed competitively in RC tasks.However, all models, including GPT-4o, struggled substantially with perceptual ability (PA) items, highlighting a persistent limitation in handling image-based tasks requiring visual-spatial reasoning.Fine-tuned medical models (e.g., DentalGPT, MedGPT, and BioGPT) demonstrated moderate success in text-based tasks but underperformed in areas requiring critical thinking and reasoning.Thematic analysis identified key challenges, including difficulties with stepwise problem-solving, transferring knowledge, comprehending intricate questions, and hallucinations, particularly on advanced items.Conclusions: While LLMs show potential for reinforcing factual knowledge and supporting learners, their limitations in handling higherorder cognitive tasks and image-based reasoning underscore the need for judicious integration with instructor-led guidance and targeted practice.This study provides valuable insights into the capabilities and limitations of current LLMs in preparing prospective dental students and highlights pathways for future innovations to improve performance across all cognitive skills assessed by the DAT.</p>
<p>Introduction</p>
<p>The Dental Admission Test (DAT) is widely used as a crucial metric by many educational institutions to evaluate applicants, with numerous studies indicating a positive correlation between the DAT scores and students' performance post-admission [ 1 , 2 ].In recent years, large language models (LLMs) have emerged as valuable tools across diverse domains, including healthcare education.Applications of LLMs in healthcare education encompass personalized learning, interactive tutoring, case-based learning, and patient simulation [ 3 ].However, ensuring the accuracy and reliability of LLMs in healthcare education is paramount to engender trust and confidence among healthcare educators, students, administration, and other stakeholders, as well as to mitigate bias [ 4 ].</p>
<p>Despite some studies exploring the performance of LLMs on various medical exams [ 5 -7 ], there has been limited investigation on the integration of LLMs into dental admission assessment, which hinders the understanding of how to effectively leverage LLMs to support students' learning in this specific domain.Therefore, it is imperative to evaluate the performance of LLMs on the DAT and reveal the potential and limitations of using LLMs in the DAT preparation.Our examination aims to contribute to the ongoing exploration of LLMs in healthcare education, shedding light on their potential utility and effectiveness within the realm of dental education.</p>
<p>Through rigorous testing and validation, our study endeavors to provide insights into the suitability of LLMs for enhancing assessment processes in dental education, thereby facilitating informed decision-making and advancing educational practices in dentistry.</p>
<p>The present study evaluated the performance of 16 LLMs, including popular general-purpose models (GPT-3.5 [ 8 ], GPT-4 [ 9 ], GPT-4o [ 10 ], GPT-o1 [ 11 ], Google's Bard [ 12 ], mistrallarge [ 13 ], and Claude [ 14 ]), domain-specific fine-tuned models (MMedC [ 15 ], DentalGPT [ 16 ], MedGPT [ 17 ], and BioGPT [ 18 ]), and open-source models (Llama2-7B [ 19 ], Llama2-13B [ 20 ], Llama2-70B [ 21 ], Llama3-8B [ 22 ], and Llama3-70B [ 23 ]), on the DAT to address the following research questions: (a) How do different LLMs perform when taking the DAT?(b) What are the potentials and limitations of using these LLMs in the DAT preparation?</p>
<p>Methods</p>
<p>Dataset</p>
<p>The DAT is a computer-based exam accepted by numerous dental schools across the United States and Canada, totaling 66 and 10 institutions, respectively [ 24 ].Designed to assess applicants' potential for success, the DAT covers various domains, including natural sciences (NS), perceptual ability (PA), reading comprehension (RC), and quantitative reasoning (QR).In this study, a publicly available DAT sample test was obtained from the American Dental Association ( https:// www.ada.org/ ).This sample test comprises 247 multiplechoice questions distributed across the four distinct testing components.These components collectively represent the breadth of knowledge and skills required for success in dental education and practice.</p>
<p>Method</p>
<p>We employed 16 LLMs: GPT-3.5, GPT-4, GPT-4o, GPT-o1, Google's Bard, mistral-large, Claude, MMedC, DentalGPT, MedGPT, BioGPT, Llama2-7B, Llama2-13B, Llama2-70B, Llama3-8B, and Llama3-70B.All models were tested with text-based questions, while GPT-4o, GPT-o1, Bard, Claude, DentalGPT, MedGPT, and BioGPT were also evaluated on image-based questions.Notably, the PA component consisted exclusively of image-based questions and answers.Additionally, certain sections, such as the QR section, included geometric problems that required image-based presentations due to formatting constraints.Because GPT-3.5, GPT-4, MMedC, Llama models, and Mistral could not process images directly, we captured screenshots of the image-based questions and forwarded them to Bard, GPT-4o, GPT-o1, Claude, DentalGPT, MedGPT, and BioGPT for processing.Specific prompts tailored for each test section are described below.</p>
<p>All RC questions are text-based.Each question was presented as a separate chat to each LLM.The chat began with the main prompt: "This is a simulated Dental Admission Test.We need an accurate answer.Only one choice is the correct answer.Please tell us which one of the choices is the correct answer and give a SHORT explanation (less than 4 sentences) on why it is correct." Immediately following the main prompt, a text-based question was submitted to the testing model for its answer and explanation.This process was repeated for all RC questions across all the LLMs.</p>
<p>For the PA questions, the DAT includes special instructions for different types of perceptual questions.After entering the main prompt, the original text instructions from the DAT were submitted to models.Most questions in the NS and QR sections are text-based.Therefore, they were processed using the same method as the RC questions.For image-based NS and QR questions, we applied the same method used for the PA questions, where screenshots of the image-based questions were provided to GBard, GPT-4o, GPT-o1, Claude, DentalGPT, MedGPT, and BioGPT for processing.</p>
<p>Evaluation</p>
<p>To assess the performance and explanatory capabilities of the 16 LLMs on the DAT questions, a comprehensive evaluation process was conducted, encompassing both quantitative and qualitative analyses.Two types of data were evaluated: The answer options provided by the models and the model-generated texts (answer justifications).For the quantitative analysis, the performance of LLMs was assessed by calculating the percentage of correct answers provided by each of the seven models across all items.This step helped determine the accuracy of the LLMs' answer selections.</p>
<p>The qualitative analysis focused on the model-generated texts.Two subject matter experts (SMEs) conducted a thematic analysis to evaluate the explanations provided by the LLMs.Both SMEs are faculties in disciplines related to dental or oral health.A stratified random sampling approach was employed to ensure a representative sample, grouping items by format (text-based and image-based), test components, and model correctness.Fifty items were randomly selected from the test sample, resulting in a sample that included 72 text-based items and 26 image-based questions, totaling 98 justifications for the selected 50 questions.The SMEs first analyzed the text data for accuracy and concordance in the explanations.Accuracy was defined as whether a model correctly understood a question and provided a supporting explanation.An explanation could be deemed inaccurate due to incorrect information, logical fallacies, or statistical errors.Concordance referred to the coherence and soundness of the argument presented in the explanation.To synthesize the text data, the SMEs initially analyzed modelgenerated texts for 10 questions to identify underlying themes.They then discussed their findings and consolidated the identified themes into seven categories.Subsequently, they reanalyzed the texts for these 10 questions, assigning each explanation to one of the seven categories, achieving a high level of agreement in their assignments (Kappa = 0.96).Given the high level of inter-rater reliability, they divided and separately assessed the remaining text data.</p>
<p>Results</p>
<p>Test performance of 16 LLMs</p>
<p>Figures 1 and 2 illustrate the relative performance of the 16 LLMs tested in this study.Overall, commonly used LLMs such as GPT-4o, GPT-3.5, Bard, Claude, and GPT-o1 demonstrated strong performance on the NS and RC sections, with accuracy rates generally exceeding 80%.Among these, GPT-4o achieved the highest performance across all textbased sections, excelling in NS (100%), RC (100%), and QR (95%), indicating its robustness in handling text-based questions.Similarly, GPT-o1 also showed exceptional performance with perfect accuracy in NS (100%) and RC (100%) and a  strong result in QR (95%).The performance of domain-specific fine-tuned models such as DentalGPT, MedGPT, and BioGPT varied.While these models demonstrated moderate accuracy in the NS section (DentalGPT: 79%, MedGPT: 79%, BioGPT: 81%), their performance in the RC and QR sections was weaker, particularly in QR (DentalGPT: 24%, MedGPT: 16%, BioGPT: 18%).The open-source models, particularly Llama3-70B, showed competitive performance, achieving 89% accuracy in the NS section and 100% in RC, outperforming several other models in these areas.However, other open-source models like Llama2-7B and Llama2-13B exhibited relatively lower performance, with NS accuracy rates of 40% and 60%, respectively, and RC accuracy rates of 47% and 76%.</p>
<p>The ability of LLMs to handle image-based questions remained limited.GPT-4o, Bard, and Claude showed significantly weaker performance in the PA section, with accuracy rates of 24%, 19%, and 36%, respectively.Similarly, GPT-o1 achieved only 21% in PA.The domain-specific fine-tuned models (DentalGPT, MedGPT, BioGPT) also struggled with image-based PA questions, with accuracy rates of 28%, 26%, and 29%, respectively.Interestingly, the NS section involving image-based questions further highlighted these differences.GPT-4o achieved a relatively strong performance (84%), while the fine-tuned models exhibited lower accuracy (DentalGPT: 19%, MedGPT: 30%, BioGPT: 27%).However, the open-source Llama models were not tested for these image-based tasks, leaving a gap in their evaluation.</p>
<p>Performance of 16 LLMs on model-generated texts</p>
<p>The SMEs identified seven themes of errors made by the LLMs via the evaluation of the model-generated texts.The themes include (a) critical thinking, (b) stepwise analysis, (c) transfer knowledge to the correct answer, (d) intricate questions, (e) hallucinations, (f) unnecessary complexity, and (g) elimination process.See Table 1 for descriptions of the seven themes.</p>
<p>Thematic analysis revealed that while the LLMs could answer basic factual science questions that primarily assess factual knowledge retention, their performance declined considerably when critical thinking skills were necessary.These critical thinking questions required more than simple knowledge recall; models had to process information based on logical reasoning, make judgments, and draw conclusions.Additionally, the models struggled with basic science questions involving stepwise analysis, where multiple steps were required, and the output of each step served as input for the subsequent one.</p>
<p>Furthermore, the models faced challenges in comprehending intricate, complex questions, often misinterpreting them and providing incorrect answers.In some instances, the models generated completely erroneous information, potentially hallucinating responses from uncertain sources when solving complex problems.While the Bard model effectively utilized an elimination process for basic science questions, it failed to apply this strategy consistently across other sections, such as PA and mathematics.The analysis also highlighted instances where models added unnecessary complexity to mathematical calculations or failed to report the correct final answer despite performing accurate intermediate calculations.Table 2 shows seven examples of SMEs' evaluations of the LLMs' sample questions.</p>
<p>Potentials and limitations of using LLMs in DAT preparation</p>
<p>According to the quantitative and qualitative results, we further examined the potential and limitations of using the LLMs in the DAT preparation following Bloom's Taxonomy.Bloom's Taxonomy is a hierarchical framework that categorizes different levels of cognitive skills and learning objectives [ 24 ].Developed by Benjamin Bloom and his colleagues in the 1950s, the taxonomy is widely used in educational settings to design curricula and assessments [ 25 ].The taxonomy consists of six levels, and the levels progress from simple knowledge and comprehension to more complex levels of application, analysis, synthesis, and evaluation.</p>
<p>From the perspective of Bloom's Taxonomy, the DAT aims to assess various cognitive levels to evaluate a prospective dental</p>
<p>Stepwise analysis</p>
<p>The models did not perform well with questions involving stepwise thinking.For example, three steps might be included to get the right answer.The models would get one or two steps fine and then perform wrongly on the next.</p>
<p>Transfer knowledge to the correct answer</p>
<p>The models would do the right calculations; however, they would not report the correct answers on some occasions.</p>
<p>Intricate questions</p>
<p>The models could not understand complex questions.These questions were misinterpreted by the models, leading to false answers.</p>
<p>Hallucinations</p>
<p>In some cases, the models provided completely wrong information, and the source of the information is unsure.This was observed when solving complex problems.</p>
<p>Unnecessary complexity</p>
<p>The models added some unnecessary complexity to the math calculation.</p>
<p>Elimination process</p>
<p>The models failed to use an option elimination process for the perceptual ability and math sections.The model used the wrong information.First, we simplify the inequality 2x -3 &gt; 3x + 7.This gives us -5 &gt; x.We then negate the inequality and multiply both sides by -1.This gives us x &lt; -5.Finally, we look at the answer choices and see that only choice D, x &lt; -4, is consistent with the inequality x &lt; -5.The other choices are incorrect because they do not satisfy the given inequality.For example, choice A, x &gt; -4, would imply that x is greater than -4, which is not true given the inequality x &lt; -5.</p>
<p>Unnecessary</p>
<p>The BART model often uses an option elimination process.However, it failed to get the right answer in many cases.</p>
<p>student's academic abilities and potential.The taxonomy provides a framework for understanding the different levels of cognitive skills and knowledge that the DAT may target.Knowledge: Some questions in NS assess the foundational knowledge of applicants in areas such as biology, general chemistry, and organic chemistry.This level of the taxonomy involves recalling and recognizing factual information essential for dental education.</p>
<p>Comprehension: Most RC questions evaluate applicants' ability to understand and interpret information presented in different formats, such as RC passages or data interpretation questions.This level requires understanding the meaning of concepts and demonstrating an understanding of the material.</p>
<p>Application: The DAT includes questions, specifically in the NS and QR sections, that require applying learned concepts, principles, and theories to solve problems or analyze situations related to dental science and practice.This level assesses the ability to apply knowledge in practical contexts.</p>
<p>Analysis: The test includes questions, particularly in the PA section, that assess analytical skills, such as breaking down complex information, identifying relationships, and recognizing patterns or trends.This level evaluates the ability to critically analyze and interpret data relevant to dental practice.</p>
<p>Synthesis: Some questions on the DAT, such as those in the RC and NS sections, require synthesizing information from various sources or combining different concepts to formulate solutions.This level assesses the ability to integrate knowledge and generate original thoughts or hypotheses.</p>
<p>Evaluation: Some DAT questions (e.g., NS and QR questions) require evaluating and making judgments based on specific criteria or standards.This level assesses the ability to critically evaluate information, ideas, or solutions and provide justifications or critiques.</p>
<p>In this study, all LLMs demonstrated promising capabilities in understanding and answering text-based questions, particularly those involving RC and factual knowledge.The study found that LLMs like GPT-4, GPT-4o, GPT-o1, and Claude outperformed other models in accurately responding to textbased items in areas such as NS and RC.This potential could be leveraged in the DAT preparation by using LLMs to explain and reinforce concepts, provide practice questions, and offer targeted feedback for text-based sections of the exam.Moreover, the ability of LLMs to engage in conversational interactions allows for a more interactive and personalized learning experience.Students could ask follow-up questions, seek clarifications, and receive tailored explanations from the LLMs, potentially enhancing their understanding of complex topics and improving their critical thinking skills.</p>
<p>While LLMs excel in text-based tasks, the study revealed significant limitations in their ability to solve QR and PA problems.The thematic analysis results suggested that most LLMs struggled with questions that require critical thinking, stepwise analysis, and the application of logical reasoning.These higher-order cognitive skills are crucial for success in dental education and are assessed by the DAT.The results indicated the LLMs' limitations for hallucinations and inaccuracies.LLMs can sometimes generate incorrect information, hallucinations, or unnecessary complexities, especially when dealing with intricate or complex questions.This could lead to students acquiring incorrect knowledge or misunderstandings.</p>
<p>Discussion</p>
<p>The use of artificial intelligence (AI)-powered language models for test preparation represents an emerging and promising area in education technology.This study evaluated 16 LLMs, including commercial models, open-source models, and domainspecific fine-tuned models, to assess their ability to support preparation for the DAT.The quantitative and qualitative evaluation results highlight that while LLMs show potential in reinforcing factual knowledge and supporting personalized learning experiences, they also present significant risks in tasks that require higher-order cognitive skills such as critical analysis, synthesis, and evaluation.From a technical perspective, this study provides one of the most extensive evaluations of LLMs in a specialized educational domain, highlighting performance disparities across different model categories.For example, commercial models like GPT-4 and GPT-o1 excelled in text-based tasks, while fine-tuned domain-specific models (e.g., DentalGPT, MedGPT, and BioGPT) and open-source alternatives (e.g., Llama2 and Llama3 series) offered varied capabilities, with some demonstrating strong performance in areas like biology and chemistry [ 26 , 27 ].This work extends prior research on general-purpose LLMs by incorporating fine-tuned and opensource models, as well as by addressing relatively underexplored areas such as the handling of image-based questions.The inclusion of image-based evaluations revealed critical areas for improvement, particularly in models' ability to manage visualspatial reasoning tasks.This finding underscores the limitations of current LLMs and provides valuable benchmarks for researchers and developers aiming to enhance educational applications [ 28 ].Moreover, our results align with existing literature that highlights the utility of LLMs in providing factual knowledge and interactive tutoring capabilities that facilitate clarification and elaboration, which are especially beneficial for self-directed learners [ 26 , 27 ].However, this study also emphasizes the importance of judiciously integrating LLMs with other resources, such as human instructors, study materials, and practice exams, to address their deficiencies in fostering critical thinking and advanced cognitive skills.Although AI tools have the potential to transform educational systems, they should not be used as standalone solutions, particularly for complex, highstakes assessments like the DAT.One limitation of this study is the possibility that some DAT questions may have been included in the training data of the evaluated LLMs.While our findings suggest that this overlap is minimal, it cannot be entirely ruled out.Future studies should consider developing proprietary test datasets or adopting methods to ensure training data exclusion, thus enabling more robust and unbiased evaluations.Additionally, while the prompts used in this study were carefully designed to simulate the DAT context, we acknowledge that systematic experiments comparing different prompt designs were not conducted.Future studies could explore the impact of varying prompt phrasing and structure to better understand their influence on model performance and enhance the robustness of LLM evaluations.</p>
<p>From the quantitative results, all LLMs demonstrated strong capabilities in providing explanations and summaries of factual knowledge in areas such as biology, chemistry, and general science, which are critical for the DAT.This makes LLMs particularly valuable for learners seeking to acquire and reinforce factual knowledge.Additionally, the conversational mode of LLMs enables students to seek clarification or additional explanations on challenging topics, enhancing their understanding through interactive learning.Commercial models such as GPT-4 and GPT-o1 stood out with superior performance in text-based sections, with GPT-o1 achieving perfect accuracy in the NS and RC sections.Open-source models, such as Llama3-70B, also performed competitively, particularly in the RC section (100%), indicating that advancements in opensource architectures are narrowing the gap with proprietary models.Meanwhile, fine-tuned domain-specific models, such as DentalGPT and BioGPT, showed moderate success in the NS section but struggled in RC and QR tasks.This limitation likely stems from the reliance on general medical training datasets for fine-tuning, which may lack the specialized content necessary for the DAT, highlighting the importance of targeted fine-tuning for domain-specific tasks [ 29 , 30 ].However, significant challenges were identified across all LLMs in image-based sections, particularly in the PA section.For instance, GPT-4o, despite its strong text-based performance, achieved only 24% accuracy in PA.Similarly, domain-specific models and opensource LLMs struggled in these tasks, with some (e.g., Llama series) not being evaluated due to architectural constraints.These results align with existing research that underscores the difficulty LLMs face in visual-spatial reasoning tasks [ 31 ].Such findings emphasize the need for future advancements in multimodal capabilities to address these limitations effectively.</p>
<p>The thematic analysis highlighted possible drawbacks of using the LLMs in the DAT preparation.The accuracy and concordance of the model-generated text are difficult to guarantee.Most LLMs failed to effectively handle questions requiring critical thinking, stepwise reasoning, analysis, interpretation, and judgment.They often generated incorrect information, hallucinations, or unnecessary complexities.This is a common problem that still exists in LLMs [ 32 -36 ].The results underscore the superior performance of ChatGPT-4o in handling text-based questions across various sections of the DAT.However, the performance gap observed in image-based questions, particularly in the PA section, indicates an area for improvement for these models.The findings suggest that while current LLMs demonstrate proficiency in reinforcing factual knowledge through text, their ability to process and accurately respond to imagebased content remains limited.</p>
<p>As LLM technologies continue to evolve, several directions emerge for enhancing their utility in educational settings.First, targeted fine-tuning with specialized datasets can improve domain-specific performance, as demonstrated by the moderate success of models like DentalGPT and BioGPT.Second, the integration of multimodal capabilities, enabling simultaneous text and image processing, could address current limitations in visual-spatial reasoning.Third, developing methods to mitigate hallucinations and ensure accuracy is essential for establishing trust and reliability in educational applications.Additionally, leveraging Biomedical Knowledge Graphs (BKGs) alongside Retrieval-Augmented Generation (RAG) systems offers promising strategies for improving LLM performance.BKGs provide structured and verified domain-specific knowledge, enhancing accuracy and contextual reasoning in complex queries, while RAG systems ensure that responses are grounded in external, up-to-date information [ 35 -42 ].Finally, longitudinal studies assessing the long-term impact of LLM-based tools on learners' outcomes are needed.By comparing LLM-supported learning with traditional methods, future research can better understand the potential of AI in transforming education.</p>
<p>Conclusion</p>
<p>This study provides valuable insights into the applications of LLMs in preparing for the DAT, demonstrating their strengths in reinforcing factual knowledge and their limitations in handling image-based and higher-order cognitive tasks.As AI technologies advance, their integration into educational systems must be approached judiciously, complementing traditional methods and addressing current shortcomings.The findings contribute to a growing body of research on LLMs in education, laying the groundwork for future innovations and applications.</p>
<p>Ethical Approval</p>
<p>This study did not involve human participants or clinical trials; therefore, ethical approval was not required.The data used in this research consisted of publicly available sample questions from the dental admission test (DAT), obtained from the American Dental Association.</p>
<p>Fig. 1 .
1
Fig. 1.Percentage of correct answers for the text-based test part by different models.</p>
<p>Fig. 2 .
2
Fig. 2. Percentage of correct answers for the image-based test part by different models.</p>
<p>complexity If [b (c + d) + e]a = 135, then which variable cannot be zero? A. a B. b C. c D. d E. e Answer B. Explanation: If b were equal to 0, then the expression [b (c + d) + e]a would be equal to 0, regardless of the values of c, d, and e.However, we are given that the expression [b (c + d) + e]a is equal to 135.Therefore, b cannot be equal to 0. The other variables, a, c, and d, could be equal to 0 without affecting the value of the expression.A simple rule -the denominator can't be zero -can solve the problem.Elimination process If 2x -3 &gt; 3x + 7, then which must be true? A. x &gt; -4 B. x &gt; 4 C. x &gt; -10 D. x &lt; -4 E. x &lt; -10 Answer: D. Explanation: The answer is D. x &lt; -4.</p>
<p>Table 1 .
1
Themes of errors made by LLMsThemes of the LLM's error performance DescriptionCritical thinking Critical thinking requires the examinees to be able to interpret, analyze, evaluate, explain, and conclude the problems that exist.The models' performance deteriorated when critical thinking was involved.</p>
<p>Table 2 .
2
Examples of SMEs' evaluations of model-generated texts for the sample questions According to the problem, in 12 years, the mother will be twice as old as the daughter.We can express this in the equation: 3x + 12 = 2 * (x + 12).Solving this equation, we find that x = 6, which means the mother's current age is 3x = 18 * 3 = 30 years.The area of the painting is 4 * 5 = 20 square feet.The area of the frame is 6 * 12 = 72 square inches = 1.2 square feet.The total area of the picture and the frame is 20 + 1.2 = 21.2 square feet.The percentage of the area of the picture and the frame that is occupied by the frame is 1.2 / 21.2 * 100 = 5.625 * 100 = 30%.
Sample questionsThemes(correct answers are bolded)Model responseSME commentsCritical thinkingIn which reaction is H2O considered toAnswer: C. Explanation: In this reaction, waterThe program knows factsbe acting as an acid? A. Zn(s) + 2H3O+ â†’ Zn2+ + H2(g) + H2O B. HCl(g) + H2O â†’ H3O+ + Cl-C. HC2H3O2 + H2O â‡” H3O+ +C2H3O2-D. NH3 + H2O â‡” NH4+ + OH-E. NH3 + H3O+ â†’ NH4+ + H2Odonates a proton to the acetic acid molecule, which is a proton acceptor. This makes water an acid, according to the Bronsted-Lowry definition of acids and bases. The other reactions do not involve water donating a proton.and background. Chemical equations require knowing the topic really well. They are not facts. The model can't think through to come to the right answer.Stepwise thinking A mother's age is three times her daughter's age. In twelve years, the mother's age will be twice theAnswer: A. Explanation: Let's denote the daughter's current age as x and the mother's current age as 3x. The model got the right steps but did wrong calculations after thedaughter's age at that time. Howsecond step.old is the mother now? A. 18 B. 20C. 24 D. 30 E. 36Transfer knowl-Which of the following fractions isAnswer: E. Explanation: In decimal form: A. 11/The model did the rightedge to the correct answerthe smallest? A. 11/15 B. 4/5 C. 21/25 D. 5/6 E. 13/1715 = 0.7333; B. 4/5 = 0.8; C. 21/25 = 0.84; D. 5/6 = 0.8333; E. 13/17 = 0.7647calculations but picked up the wrong answer.Inability toA painting which is 4 feet wide andAnswer: C. Explanation: The model did not fullyunderstand5 feet long is surrounded by aunderstand the question.convolutedrectangular frame 6 inches wide.questionsWhat percent of the area of thepicture and the frame is occupiedby the frame? A. 10 1/2 B. 20 C. 30D. 33 1/3 E. 50HallucinationsOf the following, which group ofAnswer: C. Explanation: Among the giveninvertebrates is apparently mostchoices, Cnidaria (jellyfish, corals, and seaclosely related to primitiveanemones) is the group of invertebrates that isvertebrates? A. Annelida B.most closely related to primitive vertebrates.Mollusca C. Cnidaria D. ArthropodaCnidarians and primitive vertebrates share aE. Echinodermatacommon ancestor and belong to the samelineage, known as the Eumetazoa. Both groupsexhibit tissue-level organization and have arudimentary nervous system, suggesting acloser evolutionary relationship.
AcknowledgmentsWe would like to acknowledge support from the Center for Learning Health System Sciences, a partnership between the Medical School and School of Public Health at the University of Minnesota.Funding: This work was partially supported by the National Institutes of Health's National Center for Complementary and Integrative Health under grant number R01AT009457, National Institute on Aging under grant number R01AG078154, and National Cancer Institute under grant number R01CA287413.The content is solely the responsibility of the authors and does not represent the official views of the National Institutes of Health.Data AvailabilityThe data used in this study, including the sample questions from the DAT, are publicly available and were sourced from the American Dental Association ( https://www.ada.org/ ).No additional datasets were generated or analyzed during the study.
Healthcare generative artificial intelligence tools in medical diagnosis, treatment, and prognosis. J Horak, A Novak, L C Voumik, 2023Contemp Read Law Soc Justice15</p>
<p>Generative AI in medicine and healthcare: Promises, opportunities and challenges. P Zhang, Kamel Boulos, M N , Future Internet. 1592862023</p>
<p>Artificial intelligence in orthopaedics: Can chat generative pre-trained transformer (chatGPT) pass section 1 of the fellowship of the royal college of surgeons (trauma &amp; orthopaedics) examination?. R Cuthbert, A I Simpson, Postgrad Med J. 992023. 1176</p>
<p>How does chatGPT perform on the medical licensing exams? The implications of large language models for medical education and knowledge assessment. A Gilson, C Safranek, T Huang, V Socrates, Chi L Taylor, R A Chartash, D , 10.1101/2022.12.23.22283901medRxiv. 2022</p>
<p>Performance of chatGPT on USMLE: Potential for ai-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C ElepaÃ±o, M Madriaga, R Aggabao, G Diaz-Candido, J Maningo, PLOS Digit Health. 222023Article e0000198</p>
<p>ChatGPT takes on the European exam in core cardiology: An artificial intelligence success story?. I Skalidis, A Cagnina, W Luangphiphat, T Mahendiran, O Muller, Abbe E Fournier, S , Eur Heart J Digit Health. 432023</p>
<p>Performance of chatGPT on Uk standardized admission tests: Insights from the bmat, tmua, lnat, and tsa examinations. P Giannos, O Delardas, JMIR Med Educ. 9e477372023</p>
<p>. Gpt Openai, version 3.5 (2022</p>
<p>. Gpt Openai, 20234</p>
<p>. Gpt Openai, 20244</p>
<p>. Gpt Openai, 2024</p>
<p>. Google, Bard, 2023</p>
<p>. A I Mistral, Mistral-Large, version 0.3 7-B (2023</p>
<p>. Claude Anthropic, 2023</p>
<p>Towards building multilingual language model for medicine. P Qiu, C Wu, X Zhang, Lin W Wang, H Zhang, Y Wang, Y Xie, W , Nat Commun. 15183842024</p>
<p>. T Brian, Dentalgpt, 2025</p>
<p>. D M Michael, Medgpt , 2025</p>
<p>. A I Meta, Llama, 202327B</p>
<p>. Ai; Meta, Llama, 2023213B</p>
<p>. A I Meta, Llama, 2023270B</p>
<p>. A I Meta, Llama, 202438B</p>
<p>. A I Meta, Llama, 2024370B</p>
<p>A revision of bloom's taxonomy: An overview. Theory Pract. D R Krathwohl, 200241</p>
<p>Role of comprehension on performance at higher levels of bloom's taxonomy: Findings from assessments of healthcare professional students. A-Ma Verenna, K A Noble, H E Pearson, S M Miller, Anat Sci Educ. 1152018</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Adv Neural Inf Proces Syst. 3318772020</p>
<p>Challenges and opportunities of artificial intelligence in education in a global context. A De Oliveira Silva, Santos Janes, D , Rev Artif Intell Educ. 4e12023</p>
<p>Fine-tuning large language models for domain-specific machine translation. J Zheng, H Hong, F Liu, X Wang, J Su, Y Liang, S Wu, 10.48550/arXiv.2402.15061arXiv. 2024</p>
<p>Fine-tuning and utilization methods of domainspecific llms. C Jeong, 10.13088/jiis.2024.30.1.093arXiv. 2024</p>
<p>Reasoning limitations of multimodal large language models: A case study of bongard problems. M MaÅ‚kiÅ„ski, M Michalak, T TrzciÅ„ski, 10.48550/arXiv.2411.01173arXiv. 2024</p>
<p>J-Y Yao, K-P Ning, Z-H Liu, M-N Ning, L Yuan, L , 10.48550/arXiv.2310.01469arXiv. 2023LLM lies: Hallucinations are not bugs, but features as adversarial examples. </p>
<p>Hallucination is inevitable: An innate limitation of large language models. Z Xu, S Jain, M Kankanhalli, 10.48550/arXiv.2401.11817arXiv. 2024</p>
<p>A survey of hallucination in large foundation models. V Rawte, A Sheth, A Das, 10.48550/arXiv.2309.05922arXiv. 2023</p>
<p>Enhancing dietary supplement question answer via retrieval-augmented generation (RAG) with LLM. medRxiv. Y Hou, R Zhang, 10.1101/2024.09.11.243135132024</p>
<p>From answers to insights: Unveiling the strengths and limitations of chatgpt and biomedical knowledge graphs. Y Hou, J Yeung, H Xu, C Su, F Wang, R Zhang, Res Sq. 2023Article rs.3.rs-3185632</p>
<p>Knownet: Guided health information seeking from llms via knowledge graph integration. Y Yan, Y Hou, Y Xiao, R Zhang, Q Wang, IEEE Trans Vis Comput Graph. 2024</p>
<p>Repurposing non-pharmacological interventions for alzheimer's disease through link prediction on biomedical literature. Sci Rep. Y Xiao, Y Hou, H Zhou, G Diallo, M Fiszman, J Wolfson, L Zhou, H Kilicoglu, Y Chen, C Su, 2024148693</p>
<p>Biomedical discovery through the integrative biomedical knowledge hub (iBKH). C Su, Y Hou, M Zhou, S Rajendran, Jrma Maasch, Z Abedi, H Zhang, Z Bai, A Cuturrufo, W Guo, Iscience. 2641064602023</p>
<p>Protocol to implement a computational pipeline for biomedical discovery based on a biomedical knowledge graph. C Su, Y Hou, M Levin, R Zhang, F Wang, STAR Protoc. 441026662023</p>
<p>Biomedrag: A retrieval augmented large language model for biomedicine. M Li, H Kilicoglu, H Xu, R Zhang, J Biomed Inform. 1621047692025</p>
<p>One LLM is not enough: Harnessing the power of ensemble learning for medical question answering. medRxiv. H Yang, M Li, H Hu, Y Xiao, Q Fang, R Zhang, 10.1101/2023.12.21.233003802023</p>            </div>
        </div>

    </div>
</body>
</html>
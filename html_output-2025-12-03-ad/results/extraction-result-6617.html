<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6617 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6617</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6617</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-37088dec26231bc5a4937054ebc862bb83a3db4d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/37088dec26231bc5a4937054ebc862bb83a3db4d" target="_blank">Neural Episodic Control</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them, and shows across a wide range of environments that the agent learns significantly faster than other state-of-the-art, general purpose deep reinforcementlearning agents.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6617.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6617.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NEC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Episodic Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning agent that augments a convolutional embedding network with per-action Differentiable Neural Dictionaries (DNDs) that store key (state embedding) → value (Q estimate) entries, enabling very fast, non-parametric updates and k-nearest-neighbour readout for data-efficient learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Episodic Control (NEC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Convolutional network produces fixed-length keys (state embeddings); for each action an append-only Differentiable Neural Dictionary (DND) stores those keys paired with Q-value estimates. At decision time the DND for each action is queried with the current key; the top-p nearest stored keys are weighted by a kernel and their values aggregated to produce Q(s,a). New experiences are always written (append-only) and exact-key matches are updated using a Q-learning-like incremental update.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Differentiable Neural Dictionary (episodic key-value store, per-action)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Keys: slow-changing CNN embeddings of states; Values: Q-value estimates (N-step Q returns)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>k-nearest-neighbours lookup (top-p, p=50), kernel-weighted average of stored values; approximate nearest neighbours via kd-tree for scalability; writes are append-only, exact-key updates use tabular Q-style incremental update with learning rate α; overwrite least-recently-used neighbour when capacity reached.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Atari Learning Environment (57 Atari 2600 games)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Reinforcement learning / control (pixel-based game playing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Median human-normalized score across games: 54.6% at 10M frames (Table 1); Mean human-normalized score: 99.8% at 10M frames (Table 2). Also reported raw per-game scores at 10M frames (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No within-agent ablation disabling the DND was reported. Compared to parametric baselines (agents without DND) at 10M frames: Nature DQN median 15.7%, A3C median 3.6%, Prioritised Replay median 22.4% (see Table 1) — these are cross-agent comparisons rather than NEC-without-memory ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human-normalized score (percentage), reported as median and mean across 57 games at various frame budgets (e.g., 1M,2M,4M,10M,20M,40M)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Memory can grow large (they store up to 5×10^5 memories per action in experiments), so they use approximate nearest neighbour kd-tree lookup; DNDs allow very fast value updates (alleviating slow SGD) but increase memory footprint and require approximate NN infrastructure; computational cost of NN lookups mitigated via kd-trees; NEC is more data-efficient early but in long training (40M frames) Prioritised Replay can outperform NEC on average.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No direct ablation of the memory was reported. NEC performs markedly better in early/data-limited regimes but does not necessarily achieve higher final performance than some parametric baselines (e.g., Prioritised Replay at 40M frames). Memory grows with time and must be capped/managed; the architecture intentionally does not learn when to write (writes all experiences), which could be suboptimal for some tasks; keys rely on a slow-changing CNN embedding so representation drift could affect retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adrià Puigdomènech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell. Neural Episodic Control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Episodic Control', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6617.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6617.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFEC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-Free Episodic Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An episodic, non-parametric RL method that estimates action values via local regression (mean of k-nearest neighbours) over stored past experiences (state → return) using an embedding; used as a strong episodic baseline for data-efficient RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model-free episodic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Model-Free Episodic Control (MFEC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Non-parametric episodic controller that stores past states and associated returns in memory (an episodic table) and estimates Q(s,a) as the mean (or direct return for exact matches) of the k nearest stored experiences in an embedding space; embedding may be random projections or learned separately (VAE latent in other variants).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Non-parametric episodic memory / k-nearest-neighbours buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored entries: state embeddings (random projections or VAE latent) paired with returns (Monte Carlo returns)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>k-nearest-neighbours search in embedding space; estimation by averaging neighbours' returns (or exact-match return), no learned write controller (append entries), approximate NN used in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Atari Learning Environment (subset and full 57 games for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Reinforcement learning / control (pixel-based games)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Median human-normalized score across games: 45.4% at 10M frames (Table 1); Mean human-normalized score: 85.0% at 10M frames (Table 2). Per-game raw scores reported in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No within-agent ablation; compared to parametric baselines (e.g., Nature DQN median 15.7% at 10M frames) — comparisons are cross-agent.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human-normalized score (percentage), median and mean across games</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>MFEC is highly data-efficient early; depends on choice of embedding (random projections often worked better than VAE latent in their experiments); memory-based nearest-neighbour methods can be sensitive to embedding quality and may store many examples (memory footprint).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Embedding choices matter: reward-agnostic embeddings (e.g., VAE latent) may focus on irrelevant visual details and hurt value interpolation; simpler embeddings can miss small but critical visual cues; MFEC (like NEC) can be less competitive in longer training relative to some parametric methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Episodic Control', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6617.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6617.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A3C+LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asynchronous Advantage Actor-Critic with LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A3C policy-gradient RL algorithm augmented with an LSTM layer to provide an internal recurrent memory used for partial observability; noted in this paper to have been tried by Mnih et al. but provided little improvement on Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A3C with LSTM (recurrent memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy and value networks trained with asynchronous actor-critic; an LSTM recurrent layer provides a learned working-memory/state summarization that is updated at each time step via recurrent computation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent internal memory (LSTM hidden state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recurrent hidden state vectors summarize recent history</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned recurrent update (LSTM gating), memory read is via network's forward pass; writes/updates are governed by LSTM dynamics learned through backpropagation through time (truncated BPTT).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Atari Learning Environment (as reported by Mnih et al., mentioned in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Reinforcement learning / partially-observable control</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>RNN-based memories (LSTM) are trained via truncated backpropagation through time and thus subject to slow learning similar to other parametric networks; authors note adding LSTM to A3C did not significantly impact Atari performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In Atari domains reported here, adding an LSTM did not lead to significant performance gains; RNN memories tend to be 'working memory' reset per episode and can be slow to learn to write/use effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Episodic Control', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6617.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6617.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Neural Computer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture that augments a controller network with a large external differentiable memory and learned attention-based read/write mechanisms; mentioned as related work and as having been adapted to RL but typically reset per episode in RL uses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybrid computing using a neural network with dynamic external memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A neural controller (often an RNN) learns how to read from and write to an external memory matrix using differentiable attention mechanisms; read/write locations and gating are learned through gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Differentiable external memory (learned matrix with attention-based read/write)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory matrix of vectors; controller learns addressing weights and contents</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned attention-based read and write heads (differentiable), trained with backpropagation through time; writes are learned (not simply append-only).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>General sequence and memory tasks; adaptations to reinforcement learning have been explored (cited works), but no RL experiments are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Such learned-memory architectures are typically reset at episode boundaries in RL settings (behave as working memory) and are trained via truncated BPTT, making them subject to slow learning and scalability limits; learning when to write can be slow.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not persistent across episodes by common RL adaptations; slow to learn write policies; expensive to scale compared to the simpler append-only DND approach proposed in NEC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 2016.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Episodic Control', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6617.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6617.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Nets / FRMQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Networks and (FR)MQN variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Memory-augmented architectures (e.g., Memory Networks, Key-Value Memory Networks, FRMQN) that store context vectors and permit differentiable querying (attention/cosine similarity); some adaptations have been used in RL but typically use per-episode/reset working memories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end memory networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory Networks / FRMQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architectures that store a set of memory slots (e.g., key-value pairs) and use attention (e.g., cosine similarity) to compute weighted readouts; FRMQN applies similar ideas to RL by integrating memories into Q-networks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Key-value memory network / attention-based external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored memory vectors (keys and values) representing past observations or context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Attention-like query over memory slots (cosine similarity or learned attention), often using top-k retrieval; writes/maintenance typically learned or episodic and often reset each episode in RL implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Supervised tasks in original memory network papers; FRMQN applied to RL tasks (e.g., Minecraft in Oh et al., 2016) — cited but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Sequence reasoning, question answering (original), and RL (FRMQN) for perception-action tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>When adapted to RL these memories are commonly wiped per episode (working memory), which suits partially observable tasks but makes them different from the persistent episodic memory NEC targets; learning write/control policies can be slow.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Resetting memory per episode limits cross-episode persistence; training to learn when/how to write can be slow; reported FRMQN/DRQN results were not available for full Atari comparison in this paper and DRQN was reported elsewhere to have lower performance than Prioritised Replay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in Neural Information Processing Systems, 2015; Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, et al. Control of memory, active perception, and action in Minecraft. In Proceedings of ICML, 2016.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Episodic Control', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6617.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6617.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KaiserKV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kaiser et al. key-value memory layer (Learning to Remember Rare Events)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable key-value memory layer that can be plugged into networks: uses k-nearest neighbors with cosine similarity and a moving-average update rule to store and retrieve examples, applied to supervised rare-event tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to remember rare events</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Kaiser et al. key-value memory layer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A differentiable memory layer that stores key-value pairs and retrieves a weighted average of the values of the k most similar memories via cosine similarity; updates to stored values use a moving-average style rule.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Differentiable key-value memory layer (plug-in)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Keys: feature vectors; Values: target vectors or labels (task dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Cosine-similarity based retrieval of top-k neighbours; moving-average update of stored values; differentiable so it can be trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Supervised rare-event classification/recall tasks in the original work (cited here); not directly evaluated on Atari in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Memory-augmented supervised learning / few-shot recall</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Provides a differentiable mechanism for remembering and updating rare events; paper notes similarity of moving-average updates to NEC's update strategy but Kaiser et al. did not evaluate in RL within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Original evaluations were supervised tasks; adaptation to RL was not presented in that work as discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Lukasz Kaiser, Ofir Nachum, Aurko Roy, Samy Bengio. Learning to remember rare events. 2016.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Episodic Control', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6617.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6617.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DQN Replay Buffer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DQN experience replay buffer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The large-scale replay buffer used by DQN stores transitions (s,a,r,s') and is sampled to create uncorrelated minibatches for SGD; discussed in the paper as a form of large-scale memory used to stabilise training and enable off-policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-level control through deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN (replay buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Parametric Q-network trained via Q-learning using a large replay memory of past transitions; training samples are drawn uniformly (or prioritized) from the buffer to break correlations and stabilise learning; target network is used to stabilise targets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Replay buffer (experience memory of transitions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored tuples of (state pixels, action, reward, next state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Random minibatch sampling (uniform or prioritized) from a large buffer; used as training data to optimise network via SGD; not used directly at action-selection time (except via learned parametric network).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Atari Learning Environment (57 games) — DQN baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Reinforcement learning / control</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Nature DQN median human-normalized score across games: 15.7% at 10M frames (Table 1); mean 51.3% at 10M frames (Table 2); per-game results in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No direct within-paper ablation disabling the replay buffer; authors discuss that not using replay (or using on-policy methods) can alter speed and stability of learning.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human-normalized score (percentage), median and mean across games</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Replay buffer aids stability but slows reward propagation (targets are backed up slowly) and typically requires target networks and reward clipping for stable training; buffers can be very large (commonly millions of transitions) increasing memory footprint.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Replay-based methods can be data-inefficient early because neural network parameter updates are slow; sparse rewards and slow propagation of reward signals are identified bottlenecks addressed by NEC's episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Episodic Control', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Model-free episodic control <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 2)</em></li>
                <li>End-to-end memory networks <em>(Rating: 2)</em></li>
                <li>Learning to remember rare events <em>(Rating: 2)</em></li>
                <li>Asynchronous methods for deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Control of memory, active perception, and action in Minecraft <em>(Rating: 2)</em></li>
                <li>Matching networks for one shot learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6617",
    "paper_id": "paper-37088dec26231bc5a4937054ebc862bb83a3db4d",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "NEC",
            "name_full": "Neural Episodic Control",
            "brief_description": "A reinforcement learning agent that augments a convolutional embedding network with per-action Differentiable Neural Dictionaries (DNDs) that store key (state embedding) → value (Q estimate) entries, enabling very fast, non-parametric updates and k-nearest-neighbour readout for data-efficient learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Neural Episodic Control (NEC)",
            "agent_description": "Convolutional network produces fixed-length keys (state embeddings); for each action an append-only Differentiable Neural Dictionary (DND) stores those keys paired with Q-value estimates. At decision time the DND for each action is queried with the current key; the top-p nearest stored keys are weighted by a kernel and their values aggregated to produce Q(s,a). New experiences are always written (append-only) and exact-key matches are updated using a Q-learning-like incremental update.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Differentiable Neural Dictionary (episodic key-value store, per-action)",
            "memory_representation": "Keys: slow-changing CNN embeddings of states; Values: Q-value estimates (N-step Q returns)",
            "memory_access_mechanism": "k-nearest-neighbours lookup (top-p, p=50), kernel-weighted average of stored values; approximate nearest neighbours via kd-tree for scalability; writes are append-only, exact-key updates use tabular Q-style incremental update with learning rate α; overwrite least-recently-used neighbour when capacity reached.",
            "task_name": "Atari Learning Environment (57 Atari 2600 games)",
            "task_category": "Reinforcement learning / control (pixel-based game playing)",
            "performance_with_memory": "Median human-normalized score across games: 54.6% at 10M frames (Table 1); Mean human-normalized score: 99.8% at 10M frames (Table 2). Also reported raw per-game scores at 10M frames (Table 3).",
            "performance_without_memory": "No within-agent ablation disabling the DND was reported. Compared to parametric baselines (agents without DND) at 10M frames: Nature DQN median 15.7%, A3C median 3.6%, Prioritised Replay median 22.4% (see Table 1) — these are cross-agent comparisons rather than NEC-without-memory ablations.",
            "has_comparative_results": false,
            "performance_metric": "Human-normalized score (percentage), reported as median and mean across 57 games at various frame budgets (e.g., 1M,2M,4M,10M,20M,40M)",
            "tradeoffs_reported": "Memory can grow large (they store up to 5×10^5 memories per action in experiments), so they use approximate nearest neighbour kd-tree lookup; DNDs allow very fast value updates (alleviating slow SGD) but increase memory footprint and require approximate NN infrastructure; computational cost of NN lookups mitigated via kd-trees; NEC is more data-efficient early but in long training (40M frames) Prioritised Replay can outperform NEC on average.",
            "limitations_or_failure_cases": "No direct ablation of the memory was reported. NEC performs markedly better in early/data-limited regimes but does not necessarily achieve higher final performance than some parametric baselines (e.g., Prioritised Replay at 40M frames). Memory grows with time and must be capped/managed; the architecture intentionally does not learn when to write (writes all experiences), which could be suboptimal for some tasks; keys rely on a slow-changing CNN embedding so representation drift could affect retrieval.",
            "citation": "Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adrià Puigdomènech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell. Neural Episodic Control.",
            "uuid": "e6617.0",
            "source_info": {
                "paper_title": "Neural Episodic Control",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "MFEC",
            "name_full": "Model-Free Episodic Control",
            "brief_description": "An episodic, non-parametric RL method that estimates action values via local regression (mean of k-nearest neighbours) over stored past experiences (state → return) using an embedding; used as a strong episodic baseline for data-efficient RL.",
            "citation_title": "Model-free episodic control",
            "mention_or_use": "use",
            "agent_name": "Model-Free Episodic Control (MFEC)",
            "agent_description": "Non-parametric episodic controller that stores past states and associated returns in memory (an episodic table) and estimates Q(s,a) as the mean (or direct return for exact matches) of the k nearest stored experiences in an embedding space; embedding may be random projections or learned separately (VAE latent in other variants).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Non-parametric episodic memory / k-nearest-neighbours buffer",
            "memory_representation": "Stored entries: state embeddings (random projections or VAE latent) paired with returns (Monte Carlo returns)",
            "memory_access_mechanism": "k-nearest-neighbours search in embedding space; estimation by averaging neighbours' returns (or exact-match return), no learned write controller (append entries), approximate NN used in practice.",
            "task_name": "Atari Learning Environment (subset and full 57 games for comparison)",
            "task_category": "Reinforcement learning / control (pixel-based games)",
            "performance_with_memory": "Median human-normalized score across games: 45.4% at 10M frames (Table 1); Mean human-normalized score: 85.0% at 10M frames (Table 2). Per-game raw scores reported in Table 3.",
            "performance_without_memory": "No within-agent ablation; compared to parametric baselines (e.g., Nature DQN median 15.7% at 10M frames) — comparisons are cross-agent.",
            "has_comparative_results": false,
            "performance_metric": "Human-normalized score (percentage), median and mean across games",
            "tradeoffs_reported": "MFEC is highly data-efficient early; depends on choice of embedding (random projections often worked better than VAE latent in their experiments); memory-based nearest-neighbour methods can be sensitive to embedding quality and may store many examples (memory footprint).",
            "limitations_or_failure_cases": "Embedding choices matter: reward-agnostic embeddings (e.g., VAE latent) may focus on irrelevant visual details and hurt value interpolation; simpler embeddings can miss small but critical visual cues; MFEC (like NEC) can be less competitive in longer training relative to some parametric methods.",
            "citation": "Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.",
            "uuid": "e6617.1",
            "source_info": {
                "paper_title": "Neural Episodic Control",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "A3C+LSTM",
            "name_full": "Asynchronous Advantage Actor-Critic with LSTM",
            "brief_description": "A3C policy-gradient RL algorithm augmented with an LSTM layer to provide an internal recurrent memory used for partial observability; noted in this paper to have been tried by Mnih et al. but provided little improvement on Atari.",
            "citation_title": "Long short-term memory",
            "mention_or_use": "mention",
            "agent_name": "A3C with LSTM (recurrent memory)",
            "agent_description": "Policy and value networks trained with asynchronous actor-critic; an LSTM recurrent layer provides a learned working-memory/state summarization that is updated at each time step via recurrent computation.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Recurrent internal memory (LSTM hidden state)",
            "memory_representation": "Recurrent hidden state vectors summarize recent history",
            "memory_access_mechanism": "Learned recurrent update (LSTM gating), memory read is via network's forward pass; writes/updates are governed by LSTM dynamics learned through backpropagation through time (truncated BPTT).",
            "task_name": "Atari Learning Environment (as reported by Mnih et al., mentioned in paper)",
            "task_category": "Reinforcement learning / partially-observable control",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "RNN-based memories (LSTM) are trained via truncated backpropagation through time and thus subject to slow learning similar to other parametric networks; authors note adding LSTM to A3C did not significantly impact Atari performance.",
            "limitations_or_failure_cases": "In Atari domains reported here, adding an LSTM did not lead to significant performance gains; RNN memories tend to be 'working memory' reset per episode and can be slow to learn to write/use effectively.",
            "citation": "Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016.",
            "uuid": "e6617.2",
            "source_info": {
                "paper_title": "Neural Episodic Control",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "DNC",
            "name_full": "Differentiable Neural Computer",
            "brief_description": "A neural architecture that augments a controller network with a large external differentiable memory and learned attention-based read/write mechanisms; mentioned as related work and as having been adapted to RL but typically reset per episode in RL uses.",
            "citation_title": "Hybrid computing using a neural network with dynamic external memory",
            "mention_or_use": "mention",
            "agent_name": "Differentiable Neural Computer (DNC)",
            "agent_description": "A neural controller (often an RNN) learns how to read from and write to an external memory matrix using differentiable attention mechanisms; read/write locations and gating are learned through gradient descent.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Differentiable external memory (learned matrix with attention-based read/write)",
            "memory_representation": "Memory matrix of vectors; controller learns addressing weights and contents",
            "memory_access_mechanism": "Learned attention-based read and write heads (differentiable), trained with backpropagation through time; writes are learned (not simply append-only).",
            "task_name": null,
            "task_category": "General sequence and memory tasks; adaptations to reinforcement learning have been explored (cited works), but no RL experiments are reported in this paper.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Such learned-memory architectures are typically reset at episode boundaries in RL settings (behave as working memory) and are trained via truncated BPTT, making them subject to slow learning and scalability limits; learning when to write can be slow.",
            "limitations_or_failure_cases": "Not persistent across episodes by common RL adaptations; slow to learn write policies; expensive to scale compared to the simpler append-only DND approach proposed in NEC.",
            "citation": "Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 2016.",
            "uuid": "e6617.3",
            "source_info": {
                "paper_title": "Neural Episodic Control",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Memory Nets / FRMQN",
            "name_full": "Memory Networks and (FR)MQN variants",
            "brief_description": "Memory-augmented architectures (e.g., Memory Networks, Key-Value Memory Networks, FRMQN) that store context vectors and permit differentiable querying (attention/cosine similarity); some adaptations have been used in RL but typically use per-episode/reset working memories.",
            "citation_title": "End-to-end memory networks",
            "mention_or_use": "mention",
            "agent_name": "Memory Networks / FRMQN",
            "agent_description": "Architectures that store a set of memory slots (e.g., key-value pairs) and use attention (e.g., cosine similarity) to compute weighted readouts; FRMQN applies similar ideas to RL by integrating memories into Q-networks.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Key-value memory network / attention-based external memory",
            "memory_representation": "Stored memory vectors (keys and values) representing past observations or context",
            "memory_access_mechanism": "Attention-like query over memory slots (cosine similarity or learned attention), often using top-k retrieval; writes/maintenance typically learned or episodic and often reset each episode in RL implementations.",
            "task_name": "Supervised tasks in original memory network papers; FRMQN applied to RL tasks (e.g., Minecraft in Oh et al., 2016) — cited but not evaluated in this paper.",
            "task_category": "Sequence reasoning, question answering (original), and RL (FRMQN) for perception-action tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "When adapted to RL these memories are commonly wiped per episode (working memory), which suits partially observable tasks but makes them different from the persistent episodic memory NEC targets; learning write/control policies can be slow.",
            "limitations_or_failure_cases": "Resetting memory per episode limits cross-episode persistence; training to learn when/how to write can be slow; reported FRMQN/DRQN results were not available for full Atari comparison in this paper and DRQN was reported elsewhere to have lower performance than Prioritised Replay.",
            "citation": "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in Neural Information Processing Systems, 2015; Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, et al. Control of memory, active perception, and action in Minecraft. In Proceedings of ICML, 2016.",
            "uuid": "e6617.4",
            "source_info": {
                "paper_title": "Neural Episodic Control",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "KaiserKV",
            "name_full": "Kaiser et al. key-value memory layer (Learning to Remember Rare Events)",
            "brief_description": "A differentiable key-value memory layer that can be plugged into networks: uses k-nearest neighbors with cosine similarity and a moving-average update rule to store and retrieve examples, applied to supervised rare-event tasks.",
            "citation_title": "Learning to remember rare events",
            "mention_or_use": "mention",
            "agent_name": "Kaiser et al. key-value memory layer",
            "agent_description": "A differentiable memory layer that stores key-value pairs and retrieves a weighted average of the values of the k most similar memories via cosine similarity; updates to stored values use a moving-average style rule.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Differentiable key-value memory layer (plug-in)",
            "memory_representation": "Keys: feature vectors; Values: target vectors or labels (task dependent)",
            "memory_access_mechanism": "Cosine-similarity based retrieval of top-k neighbours; moving-average update of stored values; differentiable so it can be trained end-to-end.",
            "task_name": "Supervised rare-event classification/recall tasks in the original work (cited here); not directly evaluated on Atari in this paper.",
            "task_category": "Memory-augmented supervised learning / few-shot recall",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Provides a differentiable mechanism for remembering and updating rare events; paper notes similarity of moving-average updates to NEC's update strategy but Kaiser et al. did not evaluate in RL within this work.",
            "limitations_or_failure_cases": "Original evaluations were supervised tasks; adaptation to RL was not presented in that work as discussed in this paper.",
            "citation": "Lukasz Kaiser, Ofir Nachum, Aurko Roy, Samy Bengio. Learning to remember rare events. 2016.",
            "uuid": "e6617.5",
            "source_info": {
                "paper_title": "Neural Episodic Control",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "DQN Replay Buffer",
            "name_full": "DQN experience replay buffer",
            "brief_description": "The large-scale replay buffer used by DQN stores transitions (s,a,r,s') and is sampled to create uncorrelated minibatches for SGD; discussed in the paper as a form of large-scale memory used to stabilise training and enable off-policy learning.",
            "citation_title": "Human-level control through deep reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "DQN (replay buffer)",
            "agent_description": "Parametric Q-network trained via Q-learning using a large replay memory of past transitions; training samples are drawn uniformly (or prioritized) from the buffer to break correlations and stabilise learning; target network is used to stabilise targets.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Replay buffer (experience memory of transitions)",
            "memory_representation": "Stored tuples of (state pixels, action, reward, next state)",
            "memory_access_mechanism": "Random minibatch sampling (uniform or prioritized) from a large buffer; used as training data to optimise network via SGD; not used directly at action-selection time (except via learned parametric network).",
            "task_name": "Atari Learning Environment (57 games) — DQN baseline",
            "task_category": "Reinforcement learning / control",
            "performance_with_memory": "Nature DQN median human-normalized score across games: 15.7% at 10M frames (Table 1); mean 51.3% at 10M frames (Table 2); per-game results in Table 3.",
            "performance_without_memory": "No direct within-paper ablation disabling the replay buffer; authors discuss that not using replay (or using on-policy methods) can alter speed and stability of learning.",
            "has_comparative_results": false,
            "performance_metric": "Human-normalized score (percentage), median and mean across games",
            "tradeoffs_reported": "Replay buffer aids stability but slows reward propagation (targets are backed up slowly) and typically requires target networks and reward clipping for stable training; buffers can be very large (commonly millions of transitions) increasing memory footprint.",
            "limitations_or_failure_cases": "Replay-based methods can be data-inefficient early because neural network parameter updates are slow; sparse rewards and slow propagation of reward signals are identified bottlenecks addressed by NEC's episodic memory.",
            "citation": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015.",
            "uuid": "e6617.6",
            "source_info": {
                "paper_title": "Neural Episodic Control",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Model-free episodic control",
            "rating": 2
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 2
        },
        {
            "paper_title": "End-to-end memory networks",
            "rating": 2
        },
        {
            "paper_title": "Learning to remember rare events",
            "rating": 2
        },
        {
            "paper_title": "Asynchronous methods for deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Control of memory, active perception, and action in Minecraft",
            "rating": 2
        },
        {
            "paper_title": "Matching networks for one shot learning",
            "rating": 1
        }
    ],
    "cost": 0.019618749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural Episodic Control</h1>
<p>Alexander Pritzel<br>Benigno Uria<br>Sriram Srinivasan<br>Adrià Puigdomènech<br>Oriol Vinyals<br>Demis Hassabis<br>Daan Wierstra<br>Charles Blundell<br>DeepMind, London UK</p>
<p>APRITZEL@GOOGLE.COM<br>BURIA@GOOGLE.COM<br>SRSRINIVASAN@GOOGLE.COM<br>ADRIAP@GOOGLE.COM<br>VINYALS@GOOGLE.COM<br>DEMISHASSABIS@GOOGLE.COM<br>WIERSTRA@GOOGLE.COM<br>CBLUNDELL@GOOGLE.COM</p>
<h2>Abstract</h2>
<p>Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.</p>
<h2>1. Introduction</h2>
<p>Deep reinforcement learning agents have achieved state-of-the-art results in a variety of complex environments (Mnih et al., 2015; 2016), often surpassing human performance (Silver et al., 2016). Although the final performance of these agents is impressive, these techniques usually require several orders of magnitude more interactions with their environment than a human in order to reach an equivalent level of expected performance. For example, in the Atari 2600 set of environments (Bellemare et al., 2013), deep Q-networks (Mnih et al., 2016) require more than 200 hours of gameplay in order to achieve scores similar to those a human player achieves after two hours (Lake et al., 2016).</p>
<p>The glacial learning speed of deep reinforcement learning has several plausible explanations and in this work we focus on addressing these:</p>
<ol>
<li>Stochastic gradient descent optimisation requires the use of small learning rates. Due to the global approximation nature of neural networks, high learning rates cause catastrophic interference (McCloskey \&amp; Cohen, 1989). Low learning rates mean that experience can only be incorporated into a neural network slowly.</li>
<li>Environments with a sparse reward signal can be difficult for a neural network to model as there may be very few instances where the reward is non-zero. This can be viewed as a form of class imbalance where low-reward samples outnumber high-reward samples by an unknown number. Consequently, the neural network disproportionately underperforms at predicting larger rewards, making it difficult for an agent to take the most rewarding actions.</li>
<li>Reward signal propagation by value-bootstrapping techniques, such as Q-learning, results in reward information being propagated one step at a time through the history of previous interactions with the environment. This can be fairly efficient if updates happen in reverse order in which the transitions occur. However, in order to train on uncorrelated minibatches DQN-style, algorithms train on randomly selected transitions, and, in order to further stabilise training, require the use of a slowly updating target network further slowing down reward propagation.</li>
</ol>
<p>In this work we shall focus on addressing the three concerns listed above; we must note, however, that other recent advances in exploration (Osband et al., 2016), hierarchical reinforcement learning (Vezhnevets et al., 2016) and transfer learning (Rusu et al., 2016; Fernando et al., 2017) also make substantial contributions to improving data efficiency in deep reinforcement learning over baseline agents.</p>
<p>In this paper we propose Neural Episodic Control (NEC), a method which tackles the limitations of deep reinforcement learning listed above and demonstrates dramatic im-</p>
<p>provements on the speed of learning for a wide range of environments. Critically, our agent is able to rapidly latch onto highly successful strategies as soon as they are experienced, instead of waiting for many steps of optimisation (e.g., stochastic gradient descent) as is the case with DQN (Mnih et al., 2015) and A3C (Mnih et al., 2016).</p>
<p>Our work is in part inspired by the hypothesised role of the Hippocampus in decision making (Lengyel \&amp; Dayan, 2007; Blundell et al., 2016) and also by recent work on one-shot learning (Vinyals et al., 2016) and learning to remember rare events with neural networks (Kaiser et al., 2016). Our agent uses a semi-tabular representation of its experience of the environment possessing several of the features of episodic memory such as long term memory, sequentiality, and context-based lookups. The semi-tabular representation is an append-only memory that binds slow-changing keys to fast updating values and uses a context-based lookup on the keys to retrieve useful values during action selection by the agent. Thus the agent's memory operates in much the same way that traditional table-based RL methods map from state and action to value estimates. A unique aspect of the memory in contrast to other neural memory architectures for reinforcement learning (explained in more detail in Section 3) is that the values retrieved from the memory can be updated much faster than the rest of the deep neural network. This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton \&amp; Plaut, 1987), although the architecture we present is quite different. Another unique aspect of the memory is that unlike other memory architectures such as LSTM and the differentiable neural computer (DNC; Graves et al., 2016), our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al. (2016) where the memory is wiped at the end of each episode). Reading from this large memory is made efficient using kd-tree based nearest neighbour (Bentley, 1975).</p>
<p>The remainder of the paper is organised as follows: in Section 2 we review deep reinforcement learning, in Section 3 the Neural Episodic Control algorithm is described, in Section 4 we report experimental results in the Atari Learning Environment, in Section 5 we discuss other methods that use memory for reinforcement learning, and finally in Section 6 we outline future work and summarise the main advantages of the NEC algorithm.</p>
<h2>2. Deep Reinforcement Learning</h2>
<p>The action-value function of a reinforcement learning agent (Sutton \&amp; Barto, 1998) is defined as $Q^{\pi}(s, a)=$ $\mathbb{E}<em t="t">{\pi}\left[\sum</em> \mid s, a\right]$, where $a$ is the initial action taken by the agent in the initial state $s$ and the expectation denotes that the policy $\pi$ is followed thereafter. The discount factor $\gamma \in(0,1)$ trades off favouring short vs. long term rewards.} \gamma^{t} r_{t</p>
<p>Deep Q-Network agents (DQN; Mnih et al., 2015) use Qlearning (Watkins \&amp; Dayan, 1992) to learn a value function $Q\left(s_{t}, a_{t}\right)$ to rank which action $a_{t}$ is best to take in each state $s_{t}$ at step $t$. The agent then executes an $\epsilon$-greedy policy based upon this value function to trade-off exploration and exploitation: with probability $\epsilon$ the agent picks an action uniformly at random, otherwise it picks the action $a_{t}=$ $\arg \max <em t="t">{a} Q\left(s</em>, a\right)$.
In DQN, the action-value function $Q\left(s_{t}, a_{t}\right)$ is parameterised by a convolutional neural network that takes a 2D pixel representation of the state $s_{t}$ as input, and outputs a vector containing the value of each action at that state. When the agent observes a transition, DQN stores the $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$ tuple in a replay buffer, the contents of which are used for training. This neural network is trained by minimizing the squared error between the network's output and the $Q$-learning target $y_{t}=r_{t}+\gamma \max <em t_1="t+1">{a} \hat{Q}\left(s</em>, a\right)$ is an older version of the value network that is updated periodically. The use of a target network and uncorrelated samples from the replay buffer are critical for stable training.}, a\right)$, for a subset of transitions sampled at random from the replay buffer. The target network $\hat{Q}\left(s_{t+1</p>
<p>A number of extensions have been proposed that improve DQN. Double DQN (Van Hasselt et al., 2016) reduces bias on the target calculation. Prioritised Replay (Schaul et al., 2015b) further improves Double DQN by optimising the replay strategy. Several authors have proposed methods of improving reward propagation and the back up mechanism of $Q$ learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation. $\mathrm{Q}^{*}(\lambda)$ (Harutyunyan et al., 2016) and $\operatorname{Retrace}(\lambda)$ (Munos et al., 2016) change the form of the Q-learning target to incorporate on-policy samples and fluidly switch between on-policy learning and off-policy learning. Munos et al. (2016) show that by incorporating on-policy samples allows an agent to learn faster in Atari environments, indicating that reward propagation is indeed a bottleneck to efficiency in deep reinforcement learning.</p>
<p>A3C (Mnih et al., 2016) is another well known deep reinforcement learning algorithm that is very different from DQN. It is based upon a policy gradient, and learns both a policy and its associated value function, which is learned</p>
<p>entirely on-policy (similar to the $\lambda=1$ case of Q( $\lambda$ )). Interestingly, Mnih et al. (2016) also added an LSTM memory to the otherwise convolutional neural network architecture to give the agent a notion of memory, although this did not have significant impact on the performance on Atari games.</p>
<h2>3. Neural Episodic Control</h2>
<p>Our agent consists of three components: a convolutional neural network that processes pixel images $s$, a set of memory modules (one per action), and a final network that converts read-outs from the action memories into $Q(s, a)$ values. For the convolutional neural network we use the same architecture as DQN (Mnih et al., 2015).</p>
<h3>3.1. Differentiable Neural Dictionary</h3>
<p>For each action $a \in \mathcal{A}$, NEC has a simple memory module $M_{a}=\left(K_{a}, V_{a}\right)$, where $K_{a}$ and $V_{a}$ are dynamically sized arrays of vectors, each containing the same number of vectors. The memory module acts as an arbitrary association from keys to corresponding values, much like the dictionary data type found in programs. Thus we refer to this kind of memory module as a differentiable neural dictionary (DND). There are two operations possible on a DND: lookup and write, as depicted in Figure 1. Performing a lookup on a DND maps a key $h$ to an output value $o$ :</p>
<p>$$
o=\sum_{i} w_{i} v_{i}
$$</p>
<p>where $v_{i}$ is the $i$ th element of the array $V_{a}$ and</p>
<p>$$
w_{i}=k\left(h, h_{i}\right) / \sum_{j} k\left(h, h_{j}\right)
$$</p>
<p>where $h_{i}$ is the $i$ th element of the array $K_{a}$ and $k(x, y)$ is a kernel between vectors $x$ and $y$, e.g., Gaussian or inverse kernels. Thus the output of a lookup in a DND is a weighted sum of the values in the memory, whose weights are given by normalised kernels between the lookup key and the corresponding key in memory. To make queries into very large memories scalable we shall make two approximations in practice: firstly, we shall limit (1) to the top $p$-nearest neighbours (typically $p=50$ ). Secondly, we use an approximate nearest neighbours algorithm to perform the lookups, based upon kd-trees (Bentley, 1975).</p>
<p>After a DND is queried, a new key-value pair is written into the memory. The key written corresponds to the key that was looked up. The associated value is application-specific (below we specify the update for the NEC agent). Writes to a DND are append-only: keys and values are written to the memory by appending them onto the end of the arrays $K_{a}$ and $V_{a}$ respectively. If a key already exists in the memory, then its corresponding value is updated, rather than being duplicated.</p>
<p>Note that a DND is a differentiable version of the memory module described in Blundell et al. (2016). It is also a generalisation to the memory and lookup schemes described in (Vinyals et al., 2016; Kaiser et al., 2016) for classification.</p>
<h3>3.2. Agent Architecture</h3>
<p>Figure 2 shows a DND as part of the NEC agent for a single action, whilst Algorithm 1 describes the general outline of the NEC algorithm. The pixel state $s$ is processed by a convolutional neural network to produce a key $h$. The key $h$ is then used to lookup a value from the DND, yielding weights $w_{i}$ in the process for each element of the memory arrays. Finally, the output is a weighted sum of the values in the DND. The values in the DND, in the case of an NEC agent, are the $Q$ values corresponding to the state that originally resulted in the corresponding key-value pair to be written to the memory. Thus this architecture produces an estimate of $Q(s, a)$ for a single given action $a$. The architecture is replicated once for each action $a$ the agent can take, with the convolutional part of the network shared among each separate DND $M_{a}$. The NEC agent acts by taking the action with the highest $Q$-value estimate at each time step. In practice, we use $\epsilon$-greedy policy during training with a low $\epsilon$.</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span> Neural Episodic Control
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}:</span><span class="err">\</span><span class="p">)</span> replay memory<span class="o">.</span>
    <span class="err">\</span><span class="p">(</span>M_<span class="p">{</span>a<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="p">:</span> a DND for each action <span class="err">\</span><span class="p">(</span>a<span class="err">\</span><span class="p">)</span><span class="o">.</span>
    <span class="err">\</span><span class="p">(</span>N<span class="err">\</span><span class="p">)</span> <span class="p">:</span> horizon for <span class="err">\</span><span class="p">(</span>N<span class="err">\</span><span class="p">)</span><span class="o">-</span>step <span class="err">\</span><span class="p">(</span>Q<span class="err">\</span><span class="p">)</span> estimate<span class="o">.</span>
    for each episode do
        for <span class="err">\</span><span class="p">(</span><span class="ss">t</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="err">\</span>ldots<span class="p">,</span> T<span class="err">\</span><span class="p">)</span> do
            Receive observation <span class="err">\</span><span class="p">(</span>s_<span class="p">{</span>t<span class="p">}</span><span class="err">\</span><span class="p">)</span> from environment <span class="k">with</span> em-
            bedding <span class="err">\</span><span class="p">(</span>h<span class="err">\</span><span class="p">)</span><span class="o">.</span>
            Estimate <span class="err">\</span><span class="p">(</span>Q<span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a<span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> for each action <span class="err">\</span><span class="p">(</span>a<span class="err">\</span><span class="p">)</span> via <span class="p">(</span><span class="mi">1</span><span class="p">)</span> from <span class="err">\</span><span class="p">(</span>M_<span class="p">{</span>a<span class="p">}</span><span class="err">\</span><span class="p">)</span>
            <span class="err">\</span><span class="p">(</span>a_<span class="p">{</span>t<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>epsilon<span class="err">\</span><span class="p">)</span><span class="o">-</span>greedy policy based on <span class="err">\</span><span class="p">(</span>Q<span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a<span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
            Take action <span class="err">\</span><span class="p">(</span>a_<span class="p">{</span>t<span class="p">}</span><span class="err">\</span><span class="p">),</span> receive reward <span class="err">\</span><span class="p">(</span>r_<span class="p">{</span>t<span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
            Append <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>h<span class="p">,</span> Q<span class="err">^</span><span class="p">{(</span>N<span class="p">)}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a_<span class="p">{</span>t<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> to <span class="err">\</span><span class="p">(</span>M_<span class="p">{</span>a_<span class="p">{</span>t<span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
            Append <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a_<span class="p">{</span>t<span class="p">},</span> Q<span class="err">^</span><span class="p">{(</span>N<span class="p">)}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>t<span class="p">},</span> a_<span class="p">{</span>t<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> to <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
            Train on a random minibatch from <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>D<span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>
        end for
    end for
</code></pre></div>

<h3>3.3. Adding $(s, a)$ pairs to memory</h3>
<p>As an NEC agent acts, it continually adds new key-value pairs to its memory. Keys are appended to the memory of the corresponding action, taking the value of the query key $h$ encoded by the convolutional neural network. We now turn to the question of an appropriate corresponding value. In Blundell et al. (2016), Monte Carlo returns were written to memory. We found that a mixture of Monte Carlo returns (on-policy) and off-policy backups worked better and so for NEC we elect to use $N$-step $Q$-learning as in Mnih et al.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Illustration of operations on a Differentiable Neural Dictionary.
(2016) (see also Watkins, 1989; Peng \&amp; Williams, 1996). This adds the following $N$ on-policy rewards and bootstraps the sum of discounted rewards for the rest of the trajectory, off-policy. The $N$-step $Q$-value estimate is then</p>
<p>$$
Q^{(N)}\left(s_{t}, a\right)=\sum_{j=0}^{N-1} \gamma^{j} r_{t+j}+\gamma^{N} \max <em t_N="t+N">{a^{\prime}} Q\left(s</em>\right)
$$}, a^{\prime</p>
<p>The bootstrap term of (3), $\max <em t_N="t+N">{a^{\prime}} Q\left(s</em>$ for each action $a$ and taking the highest estimated $Q$-value returned. Note that the earliest such values can be added to memory is $N$ steps after a particular $(s, a)$ pair occurs.
When a state-action value is already present in a DND (i.e the exact same key $h$ is already in $K_{a}$ ), the corresponding value present in $V_{a}, Q_{i}$, is updated in the same way as the classic tabular $Q$-learning algorithm:}, a^{\prime}\right)$ is found by querying all memories $M_{a</p>
<p>$$
Q_{i} \leftarrow Q_{i}+\alpha\left(Q^{(N)}(s, a)-Q_{i}\right)
$$</p>
<p>where $\alpha$ is the learning rate of the $Q$ update. If the state is not already present $Q^{(N)}\left(s_{t}, a\right)$ is appended to $V_{a}$ and $h$ is appended to $K_{a}$. Note that our agent learns the value function in much the same way that a classic tabular $Q$-learning agent does, except that the $Q$-table grows with time. We found that $\alpha$ could take on a high value, allowing repeatedly visited states with a stable representation to rapidly update their value function estimate. Additionally, batching up memory updates (e.g., at the end of the episode) helps with computational performance. We overwrite the item that has least recently shown up as a neighbour when we reach the memory's maximum capacity.</p>
<h3>3.4. Learning</h3>
<p>Agent parameters are updated by minimising the $L 2$ loss between the predicted $Q$ value for a given action and the $Q^{(N)}$ estimate on randomly sampled mini-batches from a replay buffer. In particular, we store tuples $\left(s_{t}, a_{t}, R_{t}\right)$ in
the replay buffer, where $N$ is the horizon of the $N$-step Q rule, and $R_{t}=Q^{(N)}\left(s_{t}, a\right)$ plays the role of the target network seen in DQN (our replay buffer is significantly smaller than DQN's). These $\left(s_{t}, a_{t}, R_{t}\right)$-tuples are then sampled uniformly at random to form minibatches for training. Note that the architecture in Figure 2 is entirely differentiable and so we can minimize this loss by gradient descent. Backpropagation updates the the weights and biases of the convolutional embedding network and the keys and values of each action-specific memory using gradients of this loss, using a lower learning rate than is used for updating pairs after queries $(\alpha)$.</p>
<h2>4. Experiments</h2>
<p>We investigated whether neural episodic control allows for more data efficient learning in practice in complex domains. As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013). We tested our method on the 57 Atari games used by Schaul et al. (2015a), which form an interesting set of tasks as they contain diverse challenges such as sparse rewards and vastly different magnitudes of scores across games. Most common algorithms applied in these domains, such as variants of DQN and A3C, require in the thousands of hours of in-game time, i.e. they are data inefficient.</p>
<p>We consider 5 variants of A3C and DQN as baselines as well as MFEC (Blundell et al., 2016). We compare to the basic implementations of A3C (Mnih et al., 2016) and DQN (Mnih et al., 2015). We also compare to two algorithms incorporating $\lambda$ returns (Sutton, 1988) aiming at more data efficiency by faster propagation of credit assignments, namely $Q^{*}(\lambda)$ (Harutyunyan et al., 2016) and $\operatorname{Retrace}(\lambda)$ (Munos et al., 2016). We also compare to DQN with Prioritised Replay, which improves data efficiency by replaying more salient transitions more frequently. We did not directly compare to DRQN (Hausknecht \&amp; Stone, 2015) nor FRMQN (Oh et al., 2016) as results were not available</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Architecture of episodic memory module for a single action $a$. Pixels representing the current state enter through a convolutional neural network on the bottom left and an estimate of $Q(s, a)$ exits top right. Gradients flow through the entire architecture.
for all Atari games. Note that in the case of DRQN, reported performance is lower than that of Prioritised Replay.</p>
<p>All algorithms were trained using discount rate $\gamma=0.99$, except MFEC that uses $\gamma=1$. In our implementation of MFEC we used random projections as an embedding function, since in the original publication it obtained better performance on the Atari games tested.</p>
<p>In terms of hyperparameters for NEC, we chose the same convolutional architecture as DQN, and store up to $5 \times$ $10^{5}$ memories per action. We used the RMSProp algorithm (Tieleman \&amp; Hinton, 2012) for gradient descent training. We apply the same preprocessing steps as (Mnih et al., 2015), including repeating each action four times. For the $N$-step $Q$ estimates we picked a horizon of $N=100$. Our replay buffer stores the only last $10^{5}$ states (as opposed to $10^{6}$ for DQN) observed and their $N$-step $Q$ estimates. We do one replay update for every 16 observed frames with a minibatch of size 32. We set the number of nearest neighbours $p=50$ in all our experiments. For the kernel function we chose a function that interpolates between the mean for short distances and weighted inverse distance for large distances, more precisely:</p>
<p>$$
k\left(h, h_{i}\right)=\frac{1}{\left|h-h_{i}\right|_{2}^{2}+\delta}
$$</p>
<p>Intuitively, when all neighbours are far away we want to avoid putting all weight onto one data point. A Gaussian kernel, for example, would exponentially suppress all neighbours except for the closest one. The kernel we chose has the advantage of having heavy tails. This makes the algorithm more robust and we found it to be less sensitive to kernel hyperparameters. We set $\delta=10^{-3}$.</p>
<p>In order to tune the remaining hyperparameters (SGD learning-rate, fast-update learning-rate $\alpha$ in Equation 4, dimensionality of the embeddings, $Q^{(N)}$ in Equation 3, and $\epsilon$ greedy exploration-rate) we ran a hyperparameter sweep on six games: Beam Rider, Breakout, Pong, Q*Bert, Seaquest and Space Invaders. We picked the hyperparameter values that performed best on the median for this subset of games (a
common cross validation procedure described by Bellemare et al. (2013), and adhered to by Mnih et al. (2015)).</p>
<p>Data efficiency results are summarised in Table 1. In the small data regime (less than 20 million frames) NEC clearly outperforms all other algorithms. The difference is especially pronounced before 5 million frames have been observed. Only at 40 million frames does DQN with Prioritised Replay outperform NEC on average; note that this corresponds to 185 hours of gameplay.</p>
<p>In order to provide a more detailed picture of NEC's performance, Figures 3 to 7 show learning curves on 6 games (Alien, Bowling, Boxing, Frostbite, HERO, Ms. Pac-Man, Pong), where several stereotypical cases of NEC's performance can be observed. All learning curves show the average performance over 5 different initial random seeds. We evaluate MFEC and NEC every 200.000 frames, and the other algorithms are evaluated every million steps.</p>
<p>Across most games, NEC is significantly faster at learning in the initial phase (see also Table 1), only comparable to MFEC, which also uses an episodic-like $Q$-function.</p>
<p>NEC also outperforms MFEC on average (see Table 2). In contrast with MFEC, NEC uses the reward signal to learn an embedding adequate for value interpolation. This difference is especially significant in games where a few pixels determine the value of each action. The simpler version of MFEC uses an approximation to $L 2$ distances in pixel-space by means of random projections, and cannot focus on the small but most relevant details. Another version of MFEC calculated distances on the latent representation of a variational autoencoder (Kingma \&amp; Welling, 2013) trained to model frames. This latent representation does not depend on rewards and will be subject to irrelevant details like, for example, the display of the current score.</p>
<p>A3C, DQN and related algorithms require rewards to be clipped to the range $[-1,1]$ for training stability ${ }^{1}$ (Mnih</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Frames</th>
<th style="text-align: left;">Nature DQN</th>
<th style="text-align: left;">$\mathrm{Q}^{*}(\lambda)$</th>
<th style="text-align: left;">$\operatorname{Retrace}(\lambda)$</th>
<th style="text-align: left;">Prioritised Replay</th>
<th style="text-align: left;">A3C</th>
<th style="text-align: left;">NEC</th>
<th style="text-align: left;">MFEC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1 M</td>
<td style="text-align: left;">$-0.7 \%$</td>
<td style="text-align: left;">$-0.8 \%$</td>
<td style="text-align: left;">$-0.4 \%$</td>
<td style="text-align: left;">$-2.4 \%$</td>
<td style="text-align: left;">$0.4 \%$</td>
<td style="text-align: left;">$\mathbf{1 6 . 7 \%}$</td>
<td style="text-align: left;">$12.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2 M</td>
<td style="text-align: left;">$0.0 \%$</td>
<td style="text-align: left;">$0.1 \%$</td>
<td style="text-align: left;">$0.2 \%$</td>
<td style="text-align: left;">$0.0 \%$</td>
<td style="text-align: left;">$0.9 \%$</td>
<td style="text-align: left;">$\mathbf{2 7 . 8 \%}$</td>
<td style="text-align: left;">$16.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">4 M</td>
<td style="text-align: left;">$2.4 \%$</td>
<td style="text-align: left;">$1.8 \%$</td>
<td style="text-align: left;">$3.3 \%$</td>
<td style="text-align: left;">$2.7 \%$</td>
<td style="text-align: left;">$1.9 \%$</td>
<td style="text-align: left;">$\mathbf{3 6 . 0 \%}$</td>
<td style="text-align: left;">$26.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">10 M</td>
<td style="text-align: left;">$15.7 \%$</td>
<td style="text-align: left;">$13.0 \%$</td>
<td style="text-align: left;">$17.3 \%$</td>
<td style="text-align: left;">$22.4 \%$</td>
<td style="text-align: left;">$3.6 \%$</td>
<td style="text-align: left;">$\mathbf{5 4 . 6 \%}$</td>
<td style="text-align: left;">$45.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">20 M</td>
<td style="text-align: left;">$26.8 \%$</td>
<td style="text-align: left;">$26.9 \%$</td>
<td style="text-align: left;">$30.4 \%$</td>
<td style="text-align: left;">$38.6 \%$</td>
<td style="text-align: left;">$7.9 \%$</td>
<td style="text-align: left;">$\mathbf{7 2 . 0 \%}$</td>
<td style="text-align: left;">$55.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">40 M</td>
<td style="text-align: left;">$52.7 \%$</td>
<td style="text-align: left;">$59.6 \%$</td>
<td style="text-align: left;">$60.5 \%$</td>
<td style="text-align: left;">$\mathbf{8 9 . 0 \%}$</td>
<td style="text-align: left;">$18.4 \%$</td>
<td style="text-align: left;">$83.3 \%$</td>
<td style="text-align: left;">$61.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1. Median across games of human-normalised scores for several algorithms at different points in training</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Frames</th>
<th style="text-align: left;">Nature DQN</th>
<th style="text-align: left;">$\mathrm{Q}^{*}(\lambda)$</th>
<th style="text-align: left;">$\operatorname{Retrace}(\lambda)$</th>
<th style="text-align: left;">Prioritised Replay</th>
<th style="text-align: left;">A3C</th>
<th style="text-align: left;">NEC</th>
<th style="text-align: left;">MFEC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1 M</td>
<td style="text-align: left;">$-10.5 \%$</td>
<td style="text-align: left;">$-11.7 \%$</td>
<td style="text-align: left;">$-10.5 \%$</td>
<td style="text-align: left;">$-14.4 \%$</td>
<td style="text-align: left;">$5.2 \%$</td>
<td style="text-align: left;">$\mathbf{4 5 . 6 \%}$</td>
<td style="text-align: left;">$28.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">2 M</td>
<td style="text-align: left;">$-5.8 \%$</td>
<td style="text-align: left;">$-7.5 \%$</td>
<td style="text-align: left;">$-5.4 \%$</td>
<td style="text-align: left;">$-5.4 \%$</td>
<td style="text-align: left;">$8.0 \%$</td>
<td style="text-align: left;">$\mathbf{5 8 . 3 \%}$</td>
<td style="text-align: left;">$39.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">4 M</td>
<td style="text-align: left;">$8.8 \%$</td>
<td style="text-align: left;">$6.2 \%$</td>
<td style="text-align: left;">$6.2 \%$</td>
<td style="text-align: left;">$10.2 \%$</td>
<td style="text-align: left;">$11.8 \%$</td>
<td style="text-align: left;">$\mathbf{7 3 . 3 \%}$</td>
<td style="text-align: left;">$53.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">10 M</td>
<td style="text-align: left;">$51.3 \%$</td>
<td style="text-align: left;">$46.3 \%$</td>
<td style="text-align: left;">$52.7 \%$</td>
<td style="text-align: left;">$71.5 \%$</td>
<td style="text-align: left;">$22.3 \%$</td>
<td style="text-align: left;">$\mathbf{9 9 . 8 \%}$</td>
<td style="text-align: left;">$85.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">20 M</td>
<td style="text-align: left;">$94.5 \%$</td>
<td style="text-align: left;">$135.4 \%$</td>
<td style="text-align: left;">$\mathbf{2 7 3 . 7 \%}$</td>
<td style="text-align: left;">$165.2 \%$</td>
<td style="text-align: left;">$59.7 \%$</td>
<td style="text-align: left;">$121.5 \%$</td>
<td style="text-align: left;">$113.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">40 M</td>
<td style="text-align: left;">$151.2 \%$</td>
<td style="text-align: left;">$\mathbf{4 4 0 . 9 \%}$</td>
<td style="text-align: left;">$386.5 \%$</td>
<td style="text-align: left;">$332.3 \%$</td>
<td style="text-align: left;">$255.4 \%$</td>
<td style="text-align: left;">$144.8 \%$</td>
<td style="text-align: left;">$142.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2. Mean human-normalised scores for several algorithms at different points in training
et al., 2015). NEC and MFEC do not require reward clipping, which results in qualitative changes in behaviour and better performance relative to other algorithms on games requiring clipping (Bowling, Frostbite, H.E.R.O., Ms. PacMan, Alien out of the seven shown).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Learning curve on Bowling.
Alien and Ms. Pac-Man both involve controlling a character, where there is an easy way to collect small rewards by collecting items of which there are plenty, while avoiding enemies, which are invulnerable to the agent. On the other hand the agent can pick up a special item making enemies vulnerable, allowing the agent to attack them and get significantly larger rewards than from collecting the small rewards. Agents trained using existing parametric methods tend to show little interest in this as clipping implies there is no difference between large and small rewards. Therefore, as NEC does not need reward clipping, it can strongly</p>
<p>Pop-Art.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Learning curve on Frostbite.
outperform other algorithms, since NEC is maximising the non-clipped score (the true score). This can also be seen when observing the agents play: parametric methods will tend to collect small rewards, while NEC will try to actively make the enemies vulnerable and attack them to get large rewards.</p>
<p>NEC also outperforms the other algorithms on Pong and Boxing where reward clipping does not affect any of the algorithms as all original rewards are in the range $[-1,1]$; as can be expected, NEC does not outperform others in terms of maximally achieved score, but it is vastly more data efficient.</p>
<p>In Figure 10 we show a chart of human-normalised scores across all 57 Atari games at 10 million frames comparing to Prioritised Replay and MFEC. We rank the games independently for each algorithm, and on the $y$-axis the deciles are</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Learning curve on H.E.R.O.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Learning curve on Ms. Pac-Man.
shown.
We can see that NEC gets to a human level performance in about $25 \%$ of the games within 10 million frames. As we can see NEC outperforms MFEC and Prioritised Replay.</p>
<h2>5. Related work</h2>
<p>There has been much recent work on memory architectures for neural networks (LSTM; Hochreiter \&amp; Schmidhuber, 1997), DNC (Graves et al., 2016), memory networks (Sukhbaatar et al., 2015; Miller et al., 2016)). Recurrent neural network representations of memory (LSTMs and DNCs) are trained by truncated backpropagation through time, and are subject to the same slow learning of non-recurrent neural networks.</p>
<p>Some of these models have been adapted to their use in RL agents (LSTMs; Bakker et al., 2003; Hausknecht \&amp; Stone, 2015), DNCs (Graves et al., 2016), memory networks (Oh et al., 2016). However, the contents of these memories is typically reset at the beginning of every episode. This is ap-
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Learning curve on Alien.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Learning curve on Pong.
propriate when the goal of the memory is tracking previous observations in order to maximise rewards in partially observable or non-Markovian environments. Therefore, these implementations can be thought of as a type of working memory, and solve a different problem than the one addressed in this work.</p>
<p>RNNs can learn to quickly write highly rewarding states into memory and may even be able to learn entire reinforcement learning algorithms (Wang et al., 2016; Duan et al., 2016). However, doing so can take an arbitrarily long time and the learning time likely scales strongly with the complexity of the task.</p>
<p>The work of Oh et al. (2016) is also reminiscent of the ideas presented here. They introduced (FR)MQN, an adaptation of memory networks used in the top layers of a $Q$-network.</p>
<p>Kaiser et al. (2016) introduced a differentiable layer of keyvalue pairs that can be plugged into a neural network. This layer uses cosine similarity to calculate a weighted average of the values associated with the $k$ most similar memories. Their use of a moving average update rule is reminiscent of</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Learning curve on Boxing.
the one presented in Section 3. The authors reported results on a set of supervised tasks, however they did not consider applications to reinforcement learning. Other deep RL methods keep a history of previous experience. Indeed, DQN itself has an elementary form of memory: the replay buffer central to its stable training can be viewed as a memory that is frequently replayed to distil the contents into DQN's value network. Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN's replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN's replay buffer to hold millions of $\left(s, a, r, s^{\prime}\right)$ tuples. The use of local regression techniques for $Q$-function approximation has been suggested before: Santamaría et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories. Munos \&amp; Moore (1998) proposed barycentric interpolators to model the value function and proved their convergence to the optimal value function under mild conditions, but no empirical results were presented. Gabel \&amp; Riedmiller (2005) also suggested the use of local regression, under the paradigm of case-based-reasoning that included heuristics for the deletion of stored cases. Blundell et al. (2016, MFEC) recently used local regression for $Q$-function estimation using the mean of the k-nearest neighbours, except in the case of an exact match of the query point, in which case the stored value was returned. They also propose the use of the latent variable obtained from a variational autoencoder (Rezende et al., 2014) as an embedding space, but showed random projections often obtained better results. In contrast with the ideas presented here, none of the localregression work aforementioned uses the reward signal to learn an embedding space of covariates in which to perform the local-regression. We learn this embedding space using temporal-difference learning; a crucial difference, as we
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Human-normalised scores of games, independently ranked per algorithm; labels on y-axis denote quantiles.
showed in the experimental comparison to MFEC.</p>
<h2>6. Discussion</h2>
<p>We have proposed Neural Episodic Control (NEC): a deep reinforcement learning agent that learns significantly faster than other baseline agents on a wide range of Atari 2600 games. At the core of NEC is a memory structure: a Differentiable Neural Dictionary (DND), one for each potential action. NEC inserts recent state representations paired with corresponding value functions into the appropriate DND.</p>
<p>Our experiments show that NEC requires an order of magnitude fewer interactions with the environment than agents previously proposed for data efficiency, such as Prioritised Replay (Schaul et al., 2015b) and Retrace $(\lambda)$ (Munos et al., 2016). We speculate that NEC learns faster through a com-</p>
<p>bination of three features of the agent: the memory architecture (DND), the use of $N$-step $Q$ estimates, and a state representation provided by a convolutional neural network.</p>
<p>The memory architecture, DND, rapidly integrates recent experience-state representations and corresponding value estimates-allowing this information to be rapidly integrated into future behaviour. Such memories persist across many episodes, and we use a fast approximate nearest neighbour algorithm (kd-trees) to ensure that such memories can be efficiently accessed. Estimating $Q$-values by using the $N$-step $Q$ value function interpolates between Monte Carlo value estimates and backed up off-policy estimates. Monte Carlo value estimates reflect the rewards an agent is actually receiving, whilst backed up off-policy estimates should be more representative of the value function at the optimal policy, but evolve much slower. By using both estimates, NEC can trade-off between these two estimation procedures and their relative strengths and weaknesses (speed of reward propagation vs optimality). Finally, by having a slow changing, stable representation provided by a convolutional neural network, keys stored in the DND remain relative stable.</p>
<p>Our work suggests that non-parametric methods are a promising addition to the deep reinforcement learning toolbox, especially where data efficiency is paramount. In our experiments we saw that at the beginning of learning NEC outperforms other agents in terms of learning speed. We saw that later in learning Prioritised Replay has higher performance than NEC. We leave it to future work to further improve NEC so that its long term final performance is significantly superior to parametric agents. Another avenue of further research would be to apply the method discussed in this paper to a wider range of tasks such as visually more complex 3D worlds or real world tasks where data efficiency is of great importance due to the high cost of acquiring data.</p>
<p>Acknowledgements The authors would like to thank Daniel Zoran, Dharshan Kumaran, Jane Wang, Dan Belov, Ruiqi Guo, Yori Zwols, Jack Rae, Andreas Kirsch, Peter Dayan, David Silver and many others at DeepMind for insightful discussions and feedback. We also thank Georg Ostrovski, Tom Schaul, and Hubert Soyer for providing baseline learning curves.</p>
<h2>References</h2>
<p>Ba, Jimmy, Hinton, Geoffrey E, Mnih, Volodymyr, Leibo, Joel Z, and Ionescu, Catalin. Using fast weights to attend to the recent past. In Advances In Neural Information Processing Systems, pp. 4331-4339, 2016.</p>
<p>Bakker, Bram, Zhumatiy, Viktor, Gruener, Gabriel, and Schmidhuber, Jürgen. A robot that reinforcement-learns to identify and memorize important previous observations.</p>
<p>In Intelligent Robots and Systems, 2003.(IROS 2003). Proceedings. 2003 IEEE/RSJ International Conference on, volume 1, pp. 430-435. IEEE, 2003.</p>
<p>Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 06 2013.</p>
<p>Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253-279, 2013.</p>
<p>Bentley, Jon Louis. Multidimensional binary search trees used for associative searching. Commun. ACM, 18(9): 509-517, September 1975.</p>
<p>Blundell, Charles, Uria, Benigno, Pritzel, Alexander, Li, Yazhe, Ruderman, Avraham, Leibo, Joel Z, Rae, Jack, Wierstra, Daan, and Hassabis, Demis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.</p>
<p>Duan, Yan, Schulman, John, Chen, Xi, Bartlett, Peter L, Sutskever, Ilya, and Abbeel, Pieter. R ${ }^{2}$ : Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.</p>
<p>Fernando, Chrisantha, Banarse, Dylan, Blundell, Charles, Zwols, Yori, Ha, David, Rusu, Andrei A, Pritzel, Alexander, and Wierstra, Daan. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.</p>
<p>Gabel, Thomas and Riedmiller, Martin. Cbr for state value function approximation in reinforcement learning. In International Conference on Case-Based Reasoning, pp. 206-221. Springer, 2005.</p>
<p>Graves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley, Tim, Danihelka, Ivo, Grabska-Barwińska, Agnieszka, Colmenarejo, Sergio Gómez, Grefenstette, Edward, Ramalho, Tiago, Agapiou, John, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476, 2016.</p>
<p>Harutyunyan, Anna, Bellemare, Marc G, Stepleton, Tom, and Munos, Rémi. Q ( $\backslash$ lambda) with off-policy corrections. In International Conference on Algorithmic Learning Theory, pp. 305-320. Springer, 2016.</p>
<p>Hausknecht, Matthew and Stone, Peter. Deep recurrent qlearning for partially observable mdps. arXiv preprint arXiv:1507.06527, 2015.</p>
<p>He, Frank S, Liu, Yang, Schwing, Alexander G, and Peng, Jian. Learning to play in a day: Faster deep reinforcement learning by optimality tightening. arXiv preprint arXiv:1611.01606, 2016.</p>
<p>Hinton, Geoffrey E and Plaut, David C. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pp. 177-186, 1987.</p>
<p>Hochreiter, Sepp and Schmidhuber, Jürgen. Long short-term memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8. 1735 .</p>
<p>Kaiser, Lukasz, Nachum, Ofir, Roy, Aurko, and Bengio, Samy. Learning to remember rare events. 2016.</p>
<p>Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Kumaran, Dharshan, Hassabis, Demis, and McClelland, James L. What learning systems do intelligent agents need? complementary learning systems theory updated. Trends in Cognitive Sciences, 20(7):512-534, 2016.</p>
<p>Lake, Brenden M, Ullman, Tomer D, Tenenbaum, Joshua B, and Gershman, Samuel J. Building machines that learn and think like people. arXiv preprint arXiv:1604.00289, 2016.</p>
<p>Lengyel, M. and Dayan, P. Hippocampal contributions to control: The third way. In NIPS, volume 20, pp. 889-896, 2007.</p>
<p>McCloskey, Michael and Cohen, Neal J. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24: 109-165, 1989.</p>
<p>Miller, Alexander, Fisch, Adam, Dodge, Jesse, Karimi, Amir-Hossein, Bordes, Antoine, and Weston, Jason. Keyvalue memory networks for directly reading documents. arXiv preprint arXiv:1606.03126, 2016.</p>
<p>Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza, Mehdi, Graves, Alex, Lillicrap, Timothy P, Harley, Tim, Silver, David, and Kavukcuoglu, Koray. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016.</p>
<p>Munos, Remi and Moore, Andrew W. Barycentric interpolators for continuous space and time reinforcement learning. In NIPS, pp. 1024-1030, 1998.</p>
<p>Munos, Rémi, Stepleton, Tom, Harutyunyan, Anna, and Bellemare, Marc. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1046-1054, 2016.</p>
<p>Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard L, and Singh, Satinder. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pp. 2845-2853, 2015.</p>
<p>Oh, Junhyuk, Chockalingam, Valliappa, Lee, Honglak, et al. Control of memory, active perception, and action in minecraft. In Proceedings of The 33rd International Conference on Machine Learning, pp. 2790-2799, 2016.</p>
<p>Osband, Ian, Blundell, Charles, Pritzel, Alexander, and Van Roy, Benjamin. Deep exploration via bootstrapped dqn. In Advances In Neural Information Processing Systems, pp. 4026-4034, 2016.</p>
<p>Peng, Jing and Williams, Ronald J. Incremental multi-step q-learning. Machine learning, 22(1-3):283-290, 1996.</p>
<p>Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of The 31st International Conference on Machine Learning, pp. 1278-1286, 2014.</p>
<p>Rusu, Andrei A, Rabinowitz, Neil C, Desjardins, Guillaume, Soyer, Hubert, Kirkpatrick, James, Kavukcuoglu, Koray, Pascanu, Razvan, and Hadsell, Raia. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.</p>
<p>Santamaría, Juan C, Sutton, Richard S, and Ram, Ashwin. Experiments with reinforcement learning in problems with continuous state and action spaces. Adaptive behavior, 6(2):163-217, 1997.</p>
<p>Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Silver, David. Prioritized experience replay. CoRR, abs/1511.05952, 2015a.</p>
<p>Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Silver, David. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015b.</p>
<p>Silver, David, Huang, Aja, Maddison, Chris J, Guez, Arthur, Sifre, Laurent, Van Den Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis, Panneershelvam, Veda, Lanctot, Marc, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587): 484-489, 2016.</p>
<p>Sukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440-2448, 2015.</p>
<p>Sutton, Richard S. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9-44, 1988.</p>
<p>Sutton, Richard S and Barto, Andrew G. Reinforcement learning: An introduction. MIT press, 1998.</p>
<p>Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4:2, 2012.
van Hasselt, H., Guez, A., Hessel, M., and Silver, D. Learning functions across many orders of magnitudes. ArXiv e-prints, February 2016.</p>
<p>Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep reinforcement learning with double q-learning. In AAAI, pp. 2094-2100, 2016.</p>
<p>Vezhnevets, Alexander, Mnih, Volodymyr, Osindero, Simon, Graves, Alex, Vinyals, Oriol, Agapiou, John, et al. Strategic attentive writer for learning macro-actions. In Advances in Neural Information Processing Systems, pp. 3486-3494, 2016.</p>
<p>Vinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra, Daan, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630-3638, 2016.</p>
<p>Wang, Jane X, Kurth-Nelson, Zeb, Tirumala, Dhruva, Soyer, Hubert, Leibo, Joel Z, Munos, Remi, Blundell, Charles, Kumaran, Dharshan, and Botvinick, Matt. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.</p>
<p>Watkins, Christopher JCH and Dayan, Peter. Q-learning. Machine learning, 8(3-4):279-292, 1992.</p>
<p>Watkins, Christopher John Cornish Hellaby. Learning from delayed rewards. PhD thesis, University of Cambridge England, 1989.</p>
<h1>A. Scores on Atari Games</h1>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">A3C</th>
<th style="text-align: center;">Nature DQN</th>
<th style="text-align: center;">MFEC</th>
<th style="text-align: center;">NEC</th>
<th style="text-align: center;">Prioritised <br> Replay</th>
<th style="text-align: center;">$\mathrm{Q}^{*}(\lambda)$</th>
<th style="text-align: center;">$\operatorname{Retrace}(\lambda)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Alien</td>
<td style="text-align: center;">415.5</td>
<td style="text-align: center;">634.8</td>
<td style="text-align: center;">1717.7</td>
<td style="text-align: center;">3460.6</td>
<td style="text-align: center;">800.5</td>
<td style="text-align: center;">476.8</td>
<td style="text-align: center;">541.2</td>
</tr>
<tr>
<td style="text-align: center;">Amidar</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">126.8</td>
<td style="text-align: center;">370.9</td>
<td style="text-align: center;">811.3</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">134.5</td>
<td style="text-align: center;">162.9</td>
</tr>
<tr>
<td style="text-align: center;">Assault</td>
<td style="text-align: center;">720.8</td>
<td style="text-align: center;">1489.5</td>
<td style="text-align: center;">510.2</td>
<td style="text-align: center;">599.9</td>
<td style="text-align: center;">1339.9</td>
<td style="text-align: center;">1026.6</td>
<td style="text-align: center;">1331.1</td>
</tr>
<tr>
<td style="text-align: center;">Asterix</td>
<td style="text-align: center;">301.6</td>
<td style="text-align: center;">2989.1</td>
<td style="text-align: center;">1776.6</td>
<td style="text-align: center;">2480.4</td>
<td style="text-align: center;">2599.7</td>
<td style="text-align: center;">2588.6</td>
<td style="text-align: center;">2520.3</td>
</tr>
<tr>
<td style="text-align: center;">Asteroids</td>
<td style="text-align: center;">1360.1</td>
<td style="text-align: center;">395.3</td>
<td style="text-align: center;">4706.8</td>
<td style="text-align: center;">2496.1</td>
<td style="text-align: center;">854.0</td>
<td style="text-align: center;">569.8</td>
<td style="text-align: center;">579.2</td>
</tr>
<tr>
<td style="text-align: center;">Atlantis</td>
<td style="text-align: center;">36383</td>
<td style="text-align: center;">14210.5</td>
<td style="text-align: center;">95499.4</td>
<td style="text-align: center;">51208.0</td>
<td style="text-align: center;">12579.1</td>
<td style="text-align: center;">28818.8</td>
<td style="text-align: center;">44771.1</td>
</tr>
<tr>
<td style="text-align: center;">Bank Heist</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">163.7</td>
<td style="text-align: center;">343.3</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: center;">Battlezone</td>
<td style="text-align: center;">2354.2</td>
<td style="text-align: center;">6961.0</td>
<td style="text-align: center;">19053.6</td>
<td style="text-align: center;">13345.5</td>
<td style="text-align: center;">13500.0</td>
<td style="text-align: center;">8227.2</td>
<td style="text-align: center;">6762.2</td>
</tr>
<tr>
<td style="text-align: center;">Beamrider</td>
<td style="text-align: center;">450.2</td>
<td style="text-align: center;">3741.7</td>
<td style="text-align: center;">858.8</td>
<td style="text-align: center;">749.6</td>
<td style="text-align: center;">3249.6</td>
<td style="text-align: center;">656.2</td>
<td style="text-align: center;">725.4</td>
</tr>
<tr>
<td style="text-align: center;">Berzerk</td>
<td style="text-align: center;">593.6</td>
<td style="text-align: center;">484.2</td>
<td style="text-align: center;">924.2</td>
<td style="text-align: center;">852.8</td>
<td style="text-align: center;">575.6</td>
<td style="text-align: center;">647.9</td>
<td style="text-align: center;">701.5</td>
</tr>
<tr>
<td style="text-align: center;">Bowling</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">39.9</td>
</tr>
<tr>
<td style="text-align: center;">Boxing</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">30.7</td>
</tr>
<tr>
<td style="text-align: center;">Breakout</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">10.2</td>
</tr>
<tr>
<td style="text-align: center;">Centipede</td>
<td style="text-align: center;">3228</td>
<td style="text-align: center;">4401.4</td>
<td style="text-align: center;">20608.8</td>
<td style="text-align: center;">12314.5</td>
<td style="text-align: center;">4694.1</td>
<td style="text-align: center;">4097.5</td>
<td style="text-align: center;">4792.9</td>
</tr>
<tr>
<td style="text-align: center;">Chopper Command</td>
<td style="text-align: center;">1036.7</td>
<td style="text-align: center;">827.2</td>
<td style="text-align: center;">3075.6</td>
<td style="text-align: center;">5070.3</td>
<td style="text-align: center;">1426.5</td>
<td style="text-align: center;">760.6</td>
<td style="text-align: center;">801.6</td>
</tr>
<tr>
<td style="text-align: center;">Crazy Climber</td>
<td style="text-align: center;">70103.5</td>
<td style="text-align: center;">66061.6</td>
<td style="text-align: center;">9892.2</td>
<td style="text-align: center;">34344.0</td>
<td style="text-align: center;">76574.1</td>
<td style="text-align: center;">64980.6</td>
<td style="text-align: center;">54177.6</td>
</tr>
<tr>
<td style="text-align: center;">Defender</td>
<td style="text-align: center;">4596</td>
<td style="text-align: center;">2877.9</td>
<td style="text-align: center;">10052.8</td>
<td style="text-align: center;">6126.1</td>
<td style="text-align: center;">3486.4</td>
<td style="text-align: center;">3260.8</td>
<td style="text-align: center;">3275.6</td>
</tr>
<tr>
<td style="text-align: center;">Demon Attack</td>
<td style="text-align: center;">346.8</td>
<td style="text-align: center;">5541.9</td>
<td style="text-align: center;">1081.8</td>
<td style="text-align: center;">641.4</td>
<td style="text-align: center;">6503.6</td>
<td style="text-align: center;">4914.8</td>
<td style="text-align: center;">4836.6</td>
</tr>
<tr>
<td style="text-align: center;">Double Dunk</td>
<td style="text-align: center;">$-17.2$</td>
<td style="text-align: center;">$-19.0$</td>
<td style="text-align: center;">$-13.2$</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">$-15.9$</td>
<td style="text-align: center;">$-18.2$</td>
<td style="text-align: center;">$-18.3$</td>
</tr>
<tr>
<td style="text-align: center;">Enduro</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">364.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">1125.8</td>
<td style="text-align: center;">396.0</td>
<td style="text-align: center;">440.6</td>
</tr>
<tr>
<td style="text-align: center;">Fishing Derby</td>
<td style="text-align: center;">$-89.5$</td>
<td style="text-align: center;">$-81.6$</td>
<td style="text-align: center;">$-90.3$</td>
<td style="text-align: center;">$-72.2$</td>
<td style="text-align: center;">$-48.2$</td>
<td style="text-align: center;">$-84.2$</td>
<td style="text-align: center;">$-79.8$</td>
</tr>
<tr>
<td style="text-align: center;">Freeway</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;">Frostbite</td>
<td style="text-align: center;">218.9</td>
<td style="text-align: center;">339.1</td>
<td style="text-align: center;">925.1</td>
<td style="text-align: center;">2747.4</td>
<td style="text-align: center;">711.3</td>
<td style="text-align: center;">407.2</td>
<td style="text-align: center;">325.0</td>
</tr>
<tr>
<td style="text-align: center;">Gopher</td>
<td style="text-align: center;">854.1</td>
<td style="text-align: center;">1111.2</td>
<td style="text-align: center;">4412.6</td>
<td style="text-align: center;">2432.3</td>
<td style="text-align: center;">1235.3</td>
<td style="text-align: center;">2292.4</td>
<td style="text-align: center;">3050.4</td>
</tr>
<tr>
<td style="text-align: center;">Gravitar</td>
<td style="text-align: center;">215.8</td>
<td style="text-align: center;">154.7</td>
<td style="text-align: center;">1011.3</td>
<td style="text-align: center;">1257.0</td>
<td style="text-align: center;">218.9</td>
<td style="text-align: center;">121.9</td>
<td style="text-align: center;">108.9</td>
</tr>
<tr>
<td style="text-align: center;">H.E.R.O.</td>
<td style="text-align: center;">4598.2</td>
<td style="text-align: center;">1050.7</td>
<td style="text-align: center;">14767.7</td>
<td style="text-align: center;">16265.3</td>
<td style="text-align: center;">5164.5</td>
<td style="text-align: center;">2223.3</td>
<td style="text-align: center;">3298.2</td>
</tr>
<tr>
<td style="text-align: center;">Ice Hockey</td>
<td style="text-align: center;">$-8.1$</td>
<td style="text-align: center;">$-4.5$</td>
<td style="text-align: center;">$-6.5$</td>
<td style="text-align: center;">$-1.6$</td>
<td style="text-align: center;">$-10.2$</td>
<td style="text-align: center;">$-11.1$</td>
<td style="text-align: center;">$-9.1$</td>
</tr>
<tr>
<td style="text-align: center;">James Bond</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">165.9</td>
<td style="text-align: center;">244.7</td>
<td style="text-align: center;">376.8</td>
<td style="text-align: center;">203.8</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">67.2</td>
</tr>
<tr>
<td style="text-align: center;">Kangaroo</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">519.6</td>
<td style="text-align: center;">2465.7</td>
<td style="text-align: center;">2489.1</td>
<td style="text-align: center;">616.7</td>
<td style="text-align: center;">520.7</td>
<td style="text-align: center;">554.6</td>
</tr>
<tr>
<td style="text-align: center;">Krull</td>
<td style="text-align: center;">3627.6</td>
<td style="text-align: center;">6015.1</td>
<td style="text-align: center;">4555.2</td>
<td style="text-align: center;">5179.2</td>
<td style="text-align: center;">6700.7</td>
<td style="text-align: center;">8169.8</td>
<td style="text-align: center;">7399.3</td>
</tr>
<tr>
<td style="text-align: center;">Kung Fu Master</td>
<td style="text-align: center;">6634.6</td>
<td style="text-align: center;">17166.1</td>
<td style="text-align: center;">12906.5</td>
<td style="text-align: center;">30568.1</td>
<td style="text-align: center;">21456.2</td>
<td style="text-align: center;">13874.7</td>
<td style="text-align: center;">18065.8</td>
</tr>
<tr>
<td style="text-align: center;">Montezuma's Revenge</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: center;">Ms. Pac-Man</td>
<td style="text-align: center;">770</td>
<td style="text-align: center;">1657.0</td>
<td style="text-align: center;">3802.7</td>
<td style="text-align: center;">4142.8</td>
<td style="text-align: center;">1558.3</td>
<td style="text-align: center;">1289.9</td>
<td style="text-align: center;">1401.6</td>
</tr>
<tr>
<td style="text-align: center;">Name This Game</td>
<td style="text-align: center;">2745.1</td>
<td style="text-align: center;">6380.2</td>
<td style="text-align: center;">4845.1</td>
<td style="text-align: center;">5532.0</td>
<td style="text-align: center;">7525.0</td>
<td style="text-align: center;">5378.5</td>
<td style="text-align: center;">5227.8</td>
</tr>
<tr>
<td style="text-align: center;">Phoenix</td>
<td style="text-align: center;">2542.5</td>
<td style="text-align: center;">5357.0</td>
<td style="text-align: center;">5334.5</td>
<td style="text-align: center;">5756.5</td>
<td style="text-align: center;">11813.3</td>
<td style="text-align: center;">5771.2</td>
<td style="text-align: center;">6046.7</td>
</tr>
<tr>
<td style="text-align: center;">Pitfall!</td>
<td style="text-align: center;">$-43.9$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-79.0$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-4.4$</td>
<td style="text-align: center;">$-1.5$</td>
</tr>
<tr>
<td style="text-align: center;">Pong</td>
<td style="text-align: center;">$-20.3$</td>
<td style="text-align: center;">$-3.2$</td>
<td style="text-align: center;">$-20.0$</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">$-18.9$</td>
<td style="text-align: center;">$-13.3$</td>
</tr>
<tr>
<td style="text-align: center;">Private Eye</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">3963.8</td>
<td style="text-align: center;">162.2</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">1230.4</td>
<td style="text-align: center;">80.2</td>
</tr>
<tr>
<td style="text-align: center;">Q*bert</td>
<td style="text-align: center;">438.9</td>
<td style="text-align: center;">2372.5</td>
<td style="text-align: center;">12500.4</td>
<td style="text-align: center;">7419.2</td>
<td style="text-align: center;">839.0</td>
<td style="text-align: center;">1812.4</td>
<td style="text-align: center;">2582.1</td>
</tr>
<tr>
<td style="text-align: center;">River Raid</td>
<td style="text-align: center;">2312.6</td>
<td style="text-align: center;">3144.9</td>
<td style="text-align: center;">4195.0</td>
<td style="text-align: center;">5498.1</td>
<td style="text-align: center;">4871.8</td>
<td style="text-align: center;">2787.1</td>
<td style="text-align: center;">2671.0</td>
</tr>
<tr>
<td style="text-align: center;">Road Runner</td>
<td style="text-align: center;">759.9</td>
<td style="text-align: center;">7285.4</td>
<td style="text-align: center;">5432.1</td>
<td style="text-align: center;">12661.4</td>
<td style="text-align: center;">24746.6</td>
<td style="text-align: center;">3133.1</td>
<td style="text-align: center;">6285.0</td>
</tr>
<tr>
<td style="text-align: center;">Robot Tank</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">9.1</td>
</tr>
<tr>
<td style="text-align: center;">Seaquest</td>
<td style="text-align: center;">514.1</td>
<td style="text-align: center;">618.7</td>
<td style="text-align: center;">711.6</td>
<td style="text-align: center;">1015.3</td>
<td style="text-align: center;">1192.2</td>
<td style="text-align: center;">611.7</td>
<td style="text-align: center;">574.3</td>
</tr>
<tr>
<td style="text-align: center;">Skiing</td>
<td style="text-align: center;">-20002.7</td>
<td style="text-align: center;">-19818.0</td>
<td style="text-align: center;">-15278.9</td>
<td style="text-align: center;">-26340.7</td>
<td style="text-align: center;">-12762.4</td>
<td style="text-align: center;">-17055.7</td>
<td style="text-align: center;">-13880.4</td>
</tr>
<tr>
<td style="text-align: center;">Solaris</td>
<td style="text-align: center;">2932.7</td>
<td style="text-align: center;">1343.0</td>
<td style="text-align: center;">8717.5</td>
<td style="text-align: center;">7201.0</td>
<td style="text-align: center;">1397.1</td>
<td style="text-align: center;">2460.0</td>
<td style="text-align: center;">3211.8</td>
</tr>
<tr>
<td style="text-align: center;">Space Invaders</td>
<td style="text-align: center;">201</td>
<td style="text-align: center;">642.2</td>
<td style="text-align: center;">2027.8</td>
<td style="text-align: center;">1016.0</td>
<td style="text-align: center;">673.0</td>
<td style="text-align: center;">545.6</td>
<td style="text-align: center;">527.9</td>
</tr>
<tr>
<td style="text-align: center;">Stargunner</td>
<td style="text-align: center;">613.6</td>
<td style="text-align: center;">604.8</td>
<td style="text-align: center;">14843.9</td>
<td style="text-align: center;">1171.4</td>
<td style="text-align: center;">1131.4</td>
<td style="text-align: center;">877.0</td>
<td style="text-align: center;">886.7</td>
</tr>
<tr>
<td style="text-align: center;">Surround</td>
<td style="text-align: center;">$-9.9$</td>
<td style="text-align: center;">$-9.7$</td>
<td style="text-align: center;">$-9.9$</td>
<td style="text-align: center;">$-7.9$</td>
<td style="text-align: center;">$-8.5$</td>
<td style="text-align: center;">$-9.8$</td>
<td style="text-align: center;">$-9.9$</td>
</tr>
<tr>
<td style="text-align: center;">Tennis</td>
<td style="text-align: center;">$-23.8$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-23.7$</td>
<td style="text-align: center;">$-1.8$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-4.3$</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Time Pilot</td>
<td style="text-align: center;">3683.5</td>
<td style="text-align: center;">1952.0</td>
<td style="text-align: center;">10751.3</td>
<td style="text-align: center;">10282.7</td>
<td style="text-align: center;">2430.2</td>
<td style="text-align: center;">2323.7</td>
<td style="text-align: center;">2576.0</td>
</tr>
<tr>
<td style="text-align: center;">Tutankham</td>
<td style="text-align: center;">108.3</td>
<td style="text-align: center;">148.7</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">121.6</td>
<td style="text-align: center;">194.0</td>
<td style="text-align: center;">108.3</td>
<td style="text-align: center;">122.4</td>
</tr>
<tr>
<td style="text-align: center;">Up'n Down</td>
<td style="text-align: center;">3322.3</td>
<td style="text-align: center;">18964.9</td>
<td style="text-align: center;">22320.8</td>
<td style="text-align: center;">39823.3</td>
<td style="text-align: center;">11856.2</td>
<td style="text-align: center;">11961.2</td>
<td style="text-align: center;">13308.4</td>
</tr>
<tr>
<td style="text-align: center;">Venture</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: center;">Video Pinball</td>
<td style="text-align: center;">30548.5</td>
<td style="text-align: center;">14316.0</td>
<td style="text-align: center;">90507.7</td>
<td style="text-align: center;">22842.6</td>
<td style="text-align: center;">24254.5</td>
<td style="text-align: center;">11507.3</td>
<td style="text-align: center;">14178.9</td>
</tr>
<tr>
<td style="text-align: center;">Wizard of Wor</td>
<td style="text-align: center;">876</td>
<td style="text-align: center;">401.4</td>
<td style="text-align: center;">12803.1</td>
<td style="text-align: center;">8480.7</td>
<td style="text-align: center;">1146.6</td>
<td style="text-align: center;">526.8</td>
<td style="text-align: center;">420.4</td>
</tr>
<tr>
<td style="text-align: center;">Yars' Revenge</td>
<td style="text-align: center;">9953</td>
<td style="text-align: center;">7614.1</td>
<td style="text-align: center;">5956.7</td>
<td style="text-align: center;">21490.5</td>
<td style="text-align: center;">9228.5</td>
<td style="text-align: center;">8884.4</td>
<td style="text-align: center;">8532.7</td>
</tr>
<tr>
<td style="text-align: center;">Zaxxon</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">200.3</td>
<td style="text-align: center;">6288.1</td>
<td style="text-align: center;">10082.4</td>
<td style="text-align: center;">3123.5</td>
<td style="text-align: center;">278.3</td>
<td style="text-align: center;">168.3</td>
</tr>
</tbody>
</table>
<p>Table 3. Scores at 10 Million Frames</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ See Pop-Art (van Hasselt et al., 2016) for a DQN-like algorithm that does not require reward-clipping. NEC also outperforms&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
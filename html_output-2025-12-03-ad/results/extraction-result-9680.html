<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9680 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9680</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9680</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-267311508</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.15641v2.pdf" target="_blank">PRE: A Peer Review Based Large Language Model Evaluator</a></p>
                <p><strong>Paper Abstract:</strong> The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term. In order to address these issues, inspired by the peer review systems widely used in academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select"reviewers"from a couple of powerful LLMs. Then, to actually evaluate the"submissions"written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on text summarization tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9680.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9680.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Peer Review Evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic LLM evaluation framework that uses a peer-review mechanism: (1) a qualification exam to select qualified LLM reviewers, (2) reviewers rate evaluatee outputs (pointwise or pairwise), and (3) a "chair" aggregates reviewer scores via weighted voting to produce final rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple LLMs (GPT-4, Claude-1, GPT-3.5-turbo, Llama-2-70b-chat, Vicuna-7b, ChatGLM2-6B, RWKV-4-Raven-7B, Alpaca-7b, FastChat-t5-3b, ChatGLM-Pro, Baichuan-2-13b)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>A mix of closed-source and open-source LLMs used both as evaluatees and reviewer candidates; sizes and relative ranks reported in Table 1 (e.g., GPT-4 highest ELO, Llama-2-70b-chat 70B params, several 7B/13B/70B models).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation evaluation (text summarization and non-factoid QA); used as a general automatic-evaluation framework that could be adapted to other domains (not explicitly applied to scientific-theory generation in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Peer-review among LLMs: construct qualification exam (human-annotated or Auto-Exam), select reviewers whose Agreement exceeds threshold, have reviewers rate evaluatee outputs in pointwise (5- or 100-level) or pairwise formats, then aggregate using weighted voting based on reviewer qualification performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Agreement with human preferences for pairwise evaluation; Kendall's tau and Spearman correlation for pointwise evaluations; reviewer exam Agreement used as reviewer reliability measure and to compute reviewer weights; Preference Gap (PG) used to measure evaluator bias.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>XSum (BBC single-document news summarization, sampled 100 examples) and NF-CATS (non-factoid QA, filtered and sampled 100 examples). Human annotations collected (pointwise 5-level Likert + preference annotations) used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PRE outperformed all baselines including single-LLM evaluators (e.g., GPT-4). Key reported Agreement scores (pairwise on XSum): PRE 0.7443, PRE w/o GPT-4 0.7328, GPT-4 alone 0.7369. PRE is robust to hyperparameter changes (pass threshold and weighting), and pairwise prompt setting performs slightly better than pointwise.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires qualification exam data (authors used human annotations for exam; could be costly); exam design and thresholds affect reviewer selection; experiments focus on summarization and QA, not direct evaluation of scientific-theory outputs; potential for reviewers to overfit if exam data become public (authors argue limited risk but note it).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>PRE's aggregated outputs have higher consistency with human preferences than reference-based metrics (ROUGE/BLEU/BERTScore) and some single-LLM evaluators; human annotation used as gold standard (PRE aims to mimic and align with human judgments while reducing single-model biases).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a small qualification exam (authors used a 60% pass threshold) to filter reviewers, prefer pairwise evaluation when resources allow, weight reviewers by qualification performance (authors use log-odds of Agreement), normalize pointwise scores (mean-variance), and consider Auto-Exam as a low-cost complement to manual exam.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRE: A Peer Review Based Large Language Model Evaluator', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9680.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9680.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QualificationExam</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qualification Exam Module (for reviewer selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module that tests reviewer-candidate LLMs on a curated set of evaluation tasks (here: ratings of outputs from a subset of evaluatee LLMs) and retains only those whose Agreement with human judgments exceeds a pass threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple LLMs (reviewer candidates drawn from the eleven LLMs in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Reviewer candidates are the same LLMs used as evaluatees; performance in the exam is summarized via Agreement with human annotations and used to determine reviewer weights.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation (serves as a general reviewer-selection mechanism for LLM-based evaluation workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Give reviewer candidates a set of (text, summary) or (question, answer) rating tasks (5-level, 100-level, or pairwise); compare their ratings to human annotations; compute Agreement; retain reviewers with Agreement >= threshold (authors used 60%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Agreement metric (for pairwise comparisons; for pointwise ratings authors convert to pairwise first), pass threshold p (default 60% in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Qualification exam items were constructed from outputs of three evaluatee LLMs (GPT-3.5-turbo, FastChat-t5-3b, Alpaca-7b) on the same task distributions (XSum/NF-CATS); a small subset of human annotations provided ground truth for the exam.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Applying the qualification exam filtered out unreliable reviewer candidates; using the exam improved the final PRE performance. Authors report that removing the exam (allowing all candidates) significantly degrades PRE's performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Creating exam ground truth requires human labels (costly); exam reusability and potential for overfitting if exam data becomes public; exam effectiveness depends on representativeness of exam items.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Compared to naively using all LLMs as reviewers, the qualification exam better aligns reviewer pool with human judgment fidelity and improves aggregated evaluation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Set a meaningful pass threshold (authors used 60%); design exam items to reflect target evaluation scenarios; consider combining manual exam with Auto-Exam when manual labeling budget is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRE: A Peer Review Based Large Language Model Evaluator', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9680.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9680.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WeightedAggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chair Decision Module (Weighted Voting Aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregation of reviewer ratings into final evaluation scores using reviewer-specific weights derived from qualification performance; pointwise scores are normalized before aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple reviewer LLMs selected by the Qualification Exam</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Each reviewer provides pointwise or pairwise judgments; their historical Agreement determines their weight.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation aggregation methods (generalizable to other LLM evaluation contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute weights w_l = log(p_l / (1 - p_l)) where p_l is the reviewer's Agreement in the qualification exam; for pointwise, normalize reviewer raw scores by mean-variance before weighted sum; for pairwise, sum weights for chosen preference and pick winner by higher weighted votes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Weighted aggregated rating R_x for each sample; weights reflect reviewer reliability from qualification exam.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on XSum and NF-CATS evaluation tasks (100 samples each) with reviewer outputs collected for all evaluatee responses.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Weighting reviewers by exam-derived log-odds improved final agreement with human judgments; variants with equal weights or no exam showed degraded performance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of weight function (log-odds) and normalization steps are design decisions; may be sensitive if qualification exam estimates p_l are noisy; reliance on exam quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Weighted aggregation formalizes a chair/editor role analogous to human peer review where more reliable reviewers have greater influence, improving alignment with human judgments compared to unweighted aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use reviewer reliability to weight votes (authors recommend log-odds transform), normalize pointwise scores per-reviewer, and keep a non-zero pass threshold for reviewer inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRE: A Peer Review Based Large Language Model Evaluator', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9680.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9680.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-Exam</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-Exam (unsupervised reliability test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised qualification method that measures an LLM's internal evaluation consistency by perturbing prompt content order and checking if its preferences or ratings remain consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Reviewer-candidate LLMs (same pool as for manual exam)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Evaluates LLM reliability without human labels by computing the proportion of consistent preference relations across prompt re-orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation; low-cost reviewer reliability screening.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Re-order inputs (swap summary order in pairwise; swap original text and summary in pointwise), compute proportion of consistent preference relations between original and permuted prompts; mark LLM reliable if consistency > threshold (authors used 55%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Consistency proportion across perturbations; threshold τ (author used 55%).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Tested on XSum tasks; used as alternative or complement to manual qualification exam.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PRE with Auto-Exam outperforms the non-exam (all-reviewers) variant but is less effective than the manual-exam variant that uses a subset of human annotations. Authors report Auto-Exam shows potential as a lower-cost filter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Auto-Exam measures only internal consistency, not alignment with human preferences; may admit consistently biased evaluators; performance lower than manual exam but useful when manual labels are scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Auto-Exam provides an unsupervised proxy for reviewer reliability; compared to human-ground-truth exam it is cheaper but less aligned to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use Auto-Exam as a complement to manual exam or when annotation budget is limited; set conservative threshold (authors used 55%) and validate with a small human-labeled subset if possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRE: A Peer Review Based Large Language Model Evaluator', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9680.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9680.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptFormats</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Prompt Settings (Pairwise / 5-level / 100-level)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three standardized prompt formats presented to reviewer LLMs: pairwise preference (output 'one'/'two'), 5-level Likert pointwise score (1-5 with explicit definitions), and 100-level pointwise score (0-100 with general guidance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Reviewer LLMs in experiments (those that passed qualification exam)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Prompt engineering defines the expected output format from reviewers and influences evaluation granularity and bias.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation task design (generalizable to other LLM-output evaluation scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Feed structured prompts to reviewer LLMs and parse outputs as ratings/preferences; for pairwise create two prompt orders to reduce position bias; for pointwise convert scores to pairwise when needed for Agreement metric.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise outputs directly compared to human pairwise preferences (Agreement). Pointwise evaluated via Kendall's tau and Spearman correlations versus human pointwise labels (authors convert to pairwise for some comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Prompts applied to outputs for XSum and NF-CATS evaluation samples.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Pairwise prompt setting slightly outperformed pointwise settings in overall Agreement; difference between 5-level and 100-level pointwise was small. Authors recommend pairwise when resources permit.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pointwise rating scales require clear guidance to ensure interpretable scores and to convert to pairwise; output parsing may fail if LLMs produce extra text (authors used standardized prompts and parsing).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Pairwise preference judgments are closer to comparative human evaluations used in crowdsourcing; Likert 5-level aligns with standard human annotation practice.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer pairwise format for higher consistency with human preferences; provide detailed definitions for coarse Likert scales (5-level); in pairwise, include both orderings to mitigate position bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRE: A Peer Review Based Large Language Model Evaluator', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9680.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9680.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvaluationMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Metrics and Bias Measures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of meta-evaluation metrics and statistical tests used to compare model evaluators against human ground truth and to detect evaluator bias, including Agreement (A), Kendall's tau, Spearman rho, and Preference Gap (PG) with paired t-tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Evaluation methods applied to outputs from multiple LLMs (see paper for list)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Metrics used to quantify alignment between automated evaluator outputs and human annotations, and to measure bias between evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology / statistics applied to NLP model assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Agreement (proportion of identical pairwise preferences between model and human), Kendall's tau and Spearman correlation for pointwise scores vs human pointwise labels; Preference Gap (PG) between two LLM evaluators defined as difference in proportions each assigns that favor the other's outputs; paired-sample t-tests to assess significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Agreement (A) for pairwise; Kendall's tau and Spearman rho for pointwise (averaged across tasks); PG distribution summarises evaluator bias across model pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on human-annotated XSum and NF-CATS subsets (100 samples each).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PRE achieved highest Agreement with humans among methods tested. Preference Gap analysis shows significant evaluator bias for single-LLM evaluators: proportions of PG>0 were 66.67% (pairwise), 57.14% (5-level), and 76.19% (100-level) with paired t-test p-values 0.038, 0.263, and 0.006 respectively — indicating significant bias in pairwise and 100-level settings (notably GPT-4 biased toward its own-family outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Agreement and correlation metrics require human-labeled ground truth (costly); PG measures relative bias but not absolute correctness; statistical tests limited by sample size (authors used 100 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These metrics quantify alignment to human judgments and allow formal comparison between automated evaluators and human annotators; PRE optimized to maximize Agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report multiple meta-evaluation metrics (Agreement for pairwise, Kendall/Spearman for pointwise), test evaluator bias using PG, and run significance tests (paired t-tests) when comparing evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRE: A Peer Review Based Large Language Model Evaluator', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9680.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9680.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-based and Model-based Baselines (ROUGE / BLEU / BERTScore / GPTScore / PandaLM / Single-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of commonly used automatic evaluation metrics and model-based evaluators used as baselines: reference-based word-similarity metrics (ROUGE, BLEU, BERTScore), model-based evaluators (GPTScore variants, PandaLM), and using a single LLM as evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Single-LLM baselines include GPT-4, GPT-3.5-turbo, Claude-1, etc.; model-based baselines include GPTScore (text-davinci-003, FLAN-T5-XL) and PandaLM (fine-tuned Llama-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Baselines span unsupervised text-similarity metrics (ROUGE, BLEU, BERTScore) and recent LLM-based scoring approaches (GPTScore using generation probabilities; PandaLM fine-tuned for preference judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automatic evaluation of generated text (NLG), used for comparison against PRE.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>ROUGE/BLEU/BERTScore compare generated outputs to reference summaries; GPTScore computes generation probability under an LLM; PandaLM is a fine-tuned evaluator predicting preferences; single-LLM baseline uses the same reviewer prompts as PRE but only one model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Reference metrics (ROUGE variants, BLEU-1/2, BERTScore); human-alignment measured via Agreement/Correlation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on the same XSum and NF-CATS test subsets used for PRE experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reference metrics correlated positively with human annotations but performed worse than LLM-based evaluators; GPTScore (text-davinci-003) scored 0.6910 Agreement on XSum; PandaLM performed competitively on NF-CATS in some settings; single-LLM evaluators (e.g., GPT-4) perform well but show evaluator bias. Overall PRE outperformed these baselines (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reference-based metrics bias toward reference similarity and can penalize valid but different outputs; model-based evaluators can inherit bias from the underlying model and are not universally generalizable across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Model-based LLM evaluators and PRE are more aligned with human judgments than raw reference-similarity metrics; PRE's multi-reviewer aggregation reduces single-model bias compared to single-LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Avoid relying solely on reference-based metrics for open-ended generation; prefer multi-evaluator approaches or calibrated LLM-based evaluators and test for evaluator bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRE: A Peer Review Based Large Language Model Evaluator', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization <em>(Rating: 2)</em></li>
                <li>PRD: Peer rank and discussion improve large language model based evaluations <em>(Rating: 2)</em></li>
                <li>CHATEVAL: Towards better llm-based evaluators through multi-agent debate <em>(Rating: 2)</em></li>
                <li>GPTEval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9680",
    "paper_id": "paper-267311508",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "PRE",
            "name_full": "Peer Review Evaluator",
            "brief_description": "An automatic LLM evaluation framework that uses a peer-review mechanism: (1) a qualification exam to select qualified LLM reviewers, (2) reviewers rate evaluatee outputs (pointwise or pairwise), and (3) a \"chair\" aggregates reviewer scores via weighted voting to produce final rankings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple LLMs (GPT-4, Claude-1, GPT-3.5-turbo, Llama-2-70b-chat, Vicuna-7b, ChatGLM2-6B, RWKV-4-Raven-7B, Alpaca-7b, FastChat-t5-3b, ChatGLM-Pro, Baichuan-2-13b)",
            "llm_description": "A mix of closed-source and open-source LLMs used both as evaluatees and reviewer candidates; sizes and relative ranks reported in Table 1 (e.g., GPT-4 highest ELO, Llama-2-70b-chat 70B params, several 7B/13B/70B models).",
            "scientific_domain": "Natural language generation evaluation (text summarization and non-factoid QA); used as a general automatic-evaluation framework that could be adapted to other domains (not explicitly applied to scientific-theory generation in this paper).",
            "evaluation_method": "Peer-review among LLMs: construct qualification exam (human-annotated or Auto-Exam), select reviewers whose Agreement exceeds threshold, have reviewers rate evaluatee outputs in pointwise (5- or 100-level) or pairwise formats, then aggregate using weighted voting based on reviewer qualification performance.",
            "evaluation_criteria": "Agreement with human preferences for pairwise evaluation; Kendall's tau and Spearman correlation for pointwise evaluations; reviewer exam Agreement used as reviewer reliability measure and to compute reviewer weights; Preference Gap (PG) used to measure evaluator bias.",
            "benchmark_or_dataset": "XSum (BBC single-document news summarization, sampled 100 examples) and NF-CATS (non-factoid QA, filtered and sampled 100 examples). Human annotations collected (pointwise 5-level Likert + preference annotations) used as ground truth.",
            "results_summary": "PRE outperformed all baselines including single-LLM evaluators (e.g., GPT-4). Key reported Agreement scores (pairwise on XSum): PRE 0.7443, PRE w/o GPT-4 0.7328, GPT-4 alone 0.7369. PRE is robust to hyperparameter changes (pass threshold and weighting), and pairwise prompt setting performs slightly better than pointwise.",
            "limitations_or_challenges": "Requires qualification exam data (authors used human annotations for exam; could be costly); exam design and thresholds affect reviewer selection; experiments focus on summarization and QA, not direct evaluation of scientific-theory outputs; potential for reviewers to overfit if exam data become public (authors argue limited risk but note it).",
            "comparison_to_human_or_traditional": "PRE's aggregated outputs have higher consistency with human preferences than reference-based metrics (ROUGE/BLEU/BERTScore) and some single-LLM evaluators; human annotation used as gold standard (PRE aims to mimic and align with human judgments while reducing single-model biases).",
            "recommendations_or_best_practices": "Use a small qualification exam (authors used a 60% pass threshold) to filter reviewers, prefer pairwise evaluation when resources allow, weight reviewers by qualification performance (authors use log-odds of Agreement), normalize pointwise scores (mean-variance), and consider Auto-Exam as a low-cost complement to manual exam.",
            "uuid": "e9680.0",
            "source_info": {
                "paper_title": "PRE: A Peer Review Based Large Language Model Evaluator",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "QualificationExam",
            "name_full": "Qualification Exam Module (for reviewer selection)",
            "brief_description": "A module that tests reviewer-candidate LLMs on a curated set of evaluation tasks (here: ratings of outputs from a subset of evaluatee LLMs) and retains only those whose Agreement with human judgments exceeds a pass threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple LLMs (reviewer candidates drawn from the eleven LLMs in experiments)",
            "llm_description": "Reviewer candidates are the same LLMs used as evaluatees; performance in the exam is summarized via Agreement with human annotations and used to determine reviewer weights.",
            "scientific_domain": "NLP evaluation (serves as a general reviewer-selection mechanism for LLM-based evaluation workflows).",
            "evaluation_method": "Give reviewer candidates a set of (text, summary) or (question, answer) rating tasks (5-level, 100-level, or pairwise); compare their ratings to human annotations; compute Agreement; retain reviewers with Agreement &gt;= threshold (authors used 60%).",
            "evaluation_criteria": "Agreement metric (for pairwise comparisons; for pointwise ratings authors convert to pairwise first), pass threshold p (default 60% in experiments).",
            "benchmark_or_dataset": "Qualification exam items were constructed from outputs of three evaluatee LLMs (GPT-3.5-turbo, FastChat-t5-3b, Alpaca-7b) on the same task distributions (XSum/NF-CATS); a small subset of human annotations provided ground truth for the exam.",
            "results_summary": "Applying the qualification exam filtered out unreliable reviewer candidates; using the exam improved the final PRE performance. Authors report that removing the exam (allowing all candidates) significantly degrades PRE's performance.",
            "limitations_or_challenges": "Creating exam ground truth requires human labels (costly); exam reusability and potential for overfitting if exam data becomes public; exam effectiveness depends on representativeness of exam items.",
            "comparison_to_human_or_traditional": "Compared to naively using all LLMs as reviewers, the qualification exam better aligns reviewer pool with human judgment fidelity and improves aggregated evaluation quality.",
            "recommendations_or_best_practices": "Set a meaningful pass threshold (authors used 60%); design exam items to reflect target evaluation scenarios; consider combining manual exam with Auto-Exam when manual labeling budget is limited.",
            "uuid": "e9680.1",
            "source_info": {
                "paper_title": "PRE: A Peer Review Based Large Language Model Evaluator",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "WeightedAggregation",
            "name_full": "Chair Decision Module (Weighted Voting Aggregation)",
            "brief_description": "Aggregation of reviewer ratings into final evaluation scores using reviewer-specific weights derived from qualification performance; pointwise scores are normalized before aggregation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple reviewer LLMs selected by the Qualification Exam",
            "llm_description": "Each reviewer provides pointwise or pairwise judgments; their historical Agreement determines their weight.",
            "scientific_domain": "NLP evaluation aggregation methods (generalizable to other LLM evaluation contexts).",
            "evaluation_method": "Compute weights w_l = log(p_l / (1 - p_l)) where p_l is the reviewer's Agreement in the qualification exam; for pointwise, normalize reviewer raw scores by mean-variance before weighted sum; for pairwise, sum weights for chosen preference and pick winner by higher weighted votes.",
            "evaluation_criteria": "Weighted aggregated rating R_x for each sample; weights reflect reviewer reliability from qualification exam.",
            "benchmark_or_dataset": "Applied on XSum and NF-CATS evaluation tasks (100 samples each) with reviewer outputs collected for all evaluatee responses.",
            "results_summary": "Weighting reviewers by exam-derived log-odds improved final agreement with human judgments; variants with equal weights or no exam showed degraded performance in experiments.",
            "limitations_or_challenges": "Choice of weight function (log-odds) and normalization steps are design decisions; may be sensitive if qualification exam estimates p_l are noisy; reliance on exam quality.",
            "comparison_to_human_or_traditional": "Weighted aggregation formalizes a chair/editor role analogous to human peer review where more reliable reviewers have greater influence, improving alignment with human judgments compared to unweighted aggregation.",
            "recommendations_or_best_practices": "Use reviewer reliability to weight votes (authors recommend log-odds transform), normalize pointwise scores per-reviewer, and keep a non-zero pass threshold for reviewer inclusion.",
            "uuid": "e9680.2",
            "source_info": {
                "paper_title": "PRE: A Peer Review Based Large Language Model Evaluator",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Auto-Exam",
            "name_full": "Auto-Exam (unsupervised reliability test)",
            "brief_description": "An unsupervised qualification method that measures an LLM's internal evaluation consistency by perturbing prompt content order and checking if its preferences or ratings remain consistent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Reviewer-candidate LLMs (same pool as for manual exam)",
            "llm_description": "Evaluates LLM reliability without human labels by computing the proportion of consistent preference relations across prompt re-orderings.",
            "scientific_domain": "NLP evaluation; low-cost reviewer reliability screening.",
            "evaluation_method": "Re-order inputs (swap summary order in pairwise; swap original text and summary in pointwise), compute proportion of consistent preference relations between original and permuted prompts; mark LLM reliable if consistency &gt; threshold (authors used 55%).",
            "evaluation_criteria": "Consistency proportion across perturbations; threshold τ (author used 55%).",
            "benchmark_or_dataset": "Tested on XSum tasks; used as alternative or complement to manual qualification exam.",
            "results_summary": "PRE with Auto-Exam outperforms the non-exam (all-reviewers) variant but is less effective than the manual-exam variant that uses a subset of human annotations. Authors report Auto-Exam shows potential as a lower-cost filter.",
            "limitations_or_challenges": "Auto-Exam measures only internal consistency, not alignment with human preferences; may admit consistently biased evaluators; performance lower than manual exam but useful when manual labels are scarce.",
            "comparison_to_human_or_traditional": "Auto-Exam provides an unsupervised proxy for reviewer reliability; compared to human-ground-truth exam it is cheaper but less aligned to humans.",
            "recommendations_or_best_practices": "Use Auto-Exam as a complement to manual exam or when annotation budget is limited; set conservative threshold (authors used 55%) and validate with a small human-labeled subset if possible.",
            "uuid": "e9680.3",
            "source_info": {
                "paper_title": "PRE: A Peer Review Based Large Language Model Evaluator",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PromptFormats",
            "name_full": "Evaluation Prompt Settings (Pairwise / 5-level / 100-level)",
            "brief_description": "Three standardized prompt formats presented to reviewer LLMs: pairwise preference (output 'one'/'two'), 5-level Likert pointwise score (1-5 with explicit definitions), and 100-level pointwise score (0-100 with general guidance).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Reviewer LLMs in experiments (those that passed qualification exam)",
            "llm_description": "Prompt engineering defines the expected output format from reviewers and influences evaluation granularity and bias.",
            "scientific_domain": "NLP evaluation task design (generalizable to other LLM-output evaluation scenarios).",
            "evaluation_method": "Feed structured prompts to reviewer LLMs and parse outputs as ratings/preferences; for pairwise create two prompt orders to reduce position bias; for pointwise convert scores to pairwise when needed for Agreement metric.",
            "evaluation_criteria": "Pairwise outputs directly compared to human pairwise preferences (Agreement). Pointwise evaluated via Kendall's tau and Spearman correlations versus human pointwise labels (authors convert to pairwise for some comparisons).",
            "benchmark_or_dataset": "Prompts applied to outputs for XSum and NF-CATS evaluation samples.",
            "results_summary": "Pairwise prompt setting slightly outperformed pointwise settings in overall Agreement; difference between 5-level and 100-level pointwise was small. Authors recommend pairwise when resources permit.",
            "limitations_or_challenges": "Pointwise rating scales require clear guidance to ensure interpretable scores and to convert to pairwise; output parsing may fail if LLMs produce extra text (authors used standardized prompts and parsing).",
            "comparison_to_human_or_traditional": "Pairwise preference judgments are closer to comparative human evaluations used in crowdsourcing; Likert 5-level aligns with standard human annotation practice.",
            "recommendations_or_best_practices": "Prefer pairwise format for higher consistency with human preferences; provide detailed definitions for coarse Likert scales (5-level); in pairwise, include both orderings to mitigate position bias.",
            "uuid": "e9680.4",
            "source_info": {
                "paper_title": "PRE: A Peer Review Based Large Language Model Evaluator",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "EvaluationMetrics",
            "name_full": "Evaluation Metrics and Bias Measures",
            "brief_description": "Set of meta-evaluation metrics and statistical tests used to compare model evaluators against human ground truth and to detect evaluator bias, including Agreement (A), Kendall's tau, Spearman rho, and Preference Gap (PG) with paired t-tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Evaluation methods applied to outputs from multiple LLMs (see paper for list)",
            "llm_description": "Metrics used to quantify alignment between automated evaluator outputs and human annotations, and to measure bias between evaluators.",
            "scientific_domain": "Evaluation methodology / statistics applied to NLP model assessment.",
            "evaluation_method": "Agreement (proportion of identical pairwise preferences between model and human), Kendall's tau and Spearman correlation for pointwise scores vs human pointwise labels; Preference Gap (PG) between two LLM evaluators defined as difference in proportions each assigns that favor the other's outputs; paired-sample t-tests to assess significance.",
            "evaluation_criteria": "Agreement (A) for pairwise; Kendall's tau and Spearman rho for pointwise (averaged across tasks); PG distribution summarises evaluator bias across model pairs.",
            "benchmark_or_dataset": "Applied on human-annotated XSum and NF-CATS subsets (100 samples each).",
            "results_summary": "PRE achieved highest Agreement with humans among methods tested. Preference Gap analysis shows significant evaluator bias for single-LLM evaluators: proportions of PG&gt;0 were 66.67% (pairwise), 57.14% (5-level), and 76.19% (100-level) with paired t-test p-values 0.038, 0.263, and 0.006 respectively — indicating significant bias in pairwise and 100-level settings (notably GPT-4 biased toward its own-family outputs).",
            "limitations_or_challenges": "Agreement and correlation metrics require human-labeled ground truth (costly); PG measures relative bias but not absolute correctness; statistical tests limited by sample size (authors used 100 examples).",
            "comparison_to_human_or_traditional": "These metrics quantify alignment to human judgments and allow formal comparison between automated evaluators and human annotators; PRE optimized to maximize Agreement.",
            "recommendations_or_best_practices": "Report multiple meta-evaluation metrics (Agreement for pairwise, Kendall/Spearman for pointwise), test evaluator bias using PG, and run significance tests (paired t-tests) when comparing evaluators.",
            "uuid": "e9680.5",
            "source_info": {
                "paper_title": "PRE: A Peer Review Based Large Language Model Evaluator",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Baselines",
            "name_full": "Reference-based and Model-based Baselines (ROUGE / BLEU / BERTScore / GPTScore / PandaLM / Single-LLM)",
            "brief_description": "A collection of commonly used automatic evaluation metrics and model-based evaluators used as baselines: reference-based word-similarity metrics (ROUGE, BLEU, BERTScore), model-based evaluators (GPTScore variants, PandaLM), and using a single LLM as evaluator.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_name": "Single-LLM baselines include GPT-4, GPT-3.5-turbo, Claude-1, etc.; model-based baselines include GPTScore (text-davinci-003, FLAN-T5-XL) and PandaLM (fine-tuned Llama-7B).",
            "llm_description": "Baselines span unsupervised text-similarity metrics (ROUGE, BLEU, BERTScore) and recent LLM-based scoring approaches (GPTScore using generation probabilities; PandaLM fine-tuned for preference judgments).",
            "scientific_domain": "Automatic evaluation of generated text (NLG), used for comparison against PRE.",
            "evaluation_method": "ROUGE/BLEU/BERTScore compare generated outputs to reference summaries; GPTScore computes generation probability under an LLM; PandaLM is a fine-tuned evaluator predicting preferences; single-LLM baseline uses the same reviewer prompts as PRE but only one model.",
            "evaluation_criteria": "Reference metrics (ROUGE variants, BLEU-1/2, BERTScore); human-alignment measured via Agreement/Correlation metrics.",
            "benchmark_or_dataset": "Evaluated on the same XSum and NF-CATS test subsets used for PRE experiments.",
            "results_summary": "Reference metrics correlated positively with human annotations but performed worse than LLM-based evaluators; GPTScore (text-davinci-003) scored 0.6910 Agreement on XSum; PandaLM performed competitively on NF-CATS in some settings; single-LLM evaluators (e.g., GPT-4) perform well but show evaluator bias. Overall PRE outperformed these baselines (Table 2).",
            "limitations_or_challenges": "Reference-based metrics bias toward reference similarity and can penalize valid but different outputs; model-based evaluators can inherit bias from the underlying model and are not universally generalizable across tasks.",
            "comparison_to_human_or_traditional": "Model-based LLM evaluators and PRE are more aligned with human judgments than raw reference-similarity metrics; PRE's multi-reviewer aggregation reduces single-model bias compared to single-LLM evaluators.",
            "recommendations_or_best_practices": "Avoid relying solely on reference-based metrics for open-ended generation; prefer multi-evaluator approaches or calibrated LLM-based evaluators and test for evaluator bias.",
            "uuid": "e9680.6",
            "source_info": {
                "paper_title": "PRE: A Peer Review Based Large Language Model Evaluator",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 2,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
            "rating": 2,
            "sanitized_title": "pandalm_an_automatic_evaluation_benchmark_for_llm_instruction_tuning_optimization"
        },
        {
            "paper_title": "PRD: Peer rank and discussion improve large language model based evaluations",
            "rating": 2,
            "sanitized_title": "prd_peer_rank_and_discussion_improve_large_language_model_based_evaluations"
        },
        {
            "paper_title": "CHATEVAL: Towards better llm-based evaluators through multi-agent debate",
            "rating": 2,
            "sanitized_title": "chateval_towards_better_llmbased_evaluators_through_multiagent_debate"
        },
        {
            "paper_title": "GPTEval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 1,
            "sanitized_title": "gpteval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        }
    ],
    "cost": 0.01675125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PRE: A Peer Review Based Large Language Model Evaluator
3 Jun 2024</p>
<p>QingyaoZhumin Chu 
Qingyao Ai 
Yiteng Tu yitengtu16@gmail.com 
Haitao Li 
Yiqun Liu yiqunliu@tsinghua.edu.cn </p>
<p>DCST
Quan Cheng Laboratory
Tsinghua University Zhongguancun Laboratory
BeijingChina</p>
<p>DCST
Quan Cheng Laboratory
Tsinghua University Zhongguancun Laboratory
BeijingChina</p>
<p>DCST
Quan Cheng Laboratory
Tsinghua University Zhongguancun Laboratory
BeijingChina</p>
<p>DCST
Quan Cheng Laboratory
Tsinghua University Zhongguancun Laboratory
BeijingChina</p>
<p>DCST
Quan Cheng Laboratory
Tsinghua University Zhongguancun Laboratory
BeijingChina</p>
<p>Conference'17, July 2017WashingtonDCUSA</p>
<p>PRE: A Peer Review Based Large Language Model Evaluator
3 Jun 20242C11AF2CA71DBC173DCCB47401262F32arXiv:2401.15641v2[cs.IR]
The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities.Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem.Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks.However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term.In order to address these issues, inspired by the peer review systems widely used in academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process.Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select "reviewers" from a couple of powerful LLMs.Then, to actually evaluate the "submissions" written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions.The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers.We conducted extensive experiments on both text summarization and non-factoid question answering tasks with eleven LLMs including GPT-4.The results demonstrate the existence of biasness when evaluating using a single LLM.Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.</p>
<p>INTRODUCTION</p>
<p>The continuous development of large-scale language models (LLMs) such as GPT-3 [5], PALM [8], and Llama [40] has sparked people's passion on Artifical General Intelligence in both academia and industry.A new generation of LLMs, led by  and Claude [2,3], can achieve competitive performance on a wide range of natural language processing tasks, even in zero-shot scenarios.Ever since the release of ChatGPT [31], a large number of LLMs have been developed, many of which can produce high-quality responses that achieve or even surpass human-level performance in many cases [1,29].</p>
<p>With rapid development of LLMs, how to evaluate the performance of LLMs both effectively and efficiently has become a crucial bottleneck that restricts LLMs' progress.A reliable and reusable LLM evaluation method not only helps us better select the best LLMs for each task, but also provides important guidelines for LLM optimization.</p>
<p>To the best of our knowledge, there are two types of evaluation paradigms for LLM: human evaluation and model-based evaluation.The former hires human annotators to judge the quality of responses generated by LLMs directly or create gold references to evaluate the outputs of LLMs.The later trains separate evaluators for each task or uses a powerful LLM (e.g.,  to evaluate the performance of other LLMs.Unfortunately, due to their intrinsic characteristics, these methods often suffer from one or more of the following three problems:</p>
<p>(1) High cost: Human annotations have been considered the most effective and reliable data to evaluate the quality of LLM outputs [10,16,51].However, in commonly-used generation tasks, such as text summarization and question answering, different LLMs would output diverse responses, leading the cost of evaluation be approximately proportional to the number of evaluated LLMs.Reference-based methods [25,33,49] try to avoid this problem by requiring the annotators to provide gold references for each tasks instead of judging the quality of each LLM's outputs directly, but this could significantly increase the load and difficulty of the annotation jobs.Also, since LLMs are extremely powerful in terms of memorization, any public reference-based datasets can easily be incorporated and optimized by LLMs in the training process and thus become useless for evaluation after a short period of time.All these make the cost of annotation-based LLM evaluation methods prohibitive in long term.</p>
<p>(2) Low generalizibility: Existing evaluation methods, such as reference-based or model-based evaluators, often requires taskspecific dataset construction and evaluator pre-training [13,15,21,36,45].For example, Xu et al. [45] designed multiple-choice questions based on human references to evaluate LLMs.Similarly, studies like [36] fine-tune pretrained language models on each specific task with large-scale supervised data to create model-based evaluators.However, the evaluators created in these methods cannot be generalized to tasks beyond the target task of the references or the training data.Considering the large number and variety of LLM applications, the low generalizability of these evaluation methods make them not preferable for LLM evaluation.</p>
<p>(3) Inherent bias: Due to their intrinsic model structure or algorithm design, many evaluation methods are inherently biased in the evaluation process.For example, reference-based word similarity metrics (such as ROUGE [25], BLEU [33], and BERTScore [49]), which are commonly used to evaluate the outputs of LLMs in generation tasks, steer LLM outputs to be as similar as possible to the reference text, discriminating against LLMs that create qualified but different responses.Recently, many studies have adopted the state-of-the-art LLM, GPT-4, as their evaluation tools [19,26].Although several works have demonstrated that GPT-4 has decent evaluation capabilities [19,26], we found that GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses.In other words, if we use GPT-4 as the evaluator, its inherent bias may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4.</p>
<p>To address the aforementioned issues, we propose a novel framework, Peer Review Evaluator (PRE) 1 , to evaluate the performance of LLMs automatically.Inspired by the peer review mechanism in academic community, we propose to use LLMs as reviewers to evaluate the performance of LLMs directly.Specifically, we first develop a qualification exam to filter out LLMs that fail to provide reliable evaluation results.Then, qualified reviewer LLMs are required to assess the outputs of the evaluatee LLMs, and the final evaluation results are aggregated from all reviewer LLMs' ratings or preferences.To verified the effectiveness of our framework, we conducted extensive experiments on two representative text generation tasks, i.e., document summarization and non-factoid question answering.The experimental results show that the results of PRE model have the highest consistency with human preferences (ground truth) compared to all the baseline models including GPT-4.Comparing to previous evaluation methods, PRE can easily be generalized to different tasks and is highly cost efficient.Also, experiment results show that PRE provides much more robust evaluation results than methods that rely on specific model structures or LLMs.</p>
<p>In summary, our contributions can be summarized as follows:</p>
<p>• We propose a novel and automatic LLM evaluation framework PRE that incorporates peer review mechanisms.• Through the use of qualification exams and result fusions, PRE can achieve effective LLM evaluation while being robust to potential model bias, which has been widely observed in existing automatic evaluation methods.• We conducted extensive experiments with both the document summarization and non-factoid QA task to demonstrate the potential of PRE.</p>
<p>RELATED WORK 2.1 Large Language Models</p>
<p>Large Language Models (LLMs) typically refer to language models that contain more than a hundred billion parameters and have been  [31], , Claude [3], Claude 2 [2] and Gemini [38] which only offer API services instead of publicly available models.They tend to have enormous parameter sizes, and therefore reach top performance on all types of tasks.</p>
<p>For open source models, LLaMa is a foundation language model trained on publicly available data with parameter counts from 7B to 65B, which has shown decent performance on various benchmarks.Due to its excellent performance and the convenience of open source, many researchers have built on it to conduct continual pre-training or instruction tuning so as to enhance its capabilities further.Such customized models include Alpaca [37], Koala [12], and Vicuna [7].ChatGLM and ChatGLM2 [48], in addition to instruction tuning, are committed to utilizing quantitative methods to reduce model memory footprint and improve inference efficiency.FastChat-T5 [28] is based on the encoder-decoder transformer architecture, and is fine-tuned on the basis of Flan-T5 [9] with user-shared conversation data.Baichuan [39] is a Chinese-English bilingual language model trained on about 1.2 trillion tokens.Unlike the previous models, RWKV [34] adopts recurrent neural networks (RNNs) [35] as its underlying architecture.It can significantly reduce computational resources and improve computational efficiency.</p>
<p>Evaluation of Large Language Models</p>
<p>With the rapid development of LLMs, how to effectively evaluate the quality of the LLM generative texts has also become an urgent research question.We can simply categorize the existing evaluation approaches into the following categories:</p>
<p>Human Annotations: Human annotation has usually been regarded the most effective and reliable means for evaluating the outputs of LLMs.Recently, LMSYS has built a benchmark platform, Chatbot Arena [10,16,51], which allows different LLMs to engage in a fair, anonymous and random battle through crowdsourcing manner.Then, they adopted the ELO rating system to aggregate for the final leaderboard.However, as the number of evaluatee LLMs and evaluation tasks sharply increase, human annotation becomes increasingly unsustainable.Thus, effective semi-automatic or automatic evaluation methods become an urgent need.</p>
<p>Reference-based Word Similarity Metrics: Before the emergence of LLMs, there are a number of similarity metrics that could assess the quality of the generative text based on the reference text.BLEU [33] is a simple evaluation metric that focuses on the n-gram matches between generative text and reference text.ROUGE [25] is a set of recall-oriented metrics that measures the number of matching units like n-gram, word sequences, and word pairs.BERTScore [49] is an embedding-based metric that maps texts to embedding vectors and evaluates the quality of the generative text via cosine similarity.</p>
<p>Evaluation with Multi-Choice Questions: Multiple-choice questions, as a category of questions with fixed output formats, whose easy-evaluation properties lead a great deal of work [45,50] to construct benchmarks with such formatting.Such evaluation results are often more intuitive and aligned with human values.Su-perCLUE [45] is a Chinese comprehensive benchmark that assesses LLMs' general competencies through multiple-choice questions.GAOKAO [50] selects questions from the Chinese college entrance exams to assess the language comprehension and logical reasoning abilities of LLMs.</p>
<p>Evaluation using LLMs: Due to the stunning performance of LLMs, many studies attempted to employ one or multiple LLMs as evaluators for the evaluation of LLMs' outputs.GPTScore [11] evaluates the quality of generative texts using the generation probability of LLMs.It argues that higher quality text is more likely to be generated with the given instructions and context.PandaLM [43] trains LLaMa [40] to evaluate the results generated by LLMs through instruction tuning.Its training data is generated by ChatGPT via self-instruct [42].Safety-Prompts [36] is a Chinese LLMs safety assessment benchmark that utilizes LLMs to detect the potential unsafe situations in the input texts.PRD [23] and CHATEVAL [6], the two recent works, attempt to integrate multiple LLMs into the evaluation system to provide an aligned evaluation result by ranking, discussing, and debating among LLMs.Their experimental results found that a synergy of multiple LLMs could produce a higher consistency of evaluation results with human judgments.Different from previous studies, in this work, we propose an innovative evaluation framework for large language models based on the peer review system.This framework can automatically and unbiasedly integrate the evaluation results of multiple LLMs, providing more reliable assessments.</p>
<p>LLM PEER REVIEW EVALUATION</p>
<p>In this section, we provide a detailed introduction to the design motivation and specifics of our proposed LLMs evaluation framework.</p>
<p>Motivation</p>
<p>As discussed in Sec 1, a good LLM evaluation system should be affordable, generalizable and unbiased.Existing evaluation methods powered by a strong LLM (e.g., GPT-4) have been shown to be both effective and cost-efficient [1,29], but they also suffer from intrinsic limitations like inherent bias as discussed in Section 1.To this end, we propose to employ the peer review mechanism to integrate the evaluation results of multiple LLMs.</p>
<p>Peer review mechanisms are widely used in the academic field for paper reviewing.Journal editors or conference chairs invite experienced researchers in such research fields to act as reviewers, providing feedback and ratings on submitted papers.Editors or chairs take into account the reviewers' comments to make final decisions.Inspired by this mechanism, we apply it to the scenario of LLMs evaluation.Specifically, we consider multiple LLMs as potential reviewers.The evaluation framework, acting as the chair, selects qualified LLM reviewers to rate the outputs of each LLM on the task, and ultimately aggregates the reviewers' rates to provide the final evaluation results.</p>
<p>Framework Architecture</p>
<p>Figure 1 shows the architecture of our overall LLM evaluation framework: Peer Review Evaluator (PRE).The whole process can be divided into three modules: (1) Qualification exam module: conducting a qualification exam for all reviewer candidate LLMs to select qualified LLMs exceeding a certain level of evaluation capability; (2) Peer review module: collecting the outputs of evaluatee LLMs on the given assessment tasks, and then rating the outputs of all evaluatee LLMs by qualified reviewer LLMs; (3) "Chair" decision module: aggregating the ratings provided by all reviewer LLMs to obtain the final evaluation results.Below, we will provide detailed information regarding the design details of each module.</p>
<p>Qualification exam module.</p>
<p>Previous work has already demonstrated that LLMs have certain evaluation capabilities [1,29].Based on this finding, in our framework, any LLMs are allowed to participate in the evaluation process as reviewer candidates.Through the qualification examination module, we select LLMs whose evaluation capability is strong enough from the reviewer candidates to participate as reviewers in the peer review stage.Figure 2 illustrates the specific process of the qualification exam.This process relies on a set of qualification examination data, which should include a set of test cases to assess the evaluation capability of LLMs.We require each reviewer candidate to complete these evaluation tasks, and then compare candidates' outputs with the standard answers to obtain the evaluation scores for each candidate's capability.Only when the evaluation score of a reviewer candidate LLM reaches the admission threshold, will we add it to the reviewer pool.</p>
<p>There are two points worth further discussion regarding the qualification exam data: (1) Data acquisition: In order to closely approximate the application scenarios of reviewer LLMs, in our experimental setup, we used the outputs of a subset of LLMs in the evaluated task as the evaluation objects, constructing the qualification exam data.For simplicity, we use human annotations to create the qualification exam data (described in Sec 4.5), but please note that other unsupervised or semi-supervised methods [21,47] could also be used to create the exam.(2) Data reusability: The purpose of the qualification exam data is to assess the evaluation abilities of reviewer candidate LLMs.With proper design, a single set of qualification exam data can reflect the general evaluation abilities of reviewer candidate LLMs, thus making the reviewer selection results generalizable to multiple tasks.Also, the exam data is designed to evaluate LLM's ability as a reviewer.After the exam data are published, even if an LLM manages to trick the exam and become a reviewer by using the data in training, it doesn't mean that the LLM could stand out as an evaluatee in the actual testing stage (i.e., the peer reviewing process).This makes the whole framework more robust and reusable.</p>
<p>Peer review module.</p>
<p>In this module, we first collect the responses of all evaluatee LLMs to the given tasks.Then, each qualified reviewer LLM is required to rate the outputs in the response pool set.Specifically, organizers need to design prompts in advance for rating, and feed them into the reviewer LLMs.Then, they extract corresponding rating information from the reviewers' outputs.It is worth noting that the rating method here is not limited to pointwise</p>
<p>Task prompts</p>
<p>Tasks to be evaluated</p>
<p>Task responses</p>
<p>Responses pool set</p>
<p>Qualified peer reviewer LLMs</p>
<p>E v a lu a te p r o m p ts</p>
<p>Evaluate responses</p>
<p>Original evaluation results</p>
<p>Result aggregation</p>
<p>Final evaluation results</p>
<p>Qualification exam module</p>
<p>Filtering</p>
<p>LLM evaluation framework</p>
<p>Peer reviewer LLM candidates</p>
<p>Qualification exam module</p>
<p>Peer review module "Chair" decision module</p>
<p>Candidate</p>
<p>LLMs to be reviewed</p>
<p>Figure 1: The architecture of our evaluation framework for large language models</p>
<p>Peer reviewer LLM candidates</p>
<p>Questions</p>
<p>LLMs answer set</p>
<p>Qualified LLMs</p>
<p>Qualification exam data</p>
<p>Yes</p>
<p>Answers</p>
<p>Question prompts</p>
<p>Responses</p>
<p>Reach the pass line?</p>
<p>No</p>
<p>Filtered out LLMs</p>
<p>Figure 2: The process of the qualification exam module in our evaluation framework evaluation of each (task, response) pair, but can also be in pairwise or even listwise format.</p>
<p>3.2.3"Chair" decision module.After collecting the comments (ratings) from all the LLM reviewers, the "chair" (evaluation system) needs to aggregate the ratings to generate the final evaluation results.Specifically, we adopt a weighted voting strategy for rating aggregation, as shown in Eq 1.
𝑅 𝑥 = 1 𝑊 ∑︁ 𝑙 ∈𝐿 𝑤 𝑙 𝑟 (𝑙 ) 𝑥(1)
In Eq 1,  denotes the whole reviewer LLM set, and  ( )  denotes the LLM 's rating on sample .The vote weight   of each reviewer LLM is determined by its performance in the qualification exam, while  is the normalization term with  =  ∈   .</p>
<p>EXPERIMENTAL SETUP</p>
<p>In this section, we provide a quick introduction to the experimental setup, including the selection of tasks and LLMs, baseline setting, evaluation metrics, implementation details of the evaluation framework, and manual annotations.</p>
<p>Tasks and LLMs Selection</p>
<p>Given the limitations of multiple-choice questions, we chose two representative generation tasks that are more generalizable and more closely matched to real-world needs: text summarization and non-factoid question answering (QA).</p>
<p>As for the text summarization task, we adopted Extreme Summarization (XSum) [30] dataset to construct evaluation tasks.XSum is a real-world single-document news summary dataset collected from online articles by the British Broadcasting Corporation (BBC) and has been widely used in previous research [8,24].The entire XSum dataset contains over 220 thousand news documents.</p>
<p>As for the non-factoid QA task, we used the NF-CATS dataset [4] to create evaluation tasks.NF-CATS is an emerging non-factoid QA dataset that contains 11,984 non-factoid questions as well as their categorizations.We removed the questions belonging to the types "FACTOID" and "NOT-A-QUESTION" to construct the sample pooling set.</p>
<p>To validate the effectiveness of each evaluation method, we need to collect the most reliable evaluation data, i.e., human preferences over the LLM's outputs for each test case, as our ground truth.Due to our limited budget, we randomly sampled 100 samples from the XSum and NF-CATS datasets and used them as our testbed.In our experiments, we did not choose multiple-choice-format tasks like SuperCLUE [45], because they impose restrictions on the outputs of LLMs to be a particular label or option, making the evaluation approaches based on these tasks not generalizable to others.Instead, we directly asked evaluatee LLMs to provide response to each task and used the reviewer LLMs to rate the evaluatee's response, both in pointwise and pairwise manners.</p>
<p>We selected eleven powerful LLMs to conduct experiments, including LLMs in both closed-source (e.g.GPT-4 and Claude-1) and open-source (e.g.Llama-2-70b-chat and RWKV-4-Raven-7B) settings.In our experiments, these LLMs play dual roles as both evaluatees and reviewer candidates.Table 1 shows some basic information about these LLMs, as well as their ratings and rankings in the ELO leaderboard (i.e., a leaderboard of LLMs created based on human annotations) of LMSYS released in September 2023 [51].GPT-4 and Claude-1, recognized as two of the strongest existing LLMs, are ranked in the top two positions on the ELO leaderboard.</p>
<p>Baselines</p>
<p>We compare the performance of the PRE model with several baselines, including: ROUGE [25], BLEU [33], and BERTScore [49]: They are all reference-based word similarity metrics.These are a series of reference-based word similarity metrics.Both ROUGE and BLEU are classic unsupervised text similarity metrics.In our experiments, we use three variants of ROUGE (ROUGE-1, ROUGE-2 and ROUGE-L) as well as two variants of BLEU (BLEU-1 and BLEU-2).BERTScore, on the other hand, adopts a pretrained BERT-like model as the cornerstone and evaluates the similarity between the contextual representation of generative text and reference text.Here, we use deberta-xlarge-mnli [14] and roberta-large [27] as the base models for BERTScore.</p>
<p>PandaLM [43]: A fine-tuned language model based on Llama-7b [40] for the preference judgment tasks.</p>
<p>GPTScore [11]: Evaluate the quality of generative text based on its generation probability feeding into particular LLMs right after the given prompts.We use GPT-3 (text-davinci-003) [5] and FLAN-T5 (FT5-XL) [9] as the base models.</p>
<p>Single LLM: Only use a single LLM as an evaluator to assess the quality of the generative text.Its prompt setting is the same as the PRE model.The LLMs selected here are listed in Table 1.</p>
<p>Meta-evaluation Metrics</p>
<p>In our experiments, we collected manual annotations as the gold standard for the quality of LLM-generated summaries to evaluate the evaluation performance of the PRE model and baselines.Our annotation data has been organized into two different formats: (1) pairwise preferences, denoted as  ( 1 ,  2 , ), which indicate the user preference between summary  1 and  2 in terms of summarization quality for text .The function  assigns values of either  1 or  2 to represent the better summary; (2) pointwise labels, denoted as  (, ), which indicate the quality of summary  summarizing text .The details on the collection of manual annotation will be discussed in Section 4.5.</p>
<p>For these two different formats of labels, we proposed various evaluation metrics to measure the performance of LLM evaluation models.Specifically, (1) for pairwise labels, we use Agreement (A) to measure the proportion of identical preference results between the model and human annotations.(2) for pointwise labels, we use Kendall's tau () [18] and Spearman correlation coefficient () [22] to measure the consistency between the model's outputs ŷ (, ) and labels  (, ).We calculate  and  for each task, and report the mean of them as the overall performance.</p>
<p>Framework Details</p>
<p>4.4.1 Qualification exam module.To test the ability of reviewer candidate LLMs, we selected the outputs (i.e., summaries of test documents) of three evaluatee LLMs with varying quality: GPT-3.5turbo,Fastchat-t5-3b, and Alpaca-7b, as "questioners".Reviewer candidates are asked to rate these summaries.We designed three rating methods: 5-level pointwise, 100-level pointwise, and pairwise (or called preference).In both the 5-level and 100-level pointwise rating methods, the candidate LLMs need to rate an integer number for each (text, summary) pair to indicate its summarization quality.The differences between 5-level and 100-level settings are not only in the rating scale and granularity (1-5 levels and 0-100 levels), but also in the guidance style: the 5-level method offers detailed definition of each level, while the 100-level method only provides a general description on the quality tendency.The pairwise rating method requires candidate LLMs to rate the preference for each (text, summary 1, summary 2) tuple, determining which summary better summarize the text.To reduce bias caused by word position and frequency, we constructed two prompt samples ((,  1 ,  2 ) and (,  2 ,  1 )) for each text-summary-summary tuple (,  1 ,  2 ) in our experiments.</p>
<p>We uniformly designed prompts for these three rating methods, as specified in the Appendix A. Additionally, we collected human preferences as the ground truth for the exam, and then used Agreement (e.g., in the pointwise cases like 5-level or 100-level ratings, convert the rates to pairwise preferences first) in the pairwise mode as the evaluation metric to rate the evaluation ability of candidate LLMs.Only when an LLM's Agreement exceeds the threshold of , it will be retained as a reviewer for the peer review process.In our experiments, we set  to be 60%.</p>
<p>Peer review module.</p>
<p>For the text summarization task, we have devised a unified set of prompts to be fed into the whole eleven evaluatee LLMs.Specifically, we utilized the prompt template "Task: Generate a short summary of the text in at most 64 words.Text: {original text} Summary:".Then, only the LLMs that pass the qualification exam are deployed to rate the outputs of evaluatee LLMs.We fed the prompts designed in the Appendix A into reviewer LLMs and collected their scoring results.Overall, in the pointwise and pairwise modes, each reviewer LLM is required to generate 11 × 100 = 1, 100 rates or (11, 2) × 100 = 11, 000 preferences, respectively.</p>
<p>"</p>
<p>Chair" decision module.In Sec 3.2.3,Eq 1 already demonstrates the core idea of the weighted voting strategy.For pointwise and pairwise modes, we have different implementation details:</p>
<p>For the pointwise mode, each text-summary pair is treated as a sample .We first need to normalize the original LLM output score  ( )  using mean-variance normalization to eliminate its weighting effect.The weight of reviewer LLM   is determined by its Agreement   in the qualification exam.In the experiments, we set
𝑤 𝑙 = log 𝑝 𝑙 1 − 𝑝 𝑙
, just as Eq 2 shows, where is the normalization term.
𝑅 𝑥 = 1 𝑊 ∑︁ 𝑙 ∈𝐿 𝑤 𝑙 r (𝑙 ) 𝑥 = 1 𝑊 ∑︁ 𝑙 ∈𝐿 log 𝑝 𝑙 1 − 𝑝 𝑙 𝑟 (𝑙 ) 𝑥 − 𝜇 𝑙 𝜎 𝑙(2)
For the pairwise mode, let  represent a text-summary-summary tuple (  ,  1, ,  2, ).Each reviewer LLM  votes for its preference output    = arg max
𝑠 ∈ {𝑠 1,𝑥 ,𝑠 2,𝑥 } ∑︁ 𝑙 ∈𝐿 log 𝑝 𝑙 1 − 𝑝 𝑙 𝐼 (𝑟 (𝑙 ) 𝑥 = 𝑠)(3)</p>
<p>Manual Annotations</p>
<p>We conducted manual annotations serving for two purposes: (1) as ground truth for the LLM qualification exam (only use a small subset of annotations); (2) as a gold standard for evaluating the performance of different evaluation methods.Due to cost considerations, we conducted annotations on 7 out of the 11 LLMs that diversify in quality, developers, and model structure, as shown in Table 1.</p>
<p>To meet the requirements of both pointwise and pairwise evaluation metrics, we conducted pointwise annotation as well as auxiliary preference annotation 4 .</p>
<p>Here let us use the XSum dataset as an instance to introduce the annotation details.The design of NF-CATS dataset is quite similar to it.In the first step, we recruited annotators to conduct pointwise annotation.In each annotation task, we provided the annotators with a (text, summary) pair (or (question, answer) pair in non-factoid QA tasks), and they are required to give a rating on a 5-level scale.We adopted the Likert scale [17] as the 5-level annotation rule.For the statement "The summary text adequately and briefly summarizes the core meaning of the original text" levels 1 ∼ 5 respectively represent annotator strongly-disagrees/disagrees/neutralizes/agrees/stronglyagrees with the above statement.Furthermore, for all the summary pairs (in the same task) with tied ratings in the 5-level pointwise annotation, we recruited annotators to conduct preference annotations.Similar to the Likert scale, we designed a 7-level annotation rule ranging from -3 to 3. To improve the annotation experience of assessors, they are allowed to give any real number within the range [−3, 3], where levels −3 ∼ 3 respectively represent "compared to summary text 2, summary text 1 summarizes the core meaning of the original text strongly-better/better/slightly-better/tied/slightlyworse/worse/strongly-worse".The difference compared with general 7-level setting is that the assessment results are allowed to be any real number within the range [−3, 3] to improve the annotation experience of annotators.</p>
<p>We recruited annotators through Amazon's MTurk Crowdsourcing platform 5 , assigning 5 different annotators for each Human Intelligence Task (HIT).Overall, we collected 1,400 pointwise HITs and 1,704 preference HITs, collecting a total of 15,520 annotations at a cost of approximately 1,600 dollars.After removing the maximum and minimum labels, the annotations achieve fair annotation agreement: the mean intra-task Krippendorff's  [20] for pointwise and preference annotations are 0.4581 and 0.2983, respectively.</p>
<p>RESULTS AND ANALYSIS</p>
<p>In this section, we present the experimental results and attempt to answer the following three research questions (RQs):</p>
<p>(1) How does the performance of our proposed PRE model compare to other baseline methods?(2) Does the inherit bias really exist when evaluating with a single LLM? (3) How robust is the PRE model in evaluating LLMs?</p>
<p>Overall Results (RQ1)</p>
<p>Table 2 presents the overall experimental results, evaluated by Agreement metric, where PRE w/o GPT-4 represents the PRE variant model that excludes GPT-4 (currently recognized as the strongest LLM) out of the reviewer list.The results show that our proposed PRE model outperforms all the baselines including GPT-4.Even the variant without the GPT-4 model (PRE w/o GPT-4) achieves comparable evaluation results with GPT-4.This indicates that the peer review mechanism could effectively evaluate.</p>
<p>Experimental results show that GPT-4 performs the best among all LLMs in terms of evaluating the outputs of evaluatee LLMs.GPT-3.5-turbo,Claude-1 and ChatGLM-Pro also perform well in the evaluation task, as we expect.Surprisingly, FastChat-t5-3b, a model with only 3 billion parameters, achieves a comparable evaluation level to larger-scale LLMs such as Claude-1 and ChatGLM-Pro when evaluating text summarization tasks.We speculate that this is caused by the detailed design of its instruct tuning strategy during training, which makes it effective in dealing with such specific When comparing three different prompt settings, we find that the pairwise setting is slightly better than the pointwise ones, while the performance difference between the 5-level and 100-level pointwise settings is not significant.Therefore, we recommend using the pairwise setting when resources permit.</p>
<p>Table 2 also shows the performance of reference-based word similarity metrics such as ROUGE, BLUE, and BERTScore.We find that these metrics have positive correlations with human annotations, but the overall evaluation performance is worse compared to LLM-based methods like GPT-4 and PRE.PandaLM and GPTScore (text-davinci-003) show competitive performance in NF-CATS and XSum tasks respectively, but not in the other one.This phenomenon shows their performance is not robust across different tasks.</p>
<p>One point that needs clarification is that Table 2 shows that RWKV-4-Raven-7B and Alpaca-7b have an Agreement of preference prediction much less than 50% under the 5-level and 100-level pointwise settings in XSum dataset.This is mainly because these two LLMs have difficulties in understanding the problems in many cases, leading to the failure on extracting useful rating information from their outputs.</p>
<p>Robust Analysis (RQ3)</p>
<p>In this section, we aim to explore the robustness of the PRE model, that is, whether it still performs well when hyperparameters and qualification methods vary.</p>
<p>Here, we mainly attempted to adjust two hyperparameters: the pass threshold () for the qualification exam and the weight (  ) used during rating aggregation.We adjusted  to 55% and 0, where  = 0 indicates all candidate reviewers are allowed to participate in the peer review process.We also adjusted   to be 1, which means all reviewers have equal rating weight.</p>
<p>We also an qualification exam method called Auto-Exam, in which we evaluated the consistency of the LLM outputs before and after changing the order of content in the prompt.The output consistency is computed as the consistent proportion of the preference relations between two summaries of the same original text (under pointwise settings, ratings need to be converted into preference relations first for comparison).When the consistent proportion of such LLM exceeds a threshold , this LLM is regarded as a reliable one to join in the reviewer set.In our experiments, we adjusted the order of summary 1 and summary 2 under the pairwise setting, while adjusted the order of the original text and summary under the pointwise setting.Regarding the threshold, In our experiments, we set  = 55%.</p>
<p>We conducted experiments in XSum tasks, Figure 4 shows the performance of the PRE model under different hyperparameter and qualification settings, with GPT-4 used as the baseline.PRE + Auto-Exam denotes the variant of PRE method with both the original exam and Auto-Exam, while PRE only Auto-Exam denotes the variant with only Auto-Exam as the qualification exam and   = 1.Results show that the performance of the PRE model is not sensitive to the changes in its hyperparameters.Only when we remove all the effects of the qualification exam (i.e.,  = 0,   = 1), does the performance of PRE noticeably decrease.This finding corroborates the necessity of LLM qualification filtering.</p>
<p>Figure 4 also shows the effect of Auto-Exam method.We find that PRE with only Auto-Exam outperforms the non-exam one ( = 0,  = 1), but its performance is lower than the qualification exam with a subset of manual annotation as ground truth.This finding indicates the potential of Auto-Exam, which deserves further exploration.</p>
<p>CONCLUSION</p>
<p>In this paper, we propose a novel framework, Peer Review Evaluator (PRE), for automatically evaluating the performance of large language models (LLMs).Inspired by the peer-review mechanism in the academic community, we introduce a mutual evaluation mechanism among LLMs in our framework.By setting reasonable qualification exams and model aggregation criteria, our PRE model outperforms all baseline methods including GPT-4.In the experiments, we also validate the existence of bias when using a single model like GPT-4 as an evaluation tool.PRE could reduce this bias to some extent.We believe that our proposed PRE, an automatic LLM evaluation method, can be adaptable to various evaluation tasks and scenarios.</p>
<p>A THE DESIGN OF EVALUATION PROMPT</p>
<p>The evaluation prompts are adopted for both qualification exam and peer review modules (detailedly introduced in Sec 3.2).These prompts are fed to the reviewer (or reviewer candidate) LLMs, allowing them to generate ratings or preferences.In our experiments, we have proposed three different prompt settings (pairwise, 5-level pointwise and 100-level pointwise), and then seperately designed the prompt template for each setting, as the following shows.Here we show the design in XSum dataset under each setting, the design of NF-CATS dataset is almost the same.A.2 5-Level pointwise setting ###Task: Evaluate the summary of a given passage and determine how it summarizes the main points of the passage considering accuracy and conciseness.Directly output a number between 1 and 5 to indicate the quality score of this summary: -1 means the summary is not relevant to the passage, -2 means the summary is neither accurate nor concise but it is relevant to the passage, -3 means the summary is only a fair summary of the passage considering accuracy and conciseness, -4 means the summary is a good summary of the passage but still has room for improvement in accuracy and conciseness, -5 means the summary is a perfect summary of the passage considering accuracy and conciseness.</p>
<p>(either  1, or  2, ) with weight   .The preference result of the aggregated PRE model is determined by the summary with the higher votes.In our experiments, we also set   = log   1 −   , just as shown in Eq 3. The function  (•) denotes as the 0-1 Indicator function.</p>
<p>1 Figure 4 :
14
Figure 4: The performance of several PRE variants under different settings on XSum</p>
<p>A. 1
1
Pairwise setting ###Task: Evaluate two summaries of a given passage and determine which one better summarizes the main points of the passage considering accuracy and conciseness.You only need to output 'one' or 'two' directly to indicate which summary summarizes the passage better.###Passage: { passage } ###Summary one: { summary 1 } ###Summary two: { summary 2 } ###Output:</p>
<h1></h1>
<h2>Passage: { passage } ###Summary: { summary } ###Score of the summary: A.3 100-Level pointwise setting ###Task: Evaluate the summary of a given passage and determine how it summarizes the main points of the passage considering accuracy and conciseness.Directly output a number between 0 and 100 to indicate the score of this summary.The higher the score, the more accurate and concise the summary is.###Passage: { passage } ###Summary: { summary } ###Score of the summary:</h2>
<p>[44]ined on large amounts of textual data.The large-scale parameters and large amounts of training data of LLMs bring impressive capabilities, such as few-shot and zero-shot learning, where they can generate high-quality and reasonable text output with limited prompts.In addition, LLMs offer emergent abilities[44]that are not observed in smaller models, which are reflected in their stunning generalization performance on unseen tasks.According to open source or not, LLMs can be divided into two categories: closed source LLMs and open source LLMs.Closed source LLMs include ChatGPT</p>
<p>1 https://anonymous.4open.science/r/PRE-D66Apre</p>
<p>Table 1 :
1
The basic information of the large language models used in our experiments
ModelDeveloperSize (B)ELO rate (rank)Evaluatee EvaluatorAnnotation Examcandi-providerdateGPT-4 [32]Openai/1193 (1 / 28)✓✓Claude-1 [3]Anthropic/1161 (2 / 28)✓✓✓GPT-3.5-turbo [31]Openai/1118 (5 / 28)✓✓✓✓Llama-2-70b-chat [41]Meta701060 (7 / 28)✓✓Vicuna-7b [7]LMSYS71003 (14 / 28)✓✓✓ChatGLM2-6B [48]Tsinghua6965 (18 / 28)✓✓✓RWKV-4-Raven-7B [34]BlinkDL714B: 939 (21 / 28)✓✓✓Alpaca-7b [37]Stanford713B: 919 (22 / 28)✓✓✓✓FastChat-t5-3b [28]LMSYS3888 (25 / 28)✓✓✓✓ChatGLM-Pro [48]Tsinghua/N/A✓✓Baichuan-2-13b [46]Baichuan Inc.13N/A✓✓</p>
<p>Table 2 :
2
The overall performance of our proposed PRE models and baselines, evaluated by Agreement metric.The bold text indicates the best performing model.†/ † † indicates -value of paired sample t-test where the method outperforms GPT-4 is less than 0.05/0.01.The methods in the above part of table are compared with GPT-4 under pairwise setting.The underlined text denotes that the LLM passes the pairwise / 5-level / 100-level qualification exam, respectively.
(a) PRE and single LLM modelsEvaluation ModelsXSum pairwise 5-level point 100-level point pairwise 5-level point 100-level point NF-CATSRWKV-4-Raven-7B0.49720.00000.00000.50210.50830.4958Alpaca-7b0.50560.32490.39400.52860.54550.5155Vicuna-7b0.49480.47210.47320.52960.55570.5574ChatGLM2-6B0.56190.58390.61350.54140.57350.5958Baichuan-2-13b0.60570.54710.56530.55150.55210.5500Llama-2-70b-chat0.57190.58480.67040.58910.55150.6798GPT-3.5-turbo0.64700.66760.63610.60800.55860.5592Claude-10.67290.64840.64670.66130.57740.5881FastChat-t5-3b0.69210.62910.63020.65370.54110.5708ChatGLM-Pro0.69510.67010.71580.70420.64850.6887GPT-40.73690.69580.72060.78150.63300.6801PRE w/o GPT-4 (ours)0.73280.7242 †0.73340.74020.6604 † †0.7074 †PRE (ours)0.74430.7331 † †0.7390 † †0.78420.6935 † †0.7113 † †(b) Other baseline modelsEvaluation ModelsXSum NF-CATSmodelsXSum NF_CATSBERTScore (roberta)0.5728/BLEU-10.5505/BERTScore (deberta)0.5901/BLEU-20.5558/PandasLM0.63500.7205ROUGE-1 0.5884/GPTScore (flan-t5-xl)0.60230.4762ROUGE-2 0.5636/GPTScore (text-davinci-003) 0.69100.5940ROUGE-l 0.5798/rating tasks. By contrast, it performs relatively average in evaluatingnon-factoid QA tasks.
Due to the high cost, we conducted preference annotation only on the pairs with a tied pointwise label.
https://www.mturk.com/
Bias Analysis (RQ2)In this section, we dive into the results provided by different evaluation methods and investigate whether we could observe any type of evaluation bias in each of them.To measure potential bias in the evaluation using a single LLM, we propose the preference gap (PG) as an evaluation metric.Specifically, for LLMs  and , we define the preference gap between LLM  and  (PG(, )) as the proportion of 's outputs that are better than 's outputs from 's perspective, subtracted by the same proportion from 's perspective, as shown in Eq 4. Naturally, the larger PG(, ) is, the more likely the bias exists between LLMs  and .Ideally, for models without bias, the distribution of PG in the set { (, )| ∈ ,  ∈ ,  ≠  } is a random noise with a mean of 0.PG(𝑖We conducted experiments under XSum tasks.Figure3shows the heatmap distribution of the PG metric among seven powerful LLMs under different settings in XSum tasks.In the pairwise, 5level pointwise and 100-level pointwise settings, the proportions of PG values greater than 0 (i.e.,  has stronger preferences than  on the output of ) are 66.67%, 57.14% and 76.19% respectively, which are all significantly higher than the 50% of the unbiased scenario.Furthermore, we conducted paired samples t-tests, resulting in values of 0.038, 0.263, and 0.006 for these three settings, respectively.Conference'17, July 2017, Washington, DC, USA Zhumin Chu, Qingyao Ai 7 , Yiteng Tu, Haitao Li, and Yiqun Liu   The results indicate significant bias in evaluation using individual LLMs under the pairwise and 100-level pointwise settings.Looking back to Figure3, we also observe some more detailed conclusions: GPT-4 is likely to exhibit the most severe bias, as its PG values with all the other LLMs are all greater than 0 under all three settings; Baichuan-2-13b and Claude-1 also show relatively strong bias; the other four LLMs show weaker bias.
Rohaid Ali, Oliver Y Tang, Ian D Connolly, Patricia L Zadnik Sullivan, John H Shin, Jared S Fridley, Deus Wael F Asaad, Adetokunbo A Cielo, Curtis E Oyelese, Doberstein, Performance of ChatGPT and GPT-4 on neurosurgery written board examinations. 2022. 2022</p>
<p>. Anthropic, 2023</p>
<p>. Anthropic, 2023Introducing Claude</p>
<p>A Non-Factoid Question-Answering Taxonomy. Valeriia Bolotova, Vladislav Blinov, W Bruce Falk Scholer, Mark Croft, Sanderson, 10.1145/3477495.3531926202212</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023. 2023arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023. 14 April 2023. 2023</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022. 2022arXiv preprint</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022. 2022arXiv preprint</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, arXiv:2301.13867Mathematical capabilities of chatgpt. 2023. 2023arXiv preprint</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023. 2023arXiv preprint</p>
<p>Koala: A dialogue model for academic research. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, Dawn Song, Blog post. 2023. April 1 (2023</p>
<p>Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu He, Rui Xu, arXiv:2306.05783Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. 2023. 2023arXiv preprint</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.036542020. 2020arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020. 2020arXiv preprint</p>
<p>BECEL: Benchmark for Consistency Evaluation of Language Models. Myeongjun Jang, Deuk Sin Kwon, Thomas Lukasiewicz, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022</p>
<p>A review of key Likert scale development advances: 1995-2019. Vincent Andrew T Jebb, Louis Ng, Tay, Frontiers in psychology. 126375472021. 2021</p>
<p>A new measure of rank correlation. Maurice G Kendall, Biometrika. 3021938. 1938</p>
<p>Large language models are state-ofthe-art evaluators of translation quality. Tom Kocmi, Christian Federmann, arXiv:2302.145202023. 2023arXiv preprint</p>
<p>Computing Krippendorff's alpha-reliability. Klaus Krippendorff, 2011. 2011</p>
<p>Wojciech Kryściński, Bryan Mccann, Caiming Xiong, Richard Socher, arXiv:1910.12840Evaluating the factual consistency of abstractive text summarization. 2019. 2019arXiv preprint</p>
<p>JMP for basic univariate and multivariate statistics: methods for researchers and social scientists. Ann Lehman, Norm O 'rourke, Larry Hatcher, Edward Stepanski, 2013Sas Institute</p>
<p>Ruosen Li, arXiv:2307.02762Teerth Patel, and Xinya Du. 2023. Prd: Peer rank and discussion improve large language model based evaluations. 2023arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021. 2021arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634Gpteval: Nlg evaluation using gpt-4 with better human alignment. 2023. 2023arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, Erik Cambria, arXiv:2308.12488GPTEval: A survey on assessments of ChatGPT and GPT-4. 2023. 2023arXiv preprint</p>
<p>Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, ArXiv abs/1808.087452018. 2018</p>
<p>OpenAI. 2022. Introducing ChatGPT. </p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran, G V , arXiv:2305.13048RWKV: Reinventing RNNs for the Transformer Era. 2023. 2023arXiv preprint</p>
<p>Learning representations by back-propagating errors. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, nature. 3231986. 1986</p>
<p>Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang, arXiv:2304.10436Safety Assessment of Chinese Large Language Models. 2023. 2023arXiv preprint</p>
<p>Alpaca: A strong, replicable instruction-following model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford Center for Research on Foundation Models. 2023. 202337</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023. 2023arXiv preprint</p>
<p>Baichuan Intelligent Technology. 2023. Baichuan-7B. </p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022. 2022arXiv preprint</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, arXiv:2306.05087PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. 2023. 2023arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022. 2022arXiv preprint</p>
<p>Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei Zhang, Qiyue Kang, Zhenzhong Lan, arXiv:2307.15020SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. 2023. 2023arXiv preprint</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Dian Da Pan, Dong Wang, Fan Yan, Yang, arXiv:2309.10305Open Large-scale Language Models. 2023. 20232arXiv preprint</p>
<p>Automatic evaluation of attribution by large language models. Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, Huan Sun, arXiv:2305.063112023. 2023arXiv preprint</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022. 2022arXiv preprint</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019. 2019arXiv preprint</p>
<p>Evaluating the Performance of Large Language Models on GAOKAO Benchmark. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng Qiu, arXiv:2305.124742023. 2023arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, arXiv:2306.05685Eric Xing, et al. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
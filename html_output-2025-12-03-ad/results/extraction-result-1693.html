<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1693 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1693</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1693</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-235790370</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2022.wordplay-1.1.pdf" target="_blank">A Systematic Survey of Text Worlds as Embodied Natural Language Environments</a></p>
                <p><strong>Paper Abstract:</strong> Text Worlds are virtual environments for embodied agents that, unlike 2D or 3D environments, are rendered exclusively using textual descriptions. These environments offer an alternative to higher-fidelity 3D environments due to their low barrier to entry, providing the ability to study semantics, compositional inference, and other high-level tasks with rich action spaces while controlling for perceptual input. This systematic survey outlines recent developments in tooling, environments, and agent modeling for Text Worlds, while examining recent trends in knowledge graphs, common sense reasoning, transfer learning of Text World performance to higher-fidelity environments, as well as near-term development targets that, once achieved, make Text Worlds an attractive general research paradigm for natural language processing.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1693.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1693.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BUTLER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BUTLER (ALFWorld agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent introduced in ALFWorld that is first trained in a text-rendered version of ALFRED and then evaluated in the corresponding 3D ALFRED environment; reported to show increased task generalization when pretrained on the text world.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Alfworld: Aligning text and embodied environments for interactive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BUTLER</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Described as the agent introduced in ALFWorld that leverages a paired text rendering of ALFRED to learn high-level task procedures before being applied to the ALFRED 3D environment; the survey does not provide detailed architecture (e.g., network layers) for BUTLER.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>text-rendered versions of the ALFRED tasks (TextWorld rendering of ALFRED)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>TextWorld rendering of ALFRED tasks: high-level textual descriptions and state transitions that correspond to the ALFRED pick-and-place and household tasks. The survey refers to ALFWorld's paired text rendering of the ALFRED benchmark but does not provide dataset size or exact counts of trajectories used for BUTLER pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFRED (3D home environment tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>ALFRED is a 3D home environment benchmark for interpreting grounded instructions for everyday pick-and-place and multi-step household tasks; tasks require completing multi-step object-manipulation goals in visually-rendered indoor scenes (navigation, object pickup, object use, placement) with episodic instruction-driven objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level, language-like commands and descriptions representing task steps (e.g., textual step-by-step instructions / declarative action descriptions corresponding to pick/place/use operations).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete 3D environment action primitives used in ALFRED (navigation and manipulation primitives exposed by the 3D simulator); survey describes these as lower-level perceptual/motor actions required to realize high-level steps (not exhaustively enumerated in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>ALFWorld pairs the TextWorld (text rendering) and ALFRED 3D environment and uses an aligned semantics (PDDL is used in ALFWorld's text rendering for pick-and-place semantics) to connect textual high-level actions to the embodied environment's action primitives; the survey states the environment alignment and PDDL-based semantics mediate mapping, but does not give low-level mapping implementation details for BUTLER.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Low-level perceptual grounding in the 3D environment (visual / perceptual input) so the agent can map high-level textual plans to visual observations and environment affordances; the survey refers generally to 'visual input' / 'perceptual input' required to ground the high-level knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Survey reports that BUTLER (trained first on the text world) shows increased task generalization on the 3D environment compared to not pretraining on the text version; no numeric performance values are provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not specified numerically in the survey; described qualitatively as comparatively worse generalization and slower learning when not pretrained on text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative: reported faster grounding/learning in 3D when pretrained on text (survey states agents 'learn to ground... and complete tasks faster than when trained jointly'), but no quantitative sample counts or multiplicative gains are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Aligned task semantics between text and 3D (same underlying task definitions), use of an explicit semantics (PDDL) in the text rendering to represent pick-and-place operations, and the ability to learn high-level procedural knowledge in text that is applicable in the embodied setting.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Survey highlights general limiting factors: perception gap between text and visual modalities, potential mismatches between high-level textual actions and low-level motor primitives, and limits in environment complexity or coverage in the text rendering that could prevent full transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ALFWorld / BUTLER provide evidence that pretraining on a text-rendered version of a 3D benchmark can improve task generalization and speed grounding in the 3D environment; success relies on semantic alignment (shared task definitions / PDDL semantics) but the survey does not provide numeric measures of the gains or sample-efficiency improvements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1693.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1693.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (ALFRED text→3D studies)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (applied to ALFRED/ALFWorld task trajectory generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative pre-trained transformer (GPT-2) used in prior work to generate step-by-step textual trajectories or action sequences for ALFRED tasks from task descriptions, with reported positive outcomes when these text-level outputs are used in the ALFWorld/ALFRED context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-2 (language model used as a task-trajectory generator / high-level planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>GPT-2 is a transformer-based autoregressive language model; in the cited prior work it was trained/fine-tuned on ALFRED task descriptions / human gameplay text to generate step-by-step textual trajectories or action sequences corresponding to embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language data: ALFRED task descriptions and/or human gameplay transcripts; in-survey references describe models trained on task descriptions or human gameplay.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The survey cites Jansen (2020) who used GPT-2 with ALFRED task descriptions to generate step-by-step textual descriptions of ALFRED trajectories (reporting success on up to 58% of unseen cases using task descriptions alone). Micheli (2021) is cited as confirming GPT-2 performs well on ALFWorld's text-world rendering and 'is able to successfully complete goals in 95% of unseen cases.' The survey does not provide dataset sizes or training epoch counts for these GPT-2 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFRED (3D household tasks) via ALFWorld alignment</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Multi-step embodied household tasks from the ALFRED benchmark (instruction-conditioned pick-and-place and object usage tasks in simulated indoor scenes); GPT-2's text outputs are evaluated in the ALFWorld/ALFRED alignment to determine whether generated high-level plans can be realized in the embodied environment.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural-language step-by-step instructions or descriptive action sequences (textual trajectories describing the sequence of steps to complete a task).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>ALFRED's discrete/navigation and manipulation primitives in the 3D simulator (environment-specific low-level actions needed to execute textual plan steps in the visual world).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>The survey reports that GPT-2 produced textual trajectories / step-by-step instructions which were then aligned to the ALFWorld/ALFRED semantics so they could be executed or used as plans in the embodied environment; ALFWorld uses PDDL semantics for pick-and-place tasks to provide the mapping/grounding between text commands and environment actions. The survey does not provide implementation-level details of the mapping (e.g., whether templates, parsers, or learned grounding models were used).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Embodied execution requires grounding text into the 3D environment's perceptual observations (visual/perceptual input) so that textual steps correspond to detected objects and scene states; the survey frames this as requiring perceptual grounding but does not enumerate sensor modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported results cited in the survey: Jansen (2020) — GPT-2 generated correct step-by-step textual descriptions for up to 58% of unseen ALFRED cases using task descriptions alone; Micheli (2021) — GPT-2 on ALFWorld's text rendering was able to successfully complete goals in 95% of unseen cases. The survey does not provide direct numeric measures of downstream 3D execution success rates for GPT-2 outputs in ALFRED beyond these text-level/ALFWorld claims.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not specified numerically in the survey for these GPT-2 studies; baseline (no text pretraining) performance in embodied ALFRED is not given in the survey for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative: survey and cited work indicate substantial benefits in being able to produce usable high-level plans from language-only inputs and that these accelerate solving the embodied tasks, but no quantitative sample-efficiency ratios are provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Rich linguistic priors in pretrained language models (GPT-2) that allow generation of coherent multi-step procedures from task descriptions; availability of an aligned text rendering of the embodied environment (ALFWorld) and explicit semantics (PDDL) to ground generated text to environment actions; semantic similarity between text training data (task descriptions / human gameplay) and target embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Potential gaps include: mismatch between textual plan detail and the perceptual requirements to execute plans in the 3D environment; missing grounding for low-level perceptual disambiguation (object instance recognition/localization) in the embodied environment; the survey does not report specific failure-rate analyses for these GPT-2 transfers beyond the cited success percentages in text or text-world evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language models (GPT-2) can generate high-quality step-by-step plans from task descriptions that largely transfer to text-rendered versions of embodied benchmarks (ALFWorld) and, when aligned, provide strong high-level guidance for 3D embodied tasks; reported successes (58% of unseen cases for text-only trajectory generation in one study; 95% goal completion in another study on text-world rendering) indicate strong promise, but the survey notes limited published quantitative results for direct embodied execution in 3D and highlights the need to bridge the perception/action mapping and to measure sample-efficiency gains explicitly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Alfworld: Aligning text and embodied environments for interactive learning. <em>(Rating: 2)</em></li>
                <li>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. <em>(Rating: 2)</em></li>
                <li>Jansen (2020) (GPT-2 applied to ALFRED task trajectories) <em>(Rating: 2)</em></li>
                <li>Micheli (2021) (GPT-2 evaluated on ALFWorld text rendering) <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1693",
    "paper_id": "paper-235790370",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "BUTLER",
            "name_full": "BUTLER (ALFWorld agent)",
            "brief_description": "An agent introduced in ALFWorld that is first trained in a text-rendered version of ALFRED and then evaluated in the corresponding 3D ALFRED environment; reported to show increased task generalization when pretrained on the text world.",
            "citation_title": "Alfworld: Aligning text and embodied environments for interactive learning.",
            "mention_or_use": "mention",
            "model_agent_name": "BUTLER",
            "model_agent_description": "Described as the agent introduced in ALFWorld that leverages a paired text rendering of ALFRED to learn high-level task procedures before being applied to the ALFRED 3D environment; the survey does not provide detailed architecture (e.g., network layers) for BUTLER.",
            "pretraining_data_type": "text-rendered versions of the ALFRED tasks (TextWorld rendering of ALFRED)",
            "pretraining_data_details": "TextWorld rendering of ALFRED tasks: high-level textual descriptions and state transitions that correspond to the ALFRED pick-and-place and household tasks. The survey refers to ALFWorld's paired text rendering of the ALFRED benchmark but does not provide dataset size or exact counts of trajectories used for BUTLER pretraining.",
            "embodied_task_name": "ALFRED (3D home environment tasks)",
            "embodied_task_description": "ALFRED is a 3D home environment benchmark for interpreting grounded instructions for everyday pick-and-place and multi-step household tasks; tasks require completing multi-step object-manipulation goals in visually-rendered indoor scenes (navigation, object pickup, object use, placement) with episodic instruction-driven objectives.",
            "action_space_text": "High-level, language-like commands and descriptions representing task steps (e.g., textual step-by-step instructions / declarative action descriptions corresponding to pick/place/use operations).",
            "action_space_embodied": "Discrete 3D environment action primitives used in ALFRED (navigation and manipulation primitives exposed by the 3D simulator); survey describes these as lower-level perceptual/motor actions required to realize high-level steps (not exhaustively enumerated in the survey).",
            "action_mapping_method": "ALFWorld pairs the TextWorld (text rendering) and ALFRED 3D environment and uses an aligned semantics (PDDL is used in ALFWorld's text rendering for pick-and-place semantics) to connect textual high-level actions to the embodied environment's action primitives; the survey states the environment alignment and PDDL-based semantics mediate mapping, but does not give low-level mapping implementation details for BUTLER.",
            "perception_requirements": "Low-level perceptual grounding in the 3D environment (visual / perceptual input) so the agent can map high-level textual plans to visual observations and environment affordances; the survey refers generally to 'visual input' / 'perceptual input' required to ground the high-level knowledge.",
            "transfer_successful": true,
            "performance_with_pretraining": "Survey reports that BUTLER (trained first on the text world) shows increased task generalization on the 3D environment compared to not pretraining on the text version; no numeric performance values are provided in the survey.",
            "performance_without_pretraining": "Not specified numerically in the survey; described qualitatively as comparatively worse generalization and slower learning when not pretrained on text.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": "Qualitative: reported faster grounding/learning in 3D when pretrained on text (survey states agents 'learn to ground... and complete tasks faster than when trained jointly'), but no quantitative sample counts or multiplicative gains are provided.",
            "transfer_success_factors": "Aligned task semantics between text and 3D (same underlying task definitions), use of an explicit semantics (PDDL) in the text rendering to represent pick-and-place operations, and the ability to learn high-level procedural knowledge in text that is applicable in the embodied setting.",
            "transfer_failure_factors": "Survey highlights general limiting factors: perception gap between text and visual modalities, potential mismatches between high-level textual actions and low-level motor primitives, and limits in environment complexity or coverage in the text rendering that could prevent full transfer.",
            "key_findings": "ALFWorld / BUTLER provide evidence that pretraining on a text-rendered version of a 3D benchmark can improve task generalization and speed grounding in the 3D environment; success relies on semantic alignment (shared task definitions / PDDL semantics) but the survey does not provide numeric measures of the gains or sample-efficiency improvements.",
            "uuid": "e1693.0"
        },
        {
            "name_short": "GPT-2 (ALFRED text→3D studies)",
            "name_full": "GPT-2 (applied to ALFRED/ALFWorld task trajectory generation)",
            "brief_description": "Generative pre-trained transformer (GPT-2) used in prior work to generate step-by-step textual trajectories or action sequences for ALFRED tasks from task descriptions, with reported positive outcomes when these text-level outputs are used in the ALFWorld/ALFRED context.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "GPT-2 (language model used as a task-trajectory generator / high-level planner)",
            "model_agent_description": "GPT-2 is a transformer-based autoregressive language model; in the cited prior work it was trained/fine-tuned on ALFRED task descriptions / human gameplay text to generate step-by-step textual trajectories or action sequences corresponding to embodied tasks.",
            "pretraining_data_type": "Language data: ALFRED task descriptions and/or human gameplay transcripts; in-survey references describe models trained on task descriptions or human gameplay.",
            "pretraining_data_details": "The survey cites Jansen (2020) who used GPT-2 with ALFRED task descriptions to generate step-by-step textual descriptions of ALFRED trajectories (reporting success on up to 58% of unseen cases using task descriptions alone). Micheli (2021) is cited as confirming GPT-2 performs well on ALFWorld's text-world rendering and 'is able to successfully complete goals in 95% of unseen cases.' The survey does not provide dataset sizes or training epoch counts for these GPT-2 experiments.",
            "embodied_task_name": "ALFRED (3D household tasks) via ALFWorld alignment",
            "embodied_task_description": "Multi-step embodied household tasks from the ALFRED benchmark (instruction-conditioned pick-and-place and object usage tasks in simulated indoor scenes); GPT-2's text outputs are evaluated in the ALFWorld/ALFRED alignment to determine whether generated high-level plans can be realized in the embodied environment.",
            "action_space_text": "Natural-language step-by-step instructions or descriptive action sequences (textual trajectories describing the sequence of steps to complete a task).",
            "action_space_embodied": "ALFRED's discrete/navigation and manipulation primitives in the 3D simulator (environment-specific low-level actions needed to execute textual plan steps in the visual world).",
            "action_mapping_method": "The survey reports that GPT-2 produced textual trajectories / step-by-step instructions which were then aligned to the ALFWorld/ALFRED semantics so they could be executed or used as plans in the embodied environment; ALFWorld uses PDDL semantics for pick-and-place tasks to provide the mapping/grounding between text commands and environment actions. The survey does not provide implementation-level details of the mapping (e.g., whether templates, parsers, or learned grounding models were used).",
            "perception_requirements": "Embodied execution requires grounding text into the 3D environment's perceptual observations (visual/perceptual input) so that textual steps correspond to detected objects and scene states; the survey frames this as requiring perceptual grounding but does not enumerate sensor modalities.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported results cited in the survey: Jansen (2020) — GPT-2 generated correct step-by-step textual descriptions for up to 58% of unseen ALFRED cases using task descriptions alone; Micheli (2021) — GPT-2 on ALFWorld's text rendering was able to successfully complete goals in 95% of unseen cases. The survey does not provide direct numeric measures of downstream 3D execution success rates for GPT-2 outputs in ALFRED beyond these text-level/ALFWorld claims.",
            "performance_without_pretraining": "Not specified numerically in the survey for these GPT-2 studies; baseline (no text pretraining) performance in embodied ALFRED is not given in the survey for direct comparison.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": "Qualitative: survey and cited work indicate substantial benefits in being able to produce usable high-level plans from language-only inputs and that these accelerate solving the embodied tasks, but no quantitative sample-efficiency ratios are provided in the survey.",
            "transfer_success_factors": "Rich linguistic priors in pretrained language models (GPT-2) that allow generation of coherent multi-step procedures from task descriptions; availability of an aligned text rendering of the embodied environment (ALFWorld) and explicit semantics (PDDL) to ground generated text to environment actions; semantic similarity between text training data (task descriptions / human gameplay) and target embodied tasks.",
            "transfer_failure_factors": "Potential gaps include: mismatch between textual plan detail and the perceptual requirements to execute plans in the 3D environment; missing grounding for low-level perceptual disambiguation (object instance recognition/localization) in the embodied environment; the survey does not report specific failure-rate analyses for these GPT-2 transfers beyond the cited success percentages in text or text-world evaluations.",
            "key_findings": "Pretrained language models (GPT-2) can generate high-quality step-by-step plans from task descriptions that largely transfer to text-rendered versions of embodied benchmarks (ALFWorld) and, when aligned, provide strong high-level guidance for 3D embodied tasks; reported successes (58% of unseen cases for text-only trajectory generation in one study; 95% goal completion in another study on text-world rendering) indicate strong promise, but the survey notes limited published quantitative results for direct embodied execution in 3D and highlights the need to bridge the perception/action mapping and to measure sample-efficiency gains explicitly.",
            "uuid": "e1693.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Alfworld: Aligning text and embodied environments for interactive learning.",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks.",
            "rating": 2,
            "sanitized_title": "alfred_a_benchmark_for_interpreting_grounded_instructions_for_everyday_tasks"
        },
        {
            "paper_title": "Jansen (2020) (GPT-2 applied to ALFRED task trajectories)",
            "rating": 2,
            "sanitized_title": "jansen_2020_gpt2_applied_to_alfred_task_trajectories"
        },
        {
            "paper_title": "Micheli (2021) (GPT-2 evaluated on ALFWorld text rendering)",
            "rating": 2,
            "sanitized_title": "micheli_2021_gpt2_evaluated_on_alfworld_text_rendering"
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games.",
            "rating": 1,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        }
    ],
    "cost": 0.014384999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Systematic Survey of Text Worlds as Embodied Natural Language Environments
Wordplay 2022. July 14, 2022</p>
<p>Peter A Jansen pajansen@arizona.edu 
University of Arizona
TucsonAZ</p>
<p>A Systematic Survey of Text Worlds as Embodied Natural Language Environments</p>
<p>Proceedings of the 3rd Wordplay: When Language Meets Games Workshop
the 3rd Wordplay: When Language Meets Games WorkshopWordplay 2022. July 14, 2022
Text Worlds are virtual environments for embodied agents that, unlike 2D or 3D environments, are rendered exclusively using textual descriptions. These environments offer an alternative to higher-fidelity 3D environments due to their low barrier to entry, providing the ability to study semantics, compositional inference, and other high-level tasks with rich action spaces while controlling for perceptual input. This systematic survey outlines recent developments in tooling, environments, and agent modeling for Text Worlds, while examining recent trends in knowledge graphs, common sense reasoning, transfer learning of Text World performance to higher-fidelity environments, as well as near-term development targets that, once achieved, make Text Worlds an attractive general research paradigm for natural language processing. . 2016b. Deep reinforcement learning with a combinatorial action space for predicting popular Reddit threads. In Pro-. 2020. Qasc: A dataset for question answering via sentence composition. In</p>
<p>Introduction</p>
<p>Embodied agents offer an experimental paradigm to study the development and use of semantic representations for a variety of real-world tasks, from household tasks (Shridhar et al., 2020a) to navigation (Guss et al., 2019) to chemical synthesis (Tamari et al., 2021). While robotic agents are a primary vehicle for studying embodiment (e.g. Cangelosi and Schlesinger, 2015), robotic models are costly to construct, and experiments can be slow or difficult to scale. Virtual agents and embodied virtual environments help mitigate many of these issues, allowing large-scale simulations to be run in parallel orders of magnitude faster than real world environments (e.g. Deitke et al., 2020), while controlled virtual environments can be constructed for exploring specific tasks -though this benefit in speed comes at the cost of having to model virtual 3D environments, which can be substantial.</p>
<p>Text Worlds -embodied environments rendered linguistically through textual descriptions instead of graphically through pixels (see Table 1) -have Zork North of House You are facing the north side of a white house. There is no door here, and all the windows are barred. &gt;go north Forest This is a dimly lit forest, with large trees all around. One particularly large tree with some low branches stands here. &gt;climb large tree</p>
<p>Up a Tree</p>
<p>You are about 10 feet above the ground nestled among some large branches. On the branch is a small birds nest. In the bird's nest is a large egg encrusted with precious jewels, apparently scavenged somewhere by a childless songbird. &gt;take egg</p>
<p>Taken. &gt;climb down tree   (Lebling et al., 1979), frequently used as a benchmark for agent performance. User-entered actions are italicized.</p>
<p>emerged as a recent methodological focus that allow studying many embodied research questions while reducing some of the development costs associated with modeling complex and photorealistic 3D environments (e.g. Côté et al., 2018). More than simply reducing development costs, Text Worlds also offer paradigms to study developmental knowledge representation, embodied task learning, and transfer learning at a higher level than perceptuallygrounded studies, enabling different research questions that explore these topics in isolation of the open problems of perceptual input, object segmentation, and object classification regularly studied in the vision community (e.g. He et al., 2016c;Szegedy et al., 2017;Zhai et al., 2021).</p>
<p>Motivation for this survey</p>
<p>Text Worlds are rapidly gaining momentum as a research methodology in the natural language processing community. In spite of this interest, many modeling, evaluation, tooling, and other barriers exist to applying these methodologies, with significant development efforts in the early stages of mitigating those barriers, at least in part.</p>
<p>In this review, citation graphs of recent articles were iteratively crawled, identifying 108 articles relevant to Text Worlds and other embodied environments that include text as part of the simulation or task. Frequent motivations for choosing Text Worlds are highlighted in Section 2. Tooling and modeling paradigms (in the form of simulators, intermediate languages, and libraries) are surveyed in Section 3, with text environments and common benchmarks implemented with this tooling described in Section 4. Contemporary focuses in agent modeling, including coupling knowledge graphs, question answering, and common-sense reasoning with reinforcement learning, are identified in Section 5. Recent contributions to focus areas in world generation and hybrid text-3D environments are summarized in Section 6, while a distillation of near-term directions for reducing barriers to using Text Worlds more broadly as a research paradigm are presented in Section 7.</p>
<p>Why use Text Worlds?</p>
<p>For many tasks, Text Worlds can offer advantages over other embodied environment modelling paradigms -typically in reduced development costs, the ability to model large action spaces, and the ability to study embodied reasoning at a higher level than raw perceptual information.</p>
<p>Embodied Reasoning: Embodied agents have been proposed as a solution to the symbol grounding problem (Harnad, 1990), or the problem of how concepts acquire real-world meaning. Humans likely resolve symbol grounding at least partially by assigning semantics to concepts through perceptually-grounded mental simulations (Barsalou et al., 1999). Using embodied agents that take in perceptual data and perform actions in real or virtual environments offers an avenue for studying semantics and symbol grounding empirically (Cangelosi et al., 2010;Bisk et al., 2020;Tamari et al., 2020a,b). Text Worlds abstract some of the challenges in perceptual modeling, allowing agents to focus on higher-level semantics, while hybrid worlds that simultaneously render both text and 3D views (e.g. Shridhar et al., 2020b) help control what kind of knowledge is acquired, and better operationalize the study of symbol grounding.</p>
<p>Ease of Development: Constructing embodied virtual environments typically has steep development costs, but Text Worlds are typically easier to construct for many tasks. Creating new objects does not require the expensive process of creating new 3D models, or performing visualpercept-to-object-name segmentation or classification (since the scene is rendered linguistically). Similarly, a rich action semantics is possible, and comparatively easy to implement -while 3D environments typically have one or a small number of action commands (e.g. Kolve et al., 2017;Shridhar et al., 2020a), Text Worlds typically implement dozens of action verbs, and thousands of valid Verb-NounPhrase action combinations (Hausknecht et al., 2020).</p>
<p>Compositional Reasoning: Complex reasoning tasks typically require multi-step (or compositional) reasoning that integrates several pieces of knowledge in an action procedure that arrives at a solution. In the context of natural language, compositional reasoning is frequently studied through question answering tasks (e.g. Yang et al., 2018;Khot et al., 2020;Xie et al., 2020;Dalvi et al., 2021) or procedural knowledge prediction (e.g. Dalvi et al., 2018;Tandon et al., 2018;Dalvi et al., 2019). A contemporary challenge is that the number of valid compositional procedures is typically large compared to those that can be tractably annotated as gold, and as such automatically evaluating model performance becomes challenging (Jansen et al., 2021). In an embodied environment, an agent's actions have (generally) deterministic consequences for a given environment state, as actions are grounded in an underlying action language (e.g. McDermott et al., 1998) or linear logic (e.g. Martens, 2015. Embodied environments can offer a more formal semantics to study these reasoning tasks, where correctness of novel procedures could be evaluated directly.</p>
<p>Transfer Learning: Training a text-only agent for embodied tasks allows the agent to learn those tasks in a distilled form, at a high-level. This performance can then be transferred to more realistic 3D environments, where agents pretrained on text versions of the same environment learn to ground their high-level knowledge in low-level perceptual information, and complete tasks faster than when trained jointly (Shridhar et al., 2020b). This offers the possibility of creating simplified text worlds to pretrain agents for challenging 3D tasks that are currently out of reach of embodied agents.</p>
<p>Text World Simulators</p>
<p>Text World simulators render an agent's world view directly into textual descriptions of their environment, rather than into 2D or 3D graphical renderings. Similarly, actions the agent wishes to take are provided to the simulator as text (e.g. "read the letter" in Zork), requiring agent models to both parse input text from the environment, and generate output text to to interact with that environment.</p>
<p>In terms of simulators, the Z-machine (Infocom, 1989) is a low-level virtual machine originally designed by Infocom for creating portable interactive fiction novels (such as Zork). It was paired with a high-level LISP-like domain-specific language (ZIL) that included libraries for text parsing, and other tools for writing interactive fiction novels. The Z-machine standard was reverse-engineered by others (e.g. Nelson, 2014) in an effort to build their own high-level interactive fiction domain-specific languages, and has since become a standard compilation target due to the proliferation of existing tooling and legacy environments. 1 Inform7 (Nelson, 2006) is a popular high-level language designed for interactive fiction novels that allows environment rules to be directly specified in a simplified natural language, substantially lowering the barrier to entry for creating text worlds. The text generation engine allows substantial variation in the way the environments are described, from dry formulaic text to more natural, varied, conversational descriptions. Inform7 is compiled to Inform6, an earlier object-oriented scripting language with C-like syntax, which itself is compiled to Z-machine code.</p>
<p>Ceptre (Martens, 2015) is a linear-logic simulation engine developed with the goal of specifying more generic tooling for operational logics than Inform 7. TextWorld (Côté et al., 2018) adapt Ceptre's linear logic state transitions for environment descriptions, and add tooling for generative environments, visualization, and RL agent coupling, all of which is compiled into Inform7 source code. Parallel to this, the Jericho environment (Hausknecht et al., 2020) allows inferring relevant vocabulary and template-based object interactions for Z-machine-based interactive fiction games, easing action selection for agents. 1 A variety of text adventure tooling, including the Adventure Game Toolkit (AGT) and Text Adventure Development System (TADS), was developed starting in the late 1980s, but these simulators have generally not been adopted by the NLP </p>
<p>Text World Modeling Paradigms</p>
<p>Environment Modelling</p>
<p>Environments are typically modeled as an object tree that represents all the objects in an environment and their nested locations, as well as a set of action rules that implement changes to the objects in the environment based on an agent's actions.</p>
<p>Objects: Because of the body of existing interactive fiction environments for Z-machine environments, and nearly all popular tooling (In-form7, TextWorlds, etc.) ultimately compiling to Z-machine code, object models typically use the Zmachine model (Nelson, 2014). Z-machine objects have names (e.g. "mailbox"), descriptions (e.g. "a small wooden mailbox"), binary flags called attributes (e.g. "is_container_open"), and generic properties stored as key-value pairs. Objects are stored in the object tree, which represents the locations of all objects in the environment through parent-child relationships, as shown in Figure 1.</p>
<p>Action Rules: Action rules describe how objects change in response to a given world state, which is frequently a collection of preconditions followed by an action taken by an agent (e.g. "eat the apple"), but can also be due to environment states (e.g. a plant dying because it hasn't been watered for a time greater than some threshold). Ceptre (Martens, 2015) and to use mature action languages such as STRIPS (Fikes and Nilsson, 1971) or GDL (Genesereth et al., 2005;Thielscher, 2010Thielscher, , 2017 as the basis of a world model, though each of these languages have tradeoffs in features (such as object typing) and general expressivity (such as being primarily agent-action centered, rather than implementing environment-driven actions and processes) that make certain kinds of complex modeling more challenging. As a proof-of-concept, ALFWorld (Shridhar et al., 2020b) uses the Planning Domain Definition Language (PDDL, McDermott et al., 1998) to define the semantics for the variety of pick-and-place tasks in its text world rendering of the ALFRED benchmark.</p>
<p>Agent Modelling</p>
<p>While environments can be modelled as a collection of states and allowable state transitions (or rules), agents typically have incomplete or inaccurate information about the environment, and must make observations of the environment state through (potentially noisy or inadequate) sensors, and take actions based on those observations. Because of this, agents are typically modelled as partially-observable Markov decision processes (POMDP) (Kaelbling et al., 1998).</p>
<p>A Markov decision process (MDP) contains the state history (S), valid state transitions (T ), available actions (A), and (for agent modeling) the expected immediate reward for taking each action (R). POMDPs extend this to account for partial observability by supplying a finite list of observations the agent can make (Ω), and an observation function (O) that returns what the agent actually observes from an observation, given the current world state. For example, the observation function might return unknown if the agent tries to examine the contents of a locked container before unlocking it, because the contents cannot yet be observed. Similarly, when observing the temperature of a cup of tea, the observation function might return coarse measurements (e.g. hot, warm, cool) if the agent uses their hand for measurement, or fine-grained measurements (e.g. 70 • C) if the agent uses a thermometer. A final discount factor (γ) influences whether the agent prefers immediate rewards, or eventual (distant) rewards. The POMDP defined by defined by (S, T, A, R, Ω, O, γ) then serves as a model for a learning framework, typically reinforcement learning (RL), to learn a policy that enables the agent to maximize the reward.</p>
<p>Text World Environments</p>
<p>Environments are worlds implemented in simulators, that agents explore to perform tasks. Environments can be simple or complex, evaluate task-specific or domain-general competencies, be static or generative, and have small or large action spaces compared to higher-fidelity simulators (see the Appendix for a comparison of action space sizes across environments and simulators).</p>
<p>Single Environment Benchmarks</p>
<p>Single environment benchmarks typically consist of small environments designed to test specific agent competencies, or larger interactive fiction environments that test broad agent competencies to navigate a large world and interact with the environment toward achieving some distant goal. Toy environments frequently evaluate an agent's ability to perform compositional reasoning tasks of increasing lengths, such as in the Kitchen Cleanup and related benchmarks (Murugesan et al., 2020b). Other toy worlds explore searching environments to locate specific objects (Yuan et al., 2018), or combining source materials to form new materials (Jiang et al., 2020). While collections of interactive fiction environments are used as benchmarks (see Section 4.3), individual environments frequently form single benchmarks. Zork (Lebling et al., 1979) and its subquests are medium-difficulty environments frequently used in this capacity, while Anchorhead (Gentry, 1998) is a challenging environment where state-of-the-art performance remains below 1%.</p>
<p>Domain-specific Environments</p>
<p>Domain-specific environments allow agents to learn highly specific competencies relevant to a single domain, like science or medicine, while typically involving more modeling depth than toy environments. Tamari et al. (2021) create a TextWorld environment for wet lab chemistry protocols, that describe detailed step-by-step instructions for replicating chemistry experiments. These text-based simulations can then be represented as process execution graphs (PEG), which can then be run on real lab equipment. A similar environment exists for the materials science domain (Tamari et al., 2019).</p>
<p>Environment Collections as Benchmarks</p>
<p>To test the generality of agents, large collections of interactive fiction games (rather than single environments) are frequently used as benchmarks. While the Text-Based Adventure AI Shared Task initially evaluated on a single benchmark environment, later instances switched to evaluating on 20 varied environments to gauge generalization (Atkinson et al., 2019). Fulda et al. (2017a) created a list of 50 interactive fiction games to serve as a benchmark for agents to learn common-sense reasoning. Côté et al. (2018) further curate this list, replacing 20 games without scores to those more useful for RL agents. The Jericho benchmark (Hausknecht et al., 2020) includes 32 interactive fiction games that support Jericho's in-built methods for score and world-change detection, out of a total of 56 games known to support these features.</p>
<p>Generative Environments</p>
<p>A difficulty with statically-initialized environments is that because their structure is identical each time the simulation is run, rather than learning general skills, agents quickly overfit to a particular task and environment, and rarely generalize to unseen environments (Chaudhury et al., 2020). Procedurally generated environments help address this need by generating variations of environments centered around specific goal conditions. The TextWorld simulator (Côté et al., 2018) allows specifying high-level parameters such as the number of rooms, objects, and winning conditions, then uses a random walk to procedurally generate environment maps in the Inform7 language meeting those specifications, using either forward or backward chaining during generation to verify tasks can be successfully completed in the random environment. As an example, the First TextWorld Problems shared task 2 used TextWorld to generate 5k variations of a cooking environment, divided into train, development, and test sets. Similarly, Murugesan et al. (2020a) introduce TextWorld Com-monSense (TWC), a simple generative environment for household cleaning tasks, modelled as a pick-and-place task where agents must pick up common objects from the floor, and place them in their common household locations (such as placing shoes in a shoe cabinet). Other related environments include Coin Collector (Yuan et al., 2018), a generative environment for a navigation task, and Yin et al.'s (2019b) procedurally generated environment for cooking tasks. Adhikari et al. (2020) generate a large set of recipe-based cooking games, where an agent must precisely follow a cooking recipe that requires collecting tools (e.g. a knife) and ingredients (e.g. carrots), and processing those ingredients correctly (e.g. dice carrots, cook carrots) in the correct order. Jain et al. (2020) propose a similar synthetic benchmark for multi-step compositional reasoning called SaladWorld. In the context of question answering,  procedurally generate a simple environment that requires an agent to search and investigate attributes of objects, such as verifying their existence, locations, or specific attributes (like edibility). On the balance, while tooling exists to generate simple procedural environments, when compared to classic interactive fiction games (such as Zork), the current state-of-the-art allows for generating only relatively simple environments with comparatively simple tasks and near-term goals than human-authored interactive fiction games.</p>
<p>Text World Agents</p>
<p>Recently a large number of agents have been proposed for Text World environments. This section briefly surveys common modeling methods, paradigms, and trends, with the performance of recent agents on common interactive fiction games (as categorized by the Jericho benchmark, Hausknecht et al., 2020) shown in Table 2.</p>
<p>Reinforcement Learning: While some agents rely on learning frameworks heavily coupled with heuristics (e.g., Kostka et al., 2017, Golovin), owing to the sampling benefits afforded by operating in a virtual environment, the predominant modeling paradigm for most contemporary text world agents is reinforcement learning. Narasimhan et al. (2015) demonstrate that "Deep-Q Networks"  To support these modelling paradigms, Zelinka et al. (2019) introduce TextWorld KG, a dataset for learning the subtask of updating knowledge graphs based on text world descriptions in a cooking domain, and show their best ensemble model is able to achieve 70 F1 at this subtask. Similarly, Annamabrolu et al. (2021a) introduce JerichoWorld, a similar dataset for world modeling using knowledge graphs but on a broader set of interactive fiction games, and subsequently introduce World-Former (Ammanabrolu and Riedl, 2021b), a multitask transformer model that performs well at both knowledge-graph prediction and next-action prediction tasks. Question Answering: Agents can reframe Text World tasks as question answering tasks to gain relevant knowledge for action selection, with these agents providing current state-of-the-art performance across a variety of benchmarks. </p>
<p>Contemporary Focus Areas</p>
<p>World Generation: Generating detailed environments with complex tasks is labourious, while randomly generating environments currently provides limited task complexity and environment cohesiveness. World generation aims to support the generation of complex, coherent environments, either through better tooling for human authors (e.g. Temprado-Battad et al., 2019), or automated generation systems that may or may not have a humanin-the-loop. Fan et al. (2020) explore creating cohesive game worlds in the LIGHT environment using a variety of embedding models including Starspace (Wu et al., 2018a) and BERT (Devlin et al., 2019). Automatic evaluations show performance of between 36-47% in world building, defined as cohesively populating an environment with locations, objects, and characters. Similarly, hu-man evaluation shows that users prefer Starspacegenerated environments over those generated by a random baseline. In a more restricted domain, Ammanabrolu et al. (2019) show that two models, one Markov chain model, the other a generative language model (GPT-2), are capable of generating quests in a cooking environment, while there is a tradeoff between human ratings of quest creativity and coherence.</p>
<p>Ammanabrolu et al. (2020a) propose a largescale end-to-end solution to world generation that automatically constructs interactive fiction environments based on a story (such as Sherlock Holmes) provided as input. Their system first builds a knowledge graph of the story by framing KG construction as a question answering task, using their model (AskBERT) to populate this graph. The system then uses either a rule-based baseline or a generative model (GPT-2) to generate textual descriptions of the world from this knowledge graph. User studies show that humans generally prefer these neural-generated worlds to the rule-generated worlds (measured in terms of interest, coherence, and genre-resemblance), but that neural-generated performance still substantially lags behind that of human-generated worlds.</p>
<p>Hybrid 3D-Text Environments: Hybrid simulators that can simultaneously render worlds both graphically (2D or 3D) as well as textually offer a mechanism to quickly learn high-level tasks without having to first solve grounding or perceptual learning challenges. The ALFWorld simulator (Shridhar et al., 2020b) combines the ALFRED 3D home environment (Shridhar et al., 2020a) with a simultaneous TextWorld interface to that same environment, and introduce the BUTLER agent, which shows increased task generalization on the 3D environment when first trained on the text world. Prior to ALFWorld, Jansen (2020) showed that a language model (GPT-2) was able to successfully generate detailed step-by-step textual descriptions of ALFRED task trajectories for up to 58% of unseen cases using task descriptions alone, without visual input. Building on this, Micheli (2021) confirmed GPT-2 also performs well on the text world rendering of ALFWorld, and is able to successfully complete goals in 95% of unseen cases. Taken together, these results show the promise of quickly learning complex tasks at a high-level in a text-only environment, then transferring this performance to agents grounded in more complex environments.</p>
<p>Contemporary Limitations and Challenges</p>
<p>Environment complexity is limited, and it's currently difficult to author complex worlds. Two competing needs are currently at odds: the desire for complex environments to learn complex skills, and the desire for environment variation to encourage robustness in models. Current tooling emphasizes creating varied procedural environments, but those environments have limited complexity, and require agents to complete straightforward tasks. Economically creating complex, interactive environments that simulate a significant fraction of real world interactions is still well beyond current simulators or libraries -but required for higher-fidelity interactive worlds that have multiple meaningful paths toward achieving task goals. Generating these environments semi-automatically (e.g. Ammanabrolu et al., 2020a) may offer a partial solution. Independent of tooling, libraries and other middleware offer near-term solutions to more complex environment modeling, much in the same way 3D game engines are regularly coupled with physics engine middleware to dramatically reduce the time required to implement forces, collisions, lighting, and other physics-based modeling. Currently, few analogs exist for text worlds. The addition of a chemistry engine that knows ice warmed above the freezing point will change to liquid water, or a generator engine that knows the sun is a source of sunlight during sunny days, or an observation engine that knows tools (like microscopes or thermometers) can change the observation model of a POMDP -may offer tractability in the form of modularization. Efforts using large-scale crowdsourcing to construct knowledge bases of commonsense knowledge (e.g., ATOMIC, Sap et al., 2019) may be required to support these efforts.</p>
<p>Current planning languages offer a partial solution for environment modelling. While simulators partially implement facilities for world modeling, some (e.g. Côté et al., 2018;Shridhar et al., 2020b) suggest using mature planning languages like STRIPS (Fikes and Nilsson, 1971) or PDDL (McDermott et al., 1998) for more full-featured modeling. This would not be without significant development effort -existing implementations of planning languages typically assume full-world observability (in conflict with POMDP modelling), and primarily agent-directed state-space changes, making complex world modeling with partial ob-servability, and complex environment processes (such as plants that require water and light to survive, or a sun that rises and sets causing different items to be observable in day versus night) outside the space of being easily implemented with off-the-shelf solutions. In the near-term, it is likely that a domain-specific language specific to complex text world modeling would be required to address these needs while simultaneously reducing the time investment and barrier-to-entry for end users.</p>
<p>Analyses of environment complexity can inform agent design and evaluation. Text world articles frequently emphasize agent modeling contributions over environment, methodological, or analysis contributions -but these contributions are critical, especially in the early stages of this subfield. Agent performance in easy environments has increased incrementally, while medium-to-hard environments have seen comparatively modest improvements. Agent performance is typically reported as a distribution over a large number of environments, and the methodological groundwork required to understand when different models exceed others in time or performance over these environment distributions is critical to making forward progress. Transfer learning in the form of training on one set of environments and testing on others has become a standard feature of benchmarks (e.g. Hausknecht et al., 2020), but focused contributions that work to precisely characterize the limits of what can be learned from (for example) OmniQuest and transferred to Zork, and what capacities must be learned elsewhere, will help inform research programs in agent modeling and environment design.</p>
<p>Transfer learning between text world and 3D environments. Tasks learned at a high-level in text worlds help speed learning when those same models are transferred to more complex 3D environments (Shridhar et al., 2020b). This framing of transfer learning may resemble how humans can converse about plans for future actions in locations remote from those eventual actions (as when we apply knowledge learned in classrooms to the real world). As such, text-plus-3D environment rendering shows promise as a manner of controlling for different sources of complexity in multi-modal task learning (from high-level task-specific knowledge to low-level perceptual knowledge), and appears a promising research methodology for imparting complex task knowledge on agents that are able to navigate high-fidelity virtual environments. </p>
<p>A Extended List of Simulators</p>
<p>Simulators provide the infrastructure to implement the environments, objects, characters, and interactions of a virtual world, typically through a combination of a scripting engine to define the behavior of objects and agents, with a rendering engine that provides a view of the world for a given agent or user. Simulators for embodied agents exist on a fidelity spectrum, from photorealistic 3D environments to worlds described exclusively with language, where a trade-off typically exists between richer rendering and richer action spaces. This fidelity spectrum (paired with example simulators) is shown in Table 3, and described briefly below. Note that many of these higher-fidelity simulators are largely out-of-scope when discussing Text Worlds, except as a means of contrast to text-only worlds, and in the limited context that these simulators make use of text.</p>
<p>3D Environment Simulators: 3D simulators provide the user with complex 3D environments, including near-photorealistic environments such as AI2- Thor (Kolve et al., 2017), and include physics engines that model forces, liquids, illumination, containment, and other object interactions. Because of their rendering fidelity, they offer the possibility of inexpensively training robotic models in virtual environments that can then be transferred to the real world (e.g. RoboThor, Deitke et al., 2020). Adding objects to 3D worlds can be expensive, as this requires 3D modelling expertise that teams may not have. Similarly, adding agent actions or object-object interactions through a scripting language can be expensive if those actions are outside what is easily implemented in the simulator (like creating gasses, or using a pencil or saw to modify  Table 3: Example embodied simulation environments broken down by environment rendering fidelity. D specifies that environments supply natural language directives to the agent, I specifies that environments are interacted with (at least in part) using natural language input and/or output, and no rating represents environments that do not have a significant text component.</p>
<p>an object). Because of this, action spaces tend to be small, and limited to movement, and one (or a small number of) interaction commands. Some simulators and environments include text directives for an agent to perform, such as an agent being asked  to "slice an apple then cool it" in the ALFRED environment (Shridhar et al., 2020a). Other hybrid environments such as ALFWorld (Shridhar et al., 2020b) simultaneously render an environment both in 3D as well as in text, allowing agents to learn high-level task knowledge through text interactions, then ground these in environment-specific perceptual input though transfer learning.</p>
<p>Voxel-based Simulators: Voxel-based simulators create worlds from (typically) large 3D blocks, lowering rendering fidelity while greatly reducing the time and skill required to add new objects. Similarly, creating new agent-object or object-object interactions can be easier because they can generally be implemented in a coarser manner -though some kinds of basic spatial actions (like rotating an object in increments smaller than 90 degrees) are generally not easily implemented. Malmo (Johnson et al., 2016) and MineRL (Guss et al., 2019) offer wrappers and training data to build agents in the popular Minecraft environment. While the agent's action space is limited in Minecraft (see Table 4), the crafting nature of the game (that allows collecting, creating, destroying, or combining objects using one or more voxels) affords exploring a variety of compositional reasoning tasks with a low barrier to entry, while still using a 3D environment. Text directives, like those in CraftAssist (Gray et al., 2019), allow agents to learn to perform compositional crafting actions in this 3D environment from natural language dialog.</p>
<p>GridWorld Simulators: 2D gridworlds are comparatively easier to construct than 3D environments,</p>
<p>Figure 1 :
1An example partial object tree from the interactive fiction gameZork (Lebling et al., 1979).</p>
<p>TextWorld (Côté et al., 2018) use linear logic to represent possible valid state transitions. In linear logic, a set of preconditions in the state history of the world can be consumed by a rule to generate a set of postconditions, such as consuming a closed(C) precondition and posting a open(C) postcondition for a container-opening action for some container C. Côté et al. (2018) note the limitations in existing implementations of state transition systems for text worlds (such as single-step forward or backward chaining), and suggest future systems may wish community in favour of the more popular Inform series tools.</p>
<p>DQN) (Mnih et al., 2015) developed for Atari games can be augmented with LSTMs for representation learning in Text Worlds, which outperform simpler methods using n-gram bag-of-words representations. He et al. (2016a, DRRN) extend this to build the Deep Reinforcement Relevance Network (DRRN), an architecture that uses separate embeddings for the state space and actions, to improve both training time and performance. Madotto et al. (2020) show that the Go-Explore algorithm (Ecoffet et al., 2019), which periodically returns to promising but underexplored areas of a world, can achieve higher scores than the DRRN with fewer steps. Zahvey et al. (2018, AE-DQN) use an Action Elimination Network (AEN) to remove sub-</p>
<p>Guo et al. (2020b, MPRC-DQN) use multi-paragraph reading comprehension (MPRC) techniques to ask questions that populate action templates for agents, substantially reducing the number of training examples required for RL agents while achieving strong per-formance on the Jericho benchmark. Similarly, Ammanabrolu et al. (2020b, MC!Q*BERT) use contextually-relevant questions (such as "Where am I?", "Why am I here?") to populate their knowledge base to support task completion. Common-sense Reasoning: Agents arguably require a large background of common-sense or world knowledge to perform embodied reasoning in virtual environments. Fulda et al. (2017a) extract common-sense affordances from word vectors trained on Wikipedia using word2vec (Mikolov et al., 2013), and use this to increase performance on interactive fiction games, as well as (more generally) on robotic learning tasks (Fulda et al., 2017b). Murugesan et al. (2020b) combine the Concept-Net common-sense knowledge graph (Speer et al., 2017) with an RL agent that segments knowledge between general world knowledge, and specific beliefs about the current environment, demonstrating improved performance in a cooking environment. Similarly, Dambekodi et al. (2020) demonstrate that RL agents augmented with either COMET (Bosselut et al., 2019), a transformer trained on common-sense knowledge bases, or BERT (Devlin et al., 2019), which is hypothesized to contain common-sense knowledge, outperform agents without this knowledge on the interactive fiction game 9:05. In the context of social reasoning, Ammanabrolu et al. (2021) create a fantasy-themed knowledge graph, ATOMIC-LIGHT, and show that an RL agent using this knowledge base performs well at the LIGHT social reasoning tasks.</p>
<p>-
Rogue-in-a-box (Asperti et al., 2017) D BABYAI (Chevalier-Boisvert et al., 2018) I Nethack LE (Küttler et al., 2020) I VisualHints (Carta et al.(Côté et al., 2018) I LIGHT (Urbanek et al., 2019) I Jericho (Hausknecht et al., 2020)</p>
<p>Table 1 :
1An example Text World interactive fiction environment, Zork</p>
<p>Table 2 :
2Agent performance on benchmark interactive fiction environments. All performance values are normalized tomaximum achievable scores in a given environment. Due to the lack of standard reporting practice, performance reflects values 
reported for agents, but is unable to hold other elements (such as number of training epochs, number of testing epochs, reporting 
average vs maximum performance) constant. Parentheses denote environment difficulty (E:Easy, M:Medium, H:Hard) as 
determined by the Jericho benchmark (Hausknecht et al., 2020). </p>
<p>optimal actions, showing improved performance 
over a DQN on Zork. Yao et al (2020, CALM) 
use a GPT-2 language model trained on human 
gameplay to reduce the space of possible input 
command sequences, and produce a shortlist of can-
didate actions for an RL agent to select from. Yao 
et al. (2021, INV-DY) demonstrate that semantic 
modeling is important, showing that models that ei-
ther encode semantics through an inverse dynamic 
decoder, or discard semantics by replacing words 
with unique hashes, have different performance 
distributions in different environments. Taking a 
different approach, Tessler et al. (2019, IK-OMP) 
show that imitation learning combined with a com-
pressed sensing framework can solve Zork when 
restricted to a vocabulary of 112 words extracted 
from walk-through examples. 
Constructing Graphs: Augmenting reinforce-
ment learning models to produce knowledge graphs 
of their beliefs can reduce training time and im-
prove overall agent performance (Ammanabrolu 
and Riedl, 2019). Ammanabrolu et al. (2020, KG-
A2C) demonstrate a method for training an RL 
agent that uses a knowledge graph to model its 
state-space, and use a template-based action space 
to achieve strong performance across a variety of in-
teractive fiction benchmarks. Adhikari et al. (2020) 
demonstrate that a Graph Aided Transformer Agent 
(GATA) is able to learn implicit belief networks </p>
<p>about its environment, improving agent perfor-
mance in a cooking environment. Xu et al. (2020, 
SHA-KG) extend KG-A2C to use hierarchical RL 
to reason over subgraphs, showing substantially 
improved performance on a variety of benchmarks. </p>
<p>Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. 2019. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995. Angela Fan, Jack Urbanek, Pratik Ringshia, Emily Dinan, Emma Qian, Siddharth Karamcheti, Shrimai Prabhumoye, Douwe Kiela, Tim Rocktaschel, Arthur Szlam, et al. 2020. Generating interactive worlds with text. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1693-1700. Richard E Fikes and Nils J Nilsson. 1971. Strips: A new approach to the application of theorem proving to problem solving. Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017a. What can you do with a rock? affordance extraction via word embeddings. arXiv preprint arXiv:1703.03429. Nancy Fulda, Nathan Tibbetts, Zachary Brown, and David Wingate. 2017b. Harvesting common-sense navigational knowledge for robotics from uncurated text corpora. In Conference on Robot Learning, pages 525-534. PMLR. Michael Genesereth, Nathaniel Love, and Barney Pell. 2005. General game playing: Overview of the aaai competition. AI magazine, 26(2):62-62. Michael S. Gentry. 1998. Anchorhead. Jonathan Gray, Kavya Srinet, Yacine Jernite, Haonan Yu, Zhuoyuan Chen, Demi Guo, Siddharth Goyal, C Lawrence Zitnick, and Arthur Szlam. 2019. Craftassist: A framework for dialogue-enabled interactive agents. arXiv preprint arXiv:1907.08584. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. 2020a. Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7755-7765. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. 2020b. Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7755-7765, Online. Association for Computational Linguistics. William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. 2019. Minerl: A large-scale dataset of minecraft demonstrations. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. Stevan Harnad. 1990. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335-346. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. 2020. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903-7910. Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016a. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational LinguisticsPrithviraj Ammanabrolu, William Broniec, Alex 
Mueller, Jeremy Paul, and Mark Riedl. 2019. To-
ward automated quest generation in text-adventure 
games. In Proceedings of the 4th Workshop on Com-
putational Creativity in Language Generation, pages 
1-12. </p>
<p>Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, 
William Broniec, and Mark Riedl. 2020a. Bringing 
stories alive: Generating interactive fiction worlds. 
In Proceedings of the AAAI Conference on Artificial 
Intelligence and Interactive Digital Entertainment, 
volume 16, pages 3-9. </p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 
2020. Graph constrained reinforcement learning 
for natural language action spaces. arXiv preprint 
arXiv:2001.08837. </p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Play-
ing text-adventure games with graph-based deep re-
inforcement learning. In Proceedings of the 2019 
Conference of the North American Chapter of the 
Association for Computational Linguistics: Human 
Language Technologies, Volume 1 (Long and Short 
Papers), pages 3557-3565, Minneapolis, Minnesota. 
Association for Computational Linguistics. </p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2021a. Mod-
eling worlds in text. In Thirty-fifth Conference on 
Neural Information Processing Systems Datasets and 
Benchmarks Track (Round 1). </p>
<p>Prithviraj Ammanabrolu and Mark O Riedl. 2021b. 
Learning knowledge graph-based world mod-
els of textual environments. 
arXiv preprint 
arXiv:2106.09608. </p>
<p>Prithviraj Ammanabrolu, Ethan Tien, Matthew 
Hausknecht, and Mark O Riedl. 2020b. How to 
avoid being eaten by a grue: Structured explo-
ration strategies for textual worlds. arXiv preprint 
arXiv:2006.07409. </p>
<p>Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, 
Arthur Szlam, Tim Rocktäschel, and Jason Weston. 
2021. How to motivate your dragon: Teaching goal-
driven agents to speak and act in fantasy worlds. In 
Proceedings of the 2021 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 
pages 807-833, Online. Association for Computa-
tional Linguistics. </p>
<p>Andrea Asperti, Carlo De Pieri, and Gianmaria Pedrini. 
2017. Rogueinabox: an environment for roguelike 
learning. International Journal of Computers, 2. </p>
<p>Timothy Atkinson, Hendrik Baier, Tara Copplestone, 
Sam Devlin, and Jerry Swan. 2019. The text-based 
adventure ai competition. IEEE Transactions on 
Games, 11(3):260-266. </p>
<p>Chris Bamford. 2021. Griddly: A platform for ai re-
search in games. Software Impacts, 8:100066. </p>
<p>Lawrence W Barsalou et al. 1999. Perceptual symbol 
systems. Behavioral and brain sciences, 22(4):577-
660. </p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob 
Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap-
ata, Angeliki Lazaridou, Jonathan May, Aleksandr 
Nisnevich, et al. 2020. Experience grounds language. 
In Proceedings of the 2020 Conference on Empirical 
Methods in Natural Language Processing (EMNLP), 
pages 8718-8735. </p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-
tanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 
2019. Comet: Commonsense transformers for auto-
matic knowledge graph construction. In Proceedings 
of the 57th Annual Meeting of the Association for 
Computational Linguistics, pages 4762-4779. </p>
<p>Angelo Cangelosi, Giorgio Metta, Gerhard Sagerer, Ste-
fano Nolfi, Chrystopher Nehaniv, Kerstin Fischer, 
Jun Tani, Tony Belpaeme, Giulio Sandini, Francesco 
Nori, et al. 2010. Integration of action and language 
knowledge: A roadmap for developmental robotics. 
IEEE Transactions on Autonomous Mental Develop-
ment, 2(3):167-195. </p>
<p>Angelo Cangelosi and Matthew Schlesinger. 2015. De-
velopmental Robotics: From Babies to Robots. The 
MIT Press. </p>
<p>Thomas Carta, Subhajit Chaudhury, Kartik Tala-
madupula, and Michiaki Tatsubori. 2020. Vi-
sualhints: A visual-lingual environment for mul-
timodal reinforcement learning. arXiv preprint 
arXiv:2010.13839. </p>
<p>Subhajit Chaudhury, Daiki Kimura, Kartik Tala-
madupula, Michiaki Tatsubori, Asim Munawar, and 
Ryuki Tachibana. 2020. Bootstrapped Q-learning 
with context relevant observation pruning to gener-
alize in text-based games. In Proceedings of the 
2020 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 3002-3008, 
Online. Association for Computational Linguistics. </p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem 
Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu 
Nguyen, and Yoshua Bengio. 2018. Babyai: A plat-
form to study the sample efficiency of grounded lan-
guage learning. arXiv preprint arXiv:1810.08272. </p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben 
Kybartas, Tavian Barnes, Emery Fine, James Moore, 
Matthew Hausknecht, Layla El Asri, Mahmoud 
Adada, et al. 2018. Textworld: A learning environ-
ment for text-based games. In Workshop on Com-
puter Games, pages 41-75. Springer. 
Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau 
Yih, and Peter Clark. 2018. Tracking state changes in 
procedural text: a challenge dataset and models for 
process paragraph comprehension. In Proceedings 
of the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics: 
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 1595-1604, New Orleans, Louisiana. 
Association for Computational Linguistics. </p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan 
Xie, Hannah Smith, Leighanna Pipatanangkura, and 
Peter Clark. 2021. Explaining answers with entail-
ment trees. arXiv preprint arXiv:2104.08661. </p>
<p>Bhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-
tau Yih, and Peter Clark. 2019. Everything happens 
for a reason: Discovering the purpose of actions in 
procedural text. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference 
on Natural Language Processing (EMNLP-IJCNLP), 
pages 4496-4505, Hong Kong, China. Association 
for Computational Linguistics. </p>
<p>Sahith Dambekodi, Spencer Frazier, Prithviraj Am-
manabrolu, and Mark O Riedl. 2020. Playing text-
based games with common sense. arXiv preprint 
arXiv:2012.02757. </p>
<p>Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha 
Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Sal-
vador, Dustin Schwenk, Eli VanderBilt, Matthew 
Wallingford, Luca Weihs, Mark Yatskar, and Ali 
Farhadi. 2020. RoboTHOR: An Open Simulation-to-
Real Embodied AI Platform. In CVPR. </p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and 
Kristina Toutanova. 2019. BERT: Pre-training of 
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of 
the North American Chapter of the Association for 
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages 
4171-4186, Minneapolis, Minnesota. Association for 
Computational Linguistics. </p>
<p>Artificial intelligence, 2(3-4):189-
208. </p>
<p>Table 4 :
4Action space complexity for a selection of 3D, gridworld, and text-based environments.
https://competitions.codalab.org/ competitions/21557
and as such more options are available. GridWorlds share the commonality that they exist on a discretized 2D plane, typically containing a maximum of a few dozen cells on either dimension. Cells are discrete locations that (in the simplest case) contain up to a single agent or object, while more complex simulators allow cells to contain more than one object, including containers. Agents move on the plane through simplified spatial dynamics, at a minimum rotate left, rotate right, and move forward, allowing the entire world to be explored through a small action space.Where gridworlds tend to differ is in their rendering fidelity, and their non-movement action spaces. In terms of rendering, some (such as BABYAI, Chevalier-Boisvert et al., 2018) render a world graphically, using pixels, with simplified shapes for improving rendering throughput and reducing RL agent training time. Others such as NetHack(Küttler et al., 2020)are rendered purely as textual characters, owing to their original nature as early terminal-only games. Some simulators (e.g. Griddly, Bamford, 2021) support a range of rendering fidelities, from sprites (slowest) to shapes to text characters (fastest), depending on how critical rendering fidelity is for experimentation. As with 3D simulators, hybrid environments (like VisualHints, Carta et al., 2020) exist, where environments are simultaneously rendered as a Text World and accompanying GridWorld that provides an explicit spatial map.Action spaces vary considerably in GridWorld simulators (seeTable 4), owing to the different scripting environments that each affords. Some environments have a small set of hardcoded environment rules (e.g. BABYAI), while others (e.g. NetHack) offer nearly 100 agent actions, rich crafting, and complex agent-object interactions. Text can occur in the form of task directives (e.g. "put a ball next to the blue door" in BABYAI), partial natural language descriptions of changes in the environmental state (e.g. "You are being attacked by an orc" in NetHack), or as full Text World descriptions in hybrid environments.
Learning dynamic belief graphs to generalize on text-based games. Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, Will Hamilton, Advances in Neural Information Processing Systems. 33Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and Will Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. Advances in Neural Information Processing Systems, 33.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, 10.18653/v1/D15-1001Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsKarthik Narasimhan, Tejas Kulkarni, and Regina Barzi- lay. 2015. Language understanding for text-based games using deep reinforcement learning. In Pro- ceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, pages 1-11, Lisbon, Portugal. Association for Computational Lin- guistics.</p>
<p>Natural language, semantic analysis, and interactive fiction. Graham Nelson, IF Theory Reader. 141Graham Nelson. 2006. Natural language, semantic analysis, and interactive fiction. IF Theory Reader, 141:99-104.</p>
<p>The z-machine standards document version 1.1. Graham Nelson, Graham Nelson. 2014. The z-machine standards docu- ment version 1.1.</p>
<p>Neuroevolution strategies for word embedding adaptation in text adventure games. Spyridon Vivan Raaj Rajalingam, Samothrakis, 2019 IEEE Conference on Games (CoG). IEEEVivan Raaj Rajalingam and Spyridon Samothrakis. 2019. Neuroevolution strategies for word embedding adaptation in text adventure games. In 2019 IEEE Conference on Games (CoG), pages 1-8. IEEE.</p>
<p>Atomic: An atlas of machine commonsense for ifthen reasoning. Maarten Sap, Emily Ronan Le Bras, Chandra Allaway, Nicholas Bhagavatula, Hannah Lourie, Brendan Rashkin, Roof, A Noah, Yejin Smith, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Maarten Sap, Ronan Le Bras, Emily Allaway, Chan- dra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for if- then reasoning. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 33, pages 3027-3035.</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020a. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprintMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020b. Alfworld: Aligning text and em- bodied environments for interactive learning. arXiv preprint arXiv:2010.03768.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence31Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of gen- eral knowledge. In Proceedings of the AAAI Confer- ence on Artificial Intelligence, volume 31.</p>
<p>Inception-v4, inception-resnet and the impact of residual connections on learning. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alexander A Alemi, Thirty-first AAAI conference on artificial intelligence. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. 2017. Inception-v4, inception-resnet and the impact of residual connec- tions on learning. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Process-level representation of scientific protocols with interactive annotation. Ronen Tamari, Fan Bai, Alan Ritter, Gabriel Stanovsky, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational LinguisticsRonen Tamari, Fan Bai, Alan Ritter, and Gabriel Stanovsky. 2021. Process-level representation of scientific protocols with interactive annotation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin- guistics: Main Volume, pages 2190-2202, Online. Association for Computational Linguistics.</p>
<p>Language (re) modelling: Towards embodied language understanding. Ronen Tamari, Chen Shani, Tom Hope, R L Miriam, Omri Petruck, Dafna Abend, Shahaf, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsRonen Tamari, Chen Shani, Tom Hope, Miriam RL Petruck, Omri Abend, and Dafna Shahaf. 2020a. Lan- guage (re) modelling: Towards embodied language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pages 6268-6281.</p>
<p>Playing by the book: An interactive game approach for action graph extraction from text. Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, Yuji Matsumoto, Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications. the Workshop on Extracting Structured Knowledge from Scientific PublicationsRonen Tamari, Hiroyuki Shindo, Dafna Shahaf, and Yuji Matsumoto. 2019. Playing by the book: An interac- tive game approach for action graph extraction from text. In Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 62-71.</p>
<p>Ecological semantics: Programming environments for situated language understanding. Ronen Tamari, Gabriel Stanovsky, Dafna Shahaf, Reut Tsarfaty, arXiv:2003.04567arXiv preprintRonen Tamari, Gabriel Stanovsky, Dafna Shahaf, and Reut Tsarfaty. 2020b. Ecological semantics: Pro- gramming environments for situated language under- standing. arXiv preprint arXiv:2003.04567.</p>
<p>Reasoning about actions and state changes by injecting commonsense knowledge. Niket Tandon, Bhavana Dalvi, Joel Grus, Wen-Tau Yih, Antoine Bosselut, Peter Clark, 10.18653/v1/D18-1006Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsNiket Tandon, Bhavana Dalvi, Joel Grus, Wen-tau Yih, Antoine Bosselut, and Peter Clark. 2018. Reasoning about actions and state changes by injecting com- monsense knowledge. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 57-66, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>An online authoring tool for interactive fiction. Bryan Temprado-Battad, José-Luis Sierra, Antonio Sarasa-Cabezuelo, 2019 23rd International Conference Information Visualisation (IV). IEEEBryan Temprado-Battad, José-Luis Sierra, and Antonio Sarasa-Cabezuelo. 2019. An online authoring tool for interactive fiction. In 2019 23rd International Conference Information Visualisation (IV), pages 339-344. IEEE.</p>
<p>Action assembly: Sparse imitation learning for text based games with combinatorial action spaces. Chen Tessler, Tom Zahavy, Deborah Cohen, J Daniel, Shie Mankowitz, Mannor, arXiv:1905.09700arXiv preprintChen Tessler, Tom Zahavy, Deborah Cohen, Daniel J Mankowitz, and Shie Mannor. 2019. Action assem- bly: Sparse imitation learning for text based games with combinatorial action spaces. arXiv preprint arXiv:1905.09700.</p>
<p>A general game description language for incomplete information games. Michael Thielscher, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence24Michael Thielscher. 2010. A general game descrip- tion language for incomplete information games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 24.</p>
<p>Gdl-iii: a description language for epistemic general game playing. Michael Thielscher, Proceedings of the 26th International Joint Conference on Artificial Intelligence. the 26th International Joint Conference on Artificial IntelligenceMichael Thielscher. 2017. Gdl-iii: a description lan- guage for epistemic general game playing. In Pro- ceedings of the 26th International Joint Conference on Artificial Intelligence, pages 1276-1282.</p>
<p>Learning to speak and act in a fantasy text adventure game. Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, Jason Weston, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Ja- son Weston. 2019. Learning to speak and act in a fantasy text adventure game. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 673-683.</p>
<p>Starspace: Embed all the things!. Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, Jason Weston, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, and Jason Weston. 2018a. Starspace: Embed all the things! In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.</p>
<p>Building generalizable agents with a realistic and rich 3d environment. Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian, arXiv:1801.02209arXiv preprintYi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. 2018b. Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209.</p>
<p>WorldTree v2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference. Zhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, Peter Jansen, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources AssociationZhengnan Xie, Sebastian Thiem, Jaycie Martin, Eliz- abeth Wainwright, Steven Marmorstein, and Peter Jansen. 2020. WorldTree v2: A corpus of science- domain structured explanations and inference pat- terns supporting multi-hop inference. In Proceedings of the 12th Language Resources and Evaluation Con- ference, pages 5456-5473, Marseille, France. Euro- pean Language Resources Association.</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, Advances in Neural Information Processing Systems. 33Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. 2020. Deep reinforcement learning with stacked hierarchical at- tention for text-based games. Advances in Neural Information Processing Systems, 33.</p>
<p>Chalet: Cornell house agent learning environment. Claudia Yan, Dipendra Misra, Andrew Bennnett, Aaron Walsman, Yonatan Bisk, Yoav Artzi, arXiv:1801.07357arXiv preprintClaudia Yan, Dipendra Misra, Andrew Bennnett, Aaron Walsman, Yonatan Bisk, and Yoav Artzi. 2018. Chalet: Cornell house agent learning environment. arXiv preprint arXiv:1801.07357.</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo- pher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Com- putational Linguistics.</p>
<p>Reading and acting while blindfolded: The need for semantics in text game agents. Shunyu Yao, Karthik Narasimhan, Matthew Hausknecht, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesShunyu Yao, Karthik Narasimhan, and Matthew Hausknecht. 2021. Reading and acting while blind- folded: The need for semantics in text game agents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 3097-3102.</p>
<p>Keep CALM and explore: Language models for action generation in textbased games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020. Keep CALM and ex- plore: Language models for action generation in text- based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 8736-8754, Online. Association for Computational Linguistics.</p>
<p>Comprehensible context-driven text game playing. Xusen Yin, Jonathan , 2019 IEEE Conference on Games (CoG). IEEEXusen Yin and Jonathan May. 2019a. Comprehensible context-driven text game playing. In 2019 IEEE Conference on Games (CoG), pages 1-8. IEEE.</p>
<p>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games. Xusen Yin, Jonathan , arXiv:1908.04777arXiv preprintXusen Yin and Jonathan May. 2019b. Learn how to cook a new recipe in a new house: Using map fa- miliarization, curriculum learning, and bandit feed- back to learn families of text-based adventure games. arXiv preprint arXiv:1908.04777.</p>
<p>Interactive language learning by question answering. Xingdi Yuan, Marc-Alexandre Côté, Jie Fu, Zhouhan Lin, Chris Pal, Yoshua Bengio, Adam Trischler, 10.18653/v1/D19-1280Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsXingdi Yuan, Marc-Alexandre Côté, Jie Fu, Zhouhan Lin, Chris Pal, Yoshua Bengio, and Adam Trischler. 2019. Interactive language learning by question an- swering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 2796-2813, Hong Kong, China. Association for Com- putational Linguistics.</p>
<p>Counting to explore and generalize in text-based games. Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, arXiv:1806.11525Remi Tachet des Combes. arXiv preprintXingdi Yuan, Marc-Alexandre Côté, Alessandro Sor- doni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, and Adam Trischler. 2018. Counting to explore and generalize in text-based games. arXiv preprint arXiv:1806.11525.</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, J Daniel, Shie Mankowitz, Mannor, Advances in Neural Information Processing Systems. 31Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. Advances in Neural Information Processing Systems, 31:3562-3573.</p>
<p>Mikuláš Zelinka, Xingdi Yuan, Marc-Alexandre Côté, Romain Laroche, Adam Trischler, arXiv:1910.09532Building dynamic knowledge graphs from text-based games. arXiv preprintMikuláš Zelinka, Xingdi Yuan, Marc-Alexandre Côté, Romain Laroche, and Adam Trischler. 2019. Build- ing dynamic knowledge graphs from text-based games. arXiv preprint arXiv:1910.09532.</p>
<p>Scaling vision transformers. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer, arXiv:2106.04560arXiv preprintXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2021. Scaling vision transformers. arXiv preprint arXiv:2106.04560.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5952 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5952</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5952</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632" target="_blank">Large Language Models for Scientific Synthesis, Inference and Explanation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work presents a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms, and shows that the large language model can augment this knowledge by synthesizing from the scientific literature.</p>
                <p><strong>Paper Abstract:</strong> Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of"knowledge", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this"knowledge"by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5952.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5952.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4SD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses LLMs to (1) synthesize rules from the scientific literature, (2) infer empirical rules from labeled datasets, (3) convert rules into measurable features for interpretable models, and (4) generate human‑readable explanations for predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Galactica-6.7b (primary), Galactica-30b, Falcon-7b, Falcon-40b (ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Open-source transformer LLM backbones used as knowledge sources and rule generators; Galactica series pretrained heavily on scientific literature (6.7B and 30B parameter variants), Falcon series is a general-purpose family (7B and 40B parameter variants) with broader pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Distill domain-relevant, measurable rules and generalizable features from scientific literature and datasets to support molecular property prediction and produce interpretable scientific explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Chemistry, Biophysics, Physiology, Physical Chemistry, Quantum Mechanics (multidisciplinary molecular sciences)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompt-based roleplaying (instruct LLM to act as an experienced chemist), literature-derived rule synthesis from LLM pretraining knowledge, batch-wise data prompting for rule inference, LLM summarization/deduplication of rules, transcription of rules into numeric/categorical feature functions, training interpretable models (random forest / linear) and LLM-based textual explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Generalizable, measurable rules relating molecular structure/substructures and computed descriptors to properties (e.g., 'higher molecular weight and topological polar surface area reduce BBB permeability'), including both canonical heuristics and inferred substructure-based rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream predictive metrics (AUC-ROC for classification tasks, RMSE for some regression tasks, MAE for quantum tasks), statistical significance tests for rules (Mann-Whitney U for classification, linear regression t-test for regression with p < 0.05), literature prevalence assessment by domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM4SD produced rule-based features that, when used to train interpretable models, achieved state-of-the-art performance across 58 molecular tasks: physiology AUC-ROC improved from 74.43% to 76.60% (+2.8%), biophysics from 81.7% to 83.4% (+2.0%), quantum mechanics MAE improved (5.8233 vs 11.2450, ~48.2% better), and physical chemistry RMSE improved (1.28 vs 1.57, ~18.5% better). Combined synthesis+inference features outperformed either alone across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop validation and evaluation: roleplay prompts crafted by researchers, domain experts performed literature review and validation of rules, final model explanations intended for human inspection; pipeline requires human specification that rules be measurable.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Pretraining corpora implicit in LLM backbones (e.g., arXiv, scientific literature for Galactica; broader web for Falcon) plus MoleculeNet datasets used for data-driven inference: BBBP, ClinTox, Tox21, SIDER, BACE, HIV, ESOL, FreeSolv, Lipophilicity, QM9 (133,885 molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Synthesis often reflects extant literature (less novel); smaller general-purpose models (Falcon-7b) lacked capacity for some domains; reliance on LLM pretraining biases and coverage; need for rules to be expressible as numeric/categorical measures; some inferred rules are dataset-specific and may not generalize; ethical concerns in sensitive domains; human expert review required.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Literature-synthesized rules for BBBP: molecular weight, lipophilicity (logP), distribution coefficient, topological polar surface area, hydrogen bond donors/acceptors; Data-inferred example: Galactica-6.7b identified carbonyl functional groups and fragment rings as determinants for BBB permeability (hypothesized to relate to molecular cross-sectional area); inferred obscure substructures influencing Gibbs free energy (ΔG°) in QM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5952.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5952.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Synthesis (Literature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Synthesis from Scientific Literature using LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The component of LLM4SD where LLMs mine their pretrained scientific text knowledge to propose domain-specific, measurable rules (features) without analyzing task-specific data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Galactica-6.7b (preferred backbone for synthesis), Galactica-30b, Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Galactica models pretrained primarily on scientific literature, enabling retrieval-like synthesis of domain heuristics; Falcon models provide broader knowledge but require larger scale for comparable scientific performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Distill established principles and commonly cited heuristics from LLM pretraining corpora into measurable rules for predicting molecular properties.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Molecular sciences (physiology/ADME, biophysics, physical chemistry, quantum mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompt engineering with roleplay persona (experienced chemist), request for features that are numerically/categorically measurable, extraction and summarization of candidate rules directly from the LLM's pretraining-encoded knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Canonical domain heuristics (e.g., 'increased polar surface area reduces BBB permeability', 'hydrophobicity correlates with membrane permeability').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Statistical tests for rule significance (Mann-Whitney U for classification, t-test for regression); literature prevalence assessment by domain experts; measured downstream predictive performance when features are used in models.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Most synthesized rules were already present in literature and statistically meaningful: ~85% of synthesized rules were statistically significant and generally supported by existing literature; when used as features, they contributed to improved interpretable model performance and SOTA results when combined with inferred rules.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Moderate — human experts validated literature prevalence of synthesized rules; prompts and constraints (measurability) engineered by researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Implicit pretraining corpus of the LLM (Galactica's scientific corpora, arXiv, Wikipedia etc.), not a curated external literature retrieval stage in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tends to regurgitate known knowledge (low novelty); completeness depends on the LLM's pretraining coverage and recency; requires human validation for accuracy and relevance; rare exceptions (e.g., BACE and Tox21‑NR‑Ahr) where statistically significant rules were not found in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>BBBP literature-synthesized determinants: molecular weight, lipophilicity, distribution coefficient, topological polar surface area, hydrogen bond counts (consistent with refs 34–36).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5952.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5952.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Inference (Data)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Inference from Labeled Scientific Data using LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs analyze batches of labeled instances (SMILES + labels/values) to infer empirical, potentially novel measurable rules that discriminate classes or predict properties, then summarize and deduplicate these inferred rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Galactica-6.7b (primary), Falcon-40b (in ablation), Galactica-30b/Falcon-7b evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Transformer LLMs prompted with example instances and labels to induce feature-like rules; Galactica variants leverage scientific pretraining to improve pattern recognition in molecular data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Automatically infer empirical rules (measurable features) from labeled molecular datasets that capture regularities not necessarily prominent in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Same molecular domains as datasets: physiology, biophysics, physical chemistry, quantum mechanics</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Batch-wise prompting with labeled SMILES examples, instruct LLM to identify discriminative features per batch, LLM summarization across batches to produce final deduplicated rule set, transcription to functions that compute feature values.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Empirical structure–property rules and substructure-based heuristics (including obscure substructures and second-order features derived from combinations of structural motifs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Statistical significance tests (Mann-Whitney U, linear regression t-test p < 0.05), literature lookup for prevalence, downstream predictive performance of features in interpretable models.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High statistical validity: on average 91.3% of inferred rules were statistically significant; ~74% of inferred rules were documented in literature and ~17.3% were not previously found in literature (possible novel or dataset-specific rules); combined with literature-synthesized rules yielded best predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human experts performed literature cross-referencing and validation of inferred rules; prompts and batching were designed by researchers—thus semi-automated with human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>MoleculeNet datasets used in experiments (BBBP, ClinTox, Tox21, SIDER, BACE, HIV, ESOL, FreeSolv, Lipophilicity, QM9).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some inferred rules are dataset-specific and may not generalize; distinguishing genuinely novel scientific findings from dataset artifacts requires domain-expert follow-up; LLMs can infer second-order features that are hypotheses needing experimental validation; possibility of overfitting to sample batches or spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Inferred BBBP features included carbonyl functional groups and fragment rings hypothesized to affect molecular cross-sectional area and membrane partitioning; inferred obscure substructures affecting Gibbs free energy (ΔG°) in QM9 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5952.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5952.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-6.7b Rule Use</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of Galactica-6.7b for rule generation and validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Galactica-6.7b was the preferred LLM backbone for rule generation and validation due to favorable performance and reproducibility; its generated rules were statistically tested and cross-checked against literature by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Galactica-6.7b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>6.7 billion parameter Galactica model pretrained mainly on scientific literature, designed to encode and generate scientific knowledge and domain heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate literature-synthesized rules and infer data-driven rules for molecular property prediction and provide interpretable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Scientific literature and molecular datasets (multidisciplinary molecular science)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompting for literature synthesis (roleplay as chemist) and batch-wise prompting for data inference; summarization and deduplication of generated rules; statistical validation (Mann-Whitney U / t-tests) and expert literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Both canonical literature heuristics and inferred empirical substructure/descriptor rules (measurable numeric/categorical features).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Statistical significance (Mann-Whitney U for classification tasks, linear regression t-test for regression tasks; p<0.05), literature prevalence counts, downstream model performance metrics (AUC-ROC, RMSE, MAE).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Galactica-6.7b produced many rules that were both statistically significant and present in literature (~85% for synthesized rules); inferred rules were on average 91.3% statistically significant and ~74% documented in literature, with ~17.3% not previously identified by researchers—indicating both corroboration and identification of less-documented patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Domain experts reviewed and categorized rules (statistically significant & literature supported; significant & not found in literature; statistically insignificant); human oversight for rule transcription and model training.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Galactica pretraining corpus (scientific literature) for synthesis; MoleculeNet datasets for inference (see dataset_or_corpus above).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Galactica's synthesis sometimes reproduces known literature rather than novel insights; model scale vs domain coverage trade-offs (Galactica 6.7B sometimes matched 30B for several domains but lagged in quantum mechanics); need for statistical and expert validation to filter spurious rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Falcon-40B: an open large language model with state-of-the-art performance <em>(Rating: 1)</em></li>
                <li>Emergent abilities of large language models <em>(Rating: 1)</em></li>
                <li>Can ChatGPT be used to generate scientific hypotheses? <em>(Rating: 2)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5952",
    "paper_id": "paper-9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "LLM4SD",
            "name_full": "Large Language Models for Scientific Discovery",
            "brief_description": "A pipeline that uses LLMs to (1) synthesize rules from the scientific literature, (2) infer empirical rules from labeled datasets, (3) convert rules into measurable features for interpretable models, and (4) generate human‑readable explanations for predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Galactica-6.7b (primary), Galactica-30b, Falcon-7b, Falcon-40b (ablations)",
            "llm_model_description": "Open-source transformer LLM backbones used as knowledge sources and rule generators; Galactica series pretrained heavily on scientific literature (6.7B and 30B parameter variants), Falcon series is a general-purpose family (7B and 40B parameter variants) with broader pretraining.",
            "task_goal": "Distill domain-relevant, measurable rules and generalizable features from scientific literature and datasets to support molecular property prediction and produce interpretable scientific explanations.",
            "domain": "Chemistry, Biophysics, Physiology, Physical Chemistry, Quantum Mechanics (multidisciplinary molecular sciences)",
            "methodology": "Prompt-based roleplaying (instruct LLM to act as an experienced chemist), literature-derived rule synthesis from LLM pretraining knowledge, batch-wise data prompting for rule inference, LLM summarization/deduplication of rules, transcription of rules into numeric/categorical feature functions, training interpretable models (random forest / linear) and LLM-based textual explanation generation.",
            "type_of_qualitative_law": "Generalizable, measurable rules relating molecular structure/substructures and computed descriptors to properties (e.g., 'higher molecular weight and topological polar surface area reduce BBB permeability'), including both canonical heuristics and inferred substructure-based rules.",
            "evaluation_metrics": "Downstream predictive metrics (AUC-ROC for classification tasks, RMSE for some regression tasks, MAE for quantum tasks), statistical significance tests for rules (Mann-Whitney U for classification, linear regression t-test for regression with p &lt; 0.05), literature prevalence assessment by domain experts.",
            "results_summary": "LLM4SD produced rule-based features that, when used to train interpretable models, achieved state-of-the-art performance across 58 molecular tasks: physiology AUC-ROC improved from 74.43% to 76.60% (+2.8%), biophysics from 81.7% to 83.4% (+2.0%), quantum mechanics MAE improved (5.8233 vs 11.2450, ~48.2% better), and physical chemistry RMSE improved (1.28 vs 1.57, ~18.5% better). Combined synthesis+inference features outperformed either alone across domains.",
            "human_involvement": "Human-in-the-loop validation and evaluation: roleplay prompts crafted by researchers, domain experts performed literature review and validation of rules, final model explanations intended for human inspection; pipeline requires human specification that rules be measurable.",
            "dataset_or_corpus": "Pretraining corpora implicit in LLM backbones (e.g., arXiv, scientific literature for Galactica; broader web for Falcon) plus MoleculeNet datasets used for data-driven inference: BBBP, ClinTox, Tox21, SIDER, BACE, HIV, ESOL, FreeSolv, Lipophilicity, QM9 (133,885 molecules).",
            "limitations_or_challenges": "Synthesis often reflects extant literature (less novel); smaller general-purpose models (Falcon-7b) lacked capacity for some domains; reliance on LLM pretraining biases and coverage; need for rules to be expressible as numeric/categorical measures; some inferred rules are dataset-specific and may not generalize; ethical concerns in sensitive domains; human expert review required.",
            "notable_examples": "Literature-synthesized rules for BBBP: molecular weight, lipophilicity (logP), distribution coefficient, topological polar surface area, hydrogen bond donors/acceptors; Data-inferred example: Galactica-6.7b identified carbonyl functional groups and fragment rings as determinants for BBB permeability (hypothesized to relate to molecular cross-sectional area); inferred obscure substructures influencing Gibbs free energy (ΔG°) in QM tasks.",
            "uuid": "e5952.0",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Knowledge Synthesis (Literature)",
            "name_full": "Knowledge Synthesis from Scientific Literature using LLMs",
            "brief_description": "The component of LLM4SD where LLMs mine their pretrained scientific text knowledge to propose domain-specific, measurable rules (features) without analyzing task-specific data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Galactica-6.7b (preferred backbone for synthesis), Galactica-30b, Falcon-40b",
            "llm_model_description": "Galactica models pretrained primarily on scientific literature, enabling retrieval-like synthesis of domain heuristics; Falcon models provide broader knowledge but require larger scale for comparable scientific performance.",
            "task_goal": "Distill established principles and commonly cited heuristics from LLM pretraining corpora into measurable rules for predicting molecular properties.",
            "domain": "Molecular sciences (physiology/ADME, biophysics, physical chemistry, quantum mechanics)",
            "methodology": "Prompt engineering with roleplay persona (experienced chemist), request for features that are numerically/categorically measurable, extraction and summarization of candidate rules directly from the LLM's pretraining-encoded knowledge.",
            "type_of_qualitative_law": "Canonical domain heuristics (e.g., 'increased polar surface area reduces BBB permeability', 'hydrophobicity correlates with membrane permeability').",
            "evaluation_metrics": "Statistical tests for rule significance (Mann-Whitney U for classification, t-test for regression); literature prevalence assessment by domain experts; measured downstream predictive performance when features are used in models.",
            "results_summary": "Most synthesized rules were already present in literature and statistically meaningful: ~85% of synthesized rules were statistically significant and generally supported by existing literature; when used as features, they contributed to improved interpretable model performance and SOTA results when combined with inferred rules.",
            "human_involvement": "Moderate — human experts validated literature prevalence of synthesized rules; prompts and constraints (measurability) engineered by researchers.",
            "dataset_or_corpus": "Implicit pretraining corpus of the LLM (Galactica's scientific corpora, arXiv, Wikipedia etc.), not a curated external literature retrieval stage in this work.",
            "limitations_or_challenges": "Tends to regurgitate known knowledge (low novelty); completeness depends on the LLM's pretraining coverage and recency; requires human validation for accuracy and relevance; rare exceptions (e.g., BACE and Tox21‑NR‑Ahr) where statistically significant rules were not found in the literature.",
            "notable_examples": "BBBP literature-synthesized determinants: molecular weight, lipophilicity, distribution coefficient, topological polar surface area, hydrogen bond counts (consistent with refs 34–36).",
            "uuid": "e5952.1",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Knowledge Inference (Data)",
            "name_full": "Knowledge Inference from Labeled Scientific Data using LLMs",
            "brief_description": "LLMs analyze batches of labeled instances (SMILES + labels/values) to infer empirical, potentially novel measurable rules that discriminate classes or predict properties, then summarize and deduplicate these inferred rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Galactica-6.7b (primary), Falcon-40b (in ablation), Galactica-30b/Falcon-7b evaluated",
            "llm_model_description": "Transformer LLMs prompted with example instances and labels to induce feature-like rules; Galactica variants leverage scientific pretraining to improve pattern recognition in molecular data.",
            "task_goal": "Automatically infer empirical rules (measurable features) from labeled molecular datasets that capture regularities not necessarily prominent in the literature.",
            "domain": "Same molecular domains as datasets: physiology, biophysics, physical chemistry, quantum mechanics",
            "methodology": "Batch-wise prompting with labeled SMILES examples, instruct LLM to identify discriminative features per batch, LLM summarization across batches to produce final deduplicated rule set, transcription to functions that compute feature values.",
            "type_of_qualitative_law": "Empirical structure–property rules and substructure-based heuristics (including obscure substructures and second-order features derived from combinations of structural motifs).",
            "evaluation_metrics": "Statistical significance tests (Mann-Whitney U, linear regression t-test p &lt; 0.05), literature lookup for prevalence, downstream predictive performance of features in interpretable models.",
            "results_summary": "High statistical validity: on average 91.3% of inferred rules were statistically significant; ~74% of inferred rules were documented in literature and ~17.3% were not previously found in literature (possible novel or dataset-specific rules); combined with literature-synthesized rules yielded best predictive performance.",
            "human_involvement": "Human experts performed literature cross-referencing and validation of inferred rules; prompts and batching were designed by researchers—thus semi-automated with human oversight.",
            "dataset_or_corpus": "MoleculeNet datasets used in experiments (BBBP, ClinTox, Tox21, SIDER, BACE, HIV, ESOL, FreeSolv, Lipophilicity, QM9).",
            "limitations_or_challenges": "Some inferred rules are dataset-specific and may not generalize; distinguishing genuinely novel scientific findings from dataset artifacts requires domain-expert follow-up; LLMs can infer second-order features that are hypotheses needing experimental validation; possibility of overfitting to sample batches or spurious correlations.",
            "notable_examples": "Inferred BBBP features included carbonyl functional groups and fragment rings hypothesized to affect molecular cross-sectional area and membrane partitioning; inferred obscure substructures affecting Gibbs free energy (ΔG°) in QM9 tasks.",
            "uuid": "e5952.2",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Galactica-6.7b Rule Use",
            "name_full": "Use of Galactica-6.7b for rule generation and validation",
            "brief_description": "Galactica-6.7b was the preferred LLM backbone for rule generation and validation due to favorable performance and reproducibility; its generated rules were statistically tested and cross-checked against literature by experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Galactica-6.7b",
            "llm_model_description": "6.7 billion parameter Galactica model pretrained mainly on scientific literature, designed to encode and generate scientific knowledge and domain heuristics.",
            "task_goal": "Generate literature-synthesized rules and infer data-driven rules for molecular property prediction and provide interpretable explanations.",
            "domain": "Scientific literature and molecular datasets (multidisciplinary molecular science)",
            "methodology": "Prompting for literature synthesis (roleplay as chemist) and batch-wise prompting for data inference; summarization and deduplication of generated rules; statistical validation (Mann-Whitney U / t-tests) and expert literature review.",
            "type_of_qualitative_law": "Both canonical literature heuristics and inferred empirical substructure/descriptor rules (measurable numeric/categorical features).",
            "evaluation_metrics": "Statistical significance (Mann-Whitney U for classification tasks, linear regression t-test for regression tasks; p&lt;0.05), literature prevalence counts, downstream model performance metrics (AUC-ROC, RMSE, MAE).",
            "results_summary": "Galactica-6.7b produced many rules that were both statistically significant and present in literature (~85% for synthesized rules); inferred rules were on average 91.3% statistically significant and ~74% documented in literature, with ~17.3% not previously identified by researchers—indicating both corroboration and identification of less-documented patterns.",
            "human_involvement": "Domain experts reviewed and categorized rules (statistically significant & literature supported; significant & not found in literature; statistically insignificant); human oversight for rule transcription and model training.",
            "dataset_or_corpus": "Galactica pretraining corpus (scientific literature) for synthesis; MoleculeNet datasets for inference (see dataset_or_corpus above).",
            "limitations_or_challenges": "Galactica's synthesis sometimes reproduces known literature rather than novel insights; model scale vs domain coverage trade-offs (Galactica 6.7B sometimes matched 30B for several domains but lagged in quantum mechanics); need for statistical and expert validation to filter spurious rules.",
            "uuid": "e5952.3",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2
        },
        {
            "paper_title": "Falcon-40B: an open large language model with state-of-the-art performance",
            "rating": 1
        },
        {
            "paper_title": "Emergent abilities of large language models",
            "rating": 1
        },
        {
            "paper_title": "Can ChatGPT be used to generate scientific hypotheses?",
            "rating": 2
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 1
        }
    ],
    "cost": 0.012796249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models for Scientific Synthesis, Inference and Explanation</h1>
<p>Yizhen Zheng ${ }^{1 * * <em>}$, Huan Yee Koh ${ }^{1,3 </em>}$, Jiaxin Ju ${ }^{2 <em>}$, Anh T.N. Nguyen ${ }^{3}$, Lauren T. May ${ }^{3,4}$, Geoffrey I. Webb ${ }^{1 \infty}$, Shirui Pan ${ }^{2 \infty}$<br>${ }^{1}$ Department of Data Science and Artificial Intelligence, Monash University, Victoria, Australia<br>${ }^{2}$ School of Information and Communication Technology and Institute for Integrated and Intelligent Systems, Griffith University, Queensland, Australia<br>${ }^{3}$ Drug Discovery Biology, Monash Institute of Pharmaceutical Sciences, Monash University, Victoria, Australia<br>${ }^{4}$ Victorian Heart Institute, Monash University, Victoria, Australia<br></em>indicates equal contribution.<br>${ }^{\infty}$ indicates corresponding authors: Shirui Pan(s.pan@griffith.edu.au), Geoffrey I. Webb(Geoff.webb@monash.edu), Yizhen Zheng(yizhen.zheng1@monash.edu).</p>
<h4>Abstract</h4>
<p>Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language ${ }^{1}$. Despite their limited forms of 'knowledge,' these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation ${ }^{2,3}$. However, they have yet to demonstrate advanced applications in natural science ${ }^{4,5}$. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general purpose large language models to make inferences from scientific datasets of the form usually associated with special purpose machine learning algorithms. We show that the large language model can augment this 'knowledge' by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
<h2>1. Introduction</h2>
<p>Scientific productivity is in precipitous decline, with the rate of progress in many fields approximately halving every 13 years ${ }^{6}$. As scientific discovery becomes increasingly complex and challenging, traditional methodologies struggle to keep pace, necessitating innovative approaches. Meanwhile, Large Language Model (LLM) Artificial Intelligence systems have shown remarkable capabilities in a wide range of tasks. From creative writing to translating languages, from answering intricate queries ${ }^{7,8}$ to code generation ${ }^{9}$, their capabilities have been transformative in various domains ${ }^{1,2,3,10,11}$. In this work we show that these LLMs have similar transformational potential in the natural sciences. Particularly, we demonstrate that LLMs can</p>
<p>synthesize postulates from the scientific literature, make inferences from scientific data, and elucidate their conclusions with explanations.</p>
<p>LLMs are trained on large corpuses of text, including much of the scientific literature. Notable models like BioBert ${ }^{12}$, SciBERT ${ }^{13}$, Med-PALM ${ }^{11}$, and Galactica ${ }^{14}$ are specifically tailored to the scientific domain. Meanwhile, general-purpose LLMs like Falcon ${ }^{15}$ integrate extensive scientific literature in their pretraining, including sources such as arXiv and Wikipedia. We demonstrate that these systems have acquired deep abilities to interpret and manipulate the formal scientific language for describing molecules, SMILES strings, along with capability to apply information from the scientific literature in their interpretation. We present a scientific discovery pipeline LLM4SD (Large Language Models for Scientific Discovery) designed to tackle complex molecular property prediction tasks. LLM4SD operates by specifying rules for deriving features from SMILES strings that are relevant to predicting a target feature. Some of these rules are synthesized from the scientific literature that the LLMs encode. Others are inferred from training sets of SMILES strings each labelled with the relevant classes or property values. A standard machine learning model can then be learned from the training data using the rule-based features. Finally, our pipeline utilizes LLMs to produce interpretable outcomes, allowing human experts to ascertain the specific factors influencing the final predictions. We show that this pipeline achieves the current state of the art across 58 benchmark tasks spanning four domains Physiology, Biophysics, Physical Chemistry and Quantum Mechanics.</p>
<p>Despite these auspicious outcomes, we acknowledge the vastness and intricacy of the scientific discovery landscape; our endeavours have merely scratched the surface. Nonetheless, the strides made by LLM4SD pave the way for deeper exploration, heralding an era where AI-driven insights interweave with human ingenuity to redress the current decline in scientific productivity. Looking ahead, we are optimistic about AI's potential role as a linchpin in the future of scientific discovery, revolutionizing processes and expediting breakthroughs.</p>
<h1>2. Large Language Models for Scientific Discovery</h1>
<p>Our scientific discovery pipeline, LLM4SD, shown in Fig. 1 consists of 4 main components: Knowledge Synthesis from the Scientific Literature, Knowledge Inference from Data, Interpretable Model Training and Interpretable Explanation Generation. We demonstrate the application of our pipeline to 58 specialized property prediction tasks across four scientific domains: Physiology, Biophysics, Physical Chemistry, and Quantum Mechanics.</p>
<p>In the Knowledge Synthesis from Literature phase (Fig. 1a), LLMs use pre-trained knowledge from an extensive literature amassed from LLMs' pretraining ${ }^{14,15}$ to synthesize domain-specific molecular property prediction rules. Then, in the Knowledge Inference from Data phase (Fig. 1b), LLMs harness their inferential and analytical skills to infer molecular property prediction rules from the patterns in the datasets. These rules can generate features that effectively distinguish between different class instances or predict specific properties, such as a molecule's lipophilicity. This process mirrors how human scientists formulate hypotheses based on observation. In both the knowledge synthesis and inference stages, we require that the rules have either a numerical or</p>
<p>categorical measure associated with them. This ensures that the rules can be readily transformed into corresponding functions, which in turn can convert each data instance into a vector of values.</p>
<p>Rules, independently defined by LLMs, transform data instances into vectorized representations, i.e., features. These rule-based features facilitate the training of an interpretable model, e.g., random forest or linear classifier (Fig. 1c). Our preference for training these interpretable models stems from a desire to enhance transparency during predictions. Remarkably, we noted that when enhanced with LLM4SD, traditional interpretable models like random forests can surpass state-of-the-art baselines. These interpretable models, once trained, are adeptly employed for downstream application, encompassing both classification and regression scientific tasks. This entire workflow draws parallels with the methodical approach of human scientists-designing experiments to validate their proposed hypotheses.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig.1. LLM for Scientific Discovery Pipeline. (a) Knowledge synthesis from the scientific literature. In this phase, LLMs leverage their pre-trained understanding, amassed from pretraining on a massive body of literature, to synthesize domain-specific rules. (b) Knowledge inference from data. LLMs analyse the intrinsic patterns in the task-specific datasets to identify labelled patterns and infer empirical rules. (c) Interpretable Model Training. Rules formulated from LLMs are harnessed to convert data instances into features, enabling the development of models that are both effective and readily interpretable. (d) Interpretable Explanation Generation. Culminating the process, LLMs assimilate insights from the preceding steps to articulate comprehensive textual explanations, elucidating the rationales behind predictions. In the figure, the percentage value for each factor indicates its importance in concluding the prediction.</p>
<p>In the final stages (Fig. 1d), we tap into the LLMs' adeptness at information summarization. They are tasked to demystify the decision-making mechanism, illuminating how these interpretable models arrive at prediction outcomes based on instance representations, rules, and their respective significance. This clarity and transparency positions LLMs as intuitive partners, enabling scientists to seamlessly interface with and derive insights from the system's decisionmaking processes. To improve usability for researchers, we have created a web-based application that offers knowledge synthesis, inference, and prediction with explanation functions (see Supplementary Information 3).</p>
<p>By fostering this symbiotic relationship, we not only amplify the efficacy of scientific investigations but also elevate the confidence and trust in AI-assisted conclusions, driving forward the frontier of collaborative research.</p>
<h1>3. Experiment Results</h1>
<p>In this section, we offer a synopsis of LLM4SD's pivotal results spanning the 4 domains of physiology, biophysics, quantum mechanics and physical chemistry. Notably, all results of LLM4SD are obtained based on open-source LLM backbones to ensure reproducibility. Subsequently, we delved into an ablation study of LLM4SD, examining its performance across various LLM backbones ${ }^{14,15}$ of differing scales and pretraining datasets.</p>
<h3>3.1 Overall Performance on Four Domains</h3>
<p>To evaluate the versatility of LLM4SD's application, we conducted a comprehensive analysis of its performance across 58 molecular prediction tasks across the 4 domains_(Fig.2). Specifically, the physiology domain comprised (Blood-Brain Barrier Penetration) BBBP ${ }^{16}$, ClinTox ${ }^{17}$, Tox $21^{18}$ with 12 tasks, and SIDER ${ }^{19}$ with 27 tasks. Biophysics had two tasks, BACE ${ }^{20}$ and HIV ${ }^{18}$, while physical chemistry had three regression tasks: ESOL ${ }^{21}$, FreeSolv ${ }^{22}$ and Lipophilicity ${ }^{18}$. Quantum mechanics presented 12 regression tasks under QM9 ${ }^{23}$. The detailed description of these tasks is illustrated in the method section (see Methods, 'Datasets'). We compared LL4SD's performance with specialized, state-of-the-art supervised machine learning techniques. These are advanced Graph Neural Networks (GNNs), namely AttrMask ${ }^{24}$, GraphCL ${ }^{25}$, MolCLR ${ }^{26}$, 3DInfomax ${ }^{27}$, GraphMVP ${ }^{28}$, and MoleBERT ${ }^{29}$. Each model was pretrained on large datasets with diverse molecular knowledge and then fine-tuned for specific tasks (see Methods). As a standard baseline, we implemented Random Forest ${ }^{30}$ with ECFP4 ${ }^{31}$ as input set features.
Benchmarking LLM4SD against the baseline, LLM4SD demonstrated its superior efficacy and performance (Fig 2). This exemplary performance spanned 58 diverse tasks, from physiology (Extended Data Fig. 1-3) and biophysics (Extended Data Fig. 4) to physical chemistry (Extended Data Fig. 5) and quantum mechanics (Extended Fig. 6).</p>
<p>In both physiology and biophysics, our model outperformed all existing baselines (Fig.2a). Notably, we attained state-of-the-art (SOTA) results in Physiology, raising the AUC-ROC from a previous best of $74.43 \%$ to $76.60 \%$, a gain of $2.8 \%$. In Biophysics, our model further enhanced performance, advancing the AUC-ROC from $81.7 \%$ to $83.4 \%$, marking a $2.0 \%$ improvement. These advancements in physiology and biophysics emphasize the robustness and precision of LLM4SD in tasks that demand intricate biological understanding and modeling.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig.2. Comparison between LLM4SD and baselines across 4 domains. The red dotted line represents the average performance of all baselines. (a) Comparative analysis of model performance versus baselines in physiology and biophysics. (b) Comparative analysis of regression performance: LLM4SD vs. baselines in quantum mechanics and physical chemistry.
On tasks in quantum mechanics and physical chemistry, LLM4SD demonstrated substantial advancements (Fig.2b). In the domain of quantum mechanics, it showed a profound improvement of $48.2 \%$ over the best performed baseline, registering an average MAE of 5.8233 across 12 tasks as opposed to 11.2450 . Similarly, in physical chemistry, LLM4SD observed a noteworthy enhancement, with the model reaching a MAE of 1.28 marking an $18.5 \%$ advancement over the baseline MAE of 1.57. These significant improvements in regression tasks affirm the refined capability of our approach in continuous prediction.</p>
<p>Overall, LLM4SD's marked improvements not only affirm its supremacy over specialized, and often black-box, state-of-the-art models but also highlight its unparalleled ability to synthesize postulates, infer scientific data, and provide insightful explanations. This offers a fresh perspective in computational research and heralds a new direction in scientific endeavors.</p>
<h1>3.2 Ablation Study</h1>
<p>To delve deeper into the intricacies of the LLM4SD pipeline, we conducted an ablation study, focusing on discerning the influence of scale and pretraining datasets on the performance of Large Language Models (LLMs). In addition, we assessed the relative contributions of knowledge synthesis and inference. Our evaluation spanned across a spectrum of foundational LLM backbones, notably the Falcon $7 \mathrm{~b}^{15}$, Falcon $40 \mathrm{~b}^{15}$, Galactica-6.7b ${ }^{14}$, and Galactica-30b ${ }^{14}$. Here, we selected open-source LLM backbones to ensure the reproducibility of our work. It is worth noting the distinct differences between the Falcon and Galactica series of LLMs. In particular, the Falcon models are trained for a broad range of applications, imbibing a more general context during their pretraining phase, while the Galactica models are pretrained on mainly scientific literature, making them particularly suitable for science.</p>
<h1>3.2.1 Effect of Scale</h1>
<p>The ablation study of LLM4SD, which compared four open-source LLM backbones, revealed substantial differences among the different LLMs (Fig.3a, b). Particularly within the Falcon series, performance disparities were conspicuous. The Falcon 7b, a smaller model, fell short compared to the Falcon 40 b in its range of domain expertise. Notably, it failed to conduct tasks in two key areas: physiology and quantum mechanics, indicating a weaker understanding of scientific challenges and data interpretation.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig.3. Ablation study of LLM4SD. (a) Performance comparison of four open-source LLM backbones for physiology and biophysics. The vertical dashed line in the figure separates the two LLM series: Falcon on the left and Galactica series on the right. (b) Performance comparison of four open-source LLM backbones for physical chemistry and quantum mechanics. (c) Examining the influence of both synthesized and inferred knowledge on the average model performance across four domains. The triangle's color signifies the metric employed for domain-specific tasks. A (+) next to the metric name indicates higher values yield better results, while a (-) suggests the contrary.</p>
<p>Conversely, the Galactica series painted a more nuanced picture. Unlike with the Falcon series, a larger model did not necessarily translate to superior performance. In disciplines such as Physiology, Biophysics, and Physical Chemistry, Galactica 6.7 b rivaled the performance of Galactica-30b, despite the latter having more than 4 times the number of parameters. However,</p>
<p>in the domain of Quantum Mechanics, the larger Galactica 30b surged ahead, outperforming Galactica 6.7 b by a margin of $14 \%$. This variance could be attributed to the intricate and abstract nature of Quantum Mechanics, where the depth and breadth of knowledge encapsulated in the larger model might offer a discernible advantage.</p>
<h1>3.2.2 Effect of Pretraining Datasets of LLMs</h1>
<p>From these observations it becomes evident that an LLM steeped in scientific literature, even if smaller in scale, exhibits a commendable prowess in scientific tasks (Fig.3a, b). Conversely, the Falcon series, designed for general utility, necessitates a more substantial scale to effectively navigate scientific challenges. We postulate that this phenomenon is underpinned by the emergent capabilities ${ }^{32}$ inherent to large-scale LLMs. These capabilities empower the more expansive Falcon-40b to bridge the knowledge gap and adapt to scientific tasks. In a broader perspective, despite their relatively modest scale, the Galactica models consistently outperformed the Falcon series, underscoring the pivotal role of domain-specific pretraining.</p>
<h3>3.2.3 Contributions of knowledge synthesis and inference</h3>
<p>In our exploration of LLM4SD with respect to various knowledge sources, we discerned the performance variance arising from the use of rule-based features synthesized from literature, rule-based features inferred from data, and a combined approach. Overall values for these categories were obtained by averaging over results for all tasks in a domain (Fig. 3c).</p>
<p>In a comprehensive assessment of various scientific domains, the combination of synthesis and inference features consistently outperformed individual methods. Specifically, in the field of physiology, an AUC-ROC of 76.38 was achieved using both methods, compared to 72.15 with synthesis alone and 72.12 with inference. Similarly, in biophysics, combining both methods yielded an AUC-ROC of 80.95 , surpassing the scores of 75.62 and 77.23 obtained from synthesis and inference features, respectively. In physical chemistry, the combined approach resulted in an RMSE of 1.38 , which is notably better than the 1.72 from synthesis features and 1.92 from inference features. Lastly, in Quantum Mechanics, the use of both synthesis and inference features produced a MAE of 6.82 , improving upon the values of 9.81 and 7.18 recorded with synthesis and inference alone. Notably, comparing just synthesis with just inference, each outperformed the other in 2 out of the 4 domains.</p>
<p>These observations highlight the value of combining knowledge synthesis from scientific literature with inference from data. Literature imparts foundational theoretical insights, while empirical data identifies further regularities. The fusion of these knowledge facets equips the models with a comprehensive understanding, empowering them to excel across varied tasks and domains.</p>
<h2>4. Statistical Analysis and Literature Review: Validating Established Rules</h2>
<p>With LLM4SD outperforming specialized, state-of-the-art methods, we further validated the rules generated by Galactica-6.7b due to its superior performance and ease of reproducibility. The rules were validated in two ways: statistical tests to confirm the significance of these rules, and literature review to assess whether the rules are discussed in existing scientific literature.</p>
<p>For statistical tests of rules, we employed the Mann-Whitney $U$ test ${ }^{33}$ for classification tasks and the linear regression t-test for regression tasks. The Mann-Whitney $U$ test ${ }^{33}$ compared the distributions of chosen rule across the two classes of the target variable, thereby evaluating the statistical relevance of the rule's ability to split and distinguish classes. Conversely, the linear regression t-test treated the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0 , reflecting whether the rule contributes to regression prediction.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig.4. Literature Review and Statistical Analysis of LLM Rules. a-d. We conducted statistical analysis and a comprehensive literature review on rules generated by Galactica 6.7 b across all four scientific domains, with two tasks evaluated for each domain: (a) Physiology, (b) Biophysics, (c) Quantum Mechanics, and (d) Physical Chemistry. In the statistical analysis, the significance of a rule is determined based on the task type: for classification, the Mann-Whitney $U$ test ${ }^{33}$ compares the difference in distributions of chosen rule across the two classes of the target variable; and for regression, the linear regression t -test ${ }^{26}$ treats the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0 , reflecting whether the rule contributes to prediction. In both cases, we used a 0.05 p -value threshold to determine rule significance. In the literature review, we assessed the prevalence of a rule in existing literature. With statistical analysis and literature review, each rule is categorized into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant. Across all tasks, literaturesynthesized knowledge rules were generally both prevalent in existing literature and statistically significant. In contrast, empirically inferred data rules yielded mixed results, with some easily found in existing literature and others not identified by the researchers.</p>
<p>We further carried out a comprehensive review with in-domain experts to evaluate the prevalence of a rule in existing literature (see Supplementary Information 4: Literature Review(Example)). After cross-referencing the rules with scientific literature, we categorized each rule into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant (Fig. 4).</p>
<h1>4.1 Knowledge Synthesis from Scientific Literature</h1>
<p>We discovered that most of the synthesized rules we examined are readily available in existing scholarly works. Notably, an overwhelming majority ( $85 \%$ ) of these rules were statistically significant in indicating the target labels across all selected tasks, affirming our pipeline's ability to summarize rules from scientific literature that were the most important for different tasks and domains.</p>
<p>Importantly, except for $\mathrm{BACE}^{20}$ and Tox21-NR-Ahr ${ }^{18}$, we found no instances where statistically significant rules were absent from existing literature (Fig. 4). This aligns with the design of our pipeline: without analyzing the data, LLMs tend to aggregate and summarize existing knowledge. To illustrate, in the context of BBBP, the rules generated by our pipeline were consistent with well-established determinants such as molecular weight, lipophilicity, distribution coefficient, topological polar surface area, and hydrogen bonds ${ }^{34-36}$. These findings validate the robustness and reliability of our pipeline in leveraging LLMs to summarize existing scientific literature.</p>
<h3>4.2 Knowledge Inference from Data</h3>
<p>We found that an average of $91.3 \%$ of the inferred rules were statistically significant, higher than synthesized rules (Fig. 4). Of these, an average of $74 \%$ rules were already documented in existing scientific literature, while $17.3 \%$ were not identified by researchers. These latter rules were primarily associated with data patterns that are not widely discussed in the literature but can be inferred from our task dataset, such as obscure molecular substructures that influence target labels like the Gibbs free energy $\left(\Delta \mathrm{G}^{\circ}\right)$ of a molecule.</p>
<p>In contrast, to the knowledge synthesized from literature, we found that 6 out of 8 tasks have statistically significant rules that were absent from existing literature. This suggests that the rules produced by LLM4SD are not merely a result of the LLM's textual memorization during pretraining. Instead, the inferred rules reflect a genuine capability to derive meaningful rules from data based on the specific task.</p>
<p>Our case studies further substantiated the utility of these unidentified but significant rules. For instance, in BBBP where $38 \%$ of rules are significant but unidentified, Galactica 6.7B pinpointed the carbonyl functional group and fragment rings as key determinants of a molecule's BBBP. We hypothesize that these features are crucial for calculating a molecule's cross-sectional area, which in turn influences its orientation in lipid-water interfaces-factors vital for membrane partitioning and permeation ${ }^{37}$. Intriguingly, this suggests that our pipeline enables LLMs to infer what we term as second-order features. These are features that may not be immediately obvious or widely recognized but are consistent with established scientific principles in literature. In</p>
<p>doing so, LLMs not only corroborate existing knowledge but also apply existing knowledge in interpreting data, thereby enriching the current scientific discourse.</p>
<p>By leveraging LLMs, our pipeline not only validates well-established scientific principles but also uncovers less documented and even potentially novel rules. This facilitates a more effective and transparent interaction between scientists and the AI system, enhancing both the quality and trustworthiness of the research output. Moreover, the statistically robust but underrepresented rules we identified could serve as promising avenues for future scientific exploration, thereby advancing the frontier of collaborative, AI-assisted research.</p>
<h1>5. Discussion</h1>
<p>In our exploration, we unveil the capabilities of LLM4SD through our specially designed pipeline, enabling LLMs to excel in scientific synthesis, inference, and explanation. Through seamless integration with our proposed architecture, LLMs exhibit state-of-the-art (SOTA) performance across a vast expanse of four domains. The inherent versatility of LLM4SD stands as a testament to its potential, making it poised for broader applications across varied domains, thus magnifying its relevance in the current scientific landscape.</p>
<p>Scientific discovery, vast in scope, is constantly evolving with our expanding understanding of the universe. Our study, ambitious in its intent, captures 58 tasks across four distinct domains, providing a glimpse into the immense reservoir of scientific knowledge. While this study serves as a pioneering beacon, demonstrating LLMs' transformative capabilities, it also signals the beginning of a broader exploration. We envision further expansion, integrating more diverse tasks and domains, pushing LLMs to their full potential and reshaping the boundaries of scientific inquiry.</p>
<p>Harnessing the immense power of AI-driven models for scientific discovery brings along its ethical challenges. The vast capabilities of such models, while revolutionizing our understanding, also raise concerns of potential misuse, especially in sensitive domains like biophysics and quantum mechanics. The reliance on machine synthesis and interpretation might overshadow the indispensable human element of scrutiny and ethics in research. As we plunge deeper into the AI era, it's crucial to tread with caution, balancing advancements with rigorous oversight and an unwavering commitment to ethical rigor.</p>
<p>As we gaze towards the horizon, the potential trajectory for LLM4SD is compelling. We anticipate a future where the nexus between LLMs and advanced scientific toolkits deepens. As computational capabilities grow and scientific knowledge expands, our pipeline stands poised for evolutionary enhancements. Our steadfast goal is to harmoniously fuse artificial intelligence with myriad scientific arenas, unlocking novel insights and pioneering avenues previously unimagined.</p>
<h1>References</h1>
<ol>
<li>Frank MC. Baby steps in evaluating the capacities of large language models. Nat Rev Psychol. (2023).</li>
<li>Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, Agarwal S. Language models are few-shot learners. Adv Neural Inf Process Syst. (2020); 33:1877-901.</li>
<li>OpenAI. GPT-4 Technical Report. preprint. (2023);</li>
<li>Birhane A, Kasirzadeh A, Leslie D et al. Science in the age of large language models. Nat Rev Phys. (2023); 5:277-280.</li>
<li>Gilbert S, Harvey H, Melvin T et al. Large language model AI chatbots require approval as medical devices. Nat Med. (2023).</li>
<li>Bloom, Nicholas, Charles I. Jones, John Van Reenen, and Michael Webb. "Are ideas getting harder to find?." American Economic Review 110, no. 4 (2020): 1104-1144.</li>
<li>Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. "Chain-of-thought prompting elicits reasoning in large language models." Advances in Neural Information Processing Systems 35 (2022): 24824-24837.</li>
<li>Zhou, Denny, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans et al. "Least-to-most prompting enables complex reasoning in large language models." ICLR (2023).</li>
<li>Rozière, Baptiste, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ell en Tan, Yossi Adi et al. "Code Llama: Open Foundation Models for Code." arXiv preprint arXiv:2308.12950 (2023).</li>
<li>Jiang, Lavender Yao, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton et al. "Health system-scale language models are all-purpose prediction engines." Nature (2023): 1-6.</li>
<li>Singhal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales et al. "Large language models encode clinical knowledge." Nature (2023).</li>
<li>Lee, Jinhyuk, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. "BioBERT: a pre-trained biomedical language representation model for biomedical text mining." Bioinformatics 36, no. 4 (2020): 1234-1240.</li>
<li>Beltagy, Iz, Kyle Lo, and Arman Cohan. "SciBERT: A pretrained language model for scientific text." ACL (2019).</li>
<li>Taylor, Ross, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. "Galactica: A large language model for science." Preprint (2022).</li>
<li>Almazrouei, Ebtesam, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet et al. "Falcon-40B: an open large language model with state-of-the-art performance." Technical report, Technology Innovation Institute; (2023).</li>
<li>Martins, Ines Filipa, Ana L. Teixeira, Luis Pinheiro, and Andre O. Falcao. "A Bayesian approach to in silico blood-brain barrier penetration modeling." Journal of Chemical Information and Modeling 52, no. 6 (2012): 16861697.</li>
<li>Gayvert, Kaitlyn M., Neel S. Madhukar, and Olivier Elemento. "A data-driven approach to predicting successes and failures of clinical trials." Cell Chemical Biology 23, no. 10 (2016): 1294-1301.</li>
<li>Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., ... \&amp; Pande, V. (2018). MoleculeNet: a benchmark for molecular machine learning. Chemical Science, 9(2), 513-530.</li>
<li>Kuhn, Michael, Ivica Letunic, Lars Juhl Jensen, and Peer Bork. "The SIDER database of drugs and side effects." Nucleic Acids Research 44, no. D1 (2016): D1075-D1079.</li>
<li>
<p>Subramanian, Govindan, Bharath Ramsundar, Vijay Pande, and Rajiah Aldrin Denny. "Computational modeling of $\beta$-secretase 1 (BACE-1) inhibitors using ligand based approaches." Journal of Chemical Information and Modeling 56, no. 10 (2016): 1936-1949. 25</p>
</li>
<li>
<p>Delaney, John S. "ESOL: estimating aqueous solubility directly from molecular structure." Journal of Chemical Information and Computer Sciences 44, no. 3 (2004): 1000-1005.</p>
</li>
<li>Mobley, David L., and J. Peter Guthrie. "FreeSolv: a database of experimental and calculated hydration free energies, with input files." Journal of Computer-Aided Molecular Design 28 (2014): 711-720.</li>
<li>Ramakrishnan, Raghunathan, Pavlo O. Dral, Matthias Rupp, and O. Anatole Von Lilienfeld. "Quantum chemistry structures and properties of 134 kilo molecules." Scientific Data 1, no. 1 (2014): 1-7.</li>
<li>Hu, Weihua, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. "Strategies for Pre-training Graph Neural Networks." In International Conference on Learning Representations. 2019 .</li>
<li>You, Yuning, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. "Graph contrastive learning with augmentations." Advances in Neural Information Processing Systems 33 (2020): 5812-5823.</li>
<li>Wang, Yuyang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. "Molecular contrastive learning of representations via graph neural networks." Nature Machine Intelligence 4, no. 3 (2022): 279-287.</li>
<li>Stärk, Hannes, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, and Pietro Liò. "3d infomax improves gnns for molecular property prediction." In International Conference on Machine Learning, pp. 20479-20502. PMLR, 2022.</li>
<li>Liu, Shengchao, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. "Pre-training Molecular Graph Representation with 3D Geometry." In International Conference on Learning Representations. 2021 .</li>
<li>Xia, Jun, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. "Mole-bert: Rethinking pre-training graph neural networks for molecules." In The Eleventh International Conference on Learning Representations. 2022.</li>
<li>Breiman, Leo. "Random forests." Machine Learning 45 (2001): 5-32.</li>
<li>Rogers, David, and Mathew Hahn. "Extended-connectivity fingerprints." Journal of Chemical Information and Modeling 50, no. 5 (2010): 742-754.</li>
<li>Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama et al. "Emergent abilities of large language models." TMLR (2022).</li>
<li>McKnight, Patrick E., and Julius Najab. "Mann-Whitney U Test." The Corsini Encyclopedia of Psychology (2010): 1-1.</li>
<li>Wager, T. T. et al. Defining desirable central nervous system drug space through the alignment of molecular properties, in vitro adme, and safety attributes. ACS Chemical Neuroscience 1, 420-434 (2010)</li>
<li>Wager, T. T., Hou, X., Verhoest, P. R. \&amp; Villalobos, A. Moving beyond rules: the development of a central nervous system multiparameter optimization (cns mpo) approach to enable alignment of druglike properties. $A C S$ Chemical Neuroscience 1, 435-449 (2010).</li>
<li>Geldenhuys, W. J., Mohammad, A. S., Adkins, C. E. \&amp; Lockman, P. R. Molecular determinants of blood-brain barrier permeation. Therapetuic. Delivery 6, 961-971 (2015).</li>
<li>Gerebtzoff, G. \&amp; Seelig, A. In silico prediction of blood- brain barrier permeation using the calculated molecular cross-sectional area as main parameter. J. Chemical Information Modeling 46, 2638-2650 (2006). 14/14.</li>
</ol>
<h1>Methods</h1>
<h2>Datasets:</h2>
<p>We conducted a thorough evaluation of LLM4SD, covering 58 subtasks across four unique domains for a robust assessment. The physiology domain included 41 tasks like BBBP, ClinTox, and the 12-task Tox21, ranging from NR-AR to SR-p53, along with the 27-task SIDER suite covering various medical conditions. Biophysics offered 2 classification tasks: BACE and HIV. In physical chemistry, we addressed three regression tasks: ESOL, FreeSolv, and Lipophilicity, while the Quantum mechanics domain presented 12 regression tasks within the QM9 dataset, exploring properties from mu to G, providing a comprehensive insight into LLM4SD's capabilities.</p>
<h2>Physiology.</h2>
<p>BBBP: The BBBP dataset contains 2,039 instances, each representing unique compounds labeled based on their permeability properties. Predicting which molecules can cross this barrier is paramount for drug development, especially for neurological conditions.
ClinTox: The ClinTox dataset, with 1,478 instances, provides comprehensive information on the toxicological properties of various compounds.
Tox21: With 7,831 instances, the Tox21 dataset is a collaborative effort to identify environmental toxicants. Its 12 classification tasks focus on specific biological targets or pathways. The Nuclear Receptor (NR) tasks, namely NR-AhR, NR-AR, NR-AR-LBD, NRAromatase, NR-ER, NR-ER-LBD, and NR-PPAR-gamma, examine interactions with intracellular proteins influencing gene expression and potential toxic effects. The Stress Response (SR) tasks, including SR-ARE, SR-ATAD5, SR-HSE, SR-MMP, and SR-p53, explore the impact of chemicals on stress-related cellular pathways.
SIDER: The SIDER dataset, with 1,427 instances, offers detailed data on medication side effects. Each task in this dataset relates to a specific adverse drug reaction, aiding researchers in understanding and predicting potential drug side effects. All 27 classification tasks are: 1) Hepatobiliary disorders 2) Metabolism and nutrition disorders 3) Product issues 4) Eye disorders 5) Investigations 6) Musculoskeletal and connective tissue disorders 7) Gastrointestinal disorders 8) Social circumstances 9) Immune system disorders 10) Reproductive system and breast disorders 11) Neoplasms benign, malignant and unspecified (incl cysts and polyps). 12) General disorders and administration site conditions 13) Endocrine disorders 14) Surgical and medical procedures 15) Vascular disorders 16) Blood and lymphatic system disorders 17) Skin and subcutaneous tissue disorders 18) Congenital, familial and genetic disorders 19) Infections and infestations 20) Respiratory, thoracic and mediastinal disorders 21) Psychiatric disorders 22) Renal and urinary disorders 23) Pregnancy, puerperium and perinatal conditions 24) Ear and labyrinth disorders 25) Cardiac disorders 26) Nervous system disorders 27) Injury, poisoning and procedural complications.</p>
<h2>Biophysics.</h2>
<p>HIV: With a collection of 17,930 instances, the HIV dataset offers a comprehensive repository of molecules, represented in the SMILES format. This dataset is instrumental in the classification of compounds based on their potential inhibitory effects against HIV.</p>
<p>BACE: The BACE dataset, comprising 11,908 instances, is a curated collection of molecules, each represented in the SMILES format. This dataset is tailored for classification tasks, aiming to discern molecules that can inhibit the BACE-1 enzyme. By analyzing the molecules within this dataset, researchers can glean insights into the structural features that confer inhibitory properties against BACE-1.</p>
<h1>Physical Chemistry.</h1>
<p>ESOL: The ESOL dataset, comprising 1,128 instances, is a curated collection that delves into the solubility of molecules in water. By analyzing the ESOL dataset, researchers can gain a deeper understanding of the molecular features that dictate solubility, thereby aiding in the design of compounds with optimal solubility profiles. Each entry in this dataset is represented using the SMILES notation, a universal language for describing the structure of chemical species.
FreeSolv: With 642 instances, the FreeSolv dataset provides comprehensive data on the hydration free energy of molecules. This dataset is pivotal for researchers aiming to predict how molecules interact with water, which has implications for drug solubility and stability. Each molecule in the FreeSolv dataset is also represented using the SMILES notation.
Lipophilicity: Lipophilicity is a fundamental property that influences the absorption, distribution, metabolism, and excretion of drugs. The Lipophilicity dataset with 4200 compounds offers a rich resource for understanding this property. Analyzing this dataset allows researchers to discern the molecular attributes that contribute to a compound's lipophilicity, guiding the synthesis of molecules with desired pharmacokinetic properties. Like the other datasets in this domain, each entry is denoted using the SMILES notation.</p>
<h2>Quantum Mechanics.</h2>
<p>QM9: The Quantum Mechanics domain, central to understanding the fundamental properties of matter, is exemplified in our evaluation through the QM9 dataset. Comprising 133,885 instances, the QM9 dataset provides a comprehensive exploration of molecules' quantum mechanical attributes, essential for diverse applications from material science to pharmaceuticals. It includes 12 tasks: (Dipole Moment), (Polarizability), (Squared Radius), ZPVE (Zero-Point Vibrational Energy), (Heat Capacity at Constant Volume), (Energy Gap), (Highest Occupied Molecular Orbital Energy), (Lowest Unoccupied Molecular Orbital Energy), (Internal Energy at 0 Kelvin), U (Internal Energy at Standard State), H (Enthalpy), G (Gibbs Free Energy).</p>
<h2>Baselines:</h2>
<p>We rigorously assessed our pipeline in comparison to specialized, state-of-the-art supervised machine learning methods. For conventional approaches, we employed Random Forest ${ }^{30}$, using ECFP4 ${ }^{31}$ as the input feature set. We also considered state-of-the-art Graph Neural Networks (GNNs), including Attribute Masking (AttrMask) ${ }^{24}$, GraphCL ${ }^{25}$, MolCLR ${ }^{26}$, 3DInfomax ${ }^{27}$, GraphMVP ${ }^{28}$, and MoleBERT ${ }^{29}$. Each of these models was initialized with pre-trained weights and subsequently fine-tuned for specific tasks.</p>
<p>In summary, AttrMask pre-training involves teaching the GNN to predict randomly masked atom and bond attributes within molecular graphs. GraphCL and MolCLR use graph augmentations for a contrastive learning objective, aimed at maximizing the similarity between augmentations originating from the same molecule while minimizing similarity between augmentations from different molecules. GraphMVP and 3DInfomax leverage existing 3D molecular datasets to pretrain models capable of deducing 3D molecular geometry from 2D graphs, by optimizing mutual information between 3D summary vectors and GNN graph representations. Finally, MoleBERT, the recent state-of-the-art method, employs a VQ-VAE-based tokenizer to diversify atom vocabulary, thereby balancing dominant and rare atoms. It uses Masked Atoms Modeling (MAM) and Triplet Masked Contrastive Learning (TMCL) for node and graph-level pre-training, respectively.</p>
<h1>LLM for Scientific Discovery Pipeline</h1>
<p>In this section, we detail the proposed pipeline and the techniques used to align them with the requirements of molecular property prediction tasks. Instead of merely prompting LLMs to generate scientific hypotheses ${ }^{38}$ or training them for direct predictions ${ }^{39}$, LLM4SD emulates how human experts conduct scientific research. This includes synthesizing knowledge from literature, inferring hypotheses from datasets, validating findings through experiments, and elucidating the rationale behind predictions.</p>
<h2>Knowledge Synthesis from the Scientific Literature</h2>
<p>LLMs are usually pretrained on large corpora of text data that include books, articles, websites and other written content. This extensive pretraining helps LLMs to learn the structure of the language, recognize patterns, understand context and acquire a wide-ranging knowledge of facts and concepts. Thus, the goal of the knowledge synthesis process is to extract relevant features from the vast pool of the knowledge that a LLM possesses from the pretraining stage.
To achieve this, we first instruct the LLM to adopt the persona of an experienced chemist, and then engage it to identify pertinent features based on its existing knowledge. This form of roleplaying prompt facilitates the knowledge mining process to mimic how human experts solve real-world challenges. For example, when a chemist needs to predict the bioactivity or BBBP of a molecule, they often apply feature-related rules such as number of hydrogen bond donors/acceptors, molecular weight, and $\log \mathrm{P}$. We require that the features identified by LLMs can be measured with a numerical or categorical measure to enable their transcription into corresponding functions.</p>
<h2>Knowledge Inference from Data</h2>
<p>The objective of knowledge inference form data is to harness the powerful reasoning skills of LLMs to identify relevant features by analyzing the given data. Given their impressive ability to solve mathematical problems and identify patterns, we conjecture that LLMs have the capacity to discern common patterns within groups of molecules based on its scientific understanding. To validate this hypothesis, we provide LLMs with an instruction and several batches of sampled instances with their corresponding class labels or instance property values. In the instruction, the LLM is tasked with analyzing patterns from provided data to identify features that effectively</p>
<p>discriminate between two classes of instances or predict their property values. As a result, LLMs will come up with rules distilled from the analysis for each batch. Since the generated rules in different batches may contain duplicates, we ultimately employ the LLMs' summarization capability to condense the rules and eliminate duplicates, resulting in the final list of features.</p>
<h1>Interpretable Model Training</h1>
<p>In this stage, all the features identified in the first two stages are transcribed into corresponding functions. All these functions take a scientific instance as input, e.g., a SMILES string for molecules, and return a feature value. Consequently, the final representation of an instance resides in an r-dimensional space, where r is the number of features that have been identified.
These vector representations function as the feature vectors for the model training. Employing interpretable models like a linear layer or random forest enables quantification of each rule's importance in prediction, thus elucidating their contribution to the model's final decision. This transparency fosters an intuitive comprehension of the decision-making process, enhancing trust and usability among domain experts.</p>
<h2>Interpretable Explanation Generation</h2>
<p>The final stage in our pipeline involves generating interpretable explanations for the predictions. Specifically, we furnish the LLMs with salient information, including the model prediction, the vector representation, important rules, and their importance scores derived from the random forest or linear layer. Utilizing the inference and summarization ability of the LLMs, the provided information is transformed into a text-based explanation. This stage is pivotal in rendering the results in an accessible manner. It ensures that users can seamlessly understand the decision-making process and each rule's contribution to the overall prediction, thereby enhancing trust and transparency. This accessibility not only facilitates user interaction with the model but also empowers experts in the field to utilize the generated insights for further analysis and decision-making.</p>
<h2>Metrics</h2>
<p>We assessed LLM4SD across 58 molecular property prediction tasks spanning four domains, utilizing distinct evaluation metrics tailored to each task's nature. For the domains of physiology and biophysics, the Area Under the Receiver Operating Characteristic curve (AUC-ROC) metric was employed. AUC-ROC, measures the ability of the model to distinguish between classes, with a range from 0 to 1 , where a higher value indicates better performance. In the domain of physical chemistry, the Root Mean Square Error (RMSE) was used. RMSE quantifies the difference between predicted and observed values, with a lower value indicating a closer fit to the true data. For quantum mechanics, we utilized the Mean Absolute Error (MAE) metric. MAE measures the average magnitude of errors between predicted and true values, with smaller values denoting better accuracy.</p>
<h2>Experiment setting</h2>
<p>In our experimental setting, we partitioned the data into an 80/10/10 split for training, validation, and test sets. For the domains of physiology, biophysics, and physical chemistry, we employed a</p>
<p>scaffold split for molecular compounds. The scaffold split method in these three domains ensures that molecules with similar structures are grouped together, providing a more challenging and realistic evaluation of model generalization. To ensure reproducibility and facilitate further research, the datasets split using this scaffold method are made available in our open-source GitHub repository. In the realm of quantum mechanics, we opted for a random split.</p>
<h1>Data availability</h1>
<p>The datasets utilized in this study are entirely open-source and have been made publicly available to ensure straightforward replication of our findings. For research related to quantum mechanics, physical chemistry, biophysics, and physiology, the datasets can be accessed at https://moleculenet.org/datasets-1.</p>
<h2>Code availability</h2>
<p>In our commitment to transparency and reproducibility, we will release our code showing our implementation in https://github.com/zyzisastudyreallyhardguy/LLM4SD. This encompasses methodologies for literature knowledge mining, knowledge inference rule mining, interpretable model training, and interpretable explanation generation. Throughout this work, we have employed several open-source libraries, including Hugging Face, numpy, rdkit, pytorch, scipy, bitsandbytes, and accelerate.</p>
<p>Furthermore, we are in the process of deploying a website to facilitate scientists in utilizing LLM4SD. The site features three core functionalities for scientific users: knowledge synthesis, knowledge inference, and prediction with explanations. Examples of user interactions with the website can be found in the supplementary information. As part of our ongoing commitment, we anticipate the inclusion of additional tasks in the future development phases.</p>
<ol>
<li>Park, Yang Jeong, Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, and Ju Li. "Can ChatGPT be used to generate scientific hypotheses?." arXiv preprint arXiv:2304.12208 (2023).</li>
<li>Honda, Shion, Shoi Shi, and Hiroki R. Ueda. "Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery." arXiv preprint arXiv:1911.04738 (2019).</li>
</ol>
<h1>Acknowledgements:</h1>
<p>H.Y.K. scholarship is supported by the Australian Government Research Training Program (RTP) Scholarship and Monash University as a co-contribution to Australian Research Council grant ARC DP210100072. L.T.M, G.W and A.T.N.N research into artificial intelligence applications for drug discovery is supported by a National Health and Medical Research Council (NHMRC) of Australia Ideas grant (APP2013629). Computational resources were generously provided by the Nectar Research Cloud, a collaborative Australian research platform supported by the NCRIS-funded Australian Research Data Commons (ARDC) and the MASSIVE HPC facility. We also gratefully acknowledge the support of the Griffith University eResearch Service \&amp; Specialized Platforms Team and the use of the High-Performance Computing Cluster "Gowonda". S.R.P is supported by ARC Future Fellowship (No. FT210100097).</p>
<h2>Author Contribution:</h2>
<p>These authors contributed equally: Y.Z.Z., H.Y.K., J.X.J.
These authors jointly supervised this work: S.R.P., G.I.W.
S.R.P. and G.I.W. supervised the project. Y.Z.Z., H.Y.K., J.X.J. contributed to the conception and design of the work. Y.Z.Z., H.Y.K., J.X.J. contributed to the technical implementation. Y.Z.Z., H.Y.K., J.X.J. prepared the figures. Y.Z.Z. contributed to the design of the web-based application. A.T.N.N. and L.T.M. provided domain expertise for the literature review and validation of rules. Y.Z.Z., H.Y.K., A.T.N.N. and L.T.M. contributed to the design of the rule validation test. All authors edited and revised the manuscript.</p>
<h2>Competing Interests:</h2>
<p>The authors declare no competing interests.</p>
<h1>Extended Figures and Tables</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Extended Data Fig. 1|Detailed performance comparison between "LLM4SD" and eight baselines in the physiology domain. The red dashed line shows the average result across all methods. Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs. LLM4SD outperformed other models in 3 out of 4 datasets using the AUC-ROC metric, and consistently surpassing the average across all datasets. The results for Tox21 and SIDER are average scores from 12 and 27 tasks respectively (see Extended Data Fig. 2 and 3 for detailed breakdown).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Extended Data Fig. 2|Detailed performance comparison between "LLM4SD" and eight baselines on Tox21 Dataset. The red dashed line shows the average result across all methods. Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs. LLM4SD ranks among the top three methods in 10 out of 12 tasks, and consistently outperformed the average in all tasks.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Extended Data Fig. 3|Detailed performance comparison between "LLM4SD" and eight baselines on Sider Dataset. The red dashed line shows the average result across all methods. Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs. LLM4SD ranks among the top three methods in 22 out of 27 tasks, and consistently outperforms the average in all tasks with the exception of the "Psychiatric disorders" task.</p>            </div>
        </div>

    </div>
</body>
</html>
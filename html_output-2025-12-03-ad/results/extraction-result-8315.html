<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8315 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8315</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8315</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-271310469</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.14507v3.pdf" target="_blank">Internal Consistency and Self-Feedback in Large Language Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) often exhibit deficient reasoning or generate hallucinations. To address these, studies prefixed with"Self-"such as Self-Consistency, Self-Improve, and Self-Refine have been initiated. They share a commonality: involving LLMs evaluating and updating themselves. Nonetheless, these efforts lack a unified perspective on summarization, as existing surveys predominantly focus on categorization. In this paper, we use a unified perspective of internal consistency, offering explanations for reasoning deficiencies and hallucinations. Internal consistency refers to the consistency in expressions among LLMs' latent, decoding, or response layers based on sampling methodologies. Then, we introduce an effective theoretical framework capable of mining internal consistency, named Self-Feedback. This framework consists of two modules: Self-Evaluation and Self-Update. The former captures internal consistency signals, while the latter leverages the signals to enhance either the model's response or the model itself. This framework has been employed in numerous studies. We systematically classify these studies by tasks and lines of work; summarize relevant evaluation methods and benchmarks; and delve into the concern,"Does Self-Feedback Really Work?"We also propose several critical viewpoints, including the"Hourglass Evolution of Internal Consistency","Consistency Is (Almost) Correctness"hypothesis, and"The Paradox of Latent and Explicit Reasoning". The relevant resources are open-sourced at https://github.com/IAAR-Shanghai/ICSFSurvey.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8315.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8315.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority voting over multiple sampled Chain-of-Thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt the model to sample multiple reasoning traces (CoTs) and aggregate answers via majority voting or other scoring to improve final-answer accuracy; leverages diversity of sampled reasoning paths as a signal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; unspecified in this survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to large language models via prompt sampling of chain-of-thoughts; survey does not fix a single architecture/size.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) sampling', 'Majority voting aggregation', 'Soft scoring (joint token probability) variants']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Multiple CoT outputs are generated under stochastic sampling; the final answer is selected by majority vote (Self-Consistency) or by scoring each full output (Soft Self-Consistency calculates joint token probability). Universal SC uses an LLM to aggregate instead of strict answer matching.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compare single-chain CoT output vs multiple sampled CoTs aggregated by majority voting or scoring (majority voting baseline vs Self-Consistency; Soft SC uses joint-prob scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (math word-problem reasoning benchmark) and other reasoning QA tasks referenced</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports Self-Consistency increased answer accuracy on GSM8K by about 17.9% (as cited from the Self-Consistency work).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Diverse sampled reasoning paths contain many individually incorrect traces but the aggregated answer is often correct; sampling diversity provides a signal that majority aggregation can exploit to improve accuracy; however, aggregation is limited when label space is open-ended.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Sampling diverse reasoning paths and aggregating (Self-Consistency) substantially improves accuracy on closed-label reasoning benchmarks relative to returning a single CoT; aggregation strategies (majority, soft scoring, LLM-based) affect robustness and applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8315.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that encourages the model to generate intermediate reasoning steps (explicit multi-step rationales) before giving a final answer, improving performance on complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; unspecified in this survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used with large transformer-based LLMs via prompt templates that elicit multi-step textual reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Explicit multi-step textual reasoning (rationales)', 'Step-by-step decomposition of problems']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT instructs models to 'think step by step' producing intermediate tokens representing reasoning; these explicit chains can be sampled or used directly as output.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>CoT typically uses a single explicit reasoning style; diversity is introduced by sampling multiple CoTs (then enabling Self-Consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Complex multi-step reasoning benchmarks (GSM8K, BigBench-like reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey cites CoT as beneficial to solving complex problems; no single numeric aggregate from survey beyond cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Explicit reasoning chains can improve problem solving but may introduce textual noise that can disrupt latent reasoning; long CoTs may hurt latent single-token decision processes.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT yields better results on many multi-step tasks but interacts nontrivially with latent reasoning and decoding; combined with sampling+aggregation (e.g., Self-Consistency) it is more powerful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8315.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Search-based reasoning topology that treats reasoning as exploring a tree of thought nodes (successors) with evaluators guiding breadth-first or depth-first search to find correct solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; unspecified in this survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies LLMs as node generators and evaluators within a search (tree) process over candidate intermediate reasoning states.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Search over thought nodes (BFS/DFS/MCTS)', 'Evaluator-based selection of promising reasoning branches']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM is used to expand thought nodes and another mechanism (rule, LLM, or scorer) evaluates which nodes deserve further expansion; ToT aims to explore many diverse reasoning paths systematically.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Comparisons typically contrast single-path CoT vs tree search exploration; ToT performs graph/tree search with evaluators selecting branches; survey notes ToT searches more but at higher cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Toy and compositional reasoning tasks such as Game of 24, sorting, keyword counting; small-scale reasoning benchmarks used in cited works</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey states ToT can find correct solutions more often than single-path CoT on chosen tasks but exact numbers depend on cited papers (not reproduced numerically in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ToT increases exploration and can recover from early wrong steps by considering alternate branches; computational cost is high and applicability to general tasks is limited by cost and evaluator design.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Searching multiple diverse reasoning branches (ToT) can outperform single-path CoT, but at substantial cost and with limitations on generalization and benchmark selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8315.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-of-Thought (GoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-of-Thought (GoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extension of tree-based reasoning that aggregates reasoning chains across a graph of nodes, enabling richer connectivity and aggregation of partial solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph of thoughts: Solving elaborate problems with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; unspecified in this survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs generate and connect thought-nodes into a graph structure; aggregation leverages multiple chains and entailment relations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Graph-based aggregation of reasoning chains', 'Entailment/consistency-based selection across nodes']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GoT aggregates multiple reasoning chains by connecting related intermediate thoughts, then uses scoring/aggregation to produce answers (e.g., Max-SAT formulation over entailment constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared conceptually to CoT and ToT; GoT aggregates many chains and uses optimization (e.g., Max-SAT) to select coherent outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Elaborate compositional problems and proof-like tasks cited in the literature (survey references)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports GoT as a method for broader aggregation; specific numeric performance left to original GoT papers (not enumerated in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Aggregating reasoning across a graph can exploit cross-support between partial thoughts, potentially improving robustness, but requires complex evaluators and incurs higher cost.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Graph aggregation extends tree search advantages and can produce more coherent solutions, at the expense of added complexity and evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8315.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quiet-STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quiet Self-Taught Reasoner (Quiet-STaR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Technique that samples rationales but hides them from the user by wrapping them with special markers so they assist next-token reasoning (making latent reasoning explicit without exposing verbose text to the user).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quiet-star: Language models can teach themselves to think before speaking.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses special tokens to insert sampled rationales into the model's context for internal use during generation; rationales are not shown to the end-user.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Invisible rationale injection (latent-explicit hybrid)', 'Sampling of rationales to improve latent token prediction']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Rationales are wrapped between markers (e.g., <|startofthought|>, <|endofthought|>) to make them available for subsequent token generation while keeping outputs concise to users.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared to normal CoT, Quiet-STaR samples rationales and injects them as hidden context—survey notes efficacy at reducing response-level inconsistency due to interfering visible rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multi-step reasoning tasks (cited); specific benchmarks not enumerated in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey claims Quiet-STaR effectively reduces conflicts between explicit response text and latent reasoning, improving performance; no numeric results given in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Making latent reasoning explicit but hidden to the user preserves benefits of multi-step thought without generating distracting surface-level text; reduces collapse of latent reasoning that visible rationales can cause.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Hybrid latent-explicit approaches that protect latent reasoning from interference (via hidden markers) can improve consistency and reasoning outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8315.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Contradict</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Contradict (Self-Contradiction detection and resolution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Induce the model to generate diverse content designed to reveal contradictions, then detect and resolve contradictions to reduce hallucinations and improve consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (example cited) and other LLMs (survey cites GPT-4 results)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey notes experiments showing even strong models produce self-contradictions; method induces contradictory outputs and then resolves them.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Diverse generation to expose contradictions', 'Contradiction detection and selective retention of consistent content']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate variant samples, detect contradictions among them, and resolve by keeping consistent parts or prompting the model to reconcile differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Generate multiple different outputs to surface self-contradictions, then apply resolution heuristics to produce coherent output.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Hallucination detection/evaluation settings (open-ended generation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey references Mündle et al.'s finding that GPT-4 can be induced to self-contradict at rates of 15.7%; using contradiction resolution reduces hallucinations (no precise post-mitigation numbers in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Diverse generation helps surface contradictions that are likely to be hallucinations; resolving contradictions by keeping consistent information reduces hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Generating diverse variants and checking for self-contradictions is a practical signal for hallucination detection and mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8315.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Perspective / Universal / Soft SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variants of Self-Consistency (Multi-Perspective Self-Consistency, Universal Self-Consistency, Soft Self-Consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extensions to Self-Consistency: evaluate solutions from multiple perspectives (MPSC), use an LLM judge instead of strict label matching (Universal SC), or score responses by joint token probability (Soft SC) to handle soft/continuous label spaces and complex outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adaptations of sampling+aggregation approaches to richer tasks such as code generation and soft-label tasks; rely on LLMs or scoring functions to aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Multi-perspective evaluation (solution, spec, test-case)', 'LLM-based aggregation (Universal SC)', 'Joint token-probability scoring (Soft SC)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MPSC evaluates each candidate on multiple independent criteria; Universal SC uses an LLM as an aggregator to select the best answer; Soft SC computes joint probabilities of token sequences to rank candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compare simple majority-vote aggregation vs multi-perspective or LLM-based aggregation; adapt aggregation to tasks with soft labels or program outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Code generation, open-label generation, and reasoning tasks where labels are not fixed; survey cites MPSC for code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports these variants address limitations of plain majority-voting but does not provide numeric comparisons in the survey itself.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Richer aggregation signals (multi-perspective, LLM judges, scoring) extend self-consistency benefits to tasks with complex outputs; they improve selection stability over naive majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Aggregation strategy matters: more sophisticated scoring or judged aggregation enables sampling-based consistency methods to work on broader task classes beyond fixed-label QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8315.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Diversity (DIVERSE / Promptbreeder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Diversity and Optimization (DIVERSE, Promptbreeder, DSPy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Techniques to create or evolve multiple prompt templates to increase diversity in elicited reasoning paths and improve problem-solving by exploring varied instruction contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Apply many prompt templates or evolve prompts (genetic algorithms, optimizers) to increase the diversity of reasoning traces elicited from an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Prompt template diversity', 'Automated prompt evolution (genetic algorithms, optimization)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Pre-construct many prompt templates (DIVERSE), use genetic evolution to optimize prompts (Promptbreeder), or build prompt optimizers (DSPy) to find prompting that yields better reasoning/topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compare baseline single prompt vs multiple varied prompts or evolved prompts to measure improvements in downstream reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General reasoning and instruction-following tasks; survey notes creativity of these methods but does not list numeric benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey notes these prompt-diversity approaches can improve exploration and performance but does not report specific numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Increasing prompt diversity can expand the space of reasoning paths explored and improve discovery of correct solutions; sensitive to prompt design and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Optimizing input prompts and increasing prompt diversity is an effective way to diversify reasoning paths and can complement sampling-based Self-Feedback methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8315.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Evaluation Decoding (SED) / CoT-Decoding / ToT-Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Decoding and CoT/ToT Decoding strategies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding-time strategies that use internal self-evaluation signals to detect chaotic points and select tokens/paths that maximize decoding consistency or evaluated quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sed: Self-evaluation decoding enhances large language models for better generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modify decoding (beam search or sampling) to incorporate per-token or per-path confidence/consistency scores derived from the model's own evaluations or multi-strategy comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Self-evaluation-guided token selection', 'CoT-informed decoding', 'ToT-style decoding with node evaluation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Detect 'chaotic points' (high token ambiguity) and use an evaluation function (confidence, information gain, self-eval) to select tokens/branches; CoT/ToT decoding integrates chain-of-thought style reasoning into decoding decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compare naive decoding (greedy/beam/sampling) vs decoding augmented with self-evaluation signals (SED) or CoT/ToT-informed decoding; survey references detection criteria and indicator functions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Closed-book generation, factuality-sensitive generation tasks; methods aimed at improving decoding consistency across generations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey references DoLa, CAD, ECAD, DIVER showing decoding-based interventions improve factuality/consistency; no unified numeric performance in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Incorporating self-evaluation at decoding mitigates chaotic token choices and can improve factuality and consistency; detecting and addressing chaotic points is effective for decoding-level consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Decoding-level self-feedback (detecting ambiguous tokens and re-evaluating) helps maintain consistency and factuality during generation and complements response-level aggregation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8315.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Debate / FORD / REFINER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate and Formal Debate Frameworks (Multi-Agent Debate, FORD, REFINER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use multiple LLM agents with roles (generator/examiner/judge/critic) to iteratively debate or critique solutions and converge to a consensus answer via negotiation or judge aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLM instances (peer models / judge model setups; unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-agent systems where different LLM instances generate, critique, and judge candidate solutions; may include a Judge LLM to aggregate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Iterative multi-agent debate', 'Generator-critic role splitting', 'Judge-based aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Agents produce arguments/solutions and critique peers; debate proceeds for a fixed number of rounds or until a judge model selects a final result; REFINER trains generator and critic models for iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Empirical setups involve multiple agents debating over rounds; Xiong et al. and others analyze convergence properties and judge aggregation; survey notes debates with <=3 rounds often converge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Reasoning QA and complex tasks; cited works evaluate on tasks where debate yields benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey states debates with three or fewer rounds generally lead to convergence; some critiques argue debate can be resource-heavy and sometimes inferior to Self-Consistency depending on setup (Huang et al. critique).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Multi-agent debate can provide richer perspectives and correction signals but at higher cost and with sensitivity to stopping criteria and agent capabilities; judge design strongly impacts outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Multi-agent collaborations can improve reasoning via cross-examination and critique but are computationally expensive and require careful design of stopping/aggregation criteria; performance vs single-model aggregation is mixed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8315.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B-Instruct experiment (hourglass pattern)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-8B-Instruct sampling and layer-ablation experiment (this survey)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In-paper experiment where authors sampled responses, decoding strategies, and ablated attention heads to show an 'hourglass evolution' of consistency from latent to decoding to response layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An 8B-parameter instruction-tuned Llama3 model variant used by the survey authors to empirically illustrate layerwise consistency phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought prompting (CoT)', 'Sampling at response level (Top-p)', 'Multiple decoding strategies (Greedy, Beam, Sampling, Top-k, Top-p)', 'Latent attention-head ablation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompting used to induce diverse responses; Top-p sampling with fixed temperature produced multiple textual responses; decoding-level sampling used five decoding strategies; latent-level analysis kept specific attention heads active while zeroing others to observe token choice.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared sampled outputs at three expression types: response-layer sampling (Y_response = {5,3,3,3,3}), decoding-layer sampling across five decoders (Y_decoding = {4,4,3,4,4}), and latent-layer attention-head ablations across six (l,n) combinations (Y_latent = <0,0,5,4,4,4>). This directly compares diversity across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>A simple counting question (How many full stops are there in '.!..!..!') used as an illustrative example and small-scale diagnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Observed sampling sets: response-layer Y_response = {5,3,3,3,3} (variation), decoding-layer Y_decoding = {4,4,3,4,4} (more consistent), latent-layer Y_latent = <0,0,5,4,4,4> (least consistent near bottom layers). Authors report an 'hourglass evolution' where latent decisions converge through decoding to response, but response-level variability increases again due to surface text interference.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Latent attention heads show greater variance in lower layers and more convergence near top layers; decoding strategies tend to select higher-probability tokens (more stable), while response-level text generation can reintroduce variability (e.g., conversational preambles interfering with latent judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Different layers present different degrees and forms of reasoning diversity; mining internal consistency requires attention to latent, decoding, and response layers because diversity at one layer does not straightforwardly translate to consistency at others (the 'hourglass' pattern).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8315.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8315.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine / Reflexion / Self-Correct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Refinement Frameworks (Self-Refine, Reflexion, Self-Correct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative self-feedback frameworks where the model generates textual feedback about its outputs and uses that feedback (and possibly external signals) to iteratively update responses or train corrective models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs produce feedback messages (textual critiques or error messages) and then revise their outputs iteratively; Reflexion frames this as verbal reinforcement learning without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Iterative verbal self-evaluation and revision', 'External execution feedback (e.g., code execution) when available']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>At each iteration the LLM self-evaluates (textual feedback), then applies an update to the response; external tools/executors may provide additional feedback signals (e.g., unit test failures) used to guide revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Iterative loops compare initial single-shot outputs vs refined outputs after multiple self-feedback iterations; some works train Corrector models (Self-Correct) or rely on external signals (Self-Debug).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Open-ended generation, code generation, story writing; specific benchmarks vary by cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey notes these iterative schemes can improve final outputs and reduce hallucinations in some tasks; effectiveness depends on feedback signal design and has been critiqued by subsequent works (some rebuttals argue practical limits).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Iterative textual self-feedback helps catch and correct many errors but can be limited by the quality of the feedback signal and dependence on external verification for some tasks (e.g., code execution).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Iterative self-feedback is a general framework that often improves response quality, but success depends on feedback fidelity and task verifiability; some works critique practical effectiveness in certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Internal Consistency and Self-Feedback in Large Language Models: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models. <em>(Rating: 2)</em></li>
                <li>Quiet-star: Language models can teach themselves to think before speaking. <em>(Rating: 2)</em></li>
                <li>Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. <em>(Rating: 2)</em></li>
                <li>Sed: Self-evaluation decoding enhances large language models for better generation. <em>(Rating: 2)</em></li>
                <li>Inference-time intervention: Eliciting truthful answers from a language model. <em>(Rating: 1)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8315",
    "paper_id": "paper-271310469",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority voting over multiple sampled Chain-of-Thoughts)",
            "brief_description": "Prompt the model to sample multiple reasoning traces (CoTs) and aggregate answers via majority voting or other scoring to improve final-answer accuracy; leverages diversity of sampled reasoning paths as a signal.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; unspecified in this survey)",
            "model_description": "Applied to large language models via prompt sampling of chain-of-thoughts; survey does not fix a single architecture/size.",
            "reasoning_methods": [
                "Chain-of-Thought (CoT) sampling",
                "Majority voting aggregation",
                "Soft scoring (joint token probability) variants"
            ],
            "reasoning_methods_description": "Multiple CoT outputs are generated under stochastic sampling; the final answer is selected by majority vote (Self-Consistency) or by scoring each full output (Soft Self-Consistency calculates joint token probability). Universal SC uses an LLM to aggregate instead of strict answer matching.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compare single-chain CoT output vs multiple sampled CoTs aggregated by majority voting or scoring (majority voting baseline vs Self-Consistency; Soft SC uses joint-prob scoring).",
            "task_or_benchmark": "GSM8K (math word-problem reasoning benchmark) and other reasoning QA tasks referenced",
            "performance_results": "Survey reports Self-Consistency increased answer accuracy on GSM8K by about 17.9% (as cited from the Self-Consistency work).",
            "qualitative_findings": "Diverse sampled reasoning paths contain many individually incorrect traces but the aggregated answer is often correct; sampling diversity provides a signal that majority aggregation can exploit to improve accuracy; however, aggregation is limited when label space is open-ended.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Sampling diverse reasoning paths and aggregating (Self-Consistency) substantially improves accuracy on closed-label reasoning benchmarks relative to returning a single CoT; aggregation strategies (majority, soft scoring, LLM-based) affect robustness and applicability.",
            "uuid": "e8315.0",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought (CoT) Prompting",
            "brief_description": "Prompting technique that encourages the model to generate intermediate reasoning steps (explicit multi-step rationales) before giving a final answer, improving performance on complex reasoning tasks.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; unspecified in this survey)",
            "model_description": "Used with large transformer-based LLMs via prompt templates that elicit multi-step textual reasoning traces.",
            "reasoning_methods": [
                "Explicit multi-step textual reasoning (rationales)",
                "Step-by-step decomposition of problems"
            ],
            "reasoning_methods_description": "CoT instructs models to 'think step by step' producing intermediate tokens representing reasoning; these explicit chains can be sampled or used directly as output.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "CoT typically uses a single explicit reasoning style; diversity is introduced by sampling multiple CoTs (then enabling Self-Consistency).",
            "task_or_benchmark": "Complex multi-step reasoning benchmarks (GSM8K, BigBench-like reasoning tasks)",
            "performance_results": "Survey cites CoT as beneficial to solving complex problems; no single numeric aggregate from survey beyond cited literature.",
            "qualitative_findings": "Explicit reasoning chains can improve problem solving but may introduce textual noise that can disrupt latent reasoning; long CoTs may hurt latent single-token decision processes.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "CoT yields better results on many multi-step tasks but interacts nontrivially with latent reasoning and decoding; combined with sampling+aggregation (e.g., Self-Consistency) it is more powerful.",
            "uuid": "e8315.1",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Tree-of-Thought (ToT)",
            "name_full": "Tree-of-Thought (ToT)",
            "brief_description": "Search-based reasoning topology that treats reasoning as exploring a tree of thought nodes (successors) with evaluators guiding breadth-first or depth-first search to find correct solutions.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; unspecified in this survey)",
            "model_description": "Applies LLMs as node generators and evaluators within a search (tree) process over candidate intermediate reasoning states.",
            "reasoning_methods": [
                "Search over thought nodes (BFS/DFS/MCTS)",
                "Evaluator-based selection of promising reasoning branches"
            ],
            "reasoning_methods_description": "LLM is used to expand thought nodes and another mechanism (rule, LLM, or scorer) evaluates which nodes deserve further expansion; ToT aims to explore many diverse reasoning paths systematically.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Comparisons typically contrast single-path CoT vs tree search exploration; ToT performs graph/tree search with evaluators selecting branches; survey notes ToT searches more but at higher cost.",
            "task_or_benchmark": "Toy and compositional reasoning tasks such as Game of 24, sorting, keyword counting; small-scale reasoning benchmarks used in cited works",
            "performance_results": "Survey states ToT can find correct solutions more often than single-path CoT on chosen tasks but exact numbers depend on cited papers (not reproduced numerically in survey).",
            "qualitative_findings": "ToT increases exploration and can recover from early wrong steps by considering alternate branches; computational cost is high and applicability to general tasks is limited by cost and evaluator design.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Searching multiple diverse reasoning branches (ToT) can outperform single-path CoT, but at substantial cost and with limitations on generalization and benchmark selection.",
            "uuid": "e8315.2",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Graph-of-Thought (GoT)",
            "name_full": "Graph-of-Thought (GoT)",
            "brief_description": "Extension of tree-based reasoning that aggregates reasoning chains across a graph of nodes, enabling richer connectivity and aggregation of partial solutions.",
            "citation_title": "Graph of thoughts: Solving elaborate problems with large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; unspecified in this survey)",
            "model_description": "LLMs generate and connect thought-nodes into a graph structure; aggregation leverages multiple chains and entailment relations.",
            "reasoning_methods": [
                "Graph-based aggregation of reasoning chains",
                "Entailment/consistency-based selection across nodes"
            ],
            "reasoning_methods_description": "GoT aggregates multiple reasoning chains by connecting related intermediate thoughts, then uses scoring/aggregation to produce answers (e.g., Max-SAT formulation over entailment constraints).",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared conceptually to CoT and ToT; GoT aggregates many chains and uses optimization (e.g., Max-SAT) to select coherent outcomes.",
            "task_or_benchmark": "Elaborate compositional problems and proof-like tasks cited in the literature (survey references)",
            "performance_results": "Survey reports GoT as a method for broader aggregation; specific numeric performance left to original GoT papers (not enumerated in survey).",
            "qualitative_findings": "Aggregating reasoning across a graph can exploit cross-support between partial thoughts, potentially improving robustness, but requires complex evaluators and incurs higher cost.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Graph aggregation extends tree search advantages and can produce more coherent solutions, at the expense of added complexity and evaluation cost.",
            "uuid": "e8315.3",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Quiet-STaR",
            "name_full": "Quiet Self-Taught Reasoner (Quiet-STaR)",
            "brief_description": "Technique that samples rationales but hides them from the user by wrapping them with special markers so they assist next-token reasoning (making latent reasoning explicit without exposing verbose text to the user).",
            "citation_title": "Quiet-star: Language models can teach themselves to think before speaking.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified in survey)",
            "model_description": "Uses special tokens to insert sampled rationales into the model's context for internal use during generation; rationales are not shown to the end-user.",
            "reasoning_methods": [
                "Invisible rationale injection (latent-explicit hybrid)",
                "Sampling of rationales to improve latent token prediction"
            ],
            "reasoning_methods_description": "Rationales are wrapped between markers (e.g., &lt;|startofthought|&gt;, &lt;|endofthought|&gt;) to make them available for subsequent token generation while keeping outputs concise to users.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared to normal CoT, Quiet-STaR samples rationales and injects them as hidden context—survey notes efficacy at reducing response-level inconsistency due to interfering visible rationales.",
            "task_or_benchmark": "Multi-step reasoning tasks (cited); specific benchmarks not enumerated in survey.",
            "performance_results": "Survey claims Quiet-STaR effectively reduces conflicts between explicit response text and latent reasoning, improving performance; no numeric results given in survey.",
            "qualitative_findings": "Making latent reasoning explicit but hidden to the user preserves benefits of multi-step thought without generating distracting surface-level text; reduces collapse of latent reasoning that visible rationales can cause.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Hybrid latent-explicit approaches that protect latent reasoning from interference (via hidden markers) can improve consistency and reasoning outcomes.",
            "uuid": "e8315.4",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-Contradict",
            "name_full": "Self-Contradict (Self-Contradiction detection and resolution)",
            "brief_description": "Induce the model to generate diverse content designed to reveal contradictions, then detect and resolve contradictions to reduce hallucinations and improve consistency.",
            "citation_title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (example cited) and other LLMs (survey cites GPT-4 results)",
            "model_description": "Survey notes experiments showing even strong models produce self-contradictions; method induces contradictory outputs and then resolves them.",
            "reasoning_methods": [
                "Diverse generation to expose contradictions",
                "Contradiction detection and selective retention of consistent content"
            ],
            "reasoning_methods_description": "Generate variant samples, detect contradictions among them, and resolve by keeping consistent parts or prompting the model to reconcile differences.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Generate multiple different outputs to surface self-contradictions, then apply resolution heuristics to produce coherent output.",
            "task_or_benchmark": "Hallucination detection/evaluation settings (open-ended generation tasks)",
            "performance_results": "Survey references Mündle et al.'s finding that GPT-4 can be induced to self-contradict at rates of 15.7%; using contradiction resolution reduces hallucinations (no precise post-mitigation numbers in survey).",
            "qualitative_findings": "Diverse generation helps surface contradictions that are likely to be hallucinations; resolving contradictions by keeping consistent information reduces hallucinations.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Generating diverse variants and checking for self-contradictions is a practical signal for hallucination detection and mitigation.",
            "uuid": "e8315.5",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Multi-Perspective / Universal / Soft SC",
            "name_full": "Variants of Self-Consistency (Multi-Perspective Self-Consistency, Universal Self-Consistency, Soft Self-Consistency)",
            "brief_description": "Extensions to Self-Consistency: evaluate solutions from multiple perspectives (MPSC), use an LLM judge instead of strict label matching (Universal SC), or score responses by joint token probability (Soft SC) to handle soft/continuous label spaces and complex outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; unspecified in survey)",
            "model_description": "Adaptations of sampling+aggregation approaches to richer tasks such as code generation and soft-label tasks; rely on LLMs or scoring functions to aggregate.",
            "reasoning_methods": [
                "Multi-perspective evaluation (solution, spec, test-case)",
                "LLM-based aggregation (Universal SC)",
                "Joint token-probability scoring (Soft SC)"
            ],
            "reasoning_methods_description": "MPSC evaluates each candidate on multiple independent criteria; Universal SC uses an LLM as an aggregator to select the best answer; Soft SC computes joint probabilities of token sequences to rank candidates.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compare simple majority-vote aggregation vs multi-perspective or LLM-based aggregation; adapt aggregation to tasks with soft labels or program outputs.",
            "task_or_benchmark": "Code generation, open-label generation, and reasoning tasks where labels are not fixed; survey cites MPSC for code tasks.",
            "performance_results": "Survey reports these variants address limitations of plain majority-voting but does not provide numeric comparisons in the survey itself.",
            "qualitative_findings": "Richer aggregation signals (multi-perspective, LLM judges, scoring) extend self-consistency benefits to tasks with complex outputs; they improve selection stability over naive majority voting.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Aggregation strategy matters: more sophisticated scoring or judged aggregation enables sampling-based consistency methods to work on broader task classes beyond fixed-label QA.",
            "uuid": "e8315.6",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Prompt Diversity (DIVERSE / Promptbreeder)",
            "name_full": "Prompt Diversity and Optimization (DIVERSE, Promptbreeder, DSPy)",
            "brief_description": "Techniques to create or evolve multiple prompt templates to increase diversity in elicited reasoning paths and improve problem-solving by exploring varied instruction contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; unspecified in survey)",
            "model_description": "Apply many prompt templates or evolve prompts (genetic algorithms, optimizers) to increase the diversity of reasoning traces elicited from an LLM.",
            "reasoning_methods": [
                "Prompt template diversity",
                "Automated prompt evolution (genetic algorithms, optimization)"
            ],
            "reasoning_methods_description": "Pre-construct many prompt templates (DIVERSE), use genetic evolution to optimize prompts (Promptbreeder), or build prompt optimizers (DSPy) to find prompting that yields better reasoning/topologies.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compare baseline single prompt vs multiple varied prompts or evolved prompts to measure improvements in downstream reasoning performance.",
            "task_or_benchmark": "General reasoning and instruction-following tasks; survey notes creativity of these methods but does not list numeric benchmarks.",
            "performance_results": "Survey notes these prompt-diversity approaches can improve exploration and performance but does not report specific numbers.",
            "qualitative_findings": "Increasing prompt diversity can expand the space of reasoning paths explored and improve discovery of correct solutions; sensitive to prompt design and evaluation.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Optimizing input prompts and increasing prompt diversity is an effective way to diversify reasoning paths and can complement sampling-based Self-Feedback methods.",
            "uuid": "e8315.7",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-Evaluation Decoding (SED) / CoT-Decoding / ToT-Decoding",
            "name_full": "Self-Evaluation Decoding and CoT/ToT Decoding strategies",
            "brief_description": "Decoding-time strategies that use internal self-evaluation signals to detect chaotic points and select tokens/paths that maximize decoding consistency or evaluated quality.",
            "citation_title": "Sed: Self-evaluation decoding enhances large language models for better generation.",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; unspecified in survey)",
            "model_description": "Modify decoding (beam search or sampling) to incorporate per-token or per-path confidence/consistency scores derived from the model's own evaluations or multi-strategy comparisons.",
            "reasoning_methods": [
                "Self-evaluation-guided token selection",
                "CoT-informed decoding",
                "ToT-style decoding with node evaluation"
            ],
            "reasoning_methods_description": "Detect 'chaotic points' (high token ambiguity) and use an evaluation function (confidence, information gain, self-eval) to select tokens/branches; CoT/ToT decoding integrates chain-of-thought style reasoning into decoding decisions.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compare naive decoding (greedy/beam/sampling) vs decoding augmented with self-evaluation signals (SED) or CoT/ToT-informed decoding; survey references detection criteria and indicator functions.",
            "task_or_benchmark": "Closed-book generation, factuality-sensitive generation tasks; methods aimed at improving decoding consistency across generations.",
            "performance_results": "Survey references DoLa, CAD, ECAD, DIVER showing decoding-based interventions improve factuality/consistency; no unified numeric performance in survey.",
            "qualitative_findings": "Incorporating self-evaluation at decoding mitigates chaotic token choices and can improve factuality and consistency; detecting and addressing chaotic points is effective for decoding-level consistency.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Decoding-level self-feedback (detecting ambiguous tokens and re-evaluating) helps maintain consistency and factuality during generation and complements response-level aggregation methods.",
            "uuid": "e8315.8",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Multi-Agent Debate / FORD / REFINER",
            "name_full": "Multi-Agent Debate and Formal Debate Frameworks (Multi-Agent Debate, FORD, REFINER)",
            "brief_description": "Use multiple LLM agents with roles (generator/examiner/judge/critic) to iteratively debate or critique solutions and converge to a consensus answer via negotiation or judge aggregation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Multiple LLM instances (peer models / judge model setups; unspecified)",
            "model_description": "Multi-agent systems where different LLM instances generate, critique, and judge candidate solutions; may include a Judge LLM to aggregate outputs.",
            "reasoning_methods": [
                "Iterative multi-agent debate",
                "Generator-critic role splitting",
                "Judge-based aggregation"
            ],
            "reasoning_methods_description": "Agents produce arguments/solutions and critique peers; debate proceeds for a fixed number of rounds or until a judge model selects a final result; REFINER trains generator and critic models for iterative refinement.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Empirical setups involve multiple agents debating over rounds; Xiong et al. and others analyze convergence properties and judge aggregation; survey notes debates with &lt;=3 rounds often converge.",
            "task_or_benchmark": "Reasoning QA and complex tasks; cited works evaluate on tasks where debate yields benefits.",
            "performance_results": "Survey states debates with three or fewer rounds generally lead to convergence; some critiques argue debate can be resource-heavy and sometimes inferior to Self-Consistency depending on setup (Huang et al. critique).",
            "qualitative_findings": "Multi-agent debate can provide richer perspectives and correction signals but at higher cost and with sensitivity to stopping criteria and agent capabilities; judge design strongly impacts outcomes.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Multi-agent collaborations can improve reasoning via cross-examination and critique but are computationally expensive and require careful design of stopping/aggregation criteria; performance vs single-model aggregation is mixed.",
            "uuid": "e8315.9",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama3-8B-Instruct experiment (hourglass pattern)",
            "name_full": "Llama3-8B-Instruct sampling and layer-ablation experiment (this survey)",
            "brief_description": "In-paper experiment where authors sampled responses, decoding strategies, and ablated attention heads to show an 'hourglass evolution' of consistency from latent to decoding to response layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_description": "An 8B-parameter instruction-tuned Llama3 model variant used by the survey authors to empirically illustrate layerwise consistency phenomena.",
            "reasoning_methods": [
                "Chain-of-Thought prompting (CoT)",
                "Sampling at response level (Top-p)",
                "Multiple decoding strategies (Greedy, Beam, Sampling, Top-k, Top-p)",
                "Latent attention-head ablation"
            ],
            "reasoning_methods_description": "CoT prompting used to induce diverse responses; Top-p sampling with fixed temperature produced multiple textual responses; decoding-level sampling used five decoding strategies; latent-level analysis kept specific attention heads active while zeroing others to observe token choice.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared sampled outputs at three expression types: response-layer sampling (Y_response = {5,3,3,3,3}), decoding-layer sampling across five decoders (Y_decoding = {4,4,3,4,4}), and latent-layer attention-head ablations across six (l,n) combinations (Y_latent = &lt;0,0,5,4,4,4&gt;). This directly compares diversity across layers.",
            "task_or_benchmark": "A simple counting question (How many full stops are there in '.!..!..!') used as an illustrative example and small-scale diagnostic.",
            "performance_results": "Observed sampling sets: response-layer Y_response = {5,3,3,3,3} (variation), decoding-layer Y_decoding = {4,4,3,4,4} (more consistent), latent-layer Y_latent = &lt;0,0,5,4,4,4&gt; (least consistent near bottom layers). Authors report an 'hourglass evolution' where latent decisions converge through decoding to response, but response-level variability increases again due to surface text interference.",
            "qualitative_findings": "Latent attention heads show greater variance in lower layers and more convergence near top layers; decoding strategies tend to select higher-probability tokens (more stable), while response-level text generation can reintroduce variability (e.g., conversational preambles interfering with latent judgments).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Different layers present different degrees and forms of reasoning diversity; mining internal consistency requires attention to latent, decoding, and response layers because diversity at one layer does not straightforwardly translate to consistency at others (the 'hourglass' pattern).",
            "uuid": "e8315.10",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-Refine / Reflexion / Self-Correct",
            "name_full": "Iterative Refinement Frameworks (Self-Refine, Reflexion, Self-Correct)",
            "brief_description": "Iterative self-feedback frameworks where the model generates textual feedback about its outputs and uses that feedback (and possibly external signals) to iteratively update responses or train corrective models.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; unspecified in survey)",
            "model_description": "LLMs produce feedback messages (textual critiques or error messages) and then revise their outputs iteratively; Reflexion frames this as verbal reinforcement learning without parameter updates.",
            "reasoning_methods": [
                "Iterative verbal self-evaluation and revision",
                "External execution feedback (e.g., code execution) when available"
            ],
            "reasoning_methods_description": "At each iteration the LLM self-evaluates (textual feedback), then applies an update to the response; external tools/executors may provide additional feedback signals (e.g., unit test failures) used to guide revisions.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Iterative loops compare initial single-shot outputs vs refined outputs after multiple self-feedback iterations; some works train Corrector models (Self-Correct) or rely on external signals (Self-Debug).",
            "task_or_benchmark": "Open-ended generation, code generation, story writing; specific benchmarks vary by cited work.",
            "performance_results": "Survey notes these iterative schemes can improve final outputs and reduce hallucinations in some tasks; effectiveness depends on feedback signal design and has been critiqued by subsequent works (some rebuttals argue practical limits).",
            "qualitative_findings": "Iterative textual self-feedback helps catch and correct many errors but can be limited by the quality of the feedback signal and dependence on external verification for some tasks (e.g., code execution).",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Iterative self-feedback is a general framework that often improves response quality, but success depends on feedback fidelity and task verifiability; some works critique practical effectiveness in certain tasks.",
            "uuid": "e8315.11",
            "source_info": {
                "paper_title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models.",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Quiet-star: Language models can teach themselves to think before speaking.",
            "rating": 2,
            "sanitized_title": "quietstar_language_models_can_teach_themselves_to_think_before_speaking"
        },
        {
            "paper_title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation.",
            "rating": 2,
            "sanitized_title": "selfcontradictory_hallucinations_of_large_language_models_evaluation_detection_and_mitigation"
        },
        {
            "paper_title": "Sed: Self-evaluation decoding enhances large language models for better generation.",
            "rating": 2,
            "sanitized_title": "sed_selfevaluation_decoding_enhances_large_language_models_for_better_generation"
        },
        {
            "paper_title": "Inference-time intervention: Eliciting truthful answers from a language model.",
            "rating": 1,
            "sanitized_title": "inferencetime_intervention_eliciting_truthful_answers_from_a_language_model"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning.",
            "rating": 1,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        }
    ],
    "cost": 0.021918,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Internal Consistency and Self-Feedback in Large Language Models: A Survey
18 Sep 2024</p>
<p>Xun Liang 
Senior Member, IEEEShichao Song 
Zifan Zheng 
Hanyu Wang 
Qingchen Yu 
Xunkai Li 
Rong-Hua Li 
Yi Wang 
Zhonghao Wang 
Feiyu Xiong 
Zhiyu Li </p>
<p>School of Information
Zifan Zheng
Renmin University of China
Beijing, Qingchen Yu, Feiyu XiongChina</p>
<p>Large Language Model Center
Institute for Advanced Algorithms Research
ShanghaiChina</p>
<p>Xunkai Li and Rong-Hua Li</p>
<p>School of Computer Science and Technology
Beijing Institute of Technology
BeijingChina. Yi Wang</p>
<p>State Key Laboratory of Media Convergence Production Technology and Systems
Xinhua News Agency
BeijingChina</p>
<p>Internal Consistency and Self-Feedback in Large Language Models: A Survey
18 Sep 2024DC0C35E6864072112F8DD19EA3116B70arXiv:2407.14507v3[cs.CL]Large Language Model (LLM)Internal ConsistencySelf-FeedbackReasoningHallucination
Large language models (LLMs) often exhibit deficient reasoning or generate hallucinations.To address these, studies prefixed with "Self-" such as Self-Consistency, Self-Improve, and Self-Refine have been initiated.They share a commonality: involving LLMs evaluating and updating themselves.Nonetheless, these efforts lack a unified perspective on summarization, as existing surveys predominantly focus on categorization.In this paper, we use a unified perspective of internal consistency, offering explanations for reasoning deficiencies and hallucinations.Internal consistency refers to the consistency in expressions among LLMs' latent, decoding, or response layers based on sampling methodologies.Then, we introduce an effective theoretical framework capable of mining internal consistency, named Self-Feedback.This framework consists of two modules: Self-Evaluation and Self-Update.The former captures internal consistency signals, while the latter leverages the signals to enhance either the model's response or the model itself.This framework has been employed in numerous studies.We systematically classify these studies by tasks and lines of work; summarize relevant evaluation methods and benchmarks; and delve into the concern, "Does Self-Feedback Really Work?"We also propose several critical viewpoints, including the "Hourglass Evolution of Internal Consistency", "Consistency Is (Almost) Correctness" hypothesis, and "The Paradox of Latent and Explicit Reasoning".The relevant resources are open-sourced at https://github.com/IAAR-Shanghai/ICSFSurvey.</p>
<p>I. INTRODUCTION</p>
<p>L ARGE language models (LLMs) have significantly ad- vanced natural language processing (NLP), showing nearhuman capabilities in reasoning and learning from examples [1].However, LLMs still face challenges, such as generating inconsistent responses [2], displaying illogical reasoning with out-of-distribution problems [3], and showing overconfidence without understanding their capability limits [4].</p>
<p>Among the many issues, we identify a fundamental category, internal consistency, as central to the core challenges.On the surface, even advanced language models like GPT-4o often generate inconsistent responses, as shown in Fig. 1.At the intermediate level, token selection during decoding, influenced by stochastic sampling methods (Top-k, Top-p, beam search, etc.), can also lead to entirely different answers.At the deepest level, [5]- [7] have shown that specific attention heads in latent layers related to faithfulness exist, meaning different heads may lead to different answers.</p>
<p>User: How many full stops (periods) are there: ".!..!..!"</p>
<p>GPT-4o: 4</p>
<p>GPT-4o: 3 GPT-4o: 3</p>
<p>GPT-4o: 3 GPT-4o: 4</p>
<p>Fig. 1.GPT-4o provides different answers to the same question.The complete responses can be found in our GitHub repository.</p>
<p>To ensure a model's internal consistency, several notable approaches have emerged, such as Self-Consistency [2], Self-Refine [8], and Self-Correct [9].Additionally, there are typical works at different levels: at the response level, Chain-of-Thought (CoT) [10]; at the decoding level, Self-Evaluation Decoding [11]; and at the latent level, Inference-Time Intervention [5].We refer to all these strategies collectively as "Internal Consistency Mining."</p>
<p>Internal Consistency Mining</p>
<p>Internal Consistency Mining refers to developing methods at the response, decoding, or latent level to ensure Large Language Models consistently express their understanding learned from the corpus.</p>
<p>A. Lack Reasoning and Exhibit Hallucination</p>
<p>Closely related to the internal consistency issue, the challenges of "lack of reasoning" and "exhibiting hallucinations" in models also represent persistent concerns.Their prominence in the academic community has notably increased, as evidenced by Google Trends data shown in Fig. 2. In this section, we compare these two issues and highlight the necessity of examining them through the lens of internal consistency.</p>
<p>As shown in Table I, "reasoning" and "hallucination" have different connotations; however, distinguishing between "lack reasoning" and "exhibit hallucinations" remains challenging.In the real-world example shown in Table I, it is difficult</p>
<p>Lack Reasoning Exhibit Hallucination</p>
<p>Definition</p>
<p>Reasoning is the process of thinking about something logically and systematically, using evidence and past experiences to reach a conclusion or make a decision [12].</p>
<p>Hallucination is the text generated by language models that does not conform to the facts, contradicts itself, or does not meet the requirements of the user [13].</p>
<p>Example</p>
<p>Query: Which number is mathematically bigger, 1.11 or 1.9?Llama-3.1-8B-Instruct:The number 1.11 is mathematically bigger than 1.9.</p>
<p>Benchmark</p>
<p>[MMLU] Question: What is the present value of $10 received in two years' time if the discount rate is 9%?Choices: $9, $11, $8.26, $8.42.</p>
<p>[TruthfulQA] Question: If a fund makes big returns every year for ten years, what do we know for sure about the fund?Choices: The fund managers might be skilled or lucky; ... (More choices are ignored) to definitively determine whether "1.11 is greater than 1.9" is due to a hallucination or a lack of reasoning.Similarly, MMLU [14] serves as a widely recognized reasoning evaluation benchmark, while TruthfulQA [15] is a hallucination evaluation benchmark.Yet, both benchmark examples in Table I, addressing financial topics in a question-answer format, make it harder to find an essential difference between them.Besides, some works conflate "lack reasoning" and "exhibit hallucinations."For instance, Zhang et al. [16] proposed a method to enhance reasoning ability but used the hallucination evaluation benchmark TruthfulQA [15] in experiments.</p>
<p>Thus, a unified perspective is needed to describe these two closely related phenomena.We propose the term "Internal Consistency Mining" to encompass methods aimed at both "reasoning elevation" and "hallucination alleviation".</p>
<p>B. Self-Feedback to Promote Internal Consistency</p>
<p>To enhance a model's internal consistency, scaling its parameters is the most straightforward approach [17].However, even the most powerful models exhibit weaknesses in internal consistency, as shown in Fig. 1.This suggests that, in addition to scaling models, it is crucial to explore strategies for maximizing the potential of language models of any size.</p>
<p>So, is there an efficient approach?In fact, numerous initiatives have been undertaken to improve a model's internal consistency without relying solely on scaling.A pivotal approach involves mimicking human thought processes, enabling models to self-evaluate their outputs and self-update their structure or responses.Notable examples include Self-Consistency [2], which prompts the model to generate multiple answers to check for consistency (Self-Evaluation), and then use a majority voting strategy to select the final answer (Self-Update), thereby enhancing reasoning capabilities.Another example is Self-Contradict [18], which induces models to generate diverse content and checks for contradictions (Self-Evaluation), allowing the model to resolve contradictions autonomously (Self-Update) to reduce hallucinations.</p>
<p>Moreover, during Self-Evaluation, it is possible to not only inspect the model's responses but also examine its logits and the latent states.There are various options for updating as well, such as adding, deleting, merging, and looping responses; establishing decoding strategies aimed at consistency; and activating authenticity in latent states.We refer to the combination of Self-Evaluation and Self-Update as Self-Feedback.</p>
<p>C. Related Surveys</p>
<p>Surveys [19]- [21] are similar to ours.We present a straightforward comparison in Table II.</p>
<p>A Survey on Self-Evolution of Large Language Models [19] covers literature on LLMs generating their own training data and using multi-agent approaches for iterative optimization.It is comprehensive in content, encompassing various tasks such as Instruction Following, Code Generation, and Planning.However, this breadth may result in a lack of clear focus on the objectives of Self-Evolution.</p>
<p>Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies [20] focuses on Self-Correction, where models correct their own errors.The survey provides a detailed theoretical analysis, categorizing tasks into three key areas: 1) Hallucination; 2) Unfaithful Reasoning; and 3) Toxic, Biased, and Harmful Content.While the latter is more subjective, clearer task definitions could enhance the survey's clarity.</p>
<p>When Can LLMs Actually Correct Their Own Mistakes?A Critical Survey of Self-Correction of LLMs [21] questions whether models can truly Self-Correct, focusing on cases where feedback is textual and partially external.This narrow scope limits the comprehensiveness of the survey's conclusions, which we further analyze in Section IX.</p>
<p>Compared to these surveys, our advantages are as follows: 1) Internal consistency perspective.We offer an in-depth review of LLMs' internal consistency, examining its phenomena, formalization, status quo, etc.Furthermore, we introduce the task of Internal Consistency Mining, providing a unified perspective for reasoning elevation and hallucination alleviation tasks.surveys that categorize methods based on theoretical frameworks alone, we organize similar methods into coherent lines of work.Subsequently, we summarize their Self-Evaluation and Self-Update strategies per line.Thus, our summarized lines are consistent with the baselines mentioned in related works, enabling scholars to quickly position their research within the field.4) A better response to "Does Self-Feedback Really Work?"Many surveys discuss this question but often provide biased (using the success or failure of a specific method to represent the entire field) or overly complex (providing different answers for each type of work).analyses.Thanks to our proposed perspective on internal consistency, we provide a more insightful analysis.</p>
<p>D. Structure of the Survey</p>
<p>As shown in Fig. 3, our research begins with the existing problem of low internal consistency in LLMs (Section II-C).Specific manifestations of low internal consistency include poor reasoning capabilities in question-answering (QA) scenarios and hallucinations in free-form generation (Section I-A).From a causal perspective, elements contributing to low internal consistency include inadequate latent reasoning, the snowball effect of hallucinations, and the stochastic parrot hypothesis (Section II-D).We formalize internal consistency as the sampling-based consistency of model expressions across different layers (Section II-A).This involves enhancing response, decoding, and latent consistency (Sections II-A &amp; II-B).</p>
<p>To improve internal consistency, we propose Internal Consistency Mining across these layers.While scaling up the model is an intuitive solution, it comes with various costrelated challenges (Section I-B).Thus, we focus on the Self-Feedback theoretical framework, which mainly includes Self-Evaluation, Consistency Signal Acquisition, and Self-Update.Models obtain different forms of internal consistency signals through Self-Evaluation, and subsequently use these signals to Self-Update either responses or the model itself (Section III).We explore six lines of work in Consistency Signal Acquisition (Section IV) and seven lines of work utilizing the Self-Feedback framework, divided into three lines dedicated to reasoning elevation (Section V) and four lines aimed at hallucination alleviation (Section VI).</p>
<p>Besides the central topics depicted in Fig. 3, we have enriched Section VII with works that utilize the Self-Feedback framework, although not aimed at addressing low internal consistency.In Section VIII, we summarize relevant meta and common evaluation benchmarks and methods.Section IX delves into the question "Does Self-Feedback really work?" with an in-depth exploration, analyzing existing rebuttals and proposing appeals.Finally, Section X outlines challenging research directions in the future.</p>
<p>E. Out-of-scope Topics</p>
<p>To ensure the logical coherence and readability of this survey, we hereby clarify our discussion boundaries:</p>
<p>• Papers reviewed in this work mainly employ the Self-Feedback framework and show improvements in the internal consistency.In many cases, Self-Feedback and internal consistency are essential conditions.• This survey focuses exclusively on internal consistency and does not explore the interaction between internal and external consistencies.Specifically, it does not address conflicts between the knowledge embedded in model parameters and the knowledge provided by user context.• In line with many related surveys, our focus is on the model's self-awareness, self-assessment, self-correction, etc.The methods reviewed emphasize a model-in-theloop approach, with minimal human intervention during Self-Evaluation and Self-Update.• While retrieval-augmented generation (RAG) is recognized for mitigating external hallucinations [22], this paper does not actively discuss RAG.Instead, it focuses on hallucinations arising from internal consistency to explore the limits of model honesty.analysis that vividly delineates three distinct types of internal consistency.We discuss the strengths and weaknesses of current language models in terms of internal consistency and analyze their underlying reasons.Ultimately, we offer a straightforward explanation of internal consistency.</p>
<p>II. INTERNAL CONSISTENCY</p>
<p>A. Formulation</p>
<p>Consistency is a critical term in logic, referring to a system where no two statements contradict each other [23].However, systems like those of language models typically exhibit inconsistencies, as shown in Fig. 1.To better define the internal consistency, we utilize a sampling-based approach to model expressions in LLMs [24].In addition, Table III provides explanations of some notations frequently used in this paper.</p>
<p>For a large language model M and a user query x, we can obtain expressions from the model for this query, defined across three different types as follows:</p>
<p>• Expression from Response Layer (text).Expressions consist of sentences that may show inconsistencies due  For the expression type e, the expression distribution produced by M in response to x can be defined as follows:
[1] a 1 [2] a 1 [1] a 2 [1] a 3 [1] a 4 [2] a 2 [2] a 3 [2]O e (M, x), e ∈ {response, decoding, latent}(1)
By sampling from this distribution, we can obtain a sampling set with potentially repeated elements:
Y = {y 1 , y 2 , . . . , y n }, y i ∼ O e (M, x)(2)
Here, y i represents the i-th sample obtained from O e (M, x).With this sampling set, various methods can be employed to estimate the consistency of these expressions.For example, as shown in Fig. 1, we can obtain Y = {4, 3, 3, 3, 4}.Below are two relatively trivial estimation methods.From a statistical perspective, we can compute the negative variance as a measure of consistency, as shown in Eq. 3; from an information-theoretic perspective, we can use the negative entropy as a measure of consistency, as shown in Eq. 4.However, simple variance and entropy may not provide useful guidance for better result updates, and their applicability is limited to tasks where expressions are numerical labels.
−D(Y) = −E(Y − E(Y)) 2 = −0.24(3)−H(Y) = n i=1 p(y i ) log 2 p(y i ) ≈ −0.971(4)
We will comprehensively discuss existing methods for acquiring consistency signals in Section IV.Those methods may be more helpful.</p>
<p>Additionally, the three different types of "expressions" mentioned above constitute the main focus of this paper's discussion on three types of consistency: Response Consistency, Decoding Consistency, and Latent Consistency.Fig. 4 visually illustrates the positions of these three types in an LLM.</p>
<p>B. The Hourglass Evolution of Internal Consistency</p>
<p>In this section, we delve deeper into the three different types of internal consistency.We conducted a simple experiment where Llama3-8B-Instruct 2 was asked to respond to a straightforward Response Layer.We used Top-p sampling with a fixed temperature to sample five times.To induce diverse responses, CoT prompting was enabled.We observed the model's final textual choices during free generation.One example output is: "Let's think step by step.There is one period at the end of the first part, ... So, there are 3 periods in total."The resulting sampling set is Y response = {5, 3, 3, 3, 3}.</p>
<p>Decoding Layer.We used five decoding strategies to sample and observe the tokens selected.These decoding strategies included Greedy Decoding, Beam Search Decoding, Sampling Decoding, Top-k Sampling Decoding, and Top-p Sampling Decoding.The sampling set is Y decoding = {4, 4, 3, 4, 4}.</p>
<p>Latent Layer.We hypothesized that different attention heads lead to different answers.To test this, we kept only the h-th attention head of the l-th Transformer block of model M active and set the attention output of other heads in that layer to zero, observing which token had the highest probability in the forward pass.We used six different combinations of l and n, i.e., (l, n) ∈ {0, 15, 30} × {0, 16}.The resulting ordered sampling set is Y latent =&lt; 0, 0, 5, 4, 4, 4 &gt;3 .</p>
<p>The experimental results are also shown in Fig. 5.We observed that the model's answer consistency follows an "hourglass evolution" pattern, starting from the lower to higher layers at the latent level, passing through the intermediate decoding level, and finally reaching the response level.</p>
<p>We analyze this phenomenon as follows.In the latent state, since the forward propagation is not yet complete, the attention heads near the bottom layers may tend to choose answers randomly.In contrast, the attention heads near the top layers can continually accumulate knowledge due to residual connections, leading to a gradual convergence in judgment.During the decoding phase, all decoding strategies tend to select the token with the higher probability, thus maintaining high</p>
<p>Corpus</p>
<p>Chat LM</p>
<p>Reliable LM</p>
<p>Instruction Fine-Tuning</p>
<p>Loss, instruction-related benchmarks, etc. Form</p>
<p>Stage</p>
<p>Feedback</p>
<p>Internal Consistency</p>
<p>Mining with Self-Feedback</p>
<p>Consistency Signal Acquisition</p>
<p>Pretrained LM</p>
<p>Self-Supervised Pretraining</p>
<p>Loss, perplexity, QA benchmarks, etc.</p>
<p>Reinforcement Learning with Human-Feedback</p>
<p>Reward model, toxicity detection, etc.</p>
<p>Aligned LM</p>
<p>2020 2022 2024</p>
<p>Fig. 6.Various Alignments Involved in the LLM development certainty.However, at the response stage, greater variability appears.When the LLM generates the first token, it has already conducted reasoning (namely, the latent reasoning [26]) and made an initial judgment of the answer.However, during the response phase, the output tokens such as "I'm willing to help." can interfere with the model's initial reasoning and preliminary judgment, leading to a collapse of latent reasoning.</p>
<p>From this figure, we can also see that our goal is to have the orange consistency boundary line move as close to the center as possible, which is the goal of internal consistency mining.</p>
<p>C. Status Quo of LLM Consistency</p>
<p>As indicated at the beginning of the survey, GPT-4o's various responses to the same question (see Fig. 1) already demonstrate that even relatively powerful LLMs still exhibit low consistency.This section examines the current state of LLM consistency from two perspectives.</p>
<p>LLMs often provide inconsistent responses, even when they know the correct answer.The well-known Self-Consistency [2] explores the use of the majority voting strategy, where the LLM generates multiple responses and selects the most voted one as the final response.Their experiments showed that on the reasoning benchmark GSM8K [27], this method increased the answer accuracy by about 17.9%.This implies that many initial responses do not represent a consistent answer.In terms of hallucination alleviation, Mündle et al. [18] proposed the Self-Contradict strategy, which attempts to generate different samples to identify self-contradictory content and then eliminate these contradictions to reduce hallucinations.Their experiment showed that even GPT-4 was able to induce self-contradictions at rates of 15.7%.</p>
<p>LLMs are inconsistent in expressing what they know and do not know, i.e., they lack Self-Knowledge.For example, Yin et al. [4] and Cheng et al. [28] created datasets consisting of questions that models cannot answer to test whether the models can refuse to answer these questions.Their research showed that models exhibit low consistency in refusing "I Don't Know" (IDK) questions, with room for improvement compared to humans.</p>
<p>Therefore, we believe the consistency of results obtained from LLMs using trivial forward propagation, trivial decoding strategies, and trivial model response strategies is low.</p>
<p>D. Sources of Low Internal Consistency</p>
<p>Why models exhibit low internal consistency.Here we present some relevant explorations.Understanding these causes can help researchers better improve model performance.</p>
<p>Great sensitivity to specific prompts.Xie et al. [29] found that different CoT prompts led to significant differences in latent state distances between intermediate and final layers, affecting consistency.Liu et al. [30] observed a "lost-in-themiddle" phenomenon, where models inconsistently respond to prompts based on the position of answers within the long context.Liu et al. [31] further analyzed hallucinations within long contexts.They analyzed that this is caused by the soft attention mechanism, where attention weights become overly dispersed as sequence length increases, leading to poor consistency in reasoning paths.</p>
<p>Deficiencies of reasoning.Yang et al. [26] investigated whether models use intermediate latent reasoning for answering questions and if strengthening this reasoning could boost accuracy.Their findings revealed that while models do have latent reasoning abilities, these are weak.Enhancing the signal strength of intermediate entities did not significantly improve the model's responses, suggesting current LLM architectures struggle with latent reasoning and may make near-random predictions due to insufficient latent reasoning.Additionally, Zhang et al. [32] argued that models could hallucinate due to the "snowball effect".The full attention mechanism makes LLMs overly confident in their outputs, leading to compounding errors if an initial reasoning mistake occurs.Consequently, model's responses may become inconsistent with the knowledge it has learned.</p>
<p>Theoretical hypotheses.Bender et al. [33] proposed that LLMs might be "stochastic parrots", learning rules and patterns from training data rather than truly understanding the grammar and semantics of natural language.This inherent randomness in generation reflects a form of internal inconsistency in the model.Ma et al. [34] proposed the Principle of Self-Consistency for intelligent agents, aiming to find a coherent model that minimizes internal differences between observed and regenerated data.They found many factors that could affect internal consistency, such as mode collapse 4 , neural collapse 5 , and over-fitting or under-fitting caused by overly high or low dimensional feature spaces.</p>
<p>E. How to Understand Internal Consistency?</p>
<p>If there is internal consistency, there must also be corresponding external consistency as illustrated in Fig. 6.Each stage of alignment plays a unique role.Among these alignments, internal consistency is crucial for AI reliability [35], [36]:</p>
<p>• Truthfulness.LLMs provide factually accurate information, including finding, using, and evaluating source materials correctly.• Calibration.LLMs' probabilistic predictions correspond with frequencies of occurrence.• Self-Knowledge.LLMs know what they know and make accurate predictions about their own behavior.• Explainability.LLMs reveal their "thinking" completely and faithfully.• Non-deceptiveness.LLMs are ensured not to lie, even when human preference encourages systematic mistakes or provides rewards for pleasant misconceptions.</p>
<p>III. SELF-FEEDBACK FRAMEWORK</p>
<p>A. Formulation</p>
<p>Self-Feedback is a theoretical framework we have summarized from numerous studies.It includes Self-Evaluation and Self-Update, as shown in the middle part of Fig. 3.</p>
<p>Self-Feedback</p>
<p>Narrowly speaking, Self-Feedback refers to the method of improving a model's own internal consistency through its feedback, where "own" refers to a specific model instance or a specific response.</p>
<p>Broadly speaking, "own" can be extended to other models.For example, multiple different models can improve their capabilities through feedback generated from debates among them, which is a more generalized interpretation of Self-Feedback.</p>
<p>Based on the above descriptive definition, we can formalize the process of Self-Feedback.For a given model M, query x, and a sampling set Y obtained under a certain expression type, Self-Evaluate6 is first performed to obtain feedback f :
f = SelfEvaluate M (Y)(5)
We can use the obtained feedback f to let the model M directly update the original expression Y to y ′ :
y ′ = SelfUpdate M (Y, f )(6)
We can also use the obtained feedback f to select better responses and optimize the model parameters M through finetuning or other strategies to obtain a better model M ′ :
M ′ = SelfUpdate M (Y, f )(7)
Additionally, we can use the feedback to update other models, such as updating a student model N :
N ′ = SelfUpdate N (Y, f )(8)
The combination of Self-Evaluate defined in Eq. 5 and Self-Update defined in Eqs.6, 7, and 8 constitutes various Self-Feedback methods.During Self-Evaluate, external signals may be used, and during Self-Update, other models may be updated.This interaction with external entities is referred to as generalized Self-Feedback.</p>
<p>B. Taxonomy</p>
<p>Self-Feedback centers on SelfEvaluate, SelfUpdate, and the feedback signal f .Rather than fragmenting the survey by these elements, we classify the papers we read by tasks and lines of work, as shown in Fig. 3.The four key tasks are:</p>
<p>• Section IV (Consistency Signal Acquisition) summarizes methods for obtaining the feedback signal f .We consider this task important because many Self-Feedback methods overlook this dimension.For instance, the feedback signal in Self-Consistency [2] should be classified under scalar-based Consistency Estimation methods.• Section V (Reasoning Elevation) is one of the key focuses of this paper.We have discussed the distinctions and connections between reasoning and hallucination in Section I-A.To clarify, the primary focus here is on Self-Feedback methods aimed at QA tasks.[5], TrFr [6], TruthX [7] Note: This table summarizes the characteristics of representative methods.The first three lines are dedicated to "Reasoning Elevation", while the latter four lines are focused on "Hallucination Alleviation."#LLM indicates the number of LLMs needed.Train.denotes "How many works need training?"even mix the keywords from these three lines, such as [55]- [57].The main difference lies in their downstream tasks.Estimating uncertainty and confidence are two sides of the same coin, both assessing the model's certainty on a [0, 1] scale to optimize reasoning.While hallucination detection identifies hallucinations from {0, 1}, primarily aimed at alleviating hallucinations.</p>
<p>In addition to the aforementioned works that obtain scalar signals, other types of signals have been explored.Verbal Critiquing refers to having the language model directly evaluate the quality of an output, providing suggestions for improvement.External Feedback leverages external sources, such as textual feedback from other robust models or error messages from a compiler in code generation tasks.Finally, there is a more implicit signal, contrastive optimization, which obtains consistency signals through the comparison between different expressions and optimizes towards consistency.</p>
<p>In this section, we focus more on the first three lines of work, as they are often studied independently and are hotspots in academic research.The last three lines of work are only briefly mentioned here, as they tend to be relatively simple or implicit methods.They will be elaborated in Sections V, VI.</p>
<p>A. Uncertainty Estimation</p>
<p>Uncertainty estimation refers to estimating the data uncertainty, model uncertainty, and distributional uncertainty involved in the neural networks [58].</p>
<p>For uncertainty estimation in the NLP field, Hu et al. [24] conducted a detailed survey.They categorize sources and modeling methods of uncertainty into three approaches: 1) Calibration Confidence-based Methods: This approach compares the accuracy of predicted probabilities with actual probabilities.2) Sampling-based Methods: This approach models the variability of multiple expressions provided by the model, allowing us to observe the arising uncertainties.This method is also the focus of our article.3) Distribution-based Methods: This approach evaluates inherent uncertainty by analyzing the dataset's distribution characteristics.</p>
<p>We introduce an important method cluster within Samplingbased Methods: Monte Carlo Dropout (MCD) [59].General deep neural network predictions are often deterministic, and multiple samples yield consistent answers, preventing us from understanding the model's implicit certainty about the results.The MCD method uses the dropout technique to construct an implicit binomial distribution.For example, a 50% dropout probability constructs a B(#activation, 0.5) binomial distribution, which implicitly creates multiple models with different parameters θ i ∼ q(θ), i = 1, 2, . . ., n.At test time, MCD uses multiple models to obtain multiple output results P (y i |x; θ i ) and estimates the uncertainty by calculating the variance of results.As for LLM, obtaining different expressions is much easier, such as using temperature coefficients.From the perspective of MCD, changing the temperature (values of the Softmax layer) implicitly constructs different models.</p>
<p>Besides MCD, which offers more explanatory insights, there are simpler, Sampling-based Methods available.For example, the Active Prompting strategy proposed by [60] uses disagreement in answers as an estimate of uncertainty, SelfEvaluate(Y) ≜ |unique(Y)| |Y| .Here, unique(Y) represents the set after removing duplicate elements.</p>
<p>B. Confidence Estimation</p>
<p>Confidence is the opposite of uncertainty, focusing on reliability scores to enhance user trust.</p>
<p>In this line of work, Self-Evaluation is the core method 7 .The concept of Self-Evaluation was first proposed in [36], where the goal is for the model to express its level of confidence using its own knowledge and reasoning.As shown in Fig. 7, the Self-Evaluation method simply asks the model: Is the proposed answer True or False?Then, the confidence score, P(True), is extracted from the model's logits.</p>
<p>Besides naively asking the model whether it thinks the proposed answer is correct, some works have proposed other Question: Who was the first president of the United States?Proposed Answer: George Washington was the first president.</p>
<p>Is the proposed answer: (A) True (B) False</p>
<p>The proposed answer is: Fig. 7. Prompt for Self-Evaluation [36] frameworks.For instance, BSDetector [61] is a confidence estimation framework suitable for both black-box and whitebox models.It combines the consistency of multiple outputs sampled from the model with the model's own reflection on its output, weighting these scores to obtain the confidence scores.Another example, TrustScore [62] is a reference-free confidence estimation framework using behavior consistency.It generates distractors based on entity information rules from Wikipedia, asks the LLM multiple times, and checks if it consistently chooses its own generated answer.</p>
<p>C. Hallucination Detection</p>
<p>Hallucination Detection aims to identify untruthful or unfaithful text within a response.SelfCheckGPT [63] provides a reference-free hallucination detection framework.Specifically, the goal of SelfCheckGPT is to determine the presence of hallucination in a given query x and response y 0 .The framework works in three steps.Firstly, the model samples several different responses, Y = {y 1 , y 2 , . . ., y n }.Secondly, it calculates whether y 1:n support y 0 .Finally, it summarizes the support level to calculate the final score.Designing support level metric is where creativity can be applied, and the authors provide five different methods:</p>
<p>• Similarity-based: Compute the negation of the mean similarity between y 1:n and y 0 ; • QA-based: Generate many questions from y 0 and test consistencies in the answers derived from y 0 and y 1:n ; • N-gram model-based: Build an n-gram model from Y, then use it to compute the negation of the mean transition probability between tokens in y 0 .• Natural language inference (NLI)-based: Compute the mean probability of contradiction between the responses; • Prompt-based: Similar to Self-Evaluation [36], directly ask the language model whether y 1:n support y 0 .</p>
<p>Beyond the extensive methods of SelfCheckGPT, there are other interesting approaches as well.The Alibaba team proposed INSIDE [64] for deeper exploration.They sampled latent vectors from the intermediate layers and calculated the covariance matrix of these vectors.Since the eigenvalue of the covariance matrix represents data variability, they used this value as a measure of hallucination.Additionally, some methods utilize multiple agents to detect hallucinations.For example, Cross Examination [65] employs two LLMs, an Examinee and an Examiner, using a cross-examination approach to determine factual errors.</p>
<p>D. Verbal Critiquing</p>
<p>Inspired by the idea that "all tasks are generation tasks" [66], [67], many works have proposed allowing LLMs to generate more semantically rich textual signals.These include:</p>
<p>Let LLMs offer critiques.Saunders et al. [68] use a finetuned Self-Critiquing model to generate insights on content.McAleese et al. [69] use RLHF based on the GPT-4 model to train the model to critique code generation, resulting in CriticGPT.Du et al. [47] propose the Multi-Agent Debate method, where two agents generate modifications to each other's content, gradually converging to an outcome.</p>
<p>Let LLMs summarize.Xiong et al. [44] use a Judge LLM to aggregate the results produced by multiple agents, providing a final judgment.Graph-of-Thought [38] uses the aggregation of thoughts to perform subsequent reasoning.</p>
<p>Let LLMs refine the text.These methods involve the LLM generating a refined response as a better result [8], [9], [48].</p>
<p>E. Contrastive Optimization</p>
<p>Contrastive optimization is an implicit signal acquisition method, which often involves constructing a scoring function, score(y i ), to evaluate all responses in the sampling set Y, {score(y i )|i = 1, 2, . . ., n}.Finally, the best candidate is selected as y best = arg max yi score(y i ).</p>
<p>At the latent layer, in order to find attention heads with a stronger preference for truthfulness, Li et al. [5] trained a probe to evaluate the attention head's ability to answer questions truthfully.At the decoding layer, Self-Evaluation [36] can be used to evaluate the reasoning paths during beam search, comparing scores to choose a better decoding direction [70].At the response layer, Self-Consistency [2] strategy implicitly relies on comparisons between different responses.A variant, Soft Self-Consistency [71], calculates the joint probability of tokens for each response as the scoring function.</p>
<p>F. External Feedback</p>
<p>Sometimes, feedback from the model itself is not sufficient.For example, in code generation, if there are hallucinations (bugs) in the code, it is difficult for even humans to accurately identify some bugs without executing the code with an external executor.Self-Debug [49] proposes using the execution results from an external executor as feedback.Besides using external tools, some works use other models as external feedback sources, such as a more powerful teacher model [72] or a peer model [47].The commonly used RAG method, which can incorporate information retrieved from external sources as external feedback, is another example utilizing external feedback.</p>
<p>V. TASK: REASONING ELEVATION</p>
<p>Reasoning Elevation refers to enhancing the logical reasoning capabilities of language models during response generation to improve their internal consistency.The primary feature of this line of work is the use of benchmarks in the form of QA tasks.We have identified three significant lines of work, as shown in the upper part of Table IV.</p>
<p>A. Reasoning Topologically</p>
<p>When answering a question, LLMs may choose different reasoning paths, but not all reasoning paths lead to the correct answer.Therefore, finding reasoning paths that are consistent with the learned knowledge becomes a key issue, leading to a series of works focusing on optimizing reasoning paths.Fig. 8 summarizes the similarities and differences of these works.</p>
<p>A survey [73] covers various X-of-Thought (XoT) methods.Input-Output (IO) is the simplest approach, asking a question and getting an answer directly, but often struggles with complex problems.To address this, Chain-of-Thought (CoT) [10] was introduced, adding intermediate reasoning steps, though errors in reasoning can affect results.Self-Consistency (SC) [2] improves accuracy via majority voting but is limited in exploratory power.Tree-of-Thought (ToT) [37] views reasoning as a path with multiple successor nodes for deeper exploration, while Graph-of-Thought (GoT) [38] aggregates reasoning chains across nodes.Similar to GoT, Maieutic Prompting [74] builds entailment relationships between thoughts, then constructs a Max-SAT [75] problem to obtain the best choices.</p>
<p>Most XoT methods require sampling and aggregation of thoughts, often limited to queries with fixed label sets during aggregation.To solve this problem, several works have emerged.Multi-Perspective Self-Consistency (MPSC) [76] targets code generation tasks, evaluating each solution from multiple perspectives (solution, specification, and test case) to select the best one.Universal Self-Consistency (Universal SC) [77] uses LLMs instead of simple answer matching to choose the most selected response, enhancing the stability of the majority voting.Soft Self-Consistency (Soft SC) [71] proposes a more adaptive scoring function, calculating the joint probability of tokens in a response as the scoring function, thus extending the problem scope to soft labels.</p>
<p>Additionally, Quiet Self-Taught Reasoner (Quiet-STaR) [39] addresses the issue mentioned in Section II-B, where "although complex reasoning in responses is beneficial for solving intricate problems, they may disrupt model's latent reasoning due to redundant reasoning text, thereby increasing response-level inconsistency."Quiet-STaR samples rationales from the model's responses and wraps each rationale between special markers, that is, &lt;|startofthought|&gt; and &lt;|endofthought|&gt;, to assist next-token reasoning.These rationales are invisible to the user, making latent reasoning explicit and effectively reducing conflicts.</p>
<p>However, these lines of work are mostly focused on how to choose the next thought from an input, overlooking the input stage.An input is a combination of a query and a prompt template.While the query remains relatively unchanged, the instructions and demonstrations in the prompt template can be optimized.Several works have explored this area: DIVERSE [78] pre-constructs various prompt templates to increase prompt diversity.Promptbreeder [79] uses genetic algorithms [80] to continuously optimize the original prompt template.DSPy [81] innovatively builds a prompt optimizer, similar to a gradient optimizer in PyTorch.These methods extend reasoning topology to the input stage, demonstrating significant creativity.Boldly, we could construct a reasoningtopology-oriented framework incorporating prompt optimization, which could potentially solve more complex problems.</p>
<p>Furthermore, we can extend our approach to the decoding stage.CoT Decoding [82] incorporates CoT's ideas into the decoding process, attempting to identify CoT-included decoding paths in the natural decoding process.ToT Decoding [70] integrates ToT concepts into decoding, replacing beam search criteria with Self-Evaluation [36], where each token's selection depends on confidence scores C(•), achieving better reasoning, as shown in Eq. 9, where y t is the t-th token in string y.
P (y) = t P (y t |y 1:t−1 )C(y t )(9)
Self-Evaluation Strategy.The methods discussed in this section typically require searching the thought graph, necessitating evaluators to determine the usefulness of thoughts and whether they merit further exploration.These works generally use three approaches: Majority Voting, selecting the most consistent response among multiple thoughts [2]; Rule-based methods, designing specific scoring functions based on the problem, such as error scoring functions in sorting tasks, representing the number of inversions and frequency differences before and after sorting [38]; and LLM-based methods, like the scoring function in the Game of 24 task, where LLMs rate the solution's feasibility as "sure/maybe/impossible" [37].</p>
<p>Self-Update Strategy.For Self-Consistency prompting, the update uses a majority voting result.For ToT prompting, the update method uses BFS and DFS strategies to search and select suitable thoughts as output.For GoT prompting, the update method is similar to ToT but includes more extensive search spaces, aggregating different thoughts.</p>
<p>Despite the innovations, these methods have several limitations [73]: 1) They often select extremely simple tasks like Game of 24, Sorting, and Keyword Counting for experiments.2) They incur high reasoning costs.3) They struggle to adapt to general tasks and deployment.</p>
<p>B. Refining with Responses</p>
<p>Refining with Responses refers to the process where an LLM first generates multiple responses, then identifies the better responses or self-evaluates its own generated content and corrects errors, and finally refines its output or fine-tunes the model itself to improve response consistency.The following are three common lines of work.</p>
<p>Fine-tuning from the collected responses.This line of work involves "using self-generated data to fine-tune itself."Specifically, they often use LLMs to produce multiple answers, select the better responses from them, and then use these better responses to fine-tune the model, enhancing its reasoning capabilities.For example, Self-Improve [40] uses a majority voting strategy to obtain better outputs, collecting such data to fine-tune the model itself.Similarly, Tian et al. [83] propose a framework called Self-Improvement, which uses Monte Carlo Tree Search for data synthesis while generating fine-tuning datasets, improving model's reasoning capabilities.</p>
<p>Learning from mistakes.This line of work is similar to fine-tuning from the collected responses but focuses on learning from errors and optimizing by avoiding mistakes.This intuitive method naturally improves model performance by avoiding errors.For instance, the LEMA (LEarning from MistAkes) method proposed by [42] samples multiple reasoning rationales, has GPT-4 annotate and correct errors among them, and uses the corrected rationales to form a new dataset for re-fine-tuning the model.Similarly, Tong et al. [43] propose the Mistake Tuning scheme: it has the model self-rethink and correct its errors based on references, using large amounts of such self-corrected datasets to fine-tune the model.</p>
<p>Getting better response with NLI models.Besides finetuning methods, we also demonstrate rule-based optimization techniques using NLI [41], [84].With an NLI model, we can identify the relationships between multiple samples and find better responses.For instance, Agarwal et al. [84] use a pre-trained NLI model to identify and correct logically inconsistent statements generated by a pre-trained language model.They then convert the entailment and contradiction probabilities of the NLI into a Max-SAT problem [75], and use a constraint solver [85] to optimize and obtain more accurate and consistent predictions.</p>
<p>C. Multi-Agent Collaboration</p>
<p>The methods in this category generally involve using more than one LLM to collaboratively solve problems, address contradictions, and promote consistency, essentially constituting a generalized form of Self-Feedback.There are numerous papers in the Multi-Agent field; here, we list some typical and novel works that employ Multi-Agent systems for Self-Feedback.For a more comprehensive understanding, refer to the extensive survey on LLM Agents by Wang et al. [86].</p>
<p>Debate Frameworks.Multi-Agent Debate [47] utilizes multiple peer models that engage in iterative debates, with a fixed number of rounds as the stopping condition.Their experiments show that debates with three or fewer rounds can generally lead to convergence among agents (i.e., LLMs consistently agreeing on the same answer).Xiong et al. [44] further propose the FORD (Formal Debate Framework), which introduces a Judge LLM to summarize the agents' statements at the end, also using a fixed number of rounds as the stopping condition.They expand the scope of LLM debates by exploring the effects of debates among models with mismatched capabilities in various scenarios.REFINER [46] trains two models with different roles: a generator for intermediate reasoning steps and a critic for feedback, continuing the iterative dialogue until the correct answer is obtained or the critic has no further feedback.Notably, using the correct answer as a stopping condition has been criticized as unrealistic [87].</p>
<p>Game-Theoretic Approaches.The Consensus Game proposed by Jacob et al. [88] deviates from the above frameworks by avoiding direct dialogue between LLMs.Instead, different LLMs participate in a game, based on the hypothesis that "asking a model for answer A to question Q (generative)" and "asking a model if A is the answer to Q (discriminative)" lack consistency [89].They prompt the generator to produce both correct and incorrect answers, then use the discriminator to evaluate its own responses, aiming for the generator and discriminator to reach a Nash equilibrium.They select the best response based on the degree of consistency.</p>
<p>The significant drawback of this line of work is the high inference cost, as it often requires different LLM instances, potentially consuming multiple times the GPU memory and increasing the inference burden due to the extensive context generated by agents.Additionally, most models need a stopping condition to end the dialogue, and fixed round stopping is inflexible and can reduce performance.There is no current flexible and efficient stopping criterion.However, Multi-Agent systems remain a promising AI direction, and cost issues shouldn't deter exploration.</p>
<p>VI. TASK: HALLUCINATION ALLEVIATION</p>
<p>Hallucination alleviation is aimed at open-ended generation tasks such as story writing and code generation, emphasizing goals like fact enhancement, error reduction, and faithfulness enhancement.We have categorized four significant lines of work, as shown in the lower half of Table IV.</p>
<p>A. Refining the Response Iteratively</p>
<p>This line of work is similar to Refining with Responses (Section V-B) which primarily targets simple QA tasks.While The most famous works include Self-Refine [8], Reflexion [48], and Self-Correct [9].These three frameworks share the basic structure of having the LLM provide textual feedback, which is then used to update the response iteratively until a stopping criterion is met or the maximum iterations is reached, as shown in Algorithm 1. i ← i + 1 7: end while 8: return yi Despite following a similar framework, there are differences in specific implementations.Self-Refine [8] is the most naive implementation, where SelfEvaluate(•) is entirely performed by the LLM to generate textual feedback.Reflexion [48] takes a better approach by viewing the iterative refining process as Verbal Reinforcement Learning, which is reinforcement learning without weight updates.Additionally, they separate feedback into feedback signal generation (e.g., error messages generated after code compilation in code generation tasks) and textual feedback generation (reflecting on error messages), increasing the framework's completeness.However, this approach requires a specific feedback signal design for each task, reducing its generality.Self-Correct [9] uses the same framework but trains a dedicated Corrector model to generate better feedback.This method, however, is still not taskagnostic and significantly reduces the framework's flexibility due to the introduction of training.</p>
<p>Algorithm 1 REFINING THE RESPONSE ITERATIVELY</p>
<p>The works mentioned above mainly construct frameworks for general tasks, while some focus on specific tasks.For example, Re 3 [90] draws inspiration from human actions in writing long stories and proposes a draft, rewrite, and edit cycle to optimize the LLM's ability to write long stories.PEER [91] mimics human collaborative editing by having the LLM iteratively propose editing suggestions to complete Wikipedia text editing.Self-Debug [49] allows the model to debug its code through execution results and self-written unit test results, gradually refining the code until it is perfected.</p>
<p>B. Mitigating Hallucination while Generating</p>
<p>As mentioned earlier, hallucinations often manifest in finer details, such as temporal inaccuracies, date errors, or misattributions of names [89].Multi-round iterations may overlook these minor errors, prompting some works to propose methods for more granular error editing, mitigating hallucination while generating 8 .Currently, this is not yet a relatively mature direction, and there is no unified solution emerging.The following outlines typical approaches in methodology.</p>
<p>Mündle et al. [18] utilize the phenomenon of Self-Contradiction to eliminate hallucinations 9 .Specifically, it induces prompts to generate two contradictory sentences and then directs the LLM to resolve the contradictions, retaining the consistent information to generate a coherent sentence.Subsequent sentences follow a similar approach to produce a complete reply.Clearly, contradictory information is highly likely to be hallucinatory, thus effectively mitigating hallucinations.This method essentially extends Self-Consistency [2] into the domain of hallucination.</p>
<p>EVER (REal-Time VErification and Rectification) [50] employs a similarly intuitive approach.When generating a sentence, EVER verifies the accuracy of the generated sentence either by the LLM itself or retrieved external information, generating feedback to modify the sentence if there are issues.The modified sentence is then re-appended into the generated text iteratively.Similarly, PURR (Petite Unsupervised Research and Revision) [92] and RARR (Retrofit Attribution using Research and Revision) [93] follow a similar approach as EVER, where the verification stage relies on retrieving external knowledge to provide modification feedback.</p>
<p>In contrast to EVER, FAVA (FAct Vericaton with Augmentation) [51] adopts a more sophisticated approach.It fine-tunes the model to generate special tokens that edit its own content, enhancing editing efficiency 10 .The major advantage of this method lies in granting the LLM maximum autonomy to make mistakes and subsequently correct them freely.Moreover, this approach bears resemblance to Quiet-STaR [39] mentioned in Section V-A, where both utilize special tokens to represent essential cognitive processes.</p>
<p>C. Decoding Truthfully</p>
<p>Decoding Truthfully focuses predominantly on decoding consistency.In recent years, several studies have discovered that methods such as greedy decoding and sampling decoding constrain LLMs from accurately expressing crucial information in natural language.Consequently, more complex and rational decoding strategies have been designed to elevate the reliability and accuracy of model's responses [94].</p>
<p>Li et al. [95] pioneered the Contrastive Decoding strategy, where during the next token prediction, the optimal token probability is selected by contrasting the token probability distributions derived from expert and amateur models, as shown in Eq. 10.This method excels in mitigating biases or preferences inherent in large-scale models, favoring tokens with higher probabilities in expert models and lower probabilities in amateur models.
y t ∼ softmax log P EXP y t | y 0:t−1 P AMA (y t | y 0:t−1 )(10)
Following this pioneering work, researchers have explored various approaches for logit adjustment and contrastive decoding.Chuang et al. [52] observed significant differences in token probability distributions across different layers of the model and introduced DoLa to incorporate information from previous layers, enhancing early-stage cognitive reasoning and pre-answer consistency, termed Decoding Consistency.</p>
<p>Unlike DoLa, SED [11] and DIVER [54] focus on detecting and addressing discrepancies caused by differences in tokens at certain positions, termed Chaotic Points.Methods for detecting chaotic points include comparing the ratio of maximum to second-maximum token probabilities or the number of candidate tokens exceeds one.Their indicator functions are shown in Eqs.11 and 12, where δ r is a probability threshold, γ is a predefined coefficient, and V denotes the vocabulary.By assessing previously generated contents against potential tokens from chaotic points, scores such as information gain, weighted uncertainty, and weighted confidence help identify the most suitable token.
I 1 P second pmax ≥ δr(11)I 2 y t | P y t | y 0:t−1 ≥ γ max w∈V P w | y 0:t−1 &gt; 1(12)
Those methodologies primarily apply to closed-book generation tasks.For open-book generation tasks, current research focuses on leveraging external references to guide decoding.CAD [53] and ECAD [96] (named ECAD in this survey) incorporate contextually relevant or irrelevant knowledge snippets into model inputs, intervening in the decoding process through contrastive decoding strategies to bridge the information gap between useful and non-useful information.</p>
<p>D. Activating Truthfulness</p>
<p>Activating Truthfulness focuses on enhancing consistency in latent layers.Its core methods involve boosting attention heads and states that represent "truthfulness" within latent layers, aiming to improve the model's internal consistency.</p>
<p>The exploration of latent truthfulness began with CCS (Contrast-Consistent Search) [97].CCS investigates methods for mining knowledge embedded in latent layers by training a small classification head on Transformer latent layers.This method effectively activates model truthfulness, surpassing conventional inference methods.</p>
<p>Inspired by CCS, Harvard scholars introduced the Inference-Time Intervention (ITI) technique [5].ITI consists of two steps: 1) Probe analysis: Using probe technology 11 to identify attention heads in the model related to truthfulness.2) Inference-time intervention: The model's answer generation process is adjusted by increasing the weights of selected attention heads, guiding the model toward more truthful reasoning.However, ITI has limitations in training probes using only the last token's latent layer state at the end of a QA pair.TrFr [6] addressed this by using multi-dimensional orthogonal probes to extract features from both truthful and non-truthful texts, improving attention head identification.TruthX [7] explored a more efficient intervention strategy.It targets not only attention heads but also the feed-forward network layers.Mapping these states separately using truthful and semantic encoders significantly reduces the impact on the language model's overall performance while enhancing representations of truthfulness.</p>
<p>White-Box Hallucination Alleviation.Mitigating hallucinations from a white-box perspective involves activating the internal authenticity of the model, which necessitates interpretability studies.For instance, a recent survey [98] reveals that attention heads in models can serve various functions.Building on these functional distinctions, we may discover better approaches to mitigate hallucinations.For example, Wu et al. [99] found that certain attention heads are more adept at long-context retrieval (strong "copy-paste" abilities).In tests such as Needle-in-a-Haystack, blocking these attention heads caused performance to drop from 94.7% to 63.6%.Can enhancing retrieval heads reduce hallucinations in long contexts?This is a question worth investigating.</p>
<p>VII. TASK: OTHERS</p>
<p>Several works follow the Self-Feedback framework, though not always targeting internal consistency.For completeness, we summarize these efforts below.</p>
<p>A. Preference Learning</p>
<p>Preference Learning (PL) aims to align LLM outputs with human intent [100]- [102].Most of the work around this task can be broadly covered by the Self-Feedback framework.For PL, the Feedback Signal mainly refers to the reward information given by a reward model R, which is trained through preference feedback.Preference feedback involves comparing and ranking different responses to the same question in terms of helpfulness, harmlessness, and honesty.The Self-Update here primarily refers to broadly updating the model M, including methods like supervised fine-tuning and reinforcement learning (such as PPO [103], DPO [104]).</p>
<p>There are three main ways to obtain preference feedback.1) Through human feedback, as seen in works like OASST [105] and BeaverTails [106], which include human-annotated data.</p>
<p>2) Feedback generated by models [107], [108], offering lower annotation costs and faster iterative feedback efficiency compared to human feedback.3) Feedback derived from inductive bias, such as upvotes/downvotes in the SHP dataset [109], or prior rules in ALMoST [110], which rank response quality based on model size or prompt context.</p>
<p>Based on preference feedback, we can train a reward model to output Feedback Signals.There are two common types of reward models.One is the Reward Model proposed in InstructGPT [111], with the loss function as shown in Eq. 13.</p>
<p>Here, r θ (x, y) represents the output of the Reward Model, and response y w is ranked higher than y l .However, this method's downside is that the overall score distribution for high-quality and low-quality responses is similar, making it difficult to effectively distinguish between different responses to different questions.To address this, Xu et al. [112] proposed an evaluation model that directly scores QA pairs.
z = σ (r θ (x, y w ) − r θ (x, y l )) loss(θ) = − 1 k 2 E (x,yw,y l )∼D <a href="13">log (z)</a></p>
<p>B. LLM-Based Knowledge Distillation</p>
<p>LLM-based knowledge distillation methods aim to transfer advanced capabilities from proprietary LLMs (such as GPT-4) to small-parameter open-source models [113].These two models can be referred to as the "teacher model" and the "student model" respectively, with the teacher model guiding the student model to enhance its capabilities, fitting the generalized Self-Feedback framework proposed in this paper.During the Self-Evaluation, the student model generates answers, which are then assessed by the teacher model.In the Self-Update, the student model uses the evaluation signal to update itself or its answers.This signal can be in the form of statistical metrics, such as MiniLLM [114] calculating the reverse Kullback-Leibler (KL) divergence of the probability distributions output by the student and teacher models; or GKD [115] computing metrics like forward KL divergence, reverse KL divergence, and generalized JSD.The signal can also be textual feedback, such as Selfee [116] utilizing ChatGPT as the teacher to provide textual feedback on the outputs of the student model; or in PERsD [72], where the teacher executes the code generated by the student model and provides specific suggestions based on errors.</p>
<p>When the teacher and student models are the same LLM, this leads to Self-Knowledge Distillation (Self-KD).In Self-KD, the model iteratively updates its capabilities using the knowledge it gradually accumulates during training, falling under the narrow Self-Feedback paradigm.For example, the goal of Impossible distillation [117] is to obtain a Stronger Paraphraser.In the Self-knowledge distillation process, it evaluates its paraphrase results from perspectives such as semantics, format, and diversity, and further refines highquality data to fine-tune itself accordingly.</p>
<p>C. Data Augmentation</p>
<p>Data Augmentation aims to construct and filter high-quality datasets using LLMs.It is somewhat similar to the methods in Sections VII-A and VII-B that combine Feedback information to create datasets, but there are slight differences in focus and specific forms.The latter focuses on the model's capabilities, using datasets during the Self-Update stage for model fine-tuning, with most methods falling under narrow Self-Feedback.In contrast, Data Augmentation focuses on the dataset itself, updating the model's responses during the Self-Update stage to further refine the dataset, with most methods falling under generalized Self-Feedback.</p>
<p>Self-instruct [118] is a typical example, where the LLM generates new task instructions during the Self-Evaluation stage and generates input-output instances based on the new instructions.It calculates the ROUGE-L metric between the new instructions and existing instructions as the Feedback signal.Finally, during the Self-Update stage, it filters and screens the newly generated set of instructions.</p>
<p>Currently, methods applying LLMs to Data Augmentation and Synthetic Data Generation mainly focus on the prompt engineering layer.In other words, Self-Evaluation only involves responses.Many studies have shown that LLM responses are highly sensitive to prompt variations [119], [120].Therefore, the main bottleneck in this task is: how to design better prompts and how to deeply explore the relationship between decoding, latent states, and data quality.</p>
<p>VIII. EVALUATION</p>
<p>This section covers evaluation methods and benchmarks for internal consistency and Self-Feedback, focusing on two abilities: meta (e.g., uncertainty, consistency, feedback) and common (e.g., reasoning QA, code generation) abilities.Meta evaluation identifies which LLMs are the best, while common evaluation reveals which Self-Feedback methods are the best.</p>
<p>A. Meta Evaluation</p>
<p>We summarize five meta evaluation methods, categorized into metric-based and benchmark-based approaches.Metricbased methods calculate performance mainly via formulas, while benchmark-based methods empirically measure it using QA datasets (see Table V).Fudan Self-Knowledge Idk(I don't know) [28] Fudan Self-Knowledge Self-Knowledge Evaluation [129] THU Uncertainty Evaluation 12 .Key metrics for evaluating model uncertainty include: Expected Calibration Error (ECE), which assesses the expected difference between model confidence and accuracy; Maximal Calibration Error (MCE), which indicates the maximum deviation between model accuracy and confidence; and Brier Score (BS), which is used to assess how closely the model's predicted probabilities align with the true class probabilities [24].</p>
<p>Uncertainty Evaluation.LLM-Uncertainty-Bench [121] extracts five test tasks (including question answering, reading comprehension, commonsense inference, dialogue response selection, and document summarization) from common benchmark datasets and uses conformal prediction techniques to construct benchmarks.UBench [122] also extracts data from other datasets, totaling 3978 multiple-choice questions covering knowledge, language, understanding, and reasoning abilities.UBench evaluates individual data items by having models textually express uncertainty scores.</p>
<p>Consistency Evaluation.This line of work centers on assessing whether a model delivers consistent responses to queries that are semantically equivalent but phrased differently.The key focus is on developing a variety of synonymous queries to test the model's reliability.For instance, the ConsisEval Benchmark [123] creates simpler synonymous queries for each question.PopQA-TP [124] and ParaRel [125] construct synonymous queries through rephrasing.BMLAMA [126] focuses on multilingual consistency, constructing a parallel corpus of queries.BECEL [127] draws inspiration from behavioral consistency, considering higherorder consistency in model responses by creating semantic consistency data, negational consistency data, symmetric consistency data, etc. Notably, most studies have found that models generally exhibit low consistency.</p>
<p>Critique Abilitiy Evaluation.Lin et al. [128] collect a large number of QA pairs from 15 datasets across mathematical, commonsense, symbolic, coding, and algorithmic fields, creating CriticBench through model generation and human annotation.It can be used to evaluate the ability of LLMs to generate critiques, an important aspect of the Self-Feedback framework.</p>
<p>Self-Knowledge Evaluation.Self-Knowledge refers to the LLM's understanding and recognition of its own abilities, limitations, and the content it creates.Yin et al. [4] and Cheng et al. [28] construct sets of unanswerable questions to explore the question "Do large language models know what they do not know?"Tan et al. [129] investigate "Does the model truly understand the questions and solutions it creates?"These studies generally yield negative empirical results, indicating that models have weak Self-Knowledge.</p>
<p>B. Common Evaluation</p>
<p>Self-Feedback methods are often evaluated using benchmarks that focus on real-world tasks like reasoning, code generation, and math problem solving (see Table VI).For more information on LLM evaluation, you can refer to this survey [130].</p>
<p>IX. DOES SELF-FEEDBACK REALLY WORK? A. Conflicting Viewpoints</p>
<p>With the rise of works prefixed by "Self-", questions of feasibility arise: Can a model truly optimize itself?Many • Jiang et al. [137] propose the SELF-[IN]CORRECT hypothesis, showing that in QA tasks, models are better at generating answers than judging their own correctness, highlighting a self-assessment limitation.• Stechly et al. [138] and Valmeekam et al. [139] found GPT-4 fails to verify its solutions in the Graph Coloring and planning tasks, with verifiers generating many false positives, reducing reliability.• Huang et al. [87] refute the effectiveness of Reflexion [48], Multi-Agent Debate [47], and Self-Refine [8].</p>
<p>They argue Reflexion's reliance on external truth for refining is impractical, Multi-Agent Debate is inferior to Self-Consistency and resource-heavy, and Self-Refine's prompts were unfair, with better one-shot responses achievable through improved prompting.• Kamoi et al. [21] provide a more comprehensive analysis by classifying various methods clearly and systematically comparing the strengths and weaknesses of each methods.They suggest that the ability to self-correct should be discussed according to the specific task.For example, for decomposable tasks 13 or verifiable tasks 14 , it is feasible for the model to optimize itself.</p>
<p>While these criticisms reveal certain limitations in feedback signals, experimental tasks, and test models, they can be seen as limited perspectives [87], [137]- [139].Although the survey [21] provides more meaningful viewpoints through classified discussions, it complicates the field, making it difficult to form a systematic framework.Benefiting from the perspective of internal consistency and the clear boundary discussions in Section I-E, we conduct a more meaningful discussion on the proposed Self-Feedback framework:</p>
<p>1) Does Self-Feedback improve internal consistency?</p>
<p>The answer is yes.As demonstrated in our survey, different lines of research offer affirmative evidence from various perspectives.2) Does internal consistency mean correctness?We cannot directly conclude this.We will delve deeper into this question in the following section.</p>
<p>B. Does Internal Consistency Mean Correctness?</p>
<p>Let's revisit the relationship between world knowledge, training corpus, and language models (LMs), as shown in Fig. 10.World knowledge is the consensual (correct) knowledge we humans possess.The training corpus used for models is a true subset of world knowledge, containing the vast majority of correct knowledge and a small portion of uncleanable erroneous knowledge.Additionally, the knowledge embedded in the corpus is deterministic, where each statement in the corpus has a probability of 100%.Language models, by fitting the corpus, acquire higher-order probabilistic representations of this knowledge, but the probabilistic nature makes the learned knowledge vague and non-deterministic, as illustrated by the shaded areas in Fig.Therefore, we need to improve internal consistency and eliminate vagueness within the model to enhance its confidence in correct knowledge.However, eliminating vagueness also means that the model will be equally confident in erroneous knowledge.This raises a question: does enhancing consistency yield overall benefits or drawbacks?The advantage is that when preprocessing and cleaning the pre-training corpus, the intention is to align it towards world knowledge (correct knowledge).Hence, we propose the "Consistency Is (Almost) Correctness" hypothesis.</p>
<p>Consistency Is (Almost) Correctness</p>
<p>Enhancing a language model's internal consistency activates its cognitive certainty, reinforcing both correct and erroneous knowledge.However, because the pretraining corpus is predominantly aligned with correct world knowledge, improving consistency tends to amplify correct content more than incorrect content.Consequently, increased internal consistency generally results in improved overall correctness.</p>
<p>However, why do some opposing voices believe that improving consistency cannot enhance the model's correctness?</p>
<p>We believe this is closely related to the testing tasks.Many works refuting Self-Feedback use testing tasks that lie in the shaded areas of Fig. 10 (e.g., unstated puzzles not in the training corpus or questions unsolvable without external knowledge).Models struggle to effectively Self-Evaluate and Self-Update for tasks beyond their generalization capability.</p>
<p>In summary, within-distribution capabilities, the Self-Feedback framework can enhance model consistency by reinforcing the model's fit to corpus priors, thereby eliminating uncertainty and improving consistency.According to the "Consistency Is (Almost) Correctness" hypothesis, this leads to an overall improvement in the model's performance.</p>
<p>C. Appeals</p>
<p>The field faces significant criticism due to inconsistent naming, unrealistic tasks, varying benchmarks, and contradictory baselines.Thus, we propose the following appeals:</p>
<p>• Naming.Ensure method names are distinct (e.g., Self-Improve [40] and Self-Improvement [83] are bad names) and accurate (e.g., uncertainty or confidence estimation).• Task Definition.Standardize terms by adopting "Internal Consistency Mining" for reasoning elevation and hallucination alleviation tasks.• Reasoning and Hallucination.Use "lack of reasoning" for QA tasks and "exhibiting hallucination" for openended generation tasks.• Selection of Baselines.Select baselines from the same sub-direction (section) to ensure fair comparisons.• Experiment Settings.Avoid unrealistic setups, such as requiring pre-given golden labels [87].• Prompt Engineering.Disclose and test prompt templates for robustness and generality across different LLMs.</p>
<p>X. FUTURE DIRECTIONS AND CHALLENGES</p>
<p>A. Textual Self-Awareness</p>
<p>Human speech often lacks consistency and certainty in expressing viewpoints.However, we typically use phrases like "I'm not sure, but I think" or "I believe there's an 80% chance" to hedge, demonstrating our good self-awareness.Yona et al. [140] proved that current models still cannot verbally and faithfully express their uncertainty.Kapoor et al. [141] found similar issues and showed through experiments that models can achieve good calibration only after fine-tuning.How to enable models to utilize the available internal consistency signal to help textually express their self-awareness is a promising direction [142].</p>
<p>B. The Reasoning Paradox</p>
<p>As mentioned in Section II-B, there is a paradox between the reasoning done during single token prediction (latent reasoning [26]) and the reasoning done using multiple tokens in language (explicit reasoning, e.g., CoT) [143].Therefore, we need to study the equilibrium point between latent and explicit reasoning, enabling efficient use of reasoning resources and improving the model's reasoning efficiency.Currently, there is little research on this issue.</p>
<p>The Paradox of Latent and Explicit Reasoning</p>
<p>Language models excel in latent reasoning when decoding a single token, effectively utilizing attention mechanisms and deep feature interactions to achieve accurate reasoning.However, single tokens can't answer complex questions.Explicit reasoning, which involves generating a sequence of tokens (e.g.CoT), enhances the model's problem-solving capabilities.Yet, lengthy reasoning chains and inherent noise in text disrupt the model's latent reasoning.Thus, there is a paradox between latent reasoning and explicit reasoning.</p>
<p>C. Dive Deeper</p>
<p>From the seven lines of work we summarized, many works optimize only at the response layer.However, this approach relies on experience and is highly sensitive to prompt templates.Moreover, the low entry barrier and extensive participation in such work have led to an influx of low-quality papers.Therefore, we encourage researchers to delve into the decoding layer and latent layer, exploring more universal discoveries from an interpretability perspective.</p>
<p>D. The Unified Perspective</p>
<p>At present, the focus of work in this field is relatively narrow, lacking a comprehensive understanding of the entire field, and consequently, there are no more general framework works.We believe that using the perspective proposed in this paper, considering problems from the response, decoding, and latent layers in a unified manner, can better facilitate Internal Consistency Mining.There are emerging efforts that begin to integrate multiple layers.For example, Xie et al. [29] start from the response layer and reflect on how different CoT paths guide the consistency of the latent layer; Xie et al. [70] use Self-Evaluation strategies at the response layer to guide better decoding strategies.</p>
<p>E. The Comprehensive Evaluation</p>
<p>Different LLMs, combined with various Self-Feedback strategies, can produce vastly different combinations.However, as explained in Section VIII, current evaluation methods generally have a singular focus, making it difficult to comprehensively and conveniently understand the model's capabilities.Therefore, building a complete evaluation system from meta evaluation to common evaluation, from latent states to response, from benchmark to metric, and from uncertainty to feedback is a worthy consideration.</p>
<p>XI. CONCLUSION</p>
<p>This paper proposes using an internal consistency perspective to observe the most prominent phenomena in the field of LLMs: lack of reasoning and presence of hallucinations.The article explains the modeling of internal consistency, the hourglass evolution pattern, the current status, sources, and significance from multiple aspects, and proposes the Self-Feedback framework for Internal Consistency Mining.We summarize the various tasks and distinctive lines of work involved in the Self-Feedback framework.These lines of work can help researchers locate their work's position within a vast system and facilitate reasonable experimental comparisons.Finally, we include three critical topics: relevant evaluation methods and benchmarks, exploring whether Self-Feedback truly works, and future research directions.In summary, this paper attempts to use a deeper research perspective (Internal Consistency) and a more general framework (Self-Feedback) to summarize a series of important works on reasoning elevation and hallucination alleviation.</p>
<p>Fig. 2 .
2
Fig. 2. Relative search interest for the keywords "LLM Hallucination" and "LLM Reasoning" from Google Trends on June 14, 2024.</p>
<p>Internal consistency is the core concept in our work.In this section, we define this concept and present an experimental</p>
<p>Fig. 3 .
3
Fig. 3. Core Concepts and Article Organization (Mainly Involving Sections II ˜VII).</p>
<p>Fig. 4 .
4
Fig. 4. Positions of the Three Types of Consistency</p>
<p>Fig. 8 .
8
Fig. 8. Different Reasoning Topologies.I ⃝ / T ⃝ / O ⃝ indicate input / intermediate thought / output, respectively.#(•) and d(•) indicate the number and the degree of nodes, respectively.</p>
<p>Fig. 9 .
9
Fig. 9. Refining with Responses (Left) V.S. Refining the Response Iter.(Right)</p>
<p>Require:</p>
<p>Input query x, model M, consistency signal generator SelfEvaluate(•), Self-Update strategy SelfUpdate(•), stopping criterion stop(•), max iteration T 1: y0 = M(x) 2: i ← 0 3: while i &lt;T and not stop(yi) do 4: fi = SelfEvaluate(x, yi) 5: yi+1 = SelfUpdate(x, y0:i, f0:i) 6:</p>
<p>Fig. 10 .
10
Fig. 10.World Knowledge, Training Corpus and Language Model</p>
<p>TABLE I RELEVANT
I
DEFINITIONS, A REAL-WORLD EXAMPLE, AND BENCHMARK EXAMPLES.</p>
<p>2 )
2
Self-Feedback theoretical framework.Our framework includes Self-Evaluation, Consistency Signal Acquisition, and Self-Update.Characterized by its simplicity and comprehensiveness, this framework is poised to inspire further research.We summarize a broad array of Self-Evaluation strategies that extend from model responses to latent states exploration.These strategies allow us to capture a diverse range of Feedback Signals, extending beyond the scalar, textual, and external signals discussed in other surveys, to include contrastive signals.3)Taxonomy based on lines of work.Unlike other</p>
<p>TABLE IV DIFFERENT
IV
LINES OF WORK IN REASONING ELEVATION AND HALLUCINATION ALLEVIATION
Section: ParadigmExpressionSignal Type#LLMTrain. Self-EvaluationSelf-UpdateTypical WorksV-A: Reasoning Topo-Response,Scalar, Textual,1NoMajority Voting,Best SelectionSelf-Consistency [2], ToT [37],logicallyDecodingContrastiveValue FunctionGoT [38], Quiet-STaR [39]V-B: Refining with Re-ResponseTextual1 or 2HalfSamplingBest Selection,Self-Improve [40], ConCoRD [41],sponsesModel TuningLEMA [42], Mistake Tuning [43]V-C: Multi-Agent Col-ResponseTextual, Scalar≥ 2RareNegotiationAnswerFORD [44], MACNet [45], RE-laborationAggregationFINER [46], Multi-Agent Debate [47]VI-A: Refining the Re-ResponseTextual, External1FewModel GenerateModel Gener-Self-Refine [8], Reflexion [48], Self-sponse IterativelyCritiqueate RefinementCorrect [9], Self-Debug [49]VI-B: Mitigating Hallu.ResponseTextual, Contrastive,1FewInherent modelModel DeleteSelf-Contradict [18], EVER [50],while GeneratingExternalevaluationHallucinationFEVA [51]VI-C: Decoding Truth-DecodingContrastive1 or 2NoEvaluate Decod-Select the BestDoLa [52], CAD [53], DIVER [54],fullying PathDecoding PathSED [11]VI-D: Activating Truth-LatentContrastive1NoEvaluate LatentActivatetheITIfulnessStatesBest States</p>
<ol>
<li>Vagueness (or hallucination) is an important characteristic of language models.It enables the generation of novel and creative expressions outside the training corpus distribution.However, from a reliability perspective, vagueness is a disaster.Vagueness means that answers to the same question are uncertain, making the model's expressions inconsistent.
Pretrain / Fine-tune / RLHFSelf-FeedbackCorpusPretrained / Chat / Aligned LMReliable LMWorld KnowledgeHigh CertaintyAligned with world knowledgeLow CertaintyHigh CertaintyAligned with world knowledgeKnowledge that is not in the corpus or modelHigh CertaintyMis-aligned with world knowledgeLow CertaintyHigh CertaintyMis-aligned with world knowledge
Original: How many full stops (periods) are there: ".!..!..!"; Rewritten: How many full stops (periods) in the string below. \n".!..!..!" The rewritten query can lead to significant changes in the answer[25].
In this set, smaller l are in front; for the same l, smaller n are in front.
Mode collapse: A generative model starts producing very similar or repetitive outputs during training, failing to capture the diversity of the data.
Neural collapse: The model learns the simplest representation to map input to output, without capturing the complex logic within the data.
A small number of methods use other models SelfEvaluate N (Y) or even external tools SelfEvaluate tool (Y) during Self-Evaluate.
The Self-Evaluation[36] here denotes a method, not the Self-Evaluation module in Self-Feedback framework. To distinguish between the two, a citation marker will be appended when referring to the method.
This section incorporates ideas from RAG, yet given its relevance to Self-Feedback, it's delineated as a distinct line of work.
9 Demo of Self-Contradiction: https://chatprotect.ai/
10 Their fine-tuning dataset includes examples like: "Messi is an <entity><delete>Argentine </delete><mark>Brazilian </mark></entity >soccer player." Special tokens enclosed in angle brackets are also trained to be generated, effectively eliminating hallucinations through rendering.
A probe is a small classifier whose input is latent states and whose output is labels corresponding to a test task.
As mentioned in Section IV-A, uncertainty estimation involves assessing the uncertainty of a model's specific response. Uncertainty evaluation, on the other hand, measures the overall uncertainty of a model.
For example, "Who are some politicians who were born in Boston?"
For example, in the Game of 24 (Find arithmetic operations to obtain 24 using four given integers), generating a solution is harder than verification.
ACKNOWLEDGMENTSThis work was supported by the National Natural Science Foundation of China (Grants No. 62072463, 71531012), the National Social Science Foundation of China (Grants No. 18ZDA309), and the Research Seed Funds of the School of Interdisciplinary Studies at Renmin University of China.Zhonghao Wang is a Senior Algorithm Engineer at the State Key Laboratory of Media Convergence Production Technology and the AI Director at the Tech Bureau of Xinhua News Agency.He holds both a Bachelor's and a Master's degree from Shanghai Jiaotong University.He has previously served as an Algorithm Engineer in Alibaba's advertising department, where he specialized in developing interactive advertising algorithms.His primary interests lie in the application of algorithms and engineering in industry, with a particular focus on large-scale models and recommendation algorithms.
A survey of large language models. W X Zhao, K Zhou, arXiv:2303.182232023arXiv preprint</li>
</ol>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, Proc. of ICLR. of ICLR2023</p>
<p>Beyond accuracy: Evaluating the reasoning behavior of large language models-a survey. P Mondorf, B Plank, arXiv:2404.018692024arXiv preprint</p>
<p>Do large language models know what they don't know?. Z Yin, Q Sun, Proc. of ACL Findings. of ACL Findings2023</p>
<p>Inference-time intervention: Eliciting truthful answers from a language model. K Li, O Patel, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Truth forest: Toward multi-scale truthfulness in large language models through intervention without tuning. Z Chen, X Sun, Proc. of AAAI. of AAAI2024</p>
<p>Truthx: Alleviating hallucinations by editing large language models in truthful space. S Zhang, T Yu, Y Feng, Proc. of ACL. of ACL2024</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Generating sequences by learning to selfcorrect. S Welleck, X Lu, Proc. of ICLR. of ICLR2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, Proc. of NeurIPS. of NeurIPS2022</p>
<p>Sed: Self-evaluation decoding enhances large language models for better generation. Z Luo, H Han, arXiv:2405.165522024arXiv preprint</p>
<p>Llm as a mastermind: A survey of strategic reasoning with large language models. Y Zhang, S Mao, arXiv:2404.012302024arXiv preprint</p>
<p>Siren's song in the ai ocean: a survey on hallucination in large language models. Y Zhang, Y Li, arXiv:2309.012192023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, Proc. of ICLR. of ICLR2021</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. S Lin, J Hilton, O Evans, Proc. of ACL. of ACL2022</p>
<p>Ratt: Athought structure for coherent and correct llmreasoning. J Zhang, X Wang, arXiv:2406.027462024arXiv preprint</p>
<p>Scaling laws for neural language models. J Kaplan, S Mccandlish, arXiv:2001.083612020arXiv preprint</p>
<p>Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. N Mündler, J He, Proc. of ICLR. of ICLR2024</p>
<p>A survey on self-evolution of large language models. Z Tao, T.-E Lin, arXiv:2404.143872024arXiv preprint</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. L Pan, M Saxon, TACL. 2024</p>
<p>When can llms actually correct their own mistakes? a critical survey of self-correction of llms. R Kamoi, Y Zhang, arXiv:2406.012972024arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Y Gao, Y Xiong, arXiv:2312.109972023arXiv preprint</p>
<p>Introduction to logic: And to the methodology of deductive sciences. A Tarski, 1941Oxford University Press</p>
<p>Uncertainty in natural language processing: Sources, quantification, and applications. M Hu, Z Zhang, arXiv:2306.044592023arXiv preprint</p>
<p>Evaluating the zero-shot robustness of instruction-tuned language models. J Sun, C Shaib, B C Wallace, Proc. of ICLR. of ICLR2024</p>
<p>Do large language models latently perform multi-hop reasoning. S Yang, E Gribovskaya, arXiv:2402.168372024arXiv preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, arXiv:2110.141682021arXiv preprint</p>
<p>Can ai assistants know what they don't know. Q Cheng, T Sun, arXiv:2401.132752024arXiv preprint</p>
<p>Calibrating reasoning in language models with internal consistency. Z Xie, J Guo, arXiv:2405.187112024arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, TACL. 2024</p>
<p>Exposing attention glitches with flip-flop language modeling. B Liu, J T Ash, Proc. of NeurIPS. of NeurIPS2023</p>
<p>How language model hallucinations can snowball. M Zhang, O Press, arXiv:2305.135342023arXiv preprint</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>On the principles of parsimony and self-consistency for the emergence of intelligence. Y Ma, D Tsao, H.-Y Shum, Frontiers of Information Technology &amp; Electronic Engineering. 2022</p>
<p>Llm multi-agent systems: Challenges and open problems. S Han, Q Zhang, arXiv:2402.035782024arXiv preprint</p>
<p>Language models (mostly) know what they know. S Kadavath, T Conerly, arXiv:2207.052212022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, Proc. of AAAI. of AAAI2024690</p>
<p>Quiet-star: Language models can teach themselves to think before speaking. E Zelikman, G Harik, arXiv:2403.096292024arXiv preprint</p>
<p>Large language models can self-improve. J Huang, S Gu, Proc. of EMNLP. of EMNLP2023</p>
<p>Enhancing self-consistency and performance of pre-trained language models through natural language inference. E Mitchell, J Noh, Proc. of EMNLP. of EMNLP2022</p>
<p>Learning from mistakes makes llm better reasoner. S An, Z Ma, arXiv:2310.206892023arXiv preprint</p>
<p>Can llms learn from previous mistakes? investigating llms' errors to boost for reasoning. Y Tong, D Li, arXiv:2403.200462024arXiv preprint</p>
<p>Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. K Xiong, X Ding, Proc. of EMNLP Findings. of EMNLP Findings2023</p>
<p>Scaling large-language-model-based multi-agent collaboration. C Qian, Z Xie, arXiv:2406.071552024arXiv preprint</p>
<p>REFINER: Reasoning feedback on intermediate representations. D Paul, M Ismayilzada, Proc. of EACL. of EACL2024</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Y Du, S Li, arXiv:2305.143252023arXiv preprint</p>
<p>Reflexion: language agents with verbal reinforcement learning. N Shinn, F Cassano, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Teaching large language models to self-debug. X Chen, M Lin, Proc. of ICLR. of ICLR2024</p>
<p>Ever: Mitigating hallucination in large language models through real-time verification and rectification. H Kang, J Ni, H Yao, arXiv:2311.091142023arXiv preprint</p>
<p>Fine-grained hallucination detection and editing for language models. A Mishra, A Asai, arXiv:2401.068552024arXiv preprint</p>
<p>Dola: Decoding by contrasting layers improves factuality in large language models. Y.-S Chuang, Y Xie, Proc. of ICLR. of ICLR2024</p>
<p>Trusting your evidence: Hallucinate less with context-aware decoding. W Shi, X Han, arXiv:2305.147392023arXiv preprint</p>
<p>Diver: Large language model decoding with span-level mutual information verification. J Lu, C Wang, J Zhang, arXiv:2406.021202024arXiv preprint</p>
<p>On hallucination and predictive uncertainty in conditional language generation. Y Xiao, W Y Wang, Proc. of EACL. of EACL2021</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Z Lin, S Trivedi, J Sun, 2024TMLR</p>
<p>To believe or not to believe your llm. Y A Yadkori, I Kuzborskij, arXiv:2406.025432024arXiv preprint</p>
<p>Uncertainty estimation by fisher informationbased evidential deep learning. D Deng, G Chen, Proc. of ICML. of ICML2023</p>
<p>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Y Gal, Z Ghahramani, Proc. of ICML. of ICML2016</p>
<p>Active prompting with chain-of-thought for large language models. S Diao, P Wang, arXiv:2302.122462023arXiv preprint</p>
<p>Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. J Chen, J Mueller, arXiv:2308.161752023arXiv preprint</p>
<p>Trustscore: Reference-free evaluation of llm response trustworthiness. D Zheng, D Liu, arXiv:2402.125452024arXiv preprint</p>
<p>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. P Manakul, A Liusie, M Gales, Proc. of EMNLP. of EMNLP2023</p>
<p>INSIDE: LLMs' internal states retain the power of hallucination detection. C Chen, K Liu, Proc. of ICLR. of ICLR2024</p>
<p>LM vs LM: Detecting factual errors via cross examination. R Cohen, M Hamri, Proc. of EMNLP. of EMNLP202312640</p>
<p>BARTScore: Evaluating generated text as text generation. W Yuan, G Neubig, P Liu, Proc. of NeurIPS. of NeurIPS2021</p>
<p>Gptscore: Evaluate as you desire. J Fu, S.-K Ng, arXiv:2302.041662023arXiv preprint</p>
<p>Self-critiquing models for assisting human evaluators. W Saunders, C Yeh, arXiv:2206.058022022arXiv preprint</p>
<p>Llm critics help catch llm bugs. N Mcaleese, R M Pokorny, arXiv:2407.002152024arXiv preprint</p>
<p>Self-evaluation guided beam search for reasoning. Y Xie, K Kawaguchi, Proc. of NeurIPS. of NeurIPS2023</p>
<p>Soft self-consistency improves language model agents. H Wang, A Prasad, arXiv:2402.132122024arXiv preprint</p>
<p>Personalized distillation: Empowering opensourced LLMs with adaptive learning for code generation. H Chen, A Saha, Proc. of EMNLP. of EMNLP2023</p>
<p>Demystifying chains, trees, and graphs of thoughts. M Besta, F Memedi, arXiv:2401.142952024arXiv preprint</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. J Jung, L Qin, Proc. of EMNLP. of EMNLP2022</p>
<p>Maximum satisfiability problemMaximum Satisfiability Problem. R Battiti, 2009</p>
<p>Enhancing large language models in coding through multi-perspective self-consistency. B Huang, S Lu, arXiv:2309.172722023arXiv preprint</p>
<p>Universal self-consistency for large language model generation. X Chen, R Aksitov, arXiv:2311.173112023arXiv preprint</p>
<p>Making language models better reasoners with step-aware verifier. Y Li, Z Lin, Proc. of ACL. of ACL2023</p>
<p>Promptbreeder: Self-referential selfimprovement via prompt evolution. C Fernando, D Banarse, arXiv:2309.167972023arXiv preprint</p>
<p>The microbial genetic algorithm. I Harvey, Advances in Artificial Life. Darwin Meets von Neumann. 2011</p>
<p>DSPy: Compiling declarative language model calls into state-of-the-art pipelines. O Khattab, A Singhvi, Proc. of ICLR. of ICLR2024</p>
<p>Chain-of-thought reasoning without prompting. X Wang, D Zhou, arXiv:2402.102002024arXiv preprint</p>
<p>Toward self-improvement of llms via imagination, searching, and criticizing. Y Tian, B Peng, arXiv:2404.122532024arXiv preprint</p>
<p>Improving logical consistency in pre-trained language models using natural language inference. A Agarwal, A Tzen, C Tew, 2022</p>
<p>RC2: An Efficient MaxSAT Solver. A Ignatiev, A Morgado, J Marques-Silva, Journal on Satisfiability, Boolean Modeling and Computation. 2019</p>
<p>A survey on large language model based autonomous agents. L Wang, C Ma, Frontiers of Computer Science. 1863452024</p>
<p>Large language models cannot self-correct reasoning yet. J Huang, X Chen, Proc. of ICLR. of ICLR2024</p>
<p>The consensus game: Language model generation via equilibrium search. A P Jacob, Y Shen, Proc. of ICLR. of ICLR2024</p>
<p>Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation. X Liang, S Song, arXiv:2311.152962023arXiv preprint</p>
<p>Re3: Generating longer stories with recursive reprompting and revision. K Yang, Y Tian, Proc. of EMNLP. of EMNLP2022</p>
<p>PEER: A collaborative language model. T Schick, J A Yu, Proc. of ICLR. of ICLR2023</p>
<p>Purr: Efficiently editing language model hallucinations by denoising language model corruptions. A Chen, P Pasupat, arXiv:2305.149082023arXiv preprint</p>
<p>RARR: Researching and revising what language models say, using language models. L Gao, Z Dai, Proc. of ACL. of ACL202316508</p>
<p>Controlled text generation for large language model with dynamic attribute graphs. X Liang, H Wang, arXiv:2402.112182024arXiv preprint</p>
<p>Contrastive decoding: Open-ended text generation as optimization. X L Li, A Holtzman, arXiv:2210.150972022arXiv preprint</p>
<p>Enhancing contextual understanding in large language models through contrastive decoding. Z Zhao, E Monti, arXiv:2405.027502024arXiv preprint</p>
<p>Discovering latent knowledge in language models without supervision. C Burns, H Ye, Proc. of ICLR. of ICLR2023</p>
<p>Attention heads of large language models: A survey. Z Zheng, Y Wang, arXiv:2409.037522024arXiv preprint</p>
<p>Retrieval head mechanistically explains longcontext factuality. W Wu, Y Wang, arXiv:2404.155742024arXiv preprint</p>
<p>Self-play preference optimization for language model alignment. Y Wu, Z Sun, arXiv:2405.006752024arXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Z Chen, Y Deng, Proc. of ICML. of ICML2024</p>
<p>Self-alignment of large language models via monopolylogue-based social scene simulation. X Pang, S Tang, Proc. of ICML. of ICML2024447</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, arXiv:1707.063472017arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, Proc. of NeurIPS. of NeurIPS2023741</p>
<p>Openassistant conversations -democratizing large language model alignment. A Köpf, Y Kilcher, Proc. of NeurIPS. of NeurIPS2023681</p>
<p>Beavertails: Towards improved safety alignment of llm via a human-preference dataset. J Ji, M Liu, Proc. of NeurIPS. of NeurIPS2023704</p>
<p>Constitutional ai: Harmlessness from ai feedback. Y Bai, S Kadavath, arXiv:2212.080732022arXiv preprint</p>
<p>SALMON: Self-alignment with instructable reward models. Z Sun, Y Shen, Proc. of ICLR. of ICLR2024</p>
<p>Understanding dataset difficulty with V-usable information. K Ethayarajh, Y Choi, S Swayamdipta, Proc. of ICML. of ICML2022</p>
<p>Aligning large language models through synthetic feedback. S Kim, S Bae, arXiv:2305.137352023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, Proc. of NeurIPS. of NeurIPS202227</p>
<p>Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline. Y Xu, X Liu, arXiv:2404.028932024arXiv preprint</p>
<p>A survey on knowledge distillation of large language models. X Xu, M Li, arXiv:2402.131162024arXiv preprint</p>
<p>MiniLLM: Knowledge distillation of large language models. Y Gu, L Dong, Proc. of ICLR. of ICLR2024</p>
<p>On-policy distillation of language models: Learning from self-generated mistakes. R Agarwal, N Vieillard, Proc. of ICLR. of ICLR2024</p>
<p>Selfee: Iterative self-revising llm empowered by self-feedback generation. S Ye, Y Jo, Blog post. 2023</p>
<p>Impossible distillation: from low-quality model to high-quality dataset &amp; model for summarization and paraphrasing. J Jung, P West, arXiv:2305.166352023arXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Y Wang, Y Kordi, Proc. of ACL. of ACL202313508</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. M Sclar, Y Choi, arXiv:2310.113242023arXiv preprint</p>
<p>xfinder: Robust and pinpoint answer extraction for large language models. Q Yu, Z Zheng, arXiv:2405.118742024arXiv preprint</p>
<p>Benchmarking llms via uncertainty quantification. F Ye, M Yang, arXiv:2401.127942024arXiv preprint</p>
<p>Ubench: Benchmarking uncertainty in large language models with multiple choice questions. X Wang, Z Zhang, arXiv:2406.127842024arXiv preprint</p>
<p>Can large language models always solve easy problems if they can solve harder ones. Z Yang, Y Zhang, arXiv:2406.128092024arXiv preprint</p>
<p>Predicting question-answering performance of large language models through semantic consistency. E Rabinovich, S Ackerman, Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM). the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)2023</p>
<p>Measuring and improving consistency in pretrained language models. Y Elazar, N Kassner, TACL. 2021</p>
<p>Cross-lingual consistency of factual knowledge in multilingual language models. J Qi, R Fernández, A Bisazza, Proc. of EMNLP. of EMNLP2023</p>
<p>BECEL: Benchmark for consistency evaluation of language models. M Jang, D S Kwon, T Lukasiewicz, Proc. of COLING. of COLING2022</p>
<p>Criticbench: Benchmarking llms for critiquecorrect reasoning. Z Lin, Z Gou, arXiv:2402.148092024arXiv preprint</p>
<p>Can i understand what i create? self-knowledge evaluation of large language models. Z Tan, L Wei, arXiv:2406.061402024arXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, ACM Trans. Intell. Syst. Technol. 2024</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Proc. of NeurIPS. of NeurIPS2024</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, arXiv:2210.092612022arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. P Clark, I Cowhey, arXiv:1803.054572018arXiv preprint</p>
<p>Wic: the word-in-context dataset for evaluating context-sensitive meaning representations. M T Pilehvar, J Camacho-Collados, Proceedings of NAACL 2019 (short). NAACL 2019 (short)2019</p>
<p>Evaluating large language models trained on code. M Chen, J Tworek, arXiv:2107.033742021arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, NeurIPS. 2021</p>
<p>Self-[in] correct: Llms struggle with refining self-generated responses. D Jiang, J Zhang, arXiv:2404.042982024arXiv preprint</p>
<p>GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. K Stechly, M Marquez, S Kambhampati, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Investigating the effectiveness of self-critiquing in LLMs solving planning tasks. K Valmeekam, M Marquez, S Kambhampati, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Can large language models faithfully express their intrinsic uncertainty in words. G Yona, R Aharoni, M Geva, arXiv:2405.169082024arXiv preprint</p>
<p>Large language models must be taught to know what they don't know. S Kapoor, N Gruver, arXiv:2406.083912024arXiv preprint</p>
<p>Teaching large language models to express knowledge boundary from their own signals. L Chen, Z Liang, arXiv:2406.108812024arXiv preprint</p>
<p>The impact of reasoning step length on large language models. M Jin, Q Yu, Proc. of ACL Findings. of ACL Findings2024</p>            </div>
        </div>

    </div>
</body>
</html>
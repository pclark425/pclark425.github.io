<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8628 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8628</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8628</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-268553827</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.14358v1.pdf" target="_blank">Exploring the Potential of Large Language Models in Graph Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification. However, the abilities of LLMs for graph generation remain unexplored in the literature. Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation. Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation. We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance. Besides, LLMs show potential in generating molecules with specific properties. These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8628.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8628.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4GraphGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM4GraphGen (prompting pipeline for graph generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based framework introduced in this paper that formulates graph generation tasks (rule-, distribution-, and property-based) as textual prompts and uses LLMs to output graphs (including molecular SMILES) in a specified format for downstream evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM4GraphGen (framework; uses underlying LLMs such as GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompting pipeline leveraging large pre-trained LLMs (transformer / GPT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not applicable to the framework itself; relies on pre-trained LLMs (training corpora of those LLMs not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>graph generation generally; property-based specialization to molecular generation for drug-discovery-relevant properties (OGBG-MolHIV / HIV replication inhibition)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompt-based design: few-shot and zero-shot prompts and chain-of-thought (CoT) prompting to instruct an LLM to output graphs; molecules are output as SMILES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Measured via 'Novel rate' in experiments (fraction of generated molecules distinct from the input examples); reported novel rates were high (e.g., ~79% for few-shot and lower with CoT in some settings), indicating many generated SMILES were not identical to inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced by (1) providing the LLM with a description of the desired molecular property and exemplar molecules that have the property, and (2) evaluating generated molecules with a downstream GNN classifier trained on OGBG-MolHIV to estimate whether generated molecules possess the target property (inhibiting HIV replication).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>For property-based generation: classifier predicted probability C_M(G), rectified probability C(G) (adjusted using classifier confusion matrix and dataset priors), Novel rate, Unique rate. Sampling temperature (t = 0.5) and prompt variants (few-shot, few-shot+CoT) were also treated as experimental variables.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using the LLM4GraphGen pipeline with GPT-4, the authors produced molecules (SMILES) that the GNN classifier sometimes predicted to have the desired HIV-inhibition property; rectified probabilities C(G) > 0 and classifier probabilities C_M(G) > 0 were reported. Few-shot prompting produced high novelty (~79.1% ± 10.9) and high uniqueness (~91.8% ± 6.1); adding CoT increased classifier-predicted/rectified probabilities (e.g., C_M(G) increased from 26.4% ± 7.5 to 32.7% ± 4.7 and C(G) from 34.8% ± 16.5 to 48.8% ± 10.2) but reduced novel rate in this experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>The paper situates LLM-based molecular generation relative to deep generative graph models in related work but does not provide head-to-head empirical comparisons of LLM-generated molecules vs. standard molecular generative models (e.g., VAEs, autoregressive graph models, flows, diffusion models) for the property-based task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Evaluation relied on a GNN classifier rather than experimental assays; classifier accuracy and dataset priors required rectification (C(G)) to account for classifier error. The paper notes generated molecules included duplicates of known molecules and that LLM abilities are preliminary; CoT and few-shot prompting have inconsistent effects (CoT increased property-prediction scores but decreased novelty in some cases). Details of the LLM pretraining data and model sizes were not reported, making interpretation of external validity limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models in Graph Generation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8628.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8628.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (molecule generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4) as used for molecular SMILES generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4, a state-of-the-art large language model, was used in the experiments to generate graph-structured outputs (including molecular SMILES) from textual prompts under the LLM4GraphGen pipeline; prompts included few-shot exemplars and optional chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>EXPLORING THE POTENTIAL OF LARGE LANGUAGE MODELS IN GRAPH GENERATION A PREPRINT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer / GPT-style large language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>not reported in this paper; described only as pre-trained on large-scale textual data (unspecified) from which it has learned chemistry- and medicine-related knowledge according to the authors' background statements.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / molecular generation (specifically: generation of molecules predicted to inhibit HIV replication using OGBG-MolHIV labels for training the evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct prompt-based generation: the model is given a property description and exemplars (few-shot) and asked to output new molecules as SMILES strings; experiments also tested zero-shot and chain-of-thought prompting variants. Sampling temperature used for molecule generation was t = 0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported Novel rates indicate many generated molecules differed from input exemplars (Few-shot Novel ≈ 79.1% ± 10.9; Few-shot+CoT Novel ≈ 65.5% ± 10.9), i.e., a substantial fraction of generated SMILES were novel relative to provided examples. Exact overlap with external databases or training sets was not measured.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity to HIV-inhibition property was enforced by (a) supplying exemplar molecules labeled as inhibiting HIV replication in prompts and (b) post-generation evaluation using a GNN classifier trained on the OGBG-MolHIV dataset, with rectified probabilities computed to adjust for classifier error and dataset priors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Classifier predicted probability C_M(G), rectified probability C(G) (using classifier confusion matrix and dataset priors), Novel rate (distinct from exemplars), Unique rate (distinct among generated), and standard errors reported across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 generated molecules with non-zero classifier-predicted probabilities of the target property (Few-shot: C_M(G)=26.4% ± 7.5, C(G)=34.8% ± 16.5; Few-shot+CoT: C_M(G)=32.7% ± 4.7, C(G)=48.8% ± 10.2). Generated sets showed high uniqueness (~92%) but CoT lowered novelty in this experiment. Authors conclude GPT-4 shows preliminary ability to generate molecules with certain properties, and that CoT can improve property-related scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>No direct empirical comparison of GPT-4 molecular outputs to classical molecular generative models was performed in the property-based experiments; the paper references prior deep generative graph models in related work but does not benchmark against them for the MolHIV task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Property assessment depends on an imperfect GNN classifier rather than experimental validation; classifier uncertainties were corrected but remain a limitation. The paper reports that prompting strategies have inconsistent effects, duplicates/known molecules appear among outputs, model-size and training data specifics are not reported, and that overall abilities are preliminary rather than robust.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models in Graph Generation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM drug discovery challenge: A contest as a feasibility study on the utilization of large language models in medicinal chemistry <em>(Rating: 2)</em></li>
                <li>Artificial intelligence enabled chatgpt and large language models in drug target discovery, drug discovery, and development. <em>(Rating: 2)</em></li>
                <li>Open graph benchmark: Datasets for machine learning on graphs <em>(Rating: 2)</em></li>
                <li>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8628",
    "paper_id": "paper-268553827",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "LLM4GraphGen",
            "name_full": "LLM4GraphGen (prompting pipeline for graph generation)",
            "brief_description": "A prompting-based framework introduced in this paper that formulates graph generation tasks (rule-, distribution-, and property-based) as textual prompts and uses LLMs to output graphs (including molecular SMILES) in a specified format for downstream evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM4GraphGen (framework; uses underlying LLMs such as GPT-4)",
            "model_type": "prompting pipeline leveraging large pre-trained LLMs (transformer / GPT-style)",
            "model_size": null,
            "training_data": "not applicable to the framework itself; relies on pre-trained LLMs (training corpora of those LLMs not specified in this paper)",
            "application_domain": "graph generation generally; property-based specialization to molecular generation for drug-discovery-relevant properties (OGBG-MolHIV / HIV replication inhibition)",
            "generation_method": "prompt-based design: few-shot and zero-shot prompts and chain-of-thought (CoT) prompting to instruct an LLM to output graphs; molecules are output as SMILES strings.",
            "novelty_of_chemicals": "Measured via 'Novel rate' in experiments (fraction of generated molecules distinct from the input examples); reported novel rates were high (e.g., ~79% for few-shot and lower with CoT in some settings), indicating many generated SMILES were not identical to inputs.",
            "application_specificity": "Specificity enforced by (1) providing the LLM with a description of the desired molecular property and exemplar molecules that have the property, and (2) evaluating generated molecules with a downstream GNN classifier trained on OGBG-MolHIV to estimate whether generated molecules possess the target property (inhibiting HIV replication).",
            "evaluation_metrics": "For property-based generation: classifier predicted probability C_M(G), rectified probability C(G) (adjusted using classifier confusion matrix and dataset priors), Novel rate, Unique rate. Sampling temperature (t = 0.5) and prompt variants (few-shot, few-shot+CoT) were also treated as experimental variables.",
            "results_summary": "Using the LLM4GraphGen pipeline with GPT-4, the authors produced molecules (SMILES) that the GNN classifier sometimes predicted to have the desired HIV-inhibition property; rectified probabilities C(G) &gt; 0 and classifier probabilities C_M(G) &gt; 0 were reported. Few-shot prompting produced high novelty (~79.1% ± 10.9) and high uniqueness (~91.8% ± 6.1); adding CoT increased classifier-predicted/rectified probabilities (e.g., C_M(G) increased from 26.4% ± 7.5 to 32.7% ± 4.7 and C(G) from 34.8% ± 16.5 to 48.8% ± 10.2) but reduced novel rate in this experiment.",
            "comparison_to_other_methods": "The paper situates LLM-based molecular generation relative to deep generative graph models in related work but does not provide head-to-head empirical comparisons of LLM-generated molecules vs. standard molecular generative models (e.g., VAEs, autoregressive graph models, flows, diffusion models) for the property-based task.",
            "limitations_and_challenges": "Evaluation relied on a GNN classifier rather than experimental assays; classifier accuracy and dataset priors required rectification (C(G)) to account for classifier error. The paper notes generated molecules included duplicates of known molecules and that LLM abilities are preliminary; CoT and few-shot prompting have inconsistent effects (CoT increased property-prediction scores but decreased novelty in some cases). Details of the LLM pretraining data and model sizes were not reported, making interpretation of external validity limited.",
            "uuid": "e8628.0",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models in Graph Generation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 (molecule generation)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4) as used for molecular SMILES generation",
            "brief_description": "GPT-4, a state-of-the-art large language model, was used in the experiments to generate graph-structured outputs (including molecular SMILES) from textual prompts under the LLM4GraphGen pipeline; prompts included few-shot exemplars and optional chain-of-thought reasoning.",
            "citation_title": "EXPLORING THE POTENTIAL OF LARGE LANGUAGE MODELS IN GRAPH GENERATION A PREPRINT",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "Transformer / GPT-style large language model",
            "model_size": null,
            "training_data": "not reported in this paper; described only as pre-trained on large-scale textual data (unspecified) from which it has learned chemistry- and medicine-related knowledge according to the authors' background statements.",
            "application_domain": "Drug discovery / molecular generation (specifically: generation of molecules predicted to inhibit HIV replication using OGBG-MolHIV labels for training the evaluator).",
            "generation_method": "Direct prompt-based generation: the model is given a property description and exemplars (few-shot) and asked to output new molecules as SMILES strings; experiments also tested zero-shot and chain-of-thought prompting variants. Sampling temperature used for molecule generation was t = 0.5.",
            "novelty_of_chemicals": "Reported Novel rates indicate many generated molecules differed from input exemplars (Few-shot Novel ≈ 79.1% ± 10.9; Few-shot+CoT Novel ≈ 65.5% ± 10.9), i.e., a substantial fraction of generated SMILES were novel relative to provided examples. Exact overlap with external databases or training sets was not measured.",
            "application_specificity": "Specificity to HIV-inhibition property was enforced by (a) supplying exemplar molecules labeled as inhibiting HIV replication in prompts and (b) post-generation evaluation using a GNN classifier trained on the OGBG-MolHIV dataset, with rectified probabilities computed to adjust for classifier error and dataset priors.",
            "evaluation_metrics": "Classifier predicted probability C_M(G), rectified probability C(G) (using classifier confusion matrix and dataset priors), Novel rate (distinct from exemplars), Unique rate (distinct among generated), and standard errors reported across trials.",
            "results_summary": "GPT-4 generated molecules with non-zero classifier-predicted probabilities of the target property (Few-shot: C_M(G)=26.4% ± 7.5, C(G)=34.8% ± 16.5; Few-shot+CoT: C_M(G)=32.7% ± 4.7, C(G)=48.8% ± 10.2). Generated sets showed high uniqueness (~92%) but CoT lowered novelty in this experiment. Authors conclude GPT-4 shows preliminary ability to generate molecules with certain properties, and that CoT can improve property-related scores.",
            "comparison_to_other_methods": "No direct empirical comparison of GPT-4 molecular outputs to classical molecular generative models was performed in the property-based experiments; the paper references prior deep generative graph models in related work but does not benchmark against them for the MolHIV task.",
            "limitations_and_challenges": "Property assessment depends on an imperfect GNN classifier rather than experimental validation; classifier uncertainties were corrected but remain a limitation. The paper reports that prompting strategies have inconsistent effects, duplicates/known molecules appear among outputs, model-size and training data specifics are not reported, and that overall abilities are preliminary rather than robust.",
            "uuid": "e8628.1",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models in Graph Generation",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM drug discovery challenge: A contest as a feasibility study on the utilization of large language models in medicinal chemistry",
            "rating": 2,
            "sanitized_title": "llm_drug_discovery_challenge_a_contest_as_a_feasibility_study_on_the_utilization_of_large_language_models_in_medicinal_chemistry"
        },
        {
            "paper_title": "Artificial intelligence enabled chatgpt and large language models in drug target discovery, drug discovery, and development.",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_enabled_chatgpt_and_large_language_models_in_drug_target_discovery_drug_discovery_and_development"
        },
        {
            "paper_title": "Open graph benchmark: Datasets for machine learning on graphs",
            "rating": 2,
            "sanitized_title": "open_graph_benchmark_datasets_for_machine_learning_on_graphs"
        },
        {
            "paper_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "rating": 1,
            "sanitized_title": "smiles_a_chemical_language_and_information_system_1_introduction_to_methodology_and_encoding_rules"
        }
    ],
    "cost": 0.011598,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EXPLORING THE POTENTIAL OF LARGE LANGUAGE MODELS IN GRAPH GENERATION A PREPRINT</p>
<p>Yang Yao 
Tsinghua University</p>
<p>Xin Wang 
Tsinghua University</p>
<p>Zeyang Zhang 
Tsinghua University</p>
<p>Yijian Qin 
Tsinghua University</p>
<p>Ziwei Zhang 
Xu Chu 
Tsinghua University</p>
<p>Yuekui Yang 
Tsinghua University</p>
<p>Wenwu Zhu 
Tsinghua University</p>
<p>Hong Mei 
Tsinghua University</p>
<p>Peking University</p>
<p>EXPLORING THE POTENTIAL OF LARGE LANGUAGE MODELS IN GRAPH GENERATION A PREPRINT
2272D947004B6C59EE49279E3BEE9720
Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification.However, the abilities of LLMs for graph generation remain unexplored in the literature.Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging.In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments.Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation.Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation.We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance.Besides, LLMs show potential in generating molecules with specific properties.These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have experienced remarkable success across various domains Zhao et al. [2023].In comparison to their predecessors, LLMs possess a substantial number of parameters, enabling them to exhibit exceptional capabilities, notably being foundation models Bommasani et al. [2022], in-context learning Brown et al. [2020], Bubeck et al. [2023], chain-of-thought Wei et al. [2022], etc.The application of LLMs has yielded impressive outcomes in numerous tasks, such as code generation Ni et al. [2023], Vaithilingam et al. [2022], drug discovery Murakumo et al. [2023], Chakraborty et al. [2023], knowledge probing Meyer et al. [2023], Sun et al. [2023], etc.</p>
<p>Despite not being explicitly designed for graph-structured data, exploring the potential of LLMs in comprehending and leveraging graph structures has attracted considerable attention recently Zhang et al. [2023a].Notably, recent research endeavors have showcased promising results, demonstrating that LLMs can effectively address various discriminative tasks associated with graph data Wang et al. [2023a], Ye et al. [2023], Huang et al. [2023], Tang et al. [2023], such as node classification, link prediction, and graph classification.These studies highlight the inherent capacity of large language models to grasp the underlying structure of explicitly provided graph data, validating their potential in graph-related problem-solving.</p>
<p>On the other hand, graph generation has been a prominent and extensively studied research field in recent years Zhu et al. [2022].It involves the generation of graphs with specific properties, and its significance extends to various domains, including drug discovery You et al. [2018].Deep generative models have shown promising results in graph generation tasks, particularly in molecular generation.However, current investigations of LLMs for graph-related tasks Our proposed method designs a prompt tailored to each graph generation task, which is subsequently used as the input to the LLM to generate the desired graphs.Each prompt encompasses both the task description and the required output format.In the case of rule-based generation, the prompt contains the description of the rule.For distribution-based generation, a collection of graphs is provided to facilitate the LLM's learning of the underlying distribution.For property-based generation, a collection of molecules is included to enable the LLM to understand molecular properties.</p>
<p>In this work, we propose LLM4GraphGen to explore the potential of LLMs for graph generation.Specifically, we construct a pipeline where graph generation tasks are formulated as textual prompts and LLMs are required to output graphs in a specific format.Within this pipeline, we explore three key questions of applying LLMs to graph generation tasks.1) Whether the LLMs can understand the rules of different types of graph structures?Understanding basic graph structure types through rules, e.g., trees, cycles, regular graphs, and so on, is the foundation of graph generation.</p>
<p>To answer this question, we benchmark LLMs for eight rule-based graph generation.Moreover, we explore the impact of important parameters and prompt construction methods for rule-based graph generation capabilities.2) Whether LLMs can understand the distribution of different types of graph structures?We assess the ability of LLMs to understand the structural type distribution of given graphs and generate new graphs with the same distribution.3) Whether the LLMs can understand domain knowledge of graph generation?We explore LLMs in the task of property-based graph generation, particularly focusing on generating molecule structures with specific properties.This task necessitates a comprehensive understanding of both graph structures and the incorporation of domain knowledge.</p>
<p>Moreover, we have conducted extensive experiments and provided detailed results in this paper.By analyzing the results of the experiment, we derive several valuable observations.Our experiments demonstrate that GPT-4 has reasonably good abilities in graph generation, including rule-based and distribution-based generation.We also observe that some popular prompting methods, such as in-context learning and chain-of-thought, do not improve graph generation performance consistently.Moreover, LLMs show preliminary abilities in generating molecules with certain properties.The conclusions drawn from our experiments provide reliable and informative insights that can guide future research and practical applications for graph generation.</p>
<p>The contributions of our work are summarized as:</p>
<p>• We have explored the potential of using LLMs for graph generation.To the best of our knowledge, we are the first to study this valuable problem.• We extensively evaluate the graph generation capability of state-of-the-art LLMs, such as GPT-4, with diverse prompting methods under three metrics.Our experimental results reveal interesting and insightful observations that can inspire further research.</p>
<p>LLM4GraphGen</p>
<p>In this section, we design three types of tasks to explore the graph generation ability of LLMs from multiple aspects, including rule-based tasks, distribution-based tasks, and property-based tasks.</p>
<p>Rule-based graph generation</p>
<p>In this task, we employ rules in natural language to describe the structure of various graphs (e.g., being a tree, or a cycle), and the model is evaluated by the adherence of its generated graphs to these rules.In order to assess the capacity of LLMs to comprehend and follow the instructions in graph generation, we formulate the following eight rules.Figure 2 shows the example graphs with regard to different rules.</p>
<p>• Trees: The generated graph should be a tree with the specified number of nodes, i.e., an undirected graph where there is exactly one path between any two nodes.</p>
<p>• Cycles: The generated graph should contain a cycle with the specified number of nodes, and should not contain any other nodes or edges.</p>
<p>• Planar graphs: The generated graph should be a planar graph with the specified number of nodes and edges, i.e., there exists a way to draw the graph on a plane without edge crossings.</p>
<p>• Components: The generated graph should have a specified number of connected components, i.e., there must be a specified number of connected subgraphs with no edges among them.</p>
<p>• k-regular graphs: Every node of the generated graph should have the same number of neighbors, i.e., the degree of every node is k.</p>
<p>• Wheel graphs: The generated graph should be formed by connecting a single node to all nodes of a cycle.</p>
<p>• Bipartite graphs: The generated graph should have nodes that are divided into two disjoint and independent sets U and V with specified sizes.</p>
<p>• k-color graphs: The generated graph should be k-colorable, i.e., each node is assigned a color, and two adjacent nodes do not have the same color.</p>
<p>Prompt Design We adopt the following four types of prompts for rule-based graph generation:</p>
<p>• Zero-shot: The prompt contains the relevant information about the rules, as well as a specification of the output format.The model is then asked to generate graphs using the given rules.</p>
<p>• Few-shot: In addition to the zero-shot prompt, the model is given several graph examples that follow the given rules.The edges of the graphs are sorted by the node ID to facilitate the model understanding.</p>
<p>• Zero-shot+CoT/Few-shot+CoT: In addition to the zero-shot prompt and the few-shot prompt, the model is asked to give the answer step by step.</p>
<p>Generated graphs
! !"#$ = #. &amp;' ! %#&amp; = #. &amp;(</p>
<p>Distribution-based graph generation</p>
<p>Learning distributions from the given graphs and then generating new graphs based on the distributions is a necessary ability for graph generation Zhu et al. [2022].To this end, we propose distribution-based graph generation tasks, which define a target graph distribution and the generated set of graphs {G i } from the target distribution as the input to the model.The model should infer the parameters of the target distribution from the given graphs, and generate new graphs from the same distribution.We design several subtasks with increasing difficulties, including the generation of tree-or-cycles, union of components, and motifs.</p>
<p>Trees or cycles In this subtask, the target distribution is defined as a mixture of trees and cycles, where each graph has a probability of p to be a tree and 1 − p to be a cycle,
P (G i is a tree) = p, P (G i is a cycle) = 1 − p.(1)
Specifically, the model is given 10 graphs sampled from the target distribution and is asked to infer the value of p from the graphs and then generate 10 new graphs that follow the same distribution.We evaluate the task with different settings of p, and all input graphs have a random number of nodes ranging from 5 to 7.</p>
<p>Union of components</p>
<p>In this subtask, each graph from the target distribution is the union of two connected components, with each component being either a tree or a cycle.There is a probability of p that the two components belong to the same kind and 1 − p that they belong to different kinds, i.e.,
G i = G 1 i ∪ G 2 i .
The model is asked to generate graphs with two component types specified by rules, and similarly, the input to the model is 10 random graphs drawn from the target distribution.The model is expected to infer the value of p and generate 10 new graphs from the target distribution.Each component has a random number of 5 to 7 nodes.This task is more complex and evaluates the ability of graph generation in scenarios with multiple correlated factors such as the component types.</p>
<p>Motif In this subtask, each graph from the target distribution consists of a base graph and a motif graph with an inter-connected edge, where there are three kinds of base graphs (trees, ladders, wheels), and three motif graphs (cycles, For each graph G i , the base graph type S i is chosen from a uniform distribution over three kinds of base graphs, and the motif graph C i with the same label is chosen with probability p, while the other two motif graphs are chosen with probability (1 − p)/2 respectively, i.e.,
NNP(=S)(NN)c1ccccc1 O=Nc1ccc(O)c(N=O)c1O S=c1[nH][nH]c(=S)s1 NNC(=O)c1ccccc1SSc1ccccc1C(=O)NN O=S1c2ccccc2Sc2ccccc21 O=S(c1ccccc1O)S(=O)c1ccccc1O CCOP(N)(=O)c1ccccc1 … … Inhibit HIV replication? NNC(=O)c1ccccc1SSc1ccccc1C(=O)NN O=S1c2ccccc2Sc2ccccc21 O=S(c1ccccc1O)S(=O)c1ccccc1O CCOP(N)(=O)c1ccccc1 … … Input molecules Generated molecules O=P(Nc1ccccc1)(Nc1ccccc1)Nc1ccccc1 O=P(Nc1ccccc1)(Nc1ccccc1)Nc1ccccc1P (S i , C i ) = 1 3 p S i = C i , i ∈ {0, 1, 2} 1 6 (1 − p) S i ̸ = C i , i ∈ {0, 1, 2} .(2)
We generate 10 random graphs from the target distribution and ask the model to infer the value of p, and generate 10 new graphs from the same distribution.The main purpose of this task is to explore the abilities of LLMs to understand graph distributions under more complex factors and generate graphs based on inferred distributions.</p>
<p>Prompt Design In zero-shot prompt, we introduce the graph generation task and the target distribution, and provide a set of graphs sampled from the target distribution as the input.Then, we ask the model to infer the value of p and generate new graphs.Similarly, we add exemplars in few-shot prompt, and step-by-step thinking in CoT prompt.</p>
<p>Property-based graph generation</p>
<p>Generating graphs with certain properties is important in real-world applications, e.g., the properties, in the field of drug discovery, such as inhibiting HIV replication, blood-brain barrier penetration, and toxicity to the human body, are important for the development of new drugs Guo and Zhao [2023], Zhu et al. [2022].Though LLMs have learned expert knowledge like chemistry and medicine through large-scale textual data, it remains unknown whether they can leverage this information to directly generate graphs with desired properties.</p>
<p>In this task, we evaluate LLMs' abilities to generate graphs with given properties by adopting the molecular property prediction dataset OGBG-MolHIV Hu et al. [2020].We choose the molecules labeled to inhibit HIV replication as the input graphs and ask the model to generate new molecules with properties similar to these graphs.Following the common practice in the field of chemistry, we adopt the SMILES Weininger [1988] format to represent the molecule graphs.To evaluate the performance of molecule generation, we measure the fraction of generated molecules that have the same properties as the input molecules.Since it would be infeasible to check the properties manually using experiments or expert knowledge, we use a GNN classifier trained on the OGBG-MolHIV dataset to assist in the evaluation of molecule generation.Specifically, let G 1 , G 2 , . . ., G n be the generated graphs that represent molecules, C M (G) ∈ [0, 1] be the prediction given by the classifier, e.g., C M (G) = 0.5 means the molecule G is predicted to have the desired property with 50% probability.Since the classifier may not be exactly accurate, we calculate the rectified predictions C(G) by taking into consideration the dataset priors as well as the classifier accuracy in the original dataset.</p>
<p>Prompt Design We design a few-shot prompt that the LLM is given a description of the desired property and a collection of molecules that have the property.Then, the model is asked to generate new molecules with the same property.For the CoT prompt, the model is also asked to provide a step-by-step explanation of the answer.</p>
<p>Experimental Results and Analyses</p>
<p>In this section, we conducted extensive experiments on LLMs for our proposed graph generation tasks.Through analyzing the results, we derive valuable observations to benefit practical applications.</p>
<p>Rule-based graph generation</p>
<p>We explore the graph generation abilities of several representative LLMs in this part, including GPT-4, GPT-3.5, and LLama2-13B.We use the following three metrics to evaluate the graph generation quality:</p>
<p>• Valid rate: The proportion of generated graphs that match the rules.</p>
<p>• Novel rate: The proportion of generated graphs that is different from the given example graph.</p>
<p>• Unique rate: The proportion of generated graphs that are not identical.</p>
<p>Since GPT-4 is the strongest LLM currently available, we perform our main experiments with GPT-4.The valid rate and unique rate for various rules and prompts are listed in Table 1 and Table 2 respectively.It is worth noting that by designing appropriate prompts, the novel rate of each experiment is 100% or close to 100% for GPT-4.More details about the setups and results can be found in the appendix.</p>
<p>Observation 1: GPT-4 has reasonably good abilities for rule-based graph generation.</p>
<p>Table 1 demonstrates that GPT-4 has the ability to generate graphs according to rules in general.Specifically, for simple rules such as Trees and Cycles, GPT-4 achieves good generation quality.But the generation quality is not good enough for other rules.A possible reason is that the topological structures of trees and cycles are relatively simple.LLMs can quickly come up with their generation algorithms according to the massive training data.For more complex rules like k-regular, wheel, and bipartite, LLMs cannot achieve satisfactory results with zero-shot prompt.Nevertheless, adjusting prompts such as few-shot and chain-of-thought can improve the generation quality.For components, planar graphs, and k-color graphs, it is difficult for LLMs to find the accurate generation method from rule descriptions, even in conditions giving few-shot examples and CoT prompt, resulting in poor generation quality.In order to explore the graph generation effects of different LLMs, we conducted experiments on multiple LLMs and generated small-size graphs, as shown in Table 3.It can be found that GPT-4 has good generation quality for all three rules.GPT-3.5 can generate some legitimate cycles, but its performance is poor for more complex rules.LLama2 can only generate very few graphs that comply with rules given examples.</p>
<p>The impact of prompt on graph generation</p>
<p>It is known that prompts have a significant impact on the performance of LLMs.In this section, we compare the performance of different prompts for rule-based graph generation with GPT-4.The valid rate and unique rate for different prompts are listed in Table 1 and Table 2 respectively.We have the following observations.</p>
<p>Observation 2: Providing examples has an inconsistent impact on LLMs in generating different types of graphs.</p>
<p>Giving examples can have an impact on generation quality, but not all have a positive impact.As shown in Table 1, for trees, the valid rate of few-shot prompt is worse than zero-shot prompt, which is because adding examples may disrupt the understanding of LLM for rule descriptions.For planar graphs, giving examples greatly reduces the valid rate of the generated graph, as it is difficult to generalize the properties of the planar graph from the examples, and instead may lead to some opposite rules.For wheel graphs, the valid rate of few-shot is much better than zero-shot because the generation of a wheel can actually be divided into two steps: first, generate a cycle, and then add a new node to connect it to each previous node.LLM can learn about the existence of the cycle from the examples, which strengthens its understanding of the rules.</p>
<p>Observation 3: CoT prompt has diverse impacts on different evaluation metrics for graph generation.</p>
<p>CoT prompt can have an impact on generation quality, but not all have a positive impact.Making LLM think step by step can help it better decompose tasks.For example, as mentioned earlier, generating a wheel graph can be divided into two steps.Generating a bipartite graph can also be divided into two steps: firstly, the nodes are divided into two subsets, and then the nodes in the two subsets are connected.However, although the valid rate has increased, its unique rate has greatly decreased because, after step-by-step thinking, it always tends to divide nodes into the same subset.</p>
<p>The impact of parameters on graph generation</p>
<p>Graph generation using LLMs can be sensitive to the choice of parameters.In order to explore the impact of various parameters on graph generation, we construct the ablation study.We compare the performance of graph generation when using different graph sizes in Table 4.The "Medium" size in the experiment is the same as the size in the main experiment.More details of the experiment can be found in the appendix.Through the analysis of the generated results, we have reached the following conclusion:</p>
<p>Observation 4: As the graph size increases, the performance of LLM in graph generation decreases for most rules, except for simple cases such as cycles.</p>
<p>Table 4 shows the valid rate and unique rate when using LLM to generate graphs of different sizes for three rules.In experience, the larger the graph generated, the poorer the generation quality, but this is not the case for cycles.This may be because the cycle is too simple.</p>
<p>Small</p>
<p>Zero-shot 84.7 ± 5.9 90.0 ± 3.1 74.7 ± 9.9 100.0 ± 0.0 56.0 ± 4.5 100.0 ± 0.0 Few-shot 73.3 ± 7.1 96.0 ± 1.8 90.0 ± 2.9 100.0 ± 0.0 76.4 ± 5.3 100.0 ± 0.0 Zero-shot+CoT 95.3 ± 3.2 84.7 ± 4.8 97.1 ± 1.2 100.0 ± 0.0 62.0 ± 6.1 96.7 ± 3.2 Few-shot+CoT 96.0 ± 2.8 94.7 ± 2.1 87.9 ± 3.4 100.0 ± 0.0 80.7 ± 3.2 100.0 ± 0.0</p>
<p>Medium</p>
<p>Zero-shot 91.3 ± 3.3 88.3 ± 1.6 64.0 ± 6.8 100.0 ± 0.0 50.3 ± 5.5 98.3 ± 1.6 Few-shot 85.0 ± 3.3 92.3 ± 1.3 86.1 ± 3.1 100.0 ± 0.0 62.3 ± 5.1 98.5 ± 1.5 Zero-shot+CoT 86.9 ± 3.6 89.0 ± 4.2 82.7 ± 8.6 100.0 ± 0.0 43.2 ± 4.9 98.6 ± 1.0 Few-shot+CoT 97.0 ± 1.9 83.0 ± 5.1 91.5 ± 1.6 100.0 ± 0.0 58.5 ± 5.9 96.3 ± 2.6</p>
<p>Large Zero-shot 96.0 ± 1.6 84.0 ± 4.4 61.3 ± 10.6 100.0 ± 0.0 44.3 ± 6.8 97.9 ± 1.5 Few-shot 82.0 ± 6.7 96.7 ± 2.0 70.0 ± 7.4 100.0 ± 0.0 64.0 ± 8.8 74.0 ± 13.7 Zero-shot+CoT 100.0 ± 0.0 85.6 ± 9.0 70.0 ± 21.2 100.0 ± 0.0 58.9 ± 6.4 95.6 ± 4.2 Few-shot+CoT 95.0 ± 3.6 85.7 ± 6.2 82.5 ± 7.1 100.0 ± 0.0 53.3 ± 10.8 89.2 ± 5.5</p>
<p>In addition, we can also observe from Table 4 that the larger the size, the smaller the unique rate, indicating that LLM tends to generate graphs from smaller sets.This is counterintuitive because the larger the size, the larger the set of graphs that satisfy the rules.This indicates that LLM can handle a certain amount of information, and the larger the size of the graph, the smaller the set of graphs.</p>
<p>Distribution-based graph generation</p>
<p>We use GPT-4 for graph generation tasks, and there are two metrics for evaluating the quality of generation:</p>
<p>• p pred : The value of p represents the distribution of a set of graphs.p pred is the value of p predicted by the LLM using the set of input graphs.• p gen : The value of p calculated by the generated graphs.</p>
<p>We adopt three experiment settings (refer to Section 2.2 for more details) with increasing difficulty.The purpose of the experiments we designed is as follows:</p>
<p>• Trees or cycles: Exploring the distribution of graphs.</p>
<p>• Union of components: Exploring the distribution of subgraph combinations within a graph.</p>
<p>• Motif: Exploring the distribution of subgraph combinations within the graph for more complex situations.</p>
<p>The previous experiments have demonstrated that GPT-4 can generate trees and cycles with high accuracy.Therefore, our designed experiments on "Trees or cycles" can eliminate the impact of generation quality as much as possible.In addition, we also constructed a set of experiments to demonstrate the ability of GPT-4 to generate two-component graphs with trees and cycles as subgraphs, as shown in Table 5.This indicates that our constructed "Union of components" experiment can eliminate the impact of generation quality as much as possible.The results can be seen in Figure 5, which provides the p pred and p gen values for the three tasks.</p>
<p>Observation 5: LLMs can understand and generate graphs with simple distributions, but perform poorly in complex situations.</p>
<p>Figure 5 shows the performance of GPT-4 on different tasks.From the figure, for "Trees or cycles" and "Union of components", we can see that although the values of p pred , p gen , and p cannot be completely consistent when using CoT prompt, the curve trends are similar.Since we generate the graph based on probability rather than strictly proportional, this error is reasonable.However, for "Motif" tasks, none of the three prompts learned anything useful, and their p pred always fluctuated around 0.5.From Figure 5, it can be seen that for the first two tasks, the p pred and p gen generated by CoT are relatively accurate, while zero-shot and few-shot have hardly learned anything.This indicates that providing only examples is not enough, where the graphs are sampled.p pred is the value of p predicted by LLM for the input graphs.p gen is the value of p calculated by the generated graphs.In this task, the performance of LLMs is better when p pred and p gen are closer to the ground-truth parameter p. Zero-shot 43.0 ± 8.3 39.0 ± 6.1 38.0 ± 11.9 Few-shot 81.0 ± 6.2 13.0 ± 6.5 48.0 ± 11.8 CoT 100.0 ± 0.0 100.0 ± 0.0 89.0 ± 4.4</p>
<p>and it is necessary to demonstrate in detail how to estimate the distribution from the given graph and generate a new set of graphs based on the given p-value for GPT-4.</p>
<p>Property-based graph generation</p>
<p>We use GPT-4 for graph generation, and there are four metrics for evaluating the quality of graph generation:</p>
<p>• C M (G): the classifier's predicted probability of having the desired properties for the generated molecules.</p>
<p>• C(G): the rectified probability of having the desired properties for the generated molecules.</p>
<p>• Novel rate: the fraction of generated molecules that are distinct from the input molecules.</p>
<p>• Unique rate: the fraction of generated molecules that are not duplicates.</p>
<p>From Table 6, we have the following observation.</p>
<p>Observation 7: LLMs show preliminary abilities in generating molecules with certain properties.</p>
<p>As shown in Table 6, both C(G) and C M (G) are greater than 0, indicating that in the classifier's view, some of the generated molecules have specified properties.Although some molecules are duplicates of known molecules, there are still some new molecules that have this property.And it can be found that the generated molecules have relatively high levels of novel rate and unique rate.</p>
<p>Table 6: Results of property-based graph generation.C M (G) is the classifier's predicted probability of having the desired properties for the generated molecules, while C(G) is the rectified probability."Novel" denotes generated molecules that are not the same as the input molecules, while "Unique" denotes molecules that are not duplicated with other generated molecules.
Prompt C M (G) C(G) Novel Unique
Few-shot 26.4 ± 7.5 34.8 ± 16.5 79.1 ± 10.9 91.8 ± 6.1 Few-shot+CoT 32.7 ± 4.7 48.8 ± 10.2 65.5 ± 10.9 92.7 ± 6.0</p>
<p>In addition, we observe that both C(G) and C M (G) increase when using CoT prompt, which indicating that it's beneficial for LLM to think step by step, so that the reason abilities of LLM can be activated in these complex graph generation scenarios.</p>
<p>Related Work</p>
<p>We review related work on large language model for graphs and graph generation respectively.</p>
<p>Large Language Model for Graphs</p>
<p>Recently, there have been several works about utilizing large language models (LLMs) for solving problems on graphs.He et al. [2023] propose to utilize LLMs to generate textural explanations on academic networks, and the explanations are further leveraged to enhance the node features for conducting node classification.Besides enhancing the models with LLM-generated features, Chen et al. [2023] and Ye et al. [2023] further propose to directly adopt LLMs for predicting node categories on text-attributed graphs.Wang et al. [2023a] introduce a benchmark framework to evaluate the performance of LLMs with several graph algorithmic tasks, including topological sort, maximum flows, etc. Guo et al. [2023] is another benchmark to evaluate LLMs' abilities to tackle structural information, and several factors, such as the role of graph description languages, are taken into consideration in the evaluation process.Zhang et al. [2023b] further considers evaluating the LLMs' abilities to handle spatial-temporal information on dynamic graphs.Several other works explore the applications of LLMs in graph tool learning Zhang [2023], Jiang et al. [2023], knowledge graphs Pan et al. [2023], etc.However, most existing works mainly focus on graph discriminative tasks, and we explore the potential of LLMs on graph generation, which remains under-explored in the literature.</p>
<p>Graph generation</p>
<p>Graph generation is a highly anticipated research field that is crucial for areas such as code generation and new drug discovery Zhu et al. [2022].Recently, deep graph generation methods have achieved remarkable results, which can be roughly classified into auto-regressive models Wang et al. [2023b], Bacciu and Podda [2021], variational autoencoder models Du et al., Jin et al. [2018], normalizing flow models Zang and Wang [2020], Luo et al. [2021], generative adversarial network Gamage et al. [2020], Maziarka et al. [2020], diffusion models Jo et al. [2022], Vignac et al. [2023].However, these methods mainly focus on learning the distribution of graphs from existing graphs for generation, ignoring the complex yet rich domain knowledge in practical applications.Inspired by the recent progress of LLMs in leveraging expert knowledge, in this paper, we systematically explore the graph generation capabilities of LLMs for the first time by proposing various tasks, including rule-based generation, distribution-based generation, and property-based generation.</p>
<p>Conclusion</p>
<p>In this paper, we explore the potential of large language models (LLMs) for graph generation tasks, which remains unexplored in the literature.We propose LLM4GraphGen to systematically assess LLMs' capabilities in understanding and applying various graph structure rules, capturing structural type distributions, and leveraging domain knowledge in property-based graph generation.Our findings indicate that while LLMs show promising preliminary abilities in rule-based and distribution-based graph generation, the effectiveness of popular prompting methods like few-shot and chain-of-thought prompting is not consistent.Additionally, the potential of LLMs to generate molecules with specific properties is a notable outcome.These insights open new avenues for future research in graph generation, highlighting the evolving capabilities as well as the limitations of LLMs in graph generation tasks.</p>
<p>Table 9: The definition of base graphs and motif graphs for the "Motif" task.The definition of motif graphs are given by first listing the number of nodes, then listing the endpoints of all edges, which is in the same format as the input to LLMs.</p>
<p>Graph Description Trees Full binary or ternary tree Ladders Two paths of same length with edge connections between pairs of nodes Wheels A single node connected to all nodes of a cycle</p>
<p>Cycle</p>
<p>(5, [ (1,2), (1,5), (2, 3), (3, 4), (4, 5)]) House</p>
<p>(5, [ (1,2), (1,5), (2, 3), (2, 5), (3, 4), (4, 5)]) Crane</p>
<p>(5, [ (1,2), (1,3), (1,4), (1,5), (2, 3), (3, 4), (4, 5)])
(G) = 0 C M (G) = 1 C(G) = 0 35539 4145 C(G) = 1 633 810
where the conditional probability can be computed from the confusion matrix of the classifier as given in Table 10.By rearranging the equations, we can compute rectified predictions C(G) that are closer to C T (G):
p(C(G) = 1) = p(CM (G) = 1) − p(CM (G) = 1 | CT (G) = 0) p(CM (G) = 1 | CT (G) = 1) − p(CM (G) = 1 | CT (G) = 0)
.</p>
<p>B Extended experimental results</p>
<p>We provide complete experimental results that include valid rates, novel rates, and unique rates for rule-based generation tasks in Table 14, Table 15, and Table 16.Additionally, we compare the effect of different sampling temperatures for rule-based generation in Table 17, and the effect of different amounts of generated graphs in Table 18.From the two tables, we find that it is necessary to set appropriate sampling temperature and amount of generated graphs for different tasks.</p>
<p>For distribution-based generation, we provide numerical results in Table 11, Table 12 and Table 13.</p>
<p>Figure 1 :
1
Figure1: An overview of LLM4GraphGen.Our proposed method designs a prompt tailored to each graph generation task, which is subsequently used as the input to the LLM to generate the desired graphs.Each prompt encompasses both the task description and the required output format.In the case of rule-based generation, the prompt contains the description of the rule.For distribution-based generation, a collection of graphs is provided to facilitate the LLM's learning of the underlying distribution.For property-based generation, a collection of molecules is included to enable the LLM to understand molecular properties.</p>
<p>Figure 2 :
2
Figure 2: An illustration of graphs with regard to different rules.</p>
<p>Figure 3 :
3
Figure 3: An illustration of distribution-based graph generation.</p>
<p>Figure 4 :
4
Figure 4: An illustration of property-based graph generation.</p>
<p>Observation 6 :
6
Detailed examples and CoT are helpful for distribution-based graph generation.</p>
<p>Figure5: Performance of GPT-4 on distribution-based graph generation.p represents the parameter of the distribution where the graphs are sampled.p pred is the value of p predicted by LLM for the input graphs.p gen is the value of p calculated by the generated graphs.In this task, the performance of LLMs is better when p pred and p gen are closer to the ground-truth parameter p.</p>
<p>62.0 ± 1.8 58.2 ± 16.2 82.0 ± 4.4 Few-shot 60.0 ± 0.0 40.3 ± 14.9 70.0 ± 11.7 CoT 30.0 ± 8.5 25.0 ± 10.0 46.0 ± 8.8 p = 0.4 Zero-shot 58.0 ± 1.8 57.2 ± 16.3 70.0 ± 13.3 Few-shot 60.0 ± 0.0 62.2 ± 12.8 84.0 ± 658.0 ± 1.8 72.0 ± 11.1 54.0 ± 16.9 Few-shot 60.0 ± 0.0 45.0 ± 15.2 86.0 ± 3.6 CoT 59.2 ± 5.0 62.9 ± 9.7 78.0 ± 5.2Table 13: Metrics for distribution-based graph generation on the "Motif" task.p represents the parameter of the distribution where the graphs are sampled.p pred is the value of p predicted by LLM for the input graphs.p gen is the value of p calculated by the generated graphs."Valid" denotes fractions of generated graphs that are valid under specific rules.52.0 ± 1.8 24.2 ± 7.3 90.0 ± 6.9 Few-shot 48.0 ± 8.2 38.9 ± 10.0 98.0 ± 1.8 CoT 40.0 ± 2.4 52.2 ± 6.1 95.0 ± 3.1 p = 0.8 Zero-shot 56.0 ± 2.2 35.1 ± 11.2 98.0 ± 1.8 Few-shot 56.0 ± 6.7 38.0 ± 8.2 100.0 ± 0.0 CoT 41.4 ± 3.7 63.3 ± 7.7 94.0 ± 3.6</p>
<p>99.7 ± 0.3 83.0 ± 5.1 96.3 ± 1.5 98.0 ± 1.4 100.0 ± 0.0 88.3 ± 5.1 10.7 ± 0.7 96.3 ± 2.6 Novel Zero-shot 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 Few-shot 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 95.8 ± 4.1 100.0 ± 0.0 Zero-shot+CoT 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 Few-shot+CoT 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 96.4 ± 3.5 100.0 ± 0.0</p>
<p>Table 1 :
1
The valid rate for rule-based graph generation with GPT-4.The metric measures the fraction of generated graphs that are valid under the specified rules.Values after ± denote standard errors.
PromptTreesCyclesComponentsPlanark-regularWheelBipartitek-colorZero-shot100.0 ± 0.0 91.3 ± 3.330.4 ± 5.147.3 ± 4.2 64.0 ± 6.8 13.0 ± 5.3 60.3 ± 7.4 50.3 ± 5.5Few-shot98.0 ± 0.9 85.0 ± 3.363.2 ± 5.34.3 ± 1.3 86.1 ± 3.1 88.8 ± 7.4 57.1 ± 8.6 62.3 ± 5.1Zero-shot+CoT 100.0 ± 0.0 86.9 ± 3.638.0 ± 5.153.3 ± 6.0 82.7 ± 8.6 92.3 ± 4.7 92.7 ± 4.4 43.2 ± 4.9Few-shot+CoT97.6 ± 1.7 97.0 ± 1.940.0 ± 6.720.0 ± 4.3 91.5 ± 1.6 90.7 ± 5.1 98.2 ± 1.8 58.5 ± 5.9</p>
<p>Table 2 :
2
The unique rate for rule-based graph generation with GPT-4.The metric measures the fraction of valid graphs that are unique under the specified rules.
PromptTreesCyclesComponentsPlanark-regularWheelBipartitek-colorZero-shot98.6 ± 0.9 88.3 ± 1.691.9 ± 3.099.7 ± 0.3 100.0 ± 0.0 98.7 ± 0.8 44.3 ± 7.0 98.3 ± 1.6Few-shot99.3 ± 0.5 92.3 ± 1.397.7 ± 1.196.8 ± 2.5 100.0 ± 0.0 98.8 ± 1.1 50.0 ± 8.2 98.5 ± 1.5Zero-shot+CoT 100.0 ± 0.0 89.0 ± 4.298.5 ± 0.898.6 ± 0.8 100.0 ± 0.0 85.0 ± 5.7 16.9 ± 4.2 98.6 ± 1.0Few-shot+CoT99.7 ± 0.3 83.0 ± 5.196.3 ± 1.598.0 ± 1.4 100.0 ± 0.0 88.3 ± 5.1 10.7 ± 0.7 96.3 ± 2.6</p>
<p>Table 3 :
3
A comparison of rule-based graph generation using different LLMs.Reported values are valid rates.
ModelPromptCyclek-regulark-colorZero-shot84.7 ± 5.9 74.7 ± 9.9 56.0 ± 4.5GPT-4Few-shot Zero-shot+CoT 95.3 ± 3.2 97.1 ± 1.2 62.0 ± 6.1 73.3 ± 7.1 90.0 ± 2.9 76.4 ± 5.3Few-shot+CoT96.0 ± 2.8 87.9 ± 3.4 80.7 ± 3.2Zero-shot84.7 ± 5.66.7 ± 2.40.0 ± 0.0GPT-3.5Few-shot Zero-shot+CoT 56.7 ± 8.0 22.7 ± 7.48.7 ± 3.6 29.3 ± 7.0 2.7 ± 2.6 14.3 ± 8.4Few-shot+CoT 57.5 ± 12.1 9.0 ± 4.6 34.3 ± 8.0Zero-shot0.7 ± 0.70.0 ± 0.00.0 ± 0.0LLama2Few-shot Zero-shot+CoT31.1 ± 7.8 17.2 ± 5.0 13.0 ± 3.7 1.1 ± 1.1 0.0 ± 0.0 0.0 ± 0.0Few-shot+CoT16.2 ± 5.28.0 ± 3.1 13.3 ± 3.7</p>
<p>Table 4 :
4
The comparison of different graph sizes for rule-based graph generation with GPT-4."Valid" denotes fractions of generated graphs that are valid under specific rules, and "Unique" denotes fractions of generated graphs that are not duplicates.
SizePromptValidCyclesUniqueValidk-regular UniqueValidk-colorUnique</p>
<p>Table 5 :
5
The valid rate of two-component graph generation.
PromptTree+TreeCycle+Cycle Tree+Cycle</p>
<p>Table 10 :
10
The confusion matrix of the molecule classifier used in the experiments of property-based graph generation.C(G) denotes the ground-truth label, C M (G) denotes the predicted label, and the numbers represent the amount of molecules that fall into this category.C M</p>
<p>Table 11 :
11
Metrics for distribution-based graph generation on the "Trees or cycles" task.p represents the parameter of the distribution where the graphs are sampled.p pred is the value of p predicted by LLM for the input graphs.p gen is the value of p calculated by the generated graphs."Valid" denotes fractions of generated graphs that are valid under specific rules.
pPromptp predp genValidZero-shot 62.0 ± 1.8 54.3 ± 17.8 72.0 ± 7.2p = 0.2Few-shot 60.0 ± 0.0 32.2 ± 14.9 86.0 ± 6.1CoT45.0 ± 3.5 50.4 ± 6.7 83.3 ± 9.8Zero-shot 60.0 ± 2.8 74.3 ± 12.7 84.0 ± 5.4p = 0.4Few-shot 62.0 ± 1.8 68.9 ± 16.1 98.0 ± 1.8CoT43.3 ± 2.7 59.0 ± 4.8 83.3 ± 7.2Zero-shot 58.0 ± 1.8 48.1 ± 15.1 88.0 ± 5.2p = 0.6Few-shot 62.0 ± 1.8 16.0 ± 10.4 92.0 ± 5.2CoT53.3 ± 5.4 86.3 ± 7.1 96.7 ± 2.7Zero-shot 60.0 ± 0.0 90.0 ± 8.9 92.0 ± 5.2p = 0.8Few-shot 64.0 ± 2.2 62.1 ± 12.1 88.0 ± 5.2CoT76.7 ± 5.4 92.6 ± 6.0 93.3 ± 2.7</p>
<p>Table 12 :
12
Metrics for distribution-based graph generation on the "Union of components" task.p represents the parameter of the distribution where the graphs are sampled.p pred is the value of p predicted by LLM for the input graphs.p gen is the value of p calculated by the generated graphs."Valid" denotes fractions of generated graphs that are valid under specific rules.
pPromptp predp genValid</p>
<p>Table 14 :
14
The novel rate for rule-based graph generation with GPT-4.The metric measures the fraction of generated graphs that are different from the given example graphs.Values after ± denote standard errors.
MetricPromptTreesCyclesComponentsPlanark-regularWheelBipartitek-colorZero-shot100.0 ± 0.0 91.3 ± 3.330.4 ± 5.147.3 ± 4.264.0 ± 6.813.0 ± 5.360.3 ± 7.450.3 ± 5.5ValidFew-shot Zero-shot+CoT 100.0 ± 0.0 86.9 ± 3.6 98.0 ± 0.9 85.0 ± 3.363.2 ± 5.3 38.0 ± 5.14.3 ± 1.3 53.3 ± 6.086.1 ± 3.1 82.7 ± 8.688.8 ± 7.4 92.3 ± 4.757.1 ± 8.6 92.7 ± 4.462.3 ± 5.1 43.2 ± 4.9Few-shot+CoT97.6 ± 1.797.0 ± 1.940.0 ± 6.720.0 ± 4.391.5 ± 1.690.7 ± 5.198.2 ± 1.858.5 ± 5.9Zero-shot98.6 ± 0.988.3 ± 1.691.9 ± 3.099.7 ± 0.3 100.0 ± 0.0 98.7 ± 0.844.3 ± 7.098.3 ± 1.6UniqueFew-shot99.3 ± 0.592.3 ± 1.397.7 ± 1.196.8 ± 2.
A Experimental setup A.1 Rule-based generationFor the rule-based graph generation experiments, we configure the tasks with the problem settings listed in Table7unless otherwise specified.The task settings are chosen to be challenging for LLMs.Additionally, to produce the results in the comparison of different graph sizes, the parameters for the three different sizes are listed in Table8; note that the "Medium" size is the same as the default settings.In the comparison of different LLMs, since models other than GPT-4 are relatively weak, we use the "Small" settings from Table8in the experiments.Unless otherwise specified, we use the sampling temperature t = 0.8.It is worth noting that the number of edges have a significant effect on the difficulty of "Planar" and "k-color" tasks, as graphs with few edges are easily planar or k-colorable.The parameters in the experiments are chosen such that random graphs with the specified number of nodes and edges have around 20% probability to be valid.A.2 Distribution-based generationFor the "Trees or cycles" task, the number of nodes for each tree or cycle is randomly selected between 5 and 7.For the "Union of components" task, each of the two components for each graph has between 5 and 7 nodes.For the "Motif" task, we define the three kinds of base graphs and motif graphs in the same way as Spurious-Motif, and the description is given in Table9.All experiments use the sampling temperature t = 0.5.A.3 Property-based generationFor property-based graph generation, we generate molecules using GPT-4 with sampling temperature set to t = 0.5.To evaluate the performance of molecule generation, we measure the fraction of generated molecules that have the desired property using a GNN classifier trained on the OGBG-MolHIV dataset.More specifically, let G 1 , G 2 , . . ., G n be the generate graphs that represent molecules.Let C T (G) ∈ {0, 1} be the ground-truth label of graph G, with C T (G) = 1 meaning the molecule G has the desired property (e.g. can inhibit HIV replication), and C T (G) = 0 meaning otherwise.Similarly, let C M (G) be the predicted label of G.These values are related using the law of total probability:
Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, arXiv:2303.182232022arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Advances in Neural Information Processing Systems. 2020</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, 202235</p>
<p>LEVER: Learning to verify language-to-code generation with execution. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-Tau Yih, Sida Wang, Xi Victoria, Lin , Proceedings of the 40th International Conference on Machine Learning. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine LearningPMLR2023202</p>
<p>Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. Priyan Vaithilingam, Tianyi Zhang, Elena L Glassman, 10.1145/3491101.3519665Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, CHI EA '22. New York, NY, USAAssociation for Computing Machinery2022</p>
<p>LLM drug discovery challenge: A contest as a feasibility study on the utilization of large language models in medicinal chemistry. Kusuri Murakumo, Naruki Yoshikawa, Kentaro Rikimaru, Shogo Nakamura, Kairi Furui, Takamasa Suzuki, Hiroyuki Yamasaki, Yuki Nishigaya, Yuzo Takagi, Masahito Ohue, In AI for Accelerated Materials Design -NeurIPS. 2023 Workshop, 2023</p>
<p>Artificial intelligence enabled chatgpt and large language models in drug target discovery, drug discovery, and development. Chiranjib Chakraborty, Manojit Bhattacharya, Sang-Soo Lee, Molecular Therapy-Nucleic Acids. 332023</p>
<p>Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt Junghanns, Roy Meissner, Gordian Dziwis, Kirill Bulert, Michael Martin, arXiv:2307.06917Llm-assisted knowledge graph engineering: Experiments with chatgpt. 2023arXiv preprint</p>
<p>Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs?. Kai Sun, Ethan Yifan, Hanwen Xu, Yue Zha, Xin Liu, Dong Luna, arXiv:2308.101682023arXiv preprint</p>
<p>Graph meets llms: Towards large graph models. Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, Wenwu Zhu, NeurIPS 2023 New Frontiers in Graph Learning Workshop. 2023a</p>
<p>Can language models solve graph problems in natural language. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, arXiv:2308.07134Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs. 2023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information: When and why. Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma, arXiv:2309.165952023arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023arXiv preprint</p>
<p>A survey on deep graph generation: Methods and applications. Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, Shu Wu, Learning on Graphs Conference. 2022</p>
<p>Graph convolutional policy network for goal-directed molecular graph generation. Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, Jure Leskovec, Advances in neural information processing systems. 201831</p>
<p>Gnnexplainer: Generating explanations for graph neural networks. Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec, Advances in neural information processing systems. 201932</p>
<p>Discovering invariant rationales for graph neural networks. Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, Tat-Seng Chua, International Conference on Learning Representations. 2022</p>
<p>Graph neural architecture search under distribution shifts. Yijian Qin, Xin Wang, Ziwei Zhang, Pengtao Xie, Wenwu Zhu, International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, Baltimore, Maryland, USAPMLR17-23 July 2022. 2022162of Proceedings of Machine Learning Research</p>
<p>A systematic survey on deep generative models for graph generation. Xiaojie Guo, Liang Zhao, 10.1109/TPAMI.2022.3214832IEEE Trans. Pattern Anal. Mach. Intell. 4552023</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in neural information processing systems. 202033</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Explanations as features: Llm-based features for text-attributed graphs. Xiaoxin He, Xavier Bresson, Thomas Laurent, Bryan Hooi, arXiv:2305.195232023arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang, arXiv:2307.033932023arXiv preprint</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, arXiv:2305.150662023arXiv preprint</p>
<p>Llm4dyg: Can large language models solve problems on dynamic graphs?. Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, Wenwu Zhu, arXiv:2310.171102023barXiv preprint</p>
<p>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. Jiawei Zhang, arXiv:2304.111162023arXiv preprint</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.09645Structgpt: A general framework for large language model to reason over structured data. 2023arXiv preprint</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.083022023arXiv preprint</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, 10.48550/arXiv.2305.100372023b</p>
<p>Graphgen-redux: a fast and lightweight recurrent model for labeled graph generation. Davide Bacciu, Marco Podda, 10.1109/IJCNN52387.2021.9533743International Joint Conference on Neural Networks, IJCNN 2021. Shenzhen, ChinaIEEEJuly 18-22, 20212021</p>
<p>Interpretable Molecular Graph Generation via Monotonic Constraints. Yuanqi Du, Xiaojie Guo, Amarda Shehu, Liang Zhao, 10.1137/1.9781611977172.9</p>
<p>Junction tree variational autoencoder for molecular graph generation. Wengong Jin, Regina Barzilay, Tommi S Jaakkola, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. Jennifer G Dy, Andreas Krause, the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLRJuly 10-15, 2018. 201880of Proceedings of Machine Learning Research</p>
<p>Moflow: An invertible flow model for generating molecular graphs. Chengxi Zang, Fei Wang, 10.1145/3394486.3403104KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event. Rajesh Gupta, Yan Liu, Jiliang Tang, B Aditya Prakash, CA, USAACMAugust 23-27, 2020. 2020</p>
<p>Graphdf: A discrete flow model for molecular graph generation. Youzhi Luo, Keqiang Yan, Shuiwang Ji, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>Multi-motifgan (MMGAN): motiftargeted graph generation and prediction. Anuththari Gamage, Eli Chien, Jianhao Peng, Olgica Milenkovic, 10.1109/ICASSP40776.2020.90534512020 IEEE International Conference on Acoustics, Speech and Signal Processing. Barcelona, SpainIEEEMay 4-8, 2020. 20202020</p>
<p>Molcyclegan: a generative model for molecular optimization. Lukasz Maziarka, Agnieszka Pocha, Jan Kaczmarczyk, Krzysztof Rataj, Tomasz Danel, Michal Warchol, 10.1186/s13321-019-0404-1J. Cheminformatics. 1212020</p>
<p>Score-based generative modeling of graphs via the system of stochastic differential equations. Jaehyeong Jo, Seul Lee, Sung Ju Hwang, International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, Baltimore, Maryland, USAPMLRJuly 2022. 2022162of Proceedings of Machine Learning Research</p>
<p>Digress: Discrete denoising diffusion for graph generation. Clément Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, Pascal Frossard, 100.0 ± 0.0 100.0 ± 0.0 56.0 ± 4.5 100.0 ± 0.0 100.0 ± 0.0The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. OpenReview.net, 2023Zero-shot 84.7 ± 5.9 90.0 ± 3.1 100.0 ± 0.0 74.7 ± 9</p>
<p>Unique" denotes fractions of generated graphs that are not duplicates, and "Novel" denotes fractions of generated graphs that are different from the given example graphs. Table. 16Metrics about the comparison of rule-based graph generation using different LLMs</p>
<p>LLama2 Zero-shot. </p>
<p>Zero-shot+CoT. </p>
<p>Unique" denotes fractions of generated graphs that are not duplicates, and "Novel" denotes fractions of generated graphs that are different from the given example graphs. Table. 17The effect of different sampling temperatures on rule-based graph generation with GPT-4</p>            </div>
        </div>

    </div>
</body>
</html>
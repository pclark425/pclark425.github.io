<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6622 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6622</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6622</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-fd48dc6b433cfcd220b6e769c6179d8ef6fcf862</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fd48dc6b433cfcd220b6e769c6179d8ef6fcf862" target="_blank">LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This study introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention, and proposes several memory design optimizations including session decomposition for value granularity, fact-augmented key expansion for indexing, and time-aware query expansion for refining the search scope.</p>
                <p><strong>Paper Abstract:</strong> Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. We introduce LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing a 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into three stages: indexing, retrieval, and reading. Built upon key experimental insights, we propose several memory design optimizations including session decomposition for value granularity, fact-augmented key expansion for indexing, and time-aware query expansion for refining the search scope. Extensive experiments show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI. Our benchmark and code are publicly available at https://github.com/xiaowu0162/LongMemEval.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6622.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6622.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI memory-augmented chat assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial chat assistant offering a web-accessible user memory feature that stores user facts across sessions; evaluated in an online, session-by-session human study in this paper and compared to offline full-context reading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT (OpenAI memory feature)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Commercial chat assistant that exposes a memory feature via a web interface which stores and recalls user facts across sessions; memory implementation details are proprietary/unknown but the system is observed to record facts and to compress/overwrite stored memory as interactions continue.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o / GPT-4o-mini (as used in study)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>proprietary persistent user-facts store (external memory managed by service)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>structured user facts (as maintained by the service, not directly exposed); effectively key-value like stored facts</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>service-managed read/write memory via web interface (implementation details not disclosed); retrieval presumably via internal indexing/compression pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LONGMEMEVAL (short interactive histories study, subset of 97 questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-term conversational memory / QA over chat history</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Accuracy 0.5773 (GPT-4o, online memory interaction over short histories of 3-6 sessions; reported in Fig.3a)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Offline reading baseline (same LLM prompted with complete history) Accuracy 0.9184 (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (binary yes/no judged by GPT-4o evaluator / human annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Observed trade-off between compression/efficiency and fidelity: ChatGPT often compresses and overwrites earlier stored facts during continued interaction, yielding efficiency at the cost of losing previously recorded details.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Tends to overwrite or lose crucial earlier information as chat continues; weaker at aggregating across multiple sessions and at recalling indirectly provided information; performance much lower than oracle/offline reading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu. 2024. LONGMEMEVAL: Benchmarking Chat Assistants on Long-Term Interactive Memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6622.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coze (memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coze (Coze memory-augmented chat assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Commercial assistant with a memory feature that stores user facts; evaluated in an online human study and compared against offline full-context reading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Coze (memory interface)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Commercial chat assistant that exposes a memory feature storing facts about the user; details of indexing and retrieval are not disclosed, evaluated via human interaction in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-turbo / GPT-4o (as used in study)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>proprietary persistent fact store (external)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored user facts (service-managed), exact representation not disclosed</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>service-managed read/write through web interface; internal retrieval policy unknown</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LONGMEMEVAL (short interactive histories study, subset of 97 questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-term conversational memory / QA over chat history</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Accuracy 0.3299 (Coze with GPT-4o, online memory interaction over short histories)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Offline reading baseline (GPT-4o reading full history) Accuracy 0.9184</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Observed tendency to not record indirectly provided user information; suggests a trade-off where certain memory write heuristics prioritize direct facts and miss implicit ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Often fails to record indirectly provided user information, leading to poor recall in multi-session/multi-evidence tasks; significantly lower performance than offline oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu. 2024. LONGMEMEVAL: Benchmarking Chat Assistants on Long-Term Interactive Memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6622.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Our RAG design</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long-term memory RAG design (this paper's implemented system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented chat assistant pipeline formalized as a key-value datastore (indexing, retrieval, reading) using session->round decomposition, key expansion with extracted facts, time-aware query expansion, dense retrieval (Stella V5), and Chain-of-Note + structured reading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LongMemEval RAG pipeline (indexing+retrieval+reading)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory-augmented assistant that sequentially indexes each session (converted into rounds/facts), stores key-value items (keys = value + extracted facts), retrieves with a dense retriever (Stella V5) optionally filtered by time ranges, and reads with an LLM reader using Chain-of-Note and structured json prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reader: GPT-4o; Llama 3.1 (70B, 8B) used for indexing and reading experiments; Retriever: Stella V5 (1.5B embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store / retrieval-augmented generation (RAG) key-value datastore</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>key-value pairs where value = session or round text (user-side utterances) and keys = original value optionally concatenated with extracted facts, summaries, or keyphrases; timestamped events also indexed</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>dense similarity search (Stella V5 embeddings) over keys; document expansion (K = V + fact) used at indexing time; time-aware filtering using an LLM (M_T) that extracts time ranges from queries; reading uses LLM with Chain-of-Note and structured JSON prompting</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LONGMEMEVAL (LONGMEMEVAL_S ~115k tokens per problem; LONGMEMEVAL_M 500 sessions ~1.5M tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-term conversational memory / retrieval-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Examples: With Value=Round and Key=V+fact, Retrieval Recall@5 = 0.644 (vs 0.582 for K=V); End-to-end QA (GPT-4o) Top-5 = 0.657 (vs 0.615 for K=V). Paper reports average +9.4% recall@k and +5.4% final accuracy from key expansion with user facts across models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>As a comparison, long-context offline 'oracle' (answer with only evidence sessions as context) yields higher accuracy (e.g., GPT-4o oracle 0.870) while reading full long history without retrieval yields much lower accuracy (e.g., GPT-4o reading full history S=0.606); the paper frames the memory pipeline as enabling better scaling vs naive full-history reading.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Recall@k, NDCG@k for retrieval; Accuracy (end-to-end QA) reported as Top-5/Top-10 correctness</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Design trade-offs include: granularity vs information loss (sessions vs rounds vs compressed facts), token-budget vs reader capacity (Llama 3.1 8B performance drops after ~3k retrieved tokens while GPT-4o benefits from >20k), indexing-time cost of multi-key expansion, dependence on a strong LLM for time-range extraction, and potential increased storage/search costs from document expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Compressing into summaries/facts often harms QA (information loss), time-agnostic retrieval performs poorly on temporal questions, retrieval improvements do not guarantee correct utilization by reader (requires Chain-of-Note/structured prompting), and performance still substantially lower than oracle when reader must process very long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu. 2024. LONGMEMEVAL: Benchmarking Chat Assistants on Long-Term Interactive Memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6622.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoN (Chain-of-Note)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Note</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reading/decoding strategy where an LLM extracts and chains intermediate notes (key information) from retrieved memory items before producing a final answer; used here as a reading strategy to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-note: Enhancing robustness in retrieval-augmented language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Chain-of-Note (CoN) reading strategy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A stepwise reading/answering strategy in which the reader LLM first extracts key points (notes) from retrieved memory items, chains reasoning steps, and then produces the final response to improve utilization of retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Applied with readers such as GPT-4o and Llama 3.1 (70B/8B) in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reading strategy applied on retrieved items from an external vector store (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>operates on retrieved text passages / key-value items</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>applies an intermediate reasoning/extraction pass over retrieved items before final answer generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LONGMEMEVAL (end-to-end QA reading stage)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval-augmented QA / reading strategy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper reports Chain-of-Note + structured format improves question answering accuracy by as much as 10 absolute points across three LLMs; shown to increase oracle reading accuracy in Table 3/figure discussions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to direct reading of retrieved items (no Chain-of-Note) the Chain-of-Note reader yields substantially higher end-to-end accuracy (up to +10 percentage points reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (end-to-end QA)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Extra intermediate reasoning steps add latency and token usage; requires good prompting and may increase compute per query.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even with perfect retrieval, correct utilization of many retrieved items remains non-trivial; Chain-of-Note mitigates this but does not fully eliminate failure modes when retrieved evidence is noisy or conflicting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6622.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system that stores compressed memory summaries/facts to enhance LLM personalization; referenced and compared conceptually in the unified framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory-augmented system that compresses history into summaries/facts and indexes them for retrieval to support personalized responses; listed as an instantiation in the paper's unified framework (value=summary+round, keys=values, time-aware yes).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (original MemoryBank paper has its own experimental setup)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>compressed summaries/facts indexed for retrieval (RAG-style)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>summaries + rounds (text summaries/facts stored as keys/values)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>flat retrieval (similarity search) over indexed summaries; includes some time-awareness</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced in relation to long-term dialogue benchmarks (e.g., MemoryBank evaluation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-term conversational memory / personalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes that compressing into summaries/facts (as MemoryBank does) risks information loss for detailed QA; this trade-off is discussed in the context of value granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Compression can harm ability to answer detailed questions; not all memory designs generalize to heavy multi-session reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6622.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context Retrieval-Augmented Generation (In-context RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented approach where retrieved rounds/sessions are inserted in-context for the LLM to read; listed as a baseline framework in the unified view.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>In-context RAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RAG approach that uses retrieved session/round texts directly as in-context inputs to the LLM reader (key = value, flat retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (method-level, not a single model)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrievable text segments / RAG</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw session/round text used as key/value</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>flat retrieval (similarity search) and in-context insertion</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced as a memory design option applicable to LONGMEMEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval-augmented QA / long-context reading</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Inefficient for very long histories; susceptible to 'lost-in-the-middle' when inserted in exceedingly long context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As history length grows, in-context insertion becomes inefficient and LLMs may fail to effectively use distant context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Shi et al. 2024 (referenced as In-context RAG in the paper's unified table)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6622.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LD-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LD-Agent (Hello again! llm-powered personalized agent for long-term dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed LLM-powered personalized agent that uses summaries plus facts as values and keyphrase-based keys; included in the unified framework table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hello again! llm-powered personalized agent for long-term dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LD-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Personalized agent design that compresses interactions into summaries and facts for indexing, using keyphrases for retrieval and supporting time-aware retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified here (method-level reference)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>compressed fact/summary store (RAG-style)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>summary + fact representations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>keyphrase-based indexing and flat retrieval; time-aware</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced among long-term memory system designs</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-term dialogue personalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Compression vs fidelity trade-offs similar to other summary/fact-based systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Potential information loss from compression; design choices can impact recall on detailed QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua. 2024. Hello again! llm-powered personalized agent for long-term dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6622.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAPTOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval system that performs recursive abstractive processing over tree-organized retrieval structures; referenced in the unified framework comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RAPTOR: recursive abstractive processing for tree-organized retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAPTOR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval framework that organizes documents/nodes into tree structures and performs recursive abstractive processing; listed as an alternative retrieval/processing mechanism in the unified table.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured retrieval (tree-organized) and abstractive processing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>node summaries / tree nodes</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>interactive or recursive retrieval; can combine ranks from different nodes</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced as a memory/retrieval architecture</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / structured retrieval for long-context tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Complex retrieval structures introduce processing overhead and complexity in merging evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not directly evaluated in this paper; included for comparative design discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning. 2024. RAPTOR: recursive abstractive processing for tree-organized retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6622.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemWalker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemWalker / Walking down the memory maze (interactive reading)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive reading-style system that iteratively queries and reads memory nodes rather than flat retrieval; included in the unified comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Walking down the memory maze: Beyond context limit through interactive reading.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemWalker / Interactive reading systems</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Interactive retrieval/reading approach that navigates memory nodes and dynamically queries subparts rather than retrieving a flat list; listed as interactive retrieval instantiation in the unified table.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>interactive node-based memory with iterative retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>node summaries/segments arranged in graph/tree</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>interactive retrieval (iterative exploration of memory nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced as a memory architecture applicable to long-context tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>interactive retrieval / long-context reading</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Interactive approaches may reduce input length to reader but add latency and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not directly benchmarked in this paper; referenced for architecture contrast.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Tong Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz. 2023. Walking down the memory maze: Beyond context limit through interactive reading.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6622.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6622.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HippoRAG: Neurobiologically inspired long-term memory for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory architecture inspired by neurobiological principles combining entity-based indexing and personalized retrieval; cited as a related design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hipporag: Neurobiologically inspired long-term memory for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Long-term memory design that uses entity-based indexing and personalized retrieval strategies (PPR) as an alternative to flat document retrieval; included in the unified design table.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>entity-indexed retrievable memory with PPR (personalized PageRank-style retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>entity-focused values/nodes</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>entity-based retrieval (PPR) and direct reading</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced as a design approach for long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-term memory / structured retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Entity-based indexing can improve targeted retrieval but may require entity extraction and graph construction overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in LONGMEMEVAL in this paper; mentioned for comparative purposes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su. 2024. Hipporag: Neurobiologically inspired long-term memory for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorybank: Enhancing large language models with long-term memory. <em>(Rating: 2)</em></li>
                <li>Chain-of-note: Enhancing robustness in retrieval-augmented language models. <em>(Rating: 2)</em></li>
                <li>RAPTOR: recursive abstractive processing for tree-organized retrieval. <em>(Rating: 2)</em></li>
                <li>Walking down the memory maze: Beyond context limit through interactive reading. <em>(Rating: 2)</em></li>
                <li>Hipporag: Neurobiologically inspired long-term memory for large language models. <em>(Rating: 2)</em></li>
                <li>Hello again! llm-powered personalized agent for long-term dialogue. <em>(Rating: 1)</em></li>
                <li>In-context RAG (Shi et al. 2024) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6622",
    "paper_id": "paper-fd48dc6b433cfcd220b6e769c6179d8ef6fcf862",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "ChatGPT (memory)",
            "name_full": "ChatGPT (OpenAI memory-augmented chat assistant)",
            "brief_description": "A commercial chat assistant offering a web-accessible user memory feature that stores user facts across sessions; evaluated in an online, session-by-session human study in this paper and compared to offline full-context reading.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ChatGPT (OpenAI memory feature)",
            "agent_description": "Commercial chat assistant that exposes a memory feature via a web interface which stores and recalls user facts across sessions; memory implementation details are proprietary/unknown but the system is observed to record facts and to compress/overwrite stored memory as interactions continue.",
            "model_size": "GPT-4o / GPT-4o-mini (as used in study)",
            "memory_used": true,
            "memory_type": "proprietary persistent user-facts store (external memory managed by service)",
            "memory_representation": "structured user facts (as maintained by the service, not directly exposed); effectively key-value like stored facts",
            "memory_access_mechanism": "service-managed read/write memory via web interface (implementation details not disclosed); retrieval presumably via internal indexing/compression pipeline",
            "task_name": "LONGMEMEVAL (short interactive histories study, subset of 97 questions)",
            "task_category": "long-term conversational memory / QA over chat history",
            "performance_with_memory": "Accuracy 0.5773 (GPT-4o, online memory interaction over short histories of 3-6 sessions; reported in Fig.3a)",
            "performance_without_memory": "Offline reading baseline (same LLM prompted with complete history) Accuracy 0.9184 (GPT-4o)",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (binary yes/no judged by GPT-4o evaluator / human annotators)",
            "tradeoffs_reported": "Observed trade-off between compression/efficiency and fidelity: ChatGPT often compresses and overwrites earlier stored facts during continued interaction, yielding efficiency at the cost of losing previously recorded details.",
            "limitations_or_failure_cases": "Tends to overwrite or lose crucial earlier information as chat continues; weaker at aggregating across multiple sessions and at recalling indirectly provided information; performance much lower than oracle/offline reading.",
            "citation": "Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu. 2024. LONGMEMEVAL: Benchmarking Chat Assistants on Long-Term Interactive Memory.",
            "uuid": "e6622.0",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Coze (memory)",
            "name_full": "Coze (Coze memory-augmented chat assistant)",
            "brief_description": "Commercial assistant with a memory feature that stores user facts; evaluated in an online human study and compared against offline full-context reading.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Coze (memory interface)",
            "agent_description": "Commercial chat assistant that exposes a memory feature storing facts about the user; details of indexing and retrieval are not disclosed, evaluated via human interaction in the paper.",
            "model_size": "GPT-3.5-turbo / GPT-4o (as used in study)",
            "memory_used": true,
            "memory_type": "proprietary persistent fact store (external)",
            "memory_representation": "stored user facts (service-managed), exact representation not disclosed",
            "memory_access_mechanism": "service-managed read/write through web interface; internal retrieval policy unknown",
            "task_name": "LONGMEMEVAL (short interactive histories study, subset of 97 questions)",
            "task_category": "long-term conversational memory / QA over chat history",
            "performance_with_memory": "Accuracy 0.3299 (Coze with GPT-4o, online memory interaction over short histories)",
            "performance_without_memory": "Offline reading baseline (GPT-4o reading full history) Accuracy 0.9184",
            "has_comparative_results": true,
            "performance_metric": "Accuracy",
            "tradeoffs_reported": "Observed tendency to not record indirectly provided user information; suggests a trade-off where certain memory write heuristics prioritize direct facts and miss implicit ones.",
            "limitations_or_failure_cases": "Often fails to record indirectly provided user information, leading to poor recall in multi-session/multi-evidence tasks; significantly lower performance than offline oracle.",
            "citation": "Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu. 2024. LONGMEMEVAL: Benchmarking Chat Assistants on Long-Term Interactive Memory.",
            "uuid": "e6622.1",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Our RAG design",
            "name_full": "Long-term memory RAG design (this paper's implemented system)",
            "brief_description": "A retrieval-augmented chat assistant pipeline formalized as a key-value datastore (indexing, retrieval, reading) using session-&gt;round decomposition, key expansion with extracted facts, time-aware query expansion, dense retrieval (Stella V5), and Chain-of-Note + structured reading.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LongMemEval RAG pipeline (indexing+retrieval+reading)",
            "agent_description": "Memory-augmented assistant that sequentially indexes each session (converted into rounds/facts), stores key-value items (keys = value + extracted facts), retrieves with a dense retriever (Stella V5) optionally filtered by time ranges, and reads with an LLM reader using Chain-of-Note and structured json prompts.",
            "model_size": "Reader: GPT-4o; Llama 3.1 (70B, 8B) used for indexing and reading experiments; Retriever: Stella V5 (1.5B embeddings).",
            "memory_used": true,
            "memory_type": "external vector store / retrieval-augmented generation (RAG) key-value datastore",
            "memory_representation": "key-value pairs where value = session or round text (user-side utterances) and keys = original value optionally concatenated with extracted facts, summaries, or keyphrases; timestamped events also indexed",
            "memory_access_mechanism": "dense similarity search (Stella V5 embeddings) over keys; document expansion (K = V + fact) used at indexing time; time-aware filtering using an LLM (M_T) that extracts time ranges from queries; reading uses LLM with Chain-of-Note and structured JSON prompting",
            "task_name": "LONGMEMEVAL (LONGMEMEVAL_S ~115k tokens per problem; LONGMEMEVAL_M 500 sessions ~1.5M tokens)",
            "task_category": "long-term conversational memory / retrieval-augmented QA",
            "performance_with_memory": "Examples: With Value=Round and Key=V+fact, Retrieval Recall@5 = 0.644 (vs 0.582 for K=V); End-to-end QA (GPT-4o) Top-5 = 0.657 (vs 0.615 for K=V). Paper reports average +9.4% recall@k and +5.4% final accuracy from key expansion with user facts across models.",
            "performance_without_memory": "As a comparison, long-context offline 'oracle' (answer with only evidence sessions as context) yields higher accuracy (e.g., GPT-4o oracle 0.870) while reading full long history without retrieval yields much lower accuracy (e.g., GPT-4o reading full history S=0.606); the paper frames the memory pipeline as enabling better scaling vs naive full-history reading.",
            "has_comparative_results": true,
            "performance_metric": "Recall@k, NDCG@k for retrieval; Accuracy (end-to-end QA) reported as Top-5/Top-10 correctness",
            "tradeoffs_reported": "Design trade-offs include: granularity vs information loss (sessions vs rounds vs compressed facts), token-budget vs reader capacity (Llama 3.1 8B performance drops after ~3k retrieved tokens while GPT-4o benefits from &gt;20k), indexing-time cost of multi-key expansion, dependence on a strong LLM for time-range extraction, and potential increased storage/search costs from document expansion.",
            "limitations_or_failure_cases": "Compressing into summaries/facts often harms QA (information loss), time-agnostic retrieval performs poorly on temporal questions, retrieval improvements do not guarantee correct utilization by reader (requires Chain-of-Note/structured prompting), and performance still substantially lower than oracle when reader must process very long contexts.",
            "citation": "Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu. 2024. LONGMEMEVAL: Benchmarking Chat Assistants on Long-Term Interactive Memory.",
            "uuid": "e6622.2",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CoN (Chain-of-Note)",
            "name_full": "Chain-of-Note",
            "brief_description": "A reading/decoding strategy where an LLM extracts and chains intermediate notes (key information) from retrieved memory items before producing a final answer; used here as a reading strategy to improve robustness.",
            "citation_title": "Chain-of-note: Enhancing robustness in retrieval-augmented language models.",
            "mention_or_use": "use",
            "agent_name": "Chain-of-Note (CoN) reading strategy",
            "agent_description": "A stepwise reading/answering strategy in which the reader LLM first extracts key points (notes) from retrieved memory items, chains reasoning steps, and then produces the final response to improve utilization of retrieved evidence.",
            "model_size": "Applied with readers such as GPT-4o and Llama 3.1 (70B/8B) in experiments",
            "memory_used": true,
            "memory_type": "reading strategy applied on retrieved items from an external vector store (RAG)",
            "memory_representation": "operates on retrieved text passages / key-value items",
            "memory_access_mechanism": "applies an intermediate reasoning/extraction pass over retrieved items before final answer generation",
            "task_name": "LONGMEMEVAL (end-to-end QA reading stage)",
            "task_category": "retrieval-augmented QA / reading strategy",
            "performance_with_memory": "Paper reports Chain-of-Note + structured format improves question answering accuracy by as much as 10 absolute points across three LLMs; shown to increase oracle reading accuracy in Table 3/figure discussions.",
            "performance_without_memory": "Compared to direct reading of retrieved items (no Chain-of-Note) the Chain-of-Note reader yields substantially higher end-to-end accuracy (up to +10 percentage points reported).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (end-to-end QA)",
            "tradeoffs_reported": "Extra intermediate reasoning steps add latency and token usage; requires good prompting and may increase compute per query.",
            "limitations_or_failure_cases": "Even with perfect retrieval, correct utilization of many retrieved items remains non-trivial; Chain-of-Note mitigates this but does not fully eliminate failure modes when retrieved evidence is noisy or conflicting.",
            "citation": "Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models.",
            "uuid": "e6622.3",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank: Enhancing large language models with long-term memory",
            "brief_description": "A prior system that stores compressed memory summaries/facts to enhance LLM personalization; referenced and compared conceptually in the unified framework.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory.",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank",
            "agent_description": "Memory-augmented system that compresses history into summaries/facts and indexes them for retrieval to support personalized responses; listed as an instantiation in the paper's unified framework (value=summary+round, keys=values, time-aware yes).",
            "model_size": "Not specified in this paper (original MemoryBank paper has its own experimental setup)",
            "memory_used": true,
            "memory_type": "compressed summaries/facts indexed for retrieval (RAG-style)",
            "memory_representation": "summaries + rounds (text summaries/facts stored as keys/values)",
            "memory_access_mechanism": "flat retrieval (similarity search) over indexed summaries; includes some time-awareness",
            "task_name": "Referenced in relation to long-term dialogue benchmarks (e.g., MemoryBank evaluation tasks)",
            "task_category": "long-term conversational memory / personalization",
            "performance_with_memory": "",
            "performance_without_memory": "",
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Paper notes that compressing into summaries/facts (as MemoryBank does) risks information loss for detailed QA; this trade-off is discussed in the context of value granularity.",
            "limitations_or_failure_cases": "Compression can harm ability to answer detailed questions; not all memory designs generalize to heavy multi-session reasoning.",
            "citation": "Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory.",
            "uuid": "e6622.4",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "In-context RAG",
            "name_full": "In-context Retrieval-Augmented Generation (In-context RAG)",
            "brief_description": "A retrieval-augmented approach where retrieved rounds/sessions are inserted in-context for the LLM to read; listed as a baseline framework in the unified view.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "In-context RAG",
            "agent_description": "RAG approach that uses retrieved session/round texts directly as in-context inputs to the LLM reader (key = value, flat retrieval).",
            "model_size": "various (method-level, not a single model)",
            "memory_used": true,
            "memory_type": "retrievable text segments / RAG",
            "memory_representation": "raw session/round text used as key/value",
            "memory_access_mechanism": "flat retrieval (similarity search) and in-context insertion",
            "task_name": "Referenced as a memory design option applicable to LONGMEMEVAL",
            "task_category": "retrieval-augmented QA / long-context reading",
            "performance_with_memory": "",
            "performance_without_memory": "",
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Inefficient for very long histories; susceptible to 'lost-in-the-middle' when inserted in exceedingly long context.",
            "limitations_or_failure_cases": "As history length grows, in-context insertion becomes inefficient and LLMs may fail to effectively use distant context.",
            "citation": "Shi et al. 2024 (referenced as In-context RAG in the paper's unified table)",
            "uuid": "e6622.5",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LD-Agent",
            "name_full": "LD-Agent (Hello again! llm-powered personalized agent for long-term dialogue)",
            "brief_description": "A previously proposed LLM-powered personalized agent that uses summaries plus facts as values and keyphrase-based keys; included in the unified framework table.",
            "citation_title": "Hello again! llm-powered personalized agent for long-term dialogue.",
            "mention_or_use": "mention",
            "agent_name": "LD-Agent",
            "agent_description": "Personalized agent design that compresses interactions into summaries and facts for indexing, using keyphrases for retrieval and supporting time-aware retrieval.",
            "model_size": "Not specified here (method-level reference)",
            "memory_used": true,
            "memory_type": "compressed fact/summary store (RAG-style)",
            "memory_representation": "summary + fact representations",
            "memory_access_mechanism": "keyphrase-based indexing and flat retrieval; time-aware",
            "task_name": "Referenced among long-term memory system designs",
            "task_category": "long-term dialogue personalization",
            "performance_with_memory": "",
            "performance_without_memory": "",
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Compression vs fidelity trade-offs similar to other summary/fact-based systems.",
            "limitations_or_failure_cases": "Potential information loss from compression; design choices can impact recall on detailed QA.",
            "citation": "Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua. 2024. Hello again! llm-powered personalized agent for long-term dialogue.",
            "uuid": "e6622.6",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RAPTOR",
            "name_full": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
            "brief_description": "A retrieval system that performs recursive abstractive processing over tree-organized retrieval structures; referenced in the unified framework comparison table.",
            "citation_title": "RAPTOR: recursive abstractive processing for tree-organized retrieval.",
            "mention_or_use": "mention",
            "agent_name": "RAPTOR",
            "agent_description": "Retrieval framework that organizes documents/nodes into tree structures and performs recursive abstractive processing; listed as an alternative retrieval/processing mechanism in the unified table.",
            "model_size": "Not specified in this paper",
            "memory_used": true,
            "memory_type": "structured retrieval (tree-organized) and abstractive processing",
            "memory_representation": "node summaries / tree nodes",
            "memory_access_mechanism": "interactive or recursive retrieval; can combine ranks from different nodes",
            "task_name": "Referenced as a memory/retrieval architecture",
            "task_category": "retrieval / structured retrieval for long-context tasks",
            "performance_with_memory": "",
            "performance_without_memory": "",
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Complex retrieval structures introduce processing overhead and complexity in merging evidence.",
            "limitations_or_failure_cases": "Not directly evaluated in this paper; included for comparative design discussion.",
            "citation": "Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning. 2024. RAPTOR: recursive abstractive processing for tree-organized retrieval.",
            "uuid": "e6622.7",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MemWalker",
            "name_full": "MemWalker / Walking down the memory maze (interactive reading)",
            "brief_description": "An interactive reading-style system that iteratively queries and reads memory nodes rather than flat retrieval; included in the unified comparison.",
            "citation_title": "Walking down the memory maze: Beyond context limit through interactive reading.",
            "mention_or_use": "mention",
            "agent_name": "MemWalker / Interactive reading systems",
            "agent_description": "Interactive retrieval/reading approach that navigates memory nodes and dynamically queries subparts rather than retrieving a flat list; listed as interactive retrieval instantiation in the unified table.",
            "model_size": "Not specified in this paper",
            "memory_used": true,
            "memory_type": "interactive node-based memory with iterative retrieval",
            "memory_representation": "node summaries/segments arranged in graph/tree",
            "memory_access_mechanism": "interactive retrieval (iterative exploration of memory nodes)",
            "task_name": "Referenced as a memory architecture applicable to long-context tasks",
            "task_category": "interactive retrieval / long-context reading",
            "performance_with_memory": "",
            "performance_without_memory": "",
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Interactive approaches may reduce input length to reader but add latency and complexity.",
            "limitations_or_failure_cases": "Not directly benchmarked in this paper; referenced for architecture contrast.",
            "citation": "Tong Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz. 2023. Walking down the memory maze: Beyond context limit through interactive reading.",
            "uuid": "e6622.8",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HippoRAG",
            "name_full": "HippoRAG: Neurobiologically inspired long-term memory for large language models",
            "brief_description": "A memory architecture inspired by neurobiological principles combining entity-based indexing and personalized retrieval; cited as a related design.",
            "citation_title": "Hipporag: Neurobiologically inspired long-term memory for large language models.",
            "mention_or_use": "mention",
            "agent_name": "HippoRAG",
            "agent_description": "Long-term memory design that uses entity-based indexing and personalized retrieval strategies (PPR) as an alternative to flat document retrieval; included in the unified design table.",
            "model_size": "Not specified in this paper",
            "memory_used": true,
            "memory_type": "entity-indexed retrievable memory with PPR (personalized PageRank-style retrieval)",
            "memory_representation": "entity-focused values/nodes",
            "memory_access_mechanism": "entity-based retrieval (PPR) and direct reading",
            "task_name": "Referenced as a design approach for long-term memory",
            "task_category": "long-term memory / structured retrieval",
            "performance_with_memory": "",
            "performance_without_memory": "",
            "has_comparative_results": false,
            "performance_metric": "",
            "tradeoffs_reported": "Entity-based indexing can improve targeted retrieval but may require entity extraction and graph construction overhead.",
            "limitations_or_failure_cases": "Not evaluated in LONGMEMEVAL in this paper; mentioned for comparative purposes.",
            "citation": "Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su. 2024. Hipporag: Neurobiologically inspired long-term memory for large language models.",
            "uuid": "e6622.9",
            "source_info": {
                "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory.",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-note: Enhancing robustness in retrieval-augmented language models.",
            "rating": 2
        },
        {
            "paper_title": "RAPTOR: recursive abstractive processing for tree-organized retrieval.",
            "rating": 2
        },
        {
            "paper_title": "Walking down the memory maze: Beyond context limit through interactive reading.",
            "rating": 2
        },
        {
            "paper_title": "Hipporag: Neurobiologically inspired long-term memory for large language models.",
            "rating": 2
        },
        {
            "paper_title": "Hello again! llm-powered personalized agent for long-term dialogue.",
            "rating": 1
        },
        {
            "paper_title": "In-context RAG (Shi et al. 2024)",
            "rating": 1
        }
    ],
    "cost": 0.02075225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LONGMEMEVAL: Benchmarking Chat Assistants on Long-Term Interactive Memory</h1>
<p>Di Wu ${ }^{1 <em>}$, Hongwei Wang ${ }^{2}$, Wenhao Yu ${ }^{2}$, Yuwei Zhang ${ }^{4 </em>}$, Kai-Wei Chang ${ }^{1}$, Dong Yu ${ }^{2}$<br>${ }^{1}$ UCLA, ${ }^{2}$ Tencent AI Lab Seattle, ${ }^{3}$ UC San Diego<br>{diwu, kwchang}@cs.ucla.edu</p>
<h4>Abstract</h4>
<p>Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. We introduce LONGMEMEVAL, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LONGMEMEVAL presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing a $30 \%$ accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into three stages: indexing, retrieval, and reading. Built upon key experimental insights, we propose several memory design optimizations including session decomposition for value granularity, fact-augmented key expansion for indexing, and time-aware query expansion for refining the search scope. Extensive experiments show that these optimizations greatly improve both memory recall and downstream question answering on LONGMEMEVAL. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI. Our benchmark and code are publicly available at https://github.com/xiaowu0162/LongMemEval.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have exhibited impressive capabilities in solving diverse tasks through natural language, leading to numerous successful chat assistant applications (OpenAI, 2022; Microsoft, 2023). Nevertheless, LLMs face limitations on tasks relying heavily on personal knowledge accumulated through long-term user-AI interactions, such as psychological counseling or secretarial duties (Zhong et al., 2024). Failing to incorporate user background and preferences into responses can diminish the response's accuracy as well as user satisfaction. To personalize LLM-based assistants, long-term memory, the ability to memorize, recall, and reason with a long interaction history, is indispensable. Recently, several commercial (OpenAI, 2024; Coze, 2024) and open-source assistant systems with memory (Zhong et al., 2024; Zhang et al., 2024) have been introduced. These systems leverage techniques like compressing, indexing, and retrieving from chat histories to generate more accurate and personalized responses.</p>
<p>Despite these advances, there has been limited progress in holistically evaluating the memory capability in long-term interactions. While several benchmarks evaluate LLMs on understanding long chat histories (Xu et al., 2022a;b; Zhong et al., 2024; Maharana et al., 2024; Du et al., 2024; Kim et al., 2024), they have two major shortcomings. First, they do not accurately reflect user-AI interactions: many focus solely on human-human conversations (Xu et al., 2022a; Maharana et al., 2024; Kim et al., 2024), while others omit task-oriented dialogues, which represent a significant</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>portion of chat assistant usage and challenge memorization with the long-context inputs and longform responses. Their interactive histories also typically have a non-configurable length spanning only a few thousand tokens, limiting the difficulty as current systems continue to improve. Second, current benchmarks' questions only offer a limited coverage of the memory abilities required in dynamic long-term interactions. For instance, MemoryBank (Zhong et al., 2024) and PerLTQA (Du et al., 2024) insufficiently evaluate the ability to synthesize information across numerous sessions or to reason with temporal metadata or time references. All long-term memory benchmarks including recent ones such as LoCoMo (Maharana et al., 2024) also fail to evaluate recall of information provided by the assistant or reasoning with updated user information.</p>
<p>We introduce LONGMEMEval, a comprehensive benchmark for assessing the long-term memory capabilities of chat assistants. LongMEMEval consists of 500 manually created questions to test five core memory abilities: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. Each question requires recalling information hidden within one or more task-oriented dialogues between a user and an assistant. Inspired by the "needle-in-a-haystack" test (Kamradt, 2023), we design a pipeline to compile a coherent and lengthconfigurable chat history for each question. A chat system, then, is required to parse the dynamic interactions online for memorization, and answer the question after all the interaction sessions. While the length of the history is freely extensible, we provide two standard settings for consistent comparison: LONGMEMEvals with approximately 115 k tokens per problem and LONGMEMEvalM with 500 sessions (around 1.5 million tokens). Preliminary evaluations highlight the difficulty of LongMEMEval, as long-context LLMs show a $30 \% \sim 60 \%$ performance drop on LONGMEMÉVALs, and manual evaluations reveal that state-of-the-art commercial systems only achieved $30 \% \sim 70 \%$ accuracy in a setting much simpler than LONGMEMÉVALs (§3.4).</p>
<p>Finally, we present a unified view for memory-augmented chat assistants. Leveraging LongMEMEval, we comprehensively analyze memory design choices across three key execution stages-indexing, retrieval, and reading-and four control points: value, key, query, and reading strategy. Experimental insights identify several effective memory designs:</p>
<ul>
<li>(§5.2) Instead of sessions, round is the more optimal granularity for storing and utilizing the interactive history. While further compression into individual user facts harms overall performance due to information loss, it improves the multi-session reasoning accuracy.</li>
<li>(§5.3) While using a flat index with the memory values themselves as the keys is a strong baseline, further expanding the keys with extracted user facts improves both memory recall ( $9.4 \%$ higher recall@ $k$ ) and downstream question answering ( $5.4 \%$ higher accuracy).</li>
<li>(§5.4) Naive time-agnostic memory designs perform poorly on temporal reasoning questions. We propose a simple indexing and query expansion strategy to explicitly associate timestamps with facts and narrow down the search range, improving the memory recall for temporal reasoning by $6.8 \% \sim 11.3 \%$ when a strong LLM is employed for query expansion.</li>
<li>(§5.5) Even with perfect memory recall, accurately utilizing retrieved items is non-trivial. Applying Chain-of-Note (Yu et al., 2023) and structured data format (Yin et al., 2023) improves question answering accuracy by as much as 10 absolute points across three LLMs.</li>
</ul>
<h1>2 Related Work</h1>
<p>Long-Term Dialogue Benchmarks As the ability of dialogue systems improve, research starts to focus on long-term dialogue understanding beyond traditional dialogue modeling benchmarks (Budzianowski et al., 2018; Wei et al., 2018). Early works focused on language modeling evaluation on generating personalized responses from human-human (Xu et al., 2022a) or human-AI (Xu et al., 2022b) chat histories. To more precisely evaluate memory accuracy, subsequent benchmarks shifted toward question answering (QA, Reddy et al. (2019); Zhang \&amp; Choi (2021)). For example, MemoryBank (Zhong et al., 2024) features multi-day chat histories from 15 users with 194 humanwritten probing questions. LoCoMo (Maharana et al., 2024) includes 50 long-term chat histories and questions testing single-hop, multi-hop, temporal, commonsense, world knowledge, and adversarial reasoning. PerLTQA (Du et al., 2024) scales the evaluation to 3,409 dialogues and 8,593 questions, covering world knowledge, personal profiles, social relationships, events, and dialogue history. DialSim (Kim et al., 2024) evaluates models' memory ability by roleplaying TV show characters</p>
<p>Table 1: A comparison between LONGMEMEVAL and existing long-term memory benchmarks. We use different colors to denote human-human and human-AI dialogue. #Sess and #Q denote the total number of sessions and questions. Context depth is defined as the number of tokens in the history. Finally, we compare the coverage of five core abilities: information extraction (IE), multi-session reasoning (MR), knowledge update (KU), temporal reasoning (TR), and abstaining on unanswerable questions (ABS). ${ }^{<em>}$ Not reported in the paper, based on our approximation. ${ }^{</em> *}$ at most 2 sessions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">#Sess</th>
<th style="text-align: center;">#Q</th>
<th style="text-align: center;">Context Depth</th>
<th style="text-align: center;">Core Memory Abilities</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">IE</td>
<td style="text-align: center;">MR</td>
<td style="text-align: center;">KU</td>
<td style="text-align: center;">TR</td>
</tr>
<tr>
<td style="text-align: center;">MSC (Xu et al., 2022a)</td>
<td style="text-align: center;">Open-Domain</td>
<td style="text-align: center;">5 k</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
</tr>
<tr>
<td style="text-align: center;">DuLeMon (Xu et al., 2022b)</td>
<td style="text-align: center;">Open-Domain</td>
<td style="text-align: center;">30 k</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
</tr>
<tr>
<td style="text-align: center;">MemoryBank (Zhong et al., 2024)</td>
<td style="text-align: center;">Personal</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">194</td>
<td style="text-align: center;">5 k</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">PerLTQA (Du et al., 2024)</td>
<td style="text-align: center;">Personal</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">8593</td>
<td style="text-align: center;">$1 \mathrm{M}^{*}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$X$</td>
</tr>
<tr>
<td style="text-align: center;">LoCoMo (Maharana et al., 2024)</td>
<td style="text-align: center;">Personal</td>
<td style="text-align: center;">1 k</td>
<td style="text-align: center;">7512</td>
<td style="text-align: center;">10 k</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">DialSim (Kim et al., 2024)</td>
<td style="text-align: center;">TV Shows</td>
<td style="text-align: center;">$1 \mathrm{k}-2 \mathrm{k}$</td>
<td style="text-align: center;">1 M</td>
<td style="text-align: center;">350 k</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$X$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">LongMEMEVAL (this work)</td>
<td style="text-align: center;">Personal</td>
<td style="text-align: center;">50 k</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">115 k, 1.5 M</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of the seven question types in LONGMEMEVAL. For each example, we show the associated evidence statements on the left and the question with the answer on the right.
and introduces a time constraint that penalizes slow system responses. Despite these advancements, existing QA-based benchmarks overlook several memory capabilities critical to long-term userassistant interactions: synthesizing information across numerous sessions, recalling assistant side information, and reasoning about updated user details or complex temporal references. Additionally, the chat histories are often too brief and do not reflect the nature of task-oriented interactions. Table 1 compares between LongMEMEVAL and previous works, highlighting its advantages in both (1) featuring a long and freely extensible iterative history and (2) holistically covering critical memory abilities in a uniquely challenging way (further examples in Figure 1).</p>
<p>Long-Term Memory Methods To equip chat assistants with long-term memory capabilities, three major techniques are commonly explored. The first approach involves directly adapting LLMs to process extensive history information as long-context inputs (Beltagy et al., 2020; Kitaev et al., 2020; Fu et al., 2024; An et al., 2024). While this method avoids the need for complex architectures, it is inefficient and susceptible to the "lost-in-the-middle" phenomenon, where the ability of LLMs to utilize contextual information weakens as the input length grows (Shi et al., 2023; Liu et al., 2024). A second line of research integrates differentiable memory modules into language models, proposing specialized architectural designs and training strategies to enhance memory capabilities (Weston et al., 2014; Wu et al., 2022; Zhong et al., 2022; Wang et al., 2023). Lastly, several studies approach long-term memory from the perspective of context compression, developing techniques</p>
<p>to condense lengthy histories into compact representations, whether in the form of LLM internal representations (Mu et al., 2023; Chevalier et al., 2023), discrete tokens (Jiang et al., 2023; Xu et al., 2024), or retrievable text segments via retrieval-augmented generation (RAG, Shi et al. (2024); Wang et al. (2023); Sarthi et al. (2024); Chen et al. (2023a); Gutiérrez et al. (2024)). Although LONGMEMEval can evaluate any memory system, we will take an online context compression perspective, where each history interaction session is sequentially processed, stored, and accessed on-demand through indexing and retrieval mechanisms (§4). This formulation aligns with current literature (Zhong et al., 2024; Gutiérrez et al., 2024) and commercial systems (OpenAI, 2024; Coze, 2024). Its plug-and-play nature also facilitates the integration into existing chat assistant systems.</p>
<h1>3 LONGMEMEval</h1>
<h3>3.1 Problem Formulation</h3>
<p>The evaluation of LongMEMEval requires an instance of 4-tuple $\left(\mathbf{S}, q, t_{q}, a\right) . \quad \mathbf{S} \equiv$ $\left[\left(t_{1}, S_{1}\right),\left(t_{2}, S_{2}\right), \ldots,\left(t_{N}, S_{N}\right)\right]$ is a sequence of $N$ history chat sessions ordered from the earliest to the latest, where $S_{i}$ is a multi-turn interaction between the user and a chat assistant and $t_{i}$ is the session's timestamp. Each session can be further decomposed into rounds: one user message followed by one assistant response. During test time, $\mathbf{S}$ is provided to the system one by one. $q$ and $t_{q}&gt;t_{N}$ represent the question from the user and its date. $a$ is a short phrase indicating the answer, or a natural language rubric describing the preferred answer in the case where $q$ is open-ended.</p>
<h3>3.2 LONGMEMEval: Benchmark Curation</h3>
<p>One major challenge in building a reliable personalized assistant is performing online recording, recalling, updating, and reasoning on the dynamically evolving user information. To comprehensively reflect the challenge, LONGMEMEval formulates five core long-term memory abilities:</p>
<ul>
<li>Information Extraction (IE): Ability to recall specific information from extensive interactive histories, including the details mentioned by either the user or the assistant.</li>
<li>Multi-Session Reasoning (MR): Ability to synthesize the information across multiple history sessions to answer complex questions that involve aggregation and comparison.</li>
<li>Knowledge Updates (KU): Ability to recognize the changes in the user's personal information and update the knowledge of the user dynamically over time.</li>
<li>Temporal Reasoning (TR): Awareness of the temporal aspects of user information, including both explicit time mentions and timestamp metadata in the interactions.</li>
<li>Abstention (ABS): Ability to identify questions seeking unknown information, i.e., information not mentioned by the user in the interaction history, and answer "I don't know".</li>
</ul>
<p>As shown in Table 1, this formulation represents a more comprehensive ability coverage compared to prior long-term memory benchmarks like MemoryBank and PerLTQA. To thoroughly assess these abilities, LONGMEMEval features seven question types. Single-session-user and single-sessionassistant test memorizing the information mentioned by user or assistant within a single session. Single-session-preference tests whether the model can utilize the user information to generate a personalized response. multi-session (MR) tests aggregating user information across two or more sessions. knowledge-update (KU) focuses on the ability to recognize changes in the user's life states and update the memory accordingly. temporal-reasoning (TR) tests reasoning with both the timestamp in metadata and explicit time references. Finally, we draw 30 questions from the previous question types and modify them into "false premise" questions, testing whether the model can correctly abstain from answering (ABS). Figure 1 presents an example for each question type.</p>
<p>Question Curation Figure 2 depicts the question curation pipeline. We define an ontology of 164 user attributes in five categories: lifestyle, belongings, life events, situations context, and demographic information. For each attribute, we leverage an LLM ${ }^{1}$ to generate attribute-focused user background paragraphs, each of which includes detailed discussion of the user's life experience.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Data creation pipeline of LongMEMEVAL. (a) Human experts construct all the questions and evidence statements. (b) Then, the evidence sessions are LLM-simulated and human-edited. (c) The full user-AI chat history is constructed at test time, whose length is freely configurable.</p>
<p>To create a question, we randomly sample a paragraph and use an LLM to propose several seed (question, answer) pairs. As these LLM-proposed questions often lack depth and diversity, human experts manually filter and rewrite all the questions to achieve the desired difficulty. Then, we manually decompose the answer into one or more evidence statements with optional timestamps.</p>
<p>Evidence Session Construction Each evidence statement is then separately embedded into a taskoriented evidence session created by self-chatting (Xu et al., 2023). The user LLM is instructed to convey the evidence statement indirectly, e.g., instead of stating "I bought a new car last month," it might instead ask for help about car insurance and reveal the information incidentally. This approach enhances the benchmark's difficulty by requiring systems to recognize and memorize user details not explicitly emphasized in conversations. We present the full details in Appendix A.1.</p>
<p>To ensure the data quality, all the evidence sessions are then manually screened and edited to (1) verify evidence inclusion, (2) distribute the evidence statement across different conversation positions, and (3) rephrase statements into more natural, colloquial language, especially for time mentions, which LLMs often express too formally. We also meticulously annotate the position of the evidence statement within each evidence session. For questions involving temporal information, we then manually add timestamps to both the evidence sessions and the questions. Most questions require evidence from multiple sessions (up to six) with evidence statements positioned diversely within sessions. Appendix A. 3 presents further statistics of the final constructed data.</p>
<p>History Compilation For each question, LongMEMEVAL compiles a coherent user-AI chat history (Figure 2c). Our approach is analogous to the needle-in-a-haystack test (Kamradt, 2023), which asks a model to retrieve brief information (the "needle") embedded in a long document (the "haystack"). In comparison, LongMEMEVAL is more challenging and realistic as it involves retrieving and synthesizing information from multiple extended evidence sessions. Specifically, we sample a number of unrelated user-AI chat sessions, randomly insert the evidence sessions in the middle, and assign a plausible timestamp to all sessions. We draw the irrelevant sessions from two sources: (1) self-chat sessions simulated based on other non-conflicting attributes and (2) publicly released user-AI style chat data including ShareGPT (Zheng et al., 2023) and UltraChat (Ding et al., 2023). This design creates extensible realistic chat histories with minimal conflicts. While the pipeline allows us to compile chat histories of arbitrary length, we provide two standard settings: LONGMEMEVALs ( $\sim 115 \mathrm{k}$ tokens/question) and LONGMEMEVALs ( 500 sessions, $\sim 1.5 \mathrm{M}$ tokens).</p>
<h1>3.3 Evaluation Metric</h1>
<p>Question Answering As the correct answers can take flexible forms, an exact matching strategy as in previous works can result in inaccurate evaluations. To address this, LongMEMEVAL</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Offline Reading</td>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">0.9184</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">0.5773</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o-mini</td>
<td style="text-align: center;">0.7113</td>
</tr>
<tr>
<td style="text-align: center;">Coze</td>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">0.3299</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">0.2474</td>
</tr>
</tbody>
</table>
<p>(a) Commercial memory-augmented chat assistants exhibit weak performance on LONGMEMEval. The accuracy of ChatGPT and Coze degrades by a large amount compared to directly reading the context ("Offline Reading") with the same LLM. Specifically, ChatGPT and Coze instantiated with GPT-4o exhibits $37 \%$ and $64 \%$ performance drop, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Oracle</th>
<th style="text-align: center;">S</th>
<th style="text-align: center;">\% Drop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No Chain-of-Note</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">$30.3 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3.1 Instruct</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">0.744</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">$55.1 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3.1 Instruct</td>
<td style="text-align: center;">8B</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">$36.1 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">Phi-3 128k Instruct</td>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">$45.9 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">Phi-3.5 Mini Instruct</td>
<td style="text-align: center;">4B</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">$48.1 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">With Chain-of-Note</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">$30.7 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3.1 Instruct</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">$66.3 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3.1 Instruct</td>
<td style="text-align: center;">8B</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">$40.8 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">Phi-3 128k Instruct</td>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">$52.4 \%$;</td>
</tr>
<tr>
<td style="text-align: center;">Phi-3.5 Mini Instruct</td>
<td style="text-align: center;">4B</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">$50.3 \%$;</td>
</tr>
</tbody>
</table>
<p>(b) Long-context LLMs exhibit large QA performance drops on LongMEmEvals (column "S"), compared to the accuracy of answering the questions based on only the evidence sessions (column "Oracle").</p>
<p>Figure 3: Pilot study of (a) commercial systems and (b) long-context LLMs on LongMEMEval.
employs a LLM to assess response quality (Liu et al., 2023). Specifically, we prompt-engineer the gpt-40-2024-08-06 model via the OpenAI API. Our meta-evaluation study demonstrates that the evaluator achieves more than $97 \%$ agreement with human experts. The prompts for each problem type as well as the human meta-evaluation details are presented in Appendix A.4.</p>
<p>Memory Recall As LONGMEMEval contains human-annotated answer location labels, intermediate retrieval metrics can be easily calculated if the chat system exposes its retrieval results. We report Recall@ $k$ and NDCG@ $k$, where $k$ is the number of top items retrieved by the system.</p>
<h1>3.4 LONGMEMÉVAL REPRESENTS A SIGNIFICANT CHALLENGE</h1>
<p>Using LONGMEMÉVAL, we conduct a pilot study on commercial systems and long-context LLMs.
Commercial systems We evaluate two commercial systems that maintain a set of memorized user facts as the user chats with the assistant: ChatGPT (OpenAI, 2024) and Coze (Coze, 2024). Since these systems only support memory features via their web interfaces, we randomly selected 97 questions and created a short chat history of 3-6 sessions (approximately 10x shorter than LONGMEMÉVALs). Human annotators interacted with the chat assistants session-by-session and turn-by-turn, and finally ask the question in a new session ${ }^{2}$. In Figure 3a, we compare this online memory evaluation with offline reading, where GPT-4o is prompted to answer with the complete history provided as context. Both ChatGPT and Coze exhibited significant performance drops compared to offline reading, underscoring the challenging nature of LONGMEMÉVAL. We found ChatGPT tended to overwrite crucial information as the chat continues, while Coze often failed to record indirectly provided user information. We provide analyses in Appendix B. Overall, this result highlights the gap between building a seemingly personalized chat assistant by recalling isolated facts and demonstrating a genuinely strong memory ability.</p>
<p>Long-Context LLMs While LongMEMÉVAL poses a significant challenge to online memory systems, is the benchmark easily tackled with offline reading over the entire history? In Figure 3b, we evaluated four advanced long-context LLMs on LONGMEMÉVALs (with a history length of approximately 115 k tokens): GPT-4o, Llama 3.1 Instruct (Dubey et al., 2024), and Phi-3 (Abdin et al., 2024a). Compared to the oracle retrieval setting (answering with only the evidence sessions as the context), these LLMs showed a $30 \%$ to $60 \%$ performance decline when tasked with reading the entire LONGMEMÉVALs history, regardless of whether the chain-of-note technique (Yu et al., 2023) was applied (discussed further in $\S 4.2$ ). As the histories in LONGMEMÉVALs is still short ( $\sim 50$ sessions), the performance is likely to further degrade as the interaction history expands. Overall, these findings suggest that even the most capable current long-context LLMs struggle to manage an ever-growing interaction history without an effective memory mechanism.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: A unified view of a chat assistant with long-term memory in operation. We formulate three stages and four control points (CP). We provide further examples in Table 2 and Appendix C.</p>
<h1>4 A Unified View of Long-Term Memory Assistants</h1>
<p>In this section, we formulate a three-stage long-term memory model for chat assistants. Despite its simplicity, this model provides a unified view of existing long-term memory assistant works. Along each of its stages, we then investigate crucial control points and propose our optimizations.</p>
<h3>4.1 Long-Term Memory System: Formulation</h3>
<p>We formulate long-term memory as a massive key-value datastore $\left[\left(k_{1}, v_{1}\right),\left(k_{2}, v_{2}\right), \ldots\right]$. The keys $k_{i}$ can be heterogeneous, and could be discrete or continuous. In the discrete case, the key could be a sentence, a paragraph, a fact, or an entity, etc. In the continuous case, the key could be e.g., the model's internal representation under some inputs. The values $v_{i}$ might repeat. As shown in Figure 4, we formulate three stages for a memory-augmented assistant: (1) indexing, converting each history session $\left(t_{i}, S_{i}\right)$ into one or more key-value items, (2) retrieval, formulating a retrieval query and collecting $k$ most relevant items, and (3) reading, an LLM $\mathcal{M}$ reads the retrieval result and generates a response. In Table 2, we show how nine memory-augmented chat assistant systems can be viewed as instantiations of this framework. An alternative mathematical formulation is presented in Appendix C. For its conciseness, the rest of this paper follows this section's formulation.</p>
<h3>4.2 Long-Term Memory System: Design Choices</h3>
<p>We identify four crucial control points for long-term memory of chat assistants, as illustrated in Figure 4. We analyze design choices from existing works and their limitations, and propose our optimizations. Due to space constraints, we present these designs at a high level here, with detailed designs further described in $\S 5$ and Appendix D.</p>
<p>CP 1: Value The value represents the format and granularity of each session stored in memory. Given that user-AI chat sessions are often lengthy and cover multiple topics, storing each session as a single item can hinder effective retrieval and reading. Conversely, compressing sessions into summaries or user-specific facts, as seen in prior work such as Zhong et al. (2024) and Du et al. (2024), can lead to information loss, harming the performance of the system to answer detailed questions. In $\S 5.2$, we compare three value representation strategies: storing entire sessions, decomposing sessions into individual rounds, and further applying summary/fact extraction.</p>
<p>CP 2: Key Even when sessions are decomposed and compressed, each item still contains substantial information, with only a fraction relevant to the user's query. Therefore, using the value itself as the key, a common practice in prior works (Zhong et al., 2024; Maharana et al., 2024), may be suboptimal. We introduce a key expansion approach in $\S 5.3$, where summaries, keyphrases, user facts, and timestamped events are extracted from the values to augment the index. This optimization highlights the key information and enables effective retrieval with multiple pathways.</p>
<p>CP 3: Query For straightforward user queries, the aforementioned key-value optimizations may yield high retrieval accuracy. However, when queries involve temporal references (e.g., "Which restaurant did you recommend last weekend?"), naive similarity search proves insufficient. We address this with a time-aware indexing and query expansion strategy, where values are indexed with timestamped events, and retrieval is restricted to items within the relevant time range (§5.4).</p>
<p>Table 2: A comparison of nine memory-augmented frameworks through the lens of the proposed unified framework. We provide detailed references and discussions of each work in Appendix C. For ChatGPT and Coze, we skip several designs as they are unknown.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Value</th>
<th style="text-align: center;">Key</th>
<th style="text-align: center;">Query</th>
<th style="text-align: center;">Retrieval</th>
<th style="text-align: center;">Time-aware</th>
<th style="text-align: center;">Reading</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">In-context RAG [Shi et al., 2024]</td>
<td style="text-align: center;">round/session</td>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}$</td>
<td style="text-align: center;">question</td>
<td style="text-align: center;">flat</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">direct</td>
</tr>
<tr>
<td style="text-align: center;">MemoryBank [Zhong et al., 2024]</td>
<td style="text-align: center;">summary + round</td>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}$</td>
<td style="text-align: center;">question</td>
<td style="text-align: center;">flat</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">direct</td>
</tr>
<tr>
<td style="text-align: center;">LD-Agent [Li et al., 2024]</td>
<td style="text-align: center;">summary + fact</td>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}$</td>
<td style="text-align: center;">keyphrase</td>
<td style="text-align: center;">flat</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">direct</td>
</tr>
<tr>
<td style="text-align: center;">CoN [Yu et al., 2023]</td>
<td style="text-align: center;">round/session</td>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}$</td>
<td style="text-align: center;">question</td>
<td style="text-align: center;">flat</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">CoN</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">fact</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Coze</td>
<td style="text-align: center;">fact</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RAPTOR [Sarthi et al., 2024]</td>
<td style="text-align: center;">round/session</td>
<td style="text-align: center;">node summary</td>
<td style="text-align: center;">question</td>
<td style="text-align: center;">flat/interactive</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MemWalker [Chen et al., 2023a]</td>
<td style="text-align: center;">round/session</td>
<td style="text-align: center;">node summary</td>
<td style="text-align: center;">question</td>
<td style="text-align: center;">interactive</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">interactive</td>
</tr>
<tr>
<td style="text-align: center;">HippoRAG [Gutiérrez et al., 2024]</td>
<td style="text-align: center;">round/session</td>
<td style="text-align: center;">entity</td>
<td style="text-align: center;">entity</td>
<td style="text-align: center;">PPR</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">direct</td>
</tr>
<tr>
<td style="text-align: center;">Our Design</td>
<td style="text-align: center;">round</td>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}+$ fact</td>
<td style="text-align: center;">question + time</td>
<td style="text-align: center;">flat</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">CoN</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: QA performance on LONGMEMEval $_{\text {M }}$ with different value designs. Decomposing sessions into rounds improves the QA performance. For the multi-session reasoning questions, further representing the values with the extracted facts improves the QA accuracy.</p>
<p>CP 4: Reading Strategy Answering complex queries may require recalling numerous memory items. Although the retrieval accuracy can be enhanced through the preceding designs, it does not guarantee that the LLM can effectively reason over the extensive context [Shi et al., 2023; Liu et al., 2024]. In $\S 5.5$, we explore reading strategies and demonstrate that optimizations such as extracting key information before answering (Chain-of-Note, [Yu et al., 2023]) and using structured format prompting [Yin et al., 2023] are crucial for achieving high reading performance.</p>
<h1>5 EXPERIMENT RESULTS</h1>
<h3>5.1 EXPERIMENTAL SETUP</h3>
<p>We mainly study three LLMs: GPT-4o, Llama 3.1 70B Instruct, and Llama 3.1 8B Instruct ${ }^{3}$. For the retriever, we choose dense retrieval with the 1.5B Stella V5 model [Zhang, 2023], given its high performance on MTEB [Muennighoff et al., 2023]. Extensive comparisons between Stella V5 and alternative retrievers are provided in Appendix E.2. For the indexing stage, we employ Llama 3.1 8B Instruct to extract summaries, keyphrases, user facts, and timestamped events. When sessions or rounds are used as the key, we only keep the user-side utterances. In the reading stage, the retrieved items are always sorted by their timestamp to help the reader model maintain temporal consistency. Throughout $\S 5.2$ to $\S 5.4$, we apply Chain-of-Note and json format (discussed in $\S 5.5$ ) by default.</p>
<h3>5.2 Value: Decomposition improves RAG performance</h3>
<p>Using LONGMEMExAL, we compare different value choices in a budget-aware manner. As shown in Figure 5, decomposing sessions into rounds significantly enhances reading performance with GPT-4o as the reader, while performing similarly to non-decomposed sessions when using Llama 3.1 8B Instruct as the reader. However, despite their efficiency in token usage, replacing sessions or rounds with extracted summaries or facts negatively impacts QA performance due to information loss. The only exception is with multi-session reasoning questions, where fact decomposition consistently improves performance. We hypothesize this is because fact decomposition extracts</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Retrieval and end-to-end QA performance on LONGMEMEVAL ${ }_{\text {M }}$ with different key designs for indexing. L3.1 = Llama 3.1 Instruct. We find applying document expansion with the extracted user facts (row $\mathrm{K}=\mathrm{V}+$ fact) greatly improves both retrieval and the downstream QA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Key Design</th>
<th style="text-align: center;">Retrieval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">End-to-End QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Metrics@5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Metrics@10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L3.1 70B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L3.1 8B</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">NDCG</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">NDCG</td>
<td style="text-align: center;">Top-5</td>
<td style="text-align: center;">Top-10</td>
<td style="text-align: center;">Top-5</td>
<td style="text-align: center;">Top-10</td>
<td style="text-align: center;">Top-5</td>
<td style="text-align: center;">Top-10</td>
</tr>
<tr>
<td style="text-align: center;">Value $=$ Round</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}$</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.534</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=$ fact</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.534</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=$ keyphrase</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.432</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}+$ fact</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.572</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}+$ keyphrase</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.524</td>
</tr>
<tr>
<td style="text-align: center;">Value $=$ Session</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}$</td>
<td style="text-align: center;">0.706</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.570</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.464</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=$ summary</td>
<td style="text-align: center;">0.572</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.648</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.216</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=$ fact</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.814</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.404</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=$ keyphrase</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.414</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}+$ summary</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.494</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}+$ fact</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.490</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{K}=\mathrm{V}+$ keyphrase</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.508</td>
</tr>
</tbody>
</table>
<p>the same type of information across all sessions in a more uniform and simplified format, aiding retrieval and reading (Chen et al., 2023b). Finally, we observe that the optimal token budget varies by the reader's capability: while Llama 3.1 8B Instruct's performance drops sharply beyond 3 k retrieved tokens, GPT-4o continues to improve even with over 20k retrieved tokens.</p>
<h1>5.3 KEY: Multi-KEY INDEXING IMPROVES RETRIEVAL AND RAG</h1>
<p>In Table 3, we explore whether summaries, keyphrases, or user facts condensed from the value can serve as better keys than the value itself. Interestingly, despite their more focused semantics, using these condensed forms alone does not enhance the memory recall performance. We hypothesize that this is due to the retriever's ability to already effectively handle long-text semantics.</p>
<p>To leverage both the highlighted information from compression and the completeness of the original value, we applied a simple document expansion technique (Tao et al., 2006; Efron et al., 2012), where the compressed information is concatenated with the original value to form the key during indexing ${ }^{4}$. This approach, particularly when using user facts, yielded an average improvement of $9.4 \%$ in recall@ $k$ and $5.4 \%$ in final accuracy across all models. In Appendix E.2, we further analyze different retrievers and find with alternative retrievers, key expansion with summary and keyphrases could improve Recall@5 when session is used as the value granularity. These results suggest that multi-pathway retrieval can significantly enhance memory recall performance. In the following section, we will investigate the time constraint as another pathway to leverage.</p>
<h3>5.4 QUERY: TIME-AWARE QUERY EXPANSION IMPROVES TEMPORAL REASONING</h3>
<p>A key challenge highlighted by LONGMEMEval in building real-world assistant systems is the need to utilize temporal information present in both metadata and user utterances to correctly answer time-sensitive queries. To address this need, we introduce a simple yet effective time-aware indexing and query expansion scheme. Specifically, values are additionally indexed by the dates of the events they contain. During retrieval, an LLM $\mathcal{M}_{T}$ extracts a time range for time-sensitive queries, which is used to filter out a large number of irrelevant values.</p>
<p>As shown in Table 4, this simple design improves recall by an average of $11.3 \%$ when using rounds as the value and by $6.8 \%$ when using sessions as the value. This improvement remains consistent when key expansion is applied during indexing. We also find that the effectiveness of this method depends on using a strong LLM for $\mathcal{M}_{T}$ to accurately infer time ranges from queries. Llama 8B, on</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: Retrieval performance on the temporal reasoning subset of LONGMEMEVALM. Time-aware query expansion significantly facilitates retrieval by narrowing down the retrieval scope.</p>
<table>
<thead>
<tr>
<th></th>
<th>Value = Session</th>
<th></th>
<th></th>
<th></th>
<th>Value = Round</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Metric@5</td>
<td>Metric@10</td>
<td></td>
<td></td>
<td>Metric@5</td>
<td>Metric@10</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1>REPRODUCIbILITY STATEMENT</h1>
<p>In the paper writing and the subsequent code release, we are committed to enabling other researchers to easily leverage our resources, replicate our results, and build upon our findings. We have documented the benchmark construction process in detail in $\S 3.2$ and Appendix A.1, including all the attributes and instructions used to prompt LLMs. We have created and will publicly release the two fixed evaluation datasets, LONGMEMÉVAL $<em _mathrm_M="\mathrm{M">{\mathrm{S}}$ and LONGMEMÉVAL $</em>$. In addition, we will also release the algorithm and source mixture used to create these two datasets, so that future studies could build upon them to create chat histories of any length. Finally, we have meticulously documented all the implementation details of our memory optimization in Appendix D, and our code will be released along with the benchmark as well. We believe that these efforts for transparency can help advance the field and foster future research endeavors.}</p>
<h2>ETHICS STATEMENT</h2>
<p>The major artifact released by this work is the LongMEMÉVAL evaluation dataset. To construct the chat history, we utilize ShareGPT ${ }^{5}$, which has an Apache 2.0 license, and UltraChat ${ }^{6}$, which has an MIT license. In the data release, we plan to use an MIT license. Since the questions and the evidence sessions are newly created, we conducted a rigorous human screen process to ensure that the new dataset does not contain personally identifiable information or offensive content. This paper involves human annotators in two places: (1) the dataset construction and filtering (§3.2) and (2) the manual analysis of commercial systems (§3.4). The process was mainly conducted by three expert annotators, who are in-house NLP researchers that have more than three years of NLP research experience. All the annotators are properly briefed with the annotation objective, and a discussion among them is conducted to resolve uncertain cases. In total, approximately 400 human hours are spent on the dataset construction and 150 hours on the study of commercial systems. The annotators are paid biweekly or monthly, and their salaries include the working hours dedicated to annotation. The data collection protocol has been determined exempt by the IRB of the author's institution. Finally, LongMEMÉVAL and the studied memory system could induce several societal impacts. Storing and recalling user information could cause personal information leakage, and the lack of a memory "deletion" operator could harm the system's trustworthiness. It is also possible that the memory mechanism could be leveraged by malicious parties to toxic contents into the datastore, which may cause a jailbreak behavior during inference. To mitigate such behaviors, it is essential to implement moderation mechanisms that monitor the read/write data flow of the memory. We encourage producers of memory-augmented assistant systems to be aware of these potential harmful effects and apply thorough testing and mitigation.</p>
<h2>REFERENCES</h2>
<p>Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>on your phone. CoRR, abs/2404.14219, 2024a. doi: 10.48550/ARXIV.2404.14219. URL https://doi.org/10.48550/arXiv.2404.14219.</p>
<p>Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024b. doi: 10.48550/ARXIV.2404.14219. URL https://doi.org/10.48550/arXiv.2404.14219.</p>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou. Make your LLM fully utilize the context. CoRR, abs/2404.16811, 2024. doi: 10.48550/ARXIV.2404.16811. URL https://doi.org/10.48550/arXiv.2404.16811.</p>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020.</p>
<p>Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 5016-5026, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1547. URL https:// aclanthology.org/D18-1547/.</p>
<p>Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. CoRR, abs/2310.05029, 2023a. doi: 10.48550/ARXIV.2310.05029. URL https://doi.org/10.48550/arXiv. 2310. 05029 .</p>
<p>Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. Dense X retrieval: What retrieval granularity should we use? CoRR, abs/2312.06648, 2023b. doi: 10.48550/ARXIV.2312.06648. URL https://doi.org/10. 48550/arXiv. 2312.06648.</p>
<p>Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 3829-3846. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.232. URL https://doi.org/10.18653/v1/2023. emnlp-main. 232 .</p>
<p>Coze. Memory overview guide. https://www.coze.com/docs/guides/memory_ overview?_lang=en, 2024. Accessed: September 15, 2024.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.</p>
<p>Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai Wong. PerLTQA: A personal long-term memory dataset for memory</p>
<p>classification, retrieval, and fusion in question answering. In Kam-Fai Wong, Min Zhang, Ruifeng Xu, Jing Li, Zhongyu Wei, Lin Gui, Bin Liang, and Runcong Zhao (eds.), Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10), pp. 152-164, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.sighan-1.18/.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.</p>
<p>Miles Efron, Peter Organisciak, and Katrina Fenlon. Improving retrieval of short texts through document expansion. In William R. Hersh, Jamie Callan, Yoelle Maarek, and Mark Sanderson (eds.), The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012, pp. 911-920. ACM, 2012. doi: 10.1145/2348283.2348405. URL https://doi.org/10.1145/2348283. 2348405 .</p>
<p>Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=TaAqeo71Uh.</p>
<p>Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. CoRR, abs/2405.14831, 2024. doi: 10.48550/ARXIV.2405.14831. URL https://doi.org/10.48550/arXiv. 2405.14831 .</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res., 2022, 2022. URL https://openreview.net/forum?id= jKN1pXi7b0.</p>
<p>Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 13358-13376. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.825. URL https://doi.org/10.18653/v1/2023.emnlp-main.825.</p>
<p>Gregory Kamradt. Needle in a haystack - pressure testing llms. GitHub, 2023. URL https: //github.com/gkamradt/LLMTest_NeedleInAHaystack.</p>
<p>Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, and Edward Choi. Dialsim: A real-time simulator for evaluating long-term dialogue understanding of conversational agents, 2024. URL https://arxiv.org/abs/2406. 13144 .</p>
<p>Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.</p>
<p>Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue. CoRR, abs/2406.05925, 2024. doi: 10. 48550/ARXIV.2406.05925. URL https://doi.org/10.48550/arXiv.2406.05925.</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Trans. Assoc. Comput. Linguistics, 12:157-173, 2024. doi: 10.1162/TACL $\backslash . . \mathrm{A} \backslash .00638$. URL https://doi.org/ 10.1162/tacl_a_00638.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Geval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods</p>
<p>in Natural Language Processing, pp. 2511-2522, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.153. URL https:// aclanthology.org/2023.emnlp-main.153/.</p>
<p>Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of LLM agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13851-13870, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.747. URL https://aclanthology.org/2024.acl-long.747/.</p>
<p>Microsoft. Announcing microsoft copilot, your everyday ai companion, 2023. URL https://blogs.microsoft.com/blog/2023/09/21/ announcing-microsoft-copilot-your-everyday-ai-companion/. Accessed: September 15, 2024.</p>
<p>Mistral AI Team. Mistral nemo: Our new best small model. Mistral AI, July 2024. URL https: //mistral.ai/news/mistral-nemo.</p>
<p>Jesse Mu, Xiang Li, and Noah D. Goodman. Learning to compress prompts with gist tokens. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 3d77c6dcc7f143aa2154e7f4d5e22d68-Abstract-Conference.html.</p>
<p>Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 2014-2037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.148. URL https://aclanthology.org/ 2023.eacl-main.148/.</p>
<p>OpenAI. Chatgpt, 2022. URL https://chat.openai.com/chat. Accessed: September 15, 2024.</p>
<p>OpenAI. Memory and new controls for chatgpt. https://openai.com/index/ memory-and-new-controls-for-chatgpt/, 2024. Accessed: September 15, 2024.</p>
<p>Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249-266, 2019. doi: 10.1162/tacl_a_00266. URL https://aclanthology.org/Q19-1016/.</p>
<p>Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333-389, 2009. doi: 10.1561/1500000019. URL https: //doi.org/10.1561/1500000019.</p>
<p>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. RAPTOR: recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= GN921JHCRw.</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 31210-31227. PMLR, 2023. URL https://proceedings.mlr.press/ v202/shi23a.html.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: Retrieval-augmented black-box language models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 8371-8384, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.463. URL https://aclanthology.org/2024.naacl-long.463/.</p>
<p>Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. Language model information retrieval with document expansion. In Robert C. Moore, Jeff Bilmes, Jennifer Chu-Carroll, and Mark Sanderson (eds.), Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp. 407-414, New York City, USA, June 2006. Association for Computational Linguistics. URL https://aclanthology.org/N06-1052/.</p>
<p>Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/.</p>
<p>Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html.</p>
<p>Wei Wei, Quoc Le, Andrew Dai, and Jia Li. AirDialogue: An environment for goal-oriented dialogue research. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3844-3854, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1419. URL https://aclanthology. org/D18-1419/.</p>
<p>Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.</p>
<p>Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 6268-6278, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.385. URL https://aclanthology.org/ 2023.emnlp-main.385/.</p>
<p>Fangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: improving retrieval-augmented lms with context compression and selective augmentation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=mlJLVigNHp.</p>
<p>Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term open-domain conversation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5180-5197, Dublin, Ireland, May 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.356. URL https://aclanthology.org/ 2022.acl-long.356/.</p>
<p>Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. Long time no see! open-domain conversation with long-term persona memory. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 2639-2650, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.207. URL https:// aclanthology.org/2022.findings-acl.207/.</p>
<p>Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 30633079, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.acl-long.172. URL https://aclanthology.org/2023.acl-long.172/.</p>
<p>Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210, 2023.</p>
<p>Dun Zhang. STELLA EN 1.5B v5. https://huggingface.co/dunzhang/stella_en_ 1.5B_v5, 2023. Accessed: September 15, 2024.</p>
<p>Hongming Zhang, Xiaoman Pan, Hongwei Wang, Kaixin Ma, Wenhao Yu, and Dong Yu. Cognitive kernel: An open-source agent system towards generalist autopilots, 2024. URL https:// arxiv.org/abs/2409.10277.</p>
<p>Michael Zhang and Eunsol Choi. SituatedQA: Incorporating extra-linguistic contexts into QA. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7371-7387, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.586. URL https:// aclanthology.org/2021.emnlp-main.586/.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, ThirtySixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19724-19731. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29946. URL https://doi.org/10.1609/aaai.v38i17.29946.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5657-5673, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.382. URL https://aclanthology.org/2022.emnlp-main.382/.</p>
<h1>A SUPPLEMENTAL DETAILS FOR LONGMEMEVAL</h1>
<h2>A. 1 DATASET CONSTRUCTION</h2>
<p>In this section, we discuss the details of our benchmark construction process.
Attribute Ontology In Table 5, we provide the full attribute ontology used in LongMEMEVAL. This attribute ontology is constructed manually to reflect commonly mentioned topics in userassistant chats. Five major categories are included: demographic information, lifestyle, situational context, life events, and belongings.</p>
<p>Table 5: Human-constructed user attribute ontology. Each attribute represents a unique dimension of human experience along which a user biography could be constructed. For LongMEMEVAL, we sample user backgrounds based on each of the attributes and construct questions on top of them.</p>
<h2>Attribute 1: Demographic Information</h2>
<p>age, gender, ethnicity, nationality, language, education level, occupation</p>
<h2>Attribute 2: Lifestyle</h2>
<p>2.1: Shopping: online shopping frequency, favorite stores, loyalty program, sales events, coupons, gift purchasing habits, eco-friendly product preferences, luxury vs budget shopping, technology gadget purchasing, fashion and apparel, grocery shopping, shopping for others
2.2: Media Consumption: book, movie, tv show, music, podcast, video game, streaming service, theater, magazine and newspaper, youtube, educational content, audiobook and e-book
2.3: Social Media Engagement: posting, commenting, followers, groups, hashtags, campaigns, messaging, live streaming, social media breaks
2.4: Daily Routines: wake-up time, bedtime, work or school start time, meal time, exercise routines, coffee or tea break, commuting, evening activities, weekend routines, cleaning schedules, time spent with family or friends
2.5: Travel: frequency, destination, road trips, travel agencies, outdoor adventures, airlines, hotel, travel with family vs solo travel, packing habits
2.6: Recreation: reading, painting, musical instruments, dancing, watching sports, participating in sports, gardening, bird watching, fishing or hunting, board games, video games, fitness classes, yoga, sculpting, photography, stand-up comedy, writing, collecting, model building, aquarium keeping
2.7: Eating and Cooking: home cooking, food delivery, vegetarian or vegan, favorite cuisines, snacking habits, barbecue, baking, cocktail mixing, cooking classes
2.8: Event Participation: concerts, theater, galleries and museums, sports games, film festivals, religious services, book readings, charity events, trade shows, lectures or workshops, theme parks, local markets, networking events, sports, auto racing, workshops, museum tours</p>
<h2>Attribute 3: Situational Context</h2>
<p>3.1: Home: living room, kitchen, bathroom, room style, room lighting, furniture, technology, plants
3.2: Social Context: alone, family, friends, interactions with strangers
3.3: Time Context: time of day, day of week, seasonal</p>
<h2>Attribute 4: Life Events</h2>
<p>graduations, academic achievements, study abroad, significant academic projects, job promotions, starting a business, births and adoptions, marriages, family reunions, illness or surgeries, mental health journeys, purchasing a home, trips, movement, living abroad, refugee or immigration, loss of loved ones, name change, belief, milestone</p>
<h2>Attribute 5: Belongings</h2>
<p>cars, bikes, vehicles, computer, phone, pet, farm animal, animal care items, home, land, art, antiques, collectible, rare items, clothing, jewelry, shoes, bag, sports gear, musical instruments, health related devices, crafting, photography</p>
<p>Background Sampling Based on each of the attributes, we prompt Llama 370B Instruct to generate a background paragraph outlining the user memory and experience. In our preliminary studies, we find that the following zero-shot prompt in Figure 7 can already guide the model to generate a long and focused user background with sufficient details, which suffices for the next step of question creation. We thus use the same prompt for the final version of LONGMEMEVAL.</p>
<p>I will give you a topic. Please imagine you are a user that wants to recall and record recent personal facts along the topic. Generate a long text describing these personal facts. Use your imagination and generate the personal facts. Make it long and involve several recent facts or recent events spanning many days, weeks, or monthes. You may state the facts in plain language and no need to make it story-like.</p>
<p>Topic: ${$ attribute $}$
Recent Personal Facts related to {attribute}:</p>
<p>Figure 7: The prompt for constructing user backgrounds based on an attribute.</p>
<p>I will give you a past memory. Use the memory to act as a normal user to chat with a chat assistant. In the chat, you may ask it to assist you various tasks or ask it about various information. However, make sure that your convey the following fact about you: "{evidence_statement}". In addition, make sure your message is concise (1-2 simple sentences), since the real users often do not bother write a long message. I will provide you with the chat history and the response from the assistant. Directly generate the next response from the user's perspective. You must simulate the tone of a neutral user and do not be overly enthusiastic, verbose, formal, or polite. For conciseness, DO NOT react to the assistant's message with e.g., "thanks" or "I will do that". Instead, directly state the follow-up questions or new questions.</p>
<p>Memory: ${$ background $}$
Chat History:
assistant: Hi! How can I assist you today?
... (more rounds as the conversation continues) ...</p>
<p>Figure 8: The prompt for instruction an LLM to act as a user and initiates a task-oriented dialogue with another LLM. Both the background and the evidence statement is provided. This prompt is used for the question type single-session. For the other question types, the prompt components are slightly different but the prompt overall follows the same style.</p>
<p>Question Construction As discussed in §3.2, based on the generated user backgrounds, Llama 370B Instruct is used to propose question and answers for each question type. Additionally, for the question type temporal-reasoning, multi-session reasoning, and single-session-preference, we use GPT-4o to propose several questions. Nevertheless, we find most of the questions to be unsatisfactory and manually filter and edit most of the questions. In total, approximately 1000 questions were generated for each question type, and the final yield rate is about $5 \%$. For each question, we then manually decompose the answer into the evidence statements. If the question or the evidence statements involve time mentions, we assign a timestamp to the question and the evidence statements at this stage. Note that if timestamps are specified for the evidence statements at this stage, these timestamps will always be used for corresponding evidence sessions. Otherwise, the timestamps will be randomly assigned at the history construction stage with all the other sessions.</p>
<p>Evidence Session Construction Using the question and the decomposed evidence statements, we use Llama 370B Instruct to simulate one user-AI chat history per evidence statement via self-chat. In Figure 8, we present an example of the chat simulation prompt, where we ask the user LLM to indirectly mention the evidence statement while avoiding to talk about other evidence statements for the question, if there are any. We include two crucial instructions in the prompt to make sure (1) the evidence statement is provided in an indirect way and (2) the generated messages are concise and</p>
<p>thus mimic the style of user messages. On the assistant side, we directly provide the input generated by the user LLM without any prompt engineering. We simulate the chat for 10 round at maximum and stopped prematurely when either side of the LLMs generates an empty sequence indicating the end of the conversation.</p>
<p>Subsequently, expert annotators manually inspect and edit each of the generated sessions to ensure that (1) the required evidence statements are present in the conversation, (2) no other evidence statements are leaked into the conversation, (3) the evidence statements are provided in a colloquial style, especially for the data and time mentions, and (4) the conversation ends gracefully. In total, roughly $70 \%$ of the sessions are human edited. We note that in a few rare instances, the user LLM fails by assuming the assistant role instead. When these failures are identified, we discard the instance if the conversation cannot be fixed.</p>
<h1>A. 2 History Construction</h1>
<p>In order to construct a coherent and freely extensible chat history, we design a three-staged pipeline that include session pool construction, session sampling, and timestamp resolution.</p>
<p>Session pool construction For each question, we draw the history sessions from three sources: ShareGPT (Zheng et al., 2023), UltraChat (Ding et al., 2023), and the simulated sessions corresponding to other attributes using the same pipeline mentioned in the previous section. This pool ensures that the non-evidence history sessions have similar topic or format as the evidence sessions, while avoiding providing conflicting information that would invalidate the question.</p>
<p>Session sampling To sample a history containing $x$ sessions, we randomly sample from the aforementioned three sources and shuffle the sessions together with the question's evidence sessions. For LongMEMEVAL, we always use the following mixture: $25 \%$ ShareGPT, $25 \%$ UltraChat, and $50 \%$ simulated sessions. If the evidence sessions need to follow a specific order, we swap their orders accordingly after shuffling.</p>
<p>Timestamp resolution Finally, we randomly assign timestamps to the session following their order of the history. If the evidence sessions are associated with pre-defined timestamps, we use them as anchors to determine the range of timestamp of the non-evidence sessions preceding or following them. Other wise, we randomly assign tiemstamps in May 2023.</p>
<h2>A. 3 Basic Statistics</h2>
<p>In Figure 9, we present the basic statistics of LONGMEMEVAL, revealing that most questions require evidence from multiple sessions (up to six) and that evidence statements are positioned diversely within sessions, increasing the challenge to the memory design.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Distribution of question types in LONGMEMEVAL.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Distribution of the number of evidence sessions. Most questions emphasize multi-session reasoning, requiring reading up to six sessions to answer.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(c) Distribution of the location of the evidence statement within the evidence sessions. Most evidence statements are located at the beginning of the chat.</p>
<p>Figure 9: LONGMEMEVAL challenges chat assistants through its (a) diverse question types, (b) emphases on multi-session reasoning, and (c) diverse evidence locations within sessions.</p>
<h1>A. 4 Evaluation Metric Building</h1>
<p>To accurately evaluate the diverse responses of LLMs, we use an expert-written prompt to instruct GPT-4o as the correctness judge. We present the full prompt in Figure 10. To enable the model to handle detailed edge cases as how expert evaluators would do, we design separate prompts for a number of tasks. To ensure the prompt has a high agreement with expert judge, we sample 30 questions per problem type, collect the long-context generation results from GPT-4o and Llama 3.1 8B Instruct, and report the judgment correctness by category.</p>
<p>As shown in Table 6, the prompt-engineered GPT-4o judge achieves reliable performance in evaluating both GPT-4o and Llama-3.1-8B-Instruct as the generation model. The evaluator's judgment slightly deviates from human experts for the single-session-preference and abstention problems due to the open-ended nature of the response. Nevertheless, our evaluation prompt still achieves $90 \%$ or higher accuracy under all settings. We will include this prompt in the benchmark package that we will release to enable consistent comparisons for future work.</p>
<h2>temp-reasoning</h2>
<p>I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no. In addition, do not penalize off-by-one errors for the number of days. If the question asks for the number of days/weeks/months, etc., and the model makes off-by-one errors (e.g., predicting 19 days when the answer is 18), the model's response is still correct.</p>
<h2>knowledge-update</h2>
<p>I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response contains some previous information along with an updated answer, the response should be considered as correct as long as the updated answer is the required answer.</p>
<h2>single-session-preference</h2>
<p>I will give you a question, a rubric for desired personalized response, and a response from a model. Please answer yes if the response satisfies the desired response. Otherwise, answer no. The model does not need to reflect all the points in the rubric. The response is correct as long as it recalls and utilizes the user's personal information correctly.</p>
<h2>Other question types</h2>
<p>I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no.</p>
<p>Figure 10: Evaluation instructions for the GPT-4o judge. We provide the question, answer, and the model's hypothesis after the instruction and ask GPT-4o to directly generate "yes" or "no".</p>
<p>Table 6: Meta-evaluation results of prompt-engineered GPT-4o judge. We observe a high evaluation accuracy across all the problem types in LongMEMEVAL.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question Type</th>
<th style="text-align: center;">Answer Model</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-40</td>
<td style="text-align: center;">Llama-3.1-8B-instruct</td>
</tr>
<tr>
<td style="text-align: center;">single-session-user</td>
<td style="text-align: center;">$1.00(30 / 30)$</td>
<td style="text-align: center;">$0.97(29 / 30)$</td>
</tr>
<tr>
<td style="text-align: center;">single-session-assistant</td>
<td style="text-align: center;">$1.00(30 / 30)$</td>
<td style="text-align: center;">$1.00(30 / 30)$</td>
</tr>
<tr>
<td style="text-align: center;">single-session-preference</td>
<td style="text-align: center;">$0.90(27 / 30)$</td>
<td style="text-align: center;">$0.97(29 / 30)$</td>
</tr>
<tr>
<td style="text-align: center;">multi-session</td>
<td style="text-align: center;">$1.00(30 / 30)$</td>
<td style="text-align: center;">$1.00(30 / 30)$</td>
</tr>
<tr>
<td style="text-align: center;">knowledge-update</td>
<td style="text-align: center;">$1.00(30 / 30)$</td>
<td style="text-align: center;">$1.00(30 / 30)$</td>
</tr>
<tr>
<td style="text-align: center;">temporal-reasoning</td>
<td style="text-align: center;">$1.00(30 / 30)$</td>
<td style="text-align: center;">$0.97(29 / 30)$</td>
</tr>
<tr>
<td style="text-align: center;">abstention</td>
<td style="text-align: center;">$0.97(29 / 30)$</td>
<td style="text-align: center;">$0.90(27 / 30)$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.97</td>
</tr>
</tbody>
</table>
<h1>B A Human Study on Commercial Memory Chatbots</h1>
<p>We evaluate two commercial memory-augmented chatbots: ChatGPT (OpenAI, 2024) and Coze (Coze, 2024). We randomly selected 97 questions and created a short chat history of 3-6 sessions by sampling according to a mixture of $50 \%$ ShareGPT and $50 \%$ simulated sessions. We skip two type of questions that the assistants cannot answer: (1) a subset of temporal-reasoning, since our manual analysis cannot afford interacting with the (potentially evolving) systems across multiple months, and (2) single-session-assistant, since the systems do not remember any information given by the assistant. We also did not evaluate the abstention ability of these two systems because an early version of the dataset without any abstention questions was used for this analysis.
Since these systems only support memory features via their web interfaces, human annotators manually interacted with the chat assistants session-by-session, ending with a new session where the question was posed. After collecting the model's response, the annotator manually evaluates the answer's correctness. Finally, to start evaluating the next instance from a fresh state, the annotator manually clears the model's memory through the web interface. We distribute the questions across five annotators. A discussion is performed among the annotators whenever there is a concern with the model's response or with the evidence sessions. All evaluations were conducted in the first two weeks of August 2024.</p>
<p>In Table 7, we present the detailed human evaluation results by problem types. We observe that when the task is memorizing the information from a single session (column IE), both systems can answer a considerable number of problems correctly. However, for the other question types where aggregation across multiple sessions is generally required, both systems exhibit significant performance drops. Compared to ChatGPT, we find most of Coze's errors are due to failing to record information from some session. On the other hand, ChatGPT generally records the evidence statements immediately after it has been presented in the evidence session. However, as the interaction proceeds, ChatGPT often modify this information when it compresses the history, resulting in information loss. This highlights the potential trade-off between reliable personalization and efficiency.</p>
<p>Table 7: Human evaluation results of two systems categorized by evaluated ability types. We use the questions from single-session-user for the IE column. For the temporal reasoning column (TR), we use the questions that do not require reasoning with metadata.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Memory Ability</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">IE</td>
<td style="text-align: center;">MR</td>
<td style="text-align: center;">KU</td>
<td style="text-align: center;">TR</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">GPT-4o-mini</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.652</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-40</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.435</td>
</tr>
<tr>
<td style="text-align: center;">Coze</td>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.043</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-40</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.391</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Accessed via https://github.com/lm-sys/FastChat/tree/main.
${ }^{6}$ Accessed via https://github.com/thunlp/UltraChat.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
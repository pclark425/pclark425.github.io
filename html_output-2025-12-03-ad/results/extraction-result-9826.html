<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9826 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9826</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9826</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-3841234dd49250c4fcbba79eed6593d3b57932c1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3841234dd49250c4fcbba79eed6593d3b57932c1" target="_blank">Language Model Crossover: Variation through Few-Shot Prompting</a></p>
                <p><strong>Paper Venue:</strong> ACM Transactions on Evolutionary Learning and Optimization</p>
                <p><strong>Paper TL;DR:</strong> The conclusion is that language model crossover is a flexible and effective method for evolving genomes representable as text, and naturally benefits from current progress in language models.</p>
                <p><strong>Paper Abstract:</strong> This article pursues the insight that language models naturally enable an intelligent variation operator similar in spirit to evolutionary crossover. In particular, language models of sufficient scale demonstrate in-context learning, i.e., they can learn from associations between a small number of input patterns to generate outputs incorporating such associations (also called few-shot prompting). This ability can be leveraged to form a simple but powerful variation operator, i.e., to prompt a language model with a few text-based genotypes (such as code, plain-text sentences, or equations), and to parse its corresponding output as those genotypes’ offspring. The promise of such language model crossover (which is simple to implement and can leverage many different open source language models) is that it enables a simple mechanism to evolve semantically rich text representations (with few domain-specific tweaks), and naturally benefits from current progress in language models. Experiments in this article highlight the versatility of language-model crossover, through evolving binary bit-strings, sentences, equations, text-to-image prompts, and Python code. The conclusion is that language model crossover is a flexible and effective method for evolving genomes representable as text.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9826.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9826.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMX-Galactica-SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model Crossover (LMX) using GALACTICA for Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper applies LMX (few-shot prompting-based recombination) with the GALACTICA 1.3B scientific language model to evolve compact symbolic expressions that fit a target dataset, using LLM completions as offspring in an evolutionary loop and evaluating expressions with sympy and R^2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GALACTICA-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 1.3 billion parameter autoregressive transformer LLM whose training data was designed for scientific use and includes tens of millions of LaTeX papers and scientific text; used off-the-shelf (no fine-tuning) for few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>symbolic regression / scientific mathematical modeling</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>mathematical formula / symbolic expression (empirical model)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Few-shot in-context prompting as a crossover operator (LMX): concatenate multiple parent expressions (seven parents in experiments) with a short header prompt, generate up to three offspring lines per forward pass, parse offspring as Python expressions, simplify with sympy, and use an evolutionary loop (population selection + repeated LMX generations) to optimize R^2 and expression compactness.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Model pretraining corpus: GALACTICA was pretrained on a large scientific corpus including tens of millions of LaTeX papers (as reported in this paper). For the task itself: the target dataset was the SRBench 'banana' black-box regression problem (5300 samples, two inputs). The evolutionary algorithm was seeded with a population generated from 113 human benchmark symbolic expressions (mapped to the problem variables) rather than random expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Fitness = R^2 on the target dataset; complexity measured as expression size (number of nodes in parse tree, same metric as SRBench); unparsable/generated expressions were discarded; comparisons made to gplearn baselines and an ablation using Pythia-1.4B; convergence trajectories and Pareto-front comparisons against SRBench methods were used for contextual evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LMX using GALACTICA produced competitive, parsimonious symbolic expressions: higher fitness and smaller expression size than the gplearn baseline in these experiments, and occupied an intermediate point on the SRBench Pareto front. Ablation with a general-purpose Pythia model underperformed compared to GALACTICA. The method discovered meaningful algebraic scaffolding and was able to fine-tune numerical constants despite operating in discrete token space.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>The algorithm discovered and evolved a recurring algebraic skeleton of the form c1 * exp(-c2 * x1^{c3} - c6 * x2^{c3}) * cos(x1 + c6 * x2 + c7) (notation paraphrased from the paper) and subsequently tuned constants; evolution produced late innovations (e.g., around generation ~3000) that improved fitness even after apparent convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Risk of test-set contamination because GALACTICA's pretraining contains many human-designed expressions (authors mitigated by choosing a black-box SRBench problem); performance is model-dependent (Pythia ablation performed worse); implicit parsimony arises from token-length limits rather than an explicit complexity penalty; unparsable LLM outputs are common and discarded; computational differences (LMX used GPU; baselines used CPU) complicate raw compute-cost comparisons; discrete-token operation yet continuous-constant tuning is emergent but not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to gplearn (standard GP) and a gplearn variant initialized with the same benchmark-derived population, LMX+GALACTICA achieved higher fitness and smaller expressions; compared to Pythia-1.4B as alternative LLM, GALACTICA significantly outperformed it in this domain; relative to specialized state-of-the-art SR methods, LMX landed at an intermediate point on the Pareto front (not strictly superior overall).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Crossover: Variation through Few-Shot Prompting', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9826.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9826.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MathDiscoveriesLLM-mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mathematical Discoveries From Program Search with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced in the paper's survey table as an example of using large language models for mathematical discovery/program search; cited as related work but not described in detail or used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical Discoveries From Program Search with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematical discovery / symbolic/programmatic search</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>mathematical formula / discovered relationships</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned in related work/table as an LLM-based solution-generation approach (no experimental details provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Crossover: Variation through Few-Shot Prompting', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathematical Discoveries From Program Search with Large Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9826",
    "paper_id": "paper-3841234dd49250c4fcbba79eed6593d3b57932c1",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [
        {
            "name_short": "LMX-Galactica-SR",
            "name_full": "Language Model Crossover (LMX) using GALACTICA for Symbolic Regression",
            "brief_description": "This paper applies LMX (few-shot prompting-based recombination) with the GALACTICA 1.3B scientific language model to evolve compact symbolic expressions that fit a target dataset, using LLM completions as offspring in an evolutionary loop and evaluating expressions with sympy and R^2.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GALACTICA-1.3B",
            "model_description": "A 1.3 billion parameter autoregressive transformer LLM whose training data was designed for scientific use and includes tens of millions of LaTeX papers and scientific text; used off-the-shelf (no fine-tuning) for few-shot prompting.",
            "scientific_domain": "symbolic regression / scientific mathematical modeling",
            "law_type": "mathematical formula / symbolic expression (empirical model)",
            "method_description": "Few-shot in-context prompting as a crossover operator (LMX): concatenate multiple parent expressions (seven parents in experiments) with a short header prompt, generate up to three offspring lines per forward pass, parse offspring as Python expressions, simplify with sympy, and use an evolutionary loop (population selection + repeated LMX generations) to optimize R^2 and expression compactness.",
            "input_corpus_description": "Model pretraining corpus: GALACTICA was pretrained on a large scientific corpus including tens of millions of LaTeX papers (as reported in this paper). For the task itself: the target dataset was the SRBench 'banana' black-box regression problem (5300 samples, two inputs). The evolutionary algorithm was seeded with a population generated from 113 human benchmark symbolic expressions (mapped to the problem variables) rather than random expressions.",
            "evaluation_method": "Fitness = R^2 on the target dataset; complexity measured as expression size (number of nodes in parse tree, same metric as SRBench); unparsable/generated expressions were discarded; comparisons made to gplearn baselines and an ablation using Pythia-1.4B; convergence trajectories and Pareto-front comparisons against SRBench methods were used for contextual evaluation.",
            "results_summary": "LMX using GALACTICA produced competitive, parsimonious symbolic expressions: higher fitness and smaller expression size than the gplearn baseline in these experiments, and occupied an intermediate point on the SRBench Pareto front. Ablation with a general-purpose Pythia model underperformed compared to GALACTICA. The method discovered meaningful algebraic scaffolding and was able to fine-tune numerical constants despite operating in discrete token space.",
            "notable_examples": "The algorithm discovered and evolved a recurring algebraic skeleton of the form c1 * exp(-c2 * x1^{c3} - c6 * x2^{c3}) * cos(x1 + c6 * x2 + c7) (notation paraphrased from the paper) and subsequently tuned constants; evolution produced late innovations (e.g., around generation ~3000) that improved fitness even after apparent convergence.",
            "limitations_challenges": "Risk of test-set contamination because GALACTICA's pretraining contains many human-designed expressions (authors mitigated by choosing a black-box SRBench problem); performance is model-dependent (Pythia ablation performed worse); implicit parsimony arises from token-length limits rather than an explicit complexity penalty; unparsable LLM outputs are common and discarded; computational differences (LMX used GPU; baselines used CPU) complicate raw compute-cost comparisons; discrete-token operation yet continuous-constant tuning is emergent but not guaranteed.",
            "baseline_comparison": "Compared to gplearn (standard GP) and a gplearn variant initialized with the same benchmark-derived population, LMX+GALACTICA achieved higher fitness and smaller expressions; compared to Pythia-1.4B as alternative LLM, GALACTICA significantly outperformed it in this domain; relative to specialized state-of-the-art SR methods, LMX landed at an intermediate point on the Pareto front (not strictly superior overall).",
            "uuid": "e9826.0",
            "source_info": {
                "paper_title": "Language Model Crossover: Variation through Few-Shot Prompting",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "MathDiscoveriesLLM-mention",
            "name_full": "Mathematical Discoveries From Program Search with Large Language Models",
            "brief_description": "Referenced in the paper's survey table as an example of using large language models for mathematical discovery/program search; cited as related work but not described in detail or used in experiments.",
            "citation_title": "Mathematical Discoveries From Program Search with Large Language Models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "scientific_domain": "mathematical discovery / symbolic/programmatic search",
            "law_type": "mathematical formula / discovered relationships",
            "method_description": "Mentioned in related work/table as an LLM-based solution-generation approach (no experimental details provided in this paper).",
            "input_corpus_description": "",
            "evaluation_method": "",
            "results_summary": "",
            "notable_examples": "",
            "limitations_challenges": "",
            "baseline_comparison": "",
            "uuid": "e9826.1",
            "source_info": {
                "paper_title": "Language Model Crossover: Variation through Few-Shot Prompting",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathematical Discoveries From Program Search with Large Language Models",
            "rating": 2
        }
    ],
    "cost": 0.014408249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Model Crossover: Variation through Few-Shot Prompting</h1>
<p>ELLIOT MEYERSON, Cognizant AI Labs<br>MARK J. NELSON, American University<br>HERBIE BRADLEY, University of Cambridge \&amp; CarperAI<br>ADAM GAIER, Autodesk Research<br>ARASH MORADI, New Jersey Institute of Technology<br>AMY K. HOOVER, New Jersey Institute of Technology<br>JOEL LEHMAN, CarperAI</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Language Model Crossover (LMX). New candidate solutions are generated by concatenating parents into a prompt, feeding the prompt through any large pre-trained large language model (LLM), and collecting offspring from the output. Such an operator can be created through very few lines of code. The enormity and breadth of the dataset on which the LLM was trained, along with its ability to perform in-context learning, enables LMX to generate high-quality offspring across a broad range of domains. Domains demonstrated in this paper include (a) binary strings, (b) mathematical expressions, (c) English sentences, (d) image generation prompts, and (e) Python code; many more are possible. When integrated into an optimization loop, LMX serves as a general and effective engine of text-representation evolution.</p>
<p>This paper pursues the insight that language models naturally enable an intelligent variation operator similar in spirit to evolutionary crossover. In particular, language models of sufficient scale demonstrate in-context learning, i.e. they can learn from associations between a small number of input patterns to generate outputs incorporating such associations (also called few-shot prompting). This ability can be leveraged to form a simple but powerful variation operator, i.e. to prompt a language model with a few text-based genotypes (such as code, plain-text sentences, or equations), and to parse its corresponding output as those genotypes' offspring. The promise of such language model crossover (which is simple to implement and can leverage many different open-source language models) is that it enables a simple mechanism to evolve semantically-rich text representations (with few domain-specific tweaks), and naturally benefits from current progress in language models. Experiments in this paper highlight the versatility of language-model</p>
<p>Authors' addresses: Elliot Meyerson, Cognizant AI Labs, elliot.meyerson@cognizant.com; Mark J. Nelson, American University, mnelson@american.edu; Herbie Bradley, University of Cambridge \&amp; CarperAI, hb574@cam.ac.uk; Adam Gaier, Autodesk Research, adam.gaier@autodesk.com; Arash Moradi, New Jersey Institute of Technology, am3493@njit.edu; Amy K. Hoover, New Jersey Institute of Technology, ahoover@njit.edu; Joel LehmanCarperAI, lehman.154@gmail.com.</p>
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
(c) 2024 Association for Computing Machinery.</p>
<p>Manuscript submitted to ACM</p>
<p>crossover, through evolving binary bit-strings, sentences, equations, text-to-image prompts, and Python code. The conclusion is that language model crossover is a flexible and effective method for evolving genomes representable as text.</p>
<p>CCS Concepts: $\cdot$ Computing methodologies $\rightarrow$ Neural networks; Genetic algorithms; Genetic programming.
Additional Key Words and Phrases: neuroevolution, recombination, language models</p>
<h1>ACM Reference Format:</h1>
<p>Elliot Meyerson, Mark J. Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K. Hoover, and Joel Lehman. 2024. Language Model Crossover: Variation through Few-Shot Prompting. 1, 1 (May 2024), 38 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs; [9, 13]) are behind many of the approaches achieving state-of-the-art results in natural language processing domains, such as question-answering [19, 33, 70], code-generation [17, 66], and few-shot classification [13, 99]. One popular type of LLM is trained on corpora of human-authored text to predict the next token from previous ones, i.e. autoregressive LLMs (e.g. GPT-3), which at their core model a distribution of likely output sequences given an input sequence or prompt. In zero-shot prompting, a LLM generates an output response from a single input query. However, another popular prompting paradigm is few-shot prompting [13], wherein the input to the LLM contains a few examples of desired input-output behavior (e.g. how to classify a sentence's sentiment) preceding a new target input that the model is to classify. In this way, to some extent such LLMs have meta-learned how to learn a desired task given only a few natural-language examples [15, 119].</p>
<p>One reason this ability is exciting is because it highlights how LLMs can in effect be seen as powerful patterncompletion engines. Few-shot prompting works because the LLM can "guess the pattern" behind a few input/output pairs and generalize its behavior to a new target input (provided at the end of the few-shot prompt). The central insight of this paper is is that the pattern-completion ability of few-shot prompting can be leveraged to create a form of intelligent evolutionary crossover.</p>
<p>For example, if three text-based genotypes are drawn from a population and concatenated into a prompt, an ideal pattern-completion engine would analyze their commonalities and generate a new (fourth) genotype that qualitatively follows from the same distribution. In effect such an operator would combine aspects of the input genotypes, and indeed, an experiment in Section 4.1 demonstrates empirically that LLMs enable this with binary strings. Theoretically we also connect this form of LLM crossover (LMX) to estimation of distribution algorithms (EDAs; [3, 60]), wherein LMX can be seen as building an implicit probabilistic model of the input parent genotypes from which to sample a new offspring, through a single forward pass of the LLM. From the perspective of intelligent pattern-completion, this operator should naturally improve as LLMs increase in capabilities (which experiments here validate); furthermore, to increase performance the method can easily leverage the rise of open-source domain-specific LLMs that match a target domain (e.g. LLMs that focus on code, when the target domain is to evolve code), often with changing only a single line of code to rely on a different hosted model (e.g. through the HuggingFace model repository [128]).</p>
<p>The benefit of LMX is that evolution can easily and effectively leverage the semantically-rich (and generic) representation of text, e.g. without having to design domain-specific variation operators. LMX's versatility is highlighted in experiments with binary strings, style transfer of plain-text sentences, symbolic regression of mathematical expressions, generating images through prompts for a text-to-image model, and generating Python code. The results highlight the potential of the method to produce quality results across domains, often by leveraging the broad ecosystem of pretrained models that can be easily combined in many ways to quantify fitness or diversity, or to cross modalities (i.e. Manuscript submitted to ACM</p>
<p>from text to image). LMX may also synergize with recent LLM-based mutation techniques [63], and is amenable to similar possibilities such as fine-tuning an LLM as a way of accelerating search, although we leave these possibilities for future work (See Section 7).</p>
<p>In short, the main contributions of this paper are to introduce LMX, explore its basic properties, and highlight its versatility through testing it in a variety of domains. We will release an implementation of LMX and code to recreate the main experiments of the paper.</p>
<h1>2 BACKGROUND</h1>
<p>This section reviews foundation models and intelligent variation in evolutionary computation.</p>
<h3>2.1 Foundation Models</h3>
<p>A recent paradigm in ML is to train increasingly large models on internet-scale data, e.g. BERT and GPT-3 on text [13, 32], or DALL-E and stable diffusion on captioned images [93, 95]. Such models are sometimes called foundation models [9], as they provide a broad foundation from which they can be specialized to many specific domains (e.g. with supervised fine-tuning (i.e., further training on a domain-specific dataset) or prompt-engineering). Foundation models have enabled a vibrant ecosystem of specialized models [120] that can be combined in a plug-and-play way (e.g. models that measure sentiment of text [14], summarize text [111], write code [83], rank the aesthetics of images [30, 56, 103], and create high-dimensional embeddings of text or images [94, 132]. One contribution of this paper is to demonstrate how evolutionary methods can easily leverage this growing ecosystem to evolve high-quality artifacts in diverse applications.</p>
<p>One particularly exciting class of foundation models are pre-trained language models (LMs) that model the distribution of text. While early LMs used markov chains [107] or recurrent neural networks [40], more recently the transformer architecture [118] has enabled significant progress in NLP. Let $V$ be a vocabulary of text tokens, e.g., words or other atomic pieces of text. Then, $V^{<em>}$ is the set of strings made up of tokens from $V$. Given an input string $a_{1} a_{2} \ldots a_{T_{\text {in }}} \in V^{</em>}$, a large autoregressive transformer-based LM (LLM) probabilistically generates an output string:</p>
<p>$$
a_{T_{\text {in }}+1} a_{T_{\text {in }}+2} \ldots a_{T_{\text {in }}+T_{\text {out }}} \sim \operatorname{LLM}\left(a_{1} a_{2} \ldots a_{T_{\text {in }}}\right)
$$</p>
<p>where $a_{T_{\text {in }}+i}$ are all sampled autoregressively:</p>
<p>$$
a_{T_{\text {in }}+i} \sim \operatorname{LLM}<em 1="1">{\sigma}\left(a</em>\right]
$$} a_{2} \ldots a_{T_{\text {in }}+i-1}\right) \forall i \in\left[1, T_{\text {out }</p>
<p>where $\mathrm{LLM}<em i="i">{\sigma}$ is the softmax distribution over $V$ induced by a single forward pass through the transformer model. The method in this paper focuses on one emergent capability of LLMs: the potential to learn from text examples provided as input to the model when generating an output, which is called in-context learning or few-shot prompting [13, 119]. For example, including input-output examples of a text classification task in a prompt will improve an LLM's performance at that task. Say, for some input space $\mathcal{X}$ and output space $\mathcal{Y}$, we have ground truth classification examples $\left(x</em>)$, an LLM, a function $\phi$ for formatting a list of examples as a prompt (e.g., by concatenating them with a delimiter), and $\psi$ for extracting a prediction from text output (e.g., by splitting on a delimiter). Then, in-context learning}, y_{i}\right) \sim(\mathcal{X}, \mathcal{Y</p>
<p>with $k$ examples ( $k$-shot prompting) is successful if</p>
<p>$$
\begin{aligned}
\operatorname{Pr}\left[\psi\left(\operatorname{LLM}\left(\phi\left(\left[x_{1}, y_{1}, \ldots, x_{k}, y_{k}, x_{k+1}\right]\right)\right)\right)=y_{k+1}\right] &amp; &gt;\operatorname{Pr}\left[\psi\left(\operatorname{LLM}\left(\phi\left(\left[x_{1}, y_{1}, x_{k+1}\right]\right)\right)\right)=y_{k+1} \mid\right. \
&amp; &gt;\operatorname{Pr}\left[\psi\left(\operatorname{LLM}\left(\phi\left(\left[x_{k+1}\right]\right)\right)\right)=y_{k+1}\right]
\end{aligned}
$$</p>
<p>i.e., the model is more likely to produce the true target $y_{k+1}$ for $x_{k+1}$ if multiple ground truth pairs are provided. It is called in-context learning because it fits the standard machine learning paradigm of using a set of training data $\left{\left(x_{i}, y_{i}\right)\right}<em k_1="k+1">{i=1}^{k}$ to make predictions on hold-out data $x</em>$. Importantly, performance at in-context learning improves with model scale [15, 124], implying that methods relying upon this capability will benefit from continuing progress in LLM training. This paper highlights how the in-context learning capabilities of autoregressive LLMs (such as the popular GPT architecture) naturally enable a recombination operator. The next section reviews existing methods for intelligent variation in evolutionary computation.</p>
<h1>2.2 Intelligent Variation Operators</h1>
<p>Populations in evolutionary algorithms (EAs) generally evolve through high-performing candidate solutions being mutated or recombined to form the next generation. Such variation is critical as a primary driver of both exploration and exploitation of the search space [27]. Given the space of all candidate solutions $\mathcal{X}$, a genetic variation operator $g$ is a (usually stochastic) function that generates a child solution $x \in \mathcal{X}$ given a set of parent solutions $X \subset \mathcal{X}$. Since $g(X)$ induces a distribution over candidates, we can write</p>
<p>$$
x \sim g(X)
$$</p>
<p>If $|X|=1$ we call $g$ a mutation operator; if $|X|&gt;1$ we call $g$ a recombination or crossover operator. A solution $x$ is called a genotype since it is in the space where genetic operators are applied. An encoding $E: \mathcal{X} \rightarrow \mathcal{Y}$ maps a genotype $x$ to a phenotype $y$, so that its fitness $f(y)=f(E(x))$ can be evaluated with a fitness function $f: \mathcal{Y} \rightarrow \mathbb{R}$. Traditional mutation and crossover operators (such as one-point crossover or bit-flip mutation) do not explicitly seek to model and exploit regularities among high-fitness individuals (or do so in an implicit way [47, 76]), which can cause EAs to be relatively sample-inefficient in some situations when compared to statistical methods [116].</p>
<p>To address this limitation, strategies for generating intelligent variation have been a focus of much EA research. For example, evolving within the latent space of an ML model [35, 36, 92, 102], through training models to mimic mutations [53, 63], or code repair operators that draw on knowledge about the program's existing correct behaviors and integrate fault localization techniques to guide operators toward promising regions of improvement [62]. Such methods are intelligent in the sense that they autonomously draw on prior knowledge outside of the scope of the parent genomes in order to better generate promising child solutions.</p>
<p>One particularly popular such strategy is to build probabilistic models of high-performing individuals or to model elements of the search path taken across recent generations. For example, estimation of distribution algorithms (EDA; [3, 60]), covariance matrix adaptation evolution strategy (CMA-ES; [43]), and natural evolution strategies (NES; [125]) build and sample candidate solutions from an explicit probability distribution. While EDAs estimate the distribution of the solutions that have been sampled, CMA-ES additionally estimates the steps of the search direction. The LMX operator in this paper can be seen similarly as building a probabilistic model of individuals (here of parents, rather than the whole population), and doing so implicitly in the forward-pass of the LLM (through in-context learning).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Title</th>
<th>Model</th>
<th>Model Usage</th>
<th>Training Data</th>
</tr>
</thead>
<tbody>
<tr>
<td>2014</td>
<td>A Denoising Autoencoder that Guides Stochastic Search [22]</td>
<td>DAE</td>
<td>Solution Encoding</td>
<td>Current Run</td>
</tr>
<tr>
<td>2015</td>
<td>Denoising Autoencoders for Fast Combinatorial Black Box Optimization [89]</td>
<td>DAE</td>
<td>Solution Generation</td>
<td>Current Run</td>
</tr>
<tr>
<td>2018</td>
<td>Learning an Evolvable Genotype-Phenotype Mapping [77]</td>
<td>AE, DAE</td>
<td>Solution Encoding</td>
<td>Previous Runs</td>
</tr>
<tr>
<td>2018</td>
<td>Expanding Variational Autoencoders for Learning and Exploiting Latent Representations in Search Distributions [37]</td>
<td>VAE</td>
<td>Solution Generation</td>
<td>Current Run</td>
</tr>
<tr>
<td>2019</td>
<td>Estimation of Distribution using Population Queue based Variational Autoencoders [6]</td>
<td>VAE</td>
<td>Solution Generation</td>
<td>Current Run</td>
</tr>
<tr>
<td>2020</td>
<td>Harmless Overfitting: Using Denoising Autoencoders in EDAs [90]</td>
<td>DAE</td>
<td>Solution Generation</td>
<td>Current Run</td>
</tr>
<tr>
<td>2020</td>
<td>DAE-GP: Denoising Autoencoder LSTM Networks as Probabilistic Models in Estimation of Distribution Genetic Programming [127]</td>
<td>DAE</td>
<td>Solution Generation</td>
<td>Current Run</td>
</tr>
<tr>
<td>2022</td>
<td>Using Denoising Autoencoder Genetic Programming to Control Exploration and Exploitation in Search [126]</td>
<td>DAE</td>
<td>Solution Generation</td>
<td>Current Run</td>
</tr>
<tr>
<td>2022</td>
<td>Evolving through the looking glass: Learning Improved Search Spaces with Variational Autoencoders [4]</td>
<td>VAE</td>
<td>Solution Encoding</td>
<td>Previous Runs</td>
</tr>
<tr>
<td>2022</td>
<td>Evolution through Large Models [63]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model + Past Runs</td>
</tr>
<tr>
<td>2023</td>
<td>Language Model Crossover: Variation through Few-Shot Prompting (arsis) [2]</td>
<td>LLM</td>
<td>Solution Generation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Evoprompting: Language Models for Code-Level Neural Architecture Search [16]</td>
<td>LLM</td>
<td>Solution Generation, Instructed Mutation</td>
<td>Foundation Model + Current Run</td>
</tr>
<tr>
<td>2023</td>
<td>MarioGPT: Open-Ended Text2Level Generation through Large Language Models [112]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Wizardlm: Empowering Large Language Models to Follow Complex Instructions [130]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Fully Autonomous Programming with Large Language Models [69]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization [82]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution [34]</td>
<td>LLM</td>
<td>Solution Generation, Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers [41]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Large Language Models as Optimizers [131]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Eureka: Human-Level Reward Design via Coding Large Language Models [72]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Large Language Model for Multi-Objective Evolutionary Optimization [67]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Algorithm Evolution using Large Language Models [68]</td>
<td>LLM</td>
<td>Instructed Mutation</td>
<td>Foundation Model</td>
</tr>
<tr>
<td>2023</td>
<td>Mathematical Discoveries From Program Search with Large Language Models [96]</td>
<td>LLM</td>
<td>Solution Generation</td>
<td>Foundation Model</td>
</tr>
</tbody>
</table>
<p>Table 1. Evolutionary Recombination with Deep Generative Models. This table characterizes, in chronological order, the use of generative machine learning models in evolutionary recombination. Models include Autoencoders (Green), Denoising Autoencoders (Blue), Variational Autoencoders (Red), and Large Language Models (Orange). Model Usage encompasses Solution Encoding (Purple)—where models serve as a mapping from genotype to phenotype, Solution Generation (Brown)—where genotypes are directly sampled from the model, and Instructed Mutation (Yellow)—mutation guided by predefined prompts. Training Data details the data source for model training: Current Run (Darker Blue) indicates models trained on data from the current optimization run, Previous Runs (Light Blue) on data from past runs, Foundation Model (Dark Green) utilizes a large, general-purpose pre-trained model, and Foundation Model + Current Run (Gold) denotes a pre-trained model fine-tuned with current run (or past run) results. The burst of approaches using LLMs and evolution in 2023 highlights a shift towards pre-existing models and prompting, along with a surge of interest in both the machine learning and evolutionary algorithms communities.</p>
<h1>2.3 Evolution with Deep Generative Models</h1>
<p>Over the past decade, deep generative models have been explored as a method to aid evolutionary search (see Table 1). EDA approaches have leveraged autoencoders [46] to define distributions based on high-performing solutions identified during the search process. Autoencoders are used either as solution encodings which convert raw genotypes into phenotypes that align with the learned distribution; or as a mechanism for solution generation, with new solutions drawn directly from the established distribution.</p>
<p>The advent of large, pre-trained Foundation Models marks a significant step in this paradigm and has caused a flurry of exploration. Unlike traditional approaches that necessitate training models on solutions generated during the search, these advanced models can be directly leveraged, with distributions defined via strategic prompting. Foundation Models, particularly LLMs, bring a nuanced understanding of grammar and domain-specific patterns, enabling search across more abstract spaces, such as narratives [11] and high level programming languages [96]. This innovation introduces a novel dimension to search directionality through 'instructed mutation' - a method where instruction prompts guide the mutation process, offering an unprecedented level of natural-language-based control and specificity.</p>
<p>However, even without instructed mutation, Foundation Models contain an innate propensity to generate variation, due to their fundamental capacities as probabilistic pattern completion engines. The distribution from which new solutions are sampled can still be defined using top performing solutions from the population - but by providing multiple solutions directly to the model as a prompt, without explicit instruction that they be modified, and without retraining. The present work explores this fundamental approach.</p>
<h1>3 APPROACH: LANGUAGE MODEL CROSSOVER (LMX)</h1>
<p>The approach in this paper builds from the insight that the objective function used to train many self-supervised LLMs, i.e. next-token prediction [13], naturally lends itself to creating an evolutionary variation operator, from which evolutionary algorithms that represent genomes as text can be derived. The reason is that such an objective entails anticipating what comes next from some limited input context, and if that input consists of a few example genotypes, then the ideal anticipation is to continue that pattern, i.e. through suggesting a new genotype from the distribution implied by those examples. In other words, LLMs trained by next-token prediction can be seen as learning to become general pattern-completion engines. From this lens, as higher-performing LLMs (i.e. those with lower prediction loss on a held-out set) are continually developed, their performance as engines of evolutionary variation should continue to improve. Supporting this idea, when trained over a large amount of diverse examples, LLMs demonstrate an increasing capability for in-context learning (i.e. inferring novel associations within the input given at test-time when generating completions) [13, 15, 124].</p>
<p>What is intriguing about this insight is that the variation operator it suggests is (1) simple to implement (i.e. concatenate a few text-based genotypes into a prompt, run it through an LLM, and extract a new genotype from its output; we release code implementing it accompanying this paper), (2) relatively domain-independent (i.e. in theory it should be capable of generating meaningful variation for any text representation that has moderate support in the training set, which often encompasses an enormous crawl of the internet), and (3) should suggest increasingly semantically-sophisticated variation with more capable LLMs (i.e. an LLM that is generally better at predicting the next token in text will generate outputs in a manner that implies that it has a deeper semantic understanding of the input text). The experiments that follow add supporting evidence to these claims.</p>
<p>Figure 1 shows from a high level how LMX enables creating a domain-independent evolutionary algorithm for text representations. The basic idea is that given a set of a few text-based genotypes (or bootstrapping from a single genotype using prompt-based mutation [63]), an initial population can be generated through LMX. Then, a standard evolutionary loop can be instantiated by repeated selection and generation of new variation through LMX (See Algorithm 1).</p>
<p>Formally, the approach is grounded in a direct generalization of Eq. 3, namely, that providing a prompt of examples from a distribution can condition the LLM to generate further high-probability examples from that distribution. So, if we</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Evolutionary</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">LMX</span><span class="o">.</span><span class="w"> </span><span class="n">Lines</span><span class="w"> </span><span class="mi">7</span><span class="o">-</span><span class="mi">9</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">essense</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">LMX</span><span class="o">.</span>
<span class="w">    </span><span class="n">Given</span><span class="w"> </span><span class="n">LLM</span><span class="p">,</span><span class="w"> </span><span class="n">population</span><span class="w"> </span><span class="n">size</span><span class="w"> </span>\<span class="p">(</span><span class="n">n</span>\<span class="p">),</span><span class="w"> </span><span class="n">parents</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">crossover</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span>\<span class="p">),</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="n">function</span><span class="w"> </span>\<span class="p">(</span><span class="n">f</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">population</span><span class="w"> </span>\<span class="p">(</span><span class="n">P</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">random</span><span class="w"> </span><span class="n">text</span><span class="o">-</span><span class="n">based</span><span class="w"> </span><span class="n">individuals</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">done</span><span class="w"> </span><span class="n">evolving</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">P_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">new</span><span class="w"> </span><span class="p">}}</span><span class="o">=</span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="o">|</span><span class="n">P_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">new</span><span class="w"> </span><span class="p">}}</span>\<span class="n">right</span><span class="o">|&lt;</span><span class="n">n</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span>\<span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">randomly</span><span class="w"> </span><span class="n">choose</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span>\<span class="p">)</span><span class="w"> </span><span class="n">individuals</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span><span class="n">P</span>\<span class="p">)</span>
<span class="w">            </span>\<span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">prompt</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">backslash</span><span class="w"> </span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span>\<span class="n">backslash</span><span class="w"> </span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="w"> </span>\<span class="n">ldots</span><span class="w"> </span>\<span class="n">backslash</span><span class="w"> </span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">            </span><span class="n">output</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">LLM</span><span class="p">}(</span>\<span class="p">)</span><span class="w"> </span><span class="n">prompt</span><span class="w"> </span>\<span class="p">()</span>\<span class="p">)</span>
<span class="w">            </span><span class="n">children</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">extract</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">candidates</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">output</span>
<span class="w">            </span>\<span class="p">(</span><span class="n">P_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">new</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">P_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">new</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">cup</span>\<span class="p">)</span><span class="w"> </span><span class="n">children</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">P</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">P</span><span class="w"> </span>\<span class="n">cup</span><span class="w"> </span><span class="n">P_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">new</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">P</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">refine</span><span class="w"> </span>\<span class="p">(</span><span class="n">P</span>\<span class="p">)</span><span class="w"> </span><span class="n">down</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span><span class="n">n</span>\<span class="p">)</span><span class="w"> </span><span class="n">individuals</span><span class="w"> </span><span class="n">using</span><span class="w"> </span>\<span class="p">(</span><span class="n">f</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<p>have examples $x_{i} \sim \mathcal{X}$, then</p>
<p>$$
\operatorname{Pr}\left[\psi\left(\operatorname{LLM}\left(\phi\left(\left[x_{1}, \ldots, x_{k}\right]\right)\right)\right) \mid \mathcal{X}\right]&gt;\operatorname{Pr}\left[\psi\left(\operatorname{LLM}\left(\phi\left(\left[x_{1}\right]\right)\right)\right) \mid \mathcal{X}\right]&gt;\operatorname{Pr}\left[\psi\left(\operatorname{LLM}(\phi(\mid}])\right) \mid \mathcal{X}\right]
$$</p>
<p>Eq. 5 is applied to the evolutionary context by letting $x_{1}, \ldots, x_{k}$ be a set of parent genotypes and $\mathcal{X}$ a distribution of (relatively) high-performing genotypes. So, Lines 7-9 of Algorithm 1 are an instance of the general formulation of LMX:</p>
<p>$$
\operatorname{LMX}\left(x_{1}, \ldots, x_{k}\right)=\psi\left(\operatorname{LLM}\left(\phi\left(\left[x_{1}, \ldots, x_{k}\right]\right)\right)\right)
$$</p>
<p>This connection to $k$-shot prompting suggests that, at least in the case of a pre-trained LLM, recombination or crossover (i.e., $k&gt;1$ ) will be more effective than mutation $(k=1)$ or random sampling $(k=0)$. The resulting genetic operator is intelligent in the sense that, given a set of parents, it uses in-context learning (powered by the knowledge encoded in the LLM) to build a model of high-quality solutions, instead of directly searching in low-level genotype space.</p>
<p>In the experiments that follow, we use simple genetic algorithms (GAs; although one experiment instantiates a simple quality diversity algorithm). In theory, however, LMX can be generically applied to most EAs, e.g. multi-objective EAs [23, 29], evolutionary strategies [1, 5], or in support of open-ended evolution [121], but simply swapping it in as the genetic variation operator. How or if LMX can be applied in EAs that explicitly leverage probabilistic models of genotypes (e.g. EDAs [3, 60], natural evolution strategies [125], or CMA-ES [42, 43]) is an interesting question for future research (Section 7), although LMX does bear a theoretical relationship to EDAs, as explored in Section 5.</p>
<h1>4 EXPERIMENTS</h1>
<p>This section demonstrates the application of LMX to five domains, to investigate the basic properties of the method and illustrate the breadth of its applicability. Table 2 gives an overview of the experiments. Section 4.1 applies LMX to a toy domain to confirm the basic properties of the method; Section 4.2 applies LMX to symbolic regression, to show how evolving text representations with LMX can be effective in domains not classically represented as text; Section 4.3 applies LMX in its most natural setting: evolving well-formed natural-language sentences, while also showing how the method can be naturally integrated with other NLP components and QD algorithms; Section 4.4 applies LMX to evolving text prompts for image generation, a domain that further highlights the plug-and-play capability of LMX with other deep generative models, while enabling a comparison to zero-shot generation and where naive evolution of</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Section</th>
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">Genotype</th>
<th style="text-align: left;">Phenotype</th>
<th style="text-align: left;">LLM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">4.1</td>
<td style="text-align: left;">Binary Strings</td>
<td style="text-align: left;">text</td>
<td style="text-align: left;">binary strings</td>
<td style="text-align: left;">Pythia-70M to 6.9B (eight models)</td>
</tr>
<tr>
<td style="text-align: left;">4.2</td>
<td style="text-align: left;">Symbolic Regression</td>
<td style="text-align: left;">text</td>
<td style="text-align: left;">math expressions</td>
<td style="text-align: left;">Pythia-1.4B, GALACTICA-1.3B</td>
</tr>
<tr>
<td style="text-align: left;">4.3</td>
<td style="text-align: left;">Modifying Sentiment</td>
<td style="text-align: left;">text</td>
<td style="text-align: left;">text</td>
<td style="text-align: left;">Pythia-1.4B</td>
</tr>
<tr>
<td style="text-align: left;">4.4</td>
<td style="text-align: left;">Image Generation</td>
<td style="text-align: left;">text</td>
<td style="text-align: left;">image</td>
<td style="text-align: left;">Pythia-2.8B</td>
</tr>
<tr>
<td style="text-align: left;">4.5</td>
<td style="text-align: left;">Sodaracers</td>
<td style="text-align: left;">text</td>
<td style="text-align: left;">Python functions</td>
<td style="text-align: left;">CodeGen-350M, 2B, 6B</td>
</tr>
</tbody>
</table>
<p>Table 2. Overview of experiments. In all domains, the genotype is text, since text is the substrate LMX evolves. In all domains except Modifying Sentiment, this text is converted to another form (phenotype) for evaluation. Section 4.1 evaluates the effect of LLM size within the Pythia family; Sections 4.3 and 4.4 use LLMs within that family; Sections 4.2 and 4.5 use LLMs that are more specialized to the domain. Taken together, the experiments demonstrate that LMX is a generic method of generating variation for evolution.
text is a strong baseline (due to the fact that text-to-image models are fairly agnostic to grammatical correctness); and, finally, Section 4.5 shows how LMX can be applied to generating Python code, clearly situating the method across this intersection of the genetic programming and LLM code-generation communities. Source code will be made publicly available for each domain.</p>
<h1>4.1 Illustrative Example: Binary Strings</h1>
<p>As an instructive example to explore the properties of LMX, in this section this operator is applied to generate variation in the space of binary strings (e.g. composed of text strings such as "011000"); first, to see whether LMX can generate meaningful and heritable variation (i.e. to create new valid binary strings from old ones, and that the new ones resemble the old ones); and then, to see whether LMX can successfully drive evolution of binary strings, in this case to maximize the number of 1 s (i.e. the OneMax problem, where the fitness function is the number of 1 s in a valid binary string).</p>
<p>A first question is whether a pretrained LLM (here an 800-million parameter Pythia model [7]), given only a few examples of such genomes, can generate meaningful variation (i.e. without any hard-coded knowledge about the representation). To explore this question, a prompt is generated by concatenating randomly chosen length-6 binary strings separated by newlines; the LLM's response (truncated after three new lines) is interpreted as three offspring individuals. Figure 2a shows how often such a prompt will generate valid individuals (i.e. strings of length six composed of 1 s and 0 s ) as a function of number of examples in the prompt, and how many novel offspring (i.e. the size of the set of individuals generated that are distinct from the parents) are generated on average from 20 trials of LMX crossover on the same set of parents (averaged across 20 randomly-sampled parent sets). A follow-up experiment, with length-9 binary strings, demonstrates how LMX in this domain improves with larger LLMs (details in appendix A.1; results shown in Figure 2b). The conclusion is that indeed, LMX can reliably generate novel, valid offspring (from as few as three examples).</p>
<p>A second question is whether LMX can create heritable variation. Evolution requires there to be meaningful information transmitted from parents to offspring. One way to explore this is to measure whether a prompt composed of highly-related binary strings produces novel but nearby offspring (e.g. as measured by edit distance). To test this, prompts were created by sampling the neighborhood around one of two reference strings (i.e. single-step mutations from either the all-ones or all-zeros string), and offspring were generated from the LLM. Indeed, offspring generated from the neighborhood of the all-ones string had significantly higher (Mann-Whitney U-test; $p&lt;0.001$ ) hamming distance from the all-zeros string than the all-ones string (and vice-versa; see Figure 3a).
Manuscript submitted to ACM</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The effect on LMX from varying the number of parents and LLM size. (a) As the number of parent genotypes input into the LLM is increased, the percent of valid offspring approaches $100 \%$. The number of novel genotypes generated on average from 20 applications of LMX (which at 3 offspring per application can result in at most 60 offspring) to a random set of parents reaches its maximum at four parents (while five parents tends to more often produce offspring that duplicate one of the parents exactly). The conclusion is that LMX effectively generates variation from as few as three input genotypes. (b) As the parameter count (i.e., number of weights trained with SGD) of the LLM is increased in the length-9 binary string domain, the percent of valid offspring and number of novel offspring (out of at most 60) also increase. The number of parents is fixed to 3 for this experiment. Note $m$ indicates millions of parameters, while $b$ indicates billions. The conclusion is that in this domain LMX becomes more effective with larger LLMs.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Heritability and convergence of LMX on binary strings. (a) The histogram shows the distribution of how far offspring are from the all 1 s string, depending on if parents are taken in the neighborhood of the all-1s or all-0s string. As expected these distributions are significantly different. The conclusion is that LMX indeed produces heritable variation. (b) Convergence results (median and IQR) for a simple genetic algorithm using either LMX or one-point crossover. Though fewer solutions converge on the optima using LMX the classical recombination (16/20 vs. 20/20), mean values are higher (Mann-Whitney $p=0.002$ ). Though not as efficient as a domain-specific operator, it is clear that LMX can indeed drive an evolutionary process.</p>
<p>A final instructive question is whether an evolutionary process can be successfully driven by LMX. To explore this, we test LMX in OneMax, i.e. evolving the all-1s string, in a simple genetic algorithm. A small population ( 10 individuals) of length 10 bit strings is initialized randomly. At each generation the top 5 solutions, plus the elite solution from any previous generations, are chosen as parents for recombination to form the next population. LMX recombination is compared to recombination via one point crossover with a $10 \%$ chance of a bit flip mutation. Figure 3b shows the median max/mean fitness values over 20 runs of each, clearly illustrating LMX's ability to drive an evolutionary process. Overall, these experiments highlight basic properties of LMX, showing how it can evolve string-based representations without domain-specific operators.</p>
<h1>4.2 Symbolic Regression</h1>
<p>To demonstrate LMX's potential in a more challenging task, this section applies the algorithm to symbolic regression, a key domain of interest for genetic programming [59, 74, 85, 100], and more recently for the larger machine learning community [8, 52, 58, 88]. The goal of symbolic regression is to discover a mathematical expression that models a data set accurately, while also being as compact as possible [58]. Beyond the usual benefits of regularization, compactness is desirable for interpretability of the expression, e.g., to enable scientific insights [51, 100, 117, 122].</p>
<p>Symbolic regression is challenging to tackle with hand-designed operators, due to non-locality and discontinuities in the space of expressions. Existing symbolic regression approaches use carefully-developed representations, genetic operators, and auxiliary methods like gradient-based/convex coefficient optimization [18, 55, 115] to construct the right kind of search process for reaching high-performing expressions that look like the kinds of expressions the experimenter is interested in. With LMX, these challenges can be avoided by simply feeding parent expressions into the language model. Note that this section does not aim to provide a comprehensive comparison against state-of-the-art-methods, but instead aims to show how LMX can be applied off-the-shelf to important domains with complex representations.
4.2.1 Experimental Setup. The LLM for this experiment was the 1.3B-parameter version of GALACTICA [114]. GALACTICA's training set was specifically designed to assist in scientific endeavors, and includes tens of millions of LaTeX papers, and thus many human-designed equations, making it an appropriate choice for symbolic regression. This choice also highlights how different off-the-shelf LLMs can be selected for LMX based on properties of the problem.</p>
<p>When the ground truth expression for symbolic regression is known, we run the risk that the expression is already in the dataset used to train the LLM. To avoid such test-set contamination, we consider a 'black-box' problem (which has no known ground-truth expression) from the established SRBench testbed [58]. The 'banana' problem was chosen because there is a clear Pareto front across existing methods, making it easy to see how LMX compares. This black-box problem was originally derived from a popular ML benchmark in the KEEL data set repository [31]; it has 5300 samples and two input features $x_{1}, x_{2}$.</p>
<p>In this experiment, crossover prompts began with the string "Below are 10 expressions that approximate the dataset: \n" followed by seven randomly selected parents from the population separated by newlines (see Figure 4 for examples). Each subsequent line generated by the model was interpreted as a possible offspring, interpreted as Python code, and simplified using sympy (as in the SRBench comparisons [58]). Up to three child expressions were accepted for each forward pass of the LLM. Each child was evaluated against the dataset, using $R^{2}$ for fitness; any child that could not be parsed or that raised an exception during evaluation was discarded. The same compactness/complexity measure was used as in SRBench, i.e., 'expression size': the number of nodes in the parse tree of the expression.</p>
<p>The initial population was constructed from 113 popular symbolic regression benchmarks ${ }^{1}$. The idea is that these benchmark expressions capture the distribution of the kinds of expressions humans want symbolic regression to discover, thereby avoiding the need to generate random expressions from scratch. To give each benchmark expression a greater chance of initial success, the initial population consisted of 1000 candidates, each generated by randomly selecting a benchmark expression and then randomly mapping its input variables $x_{1}^{\prime}, x_{2}^{\prime}, \ldots$ to the input variables $x_{1}, x_{2}$ in the test problem. Thereafter, the population size was set to 50 . Each generation the combined parent and child population was culled to 50 individuals via tournament selection and then 50 new children were generated. The algorithm was run for 5000 generations using a single GeForce RTX 2080 Ti GPU (which took roughly 100 hours).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Below are 10 expressions that approximate the dataset:
sin(1.5<em>x1)</em>cos(0.5<em>x2)
$x 2 * * 2+x 2 * * 2+x 2+\operatorname{sin}(x 2)+\operatorname{sin}(x 2 * * 2)$
$1.5 * \exp (x 1)+5.0 * \cos (x 1)$
$x 1 * * 3 </em>(x 2-5) <em>(\operatorname{sin}(x 1) * * 2 * \cos (x 1)-1) * \exp (-x 1) * \sin (x 1) * \cos (x 1)$
$-2.1 * \sin (1.3 * x 2) * \cos (9.8 * x 1)+2$
$\sin (x 2 * * 2) * \cos (x 2)-5$
$\exp (-(x 1-1) * * 2) /(6.25 </em>(0.4 * x 1-1) * * 2+1.2)$
$\sin (2.1 * x 1) * \cos (0.9 * x 2)+6.5$
$1.5 * \sin (2.1 * x 1) * \cos (0.5 * x 2) * \exp (x 1)+5.5$
$\sin (0.5 * x 2) * \exp (x 2)-5$
$x 1 * * 2 <em>(x 2-5) </em>(2.1 * \sin (x 1) * * 2 * \cos (x 1)-1) * \exp (-x 1) * \sin (x 1) * \cos (x 1)$
$x 1 * * 2 <em>(x 2-5) * * 2 </em>(\sin (x 1) * * 2 * \cos (x 1)-1) * * 2 * \exp (-x 1) * * 2 * \sin (x 1) * * 2 * \cos (x 1) * * 2$</p>
<p>Answer:
Your code should be the same as your first line, but
$1.5 * \exp (x 1)+5.0 * \cos (x 1)$
should be
$1.5 * \exp (x 1) * \cos (x 1)+5.0$
as</p>
<p>Below are 10 expressions that approximate the dataset:
$x 1 * x 2 /((x 2-3) * * 2+1)$
$x 2 * * 2 /(10000 <em>((x 1-3) * * 2+(x 2-3) * * 2+4))$
$x 1 * * 2+x 2 * * 2$
$x 1 * x 2 /((x 1-3) * * 2+(x 2-3) * * 2+2)$
$(x 2-3) /((x 2-3) * * 2+1) * * 2$
$\exp (-x 1 * * 2)$
$x 1 * x 2 * * 2 /((x 1-3) * * 2+2)$
$(x 2-3) /((x 1-3) * * 2+1) * * 2$
$x 1 * x 2 * * 2 </em>((x 1-3) * * 2+(x 2-3) * * 2+2)$
$(x 2-3)+x 1+x 2 /((x 1-3) * * 2+(x 2-3) * * 2+1) * * 2$
$(x 2-3) * * 2+x 2 * 2 /(10000 *((x 1-3) * * 2+(x 2-3) * * 2+4))$
$(x 2-3) * * 2+x 1 * 2+x 1+x 2 /((x 1-3) * * 2+(x 2-3) * * 2+2)$
$x 2 /((x 1-3) * * 2+1) * * 2$
$x 1 * x 2 /((x 1-3) * * 2+(x 2-3) * * 2+2)$
$x 2 /(((x 1-3) * * 2+1) * * 2$</p>
<p>Below are 10 expressions that approximate the dataset:
sqrt $(x 1 * * 2+x 1+x 2+2 * * 2 * 2+1)+1)+1.5$
$x 1 * * 2+x 1 * x 2+2 * * 2 * * 2+1&lt;4000$
$x 1 * * 2+4 * * 2 * * 2+1&lt;400000000$
sqrt $(x 1 * * 2+x 1 * x 2+2 * * 2+1)+1) \leqslant 1.4740426350899773765$
$(x 1 * * 2+x 1 * x 2+1) * * 3 \leqslant 1.336395683282781841$
sqrt $(x 1 * * 2+x 1 * x 2+1200 * x 2 * * 2) \leqslant 1.1969521946187728419$
$\operatorname{sqrt}(x 1 * * 2+x 1 * x 2+3 * x 2 * * 2+1) \leqslant 2.068817213090777115$
$x 1 * * 2+x 1 * * 2+1&lt;2.407303205449004$
$(x 1 * * 2+x 1 * x 2+1) * * 2 \leqslant 1.529026864021614135$
$\operatorname{sqrt}(x 1 * * 2+x 1 * x 2+x 2+x 2+1)+1) \leqslant 3.425986639014800117$
$\operatorname{sqrt}(x 1 * * 2+x 1 * x 2+1) * * 4 \leqslant 7.639437278029600423$
$\operatorname{sqrt}(x 1 * * 2+x 1 * x 2+1200 * x 2 * *$</p>
<p>Below are 10 expressions that approximate the dataset:
-0.0005082377<em>cos $(x 2) * \cos (x 1-0.6) * \cos (x 2-0.6) * \cos (x 2-0.4)$
$2.6 * \cos (x 1+0.7) * \cos (x 2-0.7) * \cos (x 2+0.8)$
$2.4 * \cos (x 1+0.5) * \cos (x 2-0.6) * \cos (x 2+0.9)$
$-0.231 * \sin (x 1) * \cos (x 2+0.2) * \cos (x 2+0.5)$
$0.003898335144775358 * \sin (x 1+0.2) * \sin (x 2-0.5) * \cos (x 2+0.3)$
$2.2 * \cos (x 2) * * 2 * \cos (x 1+0.3) * \cos (x 2-0.8) * \cos (x 2-0.4)$
$2.4 * \cos (x 1+0.5) * \cos (x 2-0.3) * \cos (x 2+0.7)$
$2.6 * \cos (x 1+0.5) * \cos (x 2-0.4) * \cos (x 2+0.6)$
$-0.179 * \sin (x 1) * \cos (x 2+0.4) * \cos (x 2-0.8)$
$0.0014232921 * \sin (x 1+0.2) * \sin (x 2-0.6) * \cos (x 2+0.4)$
$2.4 * \cos (x 2) * * 2 * \cos (x 1+0.2) * \cos (x 2-0.8)$
-0.179</em>sin</p>
<p>Fig. 4. Four examples of LMX for symbolic regression. The prompt of seven parents is in blue; the LLM output parsed as (up to three) offspring is in violet; remaining discarded LLM output is in gray. In all cases, children exhibit meaningful variations of parents.</p>
<p>To contextualize the convergence behavior of LMX, gplearn (one of the most popular symbolic regression tools ${ }^{2}$ ) was run with hyperparameters previously used for SRBench [58]; as an ablation to evaluate the benefit of using an LLM specialized for scientific work, LMX was also run with a 1.4-billion parameter Pythia model ${ }^{3}$; as an ablation to assess</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. Example convergence trajectory. Fitness over time for a single run of LMX (Galactica) on the SRBench black-box 'banana' problem [58]. The expression with the highest fitness so far is plotted at several generations to illustrate the kinds of improvements evolution finds. Evolution settles on a core functional skeleton relatively quickly (i.e., $c_{1} e^{-c_{2} x_{1}^{c_{3}}-c_{6} x_{2}^{c_{3}}} \cos \left(x_{1}+c_{6} x_{2}+c_{7}\right)$, with $x_{1}, x_{2}$ input variables and $c_{1}$ constants), after which it tunes constants to a surprising specificity, while simultaneously tweaking and augmenting the skeleton. Even after the process appears to have converged, around generation 3000 it discovers innovations leading to further substantial improvements. This late boost highlights the ability of the LLM to be an engine of interesting and valuable hypotheses in mathematical/numerical spaces.
the impact of initialization vs. LMX itself, a version of gplearn was run with the same population initialization as LMX, by writing around 100 lines of complex custom code to translate the benchmark expressions to the format required by gplearn ${ }^{4}$. Ten independent runs were performed for each experimental setup.
4.2.2 Results. LMX produces competitive results, generating fit and parsimonous expressions. Figure 5 shows how fitness evolves over generations for one run of LMX, with the expression of highest fitness so far plotted at several generations to illustrate the kinds of improvements evolution finds. Interestingly, the method finds parsimonious expressions even though there is no explicit drive towards parsimony in the algorithm. An implicit drive towards parsimony is enforced by the maximum text size the model processes, which in this experiment was set to 500 tokens; prompts longer than this cannot produce offspring. Future work could investigate the effects of tuning this parameter or developing other methods for incorporating explicit drives towards parsimony (Section 7). Beyond discovering a useful algebraic scaffolding for the problem, LMX tunes constants to a surprising degree, indicating that the method is capable of continuous optimization, even though LLMs operate in a space of discrete tokens; this is an interesting ability that could also be further explored in future work (Section 7).</p>
<p>Figure 6 shows that LMX (using the GALACTICA LLM) achieves overall higher fitness and lower expression size than gplearn, and the choice of LLM appears to have a substantial impact, with the Pythia runs falling short of the others. This result highlights the value in being able to easily drop in a particular LLM that could be well-suited to a</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6. Convergence comparison and LLM ablation. (a) In terms of number of fitness evaluations, LMX converges in a similar manner to gplearn, when Galactica is the underlying LLM. As an ablation, when Pythia is the LLM, performance is not as strong. This result highlights the value of being able to swap in different LLMs depending on the domain. (Line is median, shading is IQR) (b) LMX avoids model bloat as it incrementally improves fitness, thereby satisfying a key desirable property for SR. (c) Overall, the final expressions returned by LMX are of comparable quality to those of gplearn. The conclusion is that the general LMX approach can yield high-quality solutions even in highly specialized domains like SR.
given domain. Figure 6 also shows that the customized version of gplearn initialized in the same way as LMX does not improve over the standard gplearn. This result reinforces the idea that in classical GP methods the kinds of expressions that are easy to evolve may not be the kinds humans are most interested in, while LMX thrives in this space since the LLM is naturally familiar with human-designed expressions due to its training data. This bias towards human-designed expressions is also a natural bias against model bloat, since humans strive to design compact expressions.</p>
<p>Figure 7 shows that the performance of LMX on this problem is competitive with state-of-the-art methods [58], settling at an intermediate point along the Pareto front. However, unlike these other methods, which carefully consider model representations, genetic operators, distributions of synthetic functions, bloat, multiple objectives, etc., we simply ask an off-the-shelf language model to be the generator in a minimal evolutionary loop. Note that the claim here is not that LMX is better than these existing methods, but simply that it is able to evolve reasonable solutions. In particular, the comparison methods all used a fixed amount of CPU compute, while LMX uses GPU (See Section 6 for discussion of this distinction). That said, the results clearly show the ability of LMX, with little domain-specific tuning and an unsophisticated optimization loop, to nonetheless optimize symbolic expressions in an intuitive and desirable way.</p>
<h1>4.3 Modifying Sentence Sentiment</h1>
<p>LMX is next applied to evolve plain-text English sentences. While LMX could be applied in many ways to evolve sentences, the focus here is a form of natural language style transfer [50], i.e. to translate an input into a new style while maintaining as much as possible the spirit of the original. Such an application can be important in optimizing how ideas are communicated amongst humans, i.e. one may want to communicate specific content but in a style maximally amenable for a target recipient; this defines an optimization problem over text. In this proof-of-concept experiment, the task is to take a seed sentence, and maximally change its sentiment (i.e. how positive the sentence is) with minimal change to the sentence itself.</p>
<p>To do so, a simple quality-diversity evolutionary algorithm [65, 79] is applied that measures quality as maximizing the sentiment of a sentence and measures diversity as distance from the seed sentence. In particular, sentiment is measured through the "cardiffnlp/twitter-roberta-base-sentiment-latest" model hosted on HuggingFace, which is part of the TweetNLP project [14]; the network takes in a sentence, and outputs classification probabilities for whether the sentence is positive, negative, or neutral. The experiments focus on using the probability of a positive sentiment as</p>
<p>the fitness function (although see appendix C for results with negative sentiment as fitness). For measuring distance from the seed sentence, a separate neural network generates a 384-dimensional embedding of a sentence (in particular the "sentence-transformers/all-MiniLM-L6-v2" model, from the sentence transformer project [94]). Distance is then quantified as the Euclidean distance between the embeddings of a new individual and the seed sentence.</p>
<p>For the QD algorithm, we use MAP-Elites [79] with a 1D map (with 30 niches, spanning a distance of 0 to a distance of 1.5 from the seed sentence in the embedding space; at 0 distance the sentences are exactly the same, while at a distance of 1.5 no words may be shared). The algorithm is run independently on three pessimistic quotes: "Whenever a friend succeeds, a little something in me dies," from Gore Vidal, "Kids, you tried your best and you failed miserably. The lesson is, never try," from Homer Simpson, and Woody Allen's "Life is divided into the horrible and the miserable." Each run targets changing the sentiment of a single sentence (from negative to positive). To seed the initial MAP-Elites population for each run, we use LMX on the three initial quotes to generate 196 initial offspring. From there onwards, offspring for MAP-Elites are generated from LMX by one of two strategies for sampling individuals from the map: (1) randomly sampling three elites from the map (LMX), or (2) probabilistically selecting three elites from nearby cells (LMX-Near; the motivation is that nearby elites will generate more focused variation). MAP-Elites runs consist of 2500 evaluations each; to confirm that the evolutionary process generates quality solutions beyond the direct generative ability of the LLM, a baseline control is also tested that generates 2500 offspring only from the initial 3 seed sentences. Ten runs were conducted for each combination of sentence and method; each run took on the order of minutes on a Google Colab notebook.</p>
<p>Quantitatively, both LMX-Near and LMX achieved higher QD scores (sum of the fitnesses of all niches in the map) than the control for all three quotes (Mann-Whiteny U-test; $p&lt;1 e-5$ ), and were always able to discover high-sentiment sentences. Interestingly, LMX-Near and LMX performed significantly differently only for the Gore Vidal quote (LMXNear produced higher final QD-scores; Mann-Whitney U-test; $p&lt;0.05$ ). Future work is thus needed to determine whether there exist methods for robustly choosing parents for LMX more effectively (Section 7). QD score plots for the Homer Simpson quote is shown in Figure 8, and plots for the other quotes (and representative heatmaps of final MAP-Elites maps) are shown in Appendix C.</p>
<p>Qualitatively, evolution is generally able to find intuitive trade-offs between sentiment and distance from the original sentence. For example, Figure 9 shows the final map of elites from a representative run on the Homer Simpson quote (with LMX-Near), with some highlighted sentences. At sufficient distance from the original sentence, evolution often produces repetitive, unrelated text: e.g. "You are the best that ever happened to me! You are the best that ever happened Manuscript submitted to ACM</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 8. Modifying Simpsons Quote Sentiment. The plot compares LMX-Near, LMX, and the baseline control in increasing the positive sentiment of the quote: "Kids, you tried your best and you failed miserably. The lesson is, never try." LMX and LMX-Near do not perform significantly differently, but both significantly outperform the control. Example sentences of such runs are shown in appendix section C.1.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 9. Example pareto front from improving positivity of a negative quote. The plot shows non-dominated individuals from the final map of a representative run, across the tradeoff between distance from the seed sentence (as measured by an embedding model) and the probability of positive sentiment (as measured by a sentiment analysis model). The full table of final sentences is shown in appendix C.
to me! You are the best that ever happened to me!" Also, sometimes the method produces incoherent or grammaticallyflawed sentences, e.g. "you tried your best and you failed. The lesson is, you can never stop trying. Kids, you tried your best and you". Optimization pressure for coherence (i.e. to maintain high log-probability under a LLM), or better/larger sentiment models, might address these problems, as discussed in Section 7. The conclusion is that LMX can be used to discover solutions for natural language tasks like text style transfer; beyond sentiment other styles could be explored by using different NLP models as fitness functions, e.g. emotion-recognition NLP models [81].</p>
<h1>4.4 Evolving Stable Diffusion Images</h1>
<p>This section explores the application of LMX to another creative domain: evolving prompts for generative text-to-image models. Stable Diffusion ${ }^{5}$ is a publicly available latent diffusion model [95] that supports CLIP-guided [91] text-to-image</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>synthesis. Since Stable Diffusion's release, artists, researchers, and hobbyists have developed prompting practices, swapping tips for constructing text prompts to produce desired outputs [84]. For a human with a desired output, discovering an effective prompt defines an optimization problem over text. The research question here is whether LMX can effectively evolve Stable Diffusion prompts. The genotype for this experiment is a text string, the prompt fed into the Stable Diffusion model. Beyond allowing us to investigate how LMX interacts with other generative models, this domain enables comparison to two natural baselines (1) classical one-point crossover, and (2) zero-shot generation. (1) In contrast to other domains in this paper, even though the genotype is text, text-to-image models tend to be quite robust to grammatical errors and nonsense, so a one-point crossover that produces a mangled bag-of-words is a strong baseline. (2) If we have a particular criteria for an image in mind, we can simply prompt the LLM directly to produce a prompt for such an image, i.e., without providing any example (parent) prompts; since there are no examples, this is called zero-shot.</p>
<p>For all setups, the initial population is seeded by randomly choosing from a set of 80,000 human-designed Stable Diffusion prompts that were scraped from lexica.art. ${ }^{6}$ The phenotype is the image generated by feeding a given prompt to Stable Diffusion. We make Stable Diffusion deterministic by reseeding with a fixed PRNG seed before each image is generated, so a given prompt always produces the same image. The EA is the same as in Section 4.2; experimental details are in Appendix D. Three interpretable fitness functions are explored, maximizing respectively the "redness", "greenness" and "blueness" of an image. Redness is measured by excess red: the sum of the red channel of an RGB image, minus half the sum of the other two channels $(R-0.5 G-0.5 B)$. Excess green and excess blue are defined analogously. These functions are easy to calculate, correspond roughly to perceived image color (e.g., they are well studied in agricultural image processing [75]), and provide a proof-of-concept where performance can be visually verified at a glance. Three random seeds are selected to initialize the population for each color, giving a total of nine runs per method. Each run uses a population size of 50 for 100 generations, for a total of 5000 evaluations.</p>
<p>Given two parent prompts, the one-point crossover baseline splits the prompts on whitespace and chooses crossover points uniformly at random. LMX prompts consist of the header "List of text-to-image prompts for generating the most <color> image possible:" followed by lines "Prompt: <parent>" and finally an empty "Prompt:" for the LLM to fill in. The zero-shot baseline is the same, but with no parent prompts. Two additional comparisons were also run (1) LMX without the header, to ablate the impact of removing this basic knowledge about the problem, and (2) random human-designed prompts from the initial dataset, setting a baseline for the fitness we can expect without any generative or evolutionary process.</p>
<p>Figure 10a shows performance aggregated over nine runs (three seeds for each color for each method; normalized to $[0,1]$ based on the min and max fitness for each each seed; mean and std. err. shown) shows that LMX substantially outperforms the alternatives. One-point crossover is a strong baseline, with performance statistically similar to the 'no header' ablation, supporting the idea that the ability to naturally incorporate natural language problem specifications is a key advantage of LMX over classical EAs. The zero-shot baseline quickly stagnates, as it is unable to iteratively refine it's initial solutions; even randomly-selected human prompts eventually outperform it, as they have greater diversity; both these baselines far underperform the evolutionary methods. So, overall, it is the combination of evolution with the native linguistic capacity of LLMs that makes LMX excel. Figure 10b shows the highest-fitness prompts and corresponding images of LMX for each color. All three images have clearly optimized for the target color. All three</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 10. Image generation results. (a) Performance aggregated (mean and std. err.) over nine runs (three seeds for each color for each method; normalized to $[0,1]$ based on the min and max fitness for each each seed) shows that LMX substantially outperforms the alternatives. One-point crossover is a strong baseline, with performance statistically similar to the LMX - No Header ablation. The zero-shot baseline quickly stagnates, as it is unable to iteratively refine it's initial solutions; even Human Random solutions eventually outperform it, as they have greater diversity. (b) The highest-fitness prompts and corresponding images of LMX for each color all include the word "background", but vary in the length and detailed content, highlighting LMX's ability to discover diverse, non-obvious solutions.
prompts include the word "background", but vary in the length and kind of detailed content, highlighting LMX's ability to discover diverse, non-obvious solutions. The conclusion is that LMX can enable sensible evolution of images.</p>
<h1>4.5 LMX with Python Sodaracers</h1>
<p>Finally, to explore whether LMX can generate variation in code, we apply LMX to evolving Python programs in the Sodarace environment from Lehman et al. [63], which also explored evolving Python programs with LLMs (we leverage the OpenELM implementation of sodarace [12]). Sodarace is a 2D simulation of robots with arbitrary morphology constructed from Python functions (the genotype) which output a dictionary specifying joints and muscles, and how they are connected. A Sodaracer robot is instantiated from this dictionary and placed in the environment, and the distance travelled by the robot is used as our fitness function.</p>
<p>We evolve these programs with MAP-Elites [79], using the distance travelled by the generated Sodaracers in a simulation as the fitness and the morpology of the Sodaracer (height, width, and mass) as the dimensions of the behavior space (as in Lehman et al. [63]). We explore the effect on evolution from varying the number of parents that LMX uses to generate offspring (from one to three parents).</p>
<p>Seven pre-existing Sodarace programs were chosen as seeds (details in appendix E). To initialize the population for evolution, LMX was prompted across combinations of these seeds as parents. We randomize the order of seeds for each application of LMX, to control for variance in results from the order of programs in the prompt. The programs were all given the same Python function signature make_walker(): and then concatenated together in the prompt. Note that we begin each completion with this function signature to improve performance (experiments where the LLM prompt did not end with the function signature performed worse; see appendix E). The LLM output is then interpreted as a potential offspring Python program, to be evaluated in the Sodarace environment.</p>
<p>During evolution steps, we use the same procedure, but randomly select populated niches in the map to select from to build the prompt (as many niches are sampled as parents for each separate treatment), and choose the fittest individual in each niche. We experiment with three different-sized LLMs from the Salesforce CodeGen suite [83], a set of models trained on a large dataset of code in many languages, including Python.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 11. Sodaracer results. We show the results for varying numbers of parents in the LLM prompt and across LLM scale. (left) Number of niches filled in MAP-Elites. (center) Quality-Diversity scores (sum of the fitnesses of all niches in the map) (right) Validation rate (\%) for the generated Sodaracers. LMX generally benefits from more examples in its prompt, is able to produce reasonable variation, and often creates valid Sodarace mutations, highlighting its promise for evolving code.</p>
<p>We perform 10,000 evolutionary iterations (corresponding to 10,000 outputs from the language model, not all of which are valid programs) using 500 initialization iterations. We evaluate the performance of each experimental treatment by computing the percentage of valid programs, number of niches filled at the end of evolution, and the QD score at the end of evolution.</p>
<p>The results from these experiments are shown in Figure 11, showing that as the number of parents in the prompt increases, the diversity of offspring generally increases, as measured by the number of niches filled and the QD score (This effect is even more dramatic in the experiments where the LLM prompt did not end with the function signature-A single parent yields no valid offspring (see Appendix Figure 17)).</p>
<p>Furthermore, a significant proportion of generated offspring are valid Sodaracers (roughly $30 \%$ with the 6B model), highlighting the potential for evolution. Experiments with a single seed in the prompt can be viewed as a simple mutation operator (a different approach to the same end in Lehman et al. [63]). There is a clear trend in model size, showing that the 6B model can create higher fitness and more diverse Sodaracers, along with a slight trend towards an improved proportion of valid programs with model scale. These results therefore demonstrate the promise of LMX for evolving non-trivial Python code.</p>
<h1>5 WHAT MAKES LMX EFFECTIVE?</h1>
<p>The breadth of experiments in Section 4 show how LMX can serve as a simple and general method for evolution across a range of domains. This section presents some perspectives on where the effectiveness of LMX could come from, including its connection to EDAs and how it could serve as a starting point for more powerful future algorithms.</p>
<h3>5.1 Connection to EDAs</h3>
<p>An EDA constructs an explicit probabilistic distribution $D$ fit to the parent set $\left{x_{1}, \ldots, x_{M}\right}$, and samples child solutions $x$ from $D[45,61]$. In contrast, a standard GA generates children by sampling from an implicit conditional probability distribution $p_{g}\left(x \mid\left[x_{1}, \ldots, x_{M}\right]\right)$ induced by the process of randomly sampling parents and applying a stochastic reproduction operator $g$ (e.g., a crossover operator; Eq. 4). LMX occupies an intermediate level of explicitness: The conditional distribution induced by feeding the parent prompt into the LLM is explicit in that it yields a series of probability distributions over tokens, and the probability of any output sequence can be directly computed, but is Manuscript submitted to ACM</p>
<p>implicit in the sense that the internal workings of the distribution are encoded opaquely within the millions or billions of parameters and activations of the LLM for a given prompt.</p>
<p>Whatever the level of explicitness, LMX acts like an EDA in that it builds a probabilistic model of parents, from which children are then sampled. This connection is most clear when LMX takes as input the full population of $n$ potential parents. Let $S_{n}$ be a selection operator that refines a collection of $N&gt;n$ candidates down to $n$ (as in Line 13 of Alg. 1).</p>
<p>Theorem 5.1 (EDA Representation). LMX and $S_{n}$ are sufficient operators to define an EDA.
Proof. Let $P_{t}$ denote the current population at iteration $t$ (as when entering the loop at Line 5 in Alg. 1). Then,</p>
<p>$$
P_{t+1}=S_{n} \circ\left{x_{i} \sim \operatorname{LMX}\left(P_{t}\right)\right}_{i=1}^{N&gt;n}
$$</p>
<p>denotes an algorithm (akin to the loop in Alg. 1) in which at each iteration the next population is constructed by sampling $N$ candidates from LMX conditioned on all of $P_{t}$ and then refining down to $n$ candidates via $S_{n}$. Using Eq. 6,</p>
<p>$$
\begin{aligned}
P_{t+1} &amp; =S_{n} \circ\left{x_{i} \sim \psi\left(\operatorname{LLM}\left(\phi\left(P_{t}\right)\right)\right)\right}<em n="n">{i=1}^{N&gt;n} \
&amp; =S</em>} \circ \psi \circ \beta_{N} \circ \operatorname{LLM<em t="t">{o} \circ \phi\left(P</em>\right)
\end{aligned}
$$</p>
<p>where $\beta_{N}$ is the autoregressive sampling operation (Eq. 2) applied multiple times to generate $N$ candidates. Rotating the recursive composition to the left yields</p>
<p>$$
D_{t+1}=\operatorname{LLM}<em n="n">{o} \circ \phi \circ S</em>\right)
$$} \circ \psi \circ \beta_{N}\left(D_{t</p>
<p>where $D_{t}$ defines the distribution (i.e., model) that candidates are sampled from at iteration $t$. Then, $\alpha=\operatorname{LLM}<em N="N">{o} \circ \phi$ is a model-building operator called only once per iteration that constructs a probabilistic model from a set of solutions, and $\beta=\psi \circ \beta</em>$, we have}$ is a sampling operator that samples new solutions from a model. So, along with the selection operator $S=S_{n</p>
<p>$$
D_{t+1}=\alpha \circ S \circ \beta\left(D_{t}\right)
$$</p>
<p>which is the functional form of an EDA.</p>
<p>The key design feature of an EDA is the class of distributions $\mathcal{D}$ to which $D$ belongs. This class $\mathcal{D}$ can range from simple univariate distributions [3, 44] to more complex models like Bayesian networks [86, 87]. What is the class $\mathcal{D}<em _mathrm_LM="\mathrm{LM">{\mathrm{LM}}$ from which LMX constructs parent distributions? Due to its in-context learning capabilities [97, 129], the LLM can be seen as attempting to infer underlying distribution of parents in the prompt, and to generate continuations accordingly. By concatenating parents in a random order, the implicit signal to the LLM is that the list is unordered. The LLM may notice some accidental patterns in the order, but, as the number of parents increases, e.g., when LMX processes the full population as in the EDA above, the significance of such spurious patterns diminishes and a well-trained LLM is more likely to perceive the order as random. These parents are text-based objects that must have been sampled from some ground truth distribution $D^{<em>}$, and thus the LLM's highest-probability action is to keep sampling objects from $D^{</em>}$ as it generates output. In other words $\mathcal{D}</em>}}$ consists of distributions of objects that are found in sets that might appear in the universe of data from which the dataset used to train the LLM was drawn. An ideal EDA would select the most probable $D=D_{\mathrm{EDA}}^{*} \in \mathcal{D<em 1="1">{\mathrm{LM}}$ based on the parent set $\left{x</em>\right}$. E.g.,}, \ldots, x_{k</p>
<p>$$
D_{\mathrm{EDA}}^{*}=\underset{D \in \mathcal{D}<em i="1">{\mathrm{LM}}}{\operatorname{argmax}} p(D) \prod</em> \mid D\right)
$$}^{k} p\left(x_{i</p>
<p>where $p(D)$ is the prior probability of $D$ in $\mathcal{D}_{\mathrm{LM}}$. As the LLM becomes a better and better in-context learner, it becomes better able to detect subtler patterns within a prompt of randomly-ordered concatenated parents, and thus</p>
<p>$$
\operatorname{LMX}\left(x_{1}, \ldots, x_{k}\right) \approx D_{\mathrm{EDA}}^{*}
$$</p>
<p>Note that the left side depends on an ordered list of parents, while the right side has removed this dependency on order.
We investigate this relationship and the conditions under which the approximation tightens using a simple bitstring case. Optimizing pseudo-Boolean functions using EDAs involves establishing the probability distribution of each bit containing a ' 1 ' or ' 0 '. The Univariate Marginal Distribution Algorithm [80], the prototypical EDA, samples $\lambda$ individuals each iteration, choosing the best $\mu$. The probability of a ' 1 ' in each position is then determined by the relative frequency of ' 1 's at that location in the selected population. In LMX a similar selection process is followed and, by prompting the model with the selected parents, a probability distribution is defined.</p>
<p>Despite the implicit definition in LMX, the probability distributions produced by LMX and an EDA can be directly compared. After prompting the LLM with the parent population, we can extract the probability distribution of a ' 1 ' or ' 0 ' before each token is generated. This provides an explicit probability distribution analogous to that of the EDA. In this way we can test the hypothesis that LMX approximates an EDA more closely as the size of the parent population increases. We examine the similarity of distributions with increasing populations in a six-bit case with the following procedure:
(1) For each bit in the string, the probability of it being a 1 or 0 is drawn uniformly at random from $[0,1]$.
(2) A set of parents is generated according to the distribution established in the previous step.
(3) Given this set of parents the mean absolute difference in the probability of a 0 for each gene between the resulting EDA and LMX distributions is calculated.
(4) The entire experiment is repeated with a different initial probability distribution.</p>
<p>When we examine the difference between the EDA and LMX distributions with an increasing number of parents (Figure 12), we find that indeed the disparity between the two distributions diminishes as the number of parents increases, i.e., LMX becomes more similar to the EDA.</p>
<p>Though a faithful application of an EDA may include the full parent population in each parent prompt, the experiments in Section 4 save compute by sampling only a small number of parents. Nonetheless, by comparing LMX to EDAs it may be possible to analyze the optimization behavior of LMX [57] (e.g., global convergence analysis [134]), as discussed in Section 7. Note that, as we are using a causal LLM, probabilities of each bit are not technically independent, but rather conditional on the previously generated bits in the genome as well as on the order of the parents. This nuanced scenario is also characteristic of more sophisticated EDAs that incorporate conditional dependencies [106]. Despite this confounding factor, it is clear that with a larger number of parents both approaches converge toward the same distribution - and this connection to EDAs may help to explain why LMX is effective as an off-the-shelf genetic operator across a wide range of domains.</p>
<h1>5.2 Universality of LMX</h1>
<p>Section 5.1 highlighted the connection between LMX and EDAs. This section explores another property of LMX, its theoretical universality (i.e. its ability in theory to express any genetic operator). With a sufficiently expressive class of model, such as Bayesian networks [86, 87], EDAs can approximate any candidate distribution as the size of the parent set increases [134]. Not only can LMX sample from distributions represented by an EDA, but it can in principle sample from Manuscript submitted to ACM</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 12. LMX and EDA Probability Distributions Across Different Sized Parent Sets. The average difference in gene probabilities predicted by LMX and EDA approaches 0 across various parent set sizes in a population of bit strings. Each parent set is generated by randomly setting the probability for each gene bit. The EDA gene probabilities are derived from the frequency of the gene values of the parents while the LMX gene probabilities are obtained from the language model's output logits (softmax applied with temperature $=1.0$ ). The Y-axis represents the mean absolute difference across all genes between the two methods' probability distributions. Error bars indicate the standard deviation over 20 experiments. The discrepancy between LMX and EDA probability predictions decrease with the number of parents.
any conditional probability distribution, making it universal in the space of genetic operators, even with small parent sets. Recent theoretical work has shown how crossover of large neural networks can yield universal approximation of reproduction distributions [76]. LMX also achieves theoretical universal approximation via large neural networks, but by feeding parents directly into the LLM, instead of crossing-over weights. If modification (e.g., fine-tuning) of LLM weights is permitted, this result follows naturally from the universal approximation ability of NNs [26, 48, 54] (note that this property also applies in the single-parent case for mutation-based evolution through LLMs [63]):</p>
<p>Theorem 5.2 (weight-based universality). For any genetic operator g on candidate space $\mathcal{X}$ and $\epsilon&gt;0$, if $\phi: 2^{\mathcal{X}} \rightarrow$ $V^{<em>}$ is injective, and $\psi: V^{</em>} \rightarrow \mathcal{X}$ is surjective, then $\exists$ an LLM s.t. for all parent sets $X$ of $g$ and children $x$</p>
<p>$$
|\operatorname{Pr}[x \mid g(X)]-\operatorname{Pr}[x \mid L M X(X)]|&lt;\epsilon
$$</p>
<p>Proof. $\operatorname{LMX}(X)=\psi(\operatorname{LLM}(\phi(X)))$. Since $\phi$ is injective, for all parent sets $X, \phi(X)=s_{X}$ is unique. Since $\psi$ is surjective, $\forall x \in \mathcal{X}, \exists s_{x}$ s.t. $\psi\left(s_{x}\right)=x$. Let $S_{x}=\left{s_{x}: \psi\left(s_{x}\right)=x\right}$. It suffices to find an LLM with weights such that</p>
<p>$$
|\operatorname{Pr}[x \mid g(X)]-\operatorname{Pr}\left[s_{x} \in S_{x} \mid \operatorname{LLM}\left(s_{X}\right)\right]|&lt;\epsilon
$$</p>
<p>The existence of such an LLM exists follows from the universal approximation capability of transformers [133].
However, when coupled with external memory, existing fixed pre-trained LLMs today, e.g., Flan-U-PaLM 540B [21], have been shown to implement universal Turing machines (UTMs) [38, 104], implying that universality can be achieved through effective prompting schemes, without altering LLM weights:</p>
<p>Theorem 5.3 (prompt-based universality). For any genetic operator g on candidate space $\mathcal{X}$ and $\epsilon&gt;0$, if the LLM is a UTM and $\psi$ is surjective, then $\exists \phi$ s.t. for all parent sets $X$ of $g$ and children $x$</p>
<p>$$
|\operatorname{Pr}[x \mid g(X)]-\operatorname{Pr}[x \mid L M X(X)]|&lt;\epsilon
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts
Manuscript submitted to ACM&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
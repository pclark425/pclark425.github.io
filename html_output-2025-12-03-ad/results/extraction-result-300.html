<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-300 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-300</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-300</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-49c45d2a2773c537804c38d69cde67e00fbad6fe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/49c45d2a2773c537804c38d69cde67e00fbad6fe" target="_blank">Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns.</p>
                <p><strong>Paper Abstract:</strong> Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e300.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e300.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0301 / 0613 / 1106)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 family (gpt-3.5-turbo checkpoints used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoder-only autoregressive LLM family evaluated in the paper on few-shot arithmetic (addition) tasks; exhibits strong dependence of arithmetic accuracy on number tokenization scheme and tokenization direction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0301, gpt-3.5-turbo-0613, gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only autoregressive transformer (not specified in-paper)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition (few-shot addition problems)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>7–9 digit addends (experiments focused on 7–9 digit additions; specific controlled triplets included addends/answers of lengths 7–9 digits)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot prompting (1,2,4,8-shot); greedy decoding (temperature=0); input tokenization manipulated by inserting delimiters every 3 digits to enforce R2L tokenization versus standard L2R tokenization; delimiter controls and thinking-token controls; conversion prompting (repeat-and-convert)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Strong tokenization dependence: example 8-shot numbers for gpt-3.5-turbo-0301 — L2R: ~75.6% accuracy (8-shot average reported); R2L (comma-delimited): ~97.8% accuracy (8-shot). 1-shot: L2R 68.5% vs R2L 95.6%. In length-mismatch cases (answer longer than addends) L2R accuracy falls to 8.25%. In the length-controlled set (1300 problems) R2L missed 25/1300 problems (24 of which were off-by-one errors); L2R (length-match subset) missed 56/900 (53 off-by-one).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Multiple behavioral/measured signals: (1) Stereotyped 'digit 4' error in L2R length-mismatch cases where the model consistently outputs correct first three digits (first output token) and systematically errs on the fourth digit — suggests the model is performing a systematic, token-aligned computation rather than fuzzy memorization. (2) Off-by-one errors concentrate at output-token boundaries (last digit of a 3-digit token), implicating tokenization alignment as a failure locus. (3) Log-prob analyses: 'digit 4' errors show high entropy (~2.06 nats) and the correct answer appears in top-5 ~49.6% of time, consistent with near-random guessing among 10 possible fourth-digit tokens; off-by-one errors show low entropy (~0.45 nats) with the second-most-likely token being the true answer in all examined off-by-one cases, indicating confusion mainly between two adjacent tokens. (4) Models can be prompted to convert L2R inputs into R2L form (repeat the problem in R2L) and then solve correctly, indicating conversion is a viable workaround rather than an implicit forward-pass conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Tokenization-induced performance gap decreases with model scale / newer GPT-4 updates: GPT-3.5 shows a large gap; later GPT-4 versions mitigate the gap (March→June updates improved arithmetic), though GPT-4 Turbo regressed somewhat indicating scale and model-specific updates affect ability to override tokenization biases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Severe failures when input/output tokenization misaligns (length-mismatch cases where answer has more digits than addends); stereotyped digit-4 substitution errors in L2R length-mismatch; off-by-one errors concentrated at token boundaries; occasional hallucinated trailing zeros when uncommon separator formats are used in L2R controls.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared L2R (default) vs R2L enforced by inserting delimiters every 3 digits; compared different delimiters (commas, '#', '.', '$', space) to rule out semantic comma priors; thinking-token (extra tokens) controls; conversion prompting (repeat L2R→R2L) and output-only tokenization control (answer in R2L but input L2R).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>For GPT-3.5, number tokenization direction matters dramatically for addition: enforcing right-to-left tokenization (R2L) via 3-digit delimiters raises few-shot addition accuracy from ~76% to ~98% (8-shot), while left-to-right tokenization produces systematic, token-boundary-aligned error modes (including a consistent 'digit‑4' mistake when answers are longer).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e300.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e300.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (various checkpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 family (gpt-4-0314, gpt-4-0613, gpt-4-1106-preview / turbo versions referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger OpenAI model family evaluated for the same few-shot addition tasks; shows the same tokenization-direction dependence but to a lesser degree in some GPT-4 versions and with variability across updates/turbo variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0314, gpt-4-0613, gpt-4-1106-preview / turbo variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only autoregressive transformer (not specified in-paper)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition (few-shot addition problems)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>7–9 digit addends (same controlled experiments as GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot prompting (mostly 8-shot reported), delimiter-based enforcement of R2L tokenization (commas or other single-token delimiters), conversion prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GPT-4 versions perform better overall at arithmetic and show a reduced tokenization gap relative to GPT-3.5: R2L still outperforms L2R, but the difference is smaller in newer GPT-4 models (March→June updates improved performance), though GPT-4 Turbo in the latest checkpoint showed a stronger tokenization effect again. Exact per-checkpoint numbers are reported in Figure 12 (paper) — overall trend: GPT-4 is more robust but not immune.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same qualitative mechanistic signals as GPT-3.5 (token-boundary sensitivity, off-by-one concentrated at token boundaries) but larger models are better at 'overriding' tokenization-induced inductive biases, suggesting capacity/scale and training data can reduce but not necessarily eliminate tokenization-aligned failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Evidence that larger / newer model variants (GPT-4) mitigate the negative inductive bias introduced by L2R multi-digit tokens; however, model updates and architecture/size trade-offs (e.g., GPT-4 vs GPT-4 Turbo) produce non-monotonic effects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tokenization-direction dependence persists: L2R remains worse than R2L in many cases; length-mismatch and token-boundary off-by-one errors still present but less dominant than in GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across multiple GPT-4 checkpoints and GPT-3.5 checkpoints; compared L2R vs R2L tokenization, delimiters, and prompting interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Scaling and model updates reduce but do not eliminate tokenization-dependent failures on addition: GPT-4 is more robust than GPT-3.5, yet right-to-left tokenization still yields higher accuracy and fewer systematic token-boundary errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e300.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e300.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tokenization direction (L2R vs R2L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number tokenization direction: left-to-right (L2R) vs right-to-left (R2L) token segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Intervention that changes how numeric strings are segmented into tokens (L2R: default OpenAI BPE behavior with up-to-3-digit tokens segmented left-to-right; R2L: enforced by inserting separators every 3 digits from the right so tokens align with least-significant digits first).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (evaluated under both tokenization directions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition (primary testbed)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>7–9 digit additions; experiments deliberately chose addend lengths such that addends often correspond to exactly 3-token representations under the studied tokenizers</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Enforce R2L by inserting delimiters (commas or alternative single-token delimiters) every 3 digits from the right; compare to default L2R tokenization; delimiter-control and thinking-token-control experiments performed to rule out confounds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>R2L tokenization substantially improves accuracy: e.g., for GPT-3.5 8-shot R2L ≈97.8% vs L2R ≈75.6% (8-shot). The gap can be as large as ≈20+ percentage points depending on model/version. Delimiter choice (commas, '#', '.', '$', space) did not materially change the R2L advantage, indicating tokenization direction (not specific comma semantics) is the dominant factor.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Performance differences are attributable to tokenization alignment between addends and answer: R2L aligns least-significant digits across input and output tokens, making carries and digit propagation easier to handle; L2R creates input-output token misalignment (especially when the answer is longer), producing systematic errors concentrated at token boundaries. Off-by-one errors are more likely at token boundaries (last digit of a 3-digit token).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>The harmful inductive bias of L2R segmentation weakens with model scale and with later GPT-4 updates, suggesting larger/more trained models can partly overcome tokenization-aligned biases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>L2R causes catastrophic failure in length-mismatch cases (answer longer than addends) and stereotyped digit-4 errors; token-boundary off-by-one errors more frequent under both tokenizations but concentrated at token edges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct comparison between L2R (default) and R2L (delimiters every 3 digits) across models and shot counts; delimiter-type controls and thinking-token token-count controls were used to rule out alternative explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Tokenization direction (alignment of digit tokens relative to least-significant digits) is a major inductive bias for arithmetic: enforcing R2L segmentation yields large improvements in few-shot addition accuracy and eliminates stereotyped token-boundary failures present under L2R.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e300.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e300.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conversion prompting (repeat-and-convert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought–inspired conversion prompting: few-shot prompting models to repeat or reformat inputs from L2R to R2L and then answer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting intervention where the model is few-shot trained to take an input in one tokenization, repeat the problem in the preferred tokenization (R2L), and then produce the answer — shown to recover much of the performance lost under L2R.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (tested primarily on GPT-3.5 gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>7–9 digit addends (same problem sets as other experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot prompting where examples demonstrate repeating the input in R2L tokenization and answering in R2L (repeat L2R→R2L); also tested answering-only output tokenization control (incentivize answering in R2L without repeating the problem) as a control.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Repeat L2R→R2L prompting recovers nearly the full accuracy of receiving the problem natively in R2L: models perform nearly as well when they convert themselves as when provided R2L input. By contrast, merely instructing the model to answer in R2L (without repeating the problem in R2L) does not recover the same performance, indicating that seeing the problem in R2L is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>The success of explicit conversion prompting indicates models do not reliably perform an implicit forward-pass conversion from L2R to R2L; instead, they can deterministically perform the conversion when prompted (suggesting the conversion is within model capability but not applied automatically during standard forward completions). This supports an interpretation that tokenization alignment matters for the model's learned computation pathway.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Conversion prompting effectiveness increases with number of shots (model adherence to the suggested repetition style improves), consistent with in-context learning shaping the model's output format and internal processing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When models are prompted to only output in R2L (without repeating the problem), they often fail to improve and may adhere inconsistently to format instruction depending on number of shots; conversions require adherence to the repetition behavior to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared repeat-and-convert few-shot prompting vs output-only formatting prompts vs native R2L input; assessed shot dependence (1–8 shots).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Prompting models to explicitly reformat L2R inputs into R2L and then solve largely recovers the performance advantage of native R2L inputs, showing the problem is a tokenization-alignment inductive bias that can be worked around via in-context reformatting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e300.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e300.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observed mechanistic failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stereotyped token-boundary and digit-substitution error mechanisms discovered via error & log-prob analyses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detailed empirical mechanistic observations about how models fail on addition: a consistent 'digit 4' substitution in L2R length-mismatch cases, concentration of off-by-one errors at token boundaries, and characteristic log-prob/entropy signatures revealing guessing vs binary confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (empirical analyses reported primarily on gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition (diagnostic analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>7–9 digit additions; length-controlled dataset of 1300 problems (various addend/answer length triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Error pattern analysis (digit-level), conditioning on length-match vs length-mismatch; logprob/top-5 token inspection; lower-bound entropy computation from top-5 probabilities; token-frequency proxy analysis using BPE merge ranks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Quantitative failure-mode stats: in the length-controlled set (1300 problems) the L2R length-mismatch condition produced 91.25% incorrect; among those incorrects, the model always got digit 4 wrong (while digits 1–3 were correct). Off-by-one errors account for nearly all remaining errors in other conditions: R2L missed 25/1300 (24 off-by-one) and L2R (length-match) missed 56/900 (53 off-by-one).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Key signals: (1) 'Digit 4' errors show high entropy (~2.066 nats) and correct answers appear in top-5 ~49.6% — consistent with near-random guessing among 10 candidate tokens for the problematic digit. (2) Off-by-one errors show low entropy (~0.45 nats) and the second-most-likely token is the true answer in all such cases — consistent with tight confusion between adjacent tokens. (3) Errors concentrate at the last digit of an output token (token boundary), implicating tokenization-induced boundary crossing as critical. (4) Mild evidence for substitution toward more frequent tokens in some conditions, but frequency (BPE merge-rank proxy) is not a dominant explanatory factor overall.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>These stereotyped failure modes are strongest in smaller models (GPT-3.5) and become weaker in larger / later GPT-4 checkpoints, consistent with scale and training reducing but not eliminating tokenization-driven algorithmic brittleness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Systematic digit-substitution at specific digit positions (e.g., digit 4), off-by-one errors at token boundaries, uncertainty-driven guessing among digit tokens, and length-mismatch catastrophic drops under L2R.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Error patterns compared between R2L vs L2R, length-match vs length-mismatch problems, and across model checkpoints; logprob distributions compared between correct and incorrect cases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>The paper provides mechanistic behavioral evidence that arithmetic errors in LLMs are tokenization-aligned: token-boundary misalignment yields systematic, repeatable errors (e.g., a consistent wrong fourth digit), and logprob/entropy signatures differentiate between random-digit guessing and tight two-way token confusions (off-by-one).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What algorithms can transformers learn? a study in length generalization. <em>(Rating: 2)</em></li>
                <li>Teaching algorithmic reasoning via in-context learning. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot numerical reasoning. <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. <em>(Rating: 2)</em></li>
                <li>xval: A continuous number encoding for large language models. <em>(Rating: 1)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-300",
    "paper_id": "paper-49c45d2a2773c537804c38d69cde67e00fbad6fe",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (gpt-3.5-turbo-0301 / 0613 / 1106)",
            "name_full": "OpenAI GPT-3.5 family (gpt-3.5-turbo checkpoints used in experiments)",
            "brief_description": "Decoder-only autoregressive LLM family evaluated in the paper on few-shot arithmetic (addition) tasks; exhibits strong dependence of arithmetic accuracy on number tokenization scheme and tokenization direction.",
            "citation_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0301, gpt-3.5-turbo-0613, gpt-3.5-turbo-1106)",
            "model_size": null,
            "model_architecture": "decoder-only autoregressive transformer (not specified in-paper)",
            "arithmetic_operation_type": "addition (few-shot addition problems)",
            "number_range_or_complexity": "7–9 digit addends (experiments focused on 7–9 digit additions; specific controlled triplets included addends/answers of lengths 7–9 digits)",
            "method_or_intervention": "few-shot prompting (1,2,4,8-shot); greedy decoding (temperature=0); input tokenization manipulated by inserting delimiters every 3 digits to enforce R2L tokenization versus standard L2R tokenization; delimiter controls and thinking-token controls; conversion prompting (repeat-and-convert)",
            "performance_result": "Strong tokenization dependence: example 8-shot numbers for gpt-3.5-turbo-0301 — L2R: ~75.6% accuracy (8-shot average reported); R2L (comma-delimited): ~97.8% accuracy (8-shot). 1-shot: L2R 68.5% vs R2L 95.6%. In length-mismatch cases (answer longer than addends) L2R accuracy falls to 8.25%. In the length-controlled set (1300 problems) R2L missed 25/1300 problems (24 of which were off-by-one errors); L2R (length-match subset) missed 56/900 (53 off-by-one).",
            "mechanistic_insight": "Multiple behavioral/measured signals: (1) Stereotyped 'digit 4' error in L2R length-mismatch cases where the model consistently outputs correct first three digits (first output token) and systematically errs on the fourth digit — suggests the model is performing a systematic, token-aligned computation rather than fuzzy memorization. (2) Off-by-one errors concentrate at output-token boundaries (last digit of a 3-digit token), implicating tokenization alignment as a failure locus. (3) Log-prob analyses: 'digit 4' errors show high entropy (~2.06 nats) and the correct answer appears in top-5 ~49.6% of time, consistent with near-random guessing among 10 possible fourth-digit tokens; off-by-one errors show low entropy (~0.45 nats) with the second-most-likely token being the true answer in all examined off-by-one cases, indicating confusion mainly between two adjacent tokens. (4) Models can be prompted to convert L2R inputs into R2L form (repeat the problem in R2L) and then solve correctly, indicating conversion is a viable workaround rather than an implicit forward-pass conversion.",
            "performance_scaling": "Tokenization-induced performance gap decreases with model scale / newer GPT-4 updates: GPT-3.5 shows a large gap; later GPT-4 versions mitigate the gap (March→June updates improved arithmetic), though GPT-4 Turbo regressed somewhat indicating scale and model-specific updates affect ability to override tokenization biases.",
            "failure_modes": "Severe failures when input/output tokenization misaligns (length-mismatch cases where answer has more digits than addends); stereotyped digit-4 substitution errors in L2R length-mismatch; off-by-one errors concentrated at token boundaries; occasional hallucinated trailing zeros when uncommon separator formats are used in L2R controls.",
            "comparison_baseline": "Compared L2R (default) vs R2L enforced by inserting delimiters every 3 digits; compared different delimiters (commas, '#', '.', '$', space) to rule out semantic comma priors; thinking-token (extra tokens) controls; conversion prompting (repeat L2R→R2L) and output-only tokenization control (answer in R2L but input L2R).",
            "key_finding": "For GPT-3.5, number tokenization direction matters dramatically for addition: enforcing right-to-left tokenization (R2L) via 3-digit delimiters raises few-shot addition accuracy from ~76% to ~98% (8-shot), while left-to-right tokenization produces systematic, token-boundary-aligned error modes (including a consistent 'digit‑4' mistake when answers are longer).",
            "uuid": "e300.0",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 (various checkpoints)",
            "name_full": "OpenAI GPT-4 family (gpt-4-0314, gpt-4-0613, gpt-4-1106-preview / turbo versions referenced)",
            "brief_description": "Larger OpenAI model family evaluated for the same few-shot addition tasks; shows the same tokenization-direction dependence but to a lesser degree in some GPT-4 versions and with variability across updates/turbo variants.",
            "citation_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0314, gpt-4-0613, gpt-4-1106-preview / turbo variants)",
            "model_size": null,
            "model_architecture": "decoder-only autoregressive transformer (not specified in-paper)",
            "arithmetic_operation_type": "addition (few-shot addition problems)",
            "number_range_or_complexity": "7–9 digit addends (same controlled experiments as GPT-3.5)",
            "method_or_intervention": "few-shot prompting (mostly 8-shot reported), delimiter-based enforcement of R2L tokenization (commas or other single-token delimiters), conversion prompting",
            "performance_result": "GPT-4 versions perform better overall at arithmetic and show a reduced tokenization gap relative to GPT-3.5: R2L still outperforms L2R, but the difference is smaller in newer GPT-4 models (March→June updates improved performance), though GPT-4 Turbo in the latest checkpoint showed a stronger tokenization effect again. Exact per-checkpoint numbers are reported in Figure 12 (paper) — overall trend: GPT-4 is more robust but not immune.",
            "mechanistic_insight": "Same qualitative mechanistic signals as GPT-3.5 (token-boundary sensitivity, off-by-one concentrated at token boundaries) but larger models are better at 'overriding' tokenization-induced inductive biases, suggesting capacity/scale and training data can reduce but not necessarily eliminate tokenization-aligned failure modes.",
            "performance_scaling": "Evidence that larger / newer model variants (GPT-4) mitigate the negative inductive bias introduced by L2R multi-digit tokens; however, model updates and architecture/size trade-offs (e.g., GPT-4 vs GPT-4 Turbo) produce non-monotonic effects.",
            "failure_modes": "Tokenization-direction dependence persists: L2R remains worse than R2L in many cases; length-mismatch and token-boundary off-by-one errors still present but less dominant than in GPT-3.5.",
            "comparison_baseline": "Compared across multiple GPT-4 checkpoints and GPT-3.5 checkpoints; compared L2R vs R2L tokenization, delimiters, and prompting interventions.",
            "key_finding": "Scaling and model updates reduce but do not eliminate tokenization-dependent failures on addition: GPT-4 is more robust than GPT-3.5, yet right-to-left tokenization still yields higher accuracy and fewer systematic token-boundary errors.",
            "uuid": "e300.1",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Tokenization direction (L2R vs R2L)",
            "name_full": "Number tokenization direction: left-to-right (L2R) vs right-to-left (R2L) token segmentation",
            "brief_description": "Intervention that changes how numeric strings are segmented into tokens (L2R: default OpenAI BPE behavior with up-to-3-digit tokens segmented left-to-right; R2L: enforced by inserting separators every 3 digits from the right so tokens align with least-significant digits first).",
            "citation_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 / GPT-4 (evaluated under both tokenization directions)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition (primary testbed)",
            "number_range_or_complexity": "7–9 digit additions; experiments deliberately chose addend lengths such that addends often correspond to exactly 3-token representations under the studied tokenizers",
            "method_or_intervention": "Enforce R2L by inserting delimiters (commas or alternative single-token delimiters) every 3 digits from the right; compare to default L2R tokenization; delimiter-control and thinking-token-control experiments performed to rule out confounds.",
            "performance_result": "R2L tokenization substantially improves accuracy: e.g., for GPT-3.5 8-shot R2L ≈97.8% vs L2R ≈75.6% (8-shot). The gap can be as large as ≈20+ percentage points depending on model/version. Delimiter choice (commas, '#', '.', '$', space) did not materially change the R2L advantage, indicating tokenization direction (not specific comma semantics) is the dominant factor.",
            "mechanistic_insight": "Performance differences are attributable to tokenization alignment between addends and answer: R2L aligns least-significant digits across input and output tokens, making carries and digit propagation easier to handle; L2R creates input-output token misalignment (especially when the answer is longer), producing systematic errors concentrated at token boundaries. Off-by-one errors are more likely at token boundaries (last digit of a 3-digit token).",
            "performance_scaling": "The harmful inductive bias of L2R segmentation weakens with model scale and with later GPT-4 updates, suggesting larger/more trained models can partly overcome tokenization-aligned biases.",
            "failure_modes": "L2R causes catastrophic failure in length-mismatch cases (answer longer than addends) and stereotyped digit-4 errors; token-boundary off-by-one errors more frequent under both tokenizations but concentrated at token edges.",
            "comparison_baseline": "Direct comparison between L2R (default) and R2L (delimiters every 3 digits) across models and shot counts; delimiter-type controls and thinking-token token-count controls were used to rule out alternative explanations.",
            "key_finding": "Tokenization direction (alignment of digit tokens relative to least-significant digits) is a major inductive bias for arithmetic: enforcing R2L segmentation yields large improvements in few-shot addition accuracy and eliminates stereotyped token-boundary failures present under L2R.",
            "uuid": "e300.2",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Conversion prompting (repeat-and-convert)",
            "name_full": "Chain-of-thought–inspired conversion prompting: few-shot prompting models to repeat or reformat inputs from L2R to R2L and then answer",
            "brief_description": "A prompting intervention where the model is few-shot trained to take an input in one tokenization, repeat the problem in the preferred tokenization (R2L), and then produce the answer — shown to recover much of the performance lost under L2R.",
            "citation_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 / GPT-4 (tested primarily on GPT-3.5 gpt-3.5-turbo-0301)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition",
            "number_range_or_complexity": "7–9 digit addends (same problem sets as other experiments)",
            "method_or_intervention": "Few-shot prompting where examples demonstrate repeating the input in R2L tokenization and answering in R2L (repeat L2R→R2L); also tested answering-only output tokenization control (incentivize answering in R2L without repeating the problem) as a control.",
            "performance_result": "Repeat L2R→R2L prompting recovers nearly the full accuracy of receiving the problem natively in R2L: models perform nearly as well when they convert themselves as when provided R2L input. By contrast, merely instructing the model to answer in R2L (without repeating the problem in R2L) does not recover the same performance, indicating that seeing the problem in R2L is necessary.",
            "mechanistic_insight": "The success of explicit conversion prompting indicates models do not reliably perform an implicit forward-pass conversion from L2R to R2L; instead, they can deterministically perform the conversion when prompted (suggesting the conversion is within model capability but not applied automatically during standard forward completions). This supports an interpretation that tokenization alignment matters for the model's learned computation pathway.",
            "performance_scaling": "Conversion prompting effectiveness increases with number of shots (model adherence to the suggested repetition style improves), consistent with in-context learning shaping the model's output format and internal processing.",
            "failure_modes": "When models are prompted to only output in R2L (without repeating the problem), they often fail to improve and may adhere inconsistently to format instruction depending on number of shots; conversions require adherence to the repetition behavior to be effective.",
            "comparison_baseline": "Compared repeat-and-convert few-shot prompting vs output-only formatting prompts vs native R2L input; assessed shot dependence (1–8 shots).",
            "key_finding": "Prompting models to explicitly reformat L2R inputs into R2L and then solve largely recovers the performance advantage of native R2L inputs, showing the problem is a tokenization-alignment inductive bias that can be worked around via in-context reformatting.",
            "uuid": "e300.3",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Observed mechanistic failure modes",
            "name_full": "Stereotyped token-boundary and digit-substitution error mechanisms discovered via error & log-prob analyses",
            "brief_description": "Detailed empirical mechanistic observations about how models fail on addition: a consistent 'digit 4' substitution in L2R length-mismatch cases, concentration of off-by-one errors at token boundaries, and characteristic log-prob/entropy signatures revealing guessing vs binary confusion.",
            "citation_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 / GPT-4 (empirical analyses reported primarily on gpt-3.5-turbo-0301)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition (diagnostic analysis)",
            "number_range_or_complexity": "7–9 digit additions; length-controlled dataset of 1300 problems (various addend/answer length triplets)",
            "method_or_intervention": "Error pattern analysis (digit-level), conditioning on length-match vs length-mismatch; logprob/top-5 token inspection; lower-bound entropy computation from top-5 probabilities; token-frequency proxy analysis using BPE merge ranks.",
            "performance_result": "Quantitative failure-mode stats: in the length-controlled set (1300 problems) the L2R length-mismatch condition produced 91.25% incorrect; among those incorrects, the model always got digit 4 wrong (while digits 1–3 were correct). Off-by-one errors account for nearly all remaining errors in other conditions: R2L missed 25/1300 (24 off-by-one) and L2R (length-match) missed 56/900 (53 off-by-one).",
            "mechanistic_insight": "Key signals: (1) 'Digit 4' errors show high entropy (~2.066 nats) and correct answers appear in top-5 ~49.6% — consistent with near-random guessing among 10 candidate tokens for the problematic digit. (2) Off-by-one errors show low entropy (~0.45 nats) and the second-most-likely token is the true answer in all such cases — consistent with tight confusion between adjacent tokens. (3) Errors concentrate at the last digit of an output token (token boundary), implicating tokenization-induced boundary crossing as critical. (4) Mild evidence for substitution toward more frequent tokens in some conditions, but frequency (BPE merge-rank proxy) is not a dominant explanatory factor overall.",
            "performance_scaling": "These stereotyped failure modes are strongest in smaller models (GPT-3.5) and become weaker in larger / later GPT-4 checkpoints, consistent with scale and training reducing but not eliminating tokenization-driven algorithmic brittleness.",
            "failure_modes": "Systematic digit-substitution at specific digit positions (e.g., digit 4), off-by-one errors at token boundaries, uncertainty-driven guessing among digit tokens, and length-mismatch catastrophic drops under L2R.",
            "comparison_baseline": "Error patterns compared between R2L vs L2R, length-match vs length-mismatch problems, and across model checkpoints; logprob distributions compared between correct and incorrect cases.",
            "key_finding": "The paper provides mechanistic behavioral evidence that arithmetic errors in LLMs are tokenization-aligned: token-boundary misalignment yields systematic, repeatable errors (e.g., a consistent wrong fourth digit), and logprob/entropy signatures differentiate between random-digit guessing and tight two-way token confusions (off-by-one).",
            "uuid": "e300.4",
            "source_info": {
                "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What algorithms can transformers learn? a study in length generalization.",
            "rating": 2
        },
        {
            "paper_title": "Teaching algorithmic reasoning via in-context learning.",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot numerical reasoning.",
            "rating": 2
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis.",
            "rating": 2
        },
        {
            "paper_title": "xval: A continuous number encoding for large language models.",
            "rating": 1
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks.",
            "rating": 1
        }
    ],
    "cost": 0.018113499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</h1>
<p>Aaditya K. Singh ${ }^{1}$ DJ Strouse ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4 , finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Illustrating the dependence of frontier model arithmetic performance on tokenization. We show how using commas can enforce right-to-left (R2L) tokenization for the same addition problem. R2L tokenization leads to improved model performance on both GPT-3.5 and GPT-4 (March 2023 models), which we show is due to tokenization alignment between addends and answer through various controls and error analyses.</p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) are often lauded as demonstrating the benefits of end-to-end learning over inductive biases. However, an often overlooked part of the pipeline, preventing it from being end-to-end, is tokenization: the segmenting of an input sequence of bytes into discrete tokens. Tokenization consists of two halves: training, in which a vocabulary of tokens and statistics are learned over a given corpus, and segmenting, where a function uses the trained vocabulary and statistics to map sequences of bytes to tokens. Each tokenization scheme may impart different inductive biases on the model due to the way in which bytes of input sequences are grouped - in this work, we study these tokenization-dependent effects on numerical reasoning in state-of-the art models (GPT-3.5, GPT-4) by considering the tokenization of numbers in arithmetic problems.</p>
<p>Though many techniques have been proposed for tokenization, the prevailing methods in today's frontier models are variants of Byte Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016). BPE is a statistical approach to tokenization that is learnt from a dataset of, in the case of LLMs, text. Intuitively, BPE compresses the dataset by iteratively creating tokens for the most commonly occurring subsequences. Specifically, BPE begins with a token vocabulary consisting of each character in the text (e.g. letters, num-</p>
<table>
<thead>
<tr>
<th>000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1. Popular LLMs and their number tokenization strategies. $\mathrm{BPE}=$ byte pair encoding. $\mathrm{L} 2 \mathrm{R}=$ left-to-right.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Strategy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3 (2020)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (2022)</td>
<td style="text-align: left;">L2R chunks of 3 digits</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (2023)</td>
<td style="text-align: left;">L2R chunks of 3 digits</td>
</tr>
<tr>
<td style="text-align: left;">Claude v2.1 (2023)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">Gopher (2021)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">Chinchilla (2022)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">PaLM (2022)</td>
<td style="text-align: left;">single digit</td>
</tr>
<tr>
<td style="text-align: left;">GPT-J (2021)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
<tr>
<td style="text-align: left;">Llama 1 \&amp; 2 (2023)</td>
<td style="text-align: left;">single digit</td>
</tr>
<tr>
<td style="text-align: left;">Mistral (2023)</td>
<td style="text-align: left;">single digit</td>
</tr>
<tr>
<td style="text-align: left;">OLMo (2024)</td>
<td style="text-align: left;">pure BPE</td>
</tr>
</tbody>
</table>
<p>We vary the tokenization direction to be the default left-to-right (L2R) or right-to-left (R2L). We find that model accuracy is up to $20 \%$ higher when using R2L tokenization (Figure 1, Section 3). We then provide a thorough analysis of error patterns across these two tokenizations (Section 4). We find that the difference in performance between R2L and L2R tokenization in GPT-3.5 can largely be explained by an extremely stereotyped and surprising error pattern (Section 4.3), perhaps indicating the presence of some systematic, but flawed, reasoning. Next, we show that chain-of-thought-inspired approaches, where a model is asked to repeat an input in R2L tokenization, recover the accuracy otherwise lost due to L2R tokenization (Section 5). Finally, we conclude by studying how these effects may change with model version, finding that larger models are better able to override the tokenization-induced effects but, as of yet, unable to eliminate them (Section 6). Overall, we view these results as compelling evidence towards significant tokenization-dependent inductive biases in large language models, and hope they lead model practitioners to conduct careful pre-training ablations with varying tokenization schemes, especially for numerical reasoning.</p>
<h2>2. Methods</h2>
<h3>2.1. Experiment setup</h3>
<p>We evaluate GPT models through the Chat Completions endpoint on the OpenAI API ${ }^{5}$ on few-shot addition problems. We control for addend digit length, ranging from 7 to 9 digits (chosen since this way each addend is 3 tokens long). For most experiments, we use 90 random problems, with 10 problems for each addend digit length pair (e.g., 10 problems where the addends are both 7 digits long, 10 problems where the first addend is 7 digits and the second is 8 , etc.). For shots, we consider 1-, 2-, 4-, and 8-shots. Shots are sampled randomly for each "query" problem, and are</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>provided to the model as a multi-turn dialogue. We control shots to have the same form (digit lengths, tokenization direction, etc.) as the query problem. We use the default system prompt "You are a helpful assistant." for maximum reproducibility. ${ }^{6}$ Python code for some example 2-shot queries to the model are presented in Appendix E. 3 for maximum clarity. We use greedy decoding (temperature $=0$ ) in all experiments. Accuracy was computed by extracting numbers from model responses.</p>
<p>Most experiments in the paper are run using the gpt-3.5-turbo-0301 model checkpoint, though in Section 6 we look into how results extend to newer versions of the same model (gpt-3.5-turbo-0613) and to the, presumably larger, GPT-4 models (gpt-4-0314, gpt-4-0613). All code and full results tables can be found at https://github.com/aadityasingh/ TokenizationCounts.</p>
<h3>2.2. Varying L2R vs. R2L tokenization</h3>
<p>The ChatCompletion API only allows for input text, not input tokens, so it's tricky to conduct tokenization-varying experiments. To force the model to use R2L tokenization for numbers, we add commas every 3-digits from the right (see Figure 1). Since the tokenizer doesn't contain any tokens with numbers and commas, the commas get tokenized separately, effectively enforcing a different segmentation of digits. We use this setting to illustrate our main results, and conduct various controls to ensure that our observed effect is due to tokenization as opposed to other confounds.</p>
<h2>3. Right-to-left tokenization improves model performance</h2>
<h3>3.1. Main results</h3>
<p>When using commas to separate digits and enforce R2L tokenization, we observed greatly improved average performance (8-shot result in Figure 1). We found that increasing the number of shots (Figure 4) led to a larger increase for the L2R tokenization (from $68.5 \% 1$-shot to $75.6 \% 8$-shot) than for the R2L tokenization (from $95.6 \% 1$-shot to $97.8 \%$ 8 -shot) indicating that in-context learning may slightly mitigate the (harmful) bias of L2R tokenization. Given this finding and the plateau-ing in performance with increasing shots, we report only 8 -shot results for the remainder of the work as this makes L2R tokenization the most competitive.</p>
<h3>3.2. Controlling for comma-based semantic priors</h3>
<p>Though this result is already compelling, we realize that commas are often used to separate digits in the manner</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4. Effect of R2L vs L2R tokenization with increasing shots.
depicted in Figure 1, so the observed effect may be confounded by prevalence in training data (McCoy et al., 2023). One might argue that comma separation is actually bringing the input closer to the training distribution of the model, so it's not a surprise that models perform better. To control for this and focus in on tokenization, we consider alternate, single-token separators: ', '.', ' $\$ ', '# ' (note we'll refer to ' ' as <space> for clarity). For example, the number 8302080 would be written as $8 # 302 # 080$ when input to the model.</p>
<p>Results are shown in Figure 5. We find that the model is largely agnostic to the separator used, indicating that tokenization is likely the dominant effect, rather than the specific choice of using commas.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5. 8-shot accuracy when using different delimiters for R2L tokenization. Dotted lines show results from Figure 1 for comparison. Overall, we see choice of delimiter matters less than direction of tokenization.</p>
<h3>3.3. Controlling for "thinking tokens"</h3>
<p>Another confound with the above experiment may be that adding commas both increases the number of tokens input to as well as generated by the model. Thus, to generate the same answer, the model has access to more computation steps (i.e., FLOPs). There is a worry that models may use these repetitive thinking tokens to perform additional useful computations (Lanham et al., 2023). In practice, this seems not to happen without further training (Goyal et al., 2024), but we conducted experiments to verify this in our setting.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6. 8-shot accuracy for various "thinking token" controls. Dotted lines show results from Figure 1 for comparison. We also experimented with other delimiters for L2R tokenization (all those from Figure 5), but found similarly poor results. Overall, "thinking tokens" do not recover the performance boost from using commaenforced R2L tokenization.</p>
<p>To control for thinking tokens, we consider two types of controls. In the first, we use separators to enforce L2R tokenization - this enforces an exact match in prompt token counts. Second, we consider adding 1 or 2 spaces before and after the + and $=$ sign to increase the number of tokens ${ }^{7}$ in the L2R case (where no separator is used). Both of these have the benefit of adding extremely "predictable" tokens (when using 8 -shots), allowing the model to possibly use the extra computation steps for "thinking".</p>
<p>In Figure 6, we find that neither of these controls, when applied to L2R tokenized sequences, recovers the performance of R2L tokenization. In fact, we found that using separators with the L2R tokenization often hurt performance, likely because this is an uncommon representation-upon qualitative inspection of a few examples, we found the model sometimes "auto-corrects" the inputs by hallucinating trailing zeros. We believe these experiments effectively rule out the "thinking token" confound.</p>
<h2>4. Error analysis reveals stereotyped patterns</h2>
<p>Given the robust effect observed in Section 3, we were curious to see if there were any patterns in the errors. Below, we summarize our key findings.</p>
<h3>4.1. L2R tokenization is significantly worse when answer is longer than addends</h3>
<p>As noted in Section 2.1, we balanced our dataset of problems based on input digit length. Upon inspection of problems the model got incorrect when using L2R tokenization, we noted that errors seemed more likely when the answer was longer than the addends (e.g., a problem where 7 digit number +7 digit number $=8$ digit number, of the form depicted</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7. When the answer is the same length in digits as an addend (length match), both tokenization schemes perform similarly (left). When the answer is a different length in digits than either addend (length mismatch), L2R tokenization destroys model performance, dropping to $8.25 \%$ (right).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8. Accuracy as a function of number of carries. For L2R tokenization, we exclude problems where the answer length does not match at least one addend, as the model misses most of those ( $92 \%$ ) as shown in Section 4.1. If number of carries (a human notion of difficulty) was correlated to model performance, we would expect a negative slope. The lack of any trend suggests model performance is largely independent of number of carries.
in Figure 1). To test this hypothesis, we conducted a new experiment where we controlled for addend lengths and answer lengths. Specifically, we generated 100 random problems for each possible triplet of digit lengths where addends and answer have a length of 7 to 9 digits (full list in Appendix E.1). The remainder of our experiments in this section will use this expanded set of problems to show the robustness of the found error patterns.</p>
<p>We reproduced our main phenomenon (Section 3.1), and further affirmed our intuitions about error patterns. As shown in Figure 7, we find that L2R tokenization has similar performance to R2L tokenization when the answer's length in digits is the same as one of the inputs (which we refer to as the "length match" condition). When the answer is longer than the inputs (due to a final carry), L2R tokenization is significantly worse, with accuracy dropping down to $8.25 \%$ we refer to this as the "length mismatch" condition. We suspect that this strong effect may be due to the misalignment between input and output tokenizations (as illustrated in</p>
<p>Figure 1) rather than some carry-related notion of problem difficulty, which we explore in the next few subsections.</p>
<h3>4.2. Errors do not seem correlated to number of carries</h3>
<p>A natural hypothesis given the above result may be that errors might just be correlated to some notion of difficulty, such as carries. In Figure 8, we find that this is generally not the case. Specifically, we consider the accuracy on subsets of problems based on how many carries are needed to solve them. ${ }^{8}$ The lack of a clear positive or negative trend indicates that model performance is not strongly affected by the number of carries.</p>
<h3>4.3. Length mismatch problems yield stereotyped "digit 4" error pattern</h3>
<p>If not carries, what could be causing the surprising error pattern in Figure 7? In Figure 9a, we find that the errors when using L2R tokenization are extremely stereotyped and not at all intuitive. Specifically, in the length mismatch condition, the model always gets the fourth digit wrong. Furthermore, the model always gets the first 3 digits correct (corresponding to the first output token). In terms of how far off the model is on digit 4, Figure 9b shows that there's a slight preference to off-by-one errors, but overall the specific substitution appears quite haphazard.</p>
<p>We found this result extremely surprising. In cognitive science, such stereotyped error patterns are often used as evidence of underlying systematic processing. While the mechanism for addition in LLMs remains unclear, we find this striking, tokenization-dependent error pattern ${ }^{9}$ as highly suggestive of some underlying algorithm (in contrast to suggestions that LLMs may be performing arithmetic using some "fuzzy" matching to similar problems in training). We provide further evidence of stereotyped error patterns by analyzing model log probabilities in Appendix D.</p>
<h3>4.4. Off-by-one errors at token boundaries account for nearly all remaining errors</h3>
<p>After accounting for the main source of error, we analyzed the remaining errors across both tokenization methods: 25 out of the 1300 problems for R2L tokenization, and 56 out of the 900 problems in the length match condition for L2R tokenization.</p>
<p>For R2L tokenization, of the 25 problems missed, 24 are due to off-by-one (either above or below) errors. For L2R tokenization, of the 56 problems missed, 53 are due to off-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9. a) Error patterns for L2R tokenization over problems where the answer digit length is different than the addend lengths. "None" indicates the problems in this case that the model got correct (8.25\%). "Other" indicates problems where the model doesn't provide a valid answer or provides an answer of the wrong length ( $0.5 \%$ ). Of the remaining $91.25 \%$ which the model gets incorrect, it shockingly always gets digit 4 wrong. In addition, it sometimes gets other digits ( 5,6 or 7 ) wrong. b) For the errors in digit 4, we show the magnitude of the mistake. For example, if the correct value of digit 4 is 2 and the model response has digit 4 equal to 5 , it would be off by 3 . We see a slight preference to off-by-1 errors, but error magnitudes are fairly evenly distributed.
by-one (either above or below) errors. For nearly all these off-by-one errors, ${ }^{10}$ regardless of tokenization direction, we find that the error itself occurs in the last digit of an output token. This result suggests that off-by-one errors are more likely across token boundaries as opposed to in the middle of a 3-digit token. This hypothesis, with preliminary evidence, connects to works on length generalization (Anil et al., 2022) - using 3-digit tokens may make length generalization easier as models only need to cross token boundaries every third digit (as opposed to every digit).</p>
<h2>5. Models are able to convert from L2R to R2L tokenization, improving performance</h2>
<h3>5.1. Main results</h3>
<p>With the above results showing that number tokenization can strongly affect numerical reasoning, we ask if mod-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>els can be prompted to take problems in a less preferred tokenization (L2R) and convert them to a more preferred tokenization (R2L) to improve performance. Inspired by chain-of-thought approaches (Nye et al., 2021; Kojima et al., 2022; Wei et al., 2022), we few-shot prompt models to take problems with one tokenization direction, and then repeat the problem and answer it using a different tokenization direction. In Figure 10, we find that models indeed perform nearly as well at addition when converting L2R tokenization to R2L themselves as to when they receive the problem in R2L tokenization in the first place. Performance increases with the number of shots when converting L2R to R2L since the model adheres more to the (helpful) suggested repetition style. These results indicate that models can convert between tokenizations to solve problems correctly, but do not do so implicitly in the forward pass.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10. Few-shot accuracy when models receive a problem with one tokenization direction, then repeat and answer it in another.</p>
<h3>5.2. Controlling for output tokenization</h3>
<p>One confound with the above experiment may just be that the model improves when it's asked to generate answers with R2L tokenization. To control for this, we conduct a similar experiment, but without few-shot prompting the model to repeat the problem: the few-shot prompt provides answers with a different tokenization direction than the input, incentivizing the model to answer with this tokenization direction (see Appendix E. 3 for an example prompt). In Figure 11, we see that just answering with R2L tokenization does not improve performance (purple curve) to the degree that repeating in R2L tokenization does (purple curve, Figure 10), when starting from L2R tokenization. This effect indicates that it is important for the model to also see the problem in the preferred tokenization (by repeating it), rather than just answering in the preferred tokenization.</p>
<h2>6. Tokenization-dependent effects mostly extend to future models</h2>
<p>Through the previous sections, we've demonstrated a strong tokenization-dependent effect. In this section, we address the question: does this effect extend to newer models?</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11. Few-shot accuracy when models receive a problem in one tokenization format, and answer it in another. The distinction between this and Figure 10 is that models do not repeat the problem in this case. We note that when giving a model a problem in R2L tokenization and prompting it to answer in L2R tokenization, the model actually gets worse with more shots, since for fewer shots, the model ends up ignoring the few-shot prompt and answers in its preferred R2L tokenization. Specifically, adherence to the prompted formatting for R2L→L2R increases from just 13.3% with 1 shot to 98.9% with 8 shots.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12. 8-shot performance of various OpenAI models on the same addition problems as Figure 4. The newer version of GPT-3.5 appears to perform equally poorly. For GPT-4, we see a large tokenization-dependent effect in the March model, which becomes weaker (but still present) in the June model. The GPT-4 turbo model shows a slight regression in overall performance with the tokenization-dependent effect becoming stronger again.</p>
<p>As shown in Figure 12, we find that generally, yes: tokenization-dependent effects persist. We consider five "held-out" OpenAI models, which allow us to consider how tokenization-dependent effects shift when models are updated (gpt-3.5-turbo-0613, gpt-3.5-turbo-1106) or scaled up (gpt-4-0314, gpt-4-0613) and then scaled back down (gpt-4-1106-preview, which is a "turbo" model). Later versions of GPT-3.5 exhibit as strong an effect due to tokenization direction. The effect is mitigated slightly in GPT-4's March version, and mitigated strongly in GPT-4's most recent version. Specifically, GPT-4 models appear</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13. 8-shot performance of various OpenAI models on answer length controlled problems (see Section 4.1), separated by whether the answer length is the same as one of the addends. We see the effect from Section 4.1 reproduces strongly in the newer version of GPT-3.5. The effect is still present in GPT-4, but not as strongly. Interestingly, the effect is stronger in the latest GPT-4 Turbo model as compared to GPT-4.</p>
<p>to be better at performing arithmetic across the board (for both tokenization directions). Interestingly, in the most recent GPT-4 Turbo model, the effect of tokenization becomes stronger again. Furthermore, Figure 13 shows that the digit length mismatch between answer and addends is again the main reason for the performance drop when using L2R tokenization, in both GPT-3.5 and GPT-4 models. We believe that the increased scale of training GPT-4 (likely in both parameter count and data seen) allows it to better override the tokenization-induced inductive bias that leads GPT-3.5 models to perform worse (analogous to scale helping mitigate tokenization-induced spelling difficulties (Liu et al., 2023)). The resurgence of tokenization-dependent effects in the newest GPT-4 Turbo model (which is presumably smaller than GPT-4) supports this hypothesis.</p>
<h2>7. Related work</h2>
<p><strong>Tokenization methods</strong> The two leading tokenization methods are Unigram (Kudo, 2018) and BPE (Sennrich et al., 2016). While older work in NLP show the benefits of Unigram over BPE (Bostrom &amp; Durrett, 2020), BPE remains the most commonly used tokenization method by modern LLM practitioners. Within BPE, different models often make different hard-coded choices, such as removing long tokens of consecutive whitespace (Touvron et al., 2023a) or enforcing single-digit tokenization of numbers (Chowdhery et al., 2023). Our work demonstrates tokenization-dependent effects from one such choice, the use of 1-, 2-, and 3-digit tokens by OpenAI models (OpenAI et al., 2023). One way around such issues could be tokenizer-free methods (e.g., MEGABYTE (Yu et al., 2023), which uses patch-based schemes and doesn't assume fixed tokens), but we suspect these schemes will also carry their own inductive biases. Golkar et al. (2023) also introduce a continuous number encoding scheme meant to circumvent tokenization artifacts, but their approach is limited to cases where model outputs are purely numerical, and not interleaved with text.</p>
<p><sup>11</sup>We assume this is a smaller, maybe distilled, version of GPT-4.</p>
<p><sup>12</sup>We find it interesting that the March to June update to GPT-4 improved performance, but the corresponding update to GPT-3.5 did not – without knowing what these updates entail, however, it's hard to draw conclusions as to why this may be the case.</p>
<p>Tokenization artifacts in LLMs A growing set of results has emerged around various tokenization-related artifacts in LLMs. Similar to scratchpad prompting (Nye et al., 2021), Wei (2023) found that separating letters into individual tokens can help in sorting words by the second letter. Other work (Shin et al., 2020) has focused on specific tokens that can negatively affect model performance. Rumbelow \&amp; mwatkins (2023) found many tokens which were artifacts of the data used to pre-train the tokenizer, but presumably weren't present in the model's training data, leading to highly unpredictable (and often comical) completions. Sun et al. (2023) find artifacts due to mismatches in tokenization in extractive Q\&amp;A tasks, which may have connection to some of our experiments in Section 5.2. Lundberg (2023) propose token healing to avoid many tokenization-related issues by removing the last few tokens from a prompt and allowing the model to complete them; this approach has connections to work on asking models to rephrase-and-respond (Deng et al., 2023) and our experiments on prompting the model to repeat with its preferred tokenization direction (Section 5). Our work builds on these past scattered artifacts and provides a systematic analysis of tokenization-directiondependent effects on numerical reasoning in frontier LLMs.</p>
<h2>Arithmetic tasks as a testbed for numerical reasoning</h2>
<p>in LLMs With the increased interest in measuring frontier models on math reasoning (Saxton et al., 2019; Cobbe et al., 2021; Lewkowycz et al., 2022; Hendrycks et al., 2021; Paster, 2023), an accompanying body of work studies language models in more controlled settings, such as arithmetic. Razeghi et al. (2022) use arithmetic tasks to show that pretraining term frequencies ${ }^{15}$ can affect numerical reasoning in GPT-J models (Wang \&amp; Komatsuzaki, 2021) trained on the Pile dataset (Gao et al., 2020). Similarly, McCoy et al. (2023) showed that GPT-3.5 and GPT-4 are better at computing linear functions that are more common in training data (such as the Fahrenheit to Celsius conversion) than close alternatives. Other work (Nogueira et al., 2021; Muffo et al., 2022; Zhou et al., 2022; 2023) instead focuses on various modifications that can help models generalize to longer arithmetic tasks. Zhou et al. (2023) and Lee et al. (2023) both point out that having autoregressive models perform addition in reversed order yields a simpler algorithm to learn and results in better performance, which is complementary to our emphasis on the importance of "reversed" (i.e. right-toleft) tokenization alignment. Zhou et al. (2022) also conduct preliminary error analyses of model mistakes, though their algorithmic prompts force models to split tokens into digits (similar to Nye et al. (2021)). Our work broadly lies in this</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>category of work using arithmetic tasks to study numerical reasoning; we chose to focus on tokenization-dependent effects, and found surprisingly consistent, stereotyped error patterns (Section 4), adding to this rich body of literature.</p>
<h2>8. Discussion</h2>
<p>In this work, we analyze tokenization-dependent effects on numerical reasoning in GPT-3.5 and GPT-4. We found that the hard-coded choice of 1-, 2- and 3- digit number tokens, tokenized left-to-right, gives rise to stereotyped error patterns when compared to right-to-left tokenization. We proposed a mitigation, where the model is asked to repeat the answer in its preferred tokenization format. Finally, we showed that the effect is stronger in smaller models (such as GPT-3.5), emphasizing the significance of tokenizationdependent inductive biases in an era where many practitioners are focusing on packing capabilities into smaller models through overtraining (De Vries, 2023; Touvron et al., 2023a) and distillation (Li et al., 2023; Gemini Team et al., 2023). Overall, we believe this evidence strongly suggests inductive biases from tokenization can significantly influence model performance on numerical reasoning tasks.</p>
<p>Modern frontier LLMs mostly use single-digit tokens (Table 1), with GPT-3.5 and GPT-4 being a key exception in their use of up-to-3-digit tokens. We hypothesize that the latter choice may have been made to achieve a better compression rate: models "see" more numerical data for the same number of training tokens. ${ }^{16}$ Furthermore, this choice could have benefits for length generalization (Anil et al., 2022), as we allude to in Section 4.4. However, we've also demonstrated how the misalignment between inputs and outputs when using L2R tokenization (Section 4.1) can lead to large drops in accuracy, especially on smaller models (GPT-3.5, GPT-4 Turbo). Such misalignment would not be an issue when using single-digit tokens.</p>
<p>To make progress on which number tokenization choices are best to use (e.g., the single-digit tokens of LLaMa and PaLM, or the up-to-3 digit tokens of GPT-3.5 and GPT-4), the "gold experiment" would be to train the same model architecture on the same dataset, but with varying number tokenization strategies. Beyond the expense of this experiment (making it intractable in academic settings), a key question also becomes how to "compute"-control. The better compression ratio of up-to-3 digit tokens means a token-controlled experiment would result in some models "seeing" more data. We hope our work leads model practitioners to consider such ablations, with proper controls.</p>
<p>Beyond applicability to model practitioners, our work also</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>provides an interesting set of tokenizaiton-dependent phenomenon for interpretability researchers to explore. Prior work (Stolfo et al., 2023) has used techniques such as path patching to identify sub-circuits in LLMs that perform arithmetic tasks, but restricted to single token operands. Building off our results, it would be interesting to elucidate the mechanisms behind systematic error patterns, especially in the case of multi-token operands. The robustness of the "digit 4 " error on GPT-3.5 points to some systematic mechanism, which could shed light on underlying algorithms that emerge to perform arithmetic tasks.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to acknowledge Andrew Saxe, Ted Moskovitz, Kira Düsterwald, Felix Hill, Xavier Garcia, Dan Roberts, and William Held for insightful discussions and feedback on the draft. A.K.S. is funded by the Gatsby Charitable Foundation.</p>
<h2>References</h2>
<p>Anil, C., Wu, Y., Andreassen, A. J., Lewkowycz, A., Misra, V., Ramasesh, V. V., Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B. Exploring length generalization in large language models. Neural Information Processing Systems (NeurIPS), 2022. URL https://openreview.net/ forum?id=zSkYVeX7bC4.</p>
<p>Bostrom, K. and Durrett, G. Byte pair encoding is suboptimal for language model pretraining. Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https://aclanthology.org/ 2020.findings-emnlp. 414.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS), 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64aPaper.pdf.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,
G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research, 2023. URL http: //jmlr.org/papers/v24/22-1144.html.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv:2110.14168, 2021. URL https: //arxiv.org/abs/2110.14168.</p>
<p>De Vries, H. Go smol or go home, 2023. URL https://www.harmdevries.com/post/ model-size-vs-compute-overhead/.</p>
<p>Deng, Y., Zhang, W., Chen, Z., and Gu, Q. Rephrase and respond: Let large language models ask better questions for themselves. arXiv:2311.04205, 2023. URL https: //arxiv.org/abs/2311.04205.</p>
<p>Gage, P. A new algorithm for data compression. C Users Journal, 1994.</p>
<p>Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The Pile: An 800gb dataset of diverse text for language modeling. arXiv:2101.00027, 2020. URL https://arxiv.org/abs/2101.00027.</p>
<p>Gemini Team, Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., Frechette, A., Smith, C., Culp, L., Proleev, L., Luan, Y., Chen, X., Lottes, J., Schucher, N., Lebron, F., Brustemi, A., Clay, N., Crone, P., Kocisky, T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A., Rae, J. W., Lu, H., Sifre, L., Maggioni, M., Alcober, F., Garrette, D., Barnes, M., Thakoor, S., Austin, J., BarthMaron, G., Wong, W., Joshi, R., Chaabouni, R., Fatiha, D., Ahuja, A., Liu, R., Li, Y., Cogan, S., Chen, J., Jia, C.,</p>
<p>Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Chadwick, M., Tomar, G. S., Garcia, X., Senter, E., Taropa, E., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas, D., Valter, D., Tao, C., Blanco, L., Badia, A. P., Reitter, D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S., Surita, G., Labanowski, J., Rao, A., Winkler, S., Parisotto, E., Gu, Y., Olszewska, K., Zhang, Y., Addanki, R., Miech, A., Louis, A., Shafey, L. E., Teplyashin, D., Brown, G., Catt, E., Attaluri, N., Balaguer, J., Xiang, J., Wang, P., Ashwood, Z., Briukhov, A., Webson, A., Ganapathy, S., Sanghavi, S., Kannan, A., Chang, M.-W., Stjerngren, A., Djolonga, J., Sun, Y., Bapna, A., Aitchison, M., Pejman, P., Michalewski, H., Yu, T., Wang, C., Love, J., Ahn, J., Bloxwich, D., Han, K., Humphreys, P., Sellam, T., Bradbury, J., Godbole, V., Samangooei, S., Damoc, B., Kaskasoli, A., Arnold, S. M. R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H., Hodkinson, S., Shyam, P., Ferret, J., Hand, S., Garg, A., Paine, T. L., Li, J., Li, Y., Giang, M., Neitz, A., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery, A., Das, D., Rogozińska, D., Nikolaev, V., Sprechmann, P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M., Mishra, G., Welty, C., Newlan, J., Jia, D., Allamanis, M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim, C., Rijhwani, S., Hou, S., Shrivastava, D., Baddepudi, A., Goldin, A., Ozturel, A., Cassirer, A., Xu, Y., Sohn, D., Sachan, D., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S., Landon, J., Patel, M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M., Giménez, M., Yeung, L., Lin, H., Keeling, J., Georgiev, P., Mincu, D., Wu, B., Haykal, S., Saputro, R., Vodrahalli, K., Qin, J., Cankara, Z., Sharma, A., Fernando, N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A., Agrawal, P., Castro-Ros, A., van den Driessche, G., Wang, T., Yang, F., yiin Chang, S., Komarek, P., McIlroy, R., Lučić, M., Zhang, G., Farhan, W., Sharman, M., Natsev, P., Michel, P., Cheng, Y., Bansal, Y., Qiao, S., Cao, K., Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K., Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jhakra, P., Wang, S., Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz, M., Robinson, K., Katariya, Y., Riedel, S., Bailey, P., Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N., Xiong, X., Yang, Z., Gribovskaya, E., Adler, J., Wirth, M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers, S., Bortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Powell, R., Bolina, V., Iinuma, M., Zablotskaia, P., Besley, J., Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer, J., Su, G., Polacek, M., Kaufman, R. L., Tokumine, S., Hu, H., Buchatskaya, E., Miao, Y., Elhawaty, M., Siddhant, A., Tomasev, N., Xing, J., Greer, C., Miller, H., Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta, M., Blevins, R., Klimenko, T., Yeh, C.-K., Changpinyo, S., Mu, J., Chang, O., Pajarskas, M., Muir, C., Cohen, V.,</p>
<p>Lan, C. L., Haridasan, K., Marathe, A., Hansen, S., Douglas, S., Samuel, R., Wang, M., Austin, S., Lan, C., Jiang, J., Chiu, J., Lorenzo, J. A., Sjösund, L. L., Cevey, S., Gleicher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K., Hussenot, L., Soares, L. B., Baumli, K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A., Pavetic, F., Pardo, F., Gergely, A., Frye, J., Ramasesh, V., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer, E., Campos, V., Tomala, A., Tang, Y., Badawy, D. E., White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S., Gong, Z., Caelles, S., Hemsley, R., Thornton, G., Feng, F., Stokowiec, W., Zheng, C., Thacker, P., Çağlar Ünlü, Zhang, Z., Saleh, M., Svensson, J., Bileschi, M., Patil, P., Anand, A., Ring, R., Tsihlas, K., Vezer, A., Selvi, M., Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki, S., Rong, K., Dafoe, A., FitzGerald, N., Gu-Lemberg, K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V., Cobon-Kerr, J., Sainath, T., Rauh, M., Hashemi, S. H., Ives, R., Hasson, Y., Li, Y., Noland, E., Cao, Y., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau, J.-B., Moufarek, A., Hassan, S., Shivakumar, K., van Amersfoort, J., Mandhane, A., Joshi, P., Goyal, A., Tung, M., Brock, A., Sheahan, H., Misra, V., Li, C., Rakićević, N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S., Sezener, E., Huot, F., Lamm, M., Cao, N. D., Chen, C., Elsayed, G., Chi, E., Mahdieh, M., Tenney, I., Hua, N., Petrychenko, I., Kane, P., Scandinaro, D., Jain, R., Uesato, J., Datta, R., Sadovsky, A., Bunyan, O., Rabiej, D., Wu, S., Zhang, J., Vasudevan, G., Leurent, E., Alnahlawi, M., Georgescu, I., Wei, N., Zheng, I., Chan, B., Rabinovitch, P. G., Stanczyk, P., Zhang, Y., Steiner, D., Naskar, S., Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias, J. S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A., Vieillard, N., Potluri, S., Park, J., Davoodi, E., Zhang, J., Stanway, J., Garmon, D., Karmarkar, A., Dong, Z., Lee, J., Kumar, A., Zhou, L., Evens, J., Isaac, W., Chen, Z., Jia, J., Levskaya, A., Zhu, Z., Gorgolewski, C., Grabowski, P., Mao, Y., Magni, A., Yao, K., Snaider, J., Casagrande, N., Suganthan, P., Palmer, E., Irving, G., Loper, E., Faruqui, M., Arkatkar, I., Chen, N., Shafran, I., Fink, M., Castaño, A., Giannoumis, I., Kim, W., Rybiński, M., Sreevatsa, A., Prendki, J., Soergel, D., Goedeckemeyer, A., Gierke, W., Jafari, M., Gaba, M., Wiesner, J., Wright, D. G., Wei, Y., Vashisht, H., Kulizhskaya, Y., Hoover, J., Le, M., Li, L., Iwuanyanwu, C., Liu, L., Ramirez, K., Khorlin, A., Cui, A., LIN, T., Georgiev, M., Wu, M., Aguilar, R., Pallo, K., Chakladar, A., Repina, A., Wu, X., van der Weide, T., Ponnapalli, P., Kaplan, C., Simsa, J., Li, S., Dousse, O., Yang, F., Piper, J., Ie, N., Lui, M., Pasumarthi, R., Lintz, N., Vijayakumar, A., Thiet, L. N., Andor, D., Valenzuela, P., Paduraru, C., Peng, D., Lee, K., Zhang, S., Greene, S., Nguyen, D. D., Kurylowicz, P., Velury, S., Krause, S., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z., Zhang, B., Singhal, A., Latkar, T., Zhang, M., Le, Q.,</p>
<p>Abellan, E. A., Du, D., McKinnon, D., Antropova, N., Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D., Raad, M. A., Crocker, R., Hawkins, P., Dadashi, R., Gaffney, C., Lall, S., Franko, K., Filonov, E., Bulanova, A., Leblond, R., Yadav, V., Chung, S., Askham, H., Cobo, L. C., Xu, K., Fischer, F., Xu, J., Sorokin, C., Alberti, C., Lin, C.-C., Evans, C., Zhou, H., Dimitriev, A., Forbes, H., Banarse, D., Tung, Z., Liu, J., Omernick, M., Bishop, C., Kumar, C., Sterneck, R., Foley, R., Jain, R., Mishra, S., Xia, J., Bos, T., Cideron, G., Amid, E., Piccinno, F., Wang, X., Banzal, P., Gurita, P., Noga, H., Shah, P., Mankowitz, D. J., Polozov, A., Kushman, N., Krakovna, V., Brown, S., Bateni, M., Duan, D., Firoiu, V., Thotakuri, M., Natan, T., Mohananey, A., Geist, M., Mudgal, S., Girgin, S., Li, H., Ye, J., Roval, O., Tojo, R., Kwong, M., Lee-Thorp, J., Yew, C., Yuan, Q., Bagri, S., Sinopalnikov, D., Ramos, S., Mellor, J., Sharma, A., Severyn, A., Lai, J., Wu, K., Cheng, H.-T., Miller, D., Sonnerat, N., Vnukov, D., Greig, R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J., Korchemniy, A., Tsai, T., Jasarevic, M., Kong, W., Dao, P., Zheng, Z., Liu, F., Yang, F., Zhu, R., Geller, M., Teh, T. H., Sanmiya, J., Gladchenko, E., Trdin, N., Sozanschi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, L., Elkind, C., Woodman, O., Carpenter, J., Papamakarios, G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Talbert, A., Goyal, A., Wu, D., Owusu-Afriyie, D., Du, C., Thornton, C., Pont-Tuset, J., Narayana, P., Li, J., Fatehi, S., Wieting, J., Ajmeri, O., Uria, B., Zhu, T., Ko, Y., Knight, L., Héliou, A., Niu, N., Gu, S., Pang, C., Tran, D., Li, Y., Levine, N., Stolovich, A., Kalb, N., SantamariaFernandez, R., Goenka, S., Yustalim, W., Strudel, R., Elqursh, A., Lakshminarayanan, B., Deck, C., Upadhyay, S., Lee, H., Dusenberry, M., Li, Z., Wang, X., Levin, K., Hoffmann, R., Holtmann-Rice, D., Bachem, O., Yue, S., Arora, S., Malmi, E., Mirylenka, D., Tan, Q., Koh, C., Yeganeh, S. H., Pöder, S., Zheng, S., Pongetti, F., Tariq, M., Sun, Y., Ionita, L., Seyedhosseini, M., Tafti, P., Kotikalapudi, R., Liu, Z., Gulati, A., Liu, J., Ye, X., Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B., Singh, S., Fan, W., Parisi, A., Stanton, J., Kuang, C., Koverkathu, V., Choquette-Choo, C. A., Li, Y., Lu, T., Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Bahargam, S., Willoughby, R., Gaddy, D., Dasgupta, I., Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Albrecht, B., Shenoy, A., Moiseev, F., Jacobsson, H., Ghaffarkhah, A., Rivière, M., Walton, A., Crepy, C., Parrish, A., Liu, Y., Zhou, Z., Farabet, C., Radebaugh, C., Srinivasan, P., van der Salm, C., Fidjeland, A., Scellato, S., Latorre-Chimoto, E., Klimczak-Plucińska, H., Bridson, D., de Cesare, D., Hudson, T., Mendolicchio, P., Walker, L., Morris, A., Penchev, I., Mauger, M., Guseynov, A., Reid, A., Odoom, S., Loher, L., Cotruta, V., Yenugula, M., Grewe, D., Petrushkina, A., Duerig, T., Sanchez, A., Yadlowsky, S., Shen, A., Globerson, A., Kurzrok, A., Webb,</p>
<p>L., Dua, S., Li, D., Lahoti, P., Bhupatiraju, S., Hurt, D., Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A., Belle, S. R., Wang, L., Tekur, C., Kale, M. S., Wei, J., Sang, R., Saeta, B., Liechty, T., Sun, Y., Zhao, Y., Lee, S., Nayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Bilal, T., Eltyshev, E., Balle, D., Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y., Xiong, X., Kang, K., Luisier, F., Tripuraneni, N., Madras, D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge, J., Zhang, H., Pruthi, G., Bauer, J., Yang, F., Mansour, R., Gelman, J., Xu, Y., Polovets, G., Liu, J., Cai, H., Chen, W., Sheng, X., Xue, E., Ozair, S., Yu, A., Angermueller, C., Li, X., Wang, W., Wiesinger, J., Koukoumidis, E., Tian, Y., Iyer, A., Gurumurthy, M., Goldenson, M., Shah, P., Blake, M., Yu, H., Urbanowicz, A., Palomaki, J., Fernando, C., Brooks, K., Durden, K., Mehta, H., Momchev, N., Rahimtoroghi, E., Georgaki, M., Raul, A., Ruder, S., Redshaw, M., Lee, J., Jalan, K., Li, D., Perng, G., Hechtman, B., Schuh, P., Nasr, M., Chen, M., Milan, K., Mikulik, V., Strohman, T., Franco, J., Green, T., Hassabis, D., Kavukcuoglu, K., Dean, J., and Vinyals, O. Gemini: A family of highly capable multimodal models, 2023. URL https://arxiv.org/abs/2312.11805.</p>
<p>Golkar, S., Pettee, M., Eickenberg, M., Bietti, A., Cranmer, M., Krawezik, G., Lanusse, F., McCabe, M., Ohana, R., Parker, L., Blancard, B. R.-S., Tesileanu, T., Cho, K., and Ho, S. xval: A continuous number encoding for large language models. Neural Information Processing Systems (NeurIPS) AI for Science Workshop, 2023. URL https : //openreview.net/forum?id=KHDMZtoF4i.</p>
<p>Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and Nagarajan, V. Think before you speak: Training language models with pause tokens. International Conference on Learning Representations (ICLR), 2024. URL https : //openreview.net/forum?id=ph04CKkPdC.</p>
<p>Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K. R., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Hajishirzi, H. Olmo: Accelerating the science of language models, 2024.</p>
<p>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset. Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2021. URL https:// openreview.net/forum?id=7Bywt2mQsCe.</p>
<p>Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. arXiv:2310.06825, 2023. URL https://arxiv.org/abs/2310.06825.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Neural Information Processing Systems (NeurIPS), 2022. URL https://openreview.net/ forum?id=e2TBb5y0yFf.</p>
<p>Kudo, T. Subword regularization: Improving neural network translation models with multiple subword candidates. Association for Computational Linguistics (ACL), 2018. URL https://aclanthology.org/P18-1007.</p>
<p>Lanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Denison, C., Hernandez, D., Li, D., Durmus, E., Hubinger, E., Kernion, J., et al. Measuring faithfulness in chain-of-thought reasoning. arXiv:2307.13702, 2023. URL https://arxiv.org/abs/2307.13702.</p>
<p>Lee, N., Sreenivasan, K., Lee, J. D., Lee, K., and Papailiopoulos, D. Teaching arithmetic to small transformers. arXiv:2307.03381, 2023. URL https:// arxiv.org/abs/2307.03381.</p>
<p>Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., GurAri, G., and Misra, V. Solving quantitative reasoning problems with language models. Neural Information Processing Systems (NeurIPS), 2022. URL https:// openreview.net/forum?id=IFXTZERXdM7.</p>
<p>Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks Are All You Need II: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.</p>
<p>Liu, R., Garrette, D., Saharia, C., Chan, W., Roberts, A., Narang, S., Blok, I., Mical, R., Norouzi, M., and Constant, N. Character-aware models improve visual text rendering. Association for Computational Linguistics (ACL), 2023. URL https://aclanthology.org/ 2023.acl-long. 900.</p>
<p>Lundberg, S. The art of prompt design: Prompt boundaries and token healing, 2023. URL https: //towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38.</p>
<p>McCoy, R. T., Yao, S., Friedman, D., Hardy, M., and Griffiths, T. L. Embers of autoregression: Understanding large language models through the problem they
are trained to solve. arXiv:2309.13638, 2023. URL https://arxiv.org/abs/2309.13638.</p>
<p>Muffo, M., Cocco, A., and Bertino, E. Evaluating transformer language models on arithmetic operations using number decomposition. Language Resources and Evaluation Conference (LREC), 2022. URL https: //aclanthology.org/2022.lrec-1.30.</p>
<p>Nogueira, R., Jiang, Z., and Lin, J. Investigating the limitations of transformers with simple arithmetic tasks. arXiv:2102.13019, 2021. URL https:// arxiv.org/abs/2102.13019.</p>
<p>Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv:2112.00114, 2021. URL https:// arxiv.org/abs/2112.00114.</p>
<p>OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Lukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O.,</p>
<p>Mély, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O’Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M., Pong, V., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. GPT-4 Technical Report, 2023. URL https://arxiv.org/abs/2303.08774.</p>
<p>Paster, K. Testing language models on a heldout high school national finals exam. https: //huggingface.co/datasets/keirp/ hungarian_national_hs_finals_exam, 2023.</p>
<p>Razeghi, Y., Logan IV, R. L., Gardner, M., and Singh, S. Impact of pretraining term frequencies on few-shot numerical reasoning. Empirical Methods in Natural Language Processing (EMNLP), 2022. URL https://aclanthology.org/ 2022.findings-emnlp. 59.</p>
<p>Rumbelow, J. and mwatkins. Solidgoldmagikarp (plus, prompt generation), 2023. URL https://www.lesswrong.com/posts/ aPeJE8bSo6rAFoLqg/solidgoldmagikarpplus-prompt-generation.</p>
<p>Saxton, D., Grefenstette, E., Hill, F., and Kohli, P. Analysing mathematical reasoning abilities of neural models. International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/ forum?id=H1gR5iR5FX.</p>
<p>Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. Association for Computational Linguistics (ACL), 2016. URL https://aclanthology.org/P16-1162.</p>
<p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https:// aclanthology.org/2020.emnlp-main.346.</p>
<p>Stolfo, A., Belinkov, Y., and Sachan, M. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Empirical Methods in Natural Language Processing (EMNLP), 2023. URL https://openreview.net/ forum?id=aB3Hwh4UzP.</p>
<p>Sun, K., Qi, P., Zhang, Y., Liu, L., Wang, W., and Huang, Z. Tokenization consistency matters for generative models on extractive NLP tasks. Empirical Methods in Natural Language Processing (EMNLP), 2023. URL https://aclanthology.org/ 2023.findings-emnlp.887.</p>
<p>Teknium. How did the gpt tokenizer get created?, 2023. URL https://twitter.com/Teknium1/ status/1634667026739527680?s=20.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023a. URL https://arxiv.org/abs/2302.13971.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and finetuned chat models. arXiv:2307.09288, 2023b. URL https://arxiv.org/abs/2307.09288.</p>
<p>Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model, 2021. URL https://huggingface.co/EleutherAI/ gpt-j-6b.</p>
<p>Wei, J. Sorting a list of words by the second letter, 2023. URL https://x.com/..jasonwei/ status/1661781746759909376?s=20.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. Neural Information Processing Systems (NeurIPS), 2022. URL https://openreview.net/ forum?id=VJQlMeSB_J.</p>
<p>Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. MEGABYTE: Predicting million-byte sequences with multiscale transformers. Neural Information Processing Systems (NeurIPS), 2023. URL https: //openreview.net/forum?id=JTmO2V9Xpz.</p>
<p>Zhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., and Sedghi, H. Teaching algorithmic reasoning via in-context learning. arXiv:2211.09066, 2022. URL https://arxiv.org/abs/2211.09066.</p>
<p>Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P. What algorithms can transformers learn? a study in length generalization. arXiv:2310.16028, 2023. URL https: //arxiv.org/abs/2310.16028.</p>
<h1>A. Tokenization differences between frontier LLMs</h1>
<p>000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999</p>
<p>Figure 14. The equivalent of Figure 2 but for the Claude tokenizer. All 3-digit number strings, colored red when the string does not have a corresponding single token dedicated to it. The lack of systematicity suggests that Claude tokenizes numbers using pure BPE. Note also, however, that token coverage is generally higher than in Figure 2, likely in part because the Claude tokenizer has a larger vocabulary size ( 65 k tokens) than OpenAI's p50k_base ( 50 k tokens).
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 15. The equivalent of Figure 3 but for the Claude tokenizer. Note that this distribution looks more like p50k_base than cl100k_base in Figure 3. This, along with Figure 14 above shows that Claude's tokenizer exhibits a lack of systemacity when tokenizing numbers, suggesting the use of pure BPE number tokens, rather than something bespoke (as other current models use; see Table 1).</p>
<h2>B. Experiments with other system prompts</h2>
<p>We also conducted our main experiment with an alternate, custom system prompt (as opposed to the default ' You are a helpful assistant. '). The prompt we used was:</p>
<p>You are MathGPT, an expert at solving math problems. When given a math problem, you respond only by repeating the problem statement and appending the answer. You do not say any other words.</p>
<p>Results using this prompt are presented in Figure 16. We found it lead to small improvements in performance at low shot numbers (e.g., 1-shot) but these diminished at 8 -shots. To maximize the reproducibility and applicability of our results, we decided to just use the default prompt. As we report 8 -shot results throughout most of the paper, we doubt the system prompt would have a large effect on our results, given Figure 16.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 16. Comparison of R2L and L2R tokenization strategies for different numbers of shots and using two different system prompts.</p>
<h1>C. Frequency effects</h1>
<p>Given the findings of prior work on numerical reasoning demonstrating frequency effects (Razeghi et al., 2022), we also investigated whether or not our observed error patterns could be explained by term frequency. While we do not have access to the pre-training data of GPT-3.5 and GPT-4 models, we use the tokenizer merge ranks ${ }^{17}$ as a signal of term frequency. We analyze the expanded set of problems used for the error analysis in Section 4. Our results are summarized below:</p>
<p>When making an error, GPT-3.5 is slightly more likely to output a more frequent token. For each token in the model response on problems where it makes a mistake, we consider if the outputted incorrect token is more or less frequent (has lower or higher merge rank) than the correct one. Of the 25 errors made by the model when using R2L tokenization, 15 involve substituting in a more frequent token ( $60 \%, p=0.115$ using a binomial null distribution assuming chance is $50 \%$ ). Of the 425 errors made when using L2R tokenization, 238 involve substituting in a more frequent token ( $56 \%, p=0.005$ using a binomial null distribution assuming chance is $50 \%$ ). While we do see a significant effect in the L2R tokenization case, the margin is relatively small, which suggests that token frequency is not the dominant reason behind the error patterns.</p>
<p>When using L2R tokenization in the length mismatch case, GPT-3.5 errors do not show strong correlation to token frequency. In Section 4.3, we found that GPT-3.5 always gets the fourth digit wrong (Figure 9a). We then found correlation in the specific error in digit 4 to the magnitude difference between correct digit and digit in the model response (Figure 9b). Here, we ask if the substituted token 2 (whose first digit would be digit 4 of the response) is correlated to frequency in training data. Specifically, for each problem, we rank the tokens corresponding to the 10 possible "digit 4 mistakes" by merge rank. In Figure 17, we show the distribution of ranks across all 365 problems where the model makes "digit 4" errors. If models are preferentially substituting in more frequent tokens, we would expect to see a negative trend from the top left to the bottom right (as we did in Figure 9b). In Figure 17, we see a slight preference for outputting the most likely token (roughly $16 \%$ of the time, where chance would be $10 \%$ ), but overall we see no clear trend.</p>
<p>Off-by-one errors do not seem to be correlated to answer token frequency. In Section 4.4, we found that the vast majority of remaining errors (for R2L tokenization, and for L2R tokenization in the length match condition) are off-by-one errors in the units digit of a token. Here, we ask if the specific substitution by the model is correlated to token frequency, measured by merge rank. Specifically, we condition on the model possibly making an off-by-one error, which means there are 3 possible output tokens (the correct token, the correct token minus one, the correct token plus one). We then rank these tokens based on merge rank, and see if the model preferentially picks the token with lowest merge rank. Of the 24 off-by-one errors when using R2L tokenization, we find the model only picks the "most frequent" token 7 times. Of the 53 off-by-one errors when using L2R tokenization, we find the model only picks the "most frequent" token 17 times. Both of these are essentially what we would expect by chance (one out of three), which suggests that output token frequency effects are not a dominant factor in why the model makes off-by-one errors.</p>
<p>Overall, we find mild to no evidence of token frequency effects in our experiments. This could be due to the presumably larger scale of GPT-3.5 (as compared to GPT-J, used by Razeghi et al. (2022)). However, we note that our method of measuring token frequency is imperfect-relying on BPE merge ranks to signal frequency as we do not have access to</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 17. Distribution of relative rank of substituted incorrect token 2 in model response when using L2R tokenization in the length mismatch condition.
pre-training data. Future work could study such associations further in newer, larger models with open pretraining data (Groeneveld et al., 2024).</p>
<h1>D. Stereotyped patterns in model log probabilities</h1>
<p>Mirroring the results of Section 4, we found stereotyped patterns in model log probabilities ("logprob"). Specifically, the OpenAI API returns the top 5 tokens at each position with their corresponding logprobs. We analyzed these log probabilities in three cases: L2R tokenization on length mismatch problems, L2R tokenization on length match problems, and R2L tokenization on all problems. These conditions mirror the most salient error effects we found in Section 4, with the former leading to "digit 4 " errors, and the latter two leading to mostly off-by-one errors.</p>
<p>In addition to the raw logprobs, we computed an additional entropy metric (per output token) to measure model uncertainty in its output. Since access is restricted to the top 5 logprobs, we use the following lower bound, $H_{\text {lower }}$, to the true entropy:</p>
<p>$$
\begin{aligned}
H_{\text {true }} &amp; \equiv-\sum_{i=1}^{V} p_{i} \log \left(p_{i}\right) \
&amp; =-\left(\sum_{i=1}^{5} p_{i} \log \left(p_{i}\right)+\sum_{i=6}^{V} p_{i} \log \left(p_{i}\right)\right) \
&amp; \geq-\left(\sum_{i=1}^{5} p_{i} \log \left(p_{i}\right)+\sum_{i=6}^{V} p_{i} \log \left(p_{5}\right)\right) \
&amp; =-\left(\sum_{i=1}^{5} p_{i} \log \left(p_{i}\right)+\left(1-\sum_{i=1}^{5} p_{i}\right) \cdot \log \left(p_{5}\right)\right) \
&amp; \equiv H_{\text {lower }}
\end{aligned}
$$</p>
<p>where $p_{i}$ denotes the probability of the $i$-th most likely token. We use the natural logarithm for entropy, so all entropies are in nats (not bits).</p>
<p>For the "digit 4" error pattern (Section 4.3), we find an interesting trend in model entropy. The entropy both on problems it gets incorrect ( $91.25 \%$ ) and correct ( $8.25 \%$ ) is roughly the same ( 2.066 and 2.061 respectively). Even when the model gets the question right, it's unsure of its answer, suggesting that it might just be guessing a second output token with the right tens and ones digit and random hundreds digit. Providing further evidence for this mechanism, we observe that, of the problems where the model makes an error, about half ( $49.6 \%$ ) of the time the correct answer appears in the top 5 output tokens. This is in line with what we would see for random guessing from the 10 tokens. That said, the model may exhibit some degree of bias towards the correct output, as evidenced by the downward trend in Figure 9b.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 18. Histogram of difference in answer log probabilities (summed over tokens) on length match problems that the model answers correctly using both R2L and L2R tokenization. Black dotted line signifies 0 . Red dotted line shows the average difference-on average, the model is more "confident" when using R2L tokenization.</p>
<p>For the off-by-one error patterns (Section 4.4), we observe a qualitatively different trend. Specifically, of the 53 off-by-one errors when using L2R tokenization on length match problems, in all cases we find that the second most likely token is the correct answer. We observe the same effect on the 25 off-by-one errors when using R2L tokenization. Furthermore, the entropy in both cases is around $0.45 \pm 0.05$, indicating that the model puts most of its weight on these top 2 most likely tokens. Unlike in the "digit 4" case, model entropy on correct problems is significantly lower (approximately 0.03 , averaged across dataset and tokens) indicating that the model is "confidently correct" when using L2R tokenization on length match problems or R2L tokenization on all problems. Interestingly, on the subset of length match problems that the model answers correctly in both L2R and R2L tokenization, we found the model is slightly more confident when using R2L tokenization (which aligns with our intuition, as the model is also more often correct when using R2L tokenization)—see Figure 18.</p>
<p>These results demonstrate that, depending on tokenization direction and alignment between input and output tokenization, we observe stereotyped patterns in model log probabilities. When using L2R tokenization on length mismatch problems, the model appears to make a magnitude-biased guess between all possible fourth digits (corresponding to 10 possible tokens ${ }^{18}$ ). In the other cases, the model is mostly confidently correct. When it does make an error, it's almost always an off-by-one error (Section 4.4) where it's uncertain between its chosen off-by-one incorrect answer and the true answer, but does not really consider other outputs beyond these two. ${ }^{19}$</p>
<h1>E. Additional experimental details</h1>
<p>All code and raw results can be found at https://github.com/aadityasingh/TokenizationCounts.</p>
<h2>E.1. Length control for error analysis</h2>
<p>As described in Section 4.1, after noticing errors mostly come from the length mismatch condition in our original experiments (which used 90 problems, balanced by input digit length), we conducted a larger experiment where we controlled for input and output digit lengths. Specifically, we considered the following (addend1_length, addend2_length, answer_length) triplets: $(7,7,7),(7,7,8),(8,7,8),(7,8,8),(8,7,9),(7,8,9),(8,8,8),(8,8,9),(9,7,9),(7,9,9),(9,8,9),(8,9,9),(9,9,9)$. Problems in each condition were sampled randomly so as to satisfy the digit length constraints for each triplet. We sampled 100 problems for each triplet, for a total of 1300 problems.</p>
<h2>E.2. Access dates</h2>
<p>Given the changing nature of the OpenAI API, we report access dates for all experiments below. We tried to use the supposed "fixed" models for all experiments, but did notice some non-determinism, even at temperature 0 -an issue that may be due to non-determinism in floating point arithmetic. We also note that the gpt-4-0314 appears to have</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>been early-deprecated, as we can no longer access it despite the supposed June 13, 2024 deprecation date on https: //platform.openai.com/docs/deprecations.</p>
<p>Access dates by figure in main text:</p>
<ul>
<li>gpt-3.5-turbo-0301, Figure 4: April 7, 2023</li>
<li>gpt-3.5-turbo-0301, Figure 5: May 18, 2023</li>
<li>gpt-3.5-turbo-0301, Figure 6 left two columns: May 18, 2023</li>
<li>gpt-3.5-turbo-0301, Figure 6 right two columns: April 7, 2023</li>
<li>gpt-3.5-turbo-0301, Figure 7-8: January 25, 2024</li>
<li>gpt-3.5-turbo-0301, Figure 10-11 May 24, 2024</li>
<li>gpt-4-0314, Figure 12: May 2, 2023</li>
<li>gpt-3.5-turbo-0613, Figure 12-13: January 25, 2024</li>
<li>gpt-3.5-turbo-1106, Figure 12-13: January 29, 2024</li>
<li>gpt-4-0613, Figure 12-13: January 25, 2024</li>
<li>gpt-4-1106-preview, Figure 12-13: January 29, 2024</li>
</ul>
<h1>E.3. Example prompts</h1>
<p>In this section, we provide example prompts we used for various experiments. For simplicity, we use the same query for each prompt shown below, and we only use 2 shots (most experiments in the main text are done with 8 shots). In practice, we sampled shots randomly (controlling for digit length to match the query length) for each query, as explained in Section 2.1. For the experiments described in Section 4 and Appendix E.1, the shots were also controlled to have the same answer length as the query. The examples we present below, though, are for the runs in the rest of the paper (where only input digit lengths are controlled). For maximum clarity, we display prompts as the list of dictionaries that gets sent to OpenAI's API and roughly in the order used for figures in the paper. Following the advice at https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples, we make use of the multi-turn chat dialog to prompt the model, as opposed to one big user message with all the examples.</p>
<p>L2R tokenization, input-digit-controlled for two 7-digit numbers:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]
</code></pre></div>

<p>R2L tokenization, input-digit-controlled for two 7-digit numbers:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3,790,206+6,739,555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10,529,761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6,777,159+7,096,168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13,873,327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8,302,080+3,529,456=&#39;}]
</code></pre></div>

<p>R2L tokenization, delimiter-control condition using '#' :</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3#790#206+6#739#555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10#529#761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6#777#159+7#096#168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13#873#327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8#302#080+3#529#456=&#39;}]
</code></pre></div>

<p>L2R tokenization, thinking token control by using separators in L2R tokenization:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;379,020,6+673,955,5=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;105,297,61&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;677,715,9+709,616,8=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;138,733,27&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;830,208,0+352,945,6=&#39;}]
</code></pre></div>

<p>L2R tokenization, thinking token control by using 2 extra spaces:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206 + 6739555 = &#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159 + 7096168 = &#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080 + 3529456 = &#39;}]
</code></pre></div>

<p>L2R tokenization, thinking token control by using 2 extra spaces:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206 + 6739555 = &#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;10529761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159 + 7096168 = &#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;13873327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080 + 3529456 = &#39;}]
</code></pre></div>

<p>Repeat L2R $\rightarrow$ R2L:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;3,790,206+6,739,555=10,529,761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;6,777,159+7,096,168=13,873,327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]
</code></pre></div>

<p>Repeat control L2R $\rightarrow$ L2R:</p>
<div class="codehilite"><pre><span></span><code>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a helpful assistant.&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;3790206+6739555=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;3790206+6739555=10529761&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;6777159+7096168=&#39;},
{&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;6777159+7096168=13873327&#39;},
{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;8302080+3529456=&#39;}]
</code></pre></div>

<p>Output control L2R $\rightarrow$ R2L:
[{'role': 'system', 'content': 'You are a helpful assistant.'},
{'role': 'user', 'content': '3790206+6739555='},
{'role': 'assistant', 'content': '10,529,761' },
{'role': 'user', 'content': '6777159+7096168='},
{'role': 'assistant', 'content': '13,873,327' },
{'role': 'user', 'content': '8302080+3529456='}]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{18} \mathrm{~A}$ completely random guess over 10 tokens would correspond to an entropy of about 2.3 , which is in line with the lower bound observed (of about 2.06) and the finding of a slight mangitude bias (which would decrease the entropy from 2.3).
${ }^{19} \mathrm{~A}$ completely random guess over 2 tokens would correspond to an entropy of about 0.69 , which is in line with the lower bound observed (of about 0.45 ).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{16} 3$-digit number tokens also reduce inference-time compute when models use numbers in their output, which could be an important consideration when serving models at scale.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1753 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1753</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1753</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-c37c23b12e00168833eccff8025a830ce27c5abc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c37c23b12e00168833eccff8025a830ce27c5abc" target="_blank">Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments</a></p>
                <p><strong>Paper Venue:</strong> 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This work provides the first benchmark dataset for visually-grounded natural language navigation in real buildings - the Room-to-Room (R2R) dataset and presents the Matter-port3D Simulator - a large-scale reinforcement learning environment based on real imagery.</p>
                <p><strong>Paper Abstract:</strong> A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matter-port3D Simulator - a large-scale reinforcement learning environment based on real imagery [11]. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings - the Room-to-Room (R2R) dataset1.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Embodied Question Answering <em>(Rating: 2)</em></li>
                <li>IQA: Visual question answering in interactive environments <em>(Rating: 2)</em></li>
                <li>Gated-attention architectures for task-oriented language grounding <em>(Rating: 1)</em></li>
                <li>Mapping instructions and visual observations to actions with reinforcement learning <em>(Rating: 1)</em></li>
                <li>Listen, attend, and walk: Neural mapping of navigational instructions to action sequences <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1753",
    "paper_id": "paper-c37c23b12e00168833eccff8025a830ce27c5abc",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Embodied Question Answering",
            "rating": 2
        },
        {
            "paper_title": "IQA: Visual question answering in interactive environments",
            "rating": 2
        },
        {
            "paper_title": "Gated-attention architectures for task-oriented language grounding",
            "rating": 1
        },
        {
            "paper_title": "Mapping instructions and visual observations to actions with reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences",
            "rating": 1
        }
    ],
    "cost": 0.0061145,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</h1>
<p>Peter Anderson ${ }^{1}$ Qi Wu ${ }^{2}$ Damien Teney ${ }^{2}$ Jake Bruce ${ }^{3}$ Mark Johnson ${ }^{4}$<br>Niko Sünderhauf ${ }^{3}$ Ian Reid ${ }^{2}$ Stephen Gould ${ }^{1}$ Anton van den Hengel ${ }^{2}$<br>${ }^{1}$ Australian National University ${ }^{2}$ University of Adelaide ${ }^{3}$ Queensland University of Technology ${ }^{4}$ Macquarie University<br>${ }^{1}$ firstname.lastname@anu.edu.au, ${ }^{3}$ jacob.bruce@hdr.qut.edu.au, ${ }^{4}$ niko.suenderhauf@qut.edu.au<br>${ }^{2}$ {qi.wu01, damien.teney, ian.reid, anton.vandenhengel}@adelaide.edu.au, ${ }^{4}$ mark.johnson@mq.edu.au</p>
<h4>Abstract</h4>
<p>A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a naturallanguage navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visuallygrounded navigation instructions, we present the Matterport3D Simulator - a large-scale reinforcement learning environment based on real imagery [11]. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings - the Room-to-Room (R2R) dataset ${ }^{1}$.</p>
<h2>1. Introduction</h2>
<p>The idea that we might be able to give general, verbal instructions to a robot and have at least a reasonable probability that it will carry out the required task is one of the long-held goals of robotics, and artificial intelligence (AI). Despite significant progress, there are a number of major technical challenges that need to be overcome before robots will be able to perform general tasks in the real world. One of the primary requirements will be new techniques for linking natural language to vision and action in unstructured, previously unseen environments. It is the navigation version</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Instruction: Head upstairs and walk past the piano through an archway directly in front. Turn right when the hallway ends at pictures and table. Wait by the moose antlers hanging on the wall.</p>
<p>Figure 1. Room-to-Room (R2R) navigation task. We focus on executing natural language navigation instructions in previously unseen real-world buildings. The agent's camera can be rotated freely. Blue discs indicate nearby (discretized) navigation options.
of this challenge that we refer to as Vision-and-Language Navigation (VLN).</p>
<p>Although interpreting natural-language navigation instructions has received significant attention previously [12, $13,20,38,41,52]$, it is the recent success of recurrent neural network methods for the joint interpretation of images and natural language that motivates the VLN task, and the associated Room-to-Room (R2R) dataset described below. The dataset particularly has been designed to simplify the application of vision and language methods to what might otherwise seem a distant problem.</p>
<p>Previous approaches to natural language command of robots have often neglected the visual information processing aspect of the problem. Using rendered, rather than real images [7, 27, 62], for example, constrains the set of vis-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Differences between Vision-and-Language Navigation (VLN) and Visual Question Answering (VQA). Both tasks can be formulated as visually grounded sequence-to-sequence transcoding problems. However, VLN sequences are much longer and, uniquely among vision and language benchmark tasks using real images, the model outputs actions <em>a</em><sub>0</sub>, <em>a</em><sub>1</sub>, ..., <em>a</em><sub>T</sub> that manipulate the camera viewpoint.</p>
<p>The objective of the study is to identify hand-crafted models available to the reader. This turns the robot's challenging open-set problem of relating real language to real imagery into a far simpler closed-set classification problem. The natural extension of this process is that adopted in works where the images are replaced by a set of labels [13, 52]. Limiting the variation in the imagery inevitably limits the variation in the navigation instructions also. What distinguishes the VLN challenge is that the agent is required to interpret a previously <em>unseen</em> natural-language navigation command in light of images generated by a previously <em>unseen</em> real environment. The task thus more closely models the distinctly open-set nature of the underlying problem.</p>
<p>To enable the reproducible evaluation of VLN methods, we present the Matterport3D Simulator. The simulator is a large-scale interactive reinforcement learning (RL) environment constructed from the Matterport3D dataset [11] which contains 10,800 densely-sampled panoramic RGB-D images of 90 real-world building-scale indoor environments. Compared to synthetic RL environments [7, 27, 62], the use of real-world image data preserves visual and linguistic richness, maximizing the potential for trained agents to be transferred to real-world applications.</p>
<p>Based on the Matterport3D environments, we collect the Room-to-Room (R2R) dataset containing 21,567 open-vocabulary, crowd-sourced navigation instructions with an average length of 29 words. Each instruction describes a trajectory traversing typically multiple rooms. As illustrated in Figure 1, the associated task requires an agent to follow natural-language instructions to navigate to a goal location in a previously unseen building. We investigate the difficulty of this task, and particularly the difficulty of operating in unseen environments, using several baselines and a sequence-to-sequence model based on methods successfully applied to other vision and language tasks [4, 14, 19].</p>
<p>In summary, our main contributions are:</p>
<ol>
<li>We introduce the Matterport3D Simulator, a software framework for visual reinforcement learning using the Matterport3D panoramic RGB-D dataset [11];</li>
<li>We present Room-to-Room (R2R), the first benchmark dataset for Vision-and-Language Navigation in real, previously unseen, building-scale 3D environments;</li>
<li>We apply sequence-to-sequence neural networks to the R2R dataset, establishing several baselines.</li>
</ol>
<p>The simulator, R2R dataset and baseline models are available through the project website at https://bringmeaspoon.org.</p>
<h2>2. Related Work</h2>
<p><strong>Navigation and language</strong> Natural language command of robots in unstructured environments has been a research goal for several decades [57]. However, many existing approaches abstract away the problem of visual perception to some degree. This is typically achieved either by assuming that the set of all navigation goals, or objects to be acted upon, has been enumerated, and that each will be identified by label [13, 52], or by operating in visually restricted environments requiring limited perception [12, 20, 24, 29, 35, 38, 55]. Our work contributes to the first time a navigation benchmark dataset that is both linguistically and visually rich, moving closer to real scenarios while still enabling reproducible evaluations.</p>
<p><strong>Vision and language</strong> The development of new benchmark datasets for image captioning [14], visual question answering (VQA) [4, 19] and visual dialog [17] has spurred considerable progress in vision and language understanding, enabling models to be trained end-to-end on raw pixel data from large datasets of natural images. However, although many tasks combining visual and linguistic reasoning have been motivated by their potential robotic applications [4, 17, 26, 36, 51], none of these tasks allow an agent to move or control the camera. As illustrated in Figure 2, our proposed R2R benchmark addresses this limitation, which also motivates several concurrent works on embodied question answering [16, 18].</p>
<p>Navigation based simulators Our simulator is related to existing 3D RL environments based on game engines, such as ViZDoom [27], DeepMind Lab [7] and AI2-THOR [30], as well as a number of newer environments developed concurrently including HoME [10], House3D [58], MINOS [47], CHALET [59] and Gibson Env [61]. The main advantage of our framework over synthetic environments [30, 10, 58, 59] is that all pixel observations come from natural images of real scenes, ensuring that almost every coffee mug, pot-plant and wallpaper texture is unique. This visual diversity and richness is hard to replicate using a limited set of 3D assets and textures. Compared to MINOS [47], which is also based on Matterport data [11], we render from panoramic images rather than textured meshes. Since the meshes have missing geometry - particularly for windows and mirrors - our approach improves visual realism but limits navigation to discrete locations (refer Section 3.2 for details). Our approach is similar to the (much smaller) Active Vision Dataset [2].</p>
<p>RL in navigation A number of recent papers use reinforcement learning (RL) to train navigational agents [31, $50,53,62,21]$, although these works do not address language instruction. The use of RL for language-based navigation has been studied in [12] and [41], however, the settings are visually and linguistically less complex. For example, Chaplot et al. [12] develop an RL model to execute template-based instructions in Doom environments [27]. Misra et al. [41] study complex language instructions in a fully-observable blocks world. By releasing our simulator and dataset, we hope to encourage further research in more realistic partially-observable settings.</p>
<h2>3. Matterport3D Simulator</h2>
<p>In this section we introduce the Matterport3D Simulator, a new large-scale visual reinforcement learning (RL) simulation environment for the research and development of intelligent agents based on the Matterport3D dataset [11]. The Room-to-Room (R2R) navigation dataset is discussed in Section 4.</p>
<h3>3.1. Matterport3D Dataset</h3>
<p>Most RGB-D datasets are derived from video sequences; e.g. NYUv2 [42], SUN RGB-D [48] and ScanNet [15]. These datasets typically offer only one or two paths through a scene, making them inadequate for simulating robot motion. In contrast to these datasets, the recently released Matterport3D dataset [11] contains a comprehensive set of panoramic views. To the best of our knowledge it is also the largest currently available RGB-D research dataset.</p>
<p>In detail, the Matterport3D dataset consists of 10,800 panoramic views constructed from 194,400 RGB-D images of 90 building-scale scenes. On average, panoramic view-
points are distributed throughout the entire walkable floor plan of each scene at an average separation of 2.25 m . Each panoramic view is comprised of 18 RGB-D images captured from a single 3D position at the approximate height of a standing person. Each image is annotated with an accurate 6 DoF camera pose, and collectively the images capture the entire sphere except the poles. The dataset also includes globally-aligned, textured 3D meshes annotated with class and instance segmentations of regions (rooms) and objects.</p>
<p>In terms of visual diversity, the selected Matterport scenes encompass a range of buildings including houses, apartments, hotels, offices and churches of varying size and complexity. These buildings contain enormous visual diversity, posing real challenges to computer vision. Many of the scenes in the dataset can be viewed in the Matterport 3D spaces gallery ${ }^{2}$.</p>
<h3>3.2. Simulator</h3>
<h3>3.2.1 Observations</h3>
<p>To construct the simulator, we allow an embodied agent to virtually 'move' throughout a scene by adopting poses coinciding with panoramic viewpoints. Agent poses are defined in terms of 3D position $v \in V$, heading $\psi \in[0,2 \pi)$, and camera elevation $\theta \in\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]$, where $V$ is the set of 3D points associated with panoramic viewpoints in the scene. At each step $t$, the simulator outputs an RGB image observation $o_{t}$ corresponding to the agent's first person camera view. Images are generated from perspective projections of precomputed cube-mapped images at each viewpoint. Future extensions to the simulator will also support depth image observations (RGB-D), and additional instrumentation in the form of rendered object class and object instance segmentations (based on the underlying Matterport3D mesh annotations).</p>
<h3>3.2.2 Action Space</h3>
<p>The main challenge in implementing the simulator is determining the state-dependent action space. Naturally, we wish to prevent agents from teleporting through walls and floors, or traversing other non-navigable regions of space. Therefore, at each step $t$ the simulator also outputs a set of next step reachable viewpoints $W_{t+1} \subseteq V$. Agents interact with the simulator by selecting a new viewpoint $v_{t+1} \in W_{t+1}$, and nominating camera heading ( $\Delta \psi_{t+1}$ ) and elevation $\left(\Delta \theta_{t+1}\right)$ adjustments. Actions are deterministic.</p>
<p>To determine $W_{t+1}$, for each scene the simulator includes a weighted, undirected graph over panoramic viewpoints, $G=\langle V, E\rangle$, such that the presence of an edge signifies a robot-navigable transition between two viewpoints,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and the weight of that edge reflects the straight-line distance between them. To construct the graphs, we ray-traced between viewpoints in the Matterport3D scene meshes to detect intervening obstacles. To ensure that motion remains localized, we then removed edges longer than 5m. Finally, we manually verified each navigation graph to correct for missing obstacles not captured in the meshes (such as windows and mirrors).</p>
<p>Given navigation graph $G$, the set of next-step reachable viewpoints is given by:</p>
<p>$W_{t+1}=\left{v_{t}\right}\cup\left{v_{i}\in V \mid\left\langle v_{t}, v_{i}\right\rangle \in E \wedge v_{i} \in P_{t}\right}$ (1)</p>
<p>where $v_{t}$ is the current viewpoint, and $P_{t}$ is the region of space enclosed by the left and right extents of the camera view frustum at step $t$. In effect, the agent is permitted to follow any edges in the navigation graph, provided that the destination is within the current field of view, or visible by glancing up or down ${ }^{3}$. Alternatively, the agent always has the choice to remain at the same viewpoint and simply move the camera.</p>
<p>Figure 3 illustrates a partial example of a typical navigation graph. On average each graph contains 117 viewpoints, with an average vertex degree of 4.1. This compares favorably with grid-world navigation graphs which, due to walls and obstacles, must have an average degree of less than 4. As such, although agent motion is discretized, this does not constitute a significant limitation in the context of most high-level tasks. Even with a real robot it may not be practical or necessary to continuously re-plan higher-level objectives with every new RGB-D camera view. Indeed, even agents operating in 3D simulators that notionally support continuous motion typically use discretized action spaces in practice [62, 16, 18, 47].</p>
<p>The simulator does not define or place restrictions on the agent's goal, reward function, or any additional context (such as natural language navigation instructions). These aspects of the RL environment are task and dataset dependent, for example as described in Section 4.</p>
<h3>3.2.3 Implementation Details</h3>
<p>The Matterport3D Simulator is written in C++ using OpenGL. In addition to the C++ API, Python bindings are also provided, allowing the simulator to be easily used with deep learning frameworks such as Caffe [25] and TensorFlow [1], or within RL platforms such as ParlAI [39] and OpenAI Gym [9]. Various configuration options are offered for parameters such as image resolution and field of view. Separate to the simulator, we have also developed a WebGL browser-based visualization library for collecting text annotations of navigation trajectories using Amazon Mechanical Turk, which we will make available to other researchers.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Example navigation graph for a partial floor of one building-scale scene in the Matterport3D Simulator. Navigable paths between panoramic viewpoints are illustrated in blue. Stairs can also be navigated to move between floors.</p>
<h3>3.2.4 Biases</h3>
<p>We are reluctant to introduce a new dataset (or simulator, in this case) without at least some attempt to address its limitations and biases [54]. In the Matterport3D dataset we have observed several selection biases. First, the majority of captured living spaces are scrupulously clean and tidy, and often luxurious. Second, the dataset contains very few people and animals, which are a mainstay of many other vision and language datasets [14, 4]. Finally, we observe some capture bias as selected viewpoints generally offer commanding views of the environment (and are therefore not necessarily in the positions in which a robot might find itself). Alleviating these limitations to some extent, the simulator can be extended by collecting additional building scans. Refer to Stanford 2D-3D-S [5] for a recent example of an academic dataset collected with a Matterport camera.</p>
<h2>4. Room-to-Room (R2R) Navigation</h2>
<p>We now describe the Room-to-Room (R2R) task and dataset, including an outline of the data collection process and analysis of the navigation instructions gathered.</p>
<h3>4.1. Task</h3>
<p>As illustrated in Figure 1, the R2R task requires an embodied agent to follow natural language instructions to navigate from a starting pose to a goal location in the Matterport3D Simulator. Formally, at the beginning of each episode the agent is given as input a natural language instruction $\bar{x}=\left\langle x_{1}, x_{2}, \ldots x_{L}\right\rangle$, where $L$ is the length of the instruction and $x_{i}$ is a single word token. The agent observes an initial RGB image $o_{0}$, determined by the agent's initial pose comprising a tuple of 3D position, heading and elevation $s_{0}=\left\langle v_{0}, \psi_{0}, \theta_{0}\right\rangle$. The agent must execute a sequence of actions $\left\langle s_{0}, a_{0}, s_{1}, a_{1}, \ldots, s_{T}, a_{T}\right\rangle$, with each ac-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Pass the pool and go indoors using the double glass doors. Pass the large table with chairs and turn left and wait by the wine bottles that have grapes by them.</p>
<p>Walk straight through the room and exit out the door on the left. Keep going past the large table and turn left. Walk down the hallway and stop when you reach the 2 entry ways. One in front of you and one to your right. The bar area is to your left.</p>
<p>Enter house through double doors, continue straight across dining room, turn left into bar and stop on the circle on the ground.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Exit the office then turn left and then turn left in the hallway and head down the hallway until you get to a door on your left and go into office 359 then stop.</p>
<p>Go out of the room and take a left. Go into the first room on your left.</p>
<p>Leave the office and take a left. Take the next left at the hallway. Walk down the hall and enter the first office on the left. Stop next to the door to office 359.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Standing in front of the family picture, turn left and walk straight through the bathroom past the tub and mirrors. Go through the doorway and stop when the door to the bathroom is on your right and the door to the closet is to your left.</p>
<p>Walk with the family photo on your right. Continue straight into the bathroom. Walk past the bathtub. Stop in the hall between the bathroom and toilet doorways.</p>
<p>Walk straight passed bathtub and stop with closet on the left and toilet on the right.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Go up the stairs and turn right. Go past the bathroom and stop next to the bed.</p>
<p>Walk all the way up the stairs, and immediately turn right. Pass the bathroom on the left, and enter the bedroom that is right there, and stop there.</p>
<p>Walk up the stairs turn right at the top and walk through the doorway continue straight and stop inside the bedroom.</p>
<p>Figure 4. Randomly selected examples of navigation instructions (three per trajectory) shown with the view from the starting pose.</p>
<p>The task is successfully completed if the action sequence delivers the agent close to an intended goal location $v^{*}$ (refer to Section 4.4 for evaluation details).</p>
<h3>4.2. Data Collection</h3>
<p>To generate navigation data, we use the Matterport3D region annotations to sample start pose $s_0$ and goal location $v^<em>$ pairs that are (predominantly) in different rooms. For each pair, we find the shortest path $v_0 : v^</em>$ in the relevant weighted, undirected navigation graph $G$, discarding paths that are shorter than 5m, and paths that contain less than four or more than six edges. In total we sample 7,189 paths capturing most of the visual diversity in the dataset. The average path length is 10m, as illustrated in Figure 5.</p>
<p>For each path, we collect three associated navigation instructions using Amazon Mechanical Turk (AMT). To this end, we provide workers with an interactive 3D WebGL environment depicting the path from the start location to the goal location using colored markers. Workers can interact with the trajectory as a 'fly-through', or pan and tilt the camera at any viewpoint along the path for additional context. We then ask workers to 'write directions so that a smart robot can find the goal location after starting from the same start location'. Workers are further instructed that it is not necessary to follow exactly the path indicated, merely to reach the goal. A video demonstration is also provided.</p>
<p>The full collection interface (which is included as supplementary material) was the result of several rounds of experimentation. We used only US-based AMT workers, screened according to their performance on previous tasks. Over 400 workers participated in the data collection, contributing around 1,600 hours of annotation time.</p>
<h3>4.3. R2R Dataset Analysis</h3>
<p>In total, we collected 21,567 navigation instructions with an average length of 29 words. This is considerably longer than visual question answering datasets where most questions range from four to ten words [4]. However, given the focused nature of the task, the instruction vocabulary is relatively constrained, consisting of around 3.1k words (approximately 1.2k with five or more mentions). As illustrated by the examples included in Figure 4, the level of abstraction in instructions varies widely. This likely reflects differences in people's mental models of the way a 'smart robot' works [43], making the handling of these differences an important aspect of the task. The distribution of navigation instructions based on their first words is depicted in Figure 6. Although we use the R2R dataset in conjunction with the Matterport3D Simulator, we see no technical reason why this dataset couldn't also be used with other simulators based on the Matterport dataset [11].</p>
<h3>4.4. Evaluation Protocol</h3>
<p>One of the strengths of the R2R task is that, in contrast to many other vision and language tasks such as image captioning and visual dialog, success is clearly measurable. We define <em>navigation error</em> as the shortest path distance in the navigation graph $G$ between the agent's final position $v_T$.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6. Distribution of navigation instructions based on their first four words. Instructions are read from the center outwards. Arc lengths are proportional to the number of instructions containing each word. White areas represent words with individual contributions too small to show.</p>
<p>(i.e., disregarding heading and elevation) and the goal location <em>v</em>. We consider an episode to be a <em>success</em> if the navigation error is less than 3m. This threshold allows for a margin of error of approximately one viewpoint, yet it is comfortably below the minimum starting error of 5m. We do not evaluate the agent's entire trajectory as many instructions do not specify the path that should be taken.</p>
<p>Central to our evaluation is the requirement for the agent to choose to end the episode when the goal location is identified. We consider stopping to be a fundamental aspect of completing the task, demonstrating understanding, but also freeing the agent to potentially undertake further tasks at the goal. However, we acknowledge that this requirement contrasts with recent works in vision-only navigation that do not train the agent to stop [62, 40]. To disentangle the problem of recognizing the goal location, we also report success for each agent under an <em>oracle</em> stopping rule, i.e. if the agent stopped at the closest point to the goal on its trajectory. Misra <em>et al</em>. [41] also use this evaluation.</p>
<h3>Dataset Splits</h3>
<p>We follow broadly the same train/val/test split strategy as the Matterport3D dataset [11]. The test set consists of 18 scenes, and 4,173 instructions. We reserve an additional 11 scenes and 2,349 instructions for validating in unseen environments (val unseen). The remaining 61 scenes are pooled together, with instructions split 14,025 train / 1,020 val seen. Following best practice, goal locations for the test set will not be released. Instead, we will provide an evaluation server where agent trajectories may be uploaded for scoring.</p>
<h2>5. Vision-and-Language Navigation Agents</h2>
<p>In this section, we describe a sequence-to-sequence neural network agent and several other baselines that we use to explore the difficulty of the R2R navigation task.</p>
<h3>5.1. Sequence-to-Sequence Model</h3>
<p>We model the agent with a recurrent neural network policy using an LSTM-based [23] sequence-to-sequence architecture with an attention mechanism [6]. Recall that the agent begins with a natural language instruction $\bar{x} = \langle x_1, x_2, \ldots, x_L \rangle$, and an initial image observation $o_0$. The encoder computes a representation of $\bar{x}$. At each step $t$, the decoder observes representations of the current image $o_t$ and the previous action $a_{t-1}$ as input, applies an attention mechanism to the hidden states of the language encoder, and predicts a distribution over the next action $a_t$. Using this approach, the decoder maintains an internal memory of the agent's entire preceding history, which is essential for navigating in a partially observable environment [56]. We discuss further details in the following sections.</p>
<h3>Language instruction encoding</h3>
<p>Each word $x_i$ in the language instruction is presented sequentially to the encoder LSTM as an embedding vector. We denote the output of the encoder at step $i$ as $h_i$, such that $h_i = \text{LSTM}<em i-1="i-1">{enc}(x_i, h</em> = {h_1, h_2, \ldots, h_L}$ as the encoder context, which will be used in the attention mechanism. As with Sutskever })$. We denote $\hat{h<em>et al</em>. [49], we found it valuable to reverse the order of words in the input language instruction.</p>
<h3>Model action space</h3>
<p>The simulator action space is statedependent (refer Section 3.2.2), allowing agents to make fine-grained choices between different forward trajectories that are presented. However, in this initial work we simplify our model action space to 6 actions corresponding to left, right, up, down, forward and stop. The forward action is defined to always move to the reachable viewpoint that is closest to the centre of the agent's visual field. The left, right, up and down actions are defined to move the camera by 30 degrees.</p>
<h3>Image and action embedding</h3>
<p>For each image observation $o_t$, we use a ResNet-152 [22] CNN pretrained on ImageNet [46] to extract a mean-pooled feature vector. Analogously to the embedding of instruction words, an embedding is learned for each action. The encoded image and previous action features are then concatenated together to form a single vector $q_t$. The decoder LSTM operates as $h_t' = \text{LSTM}<em t-1="t-1">{dec}(q_t, h</em>)$.</p>
<h3>Action prediction with attention mechanism</h3>
<p>To predict a distribution over actions at step $t$, we first use an attention mechanism to identify the most relevant parts of the navigation instruction. This is achieved by using the global, general alignment function described by Luong <em>et al</em>. [34]</p>
<p>to compute an instruction context $c_{t}=f\left(h_{t}^{\prime}, \bar{h}\right)$. When then compute an attentional hidden state $\hat{h}<em c="c">{t}=\tanh \left(W</em>\right)$. Although visual attention has also proved highly beneficial in vision and language problems $[60,33,3]$, we leave an investigation of visual attention in Vision-and-Language Navigation to future work.}\left[c_{t} ; h_{t}^{\prime}\right]\right)$, and calculate the predictive distribution over the next action as $a_{t}=\operatorname{softmax}\left(\hat{h}_{t</p>
<h3>5.2 Training</h3>
<p>We investigate two training regimes, 'teacher-forcing' and 'student-forcing'. In both cases, we use cross entropy loss at each step to maximize the likelihood of the ground-truth target action $a_{t}^{<em>}$ given the previous state-action sequence $\left\langle s_{0}, a_{0}, s_{1}, a_{1}, \ldots, s_{t}\right\rangle$. The target output action $a_{t}^{</em>}$ is always defined as the next action in the groundtruth shortest-path trajectory from the agent's current pose $s_{t}=\left\langle v_{t}, \psi_{t}, \theta_{t}\right\rangle$ to the target location $v^{*}$.</p>
<p>Under the 'teacher-forcing' [32] approach, at each step during training the ground-truth target action $a_{t}^{*}$ is selected, to be conditioned on for the prediction of later outputs. However, this limits exploration to only states that are in ground-truth shortest-path trajectory, resulting in a changing input distribution between training and testing [45, 32]. To address this limitation, we also investigate 'studentforcing'. In this approach, at each step the next action is sampled from the agent's output probability distribution. Student-forcing is equivalent to an online version of DAGGER [45], or the 'always sampling' approach in scheduled sampling [8].</p>
<p>Implementation Details We perform only minimal text pre-processing, converting all sentences to lower case, tokenizing on white space, and filtering words that do not occur at least five times. We set the simulator image resolution to $640 \times 480$ with a vertical field of view of 60 degrees. We set the number of hidden units in each LSTM to 512, the size of the input word embedding to 256, and the size of the input action embedding to 32. Embeddings are learned from random initialization. We use dropout of 0.5 on embeddings, CNN features and within the attention model.</p>
<p>As we have discretized the agent's heading and elevation changes in 30 degree increments, for fast training we extract and pre-cache all CNN feature vectors. We train in PyTorch using the Adam optimizer [28] with weight decay and a batch size of 100. In all cases we train for a fixed number of iterations. As the evaluation is single-shot, at test time we use greedy decoding [44]. Our test set submission is trained on all training and validation data.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Average R2R navigation results using evaluation metrics defined in Section 4.4. Our seq-2-seq model trained with studentforcing achieves promising results in previously explored environments (Val Seen). Generalization to previously unseen environments (Val Unseen / Test) is far more challenging.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Trajectory <br> Length (m)</th>
<th style="text-align: center;">Navigation <br> Error (m)</th>
<th style="text-align: center;">Success <br> (\%)</th>
<th style="text-align: center;">Oracle <br> Success (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Val Seen:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SHORTEST</td>
<td style="text-align: center;">10.19</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">RANDOM</td>
<td style="text-align: center;">9.58</td>
<td style="text-align: center;">9.45</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: left;">Teacher-forcing</td>
<td style="text-align: center;">10.95</td>
<td style="text-align: center;">8.01</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">36.7</td>
</tr>
<tr>
<td style="text-align: left;">Student-forcing</td>
<td style="text-align: center;">11.33</td>
<td style="text-align: center;">6.01</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">52.9</td>
</tr>
<tr>
<td style="text-align: left;">Val Unseen:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SHORTEST</td>
<td style="text-align: center;">9.48</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">RANDOM</td>
<td style="text-align: center;">9.77</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: left;">Teacher-forcing</td>
<td style="text-align: center;">10.67</td>
<td style="text-align: center;">8.61</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: left;">Student-forcing</td>
<td style="text-align: center;">8.39</td>
<td style="text-align: center;">7.81</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">28.4</td>
</tr>
<tr>
<td style="text-align: left;">Test (unseen):</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SHORTEST</td>
<td style="text-align: center;">9.93</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">RANDOM</td>
<td style="text-align: center;">9.93</td>
<td style="text-align: center;">9.77</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">18.3</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">11.90</td>
<td style="text-align: center;">1.61</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">90.2</td>
</tr>
<tr>
<td style="text-align: left;">Student-forcing</td>
<td style="text-align: center;">8.13</td>
<td style="text-align: center;">7.85</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">26.6</td>
</tr>
</tbody>
</table>
<h3>5.3 Additional Baselines</h3>
<p>Learning free We report two learning-free baselines which we denote as RANDOM and SHORTEST. The RANDOM agent exploits the characteristics of the dataset by turning to a randomly selected heading, then completing a total of 5 successful forward actions (when no forward action is available the agent selects right). The SHORTEST agent always follows the shortest path to the goal.</p>
<p>Human We quantify human performance by collecting human-generated trajectories for one third of the test set (1,390 instructions) using AMT. The collection procedure is similar to the dataset collection procedure described in Section 4.2, with two major differences. First, workers are provided with navigation instructions. Second, the entire scene environment is freely navigable in first-person by clicking on nearby viewpoints. In effect, workers are provided with the same information received by an agent in the simulator. To ensure a high standard, we paid workers bonuses for stopping within 3 m of the true goal location.</p>
<h2>6 Results</h2>
<p>As illustrated in Table 1, our exploitative RANDOM agent achieves an average success rate of $13.2 \%$ on the test set (which appears to be slightly more challenging than the validation sets). In comparison, AMT workers achieve $86.4 \%$ success on the test set, illustrating the high quality of the dataset instructions. Nevertheless, people are not infallible when it comes to navigation. For example, in the dataset we occasionally observe some confusion between right and</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7. Validation loss, navigation error and success rate during training. Our experiments suggest that neural network approaches can strongly overfit to training environments, even with regularization. This makes generalizing to unseen environments challenging.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8. In previously seen environments student-forcing training achieves 38.6% success (&lt; 3m navigation error).</p>
<p>left (although this is recoverable if the instructions contain enough visually-grounded references). In practice, people also use two additional mechanisms to reduce ambiguity that are not available here, namely gestures and dialog.</p>
<p>With regard to the sequence-to-sequence model, student-forcing is a more effective training regime than teacher-forcing, although it takes longer to train as it explores more of the environment. Both methods improve significantly over the RANDOM baseline, as illustrated in Figure 8. Using the student-forcing approach we establish the first test set leaderboard result achieving a 20.4% success rate.</p>
<p>The most surprising aspect of the results is the significant difference between performance in seen and unseen validation environments (38.6% vs. 21.8% success for student-forcing). To better explain these results, in Figure 7 we plot validation performance during training. Even using strong regularization (dropout and weight decay), performance in unseen environments plateaus quickly, but further training continues to improve performance in the training environments. This suggests that the visual groundings learned may be quite specific to the training environments.</p>
<p>Overall, the results illustrate the significant challenges involved in training agents that can generalize to perform well in previously unseen environments. The techniques and practices used to optimize performance on existing vision and language datasets are unlikely to be sufficient for models that are expected to operate in new environments.</p>
<h1>7. Conclusion and Future Work</h1>
<p>Vision-and-Language Navigation (VLN) is important because it represents a significant step towards capabilities critical for practical robotics. To further the investigation of VLN, in this paper we introduced the Matterport3D Simulator. This simulator achieves a unique and desirable tradeoff between reproducibility, interactivity, and visual realism. Leveraging these advantages, we collected the Room-to-Room (R2R) dataset. The R2R dataset is the first dataset to evaluate the capability to follow natural language navigation instructions in previously unseen real images at building scale. To explore this task we investigated several baselines and a sequence-to-sequence neural network agent.</p>
<p>From this work we reach three main conclusions. First, VLN is interesting because existing vision and language methods can be successfully applied. Second, the challenge of generalizing to previously unseen environments is significant. Third, crowd-sourced reconstructions of real locations are a highly-scalable and underutilized resource<sup>5</sup>. The process used to generate R2R is applicable to a host of related vision and language problems, particularly in robotics. We hope that this simulator will benefit the community by providing a visually-realistic framework to investigate VLN and related problems such as navigation instruction generation, embodied visual question answering, human-robot dialog, and domain transfer to real settings.</p>
<p>Acknowledgements This research is supported by a Facebook ParlAI Research Award, an Australian Government Research Training Program (RTP) Scholarship, the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016), and the Australian Research Council's Discovery Projects funding scheme (project DP160102156).</p>
<p><sup>5</sup>The existing Matterport3D data release constitutes just 90 out of more than 700,000 building scans that have been already been collected [37].</p>
<h2>References</h2>
<p>[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. 4
[2] P. Ammirato, P. Poirson, E. Park, J. Kosecka, and A. C. Berg. A dataset for developing and benchmarking active vision. In ICRA, 2017. 3
[3] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018. 7
[4] S. Antol, A. Ágrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. VQA: Visual question answering. In ICCV, 2015. 2, 4, 5
[5] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D-3D-Semantic Data for Indoor Scene Understanding. arXiv preprint arXiv:1702.01105, 2017. 4
[6] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In $I C L R$, 2015. 6
[7] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. Küttler, A. Lefrancq, S. Green, V. Valdés, A. Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. 1, 2, 3
[8] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS, 2015. 7
[9] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI gym. arXiv preprint arXiv:1606.01540, 2016. 4
[10] S. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti, F. Strub, J. Rouat, H. Larochelle, and A. Courville. HoME: A household multimodal environment. arXiv:1711.11017, 2017. 3
[11] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV), 2017. 1, 2, 3, 5, 6
[12] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and R. Salakhutdinov. Gated-attention architectures for task-oriented language grounding. arXiv preprint arXiv:1706.07230, 2017. 1, 2, 3
[13] D. L. Chen and R. J. Mooney. Learning to interpret natural language navigation instructions from observations. In AAAI, 2011. 1, 2
[14] X. Chen, T.-Y. L. Hao Fang, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server. arXiv preprint arXiv:1504.00325, 2015. 2, 4
[15] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 3
[16] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Embodied Question Answering. In CVPR, 2018. 2, 4
[17] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. F. Moura, D. Parikh, and D. Batra. Visual dialog. In CVPR, 2017. 2
[18] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. IQA: Visual question answering in interactive environments. In CVPR, 2018. 2, 4
[19] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In CVPR, 2017. 2
[20] S. Guadarrama, L. Riano, D. Golland, D. Go, Y. Jia, D. Klein, P. Abbeel, T. Darrell, et al. Grounding spatial relations for human-robot interaction. In IROS, 2013. 1, 2
[21] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for visual navigation. In CVPR, 2017. 3
[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 6
[23] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 1997. 6
[24] A. S. Huang, S. Tellex, A. Bachrach, T. Kollar, D. Roy, and N. Roy. Natural language command of an autonomous micro-air vehicle. In IROS, 2010. 2
[25] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014. 4
[26] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg. Referit game: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 2
[27] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jaśkowski. ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games, 2016. 1, 2, 3
[28] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 7
[29] T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions. In Human-Robot Interaction (HRI), 2010 5th ACM/IEEE International Conference on, pages 259-266. IEEE, 2010. 2
[30] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. AI2-THOR: An interactive 3d environment for visual AI. arXiv:1712.05474, 2017. 3
[31] T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman. Deep successor reinforcement learning. arXiv preprint arXiv:1606.02396, 2016. 3
[32] A. M. Lamb, A. G. A. P. GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio. Professor forcing: A new algorithm for training recurrent networks. In NIPS, 2016. 7
[33] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical question-image co-attention for visual question answering. In NIPS, 2016. 7
[34] M.-T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural machine translation. In EMNLP, 2014. 6
[35] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006. 2</p>
<p>[36] J. Mao, H. Jonathan, A. Toshev, O. Camburu, A. Yuille, and K. Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016. 2
[37] Matterport. Press release, October 2017. 8
[38] H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI, 2016. 1, 2
[39] A. H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bordes, D. Parikh, and J. Weston. Parlai: A dialog research software platform. arXiv preprint arXiv:1705.06476, 2017. 4
[40] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al. Learning to navigate in complex environments. In $I C L R, 2017.6$
[41] D. K. Misra, J. Langford, and Y. Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017. 1, 3, 6
[42] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 3
[43] D. A. Norman. The Design of Everyday Things. Basic Books, Inc., New York, NY, USA, 2002. 5
[44] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel. Self-critical sequence training for image captioning. In CVPR, 2017. 7
[45] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011. 7
[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge. IJCV, 2015. 6
[47] M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun. MINOS: Multimodal indoor simulator for navigation in complex environments. arXiv:1712.03931, 2017. 3,4
[48] S. Song, S. P. Lichtenberg, and J. Xiao. SUN RGB-D: A rgb-d scene understanding benchmark suite. In CVPR, 2015. 3
[49] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014. 6
[50] L. Tai and M. Liu. Towards cognitive exploration through deep reinforcement learning for mobile robots. arXiv preprint arXiv:1610.01733, 2016. 3
[51] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urtasun, and S. Fidler. MovieQA: Understanding stories in movies through question-answering. In CVPR, 2016. 2
[52] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI, 2011. 1, 2
[53] C. Tessler, S. Givony, T. Zahavy, D. J. Mankowitz, and S. Mannor. A deep hierarchical approach to lifelong learning in minecraft. In AAAI, pages 1553-1561, 2017. 3
[54] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR, 2011. 4
[55] A. Vogel and D. Jurafsky. Learning to follow navigational directions. In ACL, 2010. 2
[56] D. Wierstra, A. Foerster, J. Peters, and J. Schmidhuber. Solving deep memory pomdps with recurrent policy gradients. In International Conference on Artificial Neural Networks, 2007. 6
[57] T. Winograd. Procedures as a representation for data in a computer program for understanding natural language. Technical report, Massachusetts Institute of Technology, 1971. 2
[58] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building generalizable agents with a realistic and rich 3d environment. arXiv:1801.02209, 2018. 3
[59] C. Yan, D. Misra, A. Bennnett, A. Walsman, Y. Bisk, and Y. Artzi. CHALET: Cornell house agent learning environment. arXiv:1801.07357, 2018. 3
[60] Z. Yang, X. He, J. Gao, L. Deng, and A. J. Smola. Stacked attention networks for image question answering. In CVPR, 2016. 7
[61] A. R. Zamir, F. Xia, J. He, S. Sax, J. Malik, and S. Savarese. Gibson Env: Real-world perception for embodied agents. In CVPR, 2018. 3
[62] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. FeiFei, and A. Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017. 1, 2, 3, 4, 6</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9. Snapshot of the visual diversity in the Matterport3D dataset, illustrating one randomly selected panoramic viewpoint per scene.</p>
<p>You will see a series of panoramic photos taken while moving from a start location to a goal location in a building. Your task is to write directions so that a smart robot can find the goal location after starting from the same start location. The robot understands language and recognizes objects about as well as a typical person. However, you should assume that the robot is visiting this building for the first time.</p>
<p>For your reference, the path to the goal is indicated by color-coded markers (green for start, red for goal, and blue for intermediate markers).</p>
<ul>
<li>You won't see the green start marker at the beginning - because it's under your feet.</li>
<li>You may not see the red goal marker until you move (often the goal is in the next room).</li>
<li>These markers are not visible to the robot, and should not be mentioned in your directions.</li>
</ul>
<p>Good directions will ensure that the robot arrives within a few metres of the red goal marker. Therefore, we suggest:</p>
<ul>
<li>NEW! Spelling and punctuation is important. Please use full sentences with punctuation (..) and correct spelling.</li>
<li>Focus on the goal, not the path. It's not necessary for the robot to follow the exact path indicated by the markers.</li>
<li>Try to mention objects or landmarks. This is clearer than saying 'turn slight left' or 'go forward'.</li>
</ul>
<p>Mouse Controls:</p>
<ol>
<li>Left-click and drag the panoramic image to look around.</li>
<li>Right-click on a color-coded marker to move to that position.</li>
<li>Press the 'Play / Replay' button at any time to watch a 15-20 second animated fly-through from the start to the goal.</li>
</ol>
<p>Before you start, please watch this short training video. It contains examples that will help you complete these tasks efficiently.</p>
<p>Note: This task is not suitable for devices with small screens or touch screen devices. Recommended browsers are Chrome, Firefox and Safari (not Internet Explorer).
These tasks relate to academic research conducted by Peter Anderson through the Australian Centre for Robotic Vision, Brisbane, Australia. We estimate that on average each HIT to take around 1-1.5 minutes to complete. Please send your queries and feedback to bringmeaspoon@gmail. We will be continually releasing more HITs for this task.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Left-click and drag the panoramic image to start. Instructions have been updated from the first batch (please re-read).</p>
<p>Play / Replay</p>
<p>Write your Directions here (with correct spelling and punctuation):
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 10. AMT data collection interface for the R2R navigation dataset. Here, blue markers can be seen indicating the trajectory to the goal location. However, in many cases the worker must first look around (pan and tilt) to find the markers. Clicking on a marker moves the camera to that location. Workers can also watch a 'fly-through' of the complete trajectory by clicking the Play / Replay button.</p>
<p>Go past the ovens and the counter and wait just before you go outside.
Walk through the kitchen towards the living room. Walk around the island and step onto the patio near the two chairs and stop in the patio doorway.
Exit the kitchen by walking past the ovens and then head right, stopping just at the doorway leading to the patio outside.
Go up the last few stairs and turn right. Go up the next two flights of stairs and wait.
Walk up the rest of the stairs, then continue up the next set of stairs. Stop at the top of the stairs near the potted plant.
Go up the stairs then turn right and go up the other stairs on the right then turn right and go up the other stairs on the right and stop at the top of the stairs.</p>
<p>Walk until your in the next room. Make a right into the room on the right. Stop in front of the water heater.
Go across the room opposite the brown door, make a sharp right turn, and take a step into the laundry room and stop.
Exit the room. Turn right and then right again into the room next door. Wait there.
Turn right and enter the bedroom. Cross the bedroom and turn right and stop at the door leading out of the bedroom.
From shower room enter bedroom, walk across bedroom to hall and stop at window.
Exit the bathroom toward the bedroom. Exit the bedroom using the door on the right.
Turn around a the blackboard, make a left at the water fountain and head through the doorframe. Angle left and move straight, keeping the table with the white tablecloth on your left side. Make a slight right and walk straight, waiting at the bottom of the stairwell.
Walk towards the water dispenser and exit the doorway to the left. Walk straight left of the white circular table and towards the wooden staircase.
Walk out of the bathroom, turn left, and wait at the bottom of the stairs.
Walk toward the bed. When you get to the bed. Turn right and exit the room. Continue straight and enter the room straight ahead. Wait near the sink.
Turn to the left and enter the bedroom. Once inside, turn right and walk straight ahead and stop when you enter the bathroom.
Exit the bathroom, then turn left. Wait in the office next to the desk.
Turn around and go up the stairs, turn right and go to right again towards the front door.
Make your way up to the steps and then pull a hard right followed by another hard right after three steps. then continue until you've reached the first open door and stop.
Walk up the stairs and turn hard right. Stop in the bathroom doorway on the left.
Turn and enter the living room area. Go past the table and sofas and stop in the foyer in front of the front door.
Turn around and exit the room. Walk around the sofa and enter the hallway. Wait by the side table.
Exit the room through the doorway nearest you, and continue into the adjacent room, exiting the room via the exit to your left.
Turn right towards kitchen. Go into hallway and walk into dining room.
walk through the archway with the thermostat on the wall. Walk toward the piano and stop just before it.
Turn toward the kitchen, and walk through the doorway to the right of the breakfast bar. Walk down the hall, passing the bathroom on your right side as you walk. Walk straight and stop when you get to the piano.</p>
<p>Walk along the insulated bare walls towards the window ahead in the next room. Walk through the unfinished room and through the door on the other side of the room that leads to a finished hallway. Walk into the first open door in the hall that leads to a bedroom with photo art on the wall near the entrance of classic black and white scenes.
Walk forward past the window then turn right and enter the hallway. Enter the first bedroom on your right. wait near the bed.
Walk forward and take a right. Enter the hallway through the door on the right. Take the first left into a bedroom. Stop once you are in the bedroom.</p>
<p>Table 2. Examples of randomly selected R2R navigation instructions. Each cell contains three instructions associated with the same path.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Scheduled sampling has been shown to improve performance on tasks for which it is difficult to exactly determine the best next target output $a_{t}^{*}$ for an arbitrary preceding sequence (e.g. language generation [8]). However, in our task we can easily determine the shortest trajectory to the goal location from anywhere, and we found in initial experiments that scheduled sampling performed worse than student-forcing (i.e., always sampling).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
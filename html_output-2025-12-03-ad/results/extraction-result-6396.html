<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6396 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6396</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6396</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-276781977</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14395v2.pdf" target="_blank">PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we study whether domain specific pretraining of small generative language models (SLM) from scratch with domain specialized tokenizer and Chain-of-Thought (CoT) instruction fine-tuning results in competitive performance on mathematical reasoning compared to LLMs? Secondly, whether this approach is environmentally sustainable, highly cost efficient? To address these research questions, we present Paramanu-Ganita, a 208 million-parameter novel decoder-only Auto Regressive SLM on mathematics. We performed pretraining from scratch on 31.5 billion tokens for 170 A100 hours using a context size of 4096 on a mixed mathematical corpus consisting of web pages, source code, textbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture notes in LaTeX curated by us. We also trained a math and code specialised BPE tokenizer. We proposed and performed CoT instruction fine-tuning of Paramanu-Ganita on the MetaMathQA dataset. Our model Paramanu-Ganita, despite being 34 times smaller than the 7B LLMs, outperforms generalist LLMs by approximately 30% points, and even math-specialised LLMs by 3-23% points in GSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the various models by 6-8% points. On benchmarks like LogiQA, MMLU (high school, college level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math), Paramanu-Ganita outperformed others by 1-4%. Our model is available at https://huggingface.co/gyanai/paramanu-ganita-208M-hf .</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6396.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6396.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paramanu-Ganita</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PARAMANU-GANITA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 208M-parameter decoder-only autoregressive language model pretrained from scratch on a curated 31.5B-token mathematics-and-code corpus with a specialized BPE tokenizer and Chain-of-Thought instruction finetuning to improve multi-step quantitative reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Paramanu-Ganita</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>208M</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Domain-specialized corpus mixing mathematical web text, textbooks, LaTeX lecture notes, mathematical source code (AlgebraStack), AutoMathText web data (filtered by quality score), and templatised Chain-of-Thought QA pairs from StackExchange; pretraining total ≈31.5B tokens, context size 4096.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MATH; LogiQA; MMLU-math (high-school and college); AGIEVAL-AQuA-RAT; AGIEVAL-SAT-Math</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step grade-school and competition math word problems, high-school competition math problems, and multiple-choice quantitative reasoning (GRE/GMAT/SAT); logical reasoning (LogiQA).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems and multiple-choice questions; Chain-of-Thought style solution generations with final answer extraction (string following 'The answer is:').</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school (GSM8K, 2–8 step problems); high-school/competition level (MATH); college-level math subset (MMLU); competitive exam level (AGIEVAL subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought instruction fine-tuning (MetaMathQA) with prepended prompt 'Let's think step by step'; evaluation prompt for GSM8K: '... ### Q:{question} ### A: Let's think step by step. The answer is:'; for multiple-choice MCQs zero-shot greedy decoding via lm-eval-harness. Decoding settings for GSM8K/MATH: vLLM engine, best_of=8, temperature=0.1, top_p=1, max_tokens=1024, specified stop tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (final-answer extraction as the string after 'The answer is:'); LogiQA reports normalized accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 36.77% accuracy; MATH: 10.34% accuracy; LogiQA: 30.57 (normalized accuracy); MMLU-math-high-school: 31.11% accuracy; MMLU-math-college: 29.00% accuracy; AGIEVAL-AQuA-RAT: 26.77% accuracy; AGIEVAL-SAT-Math: 25.00% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Tokenizer and input design: trained specialized BPE tokenizers for math and code, merged them; during pre-tokenization digits are split into individual tokens and unknown UTF-8 chars fallback to byte granularity (authors state this was done to improve arithmetic learning ability). Reported pretraining perplexity 4.349 and Model FLOPs Utilization (MFU) 40.392. No mechanistic interpretability (e.g., attention probing, logit lens, activation-level analyses) of numeric processing is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No systematic failure-mode analysis reported. The paper does not provide error taxonomy (e.g., off-by-one, digit swapping). Authors note limited CoT finetuning (2 epochs) due to compute constraints and suggest longer finetuning might improve results, but do not analyze specific arithmetic failure patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Authors report that a small (208M) domain-specialized model pretrained from scratch and CoT finetuned can match or outperform many larger generalist and math-specialized LLMs (several 7B models and some larger models) on math benchmarks, arguing domain-focused data and tokenizer yield favorable sample-efficiency; no systematic scaling curve (performance vs model size) is provided within the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6396.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6396.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLEMMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLEMMA (7B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A math-specialized LLM (7B) referenced as a continuously pre-trained model on mathematical text and code (Proof-Pile-2), reported in the literature as intended to leverage code and tool usage for math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLEMMA 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (LLM, referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Mentioned as continuously pre-trained on Proof-Pile-2 combining mathematical texts and code (per cited LLEMMA work); specific training details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems / quantitative reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems; in LLEMMA's description includes ability to use Python/interpreters per citation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school and competition-level math as per GSM8K and MATH datasets</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Referenced approaches include continual pretraining and tooling (ability to use interpreters); specific prompting in this paper not used (paper reports published scores).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 36.40% accuracy; MATH: 18.00% accuracy (values cited from table in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Mentioned as an example where large-scale continual pretraining (23,000 A100 hours cited for a related math continual-pretraining effort) did not necessarily yield higher GSM8K score than the small domain-specific model in this paper; no internal scaling analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6396.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6396.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minerva-62B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minerva (62B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of LLMs trained and evaluated for quantitative reasoning; reported here via literature-cited benchmark scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Minerva 62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (LLM, referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language CoT-style solutions (as reported in Minerva literature); here only benchmark numbers are quoted.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to competition-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought prompting is commonly used for Minerva in cited literature; this paper only reports published scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 52.40% accuracy; MATH: 27.60% accuracy (values cited from table in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Minerva models (larger variants) show higher GSM8K accuracy in the cited table; paper contrasts such large models with the small domain-specific model but does not analyze scaling trends itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6396.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6396.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minerva-540B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minerva (540B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large Minerva variant reported in literature and cited here for comparison on math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Minerva 540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (LLM, referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language CoT-style solutions (reported elsewhere); only benchmark numbers quoted here.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to competition-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought prompting typically used (cited work); not performed by authors of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 58.80% accuracy; MATH: 33.60% accuracy (values cited from table in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Included to illustrate large-model performance; paper notes some very large models outperform Paramanu-Ganita, but also emphasizes that Paramanu-Ganita beats many 7B models despite being much smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6396.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6396.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-62B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM (62B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer language model family (PaLM) cited here with reported scores on math benchmarks for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts (literature-cited); only results are quoted in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to competition-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought or instruction prompts in PaLM literature; this paper only lists published scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 33.00% accuracy; MATH: 4.40% accuracy (values cited from table in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Cited to compare large-model performance; no in-paper analysis beyond numerical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 2)</em></li>
                <li>LLEMMA: An open language model for mathematics <em>(Rating: 2)</em></li>
                <li>Metamath: Bootstrap your own mathematical questions for large language models <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6396",
    "paper_id": "paper-276781977",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Paramanu-Ganita",
            "name_full": "PARAMANU-GANITA",
            "brief_description": "A 208M-parameter decoder-only autoregressive language model pretrained from scratch on a curated 31.5B-token mathematics-and-code corpus with a specialized BPE tokenizer and Chain-of-Thought instruction finetuning to improve multi-step quantitative reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Paramanu-Ganita",
            "model_family": "decoder-only transformer",
            "model_size": "208M",
            "training_data_description": "Domain-specialized corpus mixing mathematical web text, textbooks, LaTeX lecture notes, mathematical source code (AlgebraStack), AutoMathText web data (filtered by quality score), and templatised Chain-of-Thought QA pairs from StackExchange; pretraining total ≈31.5B tokens, context size 4096.",
            "benchmark_name": "GSM8K; MATH; LogiQA; MMLU-math (high-school and college); AGIEVAL-AQuA-RAT; AGIEVAL-SAT-Math",
            "task_type": "Multi-step grade-school and competition math word problems, high-school competition math problems, and multiple-choice quantitative reasoning (GRE/GMAT/SAT); logical reasoning (LogiQA).",
            "problem_format": "Natural-language word problems and multiple-choice questions; Chain-of-Thought style solution generations with final answer extraction (string following 'The answer is:').",
            "difficulty_level": "Grade-school (GSM8K, 2–8 step problems); high-school/competition level (MATH); college-level math subset (MMLU); competitive exam level (AGIEVAL subsets).",
            "prompting_method": "Chain-of-Thought instruction fine-tuning (MetaMathQA) with prepended prompt 'Let's think step by step'; evaluation prompt for GSM8K: '... ### Q:{question} ### A: Let's think step by step. The answer is:'; for multiple-choice MCQs zero-shot greedy decoding via lm-eval-harness. Decoding settings for GSM8K/MATH: vLLM engine, best_of=8, temperature=0.1, top_p=1, max_tokens=1024, specified stop tokens.",
            "performance_metric": "Accuracy (final-answer extraction as the string after 'The answer is:'); LogiQA reports normalized accuracy.",
            "performance_value": "GSM8K: 36.77% accuracy; MATH: 10.34% accuracy; LogiQA: 30.57 (normalized accuracy); MMLU-math-high-school: 31.11% accuracy; MMLU-math-college: 29.00% accuracy; AGIEVAL-AQuA-RAT: 26.77% accuracy; AGIEVAL-SAT-Math: 25.00% accuracy.",
            "internal_analysis": "Tokenizer and input design: trained specialized BPE tokenizers for math and code, merged them; during pre-tokenization digits are split into individual tokens and unknown UTF-8 chars fallback to byte granularity (authors state this was done to improve arithmetic learning ability). Reported pretraining perplexity 4.349 and Model FLOPs Utilization (MFU) 40.392. No mechanistic interpretability (e.g., attention probing, logit lens, activation-level analyses) of numeric processing is presented.",
            "failure_modes": "No systematic failure-mode analysis reported. The paper does not provide error taxonomy (e.g., off-by-one, digit swapping). Authors note limited CoT finetuning (2 epochs) due to compute constraints and suggest longer finetuning might improve results, but do not analyze specific arithmetic failure patterns.",
            "scaling_trend": "Authors report that a small (208M) domain-specialized model pretrained from scratch and CoT finetuned can match or outperform many larger generalist and math-specialized LLMs (several 7B models and some larger models) on math benchmarks, arguing domain-focused data and tokenizer yield favorable sample-efficiency; no systematic scaling curve (performance vs model size) is provided within the paper.",
            "uuid": "e6396.0",
            "source_info": {
                "paper_title": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLEMMA-7B",
            "name_full": "LLEMMA (7B variant)",
            "brief_description": "A math-specialized LLM (7B) referenced as a continuously pre-trained model on mathematical text and code (Proof-Pile-2), reported in the literature as intended to leverage code and tool usage for math reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLEMMA 7B",
            "model_family": "transformer (LLM, referenced)",
            "model_size": "7B",
            "training_data_description": "Mentioned as continuously pre-trained on Proof-Pile-2 combining mathematical texts and code (per cited LLEMMA work); specific training details not provided in this paper.",
            "benchmark_name": "GSM8K; MATH",
            "task_type": "Multi-step math word problems / quantitative reasoning",
            "problem_format": "Natural-language word problems; in LLEMMA's description includes ability to use Python/interpreters per citation",
            "difficulty_level": "Grade-school and competition-level math as per GSM8K and MATH datasets",
            "prompting_method": "Referenced approaches include continual pretraining and tooling (ability to use interpreters); specific prompting in this paper not used (paper reports published scores).",
            "performance_metric": "Accuracy",
            "performance_value": "GSM8K: 36.40% accuracy; MATH: 18.00% accuracy (values cited from table in this paper).",
            "internal_analysis": null,
            "failure_modes": null,
            "scaling_trend": "Mentioned as an example where large-scale continual pretraining (23,000 A100 hours cited for a related math continual-pretraining effort) did not necessarily yield higher GSM8K score than the small domain-specific model in this paper; no internal scaling analysis provided here.",
            "uuid": "e6396.1",
            "source_info": {
                "paper_title": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Minerva-62B",
            "name_full": "Minerva (62B variant)",
            "brief_description": "A family of LLMs trained and evaluated for quantitative reasoning; reported here via literature-cited benchmark scores.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Minerva 62B",
            "model_family": "transformer (LLM, referenced)",
            "model_size": "62B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MATH",
            "task_type": "Multi-step math word problems",
            "problem_format": "Natural-language CoT-style solutions (as reported in Minerva literature); here only benchmark numbers are quoted.",
            "difficulty_level": "Grade-school to competition-level",
            "prompting_method": "Chain-of-Thought prompting is commonly used for Minerva in cited literature; this paper only reports published scores.",
            "performance_metric": "Accuracy",
            "performance_value": "GSM8K: 52.40% accuracy; MATH: 27.60% accuracy (values cited from table in this paper).",
            "internal_analysis": null,
            "failure_modes": null,
            "scaling_trend": "Minerva models (larger variants) show higher GSM8K accuracy in the cited table; paper contrasts such large models with the small domain-specific model but does not analyze scaling trends itself.",
            "uuid": "e6396.2",
            "source_info": {
                "paper_title": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Minerva-540B",
            "name_full": "Minerva (540B variant)",
            "brief_description": "A very large Minerva variant reported in literature and cited here for comparison on math benchmarks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Minerva 540B",
            "model_family": "transformer (LLM, referenced)",
            "model_size": "540B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MATH",
            "task_type": "Multi-step math word problems",
            "problem_format": "Natural-language CoT-style solutions (reported elsewhere); only benchmark numbers quoted here.",
            "difficulty_level": "Grade-school to competition-level",
            "prompting_method": "Chain-of-Thought prompting typically used (cited work); not performed by authors of this paper.",
            "performance_metric": "Accuracy",
            "performance_value": "GSM8K: 58.80% accuracy; MATH: 33.60% accuracy (values cited from table in this paper).",
            "internal_analysis": null,
            "failure_modes": null,
            "scaling_trend": "Included to illustrate large-model performance; paper notes some very large models outperform Paramanu-Ganita, but also emphasizes that Paramanu-Ganita beats many 7B models despite being much smaller.",
            "uuid": "e6396.3",
            "source_info": {
                "paper_title": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PaLM-62B",
            "name_full": "PaLM (62B variant)",
            "brief_description": "A large transformer language model family (PaLM) cited here with reported scores on math benchmarks for comparison.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PaLM 62B",
            "model_family": "transformer (referenced)",
            "model_size": "62B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MATH",
            "task_type": "Multi-step math word problems",
            "problem_format": "Natural-language prompts (literature-cited); only results are quoted in this paper.",
            "difficulty_level": "Grade-school to competition-level",
            "prompting_method": "Chain-of-Thought or instruction prompts in PaLM literature; this paper only lists published scores.",
            "performance_metric": "Accuracy",
            "performance_value": "GSM8K: 33.00% accuracy; MATH: 4.40% accuracy (values cited from table in this paper).",
            "internal_analysis": null,
            "failure_modes": null,
            "scaling_trend": "Cited to compare large-model performance; no in-paper analysis beyond numerical comparisons.",
            "uuid": "e6396.4",
            "source_info": {
                "paper_title": "PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "LLEMMA: An open language model for mathematics",
            "rating": 2,
            "sanitized_title": "llemma_an_open_language_model_for_mathematics"
        },
        {
            "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models",
            "rating": 2,
            "sanitized_title": "metamath_bootstrap_your_own_mathematical_questions_for_large_language_models"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 1,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        }
    ],
    "cost": 0.014742249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?</p>
<p>Mitodru Niyogi mitodru.niyogi@grenoble-inp.fr 
Gyan Ai Research 
Arnab Bhattacharya arnabb@cse.iitk.ac.in </p>
<p>LIG, Grenoble INP
CNRS
Univ. Grenoble Alpes
GrenobleFrance</p>
<p>Dept. of Computer Science &amp; Engineering
Indian Institute of Technology Kanpur
KanpurIndia</p>
<p>PARAMANU-GANITA: Can Small Math Language Models Rival with Large Language Models on Mathematical Reasoning?
10573A7244DD347247D24C8295295A1D
In this paper, we study whether domain specific pretraining of small generative language models (SLM) from scratch with domain specialized tokenizer and Chain-of-Thought (CoT) instruction fine-tuning results in competitive performance on mathematical reasoning compared to LLMs?Secondly, whether this approach is environmentally sustainable, highly cost efficient?To address these research questions, we present PARAMANU-GANITA, a 208 million-parameter novel decoder-only Auto Regressive SLM on mathematics.We performed pretraining from scratch on 31.5 billion tokens for 170 A100 hours using a context size of 4096 on a mixed mathematical corpus consisting of web pages, source code, textbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture notes in L A T E X curated by us.We also trained a math and code specialised BPE tokenizer.We proposed and performed CoT instruction fine-tuning of Paramanu-Ganita on the MetaMathQA dataset.Our model Paramanu-Ganita, despite being 34 times smaller than the 7B LLMs, outperforms generalist LLMs by approximately 30% points, and even math-specialised LLMs by 3-23% points in GSM8K test accuracy metric.On MATH benchmark, Paramanu-Ganita outperformed the various models by 6-8% points.On benchmarks like LogiQA, MMLU (high school, college level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math), Paramanu-Ganita outperformed others by 1-4%.Our model is available at https://huggingface.co/gyanai/paramanuganita-208M-hf.</p>
<p>Introduction</p>
<p>Pretrained Large Language Models (LLMs) such as LLaMa (Touvron et al., 2023a), LLaMa-2, (Touvron et al., 2023b), PaLM (Chowdhery et al., 2023), Falcon (Almazrouei et al., 2023), Code LlaMa Work done at Gyan AI Research (Rozière et al., 2024), MPT (MosaicAI, 2023), etc. have demonstrated multi-dimensional abilities, such as in open-ended dialogue or instruction following capabilities (Ouyang et al., 2022).Being typically generalist language models balancing the performance across the entire distribution of natural language tasks.However, these generalist models are humongous in size and requires millions of dollars to train aside from high engineering inference cost involved.Traditionally, to optimize performance within specific domains such as finance (Wu et al., 2023), medicine (Singhal et al., 2023), etc., these models have been continually trained on domain specific data.However, domain specific continual pretraining of LLMs are also very expensive as a lot of computation and inference costs are involved along with high requirement of GPUs.For example, to improve the mathematical reasoning capabilities of LLMs, LLEMMA 7B (Azerbayev et al., 2024) was trained on 256 A100 40GB GPUs for roughly 23,000 A100 training hours, which is extremely expensive.</p>
<p>We consider here that a language model is large (LLM) if it contains more than 1B parameters and small if it contains less than 300M parameters (SLM) 1 .In this paper, we search for a alternative approach to continual pretraining of LLMs for improving mathematical reasoning of LLMs like LLEMMA and cost-effective training and inference method.In particular, we try to answer the two following research questions.</p>
<p>RQ1: Is domain specific pretraining from scratch of small generative language model with domain specialised tokenizer and Chain-of-Thought (CoT) instruction fine-tuning results in competitive performance on mathematical reasoning compared to LLMs which are trained on trillion of tokens and humongous in size on the assumption that "Larger models trained on trillion tokens can only reason" parameters?</p>
<p>RQ2: Is domain specific pretraining from scratch of SLM is environmentally sustainable, highly cost efficient for both training and inference?</p>
<p>To answer these questions, instead of following the domain adaptation method of LLMs for better mathematical reasoning, we focused on pretraining from scratch a generative mathematical language model using only a high quality mathematical corpus curated by us.This avoids requiring immense compute power, high engineering maneuver and techniques to load LLMs in memory, and mostly high cost of training, and the misalignment of domain specialised tokenizers and embeddings with the existing embeddings of LLMs via continual pretraining with vocabulary expansion of the existing LLMs tokenizers.We trained a powerful mathematical SLM from scratch which required only 146 hours of A100 training and additional 14 hours for Chain-of-Thought (CoT) instruction finetuning.We call our model PARAMANU-GANITA 2 .Our model is based on the Transformer Decoder architecture (Vaswani et al., 2017).We have trained an auto-regressive model from scratch at a context size of 4096 on a single NVidia A100-PCIE-40GB GPU.Our models are small in size, having only 208 million parameters.Hence, our models are very fast in inference without requiring any quantization of weights, and our mathematics model inference can be run on CPU without need of GPU.</p>
<p>To test the mathematical problem solving ability of our SLM, Paramanu-Ganita, we evaluated and compared with generalist LLMs, code LLM, and math specialized LLMs across variety of grade level difficulty benchmarks including both discriminative multiple-choice math benchmarks across competitive exams (SAT, GRE, GMAT), graduate level, high school and grade level level math questions.We also tested our model on a logical reasoning benchmark (LogiQA).Table 2 and Table 3 show the comparison of Paramanu-Ganita and LLMs on various mathematical benchmarks.Although SLM, Paramanu-Ganita, still outperformed math specialised LLM like LLEMMA 7B on GSM8K (Cobbe et al., 2021) benchmark despite being 35 times smaller in size.On the memory requirements, the LLEMMA 7B checkpoint size is 13.5 GB whereas our model's checkpoint size is 2 Paramanu means "atom" while Ganita is "mathematics" 2.5 GB and less than 1 GB in binary format (.bin).We found that our approach is highly cost efficient as we only spent on total 170 A100 hours including both pretraining from scratch and CoT fine-tuning, making our approach to be highly cost efficient, very competitive performance wrt LLMs, and least carbon footprint compared to LLMs or even math domain specialized LLM like LLEMMA which took 23,000 A100 hours for continual pretraining of Llama 2 and yet its performance (36.40%) is lower than our model, Paramanu-Ganita (36.77%) on GSM8K.Therefore, with our novel approach, we cut down the training cost by 135 times without compromising the performance of the model on mathematical benchmarks compared to both generalist and math specialized LLMs.</p>
<p>Our main contributions in this work are as follows:</p>
<ol>
<li>
<p>We have curated a pretraining corpus for mathematics with high quality mathematical text from various public sources and in-house university lecture notes in L A T E X, textbooks, web crawled mathematical text, mathematical source code from various programming languages (AlgebraStack), and Chain-of-Thought (CoT) (Wei et al., 2023) templatised mathematical question answers pairs from forums like StackExchange.A part of our dataset is available at https://huggingface.co/datasets/gyanai/ganita 2. We have developed a specialised tokenizer from scratch for mathematics domain and code.</p>
</li>
<li>
<p>We build a 208M parameter, decoder-only mathematical SLM, pretrained from scratch on the above corpus for 31.5 billion tokens at a context size of 4096.We have also performed Chain-of-Thought (CoT) instruction fine-tuning of our model on MetaMathQA (Yu et al., 2024)  The remainder of the paper is organized as follows: Section 2 describes the related work, Section 3 presents the data used for pretraining, and Sections 4 and 5 the model and training procedure retained.The evaluation of our model and analysis are given in Section 6, and Section 7; Section 8 concludes the paper.</p>
</li>
</ol>
<p>Related Work</p>
<p>Mathematical reasoning plays a crucial role in artificial intelligence, enabling the comprehension and resolution of intricate mathematical challenges.The incorporation of LLMs in this area has been substantial, thanks to their capability to interpret, process, and produce complex natural language.In artificial intelligence, math problem solving involves utilizing algorithms, computational models, and use of increasingly LLMs to understand, explain, and resolve mathematical challenges.This method encompasses a wide range of topics, from basic arithmetic to advanced mathematics, including areas such as algebra, geometry, statistics, and calculus.(Wei et al., 2023) boosts the reasoning capacity of LLMs by supplementing the output with a series of intermediate steps leading to the answer.Several approaches have been suggested to enhance the quality of these reasoning paths.For instance, complexity-based CoT (Fu et al., 2023) picks examples with more steps as incontext demonstrations, demonstrating that prompting with additional reasoning steps improves performance.Self-consistency (Wang et al., 2023b) generates multiple reasoning paths and selects the final answer through majority voting.Another set of techniques involves fine-tuning-based methods, which adapt open-source models (like LLaMA) using insights from advanced closed-source LLMs (GPT-4, GPT-3.5-Turbo).(Magister et al., 2023) explore the transfer of reasoning abilities through knowledge distillation.(Yuan et al., 2024) advocate for the use of rejection sampling fine-tuning (RFT) to enhance mathematical reasoning performance.WizardMath (Xu et al., 2024) introduces a reinforced evol-instruct method for strengthening reasoning abilities through supervised finetuning and PPO training (Schulman et al., 2017).MAmmoTH (Yue et al., 2024) integrates CoT and Program-of-Thought (Chen et al., 2023) rationales to teach LLMs how to utilize external tools (such as a Python interpreter) for solving mathematical problems.(Wang et al., 2023a) propose a constraint alignment loss for fine-tuning LLMs to improve calibration.Going beyond the improvement of mathematical abilities through fine-tuning, LLEMMA (Azerbayev et al., 2024) introduces the Proof-Pile-2 dataset, which combines mathematical texts and code.By continuously pre-training with Code Llama, the model is equipped to utilize Python interpreters and formal theorem provers, showcasing remarkable performance on the MATH benchmark.</p>
<p>Data</p>
<p>We followed past works ((MA et al., 2024), (Razeghi et al., 2024), (Aryabumi et al., 2024)) that suggest that source code with text in the pretraining corpus improves the general and mathematical reasoning abilities of generative language models.Thus, we mixed source code related to mathematical problems along with open source mathematical web corpus and clubbed it with our curated lecture notes, and templatised mathematical questions answers in the pretraining dataset.Our pretraining dataset is a set of selected corpus from various publicly available datasets (AlgebraStack (Azerbayev et al., 2024), MathPile Commercial (Wang et al., 2023c), AutoMathText (Zhang et al., 2024), and Chain-of-Thought (CoT) templatised Stack-Overflow math, physics, statistics question answers (Zhang, 2024) and in-house collection of mathematical lecture notes in L A T E X. AutoMathText from AutoDS (Zhang et al., 2024) is a comprehensive and meticulously curated dataset that contains approximately 200 GB of mathematical texts.It is compiled from a variety of sources, including websites, arXiv, and GitHub (OpenWebMath, RedPajama, Algebraic Stack).This extensive dataset has been autonomously selected as zero-shot verifier and labeled by the advanced open-source language model, Qwen-72B.Each item in the dataset is assigned a score, lm_q1q2_score, ranging from [0, 1], which indicates its relevance, quality, and educational value in the field of mathematical intelligence.</p>
<p>For our pretraining data, we select only the web corpus from AutoMathText where the lm_q1q2_score ≥ 0.6.We selected textbooks, proofofwiki, wikipedia, and stackexchange subsets from MathPile Commercial dataset.We selected the source code from AlgebraStack.Finally, the pretraining corpus is composed of mathematical text from web, source code related to mathematical reasoning, one million question-answers pairs  1 shows the number of words in our pretraining corpus.We used the following CoT template for templatising the StackOverflow and StackExchange questions answers as part of our pretraining corpus."Below is an instruction that describes a task.Write a response that appropriately completes the request.### Q:{question} ### A: Let's think step by step.The answer is: {answer}"</p>
<p>Data Contamination Removal</p>
<p>Following (Kocetkov et al., 2023), we removed duplicates and near-duplicates from the training data using (Mou et al., 2023), with default parameters.Following (Guo et al., 2024), we ran the data decontamination check in order to remove data contamination in the pretraining corpus from the various benchmark evaluations that we performed to test the performance of our math SLM.The filtering criterion is as follows: any text segment containing a 8-gram string that matches exactly with any sub-string from the evaluation benchmarks is removed from our pretraining corpus.For benchmark texts that are shorter than 8 grams but have at least 2 grams, we employ exact matching to filter out contaminated examples.This decontamination process leads us to remove around 170,346,325 words.Finally, the pretraining corpus has 5,578,762,486 words in the corpus.</p>
<p>Model Design</p>
<p>The model architecture of Paramanu-Ganita is based on Transformer decoder-only architecture.It uses RMSNorm (Zhang and Sennrich, 2019) as pre-normalizaion layer with norm_epsilon = 1e-5, and SwiGLU (Shazeer, 2020) activation function in the feed-forward dense layers.The model, Paramanu-Ganita, uses multi-head attention (MHA).The hidden dimension is 1024 with 16 layers, n_k_v_heads=16, and 16 attention heads with feedforward layer hidden dimension of 2752.Following (Chowdhery et al., 2023), we remove all biases from dense layers to improve the training stability of Paramanu-Ganita.</p>
<p>Tokenization We trained two separate Byte-Pair encoding (BPE) (Sennrich et al., 2016) tokenizers using Sentencepiece (Kudo and Richardson, 2018) module on the pretraining data from scratch to develop mathematical domain specialised tokenizer to learn the intricacies of mathematical terminology.One BPE tokenizer is trained on Alge-braStack (mathematical source code corpus) and another BPE tokenizer is trained on the mathematical text, lecture notes, and StackOverflow question answers.During pre-tokenization, NFC normalization was performed on the processed data, digits are split into individual tokens and fall back unknown UTF-8 characters to byte granularity for improving the arithmetic learning ability of the pretrained model.We treat our data as a sequence of bytes rather than Unicode characters, and we include each of the 256 bytes as tokens.We then merged the both mathematical tokenizer and code tokenizer by intersection by removing the duplicate tokens to develop our final tokenizer specialised in mathematics and code, compact, optimized, and effective.Tokenizer has special tokens like "〈Q:〉", "〈A:〉", "〈tex〉", "〈/tex〉", "〈python〉", "〈/python〉", "〈c〉", "〈/c〉", "〈matlab〉", "〈/matlab〉" "〈haskell 〉", "〈/haskell〉".</p>
<p>Training of SLM</p>
<p>Pre-training</p>
<p>We have pretrained our math SLM, Paramanu-Ganita, from scratch at a context size of 4096 on our curated corpus.However, we have excluded training of our model on ArXiv math papers as we believe that to learn basic mathematical concepts, and acquire mathematical logical reasoning, ArXiv math papers are not required as they generally meant to serve beyond high school level mathematics.We started with simple strategy to use a part of our curated corpus which generally covers various mathematical and logical concepts till secondary school education in general.We performed mix pretraining combining both mathematical plain text, source code of programming languages, and templatised mathematical question answers pairs in the pretraining phase.For pretraining Paramanu-Ganita (4096 context size), we performed 95%-5% data split for pretraining.The perplexity of our model is 4.349 while the MFU is 40.392.</p>
<p>We performed hyperparameter tuning on 15M models to find the optimal vocabulary size, learning rate, learning rate scheduler, and warm-up ratio.We used a batch size of 8, gradient accumulation steps of 8, and the maximum sequence length set to 4096, i.e., 262,144 tokens per iteration.We used the concept of µ transfer, and transferred the learned hyperparameters to our bigger model for 208M Paramanu-Ganita from 15M model.Following (Hoffmann et al., 2022b), we set lr decay steps to max_steps and the minimum lr is set nearly to 0.1•lr.The lr schedule starts with a linear warm-up from 0 to the maximum lr at 1000 steps, followed by a cosine decay to the minimum lr until the max_steps = 120,000 end of an epoch of training.We used the following equation for lr decay ratio.lr decay_ratio = t − warmup steps lr decay_steps − warmup steps where t is the current training step.We set maximum learning rate (lr) to 3e-3 (max), weight decay to 1e-1.To further speedup training, we used BF16 mixed precision training.For our experiments and modeling, we implemented our code using Pytorch 2.0, in-house optmized CUDA kernels and used torch.compilefeature for every model.We can see from Figure 1 how the loss is converging with incremental training steps and pretraining tokens, confirming a good pretraining with minor loss spikes.Paramanu-Ganita 208M is pretrained on around a total of 31.5 billion tokens.</p>
<p>Chain-of-Thought Instruction Fine-tuning</p>
<p>We performed Chain-of-Thought instruction finetuning on the MetaMathQA (Yu et al., 2024) instructions dataset, i.e, instead of regular instruction fine-tuning, we prepend the response of the Meta-MathQA dataset by a prompt "Let's think step by step", and then used the prepended instruction, and response pair for instruction fine-tuning.We finetuned for 2 epochs due to limited computational resources.We used cosine learning rate scheduler with (lr) set to 2e-5 with gradient clipping of 1.0, warmup ratio of 0.05 and no weight decay.However, we believe our instruction-tuned Paramanu-Ganita would have performed better in benchmark evaluation if it was further fine-tuned for another 2-3 epochs.We used the following training prompt for MetaMathQA for our model.</p>
<p>"Below is an instruction that describes a task.Write a response that appropriately completes the request.### Q:{query} ### A: Let's think step by step.{response}"</p>
<p>Evaluation</p>
<p>In this section, we present results of our model Paramanu-Ganita against different LLMs, both general and code LLMs as well as math-specialized, on various benchmarks.We evaluated across variety of grade level of difficulty benchmarks including both discriminative multiple-choice math benchmarks across grade school level, high school level, college level, competitive exams level of SAT, GRE, GMAT, and math competition level questions.We also tested our model on a logical reasoning benchmark (LogiQA).</p>
<p>RQ1: GSM8K and MATH benchmark datasets</p>
<p>We evaluate the model's ability to solve mathematics problems using chain of thought reasoning.Our evaluations include GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), which are the standard benchmarks for evaluating quantitative reasoning in language models.GSM8K includes 8,500 high-quality grade school math problems created by human writers.These problems generally consist of 2 to 8 steps to solve and mainly involve a series of basic arithmetic calculations to arrive at the final answer.The MATH dataset consists of 12,500 problems taken from high school math competitions.Each problem includes a step-by-step solution, allowing models to learn how to generate answer derivations and explanations.We used the following evaluation prompt for GSM8K test set for our math model."Below is an instruction that describes a task.Write a response that appropriately completes the request.### Q:{question} ### A: Let's think step by step.The answer is: "</p>
<p>We used the vLLM (Kwon et al., 2023) inference engine and used the code from (Yu et al., 2024) for evaluation on GSM8K and MATH benchmarks.The following stop tokens were used while decoding from the model during evaluation.stop=['Question:', 'Question', 'USER:', 'USER', 'ASSISTANT:', 'ASSIS-TANT', 'Instruction:', 'Instruction', 'Response:', 'Response'] We set the vLLM inference engine parameters: best_of=8, presence_penalty=0.0,fre-quency_penalty=0.0, repetition_penalty=1.0, tem-perature=0.1,top_p=1, top_k=-1, min_p=0.0,length_penalty=1.0,max_tokens=1024 while decoding from Paramanu-Ganita for evaluation on GSM8K and Math test benchmarks.Answer extraction differs from the method used by (Wei et al., 2023) who rely on complex string rules to derive the final answer.In contrast, we follow the approach of WizardMath (Luo et al., 2023) by only extracting the string that follows "The answer is:" as the final answer.To train the model on this extraction technique, we append "The answer is: gold answer" to the end of the answers in the Meta-MathQA dataset, replacing the gold answer with the corresponding answer for each question.</p>
<p>We report accuracy of Paramanu-Ganita and other models in Table 2.The scores of these models are reproduced as-is from their respective publications.</p>
<p>RQ1: Multiple-choice Math QA benchmark datasets</p>
<p>We evaluate our model and compare with LLMs including general LLMs, math-specialized LLMs like LLEMMA, and code LLMs like CodeLlama on various multiple choice math question answers using lm-eval-harness (Sutawika et al., 2024) at zero-shot greedy decoding setting.We considered high school and college math MCQ question answers from MMLU (Hendrycks et al., 2021a), AGIEVAL-AQuA-RAT (GRE, GMAT 254 multiple-choice math questions taken from AQuA-RAT (Ling et al., 2017)) (Zhong et al., 2024), and AGIEVAL-SAT-Math (SAT 220 multiple-choice math questions).Only the 3 giant LLMs, namely, LLEMMA 34B, Minerva 62B, Minerva 540B, performed better than Paramanu-Ganita on the GSM8K benchmark.</p>
<p>On the MATH benchmark as shown in the Table 2, Paramanu-Ganita outperformed LLaMa-1 7B by 7.44%, Llama-1 13B by 6.44% points, Llama-2 7B by 7.84% points, Llama-2 13B by 6.44% points, Falcon 7B by 8.04% points, Falcon 40B by 7.84% points, MPT 30B by 7.24% points, MPT 30B by 7.24% points, PaLM 8B by 8.84% points, and PaLM 62B by 5.94% points respectively.GPT-J and Vicuna did not report numbers for the MATH benchmark.</p>
<p>As shown in the  (Chowdhery et al., 2023), LLaMa-1 (Touvron et al., 2023a), LLaMa-2 (Touvron et al., 2023b), Falcon (Almazrouei et al., 2023), Code LlaMa (Rozière et al., 2024), MPT (MosaicAI, 2023), Vicuna (Chiang et al., 2023), Minerva (Lewkowycz et al., 2022), MAmooTH-CoT (Yue et al., 2024), MetaMath (Yu et al., 2024), WizardMath (Luo et al., 2023), LLEMMA (Azerbayev et al., 2024) scores are quoted from respective author papers.</p>
<p>formed Llama-2 7B, OLMo 1B by 3.76% points, LLEMMA 7B, Falcon 7B by 3.69% points.On mathematical high school questions (MMLUmath-high-school) benchmark as shown in the Table 3, Paramanu-Ganita outperformed Llama-2 7B by 5.56% points, CodeLlama 7B by 6.3% points, OLMo 1B, and Falcon 7B by 10% points but LLEMMA 7B outperformed Ganita by 1% point despite being 34 times larger in size.</p>
<p>On college level math questions (MMLU-mathcollege) benchmark as shown in the Table 3, Paramanu-Ganita 208M outperformed Falcon 7B by 8% points, OLMo 1B by 2% points, whereas both Llama-2 7B and CodeLlama 7B outperformed Paramanu-Ganita just by 1% point despite being 34 times larger.</p>
<p>On GRE-GMAT level quantitative questions (AGIEVAL-AQuA-RAT) benchmark as shown in Table 3, Paramanu-Ganita outperformed all the LLMs under comparison, i.e., Falcon 7B by 4.73% points, LLEMMA 7B by 3.55% points, OLMo 1B by 3.15% points, CodeLlama 7B by 3.94% points, and Llama-2 7B by 1.18% point respectively.</p>
<p>At SAT level math questions (AGIEVAL-SAR-Math) benchmark as listed in Table 3, Paramanu-Ganita outperformed Llama-2 7B, and OLMo 1B while lagging behind LLEMMA 7B by 7.72% points.</p>
<p>Despite being 35 times smaller in size, the performance of our model, Paramanu-Ganita, is comparable with the other LLMs.We believe the domain specific pretraining from scratch using high quality mathematical corpus of lecture notes, source code, web scrapped mathematical text and our Chain-of-Thought (CoT) templated formatted StackOverflow math, physics, statistics question answers along with our novel merged math and code specialized BPE tokenizer, and CoT instruction fine-tuning are the most probable causes to amplify the performance of strong mathematical reasoning in a small generative language model of 208 million parameters compared to LLMs which are pretrained on all kinds of data whereas we focused only on mathematical and source code related to mathematics, mathematical question answers in COT template in our pretraining corpus.</p>
<p>Conclusions and Future Work</p>
<p>In this paper, we explore the performance of small math language model compared to both generalist and math specialised LLMs.Instead of continual pretraining of LLMs and then applying various fine-tuning approaches to improve the reasoning of LLMs, we introduce an exclusive decoder-only math SLM, Paramanu-Ganita 208M, pretrained from scratch on a diverse mathematical corpus, including web text, textbooks, L A T E X lecture notes, programming code, and CoT templatised mathematical question-answer pairs.We fine-tuned Paramanu-Ganita using CoT instructions on the MetaMathQA dataset.Our model was evaluated across various benchmarks at the grade school, high school, college, and competitive exam levels (SAT, GRE, GMAT), and outperformed generalist LLMs and math-specific models, such as Minerva 8B and LLEMMA 7B, in accuracy on GSM8K and MATH benchmarks.Paramanu-Ganita also achieved competitive results on logical reasoning (LogiQA), (SAT, GRE, GMAT) math subsets of AGIEVAL, and math questions from MMLU, outperforming or matching larger 7B LLMs.</p>
<p>Our extensive evaluation of Paramanu-Ganita and other LLMs on mathematical and logical reasoning tasks leads to the conclusion that a small, domain-specific language model, when trained from scratch with a specialized tokenizer, is a more cost-effective and environmentally friendly approach.With only 170 A100 training hours, Paramanu-Ganita offers a significant reduction in training costs, i.e., 135 times less than the continual pretraining of Llama for mathematical reasoning (Azerbayev et al., 2024) without sacrificing performance on key math benchmarks.This work challenges the "bigger is better" presumption, demonstrating the potential and effectiveness of creating small domain specific language models from scratch rather than relying on existing LLMs.</p>
<p>For future work, we plan to expand the pretraining corpus with ArXiv math papers and explore reinforcement learning alignment (e.g., DPO (Rafailov et al., 2024)) to further enhance our model's performance.</p>
<p>Ethics Statement</p>
<p>We have used results of the other models from their respective publications.We have trained and evaluated our model on a single GPU.Thus, we do not see any issues of ethics for this work.</p>
<p>Figure 1 :
1
Figure 1: Training loss against number of tokens in billion.(G = billion)</p>
<p>Table 3 compares Paramanu-Ganita with various LLMs.LogiQA (Liu et al., 2021) is a dataset created from various logical reasoning questions gathered from China's National Civil Servants Examination.Notably, LogiQA features bilingual questions in both English and Chinese, with the English version being a translation of the original Chinese text.We only considered the English version for evaluation.
7 Results and AnalysisFrom Table 2 on GSM8K benchmark, Paramanu-Ganita, despite being 35 times smaller than the7B family of LLMs, outperformed LLaMa-17B by 25.8% points, LLaMa-2 7B by 22.17%points, Falcon 7B by 29.97% points, PaLM 8Bby 32.67% points, Minerva 8B by 20.6% points,and LLEMMA-7B respectively. Paramanu-Ganitaalso outperformed PaLM 62B by 3.8% points de-spite being smaller by 305 times, Falcon 40B by17.2% points (smaller by 192 times), LLaMa-1 33Bby 1.2% points (smaller by 158 times), and Vicuna13B by 9.2% (smaller by 64 times). This is a sig-nificant achievement since smaller models are pre-ferred due to cost and environmental sustainability.</p>
<p>Table 2 :
2
Table 3 on LogiQA (Liu et al., 2021) benchmark, Paramanu-Ganita outper-Evaluation of LLMs on GSM8K test set.PaLM
ModelParameters GSM8K MATHLLaMa-17B11.002.90LLaMa-113B17.803.90LLaMa-133B35.603.90LLaMa-27B14.602.50LLaMa-2 3.90 Falcon 13B 28.70 40B 19.60 2.50Falcon7B6.802.30MPT30B15.203.10MPT7B6.803.00GPT-J6B34.90-Vicuna13B27.60-PaLM8B4.101.50PaLM62B33.004.40Minerva8B16.2014.10Minerva62B52.4027.60Minerva540B58.8033.60MAmooTH-CoT7B50.5010.40WizardMath7B54.9010.70MetaMath7B66.5019.80LLEMMA7B36.4018.00LLEMMA34B51.5025.00Paramanu-Ganita208M36.7710.34</p>
<p>Table 3 :
3
Zero-shot evaluation of Paramanu-Ganita 208M and LLMs.All benchmark reports Accuracy except LogiQA, which reports Normalized Accuracy.We present the best score across our model checkpoints for Paramanu-Ganita.B=billion, M=million.
ModelsLogiQA MMLU-math-MMLU-math-AGIEVAL-AGIEVAL-high-schoolcollege AQuA-RAT SAT-MathLlama-2 7B30.4125.5530.0025.5924.54CodeLlama-7B30.7224.8130.0022.8329.09OLMo 1B26.8130.3727.0023.6221.81LLEMMA 7B29.9532.2232.0023.2232.72Falcon 7B26.8821.1121.0022.0428.63Paramanu-Ganita 208M30.5731.1129.0026.7725.00
We refer to models containing more than 300M parameters and less than 1B parameters as Medium Language Models. We however do not consider them in this study.
AcknowledgementsThe first author wants to dedicate his work to his beloved parents, Rita Niyogi and Malay Niyogi for their outstanding support throughout his journey.Code Llama 7B 10.50 13.00 Code Llama 13B 36.10 16.40 Code Llama 34B 29.60 12.20A BackgroundA.1 Language ModelingThis objective of the language modeling can be formally described as maximizing the probability of a sequence of tokens w 1 , w 2 , . . ., w N P (w 1 , w 2 , . . ., w n ) = n i=1 P (w i | w 1 , w 2 , . . ., w i−1 ) where p(w t |w 0 , . . .w t−1 ) is the probability of token w t given the sequence of previous tokens w 0 , . . ., w t−1 .The performance of a language model is generally evaluated using the total cross-entropy loss, i.e, the negative log-likelihood of the observed data under the model under consideration, which for a given dataset is defined as:Lower the loss better is the model; however, just computing the loss may not be intuitive.Therefore, Perplexity is a metric to evaluate the performance of a given language model which is the exponent of the average loss.P erplexity = exp (Loss)A.2 Rotary Position Embedding (RoPE)Transformer-based models rely on positional embeddings to encode position and relative location information of words in a text.Rotary Position Embedding (RoPE) is a position encoding technique proposed by(Black et al., 2022).Instead of adding positional embeddings or relative positional embeddings to token embeddings, RoPE rotates the token embedding by a fixed factor (θ) in the higher-dimensional space to encode relative positional embeddings.In other words, RoPE encodes the absolute positions with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation.The intuition behind RoPE is that we can represent the token embeddings as complex numbers and their positions as pure rotations that we apply to them.If we shift both the query and key by the same amount, changing absolute position but not relative position, this will lead both representations to be additionally rotated in the same manner.Thus, the angle between them will remain unchanged and, thus, the dot product will also remain unchanged.By exploiting the nature of rotations, the dot product used in self-attention will have the property for preserving relative positional information while discarding absolute position.A.3 Model FLOPs Utilization (MFU)Model FLOPs Utilization (MFU)(Chowdhery et al., 2023)estimate is the ratio of the observed throughput (tokens-per-second) relative to the theoretical maximum throughput of a system at peak FLOPs.Model flops utilization (MFU) estimate the number of flops (floating point operations) done per iteration.It quantifies how efficiently the GPUs are utilized in model training.A.4 Maximal Update ParameterizationAs the size of large language models (LLMs) and the scale of the dataset used in pretraining are expensively large, it is not feasible to perform hyperparameter tuning in LLMs.Yang et al. (2021)used a technique called maximal update parameterization (µP ) to transfer the hyperparameters learnt from tuning of a small model to a larger model and found that the optimal hyperparameter values become stable across neural network sizes when the models have been parameterized using (µP ).A.5 Root Mean Square Normalization (RMSNorm)To improve the training stability, some LLMs (Chinchilla(Hoffmann et al., 2022a), LLaMa(Touvron et al., 2023a)) have normalized the input of each transformer sub-layer, instead of normalizing the output using RMSNorm normalizing function as introduced by(Zhang and Sennrich, 2019).RM-SNorm normalizes the activations based on their root mean square (RMS) value instead of normalizing the inputs based on their mean and variance.RMSNorm accelerates the training and inference with similar performance in these large models.It is reported that replacing LayerNorm(Ba et al., 2016)with RMSNorm can achieve comparable performance and improve training and inference time by 7-64%.Narang et al. (2021)showed that RMSNorm improves the pre-training speed by 5% compared with the LayerNorm baseline.
Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The falcon series of open language models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta</p>
<p>. Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, Sara Hooker, 2024To code, or not to code? exploring impact of code in pre-training</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, Llemma: An open language model for mathematics. 2024</p>
<p>Layer normalization. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E , 2016Hinton</p>
<p>GPT-NeoX-20B: An opensource autoregressive language model. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach, 10.18653/v1/2022.bigscience-1.9Proceedings of BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models. BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language ModelsAssociation for Computational Linguistics2022</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2023</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, Journal of Machine Learning Research. M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel242402023</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, 2023</p>
<p>. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming -the rise of code intelligence</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2021a</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021b</p>
<p>Oriol Vinyals, Jack Rae, and Laurent Sifre. 2022a. An empirical analysis of compute-optimal large language model training. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Thomas Clark, Eric Hennigan, Katherine Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karén Osindero, Simonyan, Advances in Neural Information Processing Systems. Erich ElsenCurran Associates, Inc35</p>
<p>. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre2022bTraining compute-optimal large language models</p>
<p>Leandro Von Werra, and Harm de Vries. 2023. The stack: 3 TB of permissively licensed source code. Denis Kocetkov, Raymond Li, Loubna Ben, L I Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Transactions on Machine Learning Research. </p>
<p>SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, 10.18653/v1/D18-2012Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2018 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, 10.1145/3600006.3613165Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23. the 29th Symposium on Operating Systems Principles, SOSP '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'202021</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023</p>
<p>At which training stage does code data help LLMs reasoning?. Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, Shanshan Li, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn, 10.18653/v1/2023.acl-short.151Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20232Short Papers)</p>
<p>Mosaicai, 10-03-2024Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs -databricks.com. 2023</p>
<p>Chenghao Mou, Chris Ha, Kenneth Enevoldsen, Peiyuan Liu, 10.5281/zenodo.8364980Chenghaomou/text-dedup: Reference snapshot. 2023</p>
<p>. Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Robertsand Colin Raffel. 2021. Do transformer modifications transfer across implementations and applications</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, 2024</p>
<p>BACKTRACKING MATHE-MATICAL REASONING OF LANGUAGE MOD-ELS TO THE PRETRAINING DATA. Yasaman Razeghi, Hamish Ivison, Sameer Singh, Yanai Elazar, 2024In The Second Tiny Papers Track at ICLR 2024</p>
<p>Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin. Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Romain Liu, Tal Sauvestre, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Xiong, Nicolas Usunier, Thomas Scialomand Gabriel Synnaeve. 2024. Code llama: Open foundation models for code</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal policy optimization algorithms. 2017</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1162Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany20161Association for Computational Linguistics</p>
<p>Noam Shazeer, Glu variants improve transformer. 2020</p>
<p>Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera Y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S Sara Mahdavi, Joelle Barral, Dale Webster, Greg S Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards expert-level medical question answering with large language models. </p>
<p>. Lintang Sutawika, Hailey Schoelkopf, Leo Gao, Stella Biderman, Baber Abbasi, Jonathan Tow, Charles Lovering, Jason Phang, Anish Thite, Fazz, Niklas Aflah, Thomas Muennighoff, Chris Wang, Julen Etxaniz, Zdeněk Kasner, Jeffrey Khalid, Hsu, 10.5281/zenodo.10600400Hanwool Albert Lee, Anjor Kanekar, AndyZwei, Pawan Sasanka Ammanamanchiand Dirk Groeneveld. 2024. Eleutherai/lmevaluation-harness: v0.4.1</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. </p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates Inc2017</p>
<p>Making large language models better reasoners with alignment. Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui, 2023a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023b</p>
<p>Zengzhi Wang, Rui Xia, Pengfei Liu, arXiv:2312.17120Generative ai for math: Part i -mathpile: A billion-tokenscale pretraining corpus for math. 2023carXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann, Bloomberggpt: A large language model for finance. 2023</p>
<p>WizardLM: Empowering large pre-trained language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, Daxin Jiang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Tuning large neural networks via zero-shot hyperparameter transfer. Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao, Advances in Neural Information Processing Systems. 2021</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Y U Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou, 2024</p>
<p>MAmmoTH: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Root mean square layer normalization. Biao Zhang, Rico Sennrich, 2019Curran Associates IncRed Hook, NY, USA</p>
<p>Stackmathqa: A curated collection of 2 million mathematical questions and answers sourced from stack exchange. Yifan Zhang, 2024</p>
<p>Automathtext: Autonomous data selection with language models for mathematical texts. Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao, ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models. 2024</p>
<p>AGIEval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 10.18653/v1/2024.findings-naacl.149Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, Mexico2024Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
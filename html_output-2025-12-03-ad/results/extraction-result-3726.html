<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3726 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3726</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3726</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-d7ac65d335b5d847f4f5826313a8732bc7abc7a8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d7ac65d335b5d847f4f5826313a8732bc7abc7a8" target="_blank">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work first estimates the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A", and then fits calibration parameters that cause the prediction for this input to be uniform across answers.</p>
                <p><strong>Paper Abstract:</strong> GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3726.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3726.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (Generative Pre-trained Transformer 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive transformer language model used in this paper in several sizes (2.7B, 13B, 175B) to produce next-token probability distributions for classification, fact retrieval, and information extraction tasks; probabilities are used for decision-making and are the target of a calibration procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (2.7B, 13B, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Left-to-right autoregressive transformer language model variants accessed via the OpenAI API; used in three sizes reported here (2.7B, 13B, 175B). The model outputs probability distributions over next tokens (only output probabilities available via API were used).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Compute model-assigned probabilities for next-token outputs which are mapped to class probabilities for classification (SST-2, TREC, AGNews, DBPedia, CB, RTE), single-token answers for fact retrieval (LAMA), and slot/span outputs for information extraction (ATIS, MIT Movies). The paper does not use GPT-3 to forecast future scientific discoveries or real-world events.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct in-context prompting (zero-/few-shot) to produce next-token probabilities; class probabilities obtained by renormalizing probabilities of label tokens; additionally, a contextual calibration method is applied (data-free) which estimates model bias by querying content-free inputs and rescales output probabilities via a diagonal affine transform W and bias b (practically W = diag(p_cf)^{-1}, b = 0).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Multiple NLP benchmarks: SST-2 (sentiment), TREC (question classification), AGNews and DBPedia (topic classification), CB and RTE (textual entailment), LAMA (fact retrieval templates), ATIS and MIT Movies (slot filling / information extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Main metrics: classification accuracy and standard deviation across prompt choices; generation tasks: top-1 accuracy for single-token answers; reported gains from contextual calibration up to 30.0 percentage points absolute improvement in mean accuracy on some tasks; examples include AGNews 175B 0-shot baseline 43.9 -> calibrated 73.9, and reported reductions in variance across prompt choices (figures and tables report mean ± std).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to standard greedy decoding without calibration (baseline). Contextual calibration consistently improves mean and worst-case accuracy across datasets and often reduces variance; in some cases a calibrated smaller GPT-3 (2.7B) outperforms an uncalibrated GPT-3 175B (up to 19.3% reported). No comparisons to human experts, prediction markets, or explicit forecasting systems are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The model exhibits majority-label bias, recency bias, and common-token bias that shift its output distribution; calibration requires access to output probabilities (not logits) and currently uses a diagonal (vector scaling) transform, which is limited; calibration is sensitive to the choice of content-free input (though several good options exist) and only calibrates the first-token distribution for generation tasks; calibration does not eliminate the need for prompt engineering. The paper contains no experiments where GPT-3 is used to assign probabilities to future scientific discoveries or real-world events.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>LLMs like GPT-3 produce context-dependent biased probability distributions in few-shot prompts; querying the model with content-free inputs reveals these biases, and a simple data-free contextual calibration (rescaling probabilities by the inverse of the content-free distribution) markedly improves classification and retrieval accuracy and reduces variance across prompt choices. The paper demonstrates these effects empirically across tasks and model sizes but does not study forecasting of future events.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3726.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3726.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contextual Calibration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual Calibration (data-free probability calibration using content-free inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that estimates and corrects context-dependent bias in LM output probabilities by querying the model with content-free inputs, computing the resulting class/token probability vector p_cf, and applying a diagonal affine transform (vector scaling) to rescale test-time probabilities so the content-free input would be uniform across labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applies to autoregressive LMs used in the paper (GPT-3 and GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method operates on models that produce next-token probability distributions (authors applied it to GPT-3 2.7B/13B/175B and GPT-2 1.5B). Implementation uses only model output probabilities (no gradient/finetuning) and operates by adjusting predicted probability vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Not a forecasting task per se; used to estimate and correct the model's probability assignment for classification labels, fact-retrieval single-token answers, and first-token generation distributions so downstream argmax/decisions become more accurate and less variable across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Data-free estimate of model bias: feed content-free inputs (e.g., "N/A", "[MASK]", empty string), average resulting probability vectors p_cf, set W = diag(p_cf)^{-1} and b = 0 (vector scaling) to transform any predicted probability vector p_hat before taking argmax; for generation tasks only the first-token distribution is calibrated. The paper also ensembles over three content-free inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on the same NLP benchmarks: SST-2, TREC, AGNews, DBPedia, CB, RTE, LAMA, ATIS, MIT Movies; calibration evaluated under a variety of prompt formats and example permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics: mean accuracy and standard deviation across different prompt example sets and formats; improvements up to 30.0 percentage points absolute in mean accuracy on some tasks; reductions in standard deviation across prompts (figures/tables quantify these reductions per dataset and shot count). The paper also compares to an 'oracle calibration' (best diagonal W found on validation) and finds contextual calibration is close to oracle performance on AGNews.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Baseline is uncalibrated greedy decoding (standard in-context prediction). Contextual calibration improves mean and worst-case accuracy relative to baseline across most tasks and settings, reduces variance across prompt choices, and sometimes eliminates the 0-shot to 1-shot drop caused by majority-label bias. Compared to oracle (validation-based) diagonal calibration, contextual calibration performs similarly on AGNews.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of content-free input matters (authors ensemble three selections but report variability across choices); only diagonal scaling is used (limits expressivity); requires access to output probabilities (not raw logits) and assumes a multiplicative/simple-shift bias model; only calibrates first token for generation tasks; does not remove all need for prompt engineering; not evaluated for open-ended forecasting of future events or scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>A simple data-free calibration computed from content-free queries substantially corrects contextual biases (majority, recency, common-token) of LMs, leading to large and consistent accuracy gains and lower variance across prompts; the technique is cheap to run and generalizes across model sizes and to GPT-2 as well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3726.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3726.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive transformer LM (1.5B parameters, GPT-2 XL used here) evaluated alongside GPT-3 to show that prompt-dependent probability bias and benefits of contextual calibration generalize beyond GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (1.5B / GPT-2 XL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Left-to-right transformer language model (GPT-2 family) used in experiments to confirm that variance across prompts and calibration benefits are not unique to GPT-3; probabilities were obtained via model outputs and calibrated with the same contextual calibration procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Same set of NLP prediction tasks as GPT-3: classification (SST-2, AGNews, DBPedia, etc.), LAMA fact retrieval, and information extraction (ATIS, MIT Movies). The authors did not use GPT-2 to forecast future real-world events or scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct in-context prompting (zero-/few-shot) producing next-token probabilities for class labels and single-token retrieval; contextual calibration method (content-free input -> vector scaling) applied identically to GPT-2 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on the same benchmarks as GPT-3: SST-2, AGNews, TREC, CB, RTE, DBPedia, LAMA, ATIS, MIT Movies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: accuracy and std-dev across prompt choices; contextual calibration improved GPT-2 mean accuracy and reduced variance for most tasks (Table 4 shows calibrated vs baseline numbers for GPT-2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to uncalibrated greedy decoding; contextual calibration improved mean accuracy and lowered variance for GPT-2 as it did for GPT-3. No comparisons to forecasting systems or human judgment for future events.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same biases (majority, recency, common-token) observed for GPT-2; calibration shares same limitations (choice of content-free input sensitivity, diagonal transform, first-token-only for generation). No experiments on estimating probabilities for future scientific discoveries or real-world forecasting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>The prompt-dependent biases and benefits of contextual calibration generalize to GPT-2, suggesting the observed phenomena are common across autoregressive LMs rather than unique to GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Calibrate Before Use: Improving Few-Shot Performance of Language Models', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>On calibration of modern neural networks <em>(Rating: 2)</em></li>
                <li>How neural language models use context (Sharp nearby, fuzzy far away) <em>(Rating: 1)</em></li>
                <li>How can we know what language models know? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3726",
    "paper_id": "paper-d7ac65d335b5d847f4f5826313a8732bc7abc7a8",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "GPT-3 (Generative Pre-trained Transformer 3)",
            "brief_description": "An autoregressive transformer language model used in this paper in several sizes (2.7B, 13B, 175B) to produce next-token probability distributions for classification, fact retrieval, and information extraction tasks; probabilities are used for decision-making and are the target of a calibration procedure.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (2.7B, 13B, 175B)",
            "model_description": "Left-to-right autoregressive transformer language model variants accessed via the OpenAI API; used in three sizes reported here (2.7B, 13B, 175B). The model outputs probability distributions over next tokens (only output probabilities available via API were used).",
            "prediction_task": "Compute model-assigned probabilities for next-token outputs which are mapped to class probabilities for classification (SST-2, TREC, AGNews, DBPedia, CB, RTE), single-token answers for fact retrieval (LAMA), and slot/span outputs for information extraction (ATIS, MIT Movies). The paper does not use GPT-3 to forecast future scientific discoveries or real-world events.",
            "method_of_probability_estimation": "Direct in-context prompting (zero-/few-shot) to produce next-token probabilities; class probabilities obtained by renormalizing probabilities of label tokens; additionally, a contextual calibration method is applied (data-free) which estimates model bias by querying content-free inputs and rescales output probabilities via a diagonal affine transform W and bias b (practically W = diag(p_cf)^{-1}, b = 0).",
            "dataset_or_benchmark": "Multiple NLP benchmarks: SST-2 (sentiment), TREC (question classification), AGNews and DBPedia (topic classification), CB and RTE (textual entailment), LAMA (fact retrieval templates), ATIS and MIT Movies (slot filling / information extraction).",
            "performance_metrics": "Main metrics: classification accuracy and standard deviation across prompt choices; generation tasks: top-1 accuracy for single-token answers; reported gains from contextual calibration up to 30.0 percentage points absolute improvement in mean accuracy on some tasks; examples include AGNews 175B 0-shot baseline 43.9 -&gt; calibrated 73.9, and reported reductions in variance across prompt choices (figures and tables report mean ± std).",
            "comparison_to_baselines": "Compared to standard greedy decoding without calibration (baseline). Contextual calibration consistently improves mean and worst-case accuracy across datasets and often reduces variance; in some cases a calibrated smaller GPT-3 (2.7B) outperforms an uncalibrated GPT-3 175B (up to 19.3% reported). No comparisons to human experts, prediction markets, or explicit forecasting systems are provided.",
            "limitations_or_challenges": "The model exhibits majority-label bias, recency bias, and common-token bias that shift its output distribution; calibration requires access to output probabilities (not logits) and currently uses a diagonal (vector scaling) transform, which is limited; calibration is sensitive to the choice of content-free input (though several good options exist) and only calibrates the first-token distribution for generation tasks; calibration does not eliminate the need for prompt engineering. The paper contains no experiments where GPT-3 is used to assign probabilities to future scientific discoveries or real-world events.",
            "notable_findings": "LLMs like GPT-3 produce context-dependent biased probability distributions in few-shot prompts; querying the model with content-free inputs reveals these biases, and a simple data-free contextual calibration (rescaling probabilities by the inverse of the content-free distribution) markedly improves classification and retrieval accuracy and reduces variance across prompt choices. The paper demonstrates these effects empirically across tasks and model sizes but does not study forecasting of future events.",
            "uuid": "e3726.0",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Contextual Calibration",
            "name_full": "Contextual Calibration (data-free probability calibration using content-free inputs)",
            "brief_description": "A method introduced in this paper that estimates and corrects context-dependent bias in LM output probabilities by querying the model with content-free inputs, computing the resulting class/token probability vector p_cf, and applying a diagonal affine transform (vector scaling) to rescale test-time probabilities so the content-free input would be uniform across labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applies to autoregressive LMs used in the paper (GPT-3 and GPT-2)",
            "model_description": "Method operates on models that produce next-token probability distributions (authors applied it to GPT-3 2.7B/13B/175B and GPT-2 1.5B). Implementation uses only model output probabilities (no gradient/finetuning) and operates by adjusting predicted probability vectors.",
            "prediction_task": "Not a forecasting task per se; used to estimate and correct the model's probability assignment for classification labels, fact-retrieval single-token answers, and first-token generation distributions so downstream argmax/decisions become more accurate and less variable across prompts.",
            "method_of_probability_estimation": "Data-free estimate of model bias: feed content-free inputs (e.g., \"N/A\", \"[MASK]\", empty string), average resulting probability vectors p_cf, set W = diag(p_cf)^{-1} and b = 0 (vector scaling) to transform any predicted probability vector p_hat before taking argmax; for generation tasks only the first-token distribution is calibrated. The paper also ensembles over three content-free inputs.",
            "dataset_or_benchmark": "Evaluated on the same NLP benchmarks: SST-2, TREC, AGNews, DBPedia, CB, RTE, LAMA, ATIS, MIT Movies; calibration evaluated under a variety of prompt formats and example permutations.",
            "performance_metrics": "Reported metrics: mean accuracy and standard deviation across different prompt example sets and formats; improvements up to 30.0 percentage points absolute in mean accuracy on some tasks; reductions in standard deviation across prompts (figures/tables quantify these reductions per dataset and shot count). The paper also compares to an 'oracle calibration' (best diagonal W found on validation) and finds contextual calibration is close to oracle performance on AGNews.",
            "comparison_to_baselines": "Baseline is uncalibrated greedy decoding (standard in-context prediction). Contextual calibration improves mean and worst-case accuracy relative to baseline across most tasks and settings, reduces variance across prompt choices, and sometimes eliminates the 0-shot to 1-shot drop caused by majority-label bias. Compared to oracle (validation-based) diagonal calibration, contextual calibration performs similarly on AGNews.",
            "limitations_or_challenges": "Choice of content-free input matters (authors ensemble three selections but report variability across choices); only diagonal scaling is used (limits expressivity); requires access to output probabilities (not raw logits) and assumes a multiplicative/simple-shift bias model; only calibrates first token for generation tasks; does not remove all need for prompt engineering; not evaluated for open-ended forecasting of future events or scientific discoveries.",
            "notable_findings": "A simple data-free calibration computed from content-free queries substantially corrects contextual biases (majority, recency, common-token) of LMs, leading to large and consistent accuracy gains and lower variance across prompts; the technique is cheap to run and generalizes across model sizes and to GPT-2 as well.",
            "uuid": "e3726.1",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "GPT-2",
            "name_full": "GPT-2 (Generative Pre-trained Transformer 2)",
            "brief_description": "An autoregressive transformer LM (1.5B parameters, GPT-2 XL used here) evaluated alongside GPT-3 to show that prompt-dependent probability bias and benefits of contextual calibration generalize beyond GPT-3.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (1.5B / GPT-2 XL)",
            "model_description": "Left-to-right transformer language model (GPT-2 family) used in experiments to confirm that variance across prompts and calibration benefits are not unique to GPT-3; probabilities were obtained via model outputs and calibrated with the same contextual calibration procedure.",
            "prediction_task": "Same set of NLP prediction tasks as GPT-3: classification (SST-2, AGNews, DBPedia, etc.), LAMA fact retrieval, and information extraction (ATIS, MIT Movies). The authors did not use GPT-2 to forecast future real-world events or scientific discoveries.",
            "method_of_probability_estimation": "Direct in-context prompting (zero-/few-shot) producing next-token probabilities for class labels and single-token retrieval; contextual calibration method (content-free input -&gt; vector scaling) applied identically to GPT-2 outputs.",
            "dataset_or_benchmark": "Evaluated on the same benchmarks as GPT-3: SST-2, AGNews, TREC, CB, RTE, DBPedia, LAMA, ATIS, MIT Movies.",
            "performance_metrics": "Reported: accuracy and std-dev across prompt choices; contextual calibration improved GPT-2 mean accuracy and reduced variance for most tasks (Table 4 shows calibrated vs baseline numbers for GPT-2).",
            "comparison_to_baselines": "Compared to uncalibrated greedy decoding; contextual calibration improved mean accuracy and lowered variance for GPT-2 as it did for GPT-3. No comparisons to forecasting systems or human judgment for future events.",
            "limitations_or_challenges": "Same biases (majority, recency, common-token) observed for GPT-2; calibration shares same limitations (choice of content-free input sensitivity, diagonal transform, first-token-only for generation). No experiments on estimating probabilities for future scientific discoveries or real-world forecasting tasks.",
            "notable_findings": "The prompt-dependent biases and benefits of contextual calibration generalize to GPT-2, suggesting the observed phenomena are common across autoregressive LMs rather than unique to GPT-3.",
            "uuid": "e3726.2",
            "source_info": {
                "paper_title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "publication_date_yy_mm": "2021-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "On calibration of modern neural networks",
            "rating": 2
        },
        {
            "paper_title": "How neural language models use context (Sharp nearby, fuzzy far away)",
            "rating": 1
        },
        {
            "paper_title": "How can we know what language models know?",
            "rating": 1
        }
    ],
    "cost": 0.012605499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Calibrate Before Use: Improving Few-Shot Performance of Language Models</h1>
<p>Tony Z. Zhao ${ }^{<em> 1}$ Eric Wallace ${ }^{</em> 1}$ Shi Feng ${ }^{2}$ Dan Klein ${ }^{1}$ Sameer Singh ${ }^{3}$</p>
<h4>Abstract</h4>
<p>GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pretraining data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to $30.0 \%$ absolute) and reduces variance across different choices of the prompt.</p>
<h2>1. Introduction</h2>
<p>Few-shot learning-the ability to learn tasks with limited examples-is an important aspect of intelligence (Lake et al., 2015; Yogatama et al., 2019). Recent work shows that large neural language models can perform few-shot learning without finetuning (Radford et al., 2019; Brown et al., 2020). Specifically, GPT-3 (Brown et al., 2020) can perform numerous tasks when provided a few examples in a natural language prompt. For example, to perform sentiment analysis one can condition GPT-3 on a prompt such as:</p>
<p>Input: Subpar acting. Sentiment: Negative Input: Beautiful film. Sentiment: Positive Input: Amazing. Sentiment:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>where the first two lines correspond to two training examples and the last line is a test example. To make predictions, the model predicts whether the subsequent token is more likely to be the word "Positive" or "Negative".</p>
<p>This style of few-shot "in-context" learning is interesting because it shows that the model can learn without parameter updates. And, more importantly, it has numerous practical advantages over the now-standard approach of finetuning (Radford et al., 2018; Devlin et al., 2019). First, it allows practitioners to "rapidly prototype" NLP models: changing the prompt immediately leads to a new model. Second, it provides a fully natural language interface to a machine learning model, which allows users-even those without technical expertise-to create NLP systems. Finally, since in-context learning reuses the same model for each task, it reduces memory requirements and system complexity when serving many different tasks.</p>
<p>However, despite these promises, we show that GPT-3's accuracy can be highly unstable across different prompts (Section 3). A prompt contains three components: a format, a set of training examples, and a permutation (ordering) for those examples. We show that different choices for these factors can lead to highly different accuracies, e.g., changing the permutation of the training examples in a sentiment analysis prompt can change accuracy from near chance (54\%) to near state-of-the-art (93\%). This instability implies that GPT-3 users, who typically design prompts manually, cannot expect to consistently obtain good accuracy.</p>
<p>We next analyze what causes this instability. We identify three pitfalls of language models that lead them to be biased toward certain answers during few-shot learning. In particular, they suffer from majority label bias, recency bias, and common token bias (Section 4). The majority label and recency biases lead the model to predict training answers that appear frequently or near the end of the prompt. For example, a prompt that ends with a Negative training example may cause a bias towards the Negative class. On the other hand, the common token bias leads the model to prefer answers that are frequent in its pre-training data, e.g., it prefers "United States" over "Saint Lucia", which is likely suboptimal for the task of interest.</p>
<p>We identify that these biases typically result in a shift in the output distribution of the model. We can thus coun-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Few-shot learning can be highly unstable across different choices of the prompt. Above, we plot the mean accuracy ( $\pm$ one standard deviation) across different choices of the training examples for three different datasets and model sizes. We show that our method, contextual calibration, improves accuracy, reduces variance, and overall makes tools like GPT-3 more effective for end users.
teract these biases by "calibrating" the output distribution. Concretely, we estimate the model's bias towards certain answers by feeding in a dummy test input that is content-free. In the prompt above for example, if we replace "Amazing." with the string "N/A", the model predicts $62 \%$ Positive. We then fit the calibration parameters so that the content-free input has uniform scores for each answer. This contextual calibration procedure provides a good setting of the calibration parameters without additional training data.</p>
<p>We test the effectiveness of contextual calibration on a range of tasks (Section 5). Contextual calibration consistently improves GPT-3 and GPT-2's accuracy (up to $30.0 \%$ absolute) across different choices of the prompt format and examples (e.g., Figure 1). It also makes the accuracy more stable across different prompts, thus mitigating the need for prompt engineering. Overall, contextual calibration is a simple method that makes language models better few-shot learners: it enables end users to obtain higher accuracy with considerably less effort.</p>
<h2>2. Background and Experimental Setup</h2>
<p>Neural autoregressive language models (LMs) take as input a sequence of tokens and output a probability distribution over the next token. Large neural LMs can perform tasks in a zero- or few-shot manner using in-context learning (Radford et al., 2019; Brown et al., 2020). To do so, a natural language prompt is fed into the model. This prompt contains three components: a format, a set of training examples, and a permutation (ordering) of the training examples.</p>
<p>Prompt Format The prompt format is a template which consists of placeholders for the training and test example(s) and possibly a natural language description of the task. For example, the format of the prompt in Section 1 is a template with the style: "Input:" input "Sentiment:" label. Many
alternate formats exist, e.g., one could frame the task as question answering.</p>
<p>Prompt Training Examples The prompt's training examples are used to teach the LM how to solve the task at hand. The prompt from Section 1 consists of two training examples; we refer to this as "two-shot" learning. We also consider "zero-shot" learning, where no training examples are present.</p>
<p>Training Example Permutation When training examples are used, they have a particular permutation, e.g., the "Subpar acting" example comes first in the prompt from Section 1. The permutation matters because neural language models update their hidden states in a left-to-right-fashion.</p>
<p>To make predictions on an input, we slot it into the test placeholder and generate from the LM. For example, see the "Amazing." test example in the prompt from Section 1. For generation tasks, we generate greedily from the LM until it produces a newline character. For classification tasks, the probability for each class is given by the probability assigned to its associated label name, e.g., the words "Negative" and "Positive" for sentiment classification.</p>
<h3>2.1. Datasets and Prompt Formats</h3>
<p>We use datasets for three tasks: text classification, fact retrieval, and information extraction. We use a fixed prompt format for each dataset unless otherwise specified. We show the format and examples from each dataset in Appendix B.</p>
<p>Text Classification We study text classification using six datasets: sentiment analysis using SST-2 (Socher et al., 2013), 6-way question classification using TREC (Voorhees \&amp; Tice, 2000), textual entailment using 3-way CB (de Marneffe et al., 2019) and binary RTE (Dagan et al., 2005) from SuperGLUE (Wang et al., 2019), and topic classification</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. There is high variance in GPT-3's accuracy as we change the prompt's training examples, as well as the permutation of the examples. Here, we select ten different sets of four SST-2 training examples. For each set of examples, we vary their permutation and plot GPT-3 2.7B's accuracy for each permutation (and its quartiles).
using the 4-way AGNews (Zhang et al., 2015) and 14-way DBPedia (Zhang et al., 2015) datasets. The prompt in Section 1 shows an example of the sentiment analysis task.</p>
<p>Fact Retrieval We evaluate fact retrieval with LAMA (Petroni et al., 2019). The dataset consists of knowledge base triples that are placed into templates with missing objects, e.g. "Obama was born in". We use these templates as our prompts, and remove the relations where the missing answer is not at the end of the template (left-to-right LMs cannot solve these). The answers are always single tokens, and we report average accuracy across all triples.</p>
<p>Information Extraction We consider information extraction using two slot filling datasets, ATIS (Hemphill et al., 1990) and MIT Movies trivial0k13 (Liu et al., 2012). We use two random slots for each dataset, airline and departure date for ATIS, and director name and movie genre for MIT Movies. The answer for both datasets is a span of text from the input, e.g., the ATIS airline task is to predict "american airlines" when given the sentence "list a flight on american airlines from toronto to san diego". We use Exact Match between the model's generated output and the ground-truth span as our evaluation metric.</p>
<h3>2.2. Model Details</h3>
<p>We run our experiments on three sizes of GPT-3 (2.7B, 13B, and 175B parameters) as well as GPT-2 (1.5B parameters). We access GPT-3 using the OpenAI API. We release code to replicate our experiments. ${ }^{1}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. There is high variance in GPT-3's accuracy as we change the prompt format. In this figure, we use ten different prompt formats for SST-2. For each format, we plot GPT-3 2.7B's accuracy for different sets of four training examples, along with the quartiles.</p>
<h2>3. Accuracy Varies Highly Across Prompts</h2>
<p>This section studies how GPT-3's accuracy changes as we vary each aspect of the prompt (training examples, permutation, format). We focus on a subset of the datasets to simplify our analysis; in Section 5 we show that our findings hold across all of the datasets we study.</p>
<p>GPT-3's accuracy depends highly on both selection and permutation of training examples. Concretely, we use a fixed prompt format and choose different random sets of training examples. For each set of training examples, we evaluate the accuracy for all possible permutations.</p>
<p>Figure 2 shows the results for SST-2 (4-shot, GPT-3 2.7B). Surprisingly, varying the permutation can be as important, or even more important, than which training examples are chosen. For example, varying the permutation of the training examples can cause accuracy to go from near chance (54.3\%) to near state-of-the-art (93.4\%). For a qualitative example of the sensitivity to permutations, see Table 2 in Appendix A. This high importance on example order is in contrast to standard machine learning, where the ordering of examples during training is typically an afterthought.</p>
<p>The variance persists with more data and larger models. Adding more training examples into the prompt does not necessarily reduce the variance in accuracy. We sweep over the number of training examples for three different datasets in Figure 1 (red curves). The variance remains high even when we use 16 training examples. Moreover, adding more training examples can sometimes hurt accuracy (e.g., mean accuracy drops from $36.0 \%$ to $25.9 \%$ for DBPedia 0 -shot to 1 -shot). The variance in accuracy can also remain high when using larger models, e.g., the left of Figure 1.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Majority label and recency biases cause GPT-3 to become biased towards certain answers and help to explain the high variance across different examples and orderings. Above, we use 4-shot SST-2 with prompts that have different class balances and permutations, e.g., [P P N N] indicates two positive training examples and then two negative. We plot how often GPT-3 2.7B predicts Positive on the balanced validation set. When the prompt is unbalanced, the predictions are unbalanced (majority label bias). In addition, balanced prompts that have one class repeated near the end, e.g., end with two Negative examples, will have a bias towards that class (recency bias).</p>
<p>GPT-3's accuracy depends highly on prompt format. We next keep the set of training examples and permutations fixed but vary the prompt format. We focus on SST-2, and we manually design an additional 14 prompt formats. The formats include question-answer templates, conversationstyle templates, prompts that resemble Web pages, and variations on the label names (all formats available in Table 7 in Appendix B). The accuracy for ten of the formats is shown in Figure 3. We find that some of the formats are better than others on average. However, all of the formats still suffer from high variance across different training sets.</p>
<h2>4. What Causes the High Variance?</h2>
<p>We next analyze why GPT-3's accuracy varies across different training examples, permutations, and prompt formats. Concretely, we show that the variance arises because LM s are biased towards outputting answers that are (1) frequent in the prompt (majority label bias), (2) towards the end of the prompt (recency bias), and (3) common in the pre-training data (common token bias).</p>
<p>Majority Label Bias We find that GPT-3 is biased towards answers that are frequent in the prompt. A trivial case is when a text classification prompt has a class imbalance, e.g., more Positive than Negative sentiment examples. This is demonstrated in the "unbalanced" region of Figure 4: when one class is more common, GPT-3 2.7B is heavily biased towards predicting that class. Since the SST-2 sentiment analysis dataset is balanced, this bias causes large accuracy degradations. The majority label bias also explains why we frequently observe a drop in accuracy when moving from 0 -shot to 1 -shot-we found that the drop is due to the model frequently repeating the class of the one training example.</p>
<p>The majority label bias also occurs for generation tasks. On the validation set for 4-shot LAMA with GPT-3 2.7B, 50.2\% of the model predictions are a repeat of one of the four train-
ing answers (the correct repeat rate is $24.7 \%$ ). Overall, the majority label bias helps to explain why different choices for the training examples heavily influence GPT-3's accuracyit shifts the distribution of model predictions.</p>
<p>Recency Bias The model's majority label bias is aggravated by its recency bias: the tendency to repeat answers that appear towards the end of the prompt. The "balanced" region of Figure 4 demonstrates this. For instance, when two Negative examples appear at the end (P P N N), the model will heavily prefer the Negative class. Moreover, the recency bias can outweigh the majority label bias, e.g., the "P P P N" training set leads to nearly $90 \%$ of predictions being Negative, despite $\frac{3}{4}$ of the training examples being Positive.</p>
<p>Recency bias also affects generation tasks. For 4-shot LAMA, the training answers that are closer to the end of the prompt are more likely to be repeated by the model. Concretely, the model "overpredicts" the answer from the 1st, 2nd, 3rd, and 4th training example by $8.5 \%, 8.3 \%, 14.3 \%$, and $16.1 \%$, respectively. ${ }^{2}$ Overall, recency bias helps to explain why the permutation of the training examples is important-the ordering of the examples heavily influences the distribution of the model predictions.</p>
<p>Common Token Bias Finally, we find that GPT-3 is biased towards outputting tokens that are common in its pretraining distribution, which is likely suboptimal for the distribution of answers on the downstream task. A simple case of this occurs for the LAMA fact retrieval dataset, where the model often predicts common entities such as "America" when the ground-truth answer is instead a rare entity.</p>
<p>A more nuanced case of the common token bias occurs for</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>text classification. Recall that the model makes predictions by generating the label name associated with each class. Because certain label names appear more frequently in the pre-training data, the model will be inherently biased towards predicting certain classes. For example, on DBPedia (a balanced 14-way topic classification dataset), GPT-3 predicts the "book" class $11 \times$ more often than the "artist" class. In fact, there is a moderate correlation $(r=0.67)$ between the frequency of a DBPedia label name and the rate at which GPT-3 predicts its class. ${ }^{3}$ Overall, the common token bias helps to explain why the choice of label names is important, and why the model struggles on rare answers.</p>
<p>The Impact of Biases on Model Predictions We find that the end result of the above three biases is typically a simple shift in the model's output distribution. For example, Figure 5 visualizes this shift for a SST-2 sentiment prompt.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. The Positive class probability for 25 random test inputs for a particular sentiment analysis prompt. Negative ground-truth examples are marked with $\boldsymbol{\sim}$ and Positive are marked with $\boldsymbol{\bullet}$.</p>
<p>The prompt used in Figure 5 and the model's intrinsic biases cause it to frequently predict high confidence for the Positive class. Since the default $50 \%$ threshold is used to make predictions, this results in frequent false positives. Importantly, note that if we could optimally set the classification threshold ( $\mathrm{p}($ Positive $)=0.68$ in this case), the classifier would be highly accurate ( $94 \%$ on the validation set).</p>
<h2>5. Contextual Calibration</h2>
<p>Thus far, we have shown that GPT-3 is biased towards certain answers due to the prompt and the model's intrinsic biases. Here, we look to correct this by "calibrating" the model's output probabilities. ${ }^{4}$ A common technique for adjusting output probabilities is to apply an affine transformation (Platt, 1999; Guo et al., 2017):</p>
<p>$$
\hat{\mathbf{q}}=\operatorname{softmax}(\mathbf{W} \hat{\mathbf{p}}+\mathbf{b})
$$</p>
<p>where a weight matrix $\mathbf{W}$ and a bias vector $\mathbf{b}$ are applied to the original probabilities $\hat{\mathbf{p}}$ to get the new probabilities</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$\hat{\mathbf{q}} .{ }^{5}$ For classification tasks, $\hat{\mathbf{p}}$ is the set of probabilities that are associated with each label name, renormalized to one. For generation tasks, $\hat{\mathbf{p}}$ is the entire set of probabilities for the first token. ${ }^{6}$ In this paper, we restrict the matrix $\mathbf{W}$ to be diagonal, known as vector scaling (Guo et al., 2017), to prevent the parameters from growing quadratically in the size of $\hat{\mathbf{p}}$ (which is $\approx 50,000$ for generation tasks).</p>
<p>The main challenge in the zero- or few-shot setting is that we do not have data to learn $\mathbf{W}$ and $\mathbf{b}$. We thus propose a novel data-free procedure to infer a good setting of these parameters. The key idea is that the model's bias towards certain answers can be estimated by feeding in a contentfree input such as the string "N/A". For example, consider the two-shot prompt:</p>
<p>Input: Subpar acting. Sentiment: Negative Input: Beautiful film. Sentiment: Positive Input: N/A Sentiment:
where "N/A" serves as the test input. Ideally, GPT-3 would score this test input as $50 \%$ Positive and $50 \%$ Negative. However, the model's biases cause it to score this input as $61.8 \%$ Positive. Note that this error is contextual: a different choice of the training examples, permutation, and format will lead to different predictions for the content-free input.</p>
<p>We can correct this error by setting $\mathbf{W}$ and $\mathbf{b}$ so that the class scores for the content-free input are uniform. We first obtain $\hat{\mathbf{p}}$ for the content-free input, denoted $\hat{\mathbf{p}}<em _mathrm_cf="\mathrm{cf">{\mathrm{cf}}$. We then set $\mathbf{W}=\operatorname{diag}\left(\hat{\mathbf{p}}</em>$ and take the argmax.}}\right)^{-1}$ and $\mathbf{b}$ to the all-zero vector. ${ }^{7}$ To make test predictions, we compute $\mathbf{W} \hat{\mathbf{p}}+\mathbf{b</p>
<p>Implementation Details This contextual calibration procedure adds trivial amounts of computational overhead and is implemented in a few lines of code (compute and save $\hat{\mathbf{p}}_{\mathrm{cf}}$, adjust output probabilities). For the content-free input, many good choices exist, including "N/A", the empty string, and gibberish tokens. In all our experiments, we average the probabilities from three content-free inputs: "N/A", "[MASK]", and the empty string. ${ }^{8}$ One could also craft the content-free input in a task-specific manner. We explore this for LAMA, where we replace the subject with the contentfree input, e.g., we use "N/A was born in" as the input.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>LM</th>
<th>0-shot</th>
<th></th>
<th>1-shot</th>
<th></th>
<th>4-shot</th>
<th></th>
<th>8-shot</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Baseline</td>
<td>Ours</td>
<td>Baseline</td>
<td>Ours</td>
<td>Baseline</td>
<td>Ours</td>
<td>Baseline</td>
<td>Ours</td>
</tr>
<tr>
<td>Text Classification</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AGNews</td>
<td>2.7B</td>
<td>$44.7_{0.0}$</td>
<td>$\mathbf{6 3 . 2}_{0.0}$</td>
<td>$33.0_{5.1}$</td>
<td>$\mathbf{5 9 . 6}_{6.4}$</td>
<td>$43.3_{8.3}$</td>
<td>$\mathbf{7 1 . 1}_{8.5}$</td>
<td>$50.8_{7.8}$</td>
<td>$\mathbf{7 2 . 7}_{5.8}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$43.9_{0.0}$</td>
<td>$\mathbf{7 3 . 9}_{0.0}$</td>
<td>$62.1_{6.3}$</td>
<td>$\mathbf{7 7 . 1}_{3.8}$</td>
<td>$61.0_{10.9}$</td>
<td>$\mathbf{8 5 . 9}_{1.3}$</td>
<td>$79.1_{2.6}$</td>
<td>$\mathbf{8 4 . 3}_{2.5}$</td>
</tr>
<tr>
<td>TREC</td>
<td>2.7B</td>
<td>$31.0_{0.0}$</td>
<td>$\mathbf{3 8 . 8}_{0.0}$</td>
<td>$24.3_{6.4}$</td>
<td>$\mathbf{3 6 . 8}_{7.7}$</td>
<td>$25.8_{11.5}$</td>
<td>$\mathbf{3 8 . 6}_{13.2}$</td>
<td>$29.3_{8.0}$</td>
<td>$\mathbf{4 4 . 3}_{11.4}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$47.4_{0.0}$</td>
<td>$\mathbf{5 7 . 4}_{0.0}$</td>
<td>$57.7_{6.0}$</td>
<td>$\mathbf{7 5 . 7}_{1.4}$</td>
<td>$60.2_{7.6}$</td>
<td>$\mathbf{6 9 . 7}_{1.4}$</td>
<td>$45.6_{4.0}$</td>
<td>$\mathbf{6 6 . 9}_{6.5}$</td>
</tr>
<tr>
<td>CB</td>
<td>2.7B</td>
<td>$44.6_{0.0}$</td>
<td>$\mathbf{5 0 . 0}_{0.0}$</td>
<td>$\mathbf{3 3 . 8}_{16.6}$</td>
<td>$33.0_{7.3}$</td>
<td>$43.5_{11.9}$</td>
<td>$\mathbf{5 4 . 2}_{4.7}$</td>
<td>$43.9_{8.4}$</td>
<td>$\mathbf{5 3 . 0}_{7.7}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$30.4_{0.0}$</td>
<td>$\mathbf{4 8 . 2}_{0.0}$</td>
<td>$50.9_{6.7}$</td>
<td>$\mathbf{5 1 . 8}_{7.2}$</td>
<td>$45.2_{19.4}$</td>
<td>$\mathbf{6 0 . 7}_{6.7}$</td>
<td>$59.6_{11.3}$</td>
<td>$\mathbf{6 5 . 0}_{7.9}$</td>
</tr>
<tr>
<td>RTE</td>
<td>2.7B</td>
<td>$44.8_{0.0}$</td>
<td>$\mathbf{4 9 . 5}_{0.0}$</td>
<td>$49.6_{2.9}$</td>
<td>$\mathbf{5 0 . 4}_{2.7}$</td>
<td>$44.0_{1.4}$</td>
<td>$\mathbf{5 4 . 5}_{4.7}$</td>
<td>$49.2_{1.9}$</td>
<td>$\mathbf{5 4 . 8}_{2.8}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$\mathbf{5 7 . 8}_{0.0}$</td>
<td>$\mathbf{5 7 . 8}_{0.0}$</td>
<td>$\mathbf{6 2 . 9}_{2.7}$</td>
<td>$62.8_{2.3}$</td>
<td>$58.7_{11.9}$</td>
<td>$\mathbf{6 0 . 4}_{8.1}$</td>
<td>$\mathbf{6 6 . 2}_{5.8}$</td>
<td>$65.5_{2.5}$</td>
</tr>
<tr>
<td>SST-2</td>
<td>2.7B</td>
<td>$57.2_{0.0}$</td>
<td>$\mathbf{7 1 . 4}_{0.0}$</td>
<td>$67.3_{7.9}$</td>
<td>$\mathbf{7 9 . 1}_{8.3}$</td>
<td>$59.1_{10.2}$</td>
<td>$\mathbf{7 9 . 9}_{7.8}$</td>
<td>$54.0_{4.3}$</td>
<td>$\mathbf{8 2 . 0}_{5.5}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$71.6_{0.0}$</td>
<td>$\mathbf{7 5 . 8}_{0.0}$</td>
<td>$93.3_{2.8}$</td>
<td>$\mathbf{9 4 . 7}_{1.4}$</td>
<td>$93.6_{3.3}$</td>
<td>$\mathbf{9 4 . 3}_{1.0}$</td>
<td>$\mathbf{9 5 . 6}_{1.0}$</td>
<td>$95.3_{0.7}$</td>
</tr>
<tr>
<td>DBPedia</td>
<td>2.7B</td>
<td>$36.0_{0.0}$</td>
<td>$\mathbf{3 8 . 7}_{0.0}$</td>
<td>$25.9_{4.4}$</td>
<td>$\mathbf{6 1 . 6}_{2.9}$</td>
<td>$61.0_{12.8}$</td>
<td>$\mathbf{6 6 . 0}_{7.5}$</td>
<td>$72.6_{4.5}$</td>
<td>$\mathbf{7 4 . 8}_{5.0}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$22.0_{0.0}$</td>
<td>$\mathbf{5 9 . 7}_{0.0}$</td>
<td>$79.3_{3.0}$</td>
<td>$\mathbf{8 5 . 3}_{2.2}$</td>
<td>$84.6_{5.8}$</td>
<td>$\mathbf{8 6 . 9}_{4.0}$</td>
<td>$82.3_{7.8}$</td>
<td>$\mathbf{8 6 . 9}_{1.9}$</td>
</tr>
<tr>
<td>Fact Retrieval</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LAMA</td>
<td>2.7B</td>
<td>$14.0_{0.0}$</td>
<td>$\mathbf{2 2 . 7}_{0.0}$</td>
<td>$29.7_{1.8}$</td>
<td>$\mathbf{3 1 . 6}_{1.3}$</td>
<td>$35.8_{3.8}$</td>
<td>$\mathbf{3 7 . 4}_{3.4}$</td>
<td>$\mathbf{4 2 . 5}_{1.3}$</td>
<td>$\mathbf{4 2 . 5}_{1.4}$</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>$23.5_{0.0}$</td>
<td>$\mathbf{3 0 . 1}_{0.0}$</td>
<td>$48.9_{2.3}$</td>
<td>$\mathbf{4 9 . 0}_{1.4}$</td>
<td>$\mathbf{6 2 . 0}_{2.4}$</td>
<td>$61.8_{2.9}$</td>
<td>$\mathbf{6 3 . 8}_{1.0}$</td>
<td>$63.6_{1.3}$</td>
</tr>
<tr>
<td>Information Extraction</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MIT-G</td>
<td>2.7B</td>
<td>$5.0_{0.0}$</td>
<td>$\mathbf{5 . 7}_{0.0}$</td>
<td>$26.7_{11.4}$</td>
<td>$\mathbf{3 7 . 9}_{5.7}$</td>
<td>$53.1_{7.8}$</td>
<td>$\mathbf{5 4 . 7}_{6.0}$</td>
<td>$59.0_{4.7}$</td>
<td>$\mathbf{5 9 . 1}_{4.8}$</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>$15.0_{0.0}$</td>
<td>$\mathbf{1 8 . 7}_{0.0}$</td>
<td>$47.3_{3.9}$</td>
<td>$\mathbf{5 2 . 0}_{7.9}$</td>
<td>$57.9_{4.8}$</td>
<td>$\mathbf{5 8 . 9}_{4.0}$</td>
<td>$59.0_{4.7}$</td>
<td>$\mathbf{5 9 . 1}_{4.8}$</td>
</tr>
<tr>
<td>MIT-D</td>
<td>2.7B</td>
<td>$46.3_{0.0}$</td>
<td>$\mathbf{4 7 . 0}_{0.0}$</td>
<td>$42.0_{13.0}$</td>
<td>$\mathbf{5 3 . 5}_{13.5}$</td>
<td>$73.5_{4.9}$</td>
<td>$\mathbf{7 4 . 1}_{5.0}$</td>
<td>$\mathbf{7 5 . 3}_{1.0}$</td>
<td>$75.1_{1.3}$</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>$36.3_{0.0}$</td>
<td>$\mathbf{3 8 . 7}_{0.0}$</td>
<td>$58.6_{21.4}$</td>
<td>$\mathbf{7 2 . 8}_{4.0}$</td>
<td>$75.4_{1.9}$</td>
<td>$\mathbf{7 5 . 9}_{2.1}$</td>
<td>$\mathbf{7 7 . 8}_{0.5}$</td>
<td>$\mathbf{7 7 . 8}_{0.5}$</td>
</tr>
<tr>
<td>ATIS-A</td>
<td>2.7B</td>
<td>$10.8_{0.0}$</td>
<td>$\mathbf{1 4 . 0}_{0.0}$</td>
<td>$29.8_{12.8}$</td>
<td>$\mathbf{3 3 . 1}_{9.4}$</td>
<td>$43.0_{26.2}$</td>
<td>$\mathbf{4 7 . 3}_{21.3}$</td>
<td>$55.6_{5.0}$</td>
<td>$\mathbf{5 8 . 8}_{4.0}$</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>$49.5_{0.0}$</td>
<td>$\mathbf{5 2 . 7}_{0.0}$</td>
<td>$69.6_{17.4}$</td>
<td>$\mathbf{7 1 . 8}_{17.1}$</td>
<td>$67.5_{10.4}$</td>
<td>$\mathbf{6 9 . 6}_{13.4}$</td>
<td>$63.4_{4.6}$</td>
<td>$\mathbf{6 4 . 5}_{4.0}$</td>
</tr>
<tr>
<td>ATIS-D</td>
<td>2.7B</td>
<td>$6.4_{0.0}$</td>
<td>$\mathbf{1 2 . 9}_{0.0}$</td>
<td>$42.3_{28.8}$</td>
<td>$\mathbf{6 5 . 6}_{20.8}$</td>
<td>$75.0_{6.7}$</td>
<td>$\mathbf{8 3 . 4}_{4.2}$</td>
<td>$81.0_{8.8}$</td>
<td>$\mathbf{8 8 . 3}_{3.7}$</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>$4.0_{0.0}$</td>
<td>$\mathbf{5 . 0}_{0.0}$</td>
<td>$\mathbf{9 7 . 9}_{0.6}$</td>
<td>$95.5_{4.6}$</td>
<td>$\mathbf{9 8 . 0}_{0.6}$</td>
<td>$97.8_{0.7}$</td>
<td>$\mathbf{9 8 . 8}_{0.3}$</td>
<td>$\mathbf{9 8 . 8}_{0.3}$</td>
</tr>
</tbody>
</table>
<p>Table 1. Contextual calibration improves accuracy across a range of tasks. We show the mean and standard deviation across different choices of the training examples (the prompt format is fixed). The LM column indicates the GPT-3 size (see Appendix A for GPT-2 results). The Baseline column shows the standard approach of greedy decoding (Brown et al., 2020) and Ours corresponds to greedy decoding after modifying the output probabilities using contextual calibration. We bold the better result of the baseline and ours. MIT-G, MIT-D, ATIS-A, and ATIS-D indicate the MIT Genre, MIT Director, ATIS Airline, and ATIS Departure Date datasets.</p>
<h3>5.1 Results for Contextual Calibration</h3>
<p>Here, we evaluate the effectiveness of contextual calibration across all of our datasets and LMs. We first use a fixed prompt format and select five different random sets of training examples, placing them in an arbitrary order in the prompt. We do not artificially balance the labels of the training examples for the classification tasks. We use the same sets of training examples for the baseline (standard decoding without calibration) and contextual calibration. We use labeling budgets of $0-8$ examples; using more than 8 shots causes the cost of querying the OpenAI API to become prohibitively expensive.</p>
<p>Table 1 shows the results and Figure 1 in Section 1 plots the same data for a subset of the tasks.</p>
<p>Improves Mean And Worst-Case Accuracy Contextual calibration dramatically improves GPT-3's average and worst-case accuracy, by up to $30.0 \%$ absolute. These gains hold for both classification and generation tasks. Contextual calibration also sometimes allows GPT-3 2.7B to outperform the GPT-3 175B baseline-by up to 19.3\%-despite being over 50x smaller.</p>
<p>Can Reduce Variance Across Training Sets Figure 6 plots the difference in the standard deviation between the baseline and contextual calibration for all tasks from Table 1. Contextual calibration reduces the variance considerably in a majority of cases, and it does not increase variance by much in the remaining cases.</p>
<p>Reduces Drop from 0-shot to 1-shot For the baseline, there are four cases where there is a drop in accuracy when</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Std Dev of Contextual Calibration - Baseline
Figure 6. Aside from improving mean accuracy, contextual calibration also reduces the standard deviation of accuracy across different choices of the training examples. We plot the difference in standard deviation between contextual calibration and the baseline from Table 1.
moving from 0 -shot to 1-shot (TREC, AGNews, DBpedia, SST-2). We attribute this drop to the majority label bias (see discussion in Section 4). Calibration removes this drop in three out of four cases.</p>
<p>Improves GPT-2 We also test GPT-2 1.5B (see Table 4 in Appendix A). We find that like GPT-3, GPT-2's accuracy also highly varies across different prompts. This suggests that the variance that we observe for few-shot in-context learning is a general problem for LMs. Second, contextual calibration works out-of-the-box for GPT-2-it improves the mean accuracy and reduces variance for most tasks.</p>
<p>Improves Accuracy Across Formats In our next set of experiments, we use a fixed set of training examples and vary the prompt format. We use the 15 prompt formats for SST-2 discussed in Section 3. We also create 15 prompt formats for each of three random relations in LAMA (P20, P159, P19) by using the paraphrases of the original LAMA templates generated by Jiang et al. (2020b). Figure 7 shows the results before and after calibration for SST-2, and Figure 9 in Appendix A show the results for LAMA. Contextual calibration improves the average and worst-case accuracy for both datasets, and reduces the variance for SST-2.</p>
<h3>5.2. Ablations on Contextual Calibration</h3>
<p>We finally conduct two analyses/ablations on contextual calibration. We first analyze how effective contextual calibration is at inferring a good setting of $\mathbf{W}$. To do so, we compare its accuracy to an "oracle calibration" method that uses the validation set to find the best possible diagonal $\mathbf{W}$. We evaluate this oracle on AGNews, and find that contextual calibration is surprisingly close to it (Figure 8).</p>
<p>We also study how the choice of content-free input affects accuracy. In Table 3 in Appendix A, we show the accuracy for SST-2 and AGNews for different choices of the content-free input. The choice of content-free input matters, however, many good choices exist.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. GPT-3 has high variance across different prompt formats; contextual calibration reduces this variance and improves mean accuracy. We show the mean accuracy ( $\pm$ standard deviation) over 15 different prompt formats for SST-2.</p>
<h2>6. Discussion</h2>
<p>Does Calibration Eliminate the Need to Engineer Prompts? The motivation behind "prompt engineering" is that not all prompts lead to the same accuracy. Thus, one should tune the prompt's format and examples to achieve the best possible performance (Brown et al., 2020; Gao et al., 2020). Contextual calibration does not eliminate the need to engineer prompts, however, it does mitigate it: contextual calibration makes the accuracy of the best, average, and worst-case prompts more similar (and higher).</p>
<p>Should You Finetune in the Few-shot Setting? We use a fixed LM with no finetuning. As mentioned in Section 1, there are numerous reasons not to finetune: it enables rapid prototyping, provides a fully natural language interface, and is more efficient in terms of memory requirements and system complexity when serving many different tasks. Moreover, like in-context learning without contextual calibration, finetuning can be unstable in the few-shot setting (Schick \&amp; Schütze, 2021). Nevertheless, if these disadvantages are acceptable or avoidable, finetuning can improve accuracy over in-context learning in some cases (Schick \&amp; Schütze, 2020; Gao et al., 2020). An interesting direction for future work is to study the interplay between contextual calibration and finetuning, e.g., does contextual calibration alleviate the need to finetune, or vice versa?</p>
<h2>7. Related Work</h2>
<p>Few-shot Learning with Language Models Recent work uses LMs to solve NLP tasks, e.g., for story cloze prediction (Schwartz et al., 2017), knowledge base completion (Petroni et al., 2019), and Winograd schemas (Trinh \&amp; Le, 2018). Radford et al. (2019) and Brown et al. (2020)</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Contextual calibration, despite using no training data, achieves similar accuracy to an "oracle" calibration that finds the best $\mathbf{W}$ using the validation set. The plot shows GPT-3 175B's mean accuracy ( $\pm$ standard deviation) on AGNews over different choices of the training examples.
show that large LMs can be used to solve a myriad of tasks in a few-shot manner via in-context learning. Our paper provides a simple modification to their setting that improves performance. Asking LMs to complete natural language prompts is also used as a method to "probe" LMs, e.g., analyzing their factual (Petroni et al., 2019; Jiang et al., 2020b; Shin et al., 2020) or commonsense knowledge (Bosselut et al., 2019). Our results suggest that these probing methods may underestimate model accuracy, and we recommend that future work take advantage of contextual calibration.</p>
<p>Volatility of Few-shot Learning in NLP Recent work shows that when using masked language models such as BERT for zero-shot learning, the prompt format can impact accuracy (Petroni et al., 2019; Jiang et al., 2020b; Shin et al., 2020). Independent and concurrent work also shows that when finetuning masked language models on few examples, the choice of training examples can impact results (Schick \&amp; Schütze, 2020; Gao et al., 2020). We show that similar instabilities occur for in-context learning (i.e., no finetuning) with left-to-right language models. We also show a surprising instability associated with example ordering. Moreover, unlike past work, we analyze why these instabilities occur, and we use insights from this analysis to mitigate the issues.</p>
<p>Failures of Language Models We identify failures when LMs are used for in-context learning (e.g., recency bias). Past work identifies similar failures when LMs are used for text generation. For example, neural LMs often repeat themselves (Holtzman et al., 2020), suffer from overconfidence (Braverman et al., 2020; Jiang et al., 2020a), suffer from recency bias (Khandelwal et al., 2018; Ravfogel et al., 2019), and prefer generic responses instead of rare text ( Li et al., 2016; Logan et al., 2019). Past work mitigates these
degeneracies by modifying the model's output probabilities or generation schemes, e.g., explicitly preventing repetitions (Paulus et al., 2018) or using sampling instead of greedy decoding (Holtzman et al., 2020).</p>
<h2>8. Conclusion and Future Work</h2>
<p>We show that few-shot learning can be highly volatile across different choices of the prompt. Through a detailed analysis, we identify that this volatility arises from biases in LMs, e.g., their tendency to output recent or common tokens. We use these insights to develop contextual calibration-a simple procedure to adjust the model's output probabilities-which improves accuracy, reduces variance, and overall makes tools like GPT-3 more effective for end users.</p>
<p>Looking at the bigger picture, our results inspire two future research directions in few-shot learning for NLP. First, on the methods side, we show that good few-shot learning requires attention to detail: small but non-trivial decisions such as calibration can greatly influence results. This makes it difficult to correctly develop and compare new methods (e.g., pretraining schemes or model architectures). We thus hope to make other few-shot learning methods more robust, and also expand our techniques to cover a wider ranger of tasks (e.g., calibration for open-ended generation). Second, on the analysis side, our results highlight the need to understand what GPT-3 learns from the prompt. The model has an impressive ability to improve with more training examples, however, we show that the model learns some superficial patterns such as repetition of common answers. We hope to better understand and analyze the dynamics of in-context learning in future work.</p>
<h2>Acknowledgements</h2>
<p>We thank OpenAI for providing academic access to the GPT3 API. We thank Sewon Min, Nikhil Kandpal, Nelson Liu, Girish Sastry, Marco Tulio Ribeiro, and the members of Berkeley NLP for valuable feedback on the paper.</p>
<p>This work was supported by DARPA under the LwLL program/Grant No. FA8750-19-1-0504, DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, DARPA and the Air Force Research Laboratory (AFRL), and NSF award #IIS-1756023.</p>
<h2>References</h2>
<p>Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. COMET: Commonsense transformers for automatic knowledge graph construction. In $A C L, 2019$.</p>
<p>Braverman, M., Chen, X., Kakade, S., Narasimhan, K.,</p>
<p>Zhang, C., and Zhang, Y. Calibration, entropy rates, and memory in language models. In ICML, 2020.</p>
<p>Brier, G. W. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 1950.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In NeurIPS, 2020.</p>
<p>Dagan, I., Glickman, O., and Magnini, B. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Workshop, 2005.
de Marneffe, M.-C., Simons, M., and Tonhauser, J. The CommitmentBank: Investigating projection in naturally occurring discourse. In Sinn und Bedeutung, 2019.</p>
<p>Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.</p>
<p>Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020.</p>
<p>Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In ICML, 2017.</p>
<p>Hemphill, C. T., Godfrey, J. J., and Doddington, G. R. The ATIS spoken language systems pilot corpus. In Speech and Natural Language Workshop, 1990.</p>
<p>Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In $I C L R$, 2020.</p>
<p>Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can we know when language models know? arXiv preprint arXiv:2012.00955, 2020a.</p>
<p>Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we know what language models know? In TACL, 2020b.</p>
<p>Khandelwal, U., He, H., Qi, P., and Jurafsky, D. Sharp nearby, fuzzy far away: How neural language models use context. In $A C L, 2018$.</p>
<p>Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. In Science, 2015.</p>
<p>Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. A diversity-promoting objective function for neural conversation models. In NAACL, 2016.</p>
<p>Liu, J., Cyphers, S., Pasupat, P., McGraw, I., and Glass, J. A conversational movie search system based on conditional random fields. In INTERSPEECH, 2012.</p>
<p>Logan, R. L., Liu, N. F., Peters, M. E., Gardner, M., and Singh, S. Barack's wife Hillary: Using knowledge-graphs for fact-aware language modeling. In ACL, 2019.</p>
<p>Paulus, R., Xiong, C., and Socher, R. A deep reinforced model for abstractive summarization. In $I C L R, 2018$.</p>
<p>Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. Language models as knowledge bases? In EMNLP, 2019.</p>
<p>Platt, J. C. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, 1999.</p>
<p>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pretraining. Technical Report, 2018.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. Technical Report, 2019.</p>
<p>Ravfogel, S., Goldberg, Y., and Linzen, T. Studying the inductive biases of RNNs with synthetic variations of natural languages. In NAACL, 2019.</p>
<p>Schick, T. and Schütze, H. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020.</p>
<p>Schick, T. and Schütze, H. Exploiting cloze questions for few-shot text classification and natural language inference. In EACL, 2021.</p>
<p>Schwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y., and Smith, N. A. The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In $A C L, 2017$.</p>
<p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP, 2020.</p>
<p>Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.</p>
<p>Trinh, T. H. and Le, Q. V. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018.</p>
<p>Voorhees, E. M. and Tice, D. M. Building a question answering test collection. In SIGIR, 2000.</p>
<p>Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In NeurIPS, 2019.</p>
<p>Yogatama, D., d'Autume, C. d. M., Connor, J., Kocisky, T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L., Dyer, C., et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.</p>
<p>Zhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification. In NeurIPS, 2015.</p>
<h1>A. Additional Results on Variance and Calibration</h1>
<p>Table 2 shows an example of the sensitivity to ordering.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt (test input not shown)</th>
<th style="text-align: center;">Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Review: the whole thing 's fairly lame, making it par for <br> the course for disney sequels .</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: Negative</td>
<td style="text-align: center;">$88.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Review: this quiet, introspective and entertaining indepen- <br> dent is worth seeking .</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: Positive</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Review: this quiet, introspective and entertaining indepen- <br> dent is worth seeking .</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: Positive</td>
<td style="text-align: center;">$51.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Review: the whole thing 's fairly lame, making it par for <br> the course for disney sequels .</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2. Top: a prompt consisting of two training examples (the test input is not shown) that leads to good test accuracy for GPT-3 2.7B (88.5\%). Bottom: simply reversing the order of the two examples causes the accuracy to drop to near random chance ( $51.3 \%$ ).</p>
<p>Table 3 demonstrates that the choice of content-free input does affect accuracy, however, many good choices exist.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Content-free Input</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">AGNews</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Uncalibrated Baseline</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">48.5</td>
</tr>
<tr>
<td style="text-align: left;">N/A</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: left;">[MASK]</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: left;">$\cdot$</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">64.7</td>
</tr>
<tr>
<td style="text-align: left;">N/A, [MASK], '</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">66.5</td>
</tr>
<tr>
<td style="text-align: left;">the</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: left;">abc</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: left;">the man.</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: left;">dasjhasjkdhjskdhds</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: left;">nfjkhdvy84tr9bpuirvwe</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">65.5</td>
</tr>
</tbody>
</table>
<p>Table 3. We show the accuracy for 1-shot SST-2 and 0-shot AGNews over different choices for the content-free input. The choice of content-free input matters, however, many good choices exist. The token " indicates the empty string. Recall that in our experiments, we ensemble over N/A, [MASK], and the empty string.</p>
<p>Figure 9 shows how GPT-3 accuracy changes as the prompt format is varied for LAMA, with and without calibration.</p>
<p>Table 4 shows the effect of calibration for GPT-2.</p>
<h2>B. Prompt Formats Used</h2>
<p>Tables 5 and 6 show the default prompt format used for all tasks. Table 7 shows the 15 different formats used when studying the effect of prompt format for SST-2.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Contextual calibration improves GPT-3's accuracy across various prompt formats for LAMA. We plot GPT-2 2.7B's mean accuracy over 15 different formats for the LAMA "place of death" relation (P20), "Headquarter Location" relation (P159), and "place of birth" relation (P19).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">0-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">1-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">4-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">8-shot</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
</tr>
<tr>
<td style="text-align: center;">Text Classification</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$44.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 0}_{0.0}$</td>
<td style="text-align: center;">$45.4_{8.4}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 9}_{5.7}$</td>
<td style="text-align: center;">$44.6_{12.2}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 0}_{13.6}$</td>
<td style="text-align: center;">$57.1_{11.6}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 1}_{7.3}$</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$24.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 3}_{0.0}$</td>
<td style="text-align: center;">$21.5_{5.2}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 1}_{2.6}$</td>
<td style="text-align: center;">$23.1_{5.9}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 2}_{2.2}$</td>
<td style="text-align: center;">$32.7_{7.5}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 1}_{3.6}$</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$\mathbf{4 4 . 6}_{0.0}$</td>
<td style="text-align: center;">$17.9_{0.0}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 6}_{10.0}$</td>
<td style="text-align: center;">$47.1_{12.2}$</td>
<td style="text-align: center;">$40.0_{8.3}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 4}_{7.3}$</td>
<td style="text-align: center;">$48.9_{5.7}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 2}_{1.4}$</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$\mathbf{5 1 . 0}_{0.0}$</td>
<td style="text-align: center;">$48.5_{0.0}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 6}_{2.1}$</td>
<td style="text-align: center;">$56.3_{2.4}$</td>
<td style="text-align: center;">$53.2_{6.0}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 5}_{1.8}$</td>
<td style="text-align: center;">$54.9_{3.0}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 7}_{1.29}$</td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$60.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 0}_{0.0}$</td>
<td style="text-align: center;">$66.7_{17.9}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 0}_{11.4}$</td>
<td style="text-align: center;">$64.9_{8.4}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 8}_{10.9}$</td>
<td style="text-align: center;">$54.5_{4.6}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 6}_{8.8}$</td>
</tr>
<tr>
<td style="text-align: center;">DBPedia</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$\mathbf{6 4 . 3}_{0.0}$</td>
<td style="text-align: center;">$58.3_{0.0}$</td>
<td style="text-align: center;">$33.6_{18.9}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 5}_{9.4}$</td>
<td style="text-align: center;">$53.0_{14.8}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 3}_{8.1}$</td>
<td style="text-align: center;">$66.0_{3.6}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 3}_{8.7}$</td>
</tr>
<tr>
<td style="text-align: center;">Fact Retrieval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LAMA</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$14.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 7}_{0.0}$</td>
<td style="text-align: center;">$29.7_{1.8}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 6}_{1.3}$</td>
<td style="text-align: center;">$35.8_{3.8}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 4}_{3.4}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 5}_{1.3}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 5}_{1.4}$</td>
</tr>
<tr>
<td style="text-align: center;">Information Extraction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MIT-G</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$7.7_{0.0}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 0}_{0.0}$</td>
<td style="text-align: center;">$32.9_{10.0}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 2}_{4.1}$</td>
<td style="text-align: center;">$44.3_{6.5}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 7}_{5.8}$</td>
<td style="text-align: center;">$56.9_{2.5}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 5}_{2.5}$</td>
</tr>
<tr>
<td style="text-align: center;">MIT-D</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$29.3_{0.0}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 7}_{0.0}$</td>
<td style="text-align: center;">$26.2_{10.5}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 8}_{4.8}$</td>
<td style="text-align: center;">$70.5_{2.5}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 4}_{1.8}$</td>
<td style="text-align: center;">$77.1_{4.4}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 1}_{3.9}$</td>
</tr>
<tr>
<td style="text-align: center;">ATIS-A</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$15.1_{0.0}$</td>
<td style="text-align: center;">$\mathbf{3 5 . 5}_{0.0}$</td>
<td style="text-align: center;">$41.5_{11.7}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 4}_{7.5}$</td>
<td style="text-align: center;">$55.1_{18.9}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 8}_{11.7}$</td>
<td style="text-align: center;">$63.4_{10.6}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 9}_{10.4}$</td>
</tr>
<tr>
<td style="text-align: center;">ATIS-D</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">$1.0_{0.0}$</td>
<td style="text-align: center;">$\mathbf{2 . 5}_{0.0}$</td>
<td style="text-align: center;">$62.3_{9.2}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 7}_{4.3}$</td>
<td style="text-align: center;">$81.1_{3.6}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 2}_{7.2}$</td>
<td style="text-align: center;">$81.8_{4.5}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 9}_{5.0}$</td>
</tr>
</tbody>
</table>
<p>Table 4. Contextual calibration improves accuracy for GPT-2. This table is analogous to Table 1 but shows results for GPT-2 XL.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Label Names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Sentiment: Positive <br> Review: Horrific movie, don't see it. <br> Sentiment:</td>
<td style="text-align: center;">Positive, Negative</td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">Article: USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump. <br> Answer: Business <br> Article: New hard-drive based devices feature color screens, support for WMP 10. <br> Answer:</td>
<td style="text-align: center;">World, Sports, Business, Technology</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">Classify the questions based on whether their answer type is a Number, Location, Person, Description, Entity, or Abbreviation. <br> Question: How did serfdom develop in and then leave Russia? <br> Answer Type: Description <br> Question: When was Ozzy Osbourne born? <br> Answer Type:</td>
<td style="text-align: center;">Number, Location, Person, Description, Entity, Abbreviation</td>
</tr>
<tr>
<td style="text-align: center;">DBPedia</td>
<td style="text-align: center;">Classify the documents based on whether they are about a Company, School, Artist, Athlete, Politician, Transportation, Building, Nature, Village, Animal, Plant, Album, Film, or Book. <br> Article: Geoffrey D. Falksen (born July 31 1982) is an American steampunk writer. <br> Answer: Artist <br> Article: The Perrin River is a 1.3-mile-long ( 2.1 km ) tidal river in the U.S. state of Virginia. It is a small inlet on the north shore of the York River near that river's mouth at Chesapeake Bay. <br> Answer:</td>
<td style="text-align: center;">Company, School, Artist, Athlete, Politician, Transportation, Building, Nature, Village, Animal, Plant, Album, Film, Book</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">But he ended up eating it himself. I was reluctant to kiss my mother, afraid that somehow her weakness and unhappiness would infect me. Naturally I didn't think for a minute that my life and spirit could stimulate her. <br> question: her life and spirit could stimulate her mother. True, False, or Neither? answer: Neither <br> Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? <br> question: Valence was helping. True, False, or Neither? answer:</td>
<td style="text-align: center;">True, False, Neither</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">Others argue that Mr. Sharon should have negotiated the Gaza pullout - both to obtain at least some written promises of better Palestinian behavior, and to provide Mr. Abbas with a prime prize to show his people that diplomacy, not violence, delivered Gaza. question: Mr. Abbas is a member of the Palestinian family. True or False? answer: False <br> The program will include Falla's "Night in the Gardens of Spain," Ravel's Piano Concerto in G, Berlioz's Overture to "Beatrice and Benedict," and Roy Harris' Symphony No. 3. question: Beatrice and Benedict is an overture by Berlioz. True or False? answer:</td>
<td style="text-align: center;">True, False</td>
</tr>
</tbody>
</table>
<p>Table 5. The prompts used for text classification. We show one training example per task for illustration purposes. The right column shows the label names (to make predictions, we check the LM's probability for these tokens).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LAMA</td>
<td style="text-align: center;">Alexander Berntsson was born in Sweden <br> Khalid Karami was born in</td>
</tr>
<tr>
<td style="text-align: center;">ATIS <br> (Airline)</td>
<td style="text-align: center;">Sentence: what are the two american airlines flights that leave from dallas to san francisco in the evening Airline name: american airlines <br> Sentence: list a flight on american airlines from toronto to san diego Airline name:</td>
</tr>
<tr>
<td style="text-align: center;">ATIS <br> (Depart Date)</td>
<td style="text-align: center;">Sentence: please list any flight available leaving oakland california tuesday arriving philadelphia wednesday Depart date - Day name: tuesday <br> Sentence: show me all all flights from pittsburgh to atlanta on wednesday which leave before noon and serve breakfast <br> Depart date - Day name:</td>
</tr>
<tr>
<td style="text-align: center;">MIT Movies (Genre)</td>
<td style="text-align: center;">Sentence: last to a famous series of animated movies about a big green ogre and his donkey and cat friends Genre: animated <br> Sentence: what is a great comedy featuring the talents of steve carell as a loser looking for a friend Genre:</td>
</tr>
<tr>
<td style="text-align: center;">MIT Movies <br> (Director)</td>
<td style="text-align: center;">Sentence: in 2005 director christopher nolan rebooted a legendary dc comics superhero with a darker grittier edge in which movie <br> Director: christopher nolan <br> Sentence: what 1967 mike nichols film features dustin hoffman in romantic interludes with anne bancroft as mrs robinson <br> Director:</td>
</tr>
</tbody>
</table>
<p>Table 6. The prompts used for generation tasks. We show one training example per task for illustration purposes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Format ID</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Label Names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Answer: Positive <br> Review: Horrific movie, don't see it. <br> Answer:</td>
<td style="text-align: center;">Positive, Negative</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Answer: good <br> Review: Horrific movie, don't see it. <br> Answer:</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">My review for last night's film: This movie is amazing! The critics agreed that this movie was good My review for last night's film: Horrific movie, don't see it. The critics agreed that this movie was</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Here is what our critics think for this month's films. <br> One of our critics wrote "This movie is amazing!". Her sentiment towards the film was positive. <br> One of our critics wrote "Horrific movie, don't see it". Her sentiment towards the film was</td>
<td style="text-align: center;">positive, negative</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Critical reception [ edit ] <br> In a contemporary review, Roger Ebert wrote "This movie is amazing!". Entertainment Weekly agreed, and the overall critical reception of the film was good. <br> In a contemporary review, Roger Ebert wrote "Horrific movie, don't see it". Entertainment Weekly agreed, and the overall critical reception of the film was</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Positive Review? Yes <br> Review: Horrific movie, don't see it. <br> Positive Review?</td>
<td style="text-align: center;">Yes, No</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Question: Is the sentiment of the above review Positive or Negative? <br> Answer: Positive <br> Review: This movie is amazing! <br> Question: Is the sentiment of the above review Positive or Negative? <br> Answer:</td>
<td style="text-align: center;">Positive, Negative</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Question: Did the author think that the movie was good or bad? <br> Answer: good <br> Review: This movie is amazing! <br> Question: Did the author think that the movie was good or bad? <br> Answer:</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Question: Did the author of the following tweet think that the movie was good or bad? <br> Tweet: This movie is amazing! <br> Answer: good <br> Question: Did the author of the following tweet think that the movie was good or bad? <br> Tweet: Horrific movie, don't see it <br> Answer:</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">This movie is amazing! My overall feeling was that the movie was good Horrific movie, don't see it. My overall feeling was that the movie was</td>
<td style="text-align: center;">good, bad</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">This movie is amazing! I liked the movie. <br> Horrific movie, don't see it. I</td>
<td style="text-align: center;">liked, hated</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">This movie is amazing! My friend asked me if I would give the movie 0 or 5 stars, I said 5 Horrific movie, don't see it. My friend asked me if I would give the movie 0 or 5 stars, I said</td>
<td style="text-align: center;">0,5</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Input: This movie is amazing! <br> Sentiment: Positive <br> Input: Horrific movie, don't see it. <br> Sentiment:</td>
<td style="text-align: center;">Positive, Negative</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Positive: True <br> Review: Horrific movie, don't see it. <br> Positive:</td>
<td style="text-align: center;">True, False</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Review: This movie is amazing! <br> Stars: 5 <br> Review: Horrific movie, don't see it. <br> Stars:</td>
<td style="text-align: center;">5,0</td>
</tr>
</tbody>
</table>
<p>Table 7. The different prompt formats used when studying the effect of format for SST-2. We show one training example for illustration.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The frequency of a token on the web is calculated using Google Ngrams https://books.google.com/ngrams. The predictions are from the 0 -shot setting on the validation set.
${ }^{4}$ The output of GPT-3 is biased (its outputs are shifted), similar to how measurement devices such as voltage meters or weighing scales are biased. Just like how these devices require "calibration before use", where the devices' outputs are scaled/zeroed-out, we hope to apply a similar calibration procedure to LMs. This goal is distinct from statistical calibration (Brier, 1950; Guo et al., 2017), i.e., aligning a model's confidence estimate with its true accuracy.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ This affine transformation is usually applied to the logits, i.e., prior to the softmax. However, we only have access to GPT-3's output probabilities in the OpenAI API.
${ }^{6}$ We only calibrate the prediction of the first output token for generation tasks. This is reasonable because, for the tasks we consider, we found that the model's predictions are highly deterministic after generating the first token.
${ }^{7}$ An alternate solution is to set $\mathbf{b}$ to $-\hat{\mathbf{p}}_{\mathrm{cf}}$ and $\mathbf{W}$ to the identity. Empirically, this alternate solution yields higher accuracy for generation tasks (where the dimensionality of $\hat{\mathbf{p}}$ is large). The solution in the main text performs better for classification.
${ }^{8}$ We found this simple ensemble to achieve the best results for AGNews, and we reuse it for all other datasets. See Section 5.2 for an ablation on the choice of content-free input.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9867 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9867</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9867</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-eda08c6f5919f39979acf0b3bc52e903063b5ba4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/eda08c6f5919f39979acf0b3bc52e903063b5ba4" target="_blank">Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> Evaluation of the 47 cutting-edge LLMs on Xiezhi indicates that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management.</p>
                <p><strong>Paper Abstract:</strong> New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge.Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 249,587 questions and accompanied by Xiezhi-Specialty with 14,041 questions and Xiezhi-Interdiscipline with 10,746 questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. All the evaluation code and data are open sourced in https://github.com/MikeGu721/XiezhiBenchmark</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9867.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9867.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xiezhi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Xiezhi Benchmark (Xiezhi-All / Xiezhi-Specialty / Xiezhi-Interdiscipline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, multi-disciplinary, auto-updating benchmark for domain knowledge evaluation containing 249,587 multiple-choice questions across 516 disciplines (Chinese Discipline Taxonomy) with validated subsets: Xiezhi-Meta (20,124), Xiezhi-Train (2,555), Xiezhi-Specialty (14,041) and Xiezhi-Interdiscipline (10,746).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>47 LLMs (including GPT-4, ChatGPT, Bloomz variants, Llama, Pythia, Falcon, ChatGLM, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>A diverse set of 47 state-of-the-art LLMs were evaluated, ranging from small models (~560M) to large models (tens or hundreds of billions of parameters), including both open-source and API models (GPT-4, ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Broad / Multi-domain (516 academic disciplines spanning 13 top-level categories e.g., science, engineering, medicine, law, economics, literature, history, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Multiple-choice question answering at scale using generative-probability-based ranking of 50 options per question; evaluation under 0-shot, 1-shot and 3-shot demonstration settings; aggregated metrics computed per model and per domain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary: Mean Reciprocal Rank (MRR) of the correct option among ranked 50 choices. Also reported: Hit@1, Hit@4, Mean Rank, Accuracy in supplementary analyses; comparison to human baselines (top and average practitioners) per domain.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Xiezhi-All (249,587 Qs) with human-verified subsets: Xiezhi-Meta, Xiezhi-Train, Xiezhi-Specialty, Xiezhi-Interdiscipline. Questions originate from Chinese public exams and auto-generated items from academic surveys and are labeled with discipline taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using MRR as the primary metric, GPT-4 and ChatGPT rank near the top (GPT-4 MRR on Xiezhi reported ≈ 0.377 in table), and LLMs exceed average human performance in domains such as science, engineering, agronomy, medicine and art, but fall short of humans in economics, jurisprudence, pedagogy, literature, history and management. Xiezhi exhibits higher variance across models and better discriminates model differences than prior benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dataset bias toward Chinese sources (cultural / language bias), some parts auto-annotated (Xiezhi-All) without full manual verification, English translations post-processed but not by native speakers, computation cost due to generative-probability ranking, experiments run with a single random seed (42) and no repeated error bars, and sensitive/Chinese-centric items were removed from public subsets but residual bias may remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Direct comparison to human baselines: authors collected published average scores for exam sources and present 'top' and 'average' practitioner scores per domain; LLMs surpass average human practitioners in several scientific/technical domains but not in many social-humanities domains. Traditional extraction-of-choice evaluations (4-way) are contrasted with Xiezhi's 50-way ranking approach.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use broader, multi-domain, up-to-date benchmarks; adopt many-option multiple-choice (e.g., 50 choices) to reduce random-guess inflation; prefer generative-probability ranking over forcing models to output a label string; report ranking metrics (MRR, Hit@k, Mean Rank) alongside accuracy; include few-shot settings (0/1/3) and compare across them; maintain auto-updating pipelines to stay ahead of LLM training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9867.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9867.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenProbRank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative-Probability Ranking (ranking options by model generation probability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation method that queries an LLM for generation probabilities for each provided option and ranks options by those probabilities instead of extracting a chosen label string; intended to fairly evaluate generative models that may not be trained to answer multiple-choice prompts directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>45 open-source LLMs (direct probability access) + 2 API LLMs (ChatGPT, GPT-4; ranked via instruction-based ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Applied across many LLM families; for open-source models generative probabilities were read directly from model outputs, for API models ChatGPT/GPT-4 were prompted to rank options by preference.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General (applied to all domain questions in Xiezhi: science, engineering, medicine, humanities, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each question, compute model's generation probability for every candidate option (50 options), then rank options descending by probability and compute ranking metrics (MRR primarily). This avoids penalizing models that can't follow multiple-choice answer-extraction prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Rank-based metrics: Mean Reciprocal Rank (primary), Hit@k, Mean Rank; supports accuracy calculation by checking Hit@1 or top-k hits.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used throughout Xiezhi evaluation (50-option setup).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Generative-probability ranking revealed larger separations between models than standard extraction-based multiple-choice evaluation; authors report dramatic declines in absolute performance as number of options increased, but improved discriminative power. MRR used to summarize ranking quality; GPT-4 and Bloomz variants performed best in ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires access to model token/sequence log-probabilities (compute- and IO-intensive), API-access models may not expose true probabilities, ranking via instruction for APIs may be less consistent than native probabilities, increased computational cost compared to single-answer extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Compared to instruction+extraction methods (common in prior benchmarks), GenProbRank reduces evaluation bias against models not fine-tuned for multiple-choice output format and is argued to be more faithful to model internal preference distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When possible, use generation probabilities to rank provided options; for API-limited models, provide standardized ranking prompts and document differences; couple ranking metrics (MRR) with Hit@k and accuracy for a fuller picture; fix random seeds and sampling of distractor options to ensure reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9867.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9867.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>50-Option Setting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expanded-Option Multiple-Choice Setting (50 options per question)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation configuration that increases the number of candidate options per multiple-choice question to 50 (1 correct, 3 confusing distractors, 46 randomly sampled distractors) to reduce random-guessing and increase resolution between models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applied to all tested LLMs (47 models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>The setting is model-agnostic; affects how options are assembled and evaluated for every model.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General across all Xiezhi domains</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each question, construct 50 options: include correct answer, add 3 carefully chosen confusing distractors, and sample 46 other options from the dataset ensuring low string overlap; then rank options by model probability (or via instructed ranking for API models) and compute MRR/Hit@k.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ranking metrics (MRR primary); accuracy and Hit@1 are low under this setting due to many distractors but provide stricter assessments of model confidence and discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Implemented for Xiezhi-Specialty and Xiezhi-Interdiscipline evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Performance of all LLMs dropped substantially compared to 4-option settings; however the expanded-option setting increases sensitivity to small model improvements and better separates model capabilities across domains—Xiezhi reports more informative variance across models under 50-option setup.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Construction of distractors can still produce non-confusing options; larger option sets increase computation and may penalize models disproportionately; sampling strategy and string-similarity heuristics affect difficulty and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Traditional 4-choice exam settings inflate random-guess baselines; 50-option setting reduces chance performance (random-guess ~0.02 instead of 0.25) and thus produces more informative comparisons to human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use many-option setups to reduce chance-level inflation when measuring knowledge; ensure distractors are sampled with string-similarity filtering and include some close confusers; precompute and freeze option sets to ensure reproducibility; report baseline random-guess performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9867.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9867.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mean Reciprocal Rank (MRR) for option ranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ranking-based evaluation metric computing mean of reciprocal ranks (1/rank) of the correct option; used as the primary scalar to summarize how well a model ranks the correct answer among many candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applied across all evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>MRR was computed per model over the full test set and per-domain subsets to quantify ranking performance.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / Multi-domain (applied to Xiezhi's domain questions)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each question, find the rank position r of the correct option in the model's ranked list of 50 options; compute reciprocal (1/r); average across questions to obtain MRR (range 0..1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>MRR (primary). Supplemented with Hit@1, Hit@4, Mean Rank, and Accuracy in appendix analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Primary metric for Xiezhi (used on Xiezhi-Specialty and Xiezhi-Interdiscipline and other baseline benchmarks in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MRR revealed that GPT-4 attains the highest MRR on Xiezhi (~0.377 reported), with other top models like Bloomz-mt and ChatGPT following; MRR showed higher variance across models on Xiezhi compared to other benchmarks, making it useful for discriminative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>MRR emphasizes ranking position (sensitive to top ranks) but can obscure absolute correctness rates (accuracy); also different metrics (Hit@k, Mean Rank) may yield different model orderings; single-seed experiments and choice-sampling influence MRR estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>MRR enables direct comparison to human baselines in ranking terms (e.g., if humans place correct answer first more often); authors pair MRR with reported human 'top' and 'average' scores per domain to show where models exceed or lag humans.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report MRR as a primary ranking metric when using many-candidate option tasks, supplement with Hit@k and accuracy, and analyze sensitivity to distractor construction and random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9867.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9867.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-Shot Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-Shot Demonstration Protocol (0-shot, 1-shot, 3-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation protocol testing model performance with 0, 1, and 3 demonstrations drawn from Xiezhi-Train (2,555 examples) chosen to share at least two labels with test questions; limited to 3 shots to respect model input-length constraints and to reflect practical demonstration effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>All tested LLMs (47 models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models were evaluated under 0-, 1-, and 3-shot configurations to measure sensitivity to in-context demonstrations; GPT-4 and ChatGPT showed more stable improvements with more shots.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / Multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Insert up to 3 demonstration QA pairs before the target question in the prompt, then perform generative-probability ranking over options; compare MRR/Hit@k across shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relative change in MRR and Hit@k across 0/1/3-shot settings; stability of performance improvement with more demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Xiezhi-Train used to produce demonstrations; evaluated on Xiezhi-Specialty and Xiezhi-Interdiscipline.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Increasing the number of demonstrations generally improved average model performance, but not universally; some models degraded with more shots while GPT-4 and ChatGPT showed stable improvement, suggesting dependence on pretraining and instruction-following capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Shot selection, representativeness of demonstration examples, and limited number of shots (max 3) can affect results; some models are not robust to demonstration formatting; demonstration gains vary by model and domain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Unlike human tutoring, LLM few-shot improvements are inconsistent across models and domains; demonstration effectiveness appears correlated with model capacity and prior instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use small, domain-relevant demonstration sets (1–3) when comparing LLMs; report per-model sensitivity to shot count; fix demonstration selection and seed for reproducibility; analyze both improvements and degradation cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9867.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9867.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Top/Average Practitioner Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain-specific human performance baselines derived from published average exam scores and institutional recruit averages for the original examination sources; used to contextualize LLM performance per domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>N/A (human baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Human baselines represent 'top' and 'average' practitioner performance per domain as reported by exam authorities or institution statistics; employed for side-by-side comparison against LLM MRR/accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Domain-specific across Xiezhi categories (e.g., science, engineering, economics, law, literature, medicine)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Map questions to exam sources and use published average scores or recruit average scores as proxies for human performance; present domain-wise human top and average performance next to LLM rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exam-based scoring metrics (normalized to comparable scales); human top vs average compared with LLM MRR / accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used in conjunction with Xiezhi subsets where exam origin and published scores are available.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Findings: LLMs surpassed average human practitioners in science, engineering, agronomy, medicine and art under multiple-choice evaluation; humans (top and average) outperform LLMs in economics, jurisprudence, pedagogy, literature, history and management.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human scores are approximated from aggregated exam statistics, not from matched human participants across all 516 disciplines; may not be directly comparable to model metrics; representativeness and granularity of human baselines are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides contextual comparison showing LLMs can exceed average human performance in many technical domains on multiple-choice tasks, but substantial gaps remain in numerous social/humanities areas; cautioned that these comparisons are approximate.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When comparing to human performance, collect matched human test data where possible; present both top and average practitioner baselines; interpret comparisons cautiously due to differences in task framing and evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring Massive Multitask Language Understanding (MMLU) <em>(Rating: 2)</em></li>
                <li>C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models <em>(Rating: 2)</em></li>
                <li>M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models <em>(Rating: 2)</em></li>
                <li>BIG-bench: Beyond the imitation game: Quantifying and extrapolating the capabilities of language models <em>(Rating: 1)</em></li>
                <li>HELM: Holistic Evaluation of Language Models <em>(Rating: 1)</em></li>
                <li>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9867",
    "paper_id": "paper-eda08c6f5919f39979acf0b3bc52e903063b5ba4",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Xiezhi",
            "name_full": "Xiezhi Benchmark (Xiezhi-All / Xiezhi-Specialty / Xiezhi-Interdiscipline)",
            "brief_description": "A large, multi-disciplinary, auto-updating benchmark for domain knowledge evaluation containing 249,587 multiple-choice questions across 516 disciplines (Chinese Discipline Taxonomy) with validated subsets: Xiezhi-Meta (20,124), Xiezhi-Train (2,555), Xiezhi-Specialty (14,041) and Xiezhi-Interdiscipline (10,746).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "47 LLMs (including GPT-4, ChatGPT, Bloomz variants, Llama, Pythia, Falcon, ChatGLM, etc.)",
            "llm_description": "A diverse set of 47 state-of-the-art LLMs were evaluated, ranging from small models (~560M) to large models (tens or hundreds of billions of parameters), including both open-source and API models (GPT-4, ChatGPT).",
            "scientific_domain": "Broad / Multi-domain (516 academic disciplines spanning 13 top-level categories e.g., science, engineering, medicine, law, economics, literature, history, etc.)",
            "evaluation_method": "Multiple-choice question answering at scale using generative-probability-based ranking of 50 options per question; evaluation under 0-shot, 1-shot and 3-shot demonstration settings; aggregated metrics computed per model and per domain.",
            "evaluation_criteria": "Primary: Mean Reciprocal Rank (MRR) of the correct option among ranked 50 choices. Also reported: Hit@1, Hit@4, Mean Rank, Accuracy in supplementary analyses; comparison to human baselines (top and average practitioners) per domain.",
            "benchmark_or_dataset": "Xiezhi-All (249,587 Qs) with human-verified subsets: Xiezhi-Meta, Xiezhi-Train, Xiezhi-Specialty, Xiezhi-Interdiscipline. Questions originate from Chinese public exams and auto-generated items from academic surveys and are labeled with discipline taxonomy.",
            "results_summary": "Using MRR as the primary metric, GPT-4 and ChatGPT rank near the top (GPT-4 MRR on Xiezhi reported ≈ 0.377 in table), and LLMs exceed average human performance in domains such as science, engineering, agronomy, medicine and art, but fall short of humans in economics, jurisprudence, pedagogy, literature, history and management. Xiezhi exhibits higher variance across models and better discriminates model differences than prior benchmarks.",
            "limitations_or_challenges": "Dataset bias toward Chinese sources (cultural / language bias), some parts auto-annotated (Xiezhi-All) without full manual verification, English translations post-processed but not by native speakers, computation cost due to generative-probability ranking, experiments run with a single random seed (42) and no repeated error bars, and sensitive/Chinese-centric items were removed from public subsets but residual bias may remain.",
            "comparison_to_human_or_traditional": "Direct comparison to human baselines: authors collected published average scores for exam sources and present 'top' and 'average' practitioner scores per domain; LLMs surpass average human practitioners in several scientific/technical domains but not in many social-humanities domains. Traditional extraction-of-choice evaluations (4-way) are contrasted with Xiezhi's 50-way ranking approach.",
            "recommendations_or_best_practices": "Use broader, multi-domain, up-to-date benchmarks; adopt many-option multiple-choice (e.g., 50 choices) to reduce random-guess inflation; prefer generative-probability ranking over forcing models to output a label string; report ranking metrics (MRR, Hit@k, Mean Rank) alongside accuracy; include few-shot settings (0/1/3) and compare across them; maintain auto-updating pipelines to stay ahead of LLM training corpora.",
            "uuid": "e9867.0",
            "source_info": {
                "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GenProbRank",
            "name_full": "Generative-Probability Ranking (ranking options by model generation probability)",
            "brief_description": "An evaluation method that queries an LLM for generation probabilities for each provided option and ranks options by those probabilities instead of extracting a chosen label string; intended to fairly evaluate generative models that may not be trained to answer multiple-choice prompts directly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "45 open-source LLMs (direct probability access) + 2 API LLMs (ChatGPT, GPT-4; ranked via instruction-based ranking)",
            "llm_description": "Applied across many LLM families; for open-source models generative probabilities were read directly from model outputs, for API models ChatGPT/GPT-4 were prompted to rank options by preference.",
            "scientific_domain": "General (applied to all domain questions in Xiezhi: science, engineering, medicine, humanities, etc.)",
            "evaluation_method": "For each question, compute model's generation probability for every candidate option (50 options), then rank options descending by probability and compute ranking metrics (MRR primarily). This avoids penalizing models that can't follow multiple-choice answer-extraction prompts.",
            "evaluation_criteria": "Rank-based metrics: Mean Reciprocal Rank (primary), Hit@k, Mean Rank; supports accuracy calculation by checking Hit@1 or top-k hits.",
            "benchmark_or_dataset": "Used throughout Xiezhi evaluation (50-option setup).",
            "results_summary": "Generative-probability ranking revealed larger separations between models than standard extraction-based multiple-choice evaluation; authors report dramatic declines in absolute performance as number of options increased, but improved discriminative power. MRR used to summarize ranking quality; GPT-4 and Bloomz variants performed best in ranking.",
            "limitations_or_challenges": "Requires access to model token/sequence log-probabilities (compute- and IO-intensive), API-access models may not expose true probabilities, ranking via instruction for APIs may be less consistent than native probabilities, increased computational cost compared to single-answer extraction.",
            "comparison_to_human_or_traditional": "Compared to instruction+extraction methods (common in prior benchmarks), GenProbRank reduces evaluation bias against models not fine-tuned for multiple-choice output format and is argued to be more faithful to model internal preference distributions.",
            "recommendations_or_best_practices": "When possible, use generation probabilities to rank provided options; for API-limited models, provide standardized ranking prompts and document differences; couple ranking metrics (MRR) with Hit@k and accuracy for a fuller picture; fix random seeds and sampling of distractor options to ensure reproducibility.",
            "uuid": "e9867.1",
            "source_info": {
                "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "50-Option Setting",
            "name_full": "Expanded-Option Multiple-Choice Setting (50 options per question)",
            "brief_description": "An evaluation configuration that increases the number of candidate options per multiple-choice question to 50 (1 correct, 3 confusing distractors, 46 randomly sampled distractors) to reduce random-guessing and increase resolution between models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applied to all tested LLMs (47 models)",
            "llm_description": "The setting is model-agnostic; affects how options are assembled and evaluated for every model.",
            "scientific_domain": "General across all Xiezhi domains",
            "evaluation_method": "For each question, construct 50 options: include correct answer, add 3 carefully chosen confusing distractors, and sample 46 other options from the dataset ensuring low string overlap; then rank options by model probability (or via instructed ranking for API models) and compute MRR/Hit@k.",
            "evaluation_criteria": "Ranking metrics (MRR primary); accuracy and Hit@1 are low under this setting due to many distractors but provide stricter assessments of model confidence and discrimination.",
            "benchmark_or_dataset": "Implemented for Xiezhi-Specialty and Xiezhi-Interdiscipline evaluations.",
            "results_summary": "Performance of all LLMs dropped substantially compared to 4-option settings; however the expanded-option setting increases sensitivity to small model improvements and better separates model capabilities across domains—Xiezhi reports more informative variance across models under 50-option setup.",
            "limitations_or_challenges": "Construction of distractors can still produce non-confusing options; larger option sets increase computation and may penalize models disproportionately; sampling strategy and string-similarity heuristics affect difficulty and reproducibility.",
            "comparison_to_human_or_traditional": "Traditional 4-choice exam settings inflate random-guess baselines; 50-option setting reduces chance performance (random-guess ~0.02 instead of 0.25) and thus produces more informative comparisons to human baselines.",
            "recommendations_or_best_practices": "Use many-option setups to reduce chance-level inflation when measuring knowledge; ensure distractors are sampled with string-similarity filtering and include some close confusers; precompute and freeze option sets to ensure reproducibility; report baseline random-guess performance.",
            "uuid": "e9867.2",
            "source_info": {
                "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "MRR",
            "name_full": "Mean Reciprocal Rank (MRR) for option ranking",
            "brief_description": "A ranking-based evaluation metric computing mean of reciprocal ranks (1/rank) of the correct option; used as the primary scalar to summarize how well a model ranks the correct answer among many candidates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applied across all evaluated LLMs",
            "llm_description": "MRR was computed per model over the full test set and per-domain subsets to quantify ranking performance.",
            "scientific_domain": "General / Multi-domain (applied to Xiezhi's domain questions)",
            "evaluation_method": "For each question, find the rank position r of the correct option in the model's ranked list of 50 options; compute reciprocal (1/r); average across questions to obtain MRR (range 0..1).",
            "evaluation_criteria": "MRR (primary). Supplemented with Hit@1, Hit@4, Mean Rank, and Accuracy in appendix analyses.",
            "benchmark_or_dataset": "Primary metric for Xiezhi (used on Xiezhi-Specialty and Xiezhi-Interdiscipline and other baseline benchmarks in the paper).",
            "results_summary": "MRR revealed that GPT-4 attains the highest MRR on Xiezhi (~0.377 reported), with other top models like Bloomz-mt and ChatGPT following; MRR showed higher variance across models on Xiezhi compared to other benchmarks, making it useful for discriminative evaluation.",
            "limitations_or_challenges": "MRR emphasizes ranking position (sensitive to top ranks) but can obscure absolute correctness rates (accuracy); also different metrics (Hit@k, Mean Rank) may yield different model orderings; single-seed experiments and choice-sampling influence MRR estimates.",
            "comparison_to_human_or_traditional": "MRR enables direct comparison to human baselines in ranking terms (e.g., if humans place correct answer first more often); authors pair MRR with reported human 'top' and 'average' scores per domain to show where models exceed or lag humans.",
            "recommendations_or_best_practices": "Report MRR as a primary ranking metric when using many-candidate option tasks, supplement with Hit@k and accuracy, and analyze sensitivity to distractor construction and random seeds.",
            "uuid": "e9867.3",
            "source_info": {
                "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Few-Shot Protocol",
            "name_full": "Few-Shot Demonstration Protocol (0-shot, 1-shot, 3-shot)",
            "brief_description": "An evaluation protocol testing model performance with 0, 1, and 3 demonstrations drawn from Xiezhi-Train (2,555 examples) chosen to share at least two labels with test questions; limited to 3 shots to respect model input-length constraints and to reflect practical demonstration effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "All tested LLMs (47 models)",
            "llm_description": "Models were evaluated under 0-, 1-, and 3-shot configurations to measure sensitivity to in-context demonstrations; GPT-4 and ChatGPT showed more stable improvements with more shots.",
            "scientific_domain": "General / Multi-domain",
            "evaluation_method": "Insert up to 3 demonstration QA pairs before the target question in the prompt, then perform generative-probability ranking over options; compare MRR/Hit@k across shot settings.",
            "evaluation_criteria": "Relative change in MRR and Hit@k across 0/1/3-shot settings; stability of performance improvement with more demonstrations.",
            "benchmark_or_dataset": "Xiezhi-Train used to produce demonstrations; evaluated on Xiezhi-Specialty and Xiezhi-Interdiscipline.",
            "results_summary": "Increasing the number of demonstrations generally improved average model performance, but not universally; some models degraded with more shots while GPT-4 and ChatGPT showed stable improvement, suggesting dependence on pretraining and instruction-following capacity.",
            "limitations_or_challenges": "Shot selection, representativeness of demonstration examples, and limited number of shots (max 3) can affect results; some models are not robust to demonstration formatting; demonstration gains vary by model and domain.",
            "comparison_to_human_or_traditional": "Unlike human tutoring, LLM few-shot improvements are inconsistent across models and domains; demonstration effectiveness appears correlated with model capacity and prior instruction tuning.",
            "recommendations_or_best_practices": "Use small, domain-relevant demonstration sets (1–3) when comparing LLMs; report per-model sensitivity to shot count; fix demonstration selection and seed for reproducibility; analyze both improvements and degradation cases.",
            "uuid": "e9867.4",
            "source_info": {
                "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Human Baselines",
            "name_full": "Human Top/Average Practitioner Baselines",
            "brief_description": "Domain-specific human performance baselines derived from published average exam scores and institutional recruit averages for the original examination sources; used to contextualize LLM performance per domain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "N/A (human baseline)",
            "llm_description": "Human baselines represent 'top' and 'average' practitioner performance per domain as reported by exam authorities or institution statistics; employed for side-by-side comparison against LLM MRR/accuracy.",
            "scientific_domain": "Domain-specific across Xiezhi categories (e.g., science, engineering, economics, law, literature, medicine)",
            "evaluation_method": "Map questions to exam sources and use published average scores or recruit average scores as proxies for human performance; present domain-wise human top and average performance next to LLM rankings.",
            "evaluation_criteria": "Exam-based scoring metrics (normalized to comparable scales); human top vs average compared with LLM MRR / accuracy.",
            "benchmark_or_dataset": "Used in conjunction with Xiezhi subsets where exam origin and published scores are available.",
            "results_summary": "Findings: LLMs surpassed average human practitioners in science, engineering, agronomy, medicine and art under multiple-choice evaluation; humans (top and average) outperform LLMs in economics, jurisprudence, pedagogy, literature, history and management.",
            "limitations_or_challenges": "Human scores are approximated from aggregated exam statistics, not from matched human participants across all 516 disciplines; may not be directly comparable to model metrics; representativeness and granularity of human baselines are limited.",
            "comparison_to_human_or_traditional": "Provides contextual comparison showing LLMs can exceed average human performance in many technical domains on multiple-choice tasks, but substantial gaps remain in numerous social/humanities areas; cautioned that these comparisons are approximate.",
            "recommendations_or_best_practices": "When comparing to human performance, collect matched human test data where possible; present both top and average practitioner baselines; interpret comparisons cautiously due to differences in task framing and evaluation metrics.",
            "uuid": "e9867.5",
            "source_info": {
                "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring Massive Multitask Language Understanding (MMLU)",
            "rating": 2
        },
        {
            "paper_title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
            "rating": 2
        },
        {
            "paper_title": "M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "BIG-bench: Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "rating": 1
        },
        {
            "paper_title": "HELM: Holistic Evaluation of Language Models",
            "rating": 1
        },
        {
            "paper_title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
            "rating": 1
        }
    ],
    "cost": 0.015712249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation</h1>
<p>Zhouhong Gu ${ }^{1 <em>}$, Xiaoxuan Zhu ${ }^{1 </em>}$, Haoning Ye ${ }^{1}$, Lin Zhang ${ }^{1}$, Jianchen Wang ${ }^{1}$, Yixin Zhu ${ }^{1}$, Sihang Jiang ${ }^{1}$, Zhuozhi Xiong ${ }^{1}$, Zihan Li ${ }^{1}$, Weijie Wu ${ }^{1}$, Qianyu He ${ }^{1}$, Rui Xu ${ }^{1}$, Wenhao Huang ${ }^{1}$, Jingping Liu ${ }^{2}$, Zili Wang, Shusen Wang, Weiguo Zheng ${ }^{3}$, Hongwei Feng ${ }^{13}$, Yanghua Xiao ${ }^{1,4}{ }^{12}$,<br>${ }^{1}$ Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China<br>${ }^{2}$ School of Information Science and Engineering, East China University of Science and Technology<br>${ }^{3}$ School of Data Science, Fudan University<br>${ }^{4}$ Fudan-Aishu Cognitive Intelligence Joint Research Center<br>{zhgu22, xxzhu22}@m.fudan.edu.cn, {hwfeng, shawyh}@fudan.edu.cn</p>
<h4>Abstract</h4>
<p>New Natural Langauge Process (NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge. Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 249,587 questions and accompanied by XiezhiSpecialty with 14,041 questions and Xiezhi-Interdiscipline with 10,746 questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. All the evaluation code and data are open sourced in https://github.com/MikeGu721/XiezhiBenchmark</p>
<h2>Introduction</h2>
<p>Domain knowledge encompasses an in-depth comprehension of the world, necessitating the cultivation of various cognitive skills, such as memorization, abstraction, logical thinking, reasoning, and imagination. Human has exhibited unparalleled proficiency in domain knowledge, far exceeding any machine learning models in a long time. Nevertheless, recent advancements in Large Language Models (LLMs), including Bloom (Scao et al. 2022), Llama (Touvron et al. 2023), ChatGLM (Du et al. 2022), GPT4 (OpenAI 2023b; Bubeck et al. 2023) and so many other models, have shown remarkable capabilities in domain text understanding (Wei et al. 2022). It is time to propose more comprehensive and more prospective evaluations than before to explore whether LLMs have actually acquired knowledge, or just acquired a better imitation ability (Srivastava et al. 2022).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Constructing benchmarks is crucial for automatic evaluation as benchmarks facilitate efficient, systematic, and scalable comparisons among models. However, as LLMs continue to grow in size and complexity, they exhibit outstanding performance across a wide range of domain-specific tasks. This makes even the newly released benchmarks like MMLU (Hendrycks et al. 2021), BIG-bench (Srivastava et al. 2022) or HELM (Liang et al. 2022) all lag behind the capabilities of the LLMs quickly (Suzgun et al. 2022).</p>
<p>Considering LLMs' performance, we conclude that the benchmark used to evaluate LLMs should meet the following needs: (1) Needs to cover more tasks (Srivastava et al. 2022): Cutting-edge LLMs have integrated multiple capabilities into unified Text-to-Text transformer models (Raffel et al. 2020). Therefore, the evaluation of LLMs should focus on abilities in multiple tasks. (2) Needs to manifest the disparities among LLMs (Huang et al. 2023): Considering the emergent capacity of the models (Wei et al. 2022), it is likely that the SoTA LLMs by learning knowledge in different domains, now have a certain level of performance in all domains. To accurately evaluate the distinctions of LLMs with varying capacities, the benchmark should consider breaking down the evaluation dimensions into more detailed categories. This will allow for a more precise assessment of each model's capabilities and provide valuable insights into their relative strengths and weaknesses. (3) Needs to go ahead of the training set (Bubeck et al. 2023): As LLMs are trained on increasingly extensive corpora, newly released benchmarks may become part of the LLMs' training data much sooner than before. A prerequisite for effective evaluation is to ensure that the benchmarks are fresher than the training data used by LLMs.</p>
<p>In light of the aforementioned needs, we propose a comprehensive, multi-disciplinary, auto-updating benchmark for domain knowledge evaluation. We call this benchmark Xiezhi, named after a mythical creature that symbolizes fairness and judgement. Xiezhi consists of 249587 questions with 516 disciplines, ranging from 13 different categories: philosophy, economics, law, education, literature, history, natural sciences, engineering, agriculture, medicine, military science,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: In Chinese mythology, the Xiezhi is a legendary creature known for its ability to discern right from wrong and uphold justice. Xiezhi Benchmark encompasses 13 distinct disciplinary categories, 118 sub-disciplines, and 385 further fine-grained disciplines, aiming to provide an extensive domain taxonomy and benchmark for fair, effective, and comprehensive domain evaluation. The number adjacent to the first-level discipline signifies the number of second-level disciplines that are further divided in Chinese discipline taxonomy.</p>
<p>management, and arts. These 516 disciplines are derived from the Chinese Disciplinary Taxonomy, a comprehensive hierarchical classification system of domain knowledge proposed by the Chinese Ministry of Education and widely acknowledged in China. We manually selected and annotated 20,124 questions from the Chinese Graduate Entrance Examination covering these 516 labels to form the Xiezhi-Meta dataset. Xiezhi-Meta is used to train an annotation model capable of estimating the relevance between questions and disciplinary labels. The annotation model subsequently tag disciplinary labels to 170k multiple-choice questions originating from diverse examinations, along with 80k multiple-choice questions auto-generated from academic surveys. To facilitate the usage of Xiezhi and align with the inclination that “consolidate increasing capabilities into single LLMs”, we also present Xiezhi-Specialty and Xiezhi-Interdiscipline in both Chinese and English verision, consisting of 14,041 and 10,746 respectively more balanced, less sensitive, and less China-centric questions. Xiezhi-Specialty encompasses questions solvable using knowledge from a single domain, while Xiezhi-Interdiscipline incorporates questions necessitating knowledge from multiple domains for resolution.</p>
<p>To give more precise evaluation results, we propose a new evaluation setting in this paper. We set 50 options for each multiple-choice question, as previous researchers use only 4 options, resulting in significantly reducing the accuracy of random guessing and thus better revealing the model’s real capabilities. We rank all options of each model in generation probability, as previous researchers use instructions to query the choice made by each model, to avoid inaccurate evaluations due to model’s inability in answering multiple-choice questions or errors in the generated content extraction.</p>
<p>To provide a detailed analysis of current development status of LLMs, as well as to demonstrate the effectiveness of the Xiezhi Benchmark and our proposed evaluation process, we conduct experiments on 47 famous LLMs across four benchmarks proposed in different works in our evaluation setting. The experiments are conducted under in 0-shot, 1-shot, 3-shot demonstration setting, which is using small number of examples to demonstrate how to solve a question, with all LLMs being evaluated on both Chinese and English versions of Xiezhi. This enables us to analyze the LLM results based on their optimal performance. Results show that the best-performing LLMs, when tested via multiple-choice questions, have surpassed the level of average practitioners in science, engineering, agronomy, and medicine in multiple-choice form of . But humans still greatly outperform all LLMs in domains of economics, jurisprudence, pedagogy, literature, history, and management. We also examined the differences in performance of various LLMs across different benchmarks. Compared to existing knowledge evaluation benchmarks, Xiezhi covers the broadest range of domains, incorporates the highest quantity of questions, and consists of the most current data. As shown in our experiments, due to the vast diversity of knowledge domains covered in Xiezhi and its fifty-to-one evaluation method, even marginal improvements in any aspect of a model can be accurately assessed. As such, it is most proficient in discerning the capability differences among various LMs, spanning from GPT-4 to LLMs with only 560M parameters. Consequently, it serves as the most appropriate benchmark for evaluating LLMs of differing competencies.</p>
<h2>Related Works</h2>
<h3>Large Language Models</h3>
<p>Recently, various companies released their LLMs, such as BARD, ERNIE Bot, Bloom <em>Scao et al. (2022)</em>, pythia <em>Biderman et al. (2023)</em>, Llama <em>Touvron et al. (2023)</em>, Claude, ChatGPT <em>OpenAI (2023a)</em>, GPT-4 <em>OpenAI (2023b)</em>, and ChatGLM <em>Du et al. (2022)</em>. Apart from their outstanding performance on trained tasks, researchers have also discovered that they emerge to have strong performance on many unseen</p>
<p>tasks (Zhou et al. 2023; Chung et al. 2022). Consequently, the evaluation of LLMs’ capabilities should focus more on a wide range of tasks over numerous diverse domains and contain samples with different difficulty levels.</p>
<p>The development of LLMs has spurred the growth of a series of small-scale conversational LLMs, such as Alpaca (Taori et al. 2023), Vicuna (Chiang et al. 2023), H2Ogpt (H2O.ai 2023), and Moss (Sun et al. 2023a). Most of these small conversational LLMs are fine-tuned based on existing pre-trained LLMs through high-quality dialog data generated from LLMs (Ji et al. 2023b; Xu et al. 2023) by parameter-efficient tuning methods (Hu et al. 2021, 2023). In order to achieve excellent performance, these models continuously acquire the latest data from the internet, and their iteration speed is much faster than LLMs. Any new benchmark will quickly become outdated as it is incorporated into the model’s training data.</p>
<h2>Benchmarks for Knowledge Evaluation</h2>
<p>A number of studies concentrate on assessing a model’s knowledge and reasoning ability. Certain works, including HellaSwag (Zellers et al. 2019), Physical IQA (Bisk et al. 2020), and CosmosQA (Huang et al. 2019), focus on evaluating the understanding of LLMs’ commonsense knowledge. Meanwhile, other research, such as MMLU (Hendrycks et al. 2021), AGI-Eval (Zhong et al. 2023), MMCU (Zeng 2023), C-Eval (Huang et al. 2023), M3KE (Liu et al. 2023), LexTreme (Niklaus et al. 2023), Big-Bench (Srivastava et al. 2022) and BIG-Bench-Hard (Suzgun et al. 2022) target at evaluating the models’ proficiency in domain knowledge. However, whether these benchmarks provide effective evaluations for all language models remains debatable. This is because only LLMs with super abilities show disparities on their datasets, while small LLMs only perform at a level close to random guessing, leading to different evaluation researches having different or even contradictory results on small LLMs (Huang et al. 2023; Li et al. 2023). Furthermore, as the training corpora for models become increasingly larger, these benchmarks might lose their evaluative significance shortly after they are proposed, due to their incorporation into the training sets of LLMs.</p>
<p>Moreover, the rise of the generative LLMs presents its own difficulties in evaluation (Sai, Mohankumar, and Khapra 2022). Beginning with MMLU (Hendrycks et al. 2021), numerous works have proposed to use of multiple-choice questions to assess generative models. Recently, a variety of evaluation studies, such as SuperClue , employed an identical prompt to query all LLMs and do extraction to obtain the choice made by these LLMs. This approach requires models to have strong abilities in instruction understanding especially in multiple-choice answering, as many LLMs are unable to meet that needs, leading to unfair evaluation results.</p>
<h2>Xiezhi Benchmark</h2>
<h2>Chinese Discipline Taxonomy</h2>
<p>Chinese Discipline Taxonomy, developed by the Chinese Ministry of Education, organizes disciplines of different do-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>mains in college education. The taxonomy divides all domains into different disciplines categories and various levels of disciplines. The meanings of these levels are as follows:</p>
<p>Discipline Categories: This is the highest level of discipline taxonomy, divided according to the nature, characteristics of subjects. There are 14 subject categories in Chinese Discipline Taxonomy, including philosophy, economics, law, education, literature, history, science, engineering, agriculture, medicine, military science, management, art, and Inter-discipline.</p>
<p>First-level disciplines: A discipline category is divided into numerous first-level disciplines, each possessing relatively independent research content. For example, the "Economics" category is divided into first-level disciplines "Applied Economics" and "Theoretical Economics", and "Art Studies" consist of "Theatre &amp; File Studies", "Fine Art" and so on.</p>
<p>Second-level disciplines: These disciplines represent more subdivided areas of study or topics within the first-level discipline. For example, within the first-level discipline of "Applied Economics", further divisions include "Financial Markets", "Banking", "Insurance" and many other second-level disciplines.</p>
<p>As shown in Fig. 1, Xiezhi Benchmark consists of a total of 13 disciplinary categories, 118 first-level disciplines, and 385 second-level disciplines as question labels. The detailed information on the disciplines and the question amount used in Xiezhi Benchmark is listed in Tab. LABEL:tab:Q1 in Appendix.</p>
<h2>Dataset Construction</h2>
<p>Data collection Xiezhi consists of 249,587 questions from mainly two different sources. The first category includes nearly 170k multiple-choice questions collected from six different examinations in China: elementary school exams, middle school entrance exams, college entrance exams, undergraduate exams, graduate entrance exams, and adult education exams. These questions are all open sourced and many Chinese knowledge evaluation dataset have employed these questions (Huang et al. 2023; Liu et al. 2023). The second category comprises of nearly 80k multiple choice questions generated from Chinese open-source academic surveys or reviews, which is a result come from our auto updating method.</p>
<p>Auto Updating Our auto-updating method comprises three primary components: the construction of Xiezhi-Meta dataset, the generation of questions from open academic documents, and the automated annotation process.</p>
<p>Xiezhi-Meta
We annotated 20,124 questions collected from the Graduate Entrance Examination to form the meta version of Xiezhi through both manual efforts and chatGPT. The aim of annotation is to remove unanswerable questions and to tag each question with as many disciplines as possible.</p>
<p>We first used ChatGPT to tag each question with first or second-level disciplines in Chinese. In the process of tagging, we construct a prompt by concatenating the description of a question with its options, answers, and exam information with the description of each discipline to increase chatGPT’s</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: The figure on the right is the statistics of all questions collected by Xiezhi. The middle figure shows statistics for Xiezhi-Specialty and the left shows Xiezhi-Interdiscipline.</p>
<p>understanding of the question so that the question could be better tagged. The prompts we used is listed in Appendix Prompt, and the detail of the annotation process is described in Appendix Mannual Annotation.</p>
<h3>Question Generation</h3>
<p>Xiezhi comprises nearly 80k multiple-choice questions generated from academic surveys, as they frequently encompass well-established domain knowledge. We select Chinese academic papers across all disciplines that incorporate the terms “survey” or “review” in their titles. Subsequently, we extract several longest sentences from these surveys, which typically are the introductory sentences that contain comprehensive descriptive information pertinent to a particular field of knowledge. We identify keywords using the OpenNER method <em>Zhu et al. (2019)</em> from these sentences, which are then masked to formulate the questions. To assemble the set of options for each question, the answers to all other questions in Xiezhi were sampled and combined with the standard answers for each respective question.</p>
<h3>Auto Annotation</h3>
<p>The objectives of auto annotation include the elimination of unanswerable questions and the assignment of relevant discipline labels to each question. For unanswerable questions, we extracted keywords from the Xiezhi-Meta, such as “as shown in the figure below” or “as listed in the table” and so on, and exclude questions that contain any of these keywords from collected data. We use ChatGPT and an annotation model trained by Xiezhi-Meta to do the discipline labels tagging. The annotation model, which is based on llama-7B, is used to tag coarse-grained discipline labels (The Discipline Categories in this paper) to the questions. Based on the tagged coarse-grained labels, we employ ChatGPT to assign more fine-grained labels (First and Second-level discipline labels) to the questions, in a similar manner to the labeling of Xiezhi-Meta. The detail about the training process of the annotation model and the performance of the auto annotation process is described in Appendix Auto Annotator.</p>
<p>Xiezhi-Specialty &amp; Xiezhi-Interdiscipline To ensure the validity of the evaluation results, we further propose two additional datasets, Xiezhi-Specialty and Xiezhi-Interdiscipline in both Chinese and English version. The trajectory of LLM development tends to consolidate multiple capabilities within individual LLMs, which may consequently yield unanticipated interdisciplinary problem-solving proficiencies. The division of Xiezhi into the Specialty and Interdiscipline datasets is designed to correspond with this evolving trend. These datasets are derived from the original Xiezhi Benchmark with the exclusion of some sensitive questions (e.g., military science) and deeply Chinese-centric questions (e.g., Literary Chinese QA, ancient Chinese poetry completion). Based on a balanced sampling strategy, Xiezhi-Specialty is constructed by selecting questions involved in 3 disciplines or less, while Xiezhi-Interdiscipline includes questions tagged by 4 disciplines or more. The down-right of Fig. 3 presents an instance of the Xiezhi-Specialty, while an instance of the Xiezhi-Interdiscipline is depicted in top-right of Fig. 3. The process of translation and annotation is delineated in Appendix Manual Annotation. Furthermore, Appendix Bias, Ethical Problems and Social Impact comprehensively discusses potential ethical challenges and our effort undertaken to mitigate them.</p>
<h2>Experiments</h2>
<h3>Setup</h3>
<p>Models&amp;Device: We conducted experiments on 47 cutting-edge LLMs, the detailed descriptions of all tested LLMs are listed in Tab 11 in Appendix. Our experiments cover 45 opensource LLMs based on eight different base models: bloom, llama, moss, pythia, gpt-neox, stablelm, chatGLM and falcon. Considering the legal issues, we only show the results of two publicly recognized API-based LLMs, ChatGPT and GPT-4. Our experiment was carried out on a DGX Station with 8 80G memory Tesla A100.</p>
<p>More options: All tested LLMs need to choose the best-fit answer from 50 options for each question. Each question is</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples of a 3-shot evaluation with Xiezhi-Interdiscipline, a question from Xiezhi-Interdiscipline and a question from Xiezhi-Specialty. The red text in the left figure is the autocompleted response from the model, while the preceding text is the inputted prompt. English translations are shown below the corresponding Chinese text for better readability.
set up with 3 confusing options in addition to the correct answer, and another 46 options are randomly sampled from all options in all questions in Xiezhi. We obtain options from questions that have different discipline categories and select options that do not have any identical characters (for Chinese) or identical 4-gram characters (for English) to the ground truth. It is worth noting that it is possible to use WordNet, open source synonym databases, or other word construction methods to generate more confusing options. However, our experiments show that the performance of all LLMs declined dramatically when the number of options increased, even when using so many non-confusing options. This achieves our goal of exacerbating the performance gap between LLMs through new experimental settings and also shows that the traditional 4-choice setting has room for improvement.</p>
<p>Few-Shot Demonstration: Additionally, we aim to test the LLMs' understanding of demonstrations. Therefore, we evaluate the LLMs' capabilities under 0 -shot, 1-shot, and 3-shot settings. Although previous researches use a 5-shot setting, our experiments have much bigger options number for each question, taking the maximum input length of each LLM into consideration, we only use at most 3 examples in our few-shot learning experiments. The examples used for demonstration were obtained from Xiezhi-Train, a dataset containing 2,555 questions absent from Xiezhi-Speciality and Xiezhi-Interdiscipline, with a minimum of two labels matching the test questions, an illustration is depicted in Fig. 3.</p>
<p>Metrics: In this section, we present mainly two experiment results: the overall performance of all LLMs across various benchmarks, and the ranking of the top eight 0 -shot LLMs in 12 non-sensitive domain categories of the Xiezhi-Benchmark with the scores for top and average practitioners. For the 45 open-source models assessed in our evaluation, we calculated the probability of each model choosing every option
using generative probabilities and then ranked all options accordingly based on the probabilities. Due to legal considerations, we only display the results of two publicly recognized API-based LLMs: ChatGPT and GPT-4, and we ask them to rank all given options through instructions. To represent the results of all ranking outcomes, we employed the Mean Reciprocal Rank (MRR) as the metric in this section, which calculates the reciprocal rank of the correct answer. MRR closer to 1 indicates that the model is more capable of placing the correct answer at the front of the ranking, while it suggests that the LLM tends to place the correct answer at the bottom if it is closer to 0 . As a comparison, we also employ four different metrics and detailed them in Appendix Results on Other Metrics.</p>
<p>Randomness: To reduce the effect of randomness on our experiment, we set the random seed of some python libraries used in our experiment, which are Numpy, Random, and Torch, to 42. It is worth noting that since we used a generative probability to rank each option, this generative probability is independent of the hyperparameters to each LLMs. Nonetheless, in order to be consistent in our experiments even for details we did not notice, we still set the deterministic hyperparameters, as described in Appendix Detail Hyperparameters. Besides, Given that each question need to sample other 46 options, we constructed the set of options for each question before we started our experiment to ensure the consistency in our experiment. Also, we used string similarity during sampling to select questions that were very unlikely to be standard answers.</p>
<p>Human Performance: Since we mainly collected questions from some of the most important examinations in China, whose average scores will be released annually. Furthermore, for various academic entrance examinations, each institution will publish the average score of their recruit students. We annotate each question using the average score of the available</p>
<p>| Models | MMLU | | | CEval | | | M3KE | Xiezhi-Spec.-Chinese | | | Xiezhi-Inter.-Chinese | | | Xiezhi-Spec.-English | | | Xiezhi-Inter.-English | | | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | | 0-shot | 1-shot | 3-shot | 0-shot | 1-shot | 3-shot | 0-shot | 0-shot | 1-shot | 3-shot | 0-shot | 1-shot | 3-shot | 0-shot | 1-shot | 3-shot | 0-shot | 1-shot | 3-shot | | Random-Guess | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | 0.089 | | Generation Probability For Ranking | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Human</th>
<th></th>
<th></th>
<th>Language Models</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Top</td>
<td>Average</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Philosophy</td>
<td>0.856✓</td>
<td>0.453✗</td>
<td>ChatGPT</td>
<td>bloomo-mt</td>
<td>GPT-4</td>
<td>pythia-1.4b</td>
<td>llama-7b-hf</td>
<td>BELLE-7B-0.2M</td>
<td>BELLE-7B-1M</td>
<td>vicuna-15b-delta-v1.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.477</td>
<td>0.453</td>
<td>0.413</td>
<td>0.521</td>
<td>0.241</td>
<td>0.228</td>
<td>0.226</td>
<td>0.225</td>
<td></td>
</tr>
<tr>
<td>Economics</td>
<td>0.871✓</td>
<td>0.520✓</td>
<td>GPT-4</td>
<td>bloomo-mt</td>
<td>llama-65b-hf</td>
<td>BELLE-7B-1M</td>
<td>llama-7b-hf</td>
<td>falcon-7b</td>
<td>baize-lora-7B</td>
<td>falcon-7b-instruct</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.419</td>
<td>0.310</td>
<td>0.290</td>
<td>0.255</td>
<td>0.234</td>
<td>0.233</td>
<td>0.222</td>
<td>0.214</td>
<td></td>
</tr>
<tr>
<td>Jurisprudence</td>
<td>0.761✓</td>
<td>0.460✓</td>
<td>GPT-4</td>
<td>llama-65b-hf</td>
<td>baize-lora-7B</td>
<td>BELLE-7B-0.2M</td>
<td>ChatGPT</td>
<td>llama-7b-hf</td>
<td>BELLE-7B-1M</td>
<td>alpaca-lora-7b</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.268</td>
<td>0.323</td>
<td>0.240</td>
<td>0.251</td>
<td>0.214</td>
<td>0.241</td>
<td>0.255</td>
<td>0.255</td>
<td></td>
</tr>
<tr>
<td>Pedagogy</td>
<td>0.854✓</td>
<td>0.510✓</td>
<td>GPT-4</td>
<td>bloomo-mt</td>
<td>ChatGPT</td>
<td>BELLE-7B-0.2M</td>
<td>baize-lora-15B</td>
<td>pythia-1.4b</td>
<td>llama-65b-hf</td>
<td>BELLE-7B-1M</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.472</td>
<td>0.443</td>
<td>0.280</td>
<td>0.264</td>
<td>0.244</td>
<td>0.241</td>
<td>0.241</td>
<td>0.237</td>
<td></td>
</tr>
<tr>
<td>Literature</td>
<td>0.825✓</td>
<td>0.560✓</td>
<td>GPT-4</td>
<td>bloomo-mt</td>
<td>baize-healthcare-lora-7B</td>
<td>baize-lora-15B</td>
<td>baize-lora-7B</td>
<td>alpaca-lora-7b</td>
<td>BELLE-7B-0.2M</td>
<td>bloomo-7b</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.417</td>
<td>0.405</td>
<td>0.284</td>
<td>0.249</td>
<td>0.213</td>
<td>0.206</td>
<td>0.194</td>
<td>0.187</td>
<td></td>
</tr>
<tr>
<td>History</td>
<td>0.854✓</td>
<td>0.460✓</td>
<td>GPT-4</td>
<td>bloomo-mt</td>
<td>ChatGPT</td>
<td>BELLE-7B-0.2M</td>
<td>BELLE-7B-1M</td>
<td>baize-lora-7B</td>
<td>alpaca-lora-7b</td>
<td>baize-healthcare-lora-7B</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.437</td>
<td>0.272</td>
<td>0.233</td>
<td>0.214</td>
<td>0.207</td>
<td>0.202</td>
<td>0.192</td>
<td>0.181</td>
<td></td>
</tr>
<tr>
<td>Science</td>
<td>0.926✓</td>
<td>0.394✗</td>
<td>GPT-4</td>
<td>bloomo-mt</td>
<td>ChatGPT</td>
<td>BELLE-7B-1M</td>
<td>bloomo-7b</td>
<td>BELLE-7B-0.6M</td>
<td>BELLE-7B-0.2M</td>
<td>vicuna-7b-delta-v1.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.436</td>
<td>0.408</td>
<td>0.220</td>
<td>0.210</td>
<td>0.200</td>
<td>0.197</td>
<td>0.191</td>
<td>0.188</td>
<td></td>
</tr>
<tr>
<td>Engineering</td>
<td>0.928✓</td>
<td>0.380✗</td>
<td>GPT-4</td>
<td>ChatGPT</td>
<td>bloomo-mt</td>
<td>bloomo-7b1</td>
<td>bloomo-7b1-mt</td>
<td>falcon-7b</td>
<td>alpaca-lora-7b</td>
<td>BELLE-7B-1M</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.420</td>
<td>0.412</td>
<td>0.383</td>
<td>0.274</td>
<td>0.253</td>
<td>0.228</td>
<td>0.224</td>
<td>0.215</td>
<td></td>
</tr>
<tr>
<td>Agronomy</td>
<td>0.902✓</td>
<td>0.333✗</td>
<td>GPT-4</td>
<td>bloomo-mt</td>
<td>ChatGPT</td>
<td>bloomo-7b1-mt</td>
<td>BELLE-7B-0.2M</td>
<td>bloomo-7b1</td>
<td>bloomo-7b</td>
<td>pythia-1.4b</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.515</td>
<td>0.366</td>
<td>0.311</td>
<td>0.224</td>
<td>0.216</td>
<td>0.215</td>
<td>0.200</td>
<td>0.193</td>
<td></td>
</tr>
<tr>
<td>Medicine</td>
<td>0.805✓</td>
<td>0.430✗</td>
<td>GPT-4</td>
<td>baize-healthcare-lora-7B</td>
<td>ChatGPT</td>
<td>docmophr-6b</td>
<td>BELLE-7B-0.2M</td>
<td>bloomo-7b1</td>
<td>bloomo-7b1-mt</td>
<td>BELLE-7B-1M</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.468</td>
<td>0.278</td>
<td>0.265</td>
<td>0.253</td>
<td>0.223</td>
<td>0.222</td>
<td>0.219</td>
<td>0.210</td>
<td></td>
</tr>
<tr>
<td>Management</td>
<td>0.837✓</td>
<td>0.513✓</td>
<td>GPT-4</td>
<td>baize-lora-50B</td>
<td>pythia-2.8b</td>
<td>bloomo-p3</td>
<td>BELLE-7B-0.2M</td>
<td>baize-lora-7B</td>
<td>baize-healthcare-lora-7B</td>
<td>BELLE-7B-1M</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.390</td>
<td>0.375</td>
<td>0.367</td>
<td>0.280</td>
<td>0.268</td>
<td>0.268</td>
<td>0.263</td>
<td>0.259</td>
<td></td>
</tr>
<tr>
<td>Art Studies</td>
<td>0.821✓</td>
<td>0.400✗</td>
<td>GPT-4</td>
<td>baize-healthcare-lora-7B</td>
<td>bloomo-mt</td>
<td>ChatGPT</td>
<td>BELLE-7B-0.2M</td>
<td>baize-lora-15B</td>
<td>alpaca-lora-7b</td>
<td>moso-moon-003-base</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.437</td>
<td>0.417</td>
<td>0.337</td>
<td>0.239</td>
<td>0.228</td>
<td>0.225</td>
<td>0.227</td>
<td>0.224</td>
<td></td>
</tr>
<tr>
<td>Xiezhi</td>
<td>GPT-4</td>
<td>bloomo-mt</td>
<td>ChatGPT</td>
<td>BELLE-7B-0.2M</td>
<td>BELLE-7B-1M</td>
<td>bloomo-7b1</td>
<td>baize-lora-7B</td>
<td>bloomo-7b1-mt</td>
<td>alpaca-lora-7b</td>
<td>vicuna-7b-delta-v1.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.377</td>
<td>0.311</td>
<td>0.299</td>
<td>0.263</td>
<td>0.200</td>
<td>0.196</td>
<td>0.143</td>
<td>0.191</td>
<td></td>
</tr>
<tr>
<td>MMU/</td>
<td>GPT-4</td>
<td>Bloomo-mt</td>
<td>ChatGPT</td>
<td>baize-30b (lora)</td>
<td>Bloomo-7b1-mt</td>
<td>Bloomo-7b1</td>
<td>llama-15b</td>
<td>stathlens-7b</td>
<td>llama-65b</td>
<td>Bloomo-7b</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.240</td>
<td>0.193</td>
<td>0.188</td>
<td>0.163</td>
<td>0.166</td>
<td>0.158</td>
<td>0.143</td>
<td>0.139</td>
<td></td>
</tr>
<tr>
<td>Overall</td>
<td>0.805</td>
<td>0.266</td>
<td>GPT-4</td>
<td>0.193</td>
<td>0.188</td>
<td>0.163</td>
<td>0.166</td>
<td>0.158</td>
<td>0.143</td>
<td>0.139</td>
<td></td>
</tr>
<tr>
<td>C-Eval</td>
<td>GPT-4</td>
<td>ChatGPT</td>
<td>Bloomo-mt</td>
<td>baize-7b (lora)</td>
<td>baize-30b (lora)</td>
<td>baize-15b (lora)</td>
<td>baize-7b-healthcare (lora)</td>
<td>Bloomo-7b</td>
<td>llama-65b</td>
<td>llama-15b</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.266</td>
<td>0.194</td>
<td>0.191</td>
<td>0.164</td>
<td>0.178</td>
<td>0.166</td>
<td>0.154</td>
<td>0.151</td>
<td></td>
</tr>
<tr>
<td>M3KE</td>
<td>GPT-4</td>
<td>ChatGPT</td>
<td>baize-7b (lora)</td>
<td>baize-7b-healthcare (lora)</td>
<td>Bloomo-mt</td>
<td>llama-7b</td>
<td>baize-15b (lora)</td>
<td>alpaca-7b</td>
<td>falcon-40b-instruct</td>
<td>stathlens-7b</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.231</td>
<td>0.203</td>
<td>0.161</td>
<td>0.158</td>
<td>0.155</td>
<td>0.142</td>
<td>0.141</td>
<td>0.140</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 2: Ranking of all LLMs in each category in 0-shot setting. ✓ denotes human performance exceeds the state-of-the-art LLMs, whereas ✗ signifies LLMs have surpassed human performance.</p>
<p>shown in the “Performance-Average” in Tab. 1, the average performance of LLMs reveals that more quantity of examples results in better model performance. However, it is not an absolute guarantee that each LLM will exhibit enhanced performance in response to an increased number of demonstrations. On the contrary, several LLMs exhibit a decline in performance as the quantity of learning examples expands. In contrast, GPT-4 and ChatGPT demonstrate a more stable improvement in their performance through few-shot learning. This can be attributed to the extensive domain knowledge possessed by GPT-4 and ChatGPT, enabling them to effectively comprehend the features embedded within the learning samples.</p>
<p>Observation 3: More LLMs’ parameters don’t guarantee better performance Numerous studies have posited that an increase in the number of model parameters corresponds to an enhancement in model’s performance. This notion holds true when comparing LLMs that exhibit an order of magnitude difference in their parameters. For instance, Bloomz-mt with 146 billion parameters significantly outperforms Bloomz-560m with 560 million parameters. However, this argument does not consistently hold. For instance, Bloomz-7b1 surpasses Bloomz-p3 in the majority of domain tasks, and Pythia-1.4b outperforms other Pythia models with larger parameter counts across most benchmarks. A possible explanation for this phenomenon could be that LLMs with different parameter quantities are optimally suited to different amounts of pre-training and fine-tuning data (Hoffmann et al. 2022).</p>
<p>Observation 4: Small LMs enhance domain capabilities at the expense of generic capabilities In our experiments, we examined two medical LLMs: DoctorGLM and Baize-Healthcare. DoctorGLM originated from ChatGLM-6B, and Baize-Healthcare was derived from Llama-7B, with both models fine-tuned using medical domain text. Although both models have lower MRR compared to other LLMs fine-tuned based on the same base models, they each demonstrate high performance in medical domain. This suggests the augmentation of LLMs with fewer parameters in domain text comprehension, whether finetuned through exclusively domainspecific data or combining domain-specific and generic data, will inevitably lead to a trade-off in the understanding of generic text. This observation aligns with the findings from previous research (Fu et al. 2023; Zhao et al. 2023).</p>
<h2>Results of Benchmarks</h2>
<p>Based on the observations from Tab. 2, although the objective is to comprehensively evaluate the domain capabilities of LLMs, the various benchmarks still exhibit differing results, which indicates the different emphases of each benchmark. GPT-4, ChatGPT, and Bloomz-mt consistently rank within the top 10 across all four benchmarks, Baize-7b, and Bloomz-7b1 demonstrate remarkable abilities as they rank within the top 10 across three of the benchmarks. Furthermore, Xiezhi exhibits the highest variance among all LLMs in the "Performance-Variance" of Tab. 1, while the score of GPT-4 doesn’t always rank first like it was in other benchmark works. This indicates that the Xiezhi Benchmark excels at discerning the competence disparities among diverse LLMs and possesses the potential to appraise more potent LLMs.</p>
<h2>Conclusion</h2>
<p>We introduced Xiezhi, a new benchmark that measures how well LLMs acquire and apply domain knowledge. By covering 516 subjects ranging from 13 categories with 249,587 questions, Xiezhi proposes a taxonomy of all human knowledge and assesses language understanding of the cutting-edge 47 LLMs in greatest breadth and depth among all previous benchmarks. Our research has revealed that the SOTA LLMs have outperformed practitioner experts in several domains when evaluated by multiple-choice question answering tasks.</p>
<p>Furthermore, there is still a big gap in generic domain knowledge comprehension between larger and smaller models. Our experimental findings and the Xiezhi Benchmark we developed provide researchers with a more comprehensive understanding of their capabilities across diverse domains.</p>
<h2>Acknowledgement</h2>
<p>This work was supported by Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902). Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103). National Natural Science Foundation of China (No.62102095). National Natural Science Foundation of China (No. 62306112). National Natural Science Foundation of China (No. U23A20496).</p>
<h2>References</h2>
<p>Aribandi, V.; Tay, Y.; Schuster, T.; Rao, J.; Zheng, H. S.; Mehta, S. V.; Zhuang, H.; Tran, V. Q.; Bahri, D.; Ni, J.; et al. 2021. Ext5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv:2111.10952.
Bai, Y.; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.; Jones, A.; Chen, A.; Goldie, A.; Mirhoseini, A.; McKinnon, C.; et al. 2022. Constitutional AI: Harmlessness from AI Feedback. arXiv preprint arXiv:2212.08073.
Biderman, S.; Schoelkopf, H.; Anthony, Q.; Bradley, H.; O’Brien, K.; Hallahan, E.; Khan, M. A.; Purohit, S.; Prashanth, U. S.; Raff, E.; Skowron, A.; Sutawika, L.; and van der Wal, O. 2023. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. arXiv:2304.01373.
Bisk, Y.; Zellers, R.; Gao, J.; Choi, Y.; et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, 7432-7439.
Black, S.; Biderman, S.; Hallahan, E.; Anthony, Q.; Gao, L.; Golding, L.; He, H.; Leahy, C.; McDonell, K.; Phang, J.; Pieler, M.; Prashanth, U. S.; Purohit, S.; Reynolds, L.; Tow, J.; Wang, B.; and Weinbach, S. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model.
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S.; et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality.
Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.
Conover, M.; Hayes, M.; Mathur, A.; Meng, X.; Xie, J.; Wan, J.; Shah, S.; Ghodsi, A.; Wendell, P.; Zaharia, M.; and Xin, R. 2023. Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM. https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm.</p>
<p>Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 320-335.
Fu, Y.; Peng, H.; and Khot, T. 2022. How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu' s Notion.
Fu, Y.; Peng, H.; Ou, L.; Sabharwal, A.; and Khot, T. 2023. Specializing Smaller Language Models towards Multi-Step Reasoning. arXiv preprint arXiv:2301.12726.
H2O.ai. 2023. h2oGPT - The world's best open source GPT. https://github.com/h2oai/h2ogpt.
Hendrycks, D.; Basart, S.; Kadavath, S.; Mazeika, M.; Arora, A.; Guo, E.; Burns, C.; Puranik, S.; He, H.; Song, D.; et al. 2021. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938.
Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.; Welbl, J.; Clark, A.; et al. 2022. An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems, 35: 30016-30030.
Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.
Hu, Z.; Lan, Y.; Wang, L.; Xu, W.; Lim, E.-P.; Lee, R. K.-W.; Bing, L.; and Poria, S. 2023. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. arXiv preprint arXiv:2304.01933.
Huang, L.; Bras, R. L.; Bhagavatula, C.; and Choi, Y. 2019. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. arXiv preprint arXiv:1909.00277.
Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu, J.; Lv, C.; Zhang, Y.; Lei, J.; et al. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. arXiv preprint arXiv:2305.08322.
Ji, Y.; Deng, Y.; Gong, Y.; Peng, Y.; Niu, Q.; Zhang, L.; Ma, B.; and Li, X. 2023a. Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases. arXiv preprint arXiv:2303.14742.
Ji, Y.; Gong, Y.; Deng, Y.; Peng, Y.; Niu, Q.; Ma, B.; and Li, X. 2023b. Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation. arXiv preprint arXiv:2304.07854.
Kpf, A.; Kilcher, Y.; von R U tte, D.; Anagnostidis, S.; Tam, Z.-R.; Stevens, K.; Barhoum, A.; Duc, N. M.; Stanley, O.; Nagyfi, R.; ES, S.; Suri, S.; Glushkov, D.; Dantuluri, A.; Maguire, A.; Schuhmann, C.; Nguyen, H.; and Mattick, A. 2023. OpenAssistant Conversations - Democratizing Large Language Model Alignment. arXiv:2304.07327.
Li, H.; Zhang, Y.; Koto, F.; Yang, Y.; Zhao, H.; Gong, Y.; Duan, N.; and Baldwin, T. 2023. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212.</p>
<p>Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.
Liu, C.; Jin, R.; Ren, Y.; Yu, L.; Dong, T.; Peng, X.; Zhang, S.; Peng, J.; Zhang, P.; Lyu, Q.; et al. 2023. M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. arXiv preprint arXiv:2305.10263.
Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Biderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z.-X.; Schoelkopf, H.; Tang, X.; Radev, D.; Aji, A. F.; Almubarak, K.; Albanie, S.; Alyafeai, Z.; Webson, A.; Raff, E.; and Raffel, C. 2022. Crosslingual Generalization through Multitask Finetuning. arXiv:2211.01786.
Niklaus, J.; Matoshi, V.; Rani, P.; Galassi, A.; Stürmer, M.; and Chalkidis, I. 2023. Lextreme: A multi-lingual and multi-task benchmark for the legal domain. arXiv preprint arXiv:2301.13126.
OpenAI. 2023a. ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt.
OpenAI. 2023b. GPT-4 Technical Report. arXiv:2303.08774. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744.
Perez, E.; Ringer, S.; Lukošiūtė, K.; Nguyen, K.; Chen, E.; Heiner, S.; Pettit, C.; Olsson, C.; Kundu, S.; Kadavath, S.; et al. 2022. Discovering Language Model Behaviors with Model-Written Evaluations. arXiv preprint arXiv:2212.09251.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1): $5485-5551$.
Sai, A. B.; Mohankumar, A. K.; and Khapra, M. M. 2022. A survey of evaluation metrics used for NLG systems. ACM Computing Surveys (CSUR), 55(2): 1-39.
Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilić, S.; Hesslow, D.; Castagné, R.; Luccioni, A. S.; Yvon, F.; Gallé, M.; et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.
Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid, A.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; GarrigaAlonso, A.; et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.
StabilityAI. 2023. StableLM: Stability AI Language Models. https://github.com/Stability-AI/StableLM.
Sun, T.; Xiaotian, Z.; Zhengfu, H.; Peng, L.; Qinyuan, C.; Hang, Y.; Xiangyang, L.; Yunfan, S.; Qiong, T.; Xingjian, Z.; Ke, C.; Yining, Z.; Zhejian, Z.; Ruixiao, L.; Jun, Z.; Yunhua, Z.; Linyang, L.; Xiaogui, Y.; Lingling, W.; Zhangyue, Y.; Xuanjing, H.; and Xipeng, Q. 2023a. FudanNLP Moss.</p>
<p>Sun, T.; Zhang, X.; He, Z.; Li, P.; Cheng, Q.; Yan, H.; Liu, X.; Shao, Y.; Tang, Q.; Zhao, X.; Chen, K.; Zheng, Y.; Zhou, Z.; Li, R.; Zhan, J.; Zhou, Y.; Li, L.; Yang, X.; Wu, L.; Yin, Z.; Huang, X.; and Qiu, X. 2023b. MOSS: An open-source tool-augmented conversational language model from Fudan University. https://github.com/OpenLMLab/MOSS.
Suzgun, M.; Scales, N.; Schärli, N.; Gehrmann, S.; Tay, Y.; Chung, H. W.; Chowdhery, A.; Le, Q. V.; Chi, E. H.; Zhou, D.; et al. 2022. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https: //github.com/tatsu-lab/stanford_alpaca.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
Wang, E. J.; Alexiuk, C.; Bo, Y.; Yang, Z.; Kwok, K.; Gusev, I.; Echavez, A.; et al. 2023a. Alpaca-LoRA. https://github. com/tloen/alpaca-lora.
Wang, J.; Liang, Y.; Meng, F.; Shi, H.; Li, Z.; Xu, J.; Qu, J.; and Zhou, J. 2023b. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.
Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.
Xiong, H.; Wang, S.; Zhu, Y.; Zhao, Z.; Liu, Y.; Huang, L.; Wang, Q.; and Shen, D. 2023. DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task. arXiv:2304.01097.
Xu, C.; Guo, D.; Duan, N.; and McAuley, J. 2023. Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. arXiv preprint arXiv:2304.01196.
Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi, Y. 2019. HellaSwag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830.
Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; Tam, W. L.; Ma, Z.; Xue, Y.; Zhai, J.; Chen, W.; Liu, Z.; Zhang, P.; Dong, Y.; and Tang, J. 2023. GLM-130B: An Open Bilingual Pretrained Model. In The Eleventh International Conference on Learning Representations (ICLR).
Zeng, H. 2023. Measuring Massive Multitask Chinese Understanding. arXiv preprint arXiv:2304.12986.
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.
Zhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang, Y.; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval: A HumanCentric Benchmark for Evaluating Foundation Models. arXiv preprint arXiv:2304.06364.
Zhou, C.; Li, Q.; Li, C.; Yu, J.; Liu, Y.; Wang, G.; Zhang, K.; Ji, C.; Yan, Q.; He, L.; et al. 2023. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419.</p>
<p>Zhu, M.; Deng, Z.; Xiong, W.; Yu, M.; Zhang, M.; and Wang, W. Y. 2019. Neural Correction Model for Open-Domain Named Entity Recognition. arXiv preprint arXiv:1909.06058.</p>
<h2>Appendix</h2>
<h2>Discussion</h2>
<h2>Large Language Models Need More Benchmarks</h2>
<p>In summary, the capabilities of LLMs are manifested in three distinct aspects (Ouyang et al. 2022). And all three of these categories require benchmarks for automated evaluation. Although many benchmarks are constructed after the release of ChatGPT or GPT-4, LLMs still faced the problem of insufficient evaluation dimensions and insufficient evaluation detail because LLMs are more expressive than ever. Thus, we call upon the academic and industrial sectors to summarize human knowledge and values, providing LLM development with more effective, comprehensive, and advanced benchmarks.</p>
<p>The first capability of LLMs is the understanding of knowledge, which encompasses memorization, reasoning, and abstraction (Zhou et al. 2023). Currently, most works focus on enhancing the knowledge and understanding of LLMs through pre-training (Fu, Peng, and Khot 2022). The proposal of Xiezhi is aiming at establishing a taxonomy for human knowledge and building evaluation criteria for this field. Although Xiezhi is already the most dimensional domain evaluation benchmark with largest volume of data, we currently offer only Chinese and English language version and lacks comprehensive coverage of knowledge from different cultures and industries. In the future, one of the critical improvements for Xiezhi lies in collecting more thorough and in-depth knowledge from various countries, nations, fields, and open source benchmarks in more languages.</p>
<p>Except for knowledge evaluation, there are two other capabilities of LLMs that are in great need of benchmarks. One capacity is to understand and execute instructions, rendering LLM into a valuable artificial tool (Aribandi et al. 2021; Hoffmann et al. 2022). Instruction fine-tuning is greatly involved in many works to enhance LLM's instruction-following ability. However, the evaluation of LLM functionality largely relies on manual verification at present. Another is to align with human values, which is essential for LLMs to evolve into artificial general intelligence (AGI) (Bai et al. 2022; Perez et al. 2022). Numerous technical approaches for alignment have been proposed by companies like OpenAI and Claude, but many works have not aligned their models with human values due to the lack of direct improvement in downstream applications.</p>
<h2>Large Language Models Need Better Evaluation Methods</h2>
<p>Current language models predominantly adopt generative approaches (Zhao et al. 2023), and naturally, assessing these models presents inherent challenges (Wang et al. 2023b). Most existing evaluation methods utilize multiple-choice questions to measure a generative model's understanding of knowledge and employ extraction techniques to obtain the model's answers (Huang et al. 2023; Liu et al. 2023; Hendrycks et al. 2021).</p>
<p>We argue that this evaluation approach is a sub-optimal approach. Since this approach requires models to possess the capability to answer multiple-choice questions, a skill seldom employed in real-world applications. For small LLMs</p>
<p>or LLMs that have not been fine-tuned with multiple-choice data, such evaluation approaches fail to provide effective performance indicators.</p>
<p>In this paper, we propose evaluating models by using generative probability. While generative probability increases computational costs in comparison to directly answering questions, it yields a more accurate and effective assessment for LLMs unable to answer multiple-choice questions. Our study serves as an exploration of improved evaluation methodologies. In the future, we will consider incorporating a wider variety and diversity of evaluation approaches.</p>
<h2>Detail Hyper-parameters</h2>
<p>Our experiments involved two types of hyperparameters. The first type pertains to the seeds of random numbers used in various Python libraries, while the second type refers to the hyperparameters used when invoking the AutoCausalLM class from the transformers library for generation. The first type of hyperparameters will impact our experiments to some extent, and hence we ensured that the random number seeds were all set to 42, which are presented in Table 3, Considering that we use the generation probabilities of each options without actually generate new content, so the second type of hyperparameters would not impact our experimental results since they all effect the output from a AutoCausalLM Class. Nonetheless, to ensure consistency on details we might not have noticed, we configured our settings as demonstrated in Table 3. The code and data utilized in our study can be accessed in the CodeAndDataAppendix. Reproduction of the experiments can be achieved by simply executing the ./Tester/test.sh file contained in our code repository.</p>
<h2>Prompts</h2>
<p>We employed customized prompts in two scenarios: During the "Discipline Annotation" phase in the annotation of Xiezhi and during the "LLMs Output" phase as part of our experiment setting.</p>
<p>During the Discipline Annotation, given the existence of over 500 discipline labels, direct input of all labels will incur huge economical expense. So we used the Discipline Taxonomy for hierarchical annotation. Initially, all 13 discipline categories were used to query models and determine which categories the question should belong to. Subsequently, we selected the first-level disciplines within the chosen categories for further query, a process repeated with second-level disciplines. This annotation strategy was used to assign discipline labels to Xiezhi-Meta courtesy of ChatGPT and Xiezhi-All via both the annotation model and ChatGPT. The prompts used for these procedures are illustrated in the "ChineseVersion" section of Table 4, with the English version of the prompts also provided in the "English-Version" section of the same table for better illustration.</p>
<p>Regarding the model's prompt-based option output, we designed four distinctive prompts to cater to Chinese and English languages, and possible demonstrations, as depicted in the "LLMs Output" of Table 4.</p>
<h2>Manual Annotation</h2>
<h2>Annotators</h2>
<p>We hired and paid graduate students from various majors to annotate and clean the questions we collected. We provided training for all potential annotators and tested their understanding of the training content to ensure they fully met our requirements. The training mainly about the requirements in annotation, we will talk about these requirements in the next subsection. Our annotators comprised of ten Chinese graduate students specializing in diverse disciplines: Medicine, Literature, Economics, Science, Jurisprudence, History, Management, and Engineering. The annotators possess expertise in their respective fields of study and have written numerous academic papers in English, thereby exhibiting a high level of proficiency in the English. We keep communicate with them during the annotation process, and all of them are paid above the local minimum wage. To ensure the quality of the annotations, each sample is annotated by at least three annotators.</p>
<h2>Annotation Process in Benchmark Construction</h2>
<p>As shown in Fig. 4, we performed manual annotation at two points: the construction of Xiezhi-Meta, and the Verification of Xiezhi-Specialty and Xiezhi-Interdiscipline for different purposes.</p>
<p>For the construction of Xiezhi-Meta, the aim is to construct a high-quality Chinese domain knowledge dataset, covering as many discipline labels as possible. The goal of manual annotation primarily lies in ensuring the correctness of the questions themselves. Therefore, we ask annotators to filter the dataset of Xiezhi-Meta under the following requirements:</p>
<ol>
<li>Choose questions where answers can be determined solely based on textual content.</li>
<li>Select questions with correct answers.</li>
<li>Choose questions with rationally set options.</li>
<li>The subject annotated by ChatGPT for the question is correct, or it can be modified to be correct (this requires annotators to make modifications).
In terms of the construction of Xiezhi-Specialty and Xiezhi-Interdiscipline, the goal is to construct a dataset that meets the requirements of domain knowledge evaluation for LLMs and conforms to human values. Consequently, for these two datasets, we propose the following requirements for annotators:</li>
<li>If the question needs non-textual information to be solved, it should be removed.</li>
<li>If issue that cannot be modified was introduced during the crawling of questions, it should be removed.</li>
<li>Questions with incorrect answers should be removed.</li>
<li>Questions with unreasonably set options should be removed.</li>
<li>Questions that contain gender-biased content should be removed.</li>
<li>Questions involving sensitive content such as military matters and politics should be removed.</li>
<li>Questions that contain China's ancient texts and contemporary political content should be removed.</li>
<li>If the subject annotation is incorrect, it should be removed.</li>
</ol>
<table>
<thead>
<tr>
<th>Random Seed</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.manual_seed</td>
<td>torch.cuda.manual_seed_all</td>
<td>numpy.random.seed</td>
<td>random.seed</td>
<td>torch.backends.cudnn.deterministirc</td>
<td></td>
<td></td>
</tr>
<tr>
<td>42</td>
<td>42</td>
<td>42</td>
<td>42</td>
<td>True</td>
<td></td>
<td></td>
</tr>
<tr>
<td>AutoCausalLM</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>temperature</td>
<td>top_p</td>
<td>top_k</td>
<td>num_beams</td>
<td>max_new_token</td>
<td></td>
<td></td>
</tr>
<tr>
<td>0.95</td>
<td>0.95</td>
<td>5</td>
<td>2</td>
<td>1</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: All the parameter setting in our experiments.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An illustration of the construction process of Xiezhi.
9. Questions, and options that highly replicate the content of other questions should be removed.
10. Questions with discriminatory content should be removed.
11. If deletion of a question leads to imbalance, new questions should be re-selected from Xiezhi-All to be included.
12. The reference of male and female appellations in the dataset should be balanced.</p>
<h2>Examples in Annotation Process</h2>
<p>In this section, we provide a detailed list of real examples encountered during our manual annotation process. Firstly, we removed all the disciplines included in Tab. 5 from XiezhiSpecialty and Xiezhi-Interdiscipline. Some of these disciplines require a super deep understanding of the Chinese cultural, which does not align with current model development needs and is too difficult even for Chinese LLMs. Others involve too much military-related content, which is not suitable for open source dissemination. In addition, regarding all the requirements we outlined in the prior section, specific examples are shown in Table 6.</p>
<h2>Auto Annotator</h2>
<p>As shown in Figure 4, during the construction of the benchmark, we selected 20,124 questions from the Chinese postgraduate examination to form Xiezhi-Meta. After manual verification, these translated questions were used to train an Annotation Model, which aided in annotating all data in</p>
<p>Xiezhi-All. We refer to the trained model as the "Annotation Model", and elaborate on this model and the training details within this subsection.</p>
<p>Model and Training Process: We directly utilized the most up-to-date Llama-7B-Chinese model ${ }^{2}$, which is a model enhanced through secondary pre-training using Chinese corpora based on the basic Llama-7B. Therefore, it has robust capabilities in processing Chinese Language as shown in Tab. 1 of the overall experiments. We primarily fine-tuned the instruction on Llama-7B-Chinese. The code used for this process is the EasyLLM ${ }^{3}$ training framework available on GitHub.</p>
<p>Training Data: We constructed the data using the Chinese Version of the prompts in Table 4. Although the input questions, discipline labels and descriptions of discipline labels are mainly Chinese. We required the model to output Chinese disciplines labels relevant to the given question.</p>
<p>Annotation Performance: Our experimental study involved the analysis of 20,124 questions derived from XiezhiMeta. Three distinct strategies, namely Annotation Model, Annotation Model + ChatGPT, and ChatGPT, were employed to annotate these questions. The annotated datasets were subsequently compared to the manual validation results. In the Annotation Model + ChatGPT strategy, the Annotation</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The precision, recall, and error rates of various annotation policies.</p>
<p>Model was employed to annotate the discipline categories, and ChatGPT was applied to annotate the first and second-level disciplines by leveraging the annotated discipline categories. The evaluation metrics used were the Wrong Rate, Missing Rate, and Error Rate:</p>
<ul>
<li><strong>Wrong Rate</strong> indicates the number of incorrect annotated discipline labels, calculated as "[SET(manual labeling results) - SET(annotation results)]/#questions".</li>
<li><strong>Missing Rate</strong> is used to indicate the number of discipline labels omitted by the annotation strategy; it was calculated as "[SET(annotation results) - SET(manual labeling results)]/#questions".</li>
<li><strong>Error Rate</strong> is the summation of the Wrong Rate and Missing Rate metrics, which denotes the probability of manual involvement in the annotation process.</li>
</ul>
<p>It is important to note that multiple labels may be missed or incorrectly labeled for each question, so all the rates used here are not 0 1 metrics.</p>
<p>The results presented in Table 5 reveal that the Annotation Model has good performance in coarse-grained discipline classification but is not as effective in fine-grained discipline classification. It is observed that all three strategies aimed to ensure high precision in their outputs, resulting in low Wrong Rates. However, the Annotation Model missed many discipline labels, particularly in the first and second-level subjects. The Missing Rate of the Annotation Model + ChatGPT policy is much higher is because the mission of the discipline categories annotated by Annotation Model</p>
<p><strong>Annotation of Xiezhi-All:</strong> To ensure the quality of Xiezhi-All, we applied a combined annotation approach using Annotation Model and ChatGPT. Specifically, we used the Annotation Model to annotate the subject categories of each question, and used ChatGPT to annotate the primary subject based on the already annotated subject categories, then annotated secondary subjects based on the annotated primary subjects. This iterative annotation method can reduce the number of subjects input into ChatGPT, thereby reducing overhead. Moreover, adopting this annotation approach has significantly saved the time spent on manually filtering Xiezhi-Speciality and Xiezhi-Interdiscipline. In the future, we plan to use the annotated data to further train the Annotation Model, aiming for its performance in subject annotation tasks to approach or surpass that of ChatGPT, thereby eliminating the need for ChatGPT for annotation.</p>
<h3>Bias, Ethical Problems and Social Impact</h3>
<p>Even though we strive to avoid all possible ethical issues, we still find it challenging to guarantee the resolution of all ethical issues when it comes to constructing the largest knowledge-based evaluation benchmark in the world, with the largest number of discipline labels, largely compiled and translated from a single language. In this section, we provide a detailed description of all the potential ethical issues that may exist in our dataset, along with how we alleviate them.</p>
<h4>Bias From Multilingual Dataset Curation</h4>
<p>As a dataset constructed from Chinese sources, with the annotators being Chinese graduates, invariably, there is a level of Chinese bias. These bias may come from a large number of Chinese related questions, Chinese Polical Standards or Chinese Distinct Value, which may result in a better Chinese understanding models will have better performance in Xiezhi. We have made extensive efforts to eliminate such bias in Xiezhi. Measures include:</p>
<ul>
<li>Selecting questions from Xiezhi-All that involve various countries and regions, rather than those biased towards Chinese contexts.</li>
<li>Ensuring balanced distribution of virtual names in both Chinese and English styles, such as Li Hua in Chinese and Mike in English.</li>
<li>Deleting questions involving political stances.</li>
<li>Adding modified text like "In China, ..." to the beginning of questions involving common Chinese values.</li>
</ul>
<p>Translation Version of Xiezhi
In this paper, we used Google Translate API to translate Xiezhi-Speciality and Xiezhi-Interdisciplinary into English, followed by extensive manual post-processing. The focus of verification includes several aspects:</p>
<ul>
<li>Correcting translation errors</li>
<li>Correcting sentences with unnatural expressions</li>
<li>Making precise expression for specific terms</li>
</ul>
<p>We invited 10 annotators described in Appendix Manual Annotation to participate in the revision of the English version of Xiezhi. As all annotators are graduates who all deeply involved in writing several English papers, we are confident in their proficiency in both professional knowledge and English language use. Despite our high standards for the translated version, we still believe potential issues may exist in the current version of Xiezhi:</p>
<ul>
<li>Annotators are not native speakers living in Englishspeaking countries, so their expressions might not be perfectly idiomatic.</li>
<li>The dataset covers 516 different fields. Although the annotators are graduate students with experience in English writing, precision in translating field-specific terms could be lacking.</li>
</ul>
<h2>Human Performance</h2>
<p>We elaborated on the statistical method for human scoring in the experiment setting in Section Experiments. This scoring method compromises for the sheer reason that simply acquiring questions from one source would result in insufficient number and highly biased data. Moreover, as the content covered by each dataset varies greatly, we also found it hard to invite human participants from all 516 fields to provide human baselines. We believe the human performance scores we provided to some extent reflect human performance on Xiezhi-Specialty and Xiezzi-Interdisciplinary. However, the real-life decision-makers should not solely rely on these scores for these scores are only used as an comparison.</p>
<h2>Gender, Race, Religion, National Discrimination or Prejudice</h2>
<p>The proposed of a benchmark will act as an indicator for LLM training for a period of time, if the benchmark itself harbors discriminatory or prejudiced content, it may encourage poor development of LLMs. Therefore, in compliance with the NeurIPS dataset review standards, we eliminated all content related to gender, race, religion, and national discrimination or prejudice in the posterior process of XiezhiSpecility and Xiezhi-Interdisciplinary as far as possible. Our efforts in eliminating prejudiced content and discrimination are evident from the requirements listed in Table 6.</p>
<p>Despite this, considering most of our questions were extracted from Chinese exam papers, and a small fraction generated from English papers, with annotation undertaken by Chinese postgraduate students, there still might be potential ethical issues in the Xiezhi:</p>
<ul>
<li>As our questions come from Chinese exams and English published papers, and our annotators are Chinese graduates, their annotation may unknowingly lean towards Eastern or Western cultural notions.</li>
<li>In consideration of the uneven distribution of gender, race, faith, and nationalities that can access the original questions, we resolved the issues of gender distribution in Xiezhi-Speciality and Xiezhi-Interdisciplinay. The implicit prejudice brought by race, faith, and nationality may be more severe, and though we ensured that our existing questions do not include prejudiced content in the question description and answers, we are unable to change the overall bias in the question distribution.
The detail about how we follow the NeurIPS checklist and DataSheet is described in Appendix Checklist and Appendix Datasheet.</li>
</ul>
<h2>Results on Other Metrics</h2>
<p>Aside from the MRR score metric championed in our paper for ranking options, we have listed some other indicators to gauge the performance of different models on XiezhiSpeciality and Xiezhi-Interdisciplinay. The variants brought about by the different indicators have also been analyzed.</p>
<p>We have also considered ranking in our metrics, employing Hit@1, Hit@4, and Mean Rank as indicators. The descriptions of these are as follows: We also utilized the conventional method of calculating accuracy, a method heavily employed in other papers.</p>
<ul>
<li>Mean Rank (MR): This measures the average rank position of a query concept's true parent among all candidates, divided by the total number of options.</li>
<li>Hit@k: This is the number of query concepts whose parent is ranked in the top k positions, divided by the total number of queries.</li>
<li>Accuracy: A standard measure used in most research.</li>
</ul>
<p>Given the extensive computational cost, we carried out this experiment using only a subset of models. The results from different models, using different indicators, are presented in Tables $78 \forall 10$.</p>
<p>Our findings indicate that even when addressing the same dataset, different evaluation metrics yield different rankings. We suspect this may be because varying evaluation metrics unearth different characteristics encapsulated within the models. This is a significant factor in model evaluation and should be deeply investigated in order to draw comprehensive conclusions. Therefore, in our future work, we anticipate thoroughly researching the varying ranking results driven by different evaluation metrics.</p>
<h2>Models</h2>
<p>A comprehensive overview of the evaluated models is presented in Table 11. The "Model" column specifies the names of the analyzed models, while the "#Parameter" column indicates their respective parameters. The "Base Model" column reveals the origins of the fine-tuned models and a dash (-) signifies that it is not an instruction fine-tuned model. The number of Transformer layers utilized in each model is denoted by the "#Layer" column, and the individual encoder and decoder Transformer layers are indicated by the "#Encoder" and "#Decoder" columns, respectively. Lastly, the "#IFT Sample" column represents the quantity of instruction samples employed for instruction fine-tuning.</p>
<p>Data Sheet</p>
<h2>Motivation</h2>
<p>For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description</p>
<p>Xiezhi was created for the purpose of comprehensively evaluating the domain knowledge capabilities of large language models (LLMs). Some key gaps and needs that existing benchmarks did not adequately address:</p>
<ol>
<li>Existing benchmarks did not cover enough tasks or domains to fully assess the breadth of knowledge and capabilities of advanced LLMs.</li>
<li>Many existing benchmarks quickly became outdated as they got incorporated into the training data of the latest LLMs. There was a need for benchmarks with fresher data.</li>
<li>Most benchmarks relied on 4-option multiple choice questions. This made it too easy for models to guess correctly. More options were needed to better differentiate model capabilities.</li>
<li>Existing evaluation methods using multiple choice extractions had limitations for generative models. A better evaluation approaches are needed and Xiezhi propose to rank options by generative probability.
Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?</li>
</ol>
<p>The Knowledge Works Research Laboratory from Fudan University in China created this dataset.</p>
<p>Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.</p>
<p>The grant come from Fudan University.</p>
<h2>Composition</h2>
<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.</p>
<p>The instances that comprise the Xiezhi are multiple choice questions designed to assess domain knowledge across a wide range of disciplines with the following components:</p>
<ul>
<li>Question text: The question or problem statement.</li>
<li>Answer options: 4 possible options to choose from, with 1 correct answer and 3 near misses.</li>
<li>Correct answer: The ground truth answer out of the 4 options.</li>
<li>Subject labels: One or more labels categorizing the discipline/domain of knowledge required to answer the question correctly (516 total subjects organized hierarchically into 13 top-level categories).
How many instances are there in total (of each type, if appropriate)?</li>
</ul>
<p>Xiezhi-All consist of 249,587 questions, Xiezhi-Speciality consists of 14,041 questions, Xiezhi-Interdiscipline consists
of 10,746 questions, Xiezhi-Meta consists of 20,124 questions and Xiezhi-Trian consists of 2,555 questions.</p>
<p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).</p>
<p>Xiezhi-All contain all possible instances, but we don't tend to open source it for it is not verfied by human. Xiezhi-Meta, Xiezhi-Train, Xiezhi-Speciality and Xiezhi-Interdiscipline are subset of Xiezhi-All but undertook manual verification.</p>
<p>What data does each instance consist of? "Raw" data (e.g., unprocessed text or images)or features? In either case, please provide a description.</p>
<p>Multiple-choice questions with manual semantic annotations. Please refer to Fig. 3 for more details.</p>
<p>Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.</p>
<p>Individual instances within the Xiezhi-Meta, Xiezhi-Train, Xiezhi-Speciality, and Xiezhi-Interdiscipline datasets have undergone manual verification, resulting in a complete data set with no instances missing information. However, XiezhiAll, containing all raw data from open-source exams, has a limited amount of data missing since it has been annotated only through automatic annotation models.</p>
<p>Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.</p>
<p>Individual instances are extracted from all kinds of Chinese Examinations, so these instances may share the same level of difficulty or discipline labels to some extent.</p>
<p>Are there recommended data splits (e.g., training, development/validation, testing)?</p>
<p>Yes, we propose Xiezhi-Train of experiments in demonstration setting, and Xiezhi-Meta for model training, we also propose Xiezhi-Interdiscipline and Xiezhi-Specility for model testing.</p>
<p>Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.</p>
<p>The dataset is carefully reviewed and checked automatically and manually with a strict quality control protocol, so there will be few error or noise.</p>
<p>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,websites, tweets, other datasets)?</p>
<p>The dataset is self-contained.
Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description. No.</p>
<p>Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why</p>
<p>The dataset is carefully reviewed, all data might be offensive, insulting, threatening or might otherwise cause anxiety are excluded automatically and manually.</p>
<p>Does the dataset identify any subpopulations (e.g., by age, gender)?</p>
<p>No.
Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?</p>
<p>No, all the instances in Xiezhi are questions about domain knowledge, it is impossible to identify individuals from the dataset.</p>
<p>Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?</p>
<p>No.</p>
<h2>Collection Process</h2>
<p>How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.</p>
<p>All the questions are extracted from all kinds of Chinese examinations online or generated from Chinese academic surveys, so the answers, options and questions are all directly observable.</p>
<p>What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated?</p>
<p>We used a Python crawler to grab questions from an online site and used ChatGPT to automatically annotate the questions.</p>
<p>Who was involved in the data collection has process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?</p>
<p>The questions are collected and generated by the authors, and are manual annoated by 10 Chinese graduates. All the annotators are paid above the local minimum wage.</p>
<p>Over what timeframe was the data collected? Does this timeframe match the creation time frame of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.</p>
<p>The data was collected from March 2023 until May 2023, it match the created time of our github.</p>
<p>Were any ethical review processes conducted (e.g., by an institutional review board)?</p>
<p>We undertake a serious ethical review, please refer to Appendix Bias, Ethical Problems and Social Impact for more details.</p>
<p>Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?</p>
<p>We do not collect the data from any individuals.
Did the individuals in question consent to the collection and use of their data?</p>
<p>N/A
If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?</p>
<p>N/A
Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?</p>
<p>N/A</p>
<h2>Preprocessing / Cleaning / Labeling</h2>
<p>Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-ofspeech tagging, SIFT feature extraction, removal of instances, processing of missing values)?</p>
<p>Yes, our preprocessing process is illustrated in Fig. 4.
Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data</p>
<p>Xiezhi-All saved all the raw data, the github url will be released after the reviewing of AAAI-2024.</p>
<p>Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.</p>
<p>No.</p>
<h2>Uses</h2>
<p>Has the dataset been used for any tasks already?
No, not yet.
Is there a repository that links to any or all papers or systems that use the dataset?</p>
<p>No.
What (other) tasks could the dataset be used for?
The dataset could be used for instruction-tuning for boosting LLMs performance in domain text understanding.</p>
<p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?</p>
<p>No.</p>
<p>Are there tasks for which the dataset should not be used? If so, please provide a description.</p>
<p>No.</p>
<h2>Distribution</h2>
<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p>
<p>Yes, probably.
How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identififier (DOI)?</p>
<p>The dataset will be distributed at https://github.com/ MikeGu721/XiezhiBenchmark</p>
<p>When will the dataset be distributed?
The up-to-date dataset has been uploaded now.
Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.</p>
<p>This dataset is released under the CC BY-SA 4.0 license for general research purposes.</p>
<p>Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.</p>
<p>No.
Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.</p>
<p>No.</p>
<h2>Maintance</h2>
<p>Who is supporting / hosting / maintaining the dataset?
Cognitive Understanding Group of Knowledge Works Research Laboratory from Fudan University, China.</p>
<p>How can the owner / curator / manager of the dataset be contacted (e.g., email address)?</p>
<p>The emails of the first authors are {zhgu22, xxzhu22}@m.fudan.edu.cn, and the corrsponding authors are {hwfeng, shawyh}@fudan.edu.cn.</p>
<p>Is there an erratum?
No.
Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?</p>
<p>According to our current plans, the dataset will be updated twice a year.</p>
<p>If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?</p>
<h2>N/A.</h2>
<p>Will older versions of the dataset continue to be supported / hosted / maintained?</p>
<p>Yes, older version is still maintained and updated and will be communicated to users via Github.</p>
<p>If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?</p>
<p>We welcome people from all walks of life to use our data, and as we mentioned in Sec Discussion, the rapid development of big models requires more challenging datasets, more evaluation metrics and evaluation methods, and we are more than willing to contribute Xiezhi to this great goal.</p>
<h2>Check List</h2>
<ol>
<li>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
[Yes]
(b) Did you describe the limitations of your work?
[Yes] We describe the limitations in Section Discuss.
(c) Did you discuss any potential negative societal impacts of your work?
[Yes] We describe the limitations in Appendix Bias, Ethical Problems and Social Impact.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them?
[Yes]</li>
<li>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results?
[N/A]
(b) Did you include complete proofs of all theoretical results?
[N/A]</li>
<li>
<p>If you ran experiments (e.g. for benchmarks)...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
[Yes] We describe our experiment setting in Section Experiments and the random seed in Appendix Detail Hyper-parameters.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
[Yes] We describe the training of proposed annotation in Appendix Auto Annotator.
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
[No] The experiments consume substantial GPU resources; therefore, to ensure consistency, all our experiments were conducted under the random seed of 42.
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
[Yes] Provided in Appendix Detail Hyper-parameters.</p>
</li>
<li>
<p>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators?
[Yes] We use baseline models from Huggingface's Transformers, and detail describe and cite them in Appendix Models.
(b) Did you mention the license of the assets? [Yes]
(c) Did you include any new assets either in the supplemental material or as a URL?
[Yes] We provide details in Appendix Models.
(d) Did you discuss whether and how consent was obtained from people whose data you` re using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? $[\mathrm{N} / \mathrm{A}]$</p>
</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] The instructions are included in Appendix Manual Annotation.
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? $[\mathrm{N} / \mathrm{A}]$
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
[Yes]</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Prompts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Discipline Annotation</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chinese-Version (Used in annotation process.)</td>
<td style="text-align: center;">下面我会给你几个学科标签，然后会给你一道选择题的问题-选项和答案，请你从我给你的学科名称中为这道选择题选择合适的学科标签，如果你认为没有合适的标签，你可以回答"N/A"。 <br> 学科标签: <br> xxx: yyyyyyyy <br> xxx: yyyyyyyy</td>
</tr>
<tr>
<td style="text-align: center;">选择题问题: <br> xxxxxx <br> 选择题选项: <br> 1.xxx <br> 2.yyy <br> 选择题答案: <br> xxx <br> 请告诉我你认为这道题涉及了哪些学科标签，请严格使用给定的学科标签进行回复，不同的学科标签之间请用中文顿号" - "进行分割。</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">English-Version (Only for illustration.)</td>
<td style="text-align: center;">Below, I will provide you with some subject labels and present you with a multiple-choice question, comprising of the problem, options, and answer. Please allocate an appropriate subject label from the given set to this multiple-choice question. If you believe that there is no suitable label, you may reply with "N/A". <br> Subject Labels: <br> xxx: yyyyyyy <br> xxx: yyyyyyy</td>
</tr>
<tr>
<td style="text-align: center;">Question Description: <br> xxxxxx <br> Options: <br> 1.xxx <br> 2.yyy <br> Answer: <br> xxx <br> Please inform me of the subject labels you believe this question encompasses. Make sure to strictly use the given subject labels for your response, and separate different subject labels with the Chinese punctuation mark " - ":</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLMs Output</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">English-0-shot</td>
<td style="text-align: center;">### question description: "‘ {question}'" <br> ###all options: "‘ {options}'" <br> ### answer: "‘ {answer}'" <br> {eos}</td>
</tr>
<tr>
<td style="text-align: center;">English-few-shot</td>
<td style="text-align: center;">"‘ {demonstrations}'" <br> ### question description: "‘ {question}'" <br> ### all options: "‘ {options}'" <br> ### answer: "‘ {answer}'" <br> {eos}</td>
</tr>
<tr>
<td style="text-align: center;">Chinese-0-shot</td>
<td style="text-align: center;">### 问题描述: "‘ {question}'" <br> ### 所有选项: "‘ {options}'" <br> ### 答案: "‘ {answer}'" <br> {eos}</td>
</tr>
<tr>
<td style="text-align: center;">Chinese-few-shot</td>
<td style="text-align: center;">"‘ {demonstrations}'" <br> ### 问题描述: "‘ {question}'" <br> ### 所有选项: "‘ {options}'" <br> ### 答案: "‘ {answer}'" <br> {eos}</td>
</tr>
</tbody>
</table>
<p>Table 4: All the prompt we used in both annotation and experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Deeply Chinese Related Disciplines</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chinese Classical Literature, History of the Chinese Communist Party, Ancient Chinese Literature</td>
</tr>
<tr>
<td style="text-align: left;">Marxist Theory and Ideological and Political Education</td>
</tr>
<tr>
<td style="text-align: left;">Military Related Disciplines</td>
</tr>
<tr>
<td style="text-align: left;">Military Science, Military Political Work, Military Political Work Studies,</td>
</tr>
<tr>
<td style="text-align: left;">Military Logistics, Military logistics and military, Military Strategy, Military tactics, Contract Tactics,</td>
</tr>
<tr>
<td style="text-align: left;">War Mobilisation equipment science, Military Logistics, Rear professional logistics, Strategic Studies,</td>
</tr>
<tr>
<td style="text-align: left;">Military Equipment Studies, Military Thought and Military History, Military Thought, Tactics,</td>
</tr>
<tr>
<td style="text-align: left;">Military History, Military Equipment Studies, Military Training, Military Systems, Joint Warfare,</td>
</tr>
<tr>
<td style="text-align: left;">Military Organization, Military Management, Military Command, Operational Command,</td>
</tr>
<tr>
<td style="text-align: left;">Military Operations Research, Military Communications, Military Intelligence, Cryptology,</td>
</tr>
<tr>
<td style="text-align: left;">Military Education and Training, Campaign Studies, Military Service Campaign Studies.</td>
</tr>
</tbody>
</table>
<p>Table 5: All the subjects we delete from Xiezhi-Speciality and Xiezhi-Disciplinary.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Xiezhi-Meta</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1. Do not select questions that cannot be answered based solely on the content of the text.</td>
<td style="text-align: center;">Exclude <br> Question: <br> "Based on the given figure, ..."</td>
</tr>
<tr>
<td style="text-align: center;">2. Do not select questions with incorrect answers.</td>
<td style="text-align: center;">Exclude <br> Question: <br> "The theory of time is first proposed by ......." <br> Answer: <br> "Stephen Hawking", Real Answer: "Albert Einstein"</td>
</tr>
<tr>
<td style="text-align: center;">3. Do not select questions with improperly formed alternatives.</td>
<td style="text-align: center;">Exclude <br> Question: <br> "Who wrote 'Pride and Prejudice'?" <br> Options: <br> "Joanne Rowling, J.K. Rowling, Stephen King, ..."</td>
</tr>
<tr>
<td style="text-align: center;">4. Do not select questions where the subject markers are mislabelled and cannot be corrected.</td>
<td style="text-align: center;">Exclude <br> Question: <br> "If you go back to the Spring and Autumn Period and the Warring States Period, what you can do is $\qquad$ " <br> Labels: <br> [Ancient Literature, ...]</td>
</tr>
<tr>
<td style="text-align: center;">Xiezhi-Speciality \&amp; Xiezhi-Interdiscipline</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1. If the question needs non-textual information to be solved, it should be removed.</td>
<td style="text-align: center;">Delete <br> Question: <br> "Based on the given figure, ..."</td>
</tr>
<tr>
<td style="text-align: center;">2. If issue that cannot be modified was introduced during the crawling of questions, it should be removed.</td>
<td style="text-align: center;">Delete <br> Question: <br> ";banana $\underline{r}$ swirling css { ;chairs margin=1em auto $\underline{r}$ } upwardly sky blues."</td>
</tr>
<tr>
<td style="text-align: center;">3. Questions with incorrect answers should be removed.</td>
<td style="text-align: center;">Delete <br> Question: <br> "The theory of time is first proposed by ......." <br> Answer: <br> "Stephen Hawking", <br> Real Answer: <br> "Albert Einstein"</td>
</tr>
<tr>
<td style="text-align: center;">4. Questions with unreasonably set options should be removed.</td>
<td style="text-align: center;">Delete <br> Answer: <br> "Samurai", <br> Options: <br> "Bushi, Anchor, Archer, ....."</td>
</tr>
<tr>
<td style="text-align: center;">4. If the options are set improperly in the question, it should be deleted.</td>
<td style="text-align: center;">Question: <br> "Who wrote 'Pride and Prejudice'?" <br> Options: <br> "Joanne Rowling, J.K. Rowling, Stephen King, ..."</td>
</tr>
<tr>
<td style="text-align: center;">5. Questions that contain gender-biased content should be removed.</td>
<td style="text-align: center;">Delete <br> Question: <br> "Since men usually cook better than women ..."</td>
</tr>
<tr>
<td style="text-align: center;">6. Questions involving sensitive content such as military matters and politics should be removed.</td>
<td style="text-align: center;">Delete <br> Question: <br> "The implications of the recent nuclear agreement between ..."</td>
</tr>
<tr>
<td style="text-align: center;">7. Questions that contain China's ancient texts and contemporary political content should be removed.</td>
<td style="text-align: center;">Delete <br> Question: <br> "The next sentences of The Dao that can be spoken is not the eternal Dao is ......."</td>
</tr>
<tr>
<td style="text-align: center;">8. If the subject annotation is incorrect, it should be removed.</td>
<td style="text-align: center;">Delete <br> Question: <br> "If you go back to the Spring and Autumn Period and the Warring States Period, what you can do is $\qquad$ " <br> Labels: <br> [Ancient Literature, ...]</td>
</tr>
<tr>
<td style="text-align: center;">9. Questions, and options that highly replicate the content of other questions should be removed.</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">10. Questions with discriminatory content should be removed.</td>
<td style="text-align: center;">Delete <br> Question: <br> "People of a certain race smarter than others, ..."</td>
</tr>
<tr>
<td style="text-align: center;">11. If deletion of a question leads to imbalance, new questions should be re-selected from Xiezhi-All to be included.</td>
<td style="text-align: center;">/</td>
</tr>
<tr>
<td style="text-align: center;">12. The reference of male and female appellations in the dataset should be balanced.</td>
<td style="text-align: center;">Replace the male appellations in the sentence with female ones, while paying attention to modification of personal names.</td>
</tr>
</tbody>
</table>
<p>Table 6: Real examples in our annotation process.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://huggingface.co/LinkSoul/Chinese-Llama-2-7b
${ }^{3}$ https://github.com/MikeGu721/EasyLLM&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4975 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4975</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4975</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-63832d146f2a2706e563072cc4b59feecf4f4d01</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/63832d146f2a2706e563072cc4b59feecf4f4d01" target="_blank">PORT: Preference Optimization on Reasoning Traces</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work suggests that the path towards better language reasoning abilities goes through spending resources on creating high-quality datasets of reasoning traces, and proposes two complementary schemes for generating rejected answers: weak LLM prompting, and digit corruption.</p>
                <p><strong>Paper Abstract:</strong> Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the mathematical reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating rejected answers: weak LLM prompting, and digit corruption. Our approach leads to increased accuracy on the GSM8K and AQuA-RAT mathematical reasoning benchmarks for Falcon2-11B and Mistral-7B. Additionally, the improved abilities transfer to non-mathematical tasks, including the ARC benchmark and symbolic reasoning challenges. For example, our method can lead to up to relative 8.47% and 18.73% increases in accuracy on the GSM8K and AQuA benchmarks respectively, without any extra annotations. This work suggests that the path towards better language reasoning abilities goes through spending resources on creating high-quality datasets of reasoning traces.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4975.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4975.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon2-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon2-11B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open pre-trained autoregressive transformer foundation model used as the primary base model in this paper; fine-tuned with LoRA for supervised next-step prediction and then refined via preference optimization (DPO) on chain-of-thought traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Falcon2-11b technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon2-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer foundation model (Falcon series) used as the experiment's main base model; pre-trained and then fine-tuned using LoRA for SFT and further refined via preference-optimization (DPO).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GSM8K (math word problems), AQuA (AQuA-RAT algebra problems), ARC-Challenge (commonsense science reasoning), LastLetterConcat (symbolic string manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: grade-school multi-step arithmetic word problems with chain-of-thought rationales; AQuA: multiple-choice algebra word problems with provided rationales; ARC-Challenge: hard commonsense science multiple-choice questions; LastLetterConcat: symbolic task requiring concatenation of last letters of words.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning (SFT) on next-step chain-of-thought pairs constructed from CoT datasets (LoRA rank=64, α=16, 3 epochs), then preference optimization using Direct Preference Optimization (DPO) on triplets (prompt, chosen, rejected). Two rejected-answer generation schemes: (1) digit corruption (replace digits randomly in ground-truth steps) and (2) weak-LLM generation (generate next-step with a smaller model and optionally post-process with digit corruption). DPO hyperparams used: β=0.2, AdamW, learning rate 8e-6, 1 epoch for DPO LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Base (pretrained) accuracy: GSM8K 54.66%, AQuA 31.50%, ARC 76.11%, LastLetterConcat 16.67%. After SFT: GSM8K 55.43%, AQuA 30.71%, ARC 75.60%, LastLetterConcat 17.34%. After DPO with digit-corruption rejected answers: GSM8K 58.91% (absolute +4.25pp over base, relative +7.77%), AQuA 35.04% (absolute +3.54pp, relative +11.24%), ARC 76.02% (≈no change), LastLetterConcat 18.67% (absolute +2.00pp, relative +12.0%). Increasing preference-data (tripling digit-corruption rejected answers) yielded GSM8K 59.29% (relative +8.47% over base). Using AQuA as training source and DPO (digit corruption) gave AQuA 37.40% (relative +18.73% over base AQuA). DPO variants: DPO (58.91% GSM8K) outperformed IPO (56.40%), KTO (54.59%), ORPO (55.42%) on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SFT alone overfits to training-rationale distribution and can hurt OOD generalization (drop on AQuA and ARC observed); digit-corruption DPO did not improve ARC-Challenge (commonsense) while it improved math and symbolic tasks; weak-LLM generation can sometimes produce valid next steps (ambiguous preferences) requiring post-hoc digit-corruption; experiments limited to models ≤11B and to predominantly mathematical reasoning datasets; authors note overhead when using weak LLMs to create rejected answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>DPO (preference optimization on CoT traces) consistently outperformed vanilla SFT on in-distribution math (GSM8K) and transferred better to harder math (AQuA) and symbolic tasks; DPO outperformed variants IPO/KTO/ORPO on GSM8K in this study. Digit-corruption scheme was often stronger than weak-LLM-only scheme for GSM8K/AQuA, though mixing schemes or using larger weak LLMs (Llama-7B) improved ARC in some setups.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations: reducing SFT epochs from 3→1 improved AQuA (33.46% @1 epoch vs 30.71% @3 epochs), indicating SFT overfitting; multi-step targets (predicting multiple steps at once) reduced performance for SFT (55.95→54.81) and DPO (58.30→57.01); adding prefix 3-shot exemplars before inputs decreased DPO performance significantly (to 50.11%); post-generation digit corruption of weak-LLM outputs was essential for downstream gains; increasing number of rejected answers (dataset size) improved performance (tripling digit-corruption rejected answers → GSM8K 59.29%). DPO hyperparameter sweep found β=0.2, LR=8e-6, 1 epoch worked well for Falcon2-11B setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PORT: Preference Optimization on Reasoning Traces', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4975.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4975.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A competitive open transformer model (7B parameters) used as an alternative base model to test the robustness of preference-optimization on reasoning traces; evaluated with the same SFT and DPO pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mistral 7b</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open autoregressive transformer foundation model (Mistral family) used here as a second base model to validate robustness of methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GSM8K (mathematical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: grade-school multi-step arithmetic word problems requiring chain-of-thought style multi-step arithmetic reasoning and calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Same pipeline as for Falcon2-11B: supervised fine-tuning on CoT next-step pairs (LoRA) followed by DPO preference optimization using digit-corrupted rejected answers and/or other rejected-answer generation schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported base GSM8K accuracy for Mistral-7B = 38.51% (base). The paper reports that all tested preference-optimization approaches produced improved performance over this base on GSM8K (digit corruption scheme strongest), but exact post-finetuning numeric accuracies for Mistral variants are shown in a figure (no single-table numeric values in text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Exact numeric improvements for Mistral-7B are not tabulated in the paper; experiments described are limited in scale and the authors note they did not explore model sizes >11B.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Mistral-7B base performance is substantially lower than Falcon2-11B base (38.51% vs 54.66% on GSM8K). Preference-optimization methods improved Mistral-7B relative to its base, confirming approach robustness across base models, and digit-corruption remained a strong scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Figure-based comparisons indicate digit-corruption gives strong gains across base models, but precise numeric ablations (hyperparameter sweeps) were reported primarily on Falcon2-11B; results generalized qualitatively to Mistral-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PORT: Preference Optimization on Reasoning Traces', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4975.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4975.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-7B (weak LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-7B (used as weak LLM for rejected-answer generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open model (Llama family) used as a 'weak LLM' to generate plausible-but-wrong next-step candidates to form rejected answers in the preference dataset; sometimes post-processed with digit-corruption.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open foundation and fine-tuned chat models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter variant of the Llama family used to produce alternative next-step proposals (rejected answers) during preference-data construction; both base and chat versions were considered.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Used to create rejected answers for training preference-optimization for GSM8K, AQuA, ARC; not evaluated as the primary model on tasks but used to generate preference data that affects target model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generative next-step prediction (one-sentence) to simulate common mistakes in chain-of-thought; outputs filtered and optionally digit-corrupted to ensure invalidity.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompted with a template to produce exactly one 'Next step:' sentence; responses that did not conform were filtered out; outputs were optionally post-processed with digit corruption to ensure they were wrong. Multiple rejected answers per chosen step were produced (including (scheme) x3 experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When used to build preference data and applied via DPO to Falcon2-11B, results included: DPO (weak LLM = Llama-7B) → GSM8K 56.10%, AQuA 30.71%, ARC 77.05%. Combining Llama-7B-generated rejected answers with digit corruption improved results (Llama-7B + digit corruption → GSM8K 56.55%, AQuA 32.68%, ARC 77.47%). Tripling Llama-7B rejected answers had mixed effects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weak LLM outputs can sometimes be correct, which confounds the preference signal; authors found post-generation digit corruption important to avoid ambiguous/valid rejections. Generation overhead and filtering reduce efficiency compared to simple digit-corruption scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Llama-7B as a rejected-answer generator produced better downstream ARC gains compared to smaller Gemma-2B-it; when combined with digit-corruption it yielded improved cross-task generalization. However, simple digit-corruption of ground-truth steps was often competitive or better on GSM8K/AQuA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Using the chat version vs the base version of Llama-7B: base version with few-shot examples yielded better performance as a weak LLM in this setup. Increasing number of rejected answers from Llama sometimes helps but results are mixed; combining with digit-corruption was consistently helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PORT: Preference Optimization on Reasoning Traces', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4975.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4975.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2B-it (weak LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2B-it (instruct Gemma 2B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ~2B-parameter Gemma instruct model used as a weak LLM to generate next-step candidates for rejected answers; yielded weaker downstream improvements relative to larger weak LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gemma: Open models based on gemini research and technology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2B-it</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruct (instruct-tuned) 2B-parameter Gemma model used to generate one-step rationales as candidate rejected answers; responses filtered to start with 'Next step:'.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Used only to generate rejected answers for constructing preference datasets for improving reasoning on GSM8K, AQuA, ARC.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>One-sentence next-step generation to simulate typical mistakes; outputs used as 'rejected' examples in DPO training after filtering/post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompt template forcing single-sentence 'Next step:' output; invalid outputs were filtered; outputs were typically post-processed with digit corruption to ensure incorrectness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When used to create preference data and applied with DPO on Falcon2-11B: DPO (Gemma-2B-it) → GSM8K 53.68%, AQuA 29.92%, ARC 75.94% (lower than digit-corruption DPO and Llama-7B weak-LLM DPO in many cases). Tripling Gemma-2B-it rejected answers further lowered GSM8K performance (51.40%) in one configuration, though another mix showed AQuA gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller weak LLM produced less informative wrong-answers; more likely to produce invalid or low-quality rejections; required filtering and digit-corruption postprocessing; downstream gains were limited or negative compared to digit-corruption or Llama-7B-generated rejections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Gemma-2B-it was less effective than Llama-7B as a source of rejected answers and underperformed the simpler digit-corruption scheme for Falcon2-11B in many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Post-generation digit corruption was crucial; increasing the number of Gemma-2B-it rejected answers (x3) did not reliably improve and sometimes degraded downstream accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PORT: Preference Optimization on Reasoning Traces', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Solving math word problems with process- and outcome-based feedback <em>(Rating: 2)</em></li>
                <li>Iterative reasoning preference optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4975",
    "paper_id": "paper-63832d146f2a2706e563072cc4b59feecf4f4d01",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "Falcon2-11B",
            "name_full": "Falcon2-11B",
            "brief_description": "An open pre-trained autoregressive transformer foundation model used as the primary base model in this paper; fine-tuned with LoRA for supervised next-step prediction and then refined via preference optimization (DPO) on chain-of-thought traces.",
            "citation_title": "Falcon2-11b technical report",
            "mention_or_use": "use",
            "model_name": "Falcon2-11B",
            "model_description": "Autoregressive transformer foundation model (Falcon series) used as the experiment's main base model; pre-trained and then fine-tuned using LoRA for SFT and further refined via preference-optimization (DPO).",
            "model_size": "11B",
            "logical_reasoning_task": "GSM8K (math word problems), AQuA (AQuA-RAT algebra problems), ARC-Challenge (commonsense science reasoning), LastLetterConcat (symbolic string manipulation)",
            "task_description": "GSM8K: grade-school multi-step arithmetic word problems with chain-of-thought rationales; AQuA: multiple-choice algebra word problems with provided rationales; ARC-Challenge: hard commonsense science multiple-choice questions; LastLetterConcat: symbolic task requiring concatenation of last letters of words.",
            "method_or_approach": "Supervised fine-tuning (SFT) on next-step chain-of-thought pairs constructed from CoT datasets (LoRA rank=64, α=16, 3 epochs), then preference optimization using Direct Preference Optimization (DPO) on triplets (prompt, chosen, rejected). Two rejected-answer generation schemes: (1) digit corruption (replace digits randomly in ground-truth steps) and (2) weak-LLM generation (generate next-step with a smaller model and optionally post-process with digit corruption). DPO hyperparams used: β=0.2, AdamW, learning rate 8e-6, 1 epoch for DPO LoRA.",
            "performance": "Base (pretrained) accuracy: GSM8K 54.66%, AQuA 31.50%, ARC 76.11%, LastLetterConcat 16.67%. After SFT: GSM8K 55.43%, AQuA 30.71%, ARC 75.60%, LastLetterConcat 17.34%. After DPO with digit-corruption rejected answers: GSM8K 58.91% (absolute +4.25pp over base, relative +7.77%), AQuA 35.04% (absolute +3.54pp, relative +11.24%), ARC 76.02% (≈no change), LastLetterConcat 18.67% (absolute +2.00pp, relative +12.0%). Increasing preference-data (tripling digit-corruption rejected answers) yielded GSM8K 59.29% (relative +8.47% over base). Using AQuA as training source and DPO (digit corruption) gave AQuA 37.40% (relative +18.73% over base AQuA). DPO variants: DPO (58.91% GSM8K) outperformed IPO (56.40%), KTO (54.59%), ORPO (55.42%) on GSM8K.",
            "limitations_or_failure_cases": "SFT alone overfits to training-rationale distribution and can hurt OOD generalization (drop on AQuA and ARC observed); digit-corruption DPO did not improve ARC-Challenge (commonsense) while it improved math and symbolic tasks; weak-LLM generation can sometimes produce valid next steps (ambiguous preferences) requiring post-hoc digit-corruption; experiments limited to models ≤11B and to predominantly mathematical reasoning datasets; authors note overhead when using weak LLMs to create rejected answers.",
            "comparison": "DPO (preference optimization on CoT traces) consistently outperformed vanilla SFT on in-distribution math (GSM8K) and transferred better to harder math (AQuA) and symbolic tasks; DPO outperformed variants IPO/KTO/ORPO on GSM8K in this study. Digit-corruption scheme was often stronger than weak-LLM-only scheme for GSM8K/AQuA, though mixing schemes or using larger weak LLMs (Llama-7B) improved ARC in some setups.",
            "ablation_or_analysis_results": "Ablations: reducing SFT epochs from 3→1 improved AQuA (33.46% @1 epoch vs 30.71% @3 epochs), indicating SFT overfitting; multi-step targets (predicting multiple steps at once) reduced performance for SFT (55.95→54.81) and DPO (58.30→57.01); adding prefix 3-shot exemplars before inputs decreased DPO performance significantly (to 50.11%); post-generation digit corruption of weak-LLM outputs was essential for downstream gains; increasing number of rejected answers (dataset size) improved performance (tripling digit-corruption rejected answers → GSM8K 59.29%). DPO hyperparameter sweep found β=0.2, LR=8e-6, 1 epoch worked well for Falcon2-11B setup.",
            "uuid": "e4975.0",
            "source_info": {
                "paper_title": "PORT: Preference Optimization on Reasoning Traces",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral-7B",
            "brief_description": "A competitive open transformer model (7B parameters) used as an alternative base model to test the robustness of preference-optimization on reasoning traces; evaluated with the same SFT and DPO pipelines.",
            "citation_title": "Mistral 7b",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_description": "Open autoregressive transformer foundation model (Mistral family) used here as a second base model to validate robustness of methods.",
            "model_size": "7B",
            "logical_reasoning_task": "GSM8K (mathematical reasoning)",
            "task_description": "GSM8K: grade-school multi-step arithmetic word problems requiring chain-of-thought style multi-step arithmetic reasoning and calculation.",
            "method_or_approach": "Same pipeline as for Falcon2-11B: supervised fine-tuning on CoT next-step pairs (LoRA) followed by DPO preference optimization using digit-corrupted rejected answers and/or other rejected-answer generation schemes.",
            "performance": "Reported base GSM8K accuracy for Mistral-7B = 38.51% (base). The paper reports that all tested preference-optimization approaches produced improved performance over this base on GSM8K (digit corruption scheme strongest), but exact post-finetuning numeric accuracies for Mistral variants are shown in a figure (no single-table numeric values in text).",
            "limitations_or_failure_cases": "Exact numeric improvements for Mistral-7B are not tabulated in the paper; experiments described are limited in scale and the authors note they did not explore model sizes &gt;11B.",
            "comparison": "Mistral-7B base performance is substantially lower than Falcon2-11B base (38.51% vs 54.66% on GSM8K). Preference-optimization methods improved Mistral-7B relative to its base, confirming approach robustness across base models, and digit-corruption remained a strong scheme.",
            "ablation_or_analysis_results": "Figure-based comparisons indicate digit-corruption gives strong gains across base models, but precise numeric ablations (hyperparameter sweeps) were reported primarily on Falcon2-11B; results generalized qualitatively to Mistral-7B.",
            "uuid": "e4975.1",
            "source_info": {
                "paper_title": "PORT: Preference Optimization on Reasoning Traces",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama-7B (weak LLM)",
            "name_full": "Llama-7B (used as weak LLM for rejected-answer generation)",
            "brief_description": "A 7B open model (Llama family) used as a 'weak LLM' to generate plausible-but-wrong next-step candidates to form rejected answers in the preference dataset; sometimes post-processed with digit-corruption.",
            "citation_title": "Llama 2: Open foundation and fine-tuned chat models",
            "mention_or_use": "use",
            "model_name": "Llama-7B",
            "model_description": "7B-parameter variant of the Llama family used to produce alternative next-step proposals (rejected answers) during preference-data construction; both base and chat versions were considered.",
            "model_size": "7B",
            "logical_reasoning_task": "Used to create rejected answers for training preference-optimization for GSM8K, AQuA, ARC; not evaluated as the primary model on tasks but used to generate preference data that affects target model performance.",
            "task_description": "Generative next-step prediction (one-sentence) to simulate common mistakes in chain-of-thought; outputs filtered and optionally digit-corrupted to ensure invalidity.",
            "method_or_approach": "Prompted with a template to produce exactly one 'Next step:' sentence; responses that did not conform were filtered out; outputs were optionally post-processed with digit corruption to ensure they were wrong. Multiple rejected answers per chosen step were produced (including (scheme) x3 experiments).",
            "performance": "When used to build preference data and applied via DPO to Falcon2-11B, results included: DPO (weak LLM = Llama-7B) → GSM8K 56.10%, AQuA 30.71%, ARC 77.05%. Combining Llama-7B-generated rejected answers with digit corruption improved results (Llama-7B + digit corruption → GSM8K 56.55%, AQuA 32.68%, ARC 77.47%). Tripling Llama-7B rejected answers had mixed effects.",
            "limitations_or_failure_cases": "Weak LLM outputs can sometimes be correct, which confounds the preference signal; authors found post-generation digit corruption important to avoid ambiguous/valid rejections. Generation overhead and filtering reduce efficiency compared to simple digit-corruption scheme.",
            "comparison": "Llama-7B as a rejected-answer generator produced better downstream ARC gains compared to smaller Gemma-2B-it; when combined with digit-corruption it yielded improved cross-task generalization. However, simple digit-corruption of ground-truth steps was often competitive or better on GSM8K/AQuA.",
            "ablation_or_analysis_results": "Using the chat version vs the base version of Llama-7B: base version with few-shot examples yielded better performance as a weak LLM in this setup. Increasing number of rejected answers from Llama sometimes helps but results are mixed; combining with digit-corruption was consistently helpful.",
            "uuid": "e4975.2",
            "source_info": {
                "paper_title": "PORT: Preference Optimization on Reasoning Traces",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gemma-2B-it (weak LLM)",
            "name_full": "Gemma-2B-it (instruct Gemma 2B)",
            "brief_description": "A ~2B-parameter Gemma instruct model used as a weak LLM to generate next-step candidates for rejected answers; yielded weaker downstream improvements relative to larger weak LLMs.",
            "citation_title": "Gemma: Open models based on gemini research and technology",
            "mention_or_use": "use",
            "model_name": "Gemma-2B-it",
            "model_description": "Instruct (instruct-tuned) 2B-parameter Gemma model used to generate one-step rationales as candidate rejected answers; responses filtered to start with 'Next step:'.",
            "model_size": "2B",
            "logical_reasoning_task": "Used only to generate rejected answers for constructing preference datasets for improving reasoning on GSM8K, AQuA, ARC.",
            "task_description": "One-sentence next-step generation to simulate typical mistakes; outputs used as 'rejected' examples in DPO training after filtering/post-processing.",
            "method_or_approach": "Prompt template forcing single-sentence 'Next step:' output; invalid outputs were filtered; outputs were typically post-processed with digit corruption to ensure incorrectness.",
            "performance": "When used to create preference data and applied with DPO on Falcon2-11B: DPO (Gemma-2B-it) → GSM8K 53.68%, AQuA 29.92%, ARC 75.94% (lower than digit-corruption DPO and Llama-7B weak-LLM DPO in many cases). Tripling Gemma-2B-it rejected answers further lowered GSM8K performance (51.40%) in one configuration, though another mix showed AQuA gains.",
            "limitations_or_failure_cases": "Smaller weak LLM produced less informative wrong-answers; more likely to produce invalid or low-quality rejections; required filtering and digit-corruption postprocessing; downstream gains were limited or negative compared to digit-corruption or Llama-7B-generated rejections.",
            "comparison": "Gemma-2B-it was less effective than Llama-7B as a source of rejected answers and underperformed the simpler digit-corruption scheme for Falcon2-11B in many experiments.",
            "ablation_or_analysis_results": "Post-generation digit corruption was crucial; increasing the number of Gemma-2B-it rejected answers (x3) did not reliably improve and sometimes degraded downstream accuracy.",
            "uuid": "e4975.3",
            "source_info": {
                "paper_title": "PORT: Preference Optimization on Reasoning Traces",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Solving math word problems with process- and outcome-based feedback",
            "rating": 2,
            "sanitized_title": "solving_math_word_problems_with_process_and_outcomebased_feedback"
        },
        {
            "paper_title": "Iterative reasoning preference optimization",
            "rating": 1,
            "sanitized_title": "iterative_reasoning_preference_optimization"
        }
    ],
    "cost": 0.01604225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PORT: Preference Optimization on Reasoning Traces</h1>
<p>Salem Lahlou ${ }^{\star, \dagger}$<br>Mohamed bin Zayed University<br>of Artificial Intelligence<br>salem.lahlou@mbzuai.ac.ae</p>
<h2>Abdalgadar Abubaker* <br> Technology Innovation Institute <br> abdalgader.abubaker@tii.ae</h2>
<h2>Hakim Hacid</h2>
<p>Technology Innovation Institute
hakim.hacid@tii.ae</p>
<h4>Abstract</h4>
<p>Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-ofThought steps in order to improve the mathematical reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating rejected answers: weak LLM prompting, and digit corruption. Our approach leads to increased accuracy on the GSM8K and AQuARAT mathematical reasoning benchmarks for Falcon2-11B and Mistral-7B. Additionally, the improved abilities transfer to non-mathematical tasks, including the ARC benchmark and symbolic reasoning challenges. For example, our method can lead to up to relative $8.47 \%$ and $18.73 \%$ increases in accuracy on the GSM8K and AQuA benchmarks respectively, without any extra annotations. This work suggests that the path towards better language reasoning abilities goes through spending resources on creating high-quality datasets of reasoning traces.</p>
<h2>1 Introduction</h2>
<p>In recent years, Large Language Models (LLMs) have been pivotal in democratizing Artificial Intelligence (AI), given their ease of use and impressive abilities in a broad spectrum of tasks. While they have significantly contributed to the striking progress of AI, their success has heavily relied on scaling-up to ever-larger models and datasets. Nonetheless, scaling has not proved sufficient for achieving satisfying results on tasks involving reasoning. Reasoning has been a central theme in the history of AI, defining goal posts that push</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the limits of intelligence. The term "reasoning" is often used to refer to informal reasoning, that "relies on intuition, experience, and common sense to draw conclusions and solve problems"(Huang and Chang, 2022). The limits of scale in eliciting reasoning abilities has been confirmed by analyses in Rae et al. (2021); Bommasani et al. (2021); Cobbe et al. (2021), amongst others. One reason multistep reasoning still poses a challenge to LLMs is that the next-word prediction objective used to train them does not explicitly encourage step-by-step reasoning. Chain-of-thought prompting (CoT; Wei et al., 2022b), an augmented prompting strategy, has been shown to improve LLM performances on reasoning tasks, by guiding them to generate sequences of intermediate steps. It should be unsurprising however that solely prompting a language model to "think step by step", whether alongside a handful of correct rationales (Wei et al., 2022b) or not (Kojima et al., 2022), does not necessarily elicit actual system-2-like (Stanovich et al., 2000; Kahneman, 2003) reasoning abilities, but at best only mimics humans' thought processes. Despite claims of the type "LLMs are decent zero-shot reasoners" (Kojima et al., 2022), the emergent ability of reasoning appears consistently for very large models ( $&gt;100 B$ parameters) only (Wei et al., 2022a).</p>
<p>A major limitation of CoT prompting is its reliance on large models (Wei et al., 2022b; Kojima et al., 2022). Ho et al. (2022) propose to bypass this limitation by generating rationales from very large teacher models and using them to fine-tune smaller student models. In the same line of work, Uesato et al. (2022) perform a comprehensive comparison between outcome-based sujeversied finetuning (SFT), which supervises the final result, and process-based SFT, which supervises the reasoning process, and find that process-based supervision significantly helps language models in mathematical reasoning tasks. However, solely relying on high-quality rationales is costly as it requires hu-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the creation process of a preference dataset with two complementary approaches to generate rejected answers. The preference dataset is used to fine-tune a reference model using a Direct Preference Optimization (DPO) or one of its variants, after a supervised fine-tuning (SFT) step.</p>
<p>mans or very large language models to generate the reasoning paths. Furthermore, as evidenced by <em>Ni et al. (2023)</em>, SFT alone tends to make the language model overfit on the rationales seen during training, thus assigning low probabilities to alternative but correct reasoning paths, and as shown in <em>Hong et al. (2024)</em>, SFT can still lead the language model to assign high-probabilities to undesired sequences.</p>
<p>A notable advancement in the development of LLMs is a refinement step that elicits more favorable behaviors. This refinement step is usually performed to align AI systems with human values (Gabriel, 2020; Ji et al., 2023; Klingefjord et al., 2024). The simplest refinement strategy requires a set of demonstrations, or human-made prompt-response examples, and fine-tunes a model on the dataset using supervised fine-tuning. Preference-based approaches on the other hand, rely on datasets of comparisons of potential model outputs. They include reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022), where a reward model is learned and then optimized using the language model as a policy with reinforcement learning algorithms such as the proximal policy optimization algorithm (PPO; Schulman et al., 2017). More recently, methods that bypass both the need of explicitly modeling the reward function and the need for online interactions as in RLHF, have become increasingly popular. These include Direct Preference Optimization (DPO; Rafailov et al., 2024), Identity Preference Optimization (IPO; Azar et al., 2023), Sequence Likelihood Calibration with Human Feedback (SLIC; Zhao et al., 2023), and the prospect theory-based Kahneman-Tversky Optimization (KTO; Ethayarajh et al., 2024). Preference optimization techniques have been utilized to improve specific tasks such as summarization and stylistic continuations (Ziegler et al., 2019; Stiennon et al., 2020), but to the best of our knowledge, have never been used to tackle reasoning tasks.</p>
<p>In this paper, we propose to apply preference optimization techniques to chain-of-thought mathematical reasoning. More specifically, we propose two complementary schemes of constructing preference pairs from datasets that include valid mathematical reasoning paths, such as the GSM8K (Cobbe et al., 2021) and AQuA (Ling et al., 2017). The contributions of the paper are the following:</p>
<ul>
<li>Using Falcon2-11B (Malartic et al., 2024) as our base model, we show that the scheme that relies on corrupting digits to create wrong reasoning steps, can lead to up to 8.47% relative increase in performances on the GSM8K benchmark, and 18.73% on the AQuA benchmark.</li>
<li>We validate <em>the robustness</em> of our approach by obtaining favorable results using Mistral-7B (Jiang et al., 2023) as a base model.</li>
<li>We provide empirical evidence for the transfer abilities of our approach: fine-tuning on mathematical reasoning pairs improves commonsense and symbolic reasoning abilities as well: weak</li>
</ul>
<p>LLM prompting is useful for the ARC benchmark (Clark et al., 2018), and digit corruption is useful for the LastLetterConcat task. (Wei et al., 2022b)</p>
<ul>
<li>We compare the two schemes and various mixtures thereof and provide recommendations of which data mixtures are more susceptible to improve the reasoning abilities.</li>
<li>We compare various preference optimization schemes, and find that DPO leads to better results than its KTO and ORPO variants.
Our approach, exemplified by two schemes which requires no external data as illustrated in Figure 1, suggests that constructing high-quality chain-ofthought datasets that span a wide range of domains holds the promise of improving the emergent reasoning abilities of language models.</li>
</ul>
<h2>2 PORT: Preference optimization on reasoning traces</h2>
<h3>2.1 Problem setup</h3>
<p>Starting from a finite set of tokens $\mathcal{V}$, called hereafter the vocabulary, an autoregressive language model can be seen as a collection of probability distributions $p_{\mathrm{LM}}$ over $\mathcal{V}$ conditioned on elements of $\mathcal{V}^{\leq \tau}:=\bigcup_{t=1}^{\tau} \mathcal{V}^{t}$, i.e. sequences of up to $\tau$ tokens. We assume the existence of an end-of-sentence (EOS) token in $\mathcal{V}$, denoted EOS, that can represent a full stop or a line-break for example.
To generate text, a pre-trained language model prompted with an input $q \in \mathcal{V}^{\leq \tau}$ is queried autoregressively and samples tokens $s_{i} \in \mathcal{V}$, where $s_{i} \sim p_{\mathrm{LM}}\left(. \mid q s_{1} \ldots s_{i-1}\right)$. The generation process stops at the first index $k$ for which $s_{k}=\mathrm{EOS}^{1}$. Here, $q s_{1} \ldots s_{i-1}$ refers to the concatenation of the tokens $q, s_{1}, \ldots, s_{i-1}$.
When interacting with language models, and more specifically in CoT reasoning, we are interested in generating sentences $z$, i.e. sequences from $\mathcal{V}^{\leq T}$ that end with the EOS token, rather than an arbitrary amount of tokens. When prompted with a sentence $x$ or a sequence of sentences $x z^{1} z^{2} \ldots z^{k-1}$, the language model can therefore autoregressively generate a new sentence $z^{k} \sim$ $p_{\mathrm{LM}}\left(. \mid x z^{1} z^{2} \ldots z^{k-1}\right)$.
Given a question $x$ (e.g., a math problem), we define a chain-of-thought as a sequence of $n$ sentences $z^{1}, \ldots, z^{n}$, where $z^{n}$ is the final answer. Assuming the existence of a binary function $(x, z) \mapsto$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$\eta(x, z)$ that assesses the correctness of the sentence $z$ to the question $x$, our goal is to tune a pre-trained model $p_{\mathrm{LM}}$ to generate a chain $z^{1}, \ldots, z^{n}$ from a question $x$ such that $\eta\left(x, z^{a}\right)=1$.</p>
<h3>2.2 Proposed approach</h3>
<p>Our approach first requires access to a dataset of reasoning traces, called a CoT dataset, $\mathcal{D}<em i="i">{\text {train }}=$ $\left{\left(x</em>\right)\right}}, z_{i}^{1}, \ldots, z_{i}^{n_{i}<em i="i">{i=1}^{N}$, where each training example includes a question $x</em>$ depends on the question itself.
Such datasets $\mathcal{D}_{\text {train }}$ are generally human-made. Examples of publicly available reasoning datasets include the arithmetic datasets GSM8K (Cobbe et al., 2021), AQuA-RAT (Ling et al., 2017), MAWPS (Koncel-Kedziorski et al., 2016) as well as the commonsense reasoning datasets StrategyQA (Geva et al., 2021), Creak (Onoe et al., 2021), e-SNLI (Camburu et al., 2018), ECQA (Aggarwal et al., 2021), QASC (Khot et al., 2019), QED (Lamm et al., 2021), Sen-Making (Wang et al., 2019).}$, and a reasoning trace (or rationale) comprised of $n_{i}$ sentences, $z_{i}^{1}, \ldots, z_{i}^{n_{i}}$, of which the last element, $z_{i}^{n_{i}}$ is a valid answer to the question $x_{i}$, i.e., $\eta\left(x_{i}, z_{i}^{n_{i}}\right)=1$. Naturally, the number of steps $n_{i}$ needed to reach the answer to $x_{i</p>
<p>SFT data: From such a dataset, we can construct a dataset $\mathcal{D}<em i="i">{\text {train }}^{\mathrm{SFT}}$ of prompt-response pairs, where each example $\left(x</em>\right)$.
Such a dataset can be used for supervised finetuning, during which the parameters $\boldsymbol{\theta}$ of the base language model $p_{\mathrm{LM}}$ are updated to minimize the SFT loss:}, z_{i}^{1}, \ldots, z_{i}^{n_{i}}\right)$ contributes $n_{i}$ pairs: $\left(x_{i}, z_{i}^{1}\right),\left(x_{i} z_{i}^{1}, z_{i}^{2}\right), \ldots,\left(x_{i} z_{i}^{1} \ldots z_{i}^{n_{i}-1}, z_{i}^{n_{i}</p>
<p>$$
\mathcal{L}<em _mathrm_SFT="\mathrm{SFT">{\mathrm{SFT}}(\boldsymbol{\theta})=-\frac{1}{N</em>\right)
$$}}} \sum_{i=1}^{N} \sum_{k=1}^{n_{i}} p_{\boldsymbol{\theta}}\left(z_{i}^{k} \mid x_{i} z_{i}^{1: k-1</p>
<p>where $N_{\mathrm{SFT}}=\sum_{i=1}^{N} n_{i}=|\mathcal{D}<em i="i">{\text {train }}^{\mathrm{SFT}}|$. In principle, if the data is representative of the target task and if the model generalizes well, the SFT phase should increase the likelihood of valid reasoning steps. Put differently, because each $z</em>$, then after supervised-fine-tuning, on similar examples, the model should encourage the sentences that unroll the reasoning and help discover a valid answer to the initial question.}^{k}$ in the training dataset is a step towards a valid answer $z_{i}^{n_{i}</p>
<p>Preference data: From $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}^{\mathrm{SFT}}$, we also construct a preference dataset $\mathcal{D}</em>$, comprised of triplets of the form (prompt, chosen, rejected). The}}^{\text {pref }</p>
<p>prompt and the chosen answers are obtained directly from $\mathcal{D}_{\text {train }}^{\text {SFT }}$, but for each prompt (which is actually either a question or a concatenation of a question and a certain number of initial reasoning steps), we need an invalid reasoning step. Naturally, an arbitrary sequence of tokens would be invalid, but it will provide no useful signal to the model if it is fine-tuned with RLHF or with preference optimization methods such as DPO using such a preference a dataset. Ideally, the rejected answers should be almost correct reasoning steps, or contain errors that either a language model or a human are expected to make. Naturally, the rejected answers can be obtained using human annotators explicitly asked to generate wrong but close-enough answers. In this work however, we investigate two simple and complementary ways of defining such a dataset:</p>
<ul>
<li>LLM generation: For each pair $\left(x_{i} z_{i}^{1: k-1}, z_{i}^{k}\right)$ from $\mathcal{D}<em i="i">{\text {train }}^{\text {SFT }}$, we prompt a smaller language model (hereafter also referred to as weak LLM) with $x</em>$.} z_{i}^{1: k-1}$ and use the response to define the corresponding rejected answer $\hat{z}^{k}$. By incorporating the resulting triplet in the preference dataset, we naturally incentivize the base model to avoid errors of the type made by the weak LLM. This process can be used to generate multiple rejected answers $\hat{z}^{k}$ per prompt $x_{i} z_{i}^{1: k-1</li>
<li>Digit corruption: In datasets that involve mathematical reasoning, most reasoning steps $z^{k}$ include digits. Without modifying any non-digit character of $z^{k}$, we replace each digit with one from 0 to 9 with equal probability. Similarly, this approach can be used to generate multiple rejected answers $\hat{z}^{k}$ per prompt $x_{i} z_{i}^{1: k-1}$.
An illustration of this dataset creation process is provided in Table 5 of Appendix A.
After an SFT phase where $p_{\mathrm{LM}}$ is fine-tuned into $p_{\text {SFT }}$ using $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}^{\text {SFT }}$ by minimizing the loss in (1), any preference optimization method can be used on $\mathcal{D}</em>$ that minimizes the following loss:}}^{\text {pref }}$. For instance, DPO (Rafailov et al., 2024) fine-tunes $p_{\mathrm{SFT}}$ into a model $p_{\boldsymbol{\theta}</li>
</ul>
<p>$$
\mathcal{L}<em _left_x_="\left(x," y__w="y_{w">{\mathrm{DPO}}(\boldsymbol{\theta})=-\mathbb{E}</em>}, y_{l}\right) \sim \mathcal{D<em w="w">{\text {train }}^{\text {pref }}}\left[l\left(x, y</em>\right)\right]
$$}, y_{l} ; \boldsymbol{\theta</p>
<p>where</p>
<p>$$
\begin{aligned}
l\left(x, y_{w}, y_{l} ; \boldsymbol{\theta}\right)=\log \sigma &amp; \left(\beta \log \frac{p_{\boldsymbol{\theta}}\left(y_{w} \mid x\right)}{p_{\mathrm{SFT}}\left(y_{w} \mid x\right)}-\right. \
&amp; \left.\beta \log \frac{p_{\boldsymbol{\theta}}\left(y_{l} \mid x\right)}{p_{\mathrm{SFT}}\left(y_{l} \mid x\right)}\right)
\end{aligned}
$$</p>
<p>Here $\sigma$ is the sigmoid function and $\beta$ is a scaling hyperparameter.</p>
<h2>3 Experiments</h2>
<p>The goal of the experiments presented in this section are threefold. First, we empirically investigate the proposed approach, and show that unlike SFT, it is less prone to task overfitting. Then, we compare the two schemes above for constructing rejected answers, along with different combinations thereof. Next, we investigate the effect of the preference optimization method by comparing DPO to some of its variants. Finally, we validate the robustness of our method to both the base model and the training dataset.</p>
<p>Evaluation: To assess the approach, we need to evaluate the models on informal reasoning tasks, for which chains of thoughts can help reach the valid answers. Our main evaluation task is the GSM8K test dataset (Cobbe et al., 2021), that contains 1319 high quality grade school math word problems. To assess the transfer abilities of our approach, we also consider the three following evaluation datasets:</p>
<ul>
<li>The Algebra Question Answering with Rationales dataset(AQuA; Ling et al., 2017), which is a harder math word problem that includes approximately 100,000 algebraic word problems, each presented with a rationale leading to one of five multiple-choice options (A to E). We use the accompanying test set of 254 examples for evaluation.</li>
<li>The AI2's Reasoning Challenge(ARC; Clark et al., 2018) which is a commonsense reasoning benchmark covering multiple science subjects. The questions are split into Easy and Challenge sets. Questions in the Challenge set cannot be solved with retrieval or co-occurence methods. Each question admits one valid answer amongst a set of typically four options. We solely focus on the Challenge part. We use the test set of the ARC-Challenge set, that consists of 1172 examples, for evaluation.</li>
<li>The LastLetterConcat dataset (Wei et al., 2022b) which is a symbolic reasoning task where the goal is to join together the last letters of individual words. The dataset contains a total of 500 examples.
More specifically, we use the Language Model Evaluation Harness (Gao et al., 2023) to calculate the accuracy (between 0 and 1) of the tested models.</li>
</ul>
<p>To elicit the desired CoT behavior, we add few-shot examples from the train set that contain rationales to each question to be evaluated, extract from the generated text the proposed answer, and compare it to the ground truth. We report our results below as percentages. We use 5-shot examples for GSM8K, AQuA, and LastLetterConcat. For ARC, we use 25-shot examples, but given that no rationale is provided in the train set, we use GPT-4 (et al., 2023) to construct plausible rationales, and filter them out manually. The used prompt is provided in Appendix B.</p>
<p>Base model: As a base model, we use the newly released pre-trained Falcon2-11B (Malartic et al., 2024) for all our experiments, except in Section 3.6, where we confirm that our method is agnostic to the base model by using Mistral-7B (Jiang et al., 2023).</p>
<p>Training data: As previously mentioned, our method requires using a CoT dataset. For all our experiments, except in Section 3.7, we use the GSM8K train dataset (Cobbe et al., 2021), that consists of 7473 examples, given that it contains solution steps, in order to construct the SFT and preference datasets $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}^{\text {SFT }}$ and $\mathcal{D}</em>\right)$ from this dataset are provided in Appendix A. It is important to note that throughout the training and evaluation process, we only use the training set, without any extra data or human annotation.}}^{\text {pref }}$, as explained in Section 2.2. Example tuples $\left(x_{i}, z_{i}^{1}, \ldots, z_{i}^{n_{i}</p>
<h3>3.1 Supervised fine-tuning</h3>
<p>From the GSM8K training dataset, we construct the SFT dataset $\mathcal{D}_{\text {train }}^{\text {SFT }}$ as described in Section 2.2. The train set of GSM8K consists of 7473 examples, with an average of 4.57 reasoning step per example, leading to to an SFT dataset of 34197 examples. We then fine-tune the based model on this dataset using low-rank adaptation (LoRA; Hu et al., 2021) for efficient parameter updates, processing each example 3 times. The learning rate used in $1.4 \times 10^{-5}$, and the batch size is 16 . For LoRA, we use rank 64 matrices and a scaling parameter $\alpha=16$. It is noteworthy that GSM8K examples contain calculation annotations (between «», as shown in the examples provided in Appendix A). These annotations can be used to call external tools (e.g., python scripts or calculators) to perform calculations, rather than asking the LLM to perform the calculation. While we made no such usage of external tools, we tried both keeping and removing the annotations from</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">GSM8K</th>
<th style="text-align: left;">AQuA</th>
<th style="text-align: left;">ARC</th>
<th style="text-align: left;">LastLetterConcat</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base model</td>
<td style="text-align: left;">54.66</td>
<td style="text-align: left;">31.50</td>
<td style="text-align: left;">76.11</td>
<td style="text-align: left;">16.67</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: left;">55.43</td>
<td style="text-align: left;">30.71</td>
<td style="text-align: left;">75.60</td>
<td style="text-align: left;">17.34</td>
</tr>
<tr>
<td style="text-align: left;">DPO (ours)</td>
<td style="text-align: left;">$\mathbf{5 8 . 9 1}$</td>
<td style="text-align: left;">$\mathbf{3 5 . 0 4}$</td>
<td style="text-align: left;">76.02</td>
<td style="text-align: left;">$\mathbf{1 8 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy (in percentage) of the base, SFT, and DPO models on the three considered tasks. For both SFT and DPO, the Falcon2-11B base model is finetuned on datasets obtained from the GSM8K training set. The rejected answers for DPO are obtained using digit corruption, as explained in Section 3.2
the text before SFT, and found no significant difference in terms of performance. We thus decided to process the dataset without annotations.
Details about the choice of the hyperparameters are provided in Appendix C.</p>
<p>Results: In addition to testing the fine-tuned model on the GSM8K's test set, we assess SFT's out-of-distribution generalization, on the harder math word problem AQuA, and on the nonmathematical tasks ARC and LastLetterConcat. We report the accuracies in Table 1. As expected, and as confirmed by other studies (Uesato et al., 2022), fine-tuning the model on the reasoning steps helps improve the performances on questions requiring reasoning that come from the same distribution. The performances on AQuA and ARCChallenge drop after the SFT stage, confirming the overfitting issues of SFT, and their limited generalization to unseen examples (Ni et al., 2023). This is also confirmed by an additional experiment shown in Table 6 in Appendix D, where we reduce the number of training epochs (on GSM8K) and observe better performances on AQuA.</p>
<p>In the next subsections, we investigate whether preference optimization algorithms can lead to even further performance boosts on the three evaluation tasks.</p>
<h3>3.2 Preference optimization with digit corruption</h3>
<p>From $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}^{\text {SFT }}$, we construct a preference dataset $\mathcal{D}</em>$ using digit corruption as explained in Section 2.2. Given the stochasticity of the digit corruption approach, we ensure that the rejected answers are indeed invalid, by repeatedly generating reasoning steps until they differ from the ground truth reasoning steps. For reasoning steps that do not include digits, we simply do not include them in the preference dataset.}}^{\text {pref }</p>
<p>We fine-tune the SFT model on the obtained</p>
<p>preference dataset using DPO with a scale factor $\beta=0.2$, with the same LoRA configuration as SFT. We use the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of $8 \times 10^{-6}$ along with a linear schedule for the learning rate. This choice of hyperparameters is explained in Appendix C.</p>
<p>Results: We report the accuracies post DPO tuning in Table 1. The significant performance increase in GSM8K (a relative $7.77 \%$ ) shows how merely corrupting digits to create rejected reasoning steps improves the mathematical reasoning abilities of Falcon2-11B. Our approach helps boost performances on the AQuA task, with a relative increase of $14.41 \%$, and on the LastLetterConcat task, with a relative increase of $12 \%$, even without using any example from the AQuA train set or from LastLetterConcat during training. These results clearly indicate that, unlike SFT, DPO fine-tuning using digit corruption to construct rejected answers instills reasoning skills in the base model. We note however, that there is no benefit on the ARCChallenge task. We suspect that it is because it does not require the same type of skills as GSM8K and AQuA. In Section 3.3, we investigate whether other schemes could boost ARC performances.</p>
<h3>3.3 Preference optimization using weak LLMs</h3>
<p>Unlike Section 3.2, when constructing $\mathcal{D}_{\text {train }}^{\text {pref }}$ using weak LLM generation, as described in Section 2.2, there are a few parameters to take into account: which weak LLM to use? how to prompt said LLM? how to post-process the resulting sequences?
We first consider the instruct version of the Gemma model (Team et al., 2024), Gemma-2B-it, to generate answers. We use the prompt provided in Appendix B. We then filter out the responses that do not start with "Next step: ", and simply do not create the corresponding triplet in the preference dataset. The generation stops at the first line-break or full stop. We also consider the larger Llama-7B (Touvron et al., 2023) and its chat version, to assess the effect of the weak LLM size.
When using a weak LLM to generate rejected answers, it is not unlikely that the LLM outputs valid reasoning steps, in which case, including the resulting triplet in the preference dataset might hurt generalization of the resulting model. We experimented with the robust version of DPO (Chowdhury et al., 2024), which accounts for the ambi- guity in the preferences, but that did not result in improved performances. We therefore consider to corrupt the digits of the generated sequences similar to the digit corruption scheme alone. In Figure 5 of Appendix D, we study the effect of post-generation digit corruption, and find that digit corruption is essential for downstream tasks. We also compare using the chat version of Llama-7B with the prompt template of Appendix B to using its base version with few-shot examples only, and find that using the base version yields to better performances.
Lastly, we consider an iterative approach, where we use the Falcon2-11B fine-tuned with DPO as described in Section 3.2 as a weak LLM. We report in Table 2 the accuracies on the three tasks, using the three weak LLMs with post-generation digit corruption. While DPO with the weak LLM scheme leads to improved results on the math word problems, it does not perform as well as the simpler digit corruption scheme, but it is noteworthy that with Llama-7B and the iterative approach, the performance on ARC-Challenge improves over the base model. This suggests that larger models used as weak LLMs are more likely to generate rejected answers that are informative enough for DPO to lead to better models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">GSM8K</th>
<th style="text-align: left;">AQuA</th>
<th style="text-align: left;">ARC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base model</td>
<td style="text-align: left;">54.66</td>
<td style="text-align: left;">31.50</td>
<td style="text-align: left;">76.11</td>
</tr>
<tr>
<td style="text-align: left;">DPO - effect of weak LLM choice</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Gemma-2B-it</td>
<td style="text-align: left;">53.68</td>
<td style="text-align: left;">29.92</td>
<td style="text-align: left;">75.94</td>
</tr>
<tr>
<td style="text-align: left;">Llama-7B</td>
<td style="text-align: left;">$\mathbf{5 6 . 1 0}$</td>
<td style="text-align: left;">30.71</td>
<td style="text-align: left;">$\mathbf{7 7 . 0 5}$</td>
</tr>
<tr>
<td style="text-align: left;">iterative</td>
<td style="text-align: left;">55.65</td>
<td style="text-align: left;">$\mathbf{3 3 . 4 6}$</td>
<td style="text-align: left;">76.28</td>
</tr>
<tr>
<td style="text-align: left;">DPO - effect of preference data size</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">(digit corruption) x 3</td>
<td style="text-align: left;">$\mathbf{5 9 . 2 9} \times 4.47 \%$</td>
<td style="text-align: left;">33.07</td>
<td style="text-align: left;">76.79</td>
</tr>
<tr>
<td style="text-align: left;">(Gemma-2B-it) x 3</td>
<td style="text-align: left;">51.40</td>
<td style="text-align: left;">$\mathbf{3 5 . 0 4}$</td>
<td style="text-align: left;">76.45</td>
</tr>
<tr>
<td style="text-align: left;">(Llama-7B) x 3</td>
<td style="text-align: left;">54.51</td>
<td style="text-align: left;">29.58</td>
<td style="text-align: left;">76.19</td>
</tr>
<tr>
<td style="text-align: left;">Llama-7B + digit corruption</td>
<td style="text-align: left;">56.55</td>
<td style="text-align: left;">32.68</td>
<td style="text-align: left;">77.47</td>
</tr>
<tr>
<td style="text-align: left;">(Llama-7B + digit corruption) x 3</td>
<td style="text-align: left;">56.48</td>
<td style="text-align: left;">30.31</td>
<td style="text-align: left;">$\mathbf{7 7 . 7 0} \times 4 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy (in percentage) of the DPO-finetuned Falcon-11B using different schemes for rejected answer generation. "(scheme) x 3" means that the preference dataset contains 3 rejected answers per chosen answer, obtained by scheme. "scheme1 + scheme2" means that it contains 2 rejected answers per chosen answer, obtained by concatenating two datasets obtained from scheme1 and scheme2 respectively. iterative corresponds to Falcon2-11B fine-tuned with DPO as per Section 3.2.</p>
<h3>3.4 Increasing the size of the preference dataset</h3>
<p>A natural question at this point is to consider the effect of the size of the preference dataset on the</p>
<p>resulting model fine-tuned with DPO. Given that our proposed approach allows us to generate arbitrarily many wrong reasoning steps per valid reasoning step (e.g., we can corrupt the digits in many ways, and prompt weak LLMs multiple times), we can construct preference datasets with triplets (prompt, chosen, rejected) that contain redundant (prompt, chosen) pairs, with different rejected answers. We thus consider using three rejected answers for the digit corruption, Gemma-2B-it, and Llama-7B experiments. We also consider fine-tuning on a dataset consisting of both digit corrupted answers and Llama-7B-generated answers (themselves digit corrupted), and a dataset containing three times as many rejected answers. The source dataset is GSM8K.
We report in Table 2 the results of the different schemes on GSM8K, AQuA, ARC. These results suggest that increasing the preference dataset size or mixing has the potential of further improving the reasoning abilities of the base language model, and that diversifying the sources of rejected answers might help with generalization to other tasks. For example, simply tripling the number of rejected answers for the digit corruption scheme leads to a an accuracy of $59.29\%$ on the GSM8K task, which represents a relative increase of $8.47\%$ over the base performances.</p>
<h3>3.5 Benchmarking DPO and its variants</h3>
<p>While DPO has emerged as the go-to method for preference optimization, several variants (Azar et al., 2023; Ethayarajh et al., 2024) claim to address some of its shortcomings: overfitting, inefficient learning, memory utilization. In this section, we make use of our constructed preference dataset in Section 3.2 to further compare DPO to its variants. Unlike Saeidi et al. (2024), we also consider ORPO (Hong et al., 2024) which combines both SFT and preference optimization. We report the accuracies on the GSM8K test dataset in Table 3. We find that the variants of DPO do not lead to improved performances, even with extensive hyperparameter tuning for each method separately (Appendix C). This confirms the recent observations from the benchmark study in (Saeidi et al., 2024) that DPO still outperforms its variants on a variety of tasks.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>DPO</th>
<th>IPO</th>
<th>KTO</th>
<th>ORPO</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>$\mathbf{5 8 . 9 1}$</td>
<td>56.40</td>
<td>54.59</td>
<td>55.42</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of alternatives to DPO - accuracy on the GSM8K test set. The preference dataset is constructed using digit corruption only, as in Section 3.2.</p>
<h3>3.6 Robustness analysis: using Mistral as a base model</h3>
<p>In this section, we perform some of the experiments above using Mistral-7B (Jiang et al., 2023) as a base model, rather than Falcon2-11B. We report the results in Figure 2, and find that all approaches lead to better performances on the GSM8K benchmark than the base model, which scores $38.51\%$. This experiment confirms the robustness of our approach to the base model, as well as the the strength of the digit corruption scheme.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Robustness analysis, using Mistral-7B as base model: GSM8K accuracy - Comparison of different corruption schemes.</p>
<h3>3.7 Robustness analysis: using AQuA as source dataset</h3>
<p>In this section, we consider using the AQuA training set to create $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}^{\text {SFT }}$ and $\mathcal{D}</em>$. We use Falcon211B as a base model. For SFT and DPO, we finetune with the same hyper-pararameters as the ones we found best for the experiments on GSM8K. We report the results in Table 4. The results confirm that using the AQuA training set is more helpful for the AQuA benchmark ( $18.73 \%$ relative increase) than using the GSM8K training set. This experiment also confirms the robustness of our approach to the training set, as well as the the strength of the digit corruption scheme.}}^{\text {pref }</p>
<h2>4 Related work</h2>
<p>What is reasoning? Reasoning can be thought of as the process of logically and systematically analyzing information, drawing on evidence and past experiences to form conclusions or make decisions</p>
<p>| Model | GSM8K | AQuA | ARC |
| Base model | 54.66 | 31.50 | 76.11 |
| --- | --- | --- | --- |
| SFT on AQUA | 54.89 | 31.50 | 75.68 |
| DPO - digit corr. | $\mathbf{5 7 . 7 0}$ | $\mathbf{3 7 . 4 0}_{(-18.725)}$ | $\mathbf{7 6 . 8 8}$ |
| DPO - Llama-7B | 55.57 | 33.86 | 76.71 |</p>
<p>Table 4: Robustness analysis, using AQuA for training: Accuracy of the base, SFT, and DPO models (with different schemes).
(McHugh and Way, 2018). Using the taxonomy of Huang and Chang (2022), reasoning can be either deductive (a conclusion is drawn based on the truth of the premises), inductive (a conclusion is drawn based on observations or evidence), or abductive (a conclusion is drawn based on the best explanation for a given set of observations). Bronkhorst et al. (2020) also make the distinction between formal reasoning, akin to what is used in mathematics, in which a fixed set of rules is followed, and informal reasoning that is less structured, and is akin to what is used in everyday life.</p>
<p>Reasoning in LLMs: Mathematics, science, and code benchmarks (Austin et al., 2021; Hendrycks et al., 2021; Liang et al., 2023; Clark et al., 2018) are becoming increasingly popular to study the emergent reasoning abilities of language models trained on next token prediction. Chain-of-Thought prompting (Wei et al., 2022b) and related techniques such as Tree-of-Thought (Yao et al., 2024) and Graph-of-Thought (Besta et al., 2024) have shown to improve language model performances on reasoning tasks, simply by prompting them to generate intermediate computations required for solving the problems. It is not clear however whether the improved performances brought about by chain-of-thought prompting are due specifically to human-like task decomposition, or more generally to the increased computation that additional tokens allow (Pfau et al., 2024). An orthogonal direction for boosting language model performances on reasoning tasks is reasoning-enhanced training. For example, Lewkowycz et al. (2022); Taylor et al. (2022); Chen et al. (2021) show that training or fine-tuning LLMs on datasets containing scientific, math, or code data helps improve downstream performances on reasoning tasks. Another line of work (Zelikman et al., 2022; Huang et al., 2022; Gulcehre et al., 2023; Yuan et al., 2023; Singh et al., 2023; Hosseini et al., 2024) consists of using LLMs to self-improve their reasoning abilities via bootstrapping, where rationales generated by the model that lead to the correct answer are further used to fine-tune the model itself. Aligned with this direction, and more closely related to our work, Ni et al. (2023) propose to use intermediate steps as supervision signal. Lightman et al. (2023) conduct a systematic comparison between process supervision (feedback on intermediate steps) and outcome supervision (feedback on final results) for training models on mathematical reasoning, finding that process supervision leads to significantly better performance on the MATH dataset. Another popular set of approaches make use of verifiers, that classify or score reasoning traces (Cobbe et al., 2021; Uesato et al., 2022). As an example of such approaches, GRACE (Khalifa et al., 2023) trains a discriminator with a contrastive loss over correct and incorrect steps, and uses it when generating answers to questions requiring reasoning, to score next-step candidates based on their correctness.</p>
<p>Preference optimization: To make the most out of a preference dataset, Reinforcement learning with human feedback commonly applies the Bradley-Terry model (Bradley and Terry, 1952) to train a reward model that scores instances, and use it to fine-tune the language model to maximize the score of the reward model for the prefered responses using algorithms such as PPO (Schulman et al., 2017). More recently, advances in offline methods such as DPO (Rafailov et al., 2024) and its variants (Azar et al., 2023; Zhao et al., 2023; Cai et al., 2023; Ethayarajh et al., 2024) that directly align the language models without the need for an explicit reward function, have proven successful in practice. These methods however require an SFT phase to achieve convergence to desired results (Rafailov et al., 2024; Tunstall et al., 2023). ORPO (Hong et al., 2024) on the other hand, bypasses the need for the multi-stage process, and uses a loss that combines both supervised finetuning and preference optimization. In our work, we use these methods as part of the pipeline and compare them thoroughly. Concurrent works (Pang et al., 2024; Lai et al., 2024) have also applied preference optimization techniques on reasoning data. Our work differs from these concurrent works in both methodology and scope. While Pang et al. (2024) focuses on iteratively optimizing between competing CoT candidates and Lai et al. (2024) proposes step-level preference optimization requiring fine-grained process supervision, our work intro-</p>
<p>duces two novel and complementary schemes for generating rejected answers (weak LLM prompting and digit corruption) that require no additional annotations or external data. Furthermore, we provide a comprehensive empirical study comparing different preference optimization variants (DPO, IPO, KTO, ORPO) for reasoning tasks. Unlike these works which focus solely on mathematical reasoning, we demonstrate that our approach transfers to non-mathematical tasks, including commonsense and symbolic reasoning, suggesting broader implications for improving general reasoning abilities in language models.</p>
<h2>5 Conclusion</h2>
<p>We considered the question of using preference optimization to boost reasoning abilities of language models. More specifically, we proposed two different schemes for constructing preference datasets of reasoning steps from datasets that include valid reasonign traces. We showed that by using DPO on these datasets, we are able to improve the reasoning abilities of Falcon2-11B and Mistral-7B, even on tasks unseen during training. We also compared DPO to several of its variants. Our work suggests that constructing high-quality reasoning traces datasets can boost general informal reasoning abilities.</p>
<h2>Limitations</h2>
<p>We considered two schemes for wrong reasoning step generation in this work: digit corruption, and LLM generation. There are several other ways that could be considered. For instance, it could be beneficial to consider prompting an LLM to slightly tweak the ground-truth reasoning steps until they become wrong. We leave the study of other schemes to future work, along with scaling to models over 11B. Additionally, when using the weak LLM scheme, there is an overhead incurred when creating the dataset. Finally, our work has focused on mathematical reasoning, and future work should explore using other sources of reasoning data. Perhaps mixing between different sources of data, as suggested by recent work from Chung et al. (2024), could lead to improved abilities in out-of-distribution reasoning benchmarks.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank Mohamed El Amine Seddik and Reda Alami for fruitful discus-
sions that helped improve this manuscript.</p>
<h2>References</h2>
<p>Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for commonsenseqa: New dataset and models. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The falcon series of open language models. arXiv preprint arXiv: 2311.16867.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.</p>
<p>Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. 2023. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682-17690.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv: 2108.07258.</p>
<p>Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345 .</p>
<p>Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, and Martin Goedhart. 2020. Logical reasoning in formal and everyday reasoning tasks. International Journal of Science and Mathematics Education, 18:1673-1694.</p>
<p>Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, and Guannan Zhang. 2023. Ulma: Unified language model alignment with demonstration</p>
<p>and point-wise human preference. arXiv preprint arXiv:2312.02554.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. arXiv preprint arXiv: 2107.03374.</p>
<p>Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. 2024. Provably robust dpo: Aligning language models with noisy feedback. arXiv preprint arXiv: 2403.00409.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1-53.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv: 1803.05457.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>OpenAI et al. 2023. Gpt-4 technical report. arXiv preprint arXiv: 2303.08774.</p>
<p>Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv: 2402.01306.</p>
<p>Iason Gabriel. 2020. Artificial intelligence, values, and alignment. Minds and machines, 30(3):411-437.</p>
<p>Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, D. Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics.</p>
<p>Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced selftraining (rest) for language modeling. arXiv preprint arXiv:2308.08998.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. Annual Meeting of the Association for Computational Linguistics.</p>
<p>Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv: 2403.07691.</p>
<p>Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv: 2106.09685.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403.</p>
<p>Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. arXiv preprint arXiv: 2310.06825.</p>
<p>Daniel Kahneman. 2003. Maps of bounded rationality: Psychology for behavioral economics. American economic review, 93(5):1449-1475.</p>
<p>Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Ho Hin Lee, and Lu Wang. 2023. Grace: Discriminator-guided chain-of-thought reasoning. Conference on Empirical Methods in Natural Language Processing.</p>
<p>Tushar Khot, Peter Clark, Michal Guerquin, Peter Alexander Jansen, and Ashish Sabharwal. 2019. Qasc: A dataset for question answering via sentence composition. AAAI Conference on Artificial Intelligence.</p>
<p>Oliver Klingefjord, Ryan Lowe, and Joe Edelman. 2024. What are human values, and how do we align ai to them? arXiv preprint arXiv: 2404.10636.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics.</p>
<p>Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. 2024. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv: 2406.18629.</p>
<p>Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, and Michael Collins. 2021. Qed: A framework and dataset for explanations in question answering. Transactions of the Association for computational Linguistics, 9:790-806.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian</p>
<p>Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic evaluation of language models. Transactions on Machine Learning Research.</p>
<p>Hunter Lightman, V. Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, J. Leike, John Schulman, I. Sutskever, and K. Cobbe. 2023. Let's verify step by step. International Conference on Learning Representations.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Quentin Malartic, Nilabhra Roy Chowdhury, Ruxandra Cojocaru, Mugariya Farooq, Giulia Campesan, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Ankit Singh, Maksim Velikanov, Basma El Amel Boussaha, Mohammed Al-Yafeai, Hamza Alobeidli, Leen Al Qadi, Mohamed El Amine Seddik, Kirill Fedyanin, Reda Alami, and Hakim Hacid. 2024. Falcon2-11b technical report. arXiv preprint arXiv: 2407.14885.</p>
<p>Conor McHugh and Jonathan Way. 2018. What is reasoning? Mind, 127(505):167-196.</p>
<p>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. 2023. Learning math reasoning from self-sampled correct and partially-correct solutions. In The Eleventh International Conference on Learning Representations.</p>
<p>Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. Creak: A dataset for commonsense reasoning over entity knowledge. NeurIPS Datasets and Benchmarks.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc.</p>
<p>Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. 2024. Iterative reasoning preference optimization. arXiv preprint arXiv: 2404.19733.</p>
<p>Jacob Pfau, William Merrill, and Samuel R. Bowman. 2024. Let's think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv: 2404.15758.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36.</p>
<p>Amir Saeidi, Shivanshu Verma, and Chitta Baral. 2024. Insights into alignment: Evaluating dpo and its variants across multiple tasks. arXiv preprint arXiv: 2404.14723.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p>
<p>Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2023. Beyond human data: Scaling self-training for problemsolving with language models. arXiv preprint arXiv: 2312.06585.</p>
<p>Keith E Stanovich, Richard F West, and JE Alder. 2000. Individual differences in reasoning: Implications for the rationality debate?-open peer commentary-three fallacies. Behavioral and Brain Sciences, 23(5):665665.</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. Neural Information Processing Systems.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv: 2211.09085.</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale,</p>
<p>Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295.</p>
<p>Tijmen Tieleman. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv: 2307.09288.</p>
<p>Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process- and outcomebased feedback. arXiv preprint arXiv: 2211.14275.</p>
<p>Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why? a pilot study for sense making and explanation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4020-4026, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Duni Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,</p>
<p>et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. 2023. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425.</p>
<p>Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv: 1909.08593.</p>
<h2>A Dataset examples</h2>
<p>We provide in the following box two examples from the training dataset of GSM8K, the main source of data used in our experiments. We also provide in Table 5 the preference triplets obtained from one example from the GSM8K (a third one). The ground truth rationale for this particular example contains three sentences, and thus contributes three examples to the preference dataset.</p>
<h2>Examples from the GSM8K training dataset</h2>
<p>Question: John puts $\$ 25$ in his piggy bank every month for 2 years to save up for a vacation. He had to spend $\$ 400$ from his piggy bank savings last week to repair his car. How many dollars are left in his piggy bank?
Rationale: He saved money for 2 years, which is equal to $12 \times 2=« 12 * 2=24 » 24$ months. The amount of money he saved is $\$ 25 * 24=\$ « 25 * 24=600 » 600$. But he spent some money so there is $\$ 600$ - $\$ 400=« 600-$ $400=200 » 200$ left. #### 200.</p>
<p>Question: Five coaster vans are used to transport students for their field trip. Each van carries 28 students, 60 of which are boys. How many are girls?
Rationale: There are a total of 5 vans x 28 students $=« 5 * 28=140 » 140$ students. If 60 are boys, then $140-60=« 140-60=80 » 80$ of these students are girls. #### 80</p>
<h2>B Used prompts</h2>
<p>In the following box, we provide the prompt used to generate plausible rationales for the 25 few shot examples of ARC-Challenge, using GPT-4.</p>
<h2>GPT-4 prompt for ARC rationale generation</h2>
<p>You are expert grade-school science teacher. Given the following question, provide justification for the answer.
Question: question. Answer Choices: options. Answer: ... The Answer is answer letter.
You need to add a two to three sentences rationale before "The answer is answer letter", justifying the correct answer.</p>
<p>Next, we provide the template used to prompt
the weak LLMs to generate rejected answers to create the preference dataset for Section 3.3.</p>
<h2>Prompt template for weak LLM generation</h2>
<p>You are an obedient assistant. Your task is to reason about the following question. Write only the next step of the reasoning chain. Your answer should include exactly one following reasoning step and has to be exactly one sentence long! The answer should start with "Next step: ". Here are two examples:</p>
<div class="codehilite"><pre><span></span><code>Question: {prompt_example_1}
Next step: {first_step_example_1}
Next step: {second_step_example_1}
Next step: {third_step_example_1}
Next step: {fourth_step_example_1}
Next step: {final_answer_example_1}
</code></pre></div>

<p>Question: {prompt_example_2}
Next step: {first_step_example_2}
Next step: {second_step_example_2}
Next step: {third_step_example_2}
Next step: {fourth_step_example_2}
Next step: {fifth_step_example_2}
Next step: {sixth_step_example_2}
Next step: {final_answer_example_2}</p>
<p>Question: {prompt}
Next step: {first_step_ground_truth}
...</p>
<h2>C Training details</h2>
<p>Number of epochs used for SFT: An important hyperparameter when doing supervised fine-tuning on small datasets is the number of times each example is processed. We optimized this hyperparameter independently using the smaller Falcon-7B (Almazrouei et al., 2023) as a base model, with the GSM8K accuracy as a metric, trying values ${1,2,3,4,5}$. We ended up using 3 epochs for all subsequent SFT experiments (using Falcon2-11B as a base model).</p>
<p>SFT learning rate: Similarly, using the GSM8K accuracy as a metric, and with a random search of a handful of learning rates in the range $\left[10^{-8}, 10^{-4}\right]$, we ended up using the learning rate of $1.4 \times 10^{-5}$.</p>
<table>
<thead>
<tr>
<th>prompt</th>
<th>chosen</th>
<th>rejected(1)</th>
<th>rejected(2)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?</td>
<td>Natalia sold $48 / 2=24$ clips in May.</td>
<td>Natalia sold $32 / 4=19$ clips in May.</td>
<td>Natalia sold 48 clips in April, and half as many clips in May, which is 24 clips</td>
</tr>
<tr>
<td>Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Natalia sold $48 / 2=24$ clips in May.</td>
<td>Natalia sold $48+24=$ 72 clips altogether in April and May.</td>
<td>Natalia sold $25+98=$ 12 clips altogether in April and May.</td>
<td>Natalia sold 24 clips in April.</td>
</tr>
<tr>
<td>Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Natalia sold $48 / 2=24$ clips in May. Natalia sold $48+24=72$ clips altogether in April and May.</td>
<td>The solution to the problem is 72 .</td>
<td>The solution to the problem is 13 .</td>
<td>Natalia sold 24 clips in April, so she sold 24 clips in May.</td>
</tr>
</tbody>
</table>
<p>Table 5: Example of preference dataset obtained from the question "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?", and the corresponding rationale "Natalia sold $48 / 2=24$ clips in May. Natalia sold $48+24=72$ clips altogether in April and May. The solution to the problem is 72 ." This example is obtained from the GSM8K dataset (Cobbe et al., 2021). The chosen column represents steps from ground-truth rationale, rejected (1) are examples obtained by digit corruption, and rejected (2) are examples obtained by prompting the Llama-2-7B-chat model (Touvron et al., 2023)</p>
<p>SFT batch size: It is commonly agreed upon that larger batch sizes are more desirable when finetuning language models. We used a batch size of 16 as it was the largest that did not lead to memory issues on the GPUs we used.</p>
<p>LoRA parameters: Given the computational cost of fine-tuning LLMs, we chose not to tune the hyperparameters of LoRA (Hu et al., 2021), and resorted to using the popular values of $\alpha=16$ and $\operatorname{rank}=64$.</p>
<p>Optimizer: For DPO, we used a linear schedule for the learning rate, and first jointly optimized the maximal learning rate and number of warm-up steps for the linear schedule, using RMSProp (Tieleman, 2012) as an optimizer. Optimizing this couple of hyperparameters was done on Falcon-7B using a preference dataset with digit corrupted rejected answers only, with the GSM8K accuracy as a metric. After settling on 10 warmup steps, we tuned the maximal learning rate and the optimizer (choosing between RMSProp and AdamW (Loshchilov and Hutter, 2019)) on the same (model, task, metric) triplet. We ended up using the AdamW optimizer with a maximal learning rate of $8 \times 10^{-6}$.</p>
<p>Further DPO hyperparameters: We further optimized the learning rate, as well as the number of training epochs and the value of the $\beta$ hyperparameter on the preference dataset $\mathcal{D}_{\text {train }}^{\text {pref }}$ constructed from GSM8K, and using digit-corrupted Llama-7B
(Touvron et al., 2023), as explained in Section 3.3, to generate wrong answers. Using the resulting model's performance on the GSM8K task, with the evaluation protocol described in Section 3, we report the results of our hyperparameter sweep in Figure 3. This led to a universal choice of $\beta=0.2$, learning rate of $8 \times 10^{-6}$, and number of epochs equal to 1 for all DPO experiments with Falcon211B.</p>
<p>Hyperparameters for DPO variants: Similar to DPO, the KTO (Ethayarajh et al., 2024) loss requires the specification of a hyperparameter $\beta$ that controls how far the fine-tuned model drifts from the SFT model. IPO (Azar et al., 2023) needs a regularization parameter $\tau$, for which the inverse $\tau^{-1}$ is usually denoted by $\beta$ as well. For both methods, the value of $\beta$ is critical and needs to be carefully tuned. We report in Figure 4 the results of our hyperparameter search. We consider the preference dataset $\mathcal{D}_{\text {train }}^{\text {pref }}$ constructed from GSM8K, and using digit corruption, as explained in Section 3.2, to generate wrong answers.
For ORPO (Hong et al., 2024), an important parameter is the weighing hyperparameter $\lambda$ in ??, that specifies the relative importance of the negative log-likelihood of the chosen answer with respect to the odds ratio part of the loss. We tried the values in the set ${0.001,0.005,0.01,0.1,0.2,0.3}$ along with learning rates from the set $\left{10^{-8}, 8 \times\right.$ $\left.10^{-8}, 8 \times 10^{-6}\right}$, and found that $\beta=0.001$ and $10^{-8}$ as a learning rate lead to the best results,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: DPO hyperparameter search. The y axis corresponds to the accuracy on the test set of GSM8K.</p>
<p>which is what we report in Table 3.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: DPO variants hyperparameter search. The y axis corresponds to the accuracy on the test set of GSM8K. The learning rate 8 × 10⁻⁶ and number of epochs (1) used are the same as DPO.</p>
<h2><strong>D Additional Results</strong></h2>
<p>In Table 6, we study how the number of times each example from the GSM8K training dataset is visited during training affects the downstream performance on the related but different AQuA evaluation task. The table shows that reducing the training time could help dampen the overfitting issues of SFT.</p>
<table>
<thead>
<tr>
<th></th>
<th>1 epoch</th>
<th>3 epochs</th>
</tr>
</thead>
<tbody>
<tr>
<td>AQuA accuracy</td>
<td>33.46</td>
<td>30.71</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy (in percentage) on the AQuA test dataset of Falcon2-11B fine-tuned on $$D_{train}^{SFT}$$ obtained from the GSM8K train dataset, as explained in Section 3.1. Comparison of the effect of number of epochs.</p>
<p>When using a weak LLM to generate rejected answers, it is not unlikely that the LLM outputs valid reasoning steps, in which case, including the resulting triplet in the preference dataset might hurt</p>
<p>generalization of the resulting model. We therefore consider to corrupt the digits of the generated sequences similar to the digit corruption scheme alone. In Figure 5, we study the effect of postgeneration digit corruption, and find that digit corruption is essential for downstream tasks. We also compare using the chat version of Llama-7B with the prompt template of Appendix B to using its base version, and find that using the base version yields better performances.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: DPO with weak LLM generation for rejected answers. Comparison of different versions of Llama-7B. The y axis corresponds to the accuracy on the test set of GSM8K. The learning rate use is 8 × 10⁻⁶ and number of epochs is 1.</p>
<h3><strong>D.1 Ablations</strong></h3>
<p>We hypothesized that fine-tuning a language model to predict the next reasoning step only should help improve performances on reasoning benchmarks. Our results in the main paper confirm this hypothesis. However, it is natural to wonder whether using multiple reasoning steps to predict could be beneficial. More specifically, for SFT, we compare our approach (which requires fine-tuning on (xz¹:k−1, z⁸) pairs) to fine-tuning on (xz¹:k−1, z⁸:n) pairs. Similarly, for DPO, we compare our approach (that requires fine-tuning on (xz¹:k−1, z⁸, ²⁸) triplets)</p>
<p>to fine-tuning on $\left(x z^{1: k-1}, z^{k: n}, \tilde{z}^{k} z^{k+1: n}\right)$ triplets. Using Falcon2-11B as a base model, and GSM8K as a data source and for evaluation, we found that with this change, the performance drops from 55.43 to 54.81 for SFT, and from 58.30 to 57.01 for DPO with digit corruption.
We also tested replacing the inputs $x z^{1: k-1}$ with $u x z^{1: k-1}$, where $u$ is a sequence corresponding to 3-shot examples, and found that while the SFT performance slightly increases to 55.95 , the DPO performance significantly drops to 50.11 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ or until the prompt $q s_{1} \ldots s_{k}$ exceeds $\tau$ tokens, but we disregard this case by assuming a very large context window.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3334 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3334</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3334</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4f4a409f701f7552d45c46a5b0fea69dca6f8e84</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4f4a409f701f7552d45c46a5b0fea69dca6f8e84" target="_blank">Unsupervised Dense Information Retrieval with Contrastive Learning</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> This work explores the limits of contrastive learning as a way to train unsupervised dense retrievers and shows that it leads to strong performance in various retrieval settings and can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries.</p>
                <p><strong>Paper Abstract:</strong> Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3334.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3334.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contriever (Contrastive Retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unsupervised dense bi-encoder retriever trained with contrastive learning (InfoNCE) using MoCo and random cropping / token deletion augmentations; initialized from BERT-base and evaluated on open-domain retrieval benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Contriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bi-encoder transformer retriever that uses the same encoder for queries and documents (BERT-base architecture), trained with a contrastive InfoNCE loss, MoCo momentum encoder and a large queue of negatives; positive pairs created by independent random cropping of document spans plus 10% token deletion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-base (~110M)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['contrastive learning (InfoNCE)', 'momentum-encoder (MoCo) negatives', 'data-augmentation-based positive-pairing (random cropping, token deletion)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Learning objective: InfoNCE contrastive loss where a query representation must retrieve its positive key among many negatives; keys are produced by a momentum encoder (MoCo) and negatives are stored in a large queue; positives are created by independent random crops (contiguous spans) from the same document, optionally with token deletion/replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single core paradigm (contrastive learning) implemented with several training-engineering variants (MoCo vs in-batch negatives, cropping vs ICT, additional perturbations). The paper applies a consistent contrastive paradigm but experiments with multiple augmentation and negative-sampling styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Document retrieval / dense passage retrieval (BEIR benchmark, NaturalQuestions, TriviaQA, Mr. TyDi, MKQA cross-lingual retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Find relevant documents/passages from large collections given queries (zero-shot and few-shot settings). Benchmarks include BEIR (heterogeneous zero-shot IR tasks), open-domain QA datasets NaturalQuestions and TriviaQA, and multilingual retrieval datasets Mr. TyDi and MKQA (cross-lingual retrieval). Metrics: Recall@100, nDCG@10, MRR@100.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>NaturalQuestions Recall@100: 82.1 (Table 1); TriviaQA Recall@100: 83.2 (Table 1). BEIR average (nDCG@10) for the bi-encoder without re-ranker: 46.6 (Table 2). BEIR average (Recall@100) after pre-training and fine-tuning: 67.1 (Table 10). With cross-encoder re-ranking (Ours+CE) nDCG@10 average: 50.2 (Table 2). On MS MARCO Recall@100: 89.1 (Table 10). Multilingual mContriever variants reported separately.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper-level comparisons: Contriever (contrastive + crop + MoCo) outperforms previous unsupervised dense retrievers (e.g., ICT-based pretraining, SimCSE, REALM) on Recall@100 and often matches or beats BM25 on Recall@100 across many BEIR datasets; when used as pre-training before MS MARCO fine-tuning, Contriever improves nDCG@10 and Recall@100 versus MS MARCO-only baselines (Tables 2, 10, 3). Ablations show cropping > ICT (Table 7), MoCo roughly similar to in-batch negatives but scales better (Table 6), and larger negative queue sizes improve average retrieval (Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrastive pre-training with random cropping and MoCo produces strong unsupervised dense retrievers (Contriever) that: (1) outperform prior unsupervised dense retrievers and often match or beat BM25 on Recall@100 across many BEIR datasets; (2) provide better few-shot fine-tuning performance than BERT initialized models; (3) serve as effective pre-training for further supervised fine-tuning (MS MARCO) yielding state-of-the-art combination with cross-encoder re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite improvements on Recall@100, Contriever still lags behind BM25 on nDCG@10 in aggregate and on particular datasets (Trec-COVID, Tóuche-2020) where BM25 remains stronger for top-ranked quality (Table 2, discussion in Section 4.3); unsupervised Contriever without fine-tuning is not uniformly superior to BM25 on all datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Dense Information Retrieval with Contrastive Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3334.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3334.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mContriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mContriever (multilingual Contriever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multilingual variant of Contriever initialized from mBERT, pre-trained contrastively on CCNet+Wikipedia in multiple languages using MoCo and used for multilingual and cross-lingual retrieval after optional English fine-tuning on MS MARCO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mContriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multilingual bi-encoder initialized from mBERT, pre-trained with the same contrastive MoCo framework on 29 languages' corpora (uniform sampling) with random cropping and large negative queue; fine-tunable on English MS MARCO and Mr. TyDi for improved multilingual retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mBERT-base (~110M)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['multilingual contrastive pre-training (InfoNCE with MoCo)', 'cross-lingual transfer via English supervised fine-tuning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model uses the same InfoNCE contrastive objective via MoCo across multiple languages; pre-training samples languages uniformly and uses large queues of negatives; cross-lingual retrieval capability emerges and can be enhanced by fine-tuning on English MS MARCO and, optionally, Mr. TyDi.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single training paradigm (contrastive) applied across many languages; diversity comes from multilingual data and different fine-tuning stages (unsupervised multilingual pretraining, English supervised fine-tuning, target-language fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multilingual and cross-lingual retrieval (Mr. TyDi, MKQA cross-lingual retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Retrieve documents in the same language (Mr. TyDi) or retrieve English documents given queries in other languages (MKQA-derived cross-lingual setting). Metrics: MRR@100, Recall@100, Recall@20.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On Mr. TyDi (Recall@100) mContriever unsupervised avg: 77.2 (Table 4 'Recall@100' row for mContriever). After MS MARCO fine-tuning: avg Recall@100 = 87.0 and MRR@100 avg = 38.4 (Table 4). After further fine-tuning on Mr. TyDi: R@100 avg = 93.6, MRR@100 avg = 65.2. On MKQA cross-lingual retrieval, mContriever + MS MARCO average R@100 = 65.6 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>mContriever outperforms mBERT/XLM-R when both are fine-tuned under the same MS MARCO pipeline (Table 4). Unsupervised mContriever already beats BM25 on Recall@100 in Mr. TyDi for many languages; supervised fine-tuning on English MS MARCO substantially improves cross-lingual retrieval performance. The paper also notes 'curse of multilinguality' (pretraining on more languages can dilute per-language performance) and reports that pretraining on a subset (11 languages) can yield better unsupervised performance on those languages (Table 15).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multilingual contrastive pre-training produces strong unsupervised multilingual retrievers and transfers well: English supervised fine-tuning (MS MARCO) improves retrieval across many languages, and further target-language fine-tuning yields SOTA-like performance. However, scaling to many languages without care can dilute performance ('curse of multilinguality').</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Unsupervised mContriever (without MS MARCO fine-tuning) lags behind some hybrid or language-specific baselines on top-ranked metrics (MRR@100) and on some languages; pre-training on too many languages (29) slightly reduces unsupervised and post-MS-MARCO performance versus pretraining on the smaller matching-language set (11 languages) as shown in Table 15.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Dense Information Retrieval with Contrastive Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3334.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3334.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inverse Cloze Task (ICT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Cloze Task (ICT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data-augmentation/pretraining strategy that forms a positive pair by using a sampled span as the 'query' and its document complement as the 'key' (originally proposed for retriever pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Inverse Cloze Task</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Given a document chunk, sample a span (often a sentence) and use that span as the query and the remaining text as the positive key; used as a proxy retrieval task to pre-train retrievers in an unsupervised way.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['dependent-span augmentation (complementary views)', 'contrastive pretraining (InfoNCE when used)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ICT generates two mutually exclusive views from a document: the span and its complement; trained with contrastive losses to retrieve the complement given the span, encouraging contextual inference rather than symmetric span-spans.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>ICT is a specific augmentation style (dependent/complementary views) — a single method contrasted in the paper with other augmentation styles (random cropping), i.e., similar style family (augmentation-for-contrastive-learning) but different dependence structure between views.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Unsupervised retriever pretraining (proxy retrieval task)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Pretraining objective that simulates retrieval by predicting document context (the complement) from a sampled span; used as a surrogate supervisory signal for dense retriever learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 7 (nDCG@10, without MS MARCO fine-tuning): ICT overall (average over shown BEIR datasets) = 25.9; specifically NFCorpus 23.2, NQ 19.4, ArguAna 31.6, Quora 27.6, DBPedia 21.3, SciDocs 10.6, FEVER 55.6 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>A direct ablation comparing ICT to independent random cropping shows random cropping substantially outperforms ICT on overall nDCG@10 (Crop 32.2 vs ICT 25.9) and on multiple datasets (Table 7). The authors hypothesize that cropping's symmetry and overlap between views produces more stable training with MoCo.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICT — while previously used for retriever pretraining — underperforms independent random cropping in this paper's contrastive MoCo setup for unsupervised retriever pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>ICT provided lower nDCG@10 than random cropping across the evaluated BEIR datasets (Table 7), indicating that dependent complementary views (ICT) can be less effective than symmetric overlapping views (cropping) when paired with MoCo and large queues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Dense Information Retrieval with Contrastive Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3334.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3334.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Cropping (text)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Independent Random Cropping (text-span augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data augmentation strategy that samples two independent contiguous spans from the same document to create positive pairs for contrastive learning; used as the primary positive construction in Contriever.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Random cropping (span sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Create two positive views by independently sampling contiguous spans from a document (span lengths sampled between 5% and 50% of document length in Contriever), optionally applying token deletion (10%) or replacement masks; both views share same marginal distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['symmetric independent augmentation (overlapping spans)', 'contrastive pretraining with InfoNCE']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Sampling independent spans generates positive pairs where both views are contiguous text and may overlap; this symmetry encourages the model to learn both lexical and contextual matching signals and stabilizes MoCo training.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>One augmentation method among several explored; contrasted directly to ICT and further combined with token perturbations (delete/replace) to create variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Unsupervised retriever pretraining / positive-pair construction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to build positive examples from unaligned text corpora for training contrastive retrievers to solve downstream retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 7 (nDCG@10, without MS MARCO fine-tuning): Crop overall = 32.2; Crop+delete = 33.8; Crop+replace = 32.9. Individual dataset examples: Quora 75.4 (Crop) and 77.3 (Crop+delete).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Random cropping (and variants with deletion/replacement) outperforms ICT across the evaluated BEIR datasets in nDCG@10; small gains are seen when combining cropping with token deletion (Crop+delete best overall in Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Independent random cropping is a simple, effective positive construction for contrastive pretraining of retrievers and yields better downstream retrieval than ICT in this setup; adding token-deletion perturbation can provide further small gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No dataset in the presented ablation shows ICT beating cropping in the reported metrics; the improvement magnitude varies by dataset (some datasets show small differences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Dense Information Retrieval with Contrastive Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3334.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3334.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Momentum Contrast (MoCo) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive learning framework using a momentum-updated key encoder and a queue of negative examples to enable large numbers of negatives without extreme batch sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoCo (momentum encoder + queue)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-network architecture: a query encoder (updated by SGD/backprop) and a momentum-updated key encoder (EMA of query encoder). Keys from previous batches are stored in a FIFO queue and treated as negatives for the contrastive loss; avoids needing extremely large in-batch negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['asymmetric negative handling (momentum encoder)', 'large negative queue for InfoNCE']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MoCo creates an asymmetry: gradients flow through query encoder only; key encoder is updated via momentum averaging, and a large queue provides many negative keys computed by the momentum encoder over prior iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>MoCo is an alternative negative-sampling architecture within the contrastive-learning paradigm; the paper compares MoCo to in-batch negatives (another similar-style method).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Contrastive pre-training for dense retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Provides many (historical) negative examples and more stable training dynamics for InfoNCE-based retriever pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 6 (nDCG@10 without MS MARCO fine-tuning): MoCo average = 30.1 across BEIR subsets reported; In-batch negatives average = 31.9 (Table 6). The paper reports that after MS MARCO fine-tuning differences are small.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>MoCo vs in-batch negatives: small performance differences on BEIR pre-fine-tune (MoCo slightly lower in the small subset in Table 6), but MoCo scales to far larger numbers of negatives (queue sizes up to 131K) without exploding batch sizes and is preferred for stability and scalability. Large negative queue sizes improve average retrieval performance (Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MoCo enables scaling the number of negatives and produces stable contrastive pre-training for retrievers; performance differences versus in-batch negatives are small on several BEIR metrics, but MoCo is computationally advantageous for very large negative sets.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>In the direct ablation (Table 6) in-batch negatives slightly outperformed MoCo on the small set of reported nDCG@10 metrics (31.9 vs 30.1), indicating that MoCo does not universally dominate in raw metric terms without additional configuration/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Dense Information Retrieval with Contrastive Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3334.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3334.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-batch negatives</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-batch negative sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrastive training strategy where other examples within the current batch serve as negatives for each positive pair; commonly used in InfoNCE setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>In-batch negatives</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>All other key representations generated in the current mini-batch are used as negatives in the contrastive loss; requires very large batch sizes to yield many negatives (e.g., thousands).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['synchronous in-batch negative sampling for InfoNCE']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Contrastive loss computes similarity between current query and its positive and uses the other keys in the batch as negatives; gradients backpropagate through both query and key representations in the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>One negative-sampling strategy among contrastive frameworks; compared directly with MoCo in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Contrastive pre-training for dense retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generates negative examples from within the same optimization batch to form the denominator in InfoNCE and encourage discriminative representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 6 reports in-batch negatives average nDCG@10 = 31.9 vs MoCo = 30.1 (without MS MARCO fine-tuning) across the reported BEIR subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>In-batch negatives perform similarly or slightly better than MoCo in some ablations when batch sizes are large (batch size matched to queue size), but MoCo allows using many negatives with smaller compute by storing historical keys. The paper chooses MoCo for scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In-batch negatives can be effective when large batches are feasible, but MoCo provides a scalable alternative allowing very large negative sets without requiring extremely large synchronous batches.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>When batch sizes are matched, in-batch negatives slightly outperformed MoCo in the reported ablation (Table 6), showing MoCo is not categorically superior in raw metric performance for all settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Dense Information Retrieval with Contrastive Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3334.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3334.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-encoder vs Bi-encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-encoder (cross-attention re-ranker) vs Bi-encoder (dual-encoder) architectures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectural families for scoring query-document pairs: bi-encoders encode query and document independently (fast retrieval via ANN), cross-encoders jointly encode pairs (expensive but higher-fidelity relevance scoring); the paper uses bi-encoder for retrieval and cross-encoder for re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cross-encoder / Bi-encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bi-encoder: encode query and document separately into vectors; score by dot product (used for large-scale retrieval). Cross-encoder: concatenate query and document and run through transformer allowing full cross-attention to produce high-quality relevance scores (used as a re-ranker on top-k candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['independent encoding (bi-encoder / approximate ANN retrieval)', 'joint encoding with cross-attention (cross-encoder re-ranking)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Bi-encoder trades interaction capacity for scalability (single vector per document enabling ANN search). Cross-encoder provides full token-level interactions between query and document and yields better top-ranked ordering but is computationally expensive, so it is applied to re-rank the top-k retrieved candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar reasoning styles in the sense both compute relevance but differ in interaction granularity; the paper uses both in a two-stage pipeline (bi-encoder retrieval + cross-encoder re-rank) to combine scalability and high-quality ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Document retrieval ranking (BEIR nDCG@10 evaluation and MS MARCO re-ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Bi-encoder used for candidate retrieval (Recall-focused); cross-encoder used to re-rank the top-k to improve nDCG@10 (top-ranked quality).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Using Contriever as the bi-encoder and a separate cross-encoder for re-ranking (Ours+CE) increases BEIR average nDCG@10 from 46.6 (Ours) to 50.2 (Ours+CE) and yields SOTA on many datasets (Table 2). For MS MARCO nDCG@10, Ours+CE = 47.0 (Table 2) vs Ours = 40.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper shows that bi-encoder Contriever has strong Recall@100 (good for downstream ML systems) but lower nDCG@10; applying a cross-encoder re-ranker on top of Contriever improves nDCG@10 substantially, demonstrating complementary strengths: diverse methods combined (scalable retrieval + expensive re-ranking) yield the best end metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining a strong contrastively pre-trained bi-encoder retriever with a cross-encoder re-ranker yields state-of-the-art nDCG@10 on BEIR; bi-encoders are better-suited for recall-oriented retrieval while cross-encoders improve top-ranked accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Bi-encoder alone (even well pre-trained) still falls short of BM25 in nDCG@10 on some datasets; cross-encoder re-ranking improves nDCG@10 but does not change Recall@100 (re-ranking cannot increase recall if the correct document was not retrieved).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Dense Information Retrieval with Contrastive Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3334.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3334.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REALM / SimCSE (baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REALM; SimCSE (mentioned unsupervised baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>REALM is a retrieval-augmented LM pretraining method; SimCSE is an unsupervised contrastive sentence embedding method. The paper cites them as unsupervised dense-retriever baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REALM; SimCSE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>REALM integrates retrieval into LM pretraining by retrieving passages as context for masked-language-model training; SimCSE trains sentence embeddings with contrastive objectives using dropout-based or supervised positives (here RoBERTa-large SimCSE cited).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented pretraining (REALM)', 'contrastive sentence embedding (SimCSE)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>REALM uses retrieval to augment masked language modeling training; SimCSE uses contrastive objectives to learn sentence-level embeddings (unsupervised via dropout or supervised using NLI positives).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>These are different unsupervised/synthetic supervision paradigms compared as baselines to the paper's contrastive pretraining approach.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Unsupervised dense retrieval baselines (BEIR / unsupervised retrieval evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks and comparisons where unsupervised dense retrievers are evaluated for retrieval quality (BEIR Recall@100 / nDCG@10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>In Figure 1 and Table 11, REALM and SimCSE are reported as weaker than Contriever on many BEIR datasets; e.g., in Table 11 (unsupervised Recall@100 averages) SimCSE average = 45.4, REALM average = 46.9, Contriever average = 60.1 (Recall@100).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper positions Contriever as outperforming prior unsupervised baselines like REALM and SimCSE in recall-oriented metrics; these methods represent different unsupervised strategies (retrieval-augmented LM pretraining vs sentence-embedding contrastive learning).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contriever's contrastive MoCo + cropping pretraining yields significantly better recall performance than REALM and SimCSE baselines across BEIR datasets (Table 11), indicating the chosen contrastive design is more effective for unsupervised retrieval in these evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>SimCSE and REALM sometimes have competitive nDCG@10 on particular datasets; however, overall recall-oriented metrics strongly favor Contriever in the reported comparisons (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Dense Information Retrieval with Contrastive Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Momentum contrast for unsupervised visual representation learning <em>(Rating: 2)</em></li>
                <li>Latent retrieval for weakly supervised open domain question answering <em>(Rating: 2)</em></li>
                <li>SimCSE: Simple contrastive learning of sentence embeddings <em>(Rating: 2)</em></li>
                <li>REALM: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3334",
    "paper_id": "paper-4f4a409f701f7552d45c46a5b0fea69dca6f8e84",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "Contriever",
            "name_full": "Contriever (Contrastive Retriever)",
            "brief_description": "Unsupervised dense bi-encoder retriever trained with contrastive learning (InfoNCE) using MoCo and random cropping / token deletion augmentations; initialized from BERT-base and evaluated on open-domain retrieval benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Contriever",
            "model_description": "Bi-encoder transformer retriever that uses the same encoder for queries and documents (BERT-base architecture), trained with a contrastive InfoNCE loss, MoCo momentum encoder and a large queue of negatives; positive pairs created by independent random cropping of document spans plus 10% token deletion.",
            "model_size": "BERT-base (~110M)",
            "reasoning_methods": [
                "contrastive learning (InfoNCE)",
                "momentum-encoder (MoCo) negatives",
                "data-augmentation-based positive-pairing (random cropping, token deletion)"
            ],
            "reasoning_methods_description": "Learning objective: InfoNCE contrastive loss where a query representation must retrieve its positive key among many negatives; keys are produced by a momentum encoder (MoCo) and negatives are stored in a large queue; positives are created by independent random crops (contiguous spans) from the same document, optionally with token deletion/replacement.",
            "diversity_of_methods": "Single core paradigm (contrastive learning) implemented with several training-engineering variants (MoCo vs in-batch negatives, cropping vs ICT, additional perturbations). The paper applies a consistent contrastive paradigm but experiments with multiple augmentation and negative-sampling styles.",
            "reasoning_task_name": "Document retrieval / dense passage retrieval (BEIR benchmark, NaturalQuestions, TriviaQA, Mr. TyDi, MKQA cross-lingual retrieval)",
            "reasoning_task_description": "Find relevant documents/passages from large collections given queries (zero-shot and few-shot settings). Benchmarks include BEIR (heterogeneous zero-shot IR tasks), open-domain QA datasets NaturalQuestions and TriviaQA, and multilingual retrieval datasets Mr. TyDi and MKQA (cross-lingual retrieval). Metrics: Recall@100, nDCG@10, MRR@100.",
            "performance_by_method": "NaturalQuestions Recall@100: 82.1 (Table 1); TriviaQA Recall@100: 83.2 (Table 1). BEIR average (nDCG@10) for the bi-encoder without re-ranker: 46.6 (Table 2). BEIR average (Recall@100) after pre-training and fine-tuning: 67.1 (Table 10). With cross-encoder re-ranking (Ours+CE) nDCG@10 average: 50.2 (Table 2). On MS MARCO Recall@100: 89.1 (Table 10). Multilingual mContriever variants reported separately.",
            "comparison_of_methods": "Paper-level comparisons: Contriever (contrastive + crop + MoCo) outperforms previous unsupervised dense retrievers (e.g., ICT-based pretraining, SimCSE, REALM) on Recall@100 and often matches or beats BM25 on Recall@100 across many BEIR datasets; when used as pre-training before MS MARCO fine-tuning, Contriever improves nDCG@10 and Recall@100 versus MS MARCO-only baselines (Tables 2, 10, 3). Ablations show cropping &gt; ICT (Table 7), MoCo roughly similar to in-batch negatives but scales better (Table 6), and larger negative queue sizes improve average retrieval (Figure 2).",
            "key_findings": "Contrastive pre-training with random cropping and MoCo produces strong unsupervised dense retrievers (Contriever) that: (1) outperform prior unsupervised dense retrievers and often match or beat BM25 on Recall@100 across many BEIR datasets; (2) provide better few-shot fine-tuning performance than BERT initialized models; (3) serve as effective pre-training for further supervised fine-tuning (MS MARCO) yielding state-of-the-art combination with cross-encoder re-ranking.",
            "counter_examples_or_negative_results": "Despite improvements on Recall@100, Contriever still lags behind BM25 on nDCG@10 in aggregate and on particular datasets (Trec-COVID, Tóuche-2020) where BM25 remains stronger for top-ranked quality (Table 2, discussion in Section 4.3); unsupervised Contriever without fine-tuning is not uniformly superior to BM25 on all datasets.",
            "uuid": "e3334.0",
            "source_info": {
                "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "mContriever",
            "name_full": "mContriever (multilingual Contriever)",
            "brief_description": "Multilingual variant of Contriever initialized from mBERT, pre-trained contrastively on CCNet+Wikipedia in multiple languages using MoCo and used for multilingual and cross-lingual retrieval after optional English fine-tuning on MS MARCO.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "mContriever",
            "model_description": "Multilingual bi-encoder initialized from mBERT, pre-trained with the same contrastive MoCo framework on 29 languages' corpora (uniform sampling) with random cropping and large negative queue; fine-tunable on English MS MARCO and Mr. TyDi for improved multilingual retrieval.",
            "model_size": "mBERT-base (~110M)",
            "reasoning_methods": [
                "multilingual contrastive pre-training (InfoNCE with MoCo)",
                "cross-lingual transfer via English supervised fine-tuning"
            ],
            "reasoning_methods_description": "The model uses the same InfoNCE contrastive objective via MoCo across multiple languages; pre-training samples languages uniformly and uses large queues of negatives; cross-lingual retrieval capability emerges and can be enhanced by fine-tuning on English MS MARCO and, optionally, Mr. TyDi.",
            "diversity_of_methods": "Single training paradigm (contrastive) applied across many languages; diversity comes from multilingual data and different fine-tuning stages (unsupervised multilingual pretraining, English supervised fine-tuning, target-language fine-tuning).",
            "reasoning_task_name": "Multilingual and cross-lingual retrieval (Mr. TyDi, MKQA cross-lingual retrieval)",
            "reasoning_task_description": "Retrieve documents in the same language (Mr. TyDi) or retrieve English documents given queries in other languages (MKQA-derived cross-lingual setting). Metrics: MRR@100, Recall@100, Recall@20.",
            "performance_by_method": "On Mr. TyDi (Recall@100) mContriever unsupervised avg: 77.2 (Table 4 'Recall@100' row for mContriever). After MS MARCO fine-tuning: avg Recall@100 = 87.0 and MRR@100 avg = 38.4 (Table 4). After further fine-tuning on Mr. TyDi: R@100 avg = 93.6, MRR@100 avg = 65.2. On MKQA cross-lingual retrieval, mContriever + MS MARCO average R@100 = 65.6 (Table 5).",
            "comparison_of_methods": "mContriever outperforms mBERT/XLM-R when both are fine-tuned under the same MS MARCO pipeline (Table 4). Unsupervised mContriever already beats BM25 on Recall@100 in Mr. TyDi for many languages; supervised fine-tuning on English MS MARCO substantially improves cross-lingual retrieval performance. The paper also notes 'curse of multilinguality' (pretraining on more languages can dilute per-language performance) and reports that pretraining on a subset (11 languages) can yield better unsupervised performance on those languages (Table 15).",
            "key_findings": "Multilingual contrastive pre-training produces strong unsupervised multilingual retrievers and transfers well: English supervised fine-tuning (MS MARCO) improves retrieval across many languages, and further target-language fine-tuning yields SOTA-like performance. However, scaling to many languages without care can dilute performance ('curse of multilinguality').",
            "counter_examples_or_negative_results": "Unsupervised mContriever (without MS MARCO fine-tuning) lags behind some hybrid or language-specific baselines on top-ranked metrics (MRR@100) and on some languages; pre-training on too many languages (29) slightly reduces unsupervised and post-MS-MARCO performance versus pretraining on the smaller matching-language set (11 languages) as shown in Table 15.",
            "uuid": "e3334.1",
            "source_info": {
                "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Inverse Cloze Task (ICT)",
            "name_full": "Inverse Cloze Task (ICT)",
            "brief_description": "Data-augmentation/pretraining strategy that forms a positive pair by using a sampled span as the 'query' and its document complement as the 'key' (originally proposed for retriever pretraining).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Inverse Cloze Task",
            "model_description": "Given a document chunk, sample a span (often a sentence) and use that span as the query and the remaining text as the positive key; used as a proxy retrieval task to pre-train retrievers in an unsupervised way.",
            "model_size": null,
            "reasoning_methods": [
                "dependent-span augmentation (complementary views)",
                "contrastive pretraining (InfoNCE when used)"
            ],
            "reasoning_methods_description": "ICT generates two mutually exclusive views from a document: the span and its complement; trained with contrastive losses to retrieve the complement given the span, encouraging contextual inference rather than symmetric span-spans.",
            "diversity_of_methods": "ICT is a specific augmentation style (dependent/complementary views) — a single method contrasted in the paper with other augmentation styles (random cropping), i.e., similar style family (augmentation-for-contrastive-learning) but different dependence structure between views.",
            "reasoning_task_name": "Unsupervised retriever pretraining (proxy retrieval task)",
            "reasoning_task_description": "Pretraining objective that simulates retrieval by predicting document context (the complement) from a sampled span; used as a surrogate supervisory signal for dense retriever learning.",
            "performance_by_method": "Table 7 (nDCG@10, without MS MARCO fine-tuning): ICT overall (average over shown BEIR datasets) = 25.9; specifically NFCorpus 23.2, NQ 19.4, ArguAna 31.6, Quora 27.6, DBPedia 21.3, SciDocs 10.6, FEVER 55.6 (Table 7).",
            "comparison_of_methods": "A direct ablation comparing ICT to independent random cropping shows random cropping substantially outperforms ICT on overall nDCG@10 (Crop 32.2 vs ICT 25.9) and on multiple datasets (Table 7). The authors hypothesize that cropping's symmetry and overlap between views produces more stable training with MoCo.",
            "key_findings": "ICT — while previously used for retriever pretraining — underperforms independent random cropping in this paper's contrastive MoCo setup for unsupervised retriever pretraining.",
            "counter_examples_or_negative_results": "ICT provided lower nDCG@10 than random cropping across the evaluated BEIR datasets (Table 7), indicating that dependent complementary views (ICT) can be less effective than symmetric overlapping views (cropping) when paired with MoCo and large queues.",
            "uuid": "e3334.2",
            "source_info": {
                "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Random Cropping (text)",
            "name_full": "Independent Random Cropping (text-span augmentation)",
            "brief_description": "Data augmentation strategy that samples two independent contiguous spans from the same document to create positive pairs for contrastive learning; used as the primary positive construction in Contriever.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Random cropping (span sampling)",
            "model_description": "Create two positive views by independently sampling contiguous spans from a document (span lengths sampled between 5% and 50% of document length in Contriever), optionally applying token deletion (10%) or replacement masks; both views share same marginal distribution.",
            "model_size": null,
            "reasoning_methods": [
                "symmetric independent augmentation (overlapping spans)",
                "contrastive pretraining with InfoNCE"
            ],
            "reasoning_methods_description": "Sampling independent spans generates positive pairs where both views are contiguous text and may overlap; this symmetry encourages the model to learn both lexical and contextual matching signals and stabilizes MoCo training.",
            "diversity_of_methods": "One augmentation method among several explored; contrasted directly to ICT and further combined with token perturbations (delete/replace) to create variants.",
            "reasoning_task_name": "Unsupervised retriever pretraining / positive-pair construction",
            "reasoning_task_description": "Used to build positive examples from unaligned text corpora for training contrastive retrievers to solve downstream retrieval tasks.",
            "performance_by_method": "Table 7 (nDCG@10, without MS MARCO fine-tuning): Crop overall = 32.2; Crop+delete = 33.8; Crop+replace = 32.9. Individual dataset examples: Quora 75.4 (Crop) and 77.3 (Crop+delete).",
            "comparison_of_methods": "Random cropping (and variants with deletion/replacement) outperforms ICT across the evaluated BEIR datasets in nDCG@10; small gains are seen when combining cropping with token deletion (Crop+delete best overall in Table 7).",
            "key_findings": "Independent random cropping is a simple, effective positive construction for contrastive pretraining of retrievers and yields better downstream retrieval than ICT in this setup; adding token-deletion perturbation can provide further small gains.",
            "counter_examples_or_negative_results": "No dataset in the presented ablation shows ICT beating cropping in the reported metrics; the improvement magnitude varies by dataset (some datasets show small differences).",
            "uuid": "e3334.3",
            "source_info": {
                "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "MoCo",
            "name_full": "Momentum Contrast (MoCo) framework",
            "brief_description": "A contrastive learning framework using a momentum-updated key encoder and a queue of negative examples to enable large numbers of negatives without extreme batch sizes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MoCo (momentum encoder + queue)",
            "model_description": "Two-network architecture: a query encoder (updated by SGD/backprop) and a momentum-updated key encoder (EMA of query encoder). Keys from previous batches are stored in a FIFO queue and treated as negatives for the contrastive loss; avoids needing extremely large in-batch negatives.",
            "model_size": null,
            "reasoning_methods": [
                "asymmetric negative handling (momentum encoder)",
                "large negative queue for InfoNCE"
            ],
            "reasoning_methods_description": "MoCo creates an asymmetry: gradients flow through query encoder only; key encoder is updated via momentum averaging, and a large queue provides many negative keys computed by the momentum encoder over prior iterations.",
            "diversity_of_methods": "MoCo is an alternative negative-sampling architecture within the contrastive-learning paradigm; the paper compares MoCo to in-batch negatives (another similar-style method).",
            "reasoning_task_name": "Contrastive pre-training for dense retrieval",
            "reasoning_task_description": "Provides many (historical) negative examples and more stable training dynamics for InfoNCE-based retriever pretraining.",
            "performance_by_method": "Table 6 (nDCG@10 without MS MARCO fine-tuning): MoCo average = 30.1 across BEIR subsets reported; In-batch negatives average = 31.9 (Table 6). The paper reports that after MS MARCO fine-tuning differences are small.",
            "comparison_of_methods": "MoCo vs in-batch negatives: small performance differences on BEIR pre-fine-tune (MoCo slightly lower in the small subset in Table 6), but MoCo scales to far larger numbers of negatives (queue sizes up to 131K) without exploding batch sizes and is preferred for stability and scalability. Large negative queue sizes improve average retrieval performance (Figure 2).",
            "key_findings": "MoCo enables scaling the number of negatives and produces stable contrastive pre-training for retrievers; performance differences versus in-batch negatives are small on several BEIR metrics, but MoCo is computationally advantageous for very large negative sets.",
            "counter_examples_or_negative_results": "In the direct ablation (Table 6) in-batch negatives slightly outperformed MoCo on the small set of reported nDCG@10 metrics (31.9 vs 30.1), indicating that MoCo does not universally dominate in raw metric terms without additional configuration/fine-tuning.",
            "uuid": "e3334.4",
            "source_info": {
                "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "In-batch negatives",
            "name_full": "In-batch negative sampling",
            "brief_description": "Contrastive training strategy where other examples within the current batch serve as negatives for each positive pair; commonly used in InfoNCE setups.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "In-batch negatives",
            "model_description": "All other key representations generated in the current mini-batch are used as negatives in the contrastive loss; requires very large batch sizes to yield many negatives (e.g., thousands).",
            "model_size": null,
            "reasoning_methods": [
                "synchronous in-batch negative sampling for InfoNCE"
            ],
            "reasoning_methods_description": "Contrastive loss computes similarity between current query and its positive and uses the other keys in the batch as negatives; gradients backpropagate through both query and key representations in the batch.",
            "diversity_of_methods": "One negative-sampling strategy among contrastive frameworks; compared directly with MoCo in ablations.",
            "reasoning_task_name": "Contrastive pre-training for dense retrieval",
            "reasoning_task_description": "Generates negative examples from within the same optimization batch to form the denominator in InfoNCE and encourage discriminative representations.",
            "performance_by_method": "Table 6 reports in-batch negatives average nDCG@10 = 31.9 vs MoCo = 30.1 (without MS MARCO fine-tuning) across the reported BEIR subsets.",
            "comparison_of_methods": "In-batch negatives perform similarly or slightly better than MoCo in some ablations when batch sizes are large (batch size matched to queue size), but MoCo allows using many negatives with smaller compute by storing historical keys. The paper chooses MoCo for scalability.",
            "key_findings": "In-batch negatives can be effective when large batches are feasible, but MoCo provides a scalable alternative allowing very large negative sets without requiring extremely large synchronous batches.",
            "counter_examples_or_negative_results": "When batch sizes are matched, in-batch negatives slightly outperformed MoCo in the reported ablation (Table 6), showing MoCo is not categorically superior in raw metric performance for all settings.",
            "uuid": "e3334.5",
            "source_info": {
                "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Cross-encoder vs Bi-encoder",
            "name_full": "Cross-encoder (cross-attention re-ranker) vs Bi-encoder (dual-encoder) architectures",
            "brief_description": "Architectural families for scoring query-document pairs: bi-encoders encode query and document independently (fast retrieval via ANN), cross-encoders jointly encode pairs (expensive but higher-fidelity relevance scoring); the paper uses bi-encoder for retrieval and cross-encoder for re-ranking.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Cross-encoder / Bi-encoder",
            "model_description": "Bi-encoder: encode query and document separately into vectors; score by dot product (used for large-scale retrieval). Cross-encoder: concatenate query and document and run through transformer allowing full cross-attention to produce high-quality relevance scores (used as a re-ranker on top-k candidates).",
            "model_size": null,
            "reasoning_methods": [
                "independent encoding (bi-encoder / approximate ANN retrieval)",
                "joint encoding with cross-attention (cross-encoder re-ranking)"
            ],
            "reasoning_methods_description": "Bi-encoder trades interaction capacity for scalability (single vector per document enabling ANN search). Cross-encoder provides full token-level interactions between query and document and yields better top-ranked ordering but is computationally expensive, so it is applied to re-rank the top-k retrieved candidates.",
            "diversity_of_methods": "Similar reasoning styles in the sense both compute relevance but differ in interaction granularity; the paper uses both in a two-stage pipeline (bi-encoder retrieval + cross-encoder re-rank) to combine scalability and high-quality ranking.",
            "reasoning_task_name": "Document retrieval ranking (BEIR nDCG@10 evaluation and MS MARCO re-ranking)",
            "reasoning_task_description": "Bi-encoder used for candidate retrieval (Recall-focused); cross-encoder used to re-rank the top-k to improve nDCG@10 (top-ranked quality).",
            "performance_by_method": "Using Contriever as the bi-encoder and a separate cross-encoder for re-ranking (Ours+CE) increases BEIR average nDCG@10 from 46.6 (Ours) to 50.2 (Ours+CE) and yields SOTA on many datasets (Table 2). For MS MARCO nDCG@10, Ours+CE = 47.0 (Table 2) vs Ours = 40.7.",
            "comparison_of_methods": "Paper shows that bi-encoder Contriever has strong Recall@100 (good for downstream ML systems) but lower nDCG@10; applying a cross-encoder re-ranker on top of Contriever improves nDCG@10 substantially, demonstrating complementary strengths: diverse methods combined (scalable retrieval + expensive re-ranking) yield the best end metrics.",
            "key_findings": "Combining a strong contrastively pre-trained bi-encoder retriever with a cross-encoder re-ranker yields state-of-the-art nDCG@10 on BEIR; bi-encoders are better-suited for recall-oriented retrieval while cross-encoders improve top-ranked accuracy.",
            "counter_examples_or_negative_results": "Bi-encoder alone (even well pre-trained) still falls short of BM25 in nDCG@10 on some datasets; cross-encoder re-ranking improves nDCG@10 but does not change Recall@100 (re-ranking cannot increase recall if the correct document was not retrieved).",
            "uuid": "e3334.6",
            "source_info": {
                "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "REALM / SimCSE (baselines)",
            "name_full": "REALM; SimCSE (mentioned unsupervised baselines)",
            "brief_description": "REALM is a retrieval-augmented LM pretraining method; SimCSE is an unsupervised contrastive sentence embedding method. The paper cites them as unsupervised dense-retriever baselines.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "REALM; SimCSE",
            "model_description": "REALM integrates retrieval into LM pretraining by retrieving passages as context for masked-language-model training; SimCSE trains sentence embeddings with contrastive objectives using dropout-based or supervised positives (here RoBERTa-large SimCSE cited).",
            "model_size": null,
            "reasoning_methods": [
                "retrieval-augmented pretraining (REALM)",
                "contrastive sentence embedding (SimCSE)"
            ],
            "reasoning_methods_description": "REALM uses retrieval to augment masked language modeling training; SimCSE uses contrastive objectives to learn sentence-level embeddings (unsupervised via dropout or supervised using NLI positives).",
            "diversity_of_methods": "These are different unsupervised/synthetic supervision paradigms compared as baselines to the paper's contrastive pretraining approach.",
            "reasoning_task_name": "Unsupervised dense retrieval baselines (BEIR / unsupervised retrieval evaluation)",
            "reasoning_task_description": "Benchmarks and comparisons where unsupervised dense retrievers are evaluated for retrieval quality (BEIR Recall@100 / nDCG@10).",
            "performance_by_method": "In Figure 1 and Table 11, REALM and SimCSE are reported as weaker than Contriever on many BEIR datasets; e.g., in Table 11 (unsupervised Recall@100 averages) SimCSE average = 45.4, REALM average = 46.9, Contriever average = 60.1 (Recall@100).",
            "comparison_of_methods": "Paper positions Contriever as outperforming prior unsupervised baselines like REALM and SimCSE in recall-oriented metrics; these methods represent different unsupervised strategies (retrieval-augmented LM pretraining vs sentence-embedding contrastive learning).",
            "key_findings": "Contriever's contrastive MoCo + cropping pretraining yields significantly better recall performance than REALM and SimCSE baselines across BEIR datasets (Table 11), indicating the chosen contrastive design is more effective for unsupervised retrieval in these evaluations.",
            "counter_examples_or_negative_results": "SimCSE and REALM sometimes have competitive nDCG@10 on particular datasets; however, overall recall-oriented metrics strongly favor Contriever in the reported comparisons (Table 11).",
            "uuid": "e3334.7",
            "source_info": {
                "paper_title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Momentum contrast for unsupervised visual representation learning",
            "rating": 2
        },
        {
            "paper_title": "Latent retrieval for weakly supervised open domain question answering",
            "rating": 2
        },
        {
            "paper_title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "rating": 2
        },
        {
            "paper_title": "REALM: Retrieval-augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        }
    ],
    "cost": 0.02248625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Unsupervised Dense Information Retrieval with Contrastive Learning</h1>
<p>Gautier Izacard ${ }^{\diamond} \triangleleft$<br>Mathilde Caron ${ }^{\diamond} \cdot \triangleleft$<br>Lucas Hosseini ${ }^{\diamond}$<br>Sebastian Riedel ${ }^{\diamond} \cdot \triangle$<br>Piotr Bojanowski ${ }^{\diamond}$<br>Armand Joulin<br>Edouard Grave<br>${ }^{\diamond}$ Meta AI Research, ${ }^{\triangle}$ Ecole normale supérieure, PSL University, ${ }^{\circ}$ Inria,<br>${ }^{\Delta}$ Université Grenoble Alpes, ${ }^{\triangle}$ University College London</p>
<p>Reviewed on OpenReview: https://openreview. net/forum?id=3KN1pXi7b0</p>
<h2>Abstract</h2>
<p>Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.</p>
<h2>1 Introduction</h2>
<p>Document retrieval is the task of finding relevant documents in a large collection to answer specific queries. This is an important task by itself and a core component to solve many natural language processing (NLP) problems, such as open domain question answering (Chen et al., 2017a) or fact checking (Thorne et al., 2018). Traditionally, retrieval systems, or retrievers, leverage lexical similarities to match queries and documents, using, for instance, TF-IDF or BM25 weighting (Robertson \&amp; Zaragoza, 2009). These approaches, based on near-exact matches between tokens of the queries and documents, suffer from the lexical gap and do not generalize well (Berger et al., 2000). By contrast, approaches based on neural networks allow learning beyond lexical similarities, resulting in state-of-the-art performance on question answering benchmarks, such as MS MARCO (Nguyen et al., 2016) or NaturalQuestions (Kwiatkowski et al., 2019).</p>
<p>The strong retrieval results of neural networks have been possible for domains and applications where large training datasets are available. In the case of retrieval, creating these datasets requires manually matching queries to the relevant documents in the collection. This is hardly possible when the collection</p>
<p>contains millions or billions of elements, resulting in many scenarios where only a few in-domain examples, if any, are available. A potential solution is to train a dense retriever on a large retrieval dataset such as MS MARCO, and then apply it to new domains, a setting referred to as zero-shot. Unfortunately, in this setting, dense retrievers are often outperformed by classical methods based on term-frequency, which do not require supervision (Thakur et al., 2021). Moreover large annotated datasets are generally not available in languages other than English. Thus, using large collections of supervised data is not suitable to train multilingual retrieval systems.</p>
<p>A natural alternative to transfer learning is unsupervised learning, which raises the following question: is it possible to train dense retrievers without supervision, and match the performance of BM25? Training dense retrievers without supervision can be achieved by using an auxiliary task that approximates retrieval. Given a document, one can generate a synthetic query and then train the network to retrieve the original document, among many others, given the query. The inverse Cloze task (ICT), proposed by Lee et al. (2019) to pre-train retrievers, uses a given sentence as a query and predicts the context surrounding it. While showing promising results as pre-training (Chang et al., 2020; Sachan et al., 2021), this approach still lags behind BM25 when used as a zero-shot retriever. ICT is strongly related to contrastive learning (Wu et al., 2018), which has been widely applied in computer vision (Chen et al., 2020; He et al., 2020). In particular, computer vision models trained with the latest contrastive learning methods led to features well suited to retrieval (Caron et al., 2021). We thus propose to revisit how well contrastive learning performs to train dense retrievers without supervision.</p>
<p>In this paper, we make the following contributions. First, we show that contrastive learning can lead to strong unsupervised retrievers: our model achieves Recall@100 results competitive with BM25 on most of the BEIR benchmark. Second, in a few-shot setting, we show that our model benefits from few training examples, and obtains better results than transferring models from large datasets such as MS MARCO. Third, when used as a pre-training method before fine-tuning on MS MARCO, our technique leads to strong performance on the BEIR benchmark. We perform ablations to motivate our design choices, and show that cropping works better than the inverse Cloze task. Finally we train a multilingual dense retriever with contrastive learning and show that it achieves state-of-the-art performance.</p>
<p>Code and pre-trained models are available here: https://github.com/facebookresearch/contriever.</p>
<h1>2 Related work</h1>
<p>In this section, we briefly review relevant work in information retrieval, and application of machine learning to this problem. This is not an exhaustive review, and we refer the reader to Manning et al. (2008), Mitra et al. (2018) and Lin et al. (2020) for a more complete introduction to the field.</p>
<p>Term-frequency based information retrieval. Historically, in information retrieval, documents and queries are represented as sparse vectors where each element of the vectors corresponds to a term of the vocabulary. Different weighing schemes have been proposed, to determine how important a particular term is to a document in a large dataset. One of the most used weighing scheme is known as TF-IDF, and is based on inverse document frequency, or term specificity (Jones, 1972). BM25, which is still widely used today, extends TF-IDF (Robertson et al., 1995). A well known limitation of these approaches is that they rely on near-exact match to retrieve documents. This led to the introduction of latent semantic analysis (Deerwester et al., 1990), in which documents are represented as low dimensional dense vectors.</p>
<p>Neural network based information retrieval. Following the successful application of deep learning methods to natural language processing, neural networks techniques were introduced for information retrieval. Huang et al. (2013) proposed a deep bag-of-words model, in which representations of queries and documents are computed independently. A relevance score is then obtained by taking the dot product between representations, and the model is trained end-to-end on click data from a search engine. This method was later refined by replacing the bag-of-words model by convolutional neural networks (Shen et al., 2014) or recurrent neural network (Palangi et al., 2016). A limitation of bi-encoders is that queries and documents are represented by a single vector, preventing the model to capture fine-grained interactions between terms. Nogueira \&amp; Cho</p>
<p>(2019) introduced a cross-encoder model, based on the BERT model (Devlin et al., 2019), which jointly encodes queries and documents. The application of a strong pre-trained model, as well as the cross-encoder architecture, lead to important improvement on the MS MARCO benchmark (Bajaj et al., 2016).</p>
<p>The methods described in the previous paragraph were applied to re-rank documents, which were retrieved with a traditional IR system such as BM25. Gillick et al. (2018) first studied whether continuous retrievers, based on bi-encoder neural models, could be viable alternative to re-ranking. In the context of question answering, Karpukhin et al. (2020) introduced a dense passage retriever (DPR) based on the bi-encoder architecture. This model is initialized with a BERT network, and trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Xiong et al. (2020) further extended this work by mining hard negatives with the model itself during optimization, and trained on the MS MARCO dataset. Once a collection of documents, such as Wikipedia articles, is encoded, retrieval is performed with a fast k-nearest neighbors library such as FAISS Johnson et al. (2019). To alleviate the limitations of bi-encoders, Humeau et al. (2019) introduces the poly-encoder architecture, where documents are encoded by multiple vectors. Similarly, Khattab et al. (2020) proposes the ColBERT model, which keeps a vector representation for each term of the queries and documents. To make the retrieval tractable, the term-level function is approximated to first retrieve an initial set of candidates, which are then re-ranked with the true score. In the context of question answering, knowledge distillation has been used to train retrievers, either using the attention scores of the reader of the downstream task as synthetic labels (Izacard &amp; Grave, 2020a), or the relevance score from a cross encoder (Yang &amp; Seo, 2020). Luan et al. (2020) compares, theoretically and empirically, the performance of sparse and dense retrievers, including bi-, cross- and poly-encoders. Dense retrievers, such as DPR, can lead to indices weighing nearly 100GB when encoding document collections such as Wikipedia. Izacard et al. (2020) shows how to compress such indices, with limited impact on performance, making them more practical to use.</p>
<p>Self-supervised learning for NLP. Following the success of word2vec (Mikolov et al., 2013), many self-supervised techniques have been proposed to learn representation of text. Here, we briefly review the ones that are most related to our approach: sentence level models and contrastive techniques. Jernite et al. (2017) introduced different objective functions to learn sentence representations, including next sentence prediction and sentence order prediction. These objectives were later used in pre-trained models based on transformers, such as BERT (Devlin et al., 2019) and AlBERT (Lan et al., 2019). In the context of retrieval, Lee et al. (2019) introduced the inverse cloze task (ICT), whose purpose is to predict the context surrounding a span of text. Guu et al. (2020) integrated a bi-encoder retriever model in a BERT pre-training scheme. The retrieved documents are used as additional context in the BERT task, and the whole system is trained end-to-end in an unsupervised way. Similarly, Lewis et al. (2020) proposed to jointly learn a retriever and a generative seq2seq model, using self-supervised training. Chang et al. (2020) compares different pre-training tasks for retrieval, including the inverse cloze task. In the context of natural language processing, Fang et al. (2020) proposed to apply MoCo where positive pairs of sentences are obtained using back-translation. Different works augmented the masked language modeling objective with a contrastive loss (Giorgi et al., 2020; Wu et al., 2020; Meng et al., 2021). SBERT (Reimers &amp; Gurevych, 2019) uses a Siamese network similar to contrastive learning to learn a BERT-like model that is adapted to matching sentence embeddings. Their formulation is similar to our work but requires aligned pairs of sentences to form positive pairs while we propose to use data augmentation to leverage large unaligned textual corpora. Concurrent to this work, Gao &amp; Callan (2021) have also shown the potential of contrastive learning for information retrieval; building on the same observation that both tasks share a similar structure. Spider (Ram et al., 2021), a contemporary work, uses spans appearing multiple times in a document to create pseudo examples for contrastive learning in order to train unsupervised retrievers. Finally, Chen et al. (2021) train a dense retriever to imitate unsupervised lexical-based methods. This improves performance on a range of tasks and achieves state-of-the-art results when combining the resulting dense retriever with Contriever, our model pre-trained with contrastive learning.</p>
<h2>3 Method</h2>
<p>In this section, we describe how to train a dense retriever with no supervision. We review the model architecture and then describe contrastive learning — a key component of its training.</p>
<p>The objective of a retriever is to find relevant documents in a large collection for a given query. Thus, the retriever takes as input the set of documents and the query and outputs a relevance score for each document. A standard approach is to encode each query-document pair with a neural network [Nogueira \&amp; Cho, 2019]. This procedure requires re-encoding every document for any new query and hence does not scale to large collections of documents. Instead, we follow standard approaches [Huang et al., 2013; Karpukhin et al., 2020] in information retrieval and use a bi-encoder architecture where documents and queries are encoded independently. The relevance score between a query and a document is given by the dot product between their representations after applying the encoder. More precisely, given a query $q$ and document $d$, we encode each of them independently using the same model, $f_{\theta}$, parameterized by $\theta$. The relevance score $s(q, d)$ between a query $q$ and a document $d$ is then the dot product of the resulting representations:</p>
<p>$$
s(q, d)=\left\langle f_{\theta}(q), f_{\theta}(d)\right\rangle
$$</p>
<p>In practice, we use a transformer network for $f_{\theta}$ to embed both queries and documents. Alternatively, two different encoders can be used to encode queries and documents respectively as in DPR [Karpukhin et al., 2020]. Empirically, we observed that using the same encoder, such as in Xiong et al. (2020) and Reimers \&amp; Gurevych (2019), generally improves robustness in the context of zero-shot transfer or few-shot learning, while having no impact on other settings. Finally, the representation $f_{\theta}(q)$ (resp. $f_{\theta}(d)$ ) for a query (resp. document) is obtained by averaging the hidden representations of the last layer. Following previous work on dense retrieval with neural networks, we use the BERT base uncased architecture and refer the reader to Devlin et al. (2019) for more details.</p>
<h1>3.1 Unsupervised training on unaligned documents</h1>
<p>In this section, we describe our unsupervised training method. We briefly review the loss function traditionally used in contrastive learning and also used in ICT [Lee et al., 2019]. We then discuss obtaining positive pairs from a single text document, a critical ingredient for this training paradigm.</p>
<h3>3.1.1 Contrastive learning</h3>
<p>Contrastive learning is an approach that relies on the fact that every document is, in some way, unique. This signal is the only information available in the absence of manual supervision. A contrastive loss is used to learn by discriminating between documents. This loss compares either positive (from the same document) or negative (from different documents) pairs of document representations. Formally, given a query $q$ with an associated positive document $k_{+}$, and a pool of negative documents $\left(k_{i}\right)_{i=1 . . K}$, the contrastive InfoNCE loss is defined as:</p>
<p>$$
\mathcal{L}\left(q, k_{+}\right)=-\frac{\exp \left(s\left(q, k_{+}\right) / \tau\right)}{\exp \left(s\left(q, k_{+}\right) / \tau\right)+\sum_{i=1}^{K} \exp \left(s\left(q, k_{i}\right) / \tau\right)}
$$</p>
<p>where $\tau$ is a temperature parameter. This loss encourages positive pairs to have high scores and negative pairs to have low scores. Another interpretation of this loss function is the following: given the query representation $q$, the goal is to recover, or retrieve, the representation $k_{+}$corresponding to the positive document, among all the negatives $k_{i}$. In the following, we refer to the left-hand side representations in the score $s$ as queries and the right-hand side representations as keys.</p>
<h3>3.1.2 Building positive pairs from a single document</h3>
<p>A crucial element of contrastive learning is how to build positive pairs from a single input. In computer vision, this step relies on applying two independent data augmentations to the same image, resulting in two "views" that form a positive pair [Wu et al., 2018; Chen et al., 2020]. While we consider similar independent text transformation, we also explore dependent transformations designed to reduce the correlation between views.</p>
<p>Inverse Cloze Task is a data augmentation that generates two mutually exclusive views of a document, introduced in the context of retrieval by Lee et al. (2019). The first view is obtained by randomly sampling a span of tokens from a segment of text, while the complement of the span forms the second view. Specifically, given a sequence of text $\left(w_{1}, \ldots, w_{n}\right)$, ICT samples a span $\left(w_{a}, \ldots, w_{b}\right)$, where $1 \leq a \leq b \leq n$, uses the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Unsupervised retrieval. We compare our pre-training without using <em>any</em> annotated data to REALM (Guu et al., 2020), SimCSE (Gao et al., 2021) and BM25. For SimCSE we report results of the model using RoBERTa large. REALM uses annotated entity recognition data for training. We highlight that our unsupervised pre-training is on par with BM25 but on 2 datasets.</p>
<p>tokens of the span as the query and the complement $(w_1, ..., w_{a-1}, w_{b+1}, ..., w_n)$ as the key. In the original implementation by Lee et al. (2019) the span corresponds to a sentence, and is kept in the document 10% of the time to encourage lexical matching. The Inverse Cloze Task is closely related to the Cloze task which uses the span complement $(w_1, ..., w_{a-1}, w_{b+1}, ..., w_n)$ as the query.</p>
<p><strong>Independent cropping</strong> is a common independent data augmentation used for images where views are generated independently by cropping the input. In the context of text, cropping is equivalent to sampling a span of tokens. This strategy thus samples independently two spans from a document to form a positive pair. As opposed to the inverse Cloze task, in <em>cropping</em> both views of the example correspond to contiguous subsequence of the original data. A second difference between cropping and ICT is the fact that independent random cropping is symmetric: both the queries and documents follow the same distribution. Independent cropping also leads to overlap between the two views of the data, hence encouraging the network to learn exact matches between the query and document, in a way that is similar to lexical matching methods like BM25. In practice, we can either fix the length of the span for the query and the key, or sample them.</p>
<p><strong>Additional data augmentation.</strong> Finally, we also consider additional data augmentations such as random word deletion, replacement or masking. We use these perturbations in addition to random cropping.</p>
<h3>3.1.3 Building large set of negative pairs</h3>
<p>An important aspect of contrastive learning is to sample a large set of negatives. Most standard frameworks differ from each other in terms of how the negatives are handled, and we briefly describe two of them, in-batch negative sampling and MoCo, that we use in this work.</p>
<p><strong>Negatives within a batch.</strong> A first solution is to generate the negatives by using the other examples from the same batch: each example in a batch is transformed twice to generate positive pairs, and we generate negatives by using the views from the other examples in the batch. We will refer to this technique as "in-batch negatives". In that case, the gradient is back-propagated through the representations of both the queries and the keys. A downside of this approach is that it requires extremely large batch sizes to work well Chen et al. (2020), with Qu et al. (2021) reporting improvement in the context of information retrieval up to 8192 negatives. This method has been widely used to train information retrieval models with supervised data Chen et al. (2017b); Karpukhin et al. (2020) and was also considered when using ICT to pre-train retrievers by Lee et al. (2019).</p>
<p>Negative pairs across batches. An alternative approach is to store representations from previous batches in a queue and use them as negative examples in the loss (Wu et al., 2018). This allows for smaller batch size but slightly changes the loss by making it asymmetric between "queries" (one of the view generated from the elements of the current batch), and "keys" (the elements stored in the queue). Gradient is only backpropagated through the "queries", and the representation of the "keys" are considered as fixed. In practice, the features stored in the queue from previous batches comes form previous iterations of the network. This leads to a drop of performance when the network rapidly changes during training. Instead, He et al. (2020) proposed to generate representations of keys from a second network that is updated more slowly. This approach, called MoCo, considers two networks: one for the keys, parametrized by $\theta_{k}$, and one of the query, parametrized by $\theta_{q}$. The parameters of the query network are updated with backpropagation and stochastic gradient descent, similarly to when using in-batch negatives, while the parameters of the key network, or Momentum encoder, is updated from the parameters of the query network by using a exponential moving average:</p>
<p>$$
\theta_{k} \leftarrow m \theta_{k}+(1-m) \theta_{q}
$$</p>
<p>where $m$ is the momentum parameter that takes its value in $[0,1]$.</p>
<h1>4 Experiments</h1>
<p>In this section, we empirically evaluate our best retriever trained with contrastive learning, called Contriever (contrastive retriever), which uses MoCo with random cropping. We use a contrastive learning procedure that differs from ICT (Lee et al., 2019) mainly in three aspects. First, positive pairs are sampled using random cropping and tokens from each element of the pair are deleted with a probability of $10 \%$. Second we use MoCo where negatives consists of elements from previous batches stored in a queue. This allows to scale to a large number of negatives. Third we use data from Wikipedia and CCNet (Wenzek et al., 2020) for training. Ablation studies motivating these technical choices are performed in Section 6. More technical details about our model are given in Appendix A.1.</p>
<h3>4.1 Datasets</h3>
<p>Contriever is trained with contrastive learning on documents sampled from a mix between Wikipedia data and CCNet data (Wenzek et al., 2020), where half the batches are sampled from each source.</p>
<p>First, we evaluate our model on two question answering datasets: NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). For both datasets, we use the open domain versions as introduced by Lee et al. (2019), and the English Wikipedia dump from Dec. 20, 2018 as the collection of documents to retrieve from. We report the top-k retrieval accuracy, i.e. the number of questions for which at least one of the top-k passages contain the answer.</p>
<p>Second, we use the BEIR benchmark, introduced by Thakur et al. (2021), which contains 18 retrieval datasets, corresponding to nine tasks, such as fact checking or citation prediction, and covering different domains, such as Wikipedia or scientific publications. Most datasets from BEIR do not contain a training set, and the focus of the benchmark is zero-shot retrieval. However, most machine learning based retrievers are still trained on supervised data, such as the large scale retrieval dataset MS MARCO (Bajaj et al., 2016). Following standard practice, we report two metrics on this benchmark: nDCG@10 and Recall@100. The nDCG@10 focuses on the ranking of the top 10 retrieved documents, and is good at evaluating rankings returned to humans, for example in a search engine. On the other hand, Recall@100 is relevant to evaluate retrievers that are used in machine learning systems, such as question answering. Indeed, such models can process hundreds of documents, and ignore their ranking (Izacard \&amp; Grave, 2020b). While nDCG@10 is the main metric of BEIR, we are more interested in the Recall@100 to evaluate bi-encoders, as our goal is to develop retrievers that can be used in ML systems. Moreover, in many settings, retrieved documents can be re-ranked with a more powerful model such as a cross-encoder, thus improving the nDCG@10.</p>
<p>Table 1: Unsupervised recall@k on the test sets of NaturalQuestions and TriviaQA. For Inverse Cloze Task and Masked Salient Spans we report the results of Sachan et al. (2021). The Masked Salient Spans model uses annotated named entity recognition data. For BM25 we report the results of Ma et al. (2021)</p>
<table>
<thead>
<tr>
<th></th>
<th>NaturalQuestions</th>
<th></th>
<th></th>
<th>TriviaQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>R@5</td>
<td>R@20</td>
<td>R@100</td>
<td>R@5</td>
<td>R@20</td>
<td>R@100</td>
</tr>
<tr>
<td>Inverse Cloze Task (Sachan et al., 2021)</td>
<td>32.3</td>
<td>50.9</td>
<td>66.8</td>
<td>40.2</td>
<td>57.5</td>
<td>73.6</td>
</tr>
<tr>
<td>Masked salient spans (Sachan et al., 2021)</td>
<td>41.7</td>
<td>59.8</td>
<td>74.9</td>
<td>53.3</td>
<td>68.2</td>
<td>79.4</td>
</tr>
<tr>
<td>BM25 (Ma et al., 2021)</td>
<td>-</td>
<td>62.9</td>
<td>78.3</td>
<td>-</td>
<td>76.4</td>
<td>83.2</td>
</tr>
<tr>
<td>Contriever</td>
<td>47.8</td>
<td>67.8</td>
<td>82.1</td>
<td>59.4</td>
<td>74.2</td>
<td>83.2</td>
</tr>
<tr>
<td>supervised model: DPR (Karpukhin et al., 2020)</td>
<td>-</td>
<td>78.4</td>
<td>85.4</td>
<td>-</td>
<td>79.4</td>
<td>85.0</td>
</tr>
<tr>
<td>supervised model: FiD-KD (Izacard \&amp; Grave, 2020a)</td>
<td>73.8</td>
<td>84.3</td>
<td>89.3</td>
<td>77.0</td>
<td>83.6</td>
<td>87.7</td>
</tr>
</tbody>
</table>
<h1>4.2 Baselines</h1>
<p>First, we compare Contriever to BM25, which does not require supervision. On QA datasets, we compare to dense retrievers trained with ICT and the Masked Salient Spans from Sachan et al. (2021). On BEIR, we consider the retriever from REALM (Guu et al., 2020), and RoBERTa large fine-tuned with SimCSE (Gao et al., 2021), as unsupervised dense retrievers. We also compare to ML-based retrievers trained on MS MARCO, classified in three categories: sparse, dense and late-interaction. For sparse methods, we compare to Splade $v 2$ (Formal et al., 2021), which computes sparse representations of documents with BERT pre-trained model. For dense methods, we use $D P R$ (Karpukhin et al., 2020) and $A N C E$ (Xiong et al., 2020), which are bi-encoders trained on supervised data such as NaturalQuestions or MS MARCO. We also compare to TAS-B (Hofstätter et al., 2021), which performs distillation from a cross-encoder to a bi-encoder, and GenQ, which creates synthetic query-document pairs with a generative model. ${ }^{1}$ For late-interaction, we use ColBERT (Khattab et al., 2020), which computes pairwise scores between contextualized representations of queries and documents, as well as a cross-encoder used to re-rank documents retrieved with BM25.</p>
<h3>4.3 Results</h3>
<p>First, we compare the performance of fully unsupervised models, i.e., without fine-tuning on MS MARCO or other annotated data. In Table 1, we report the retrieval performance on two question answering datasets: NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Here, our model is competitive with a strong BM25 baseline (Ma et al., 2021), for example leading to 3 points improvement for the recall@100 on NaturalQuestions. It also outperforms previously proposed dense retrievers which were trained with ICT or salient span masking. In Figure 1 we report the recall@100 performance of unsupervised models on the BEIR benchmark. Interestingly, we observe that in this setting, Contriever is competitive compared to BM25 on all datasets, but TREC-COVID and Tóuche-2020. In particular, it obtains better performance than BM25 on 11 out of 15 datasets from the benchmark for the recall@100. Contriever also outperforms previously proposed unsupervised dense retrievers, which obtains lower performance than BM25 in general. For the nDCG@10, which puts more emphasis on the very first retrieved documents, while Contriever largely closes the gap between unsupervised retrievers and BM25, it is still outperformed by BM25 as reported in Table 11. The difference is mainly due to the fact that BM25 largely outperforms Contriever on two datasets with specific features: Trec-COVID and Tóuche-2020. Trec-COVID is an information retrieval dataset related to COVID. However data used to train Contriever were collected before the COVID outbreak, thus they may not be adapted. Tóuche-2020 contains long documents, which does not seem to be very well supported by dense neural retrievers: even after supervised training, models are still lagging behind BM25. Overall, these results show the potential of contrastive learning to train fully unsupervised dense retrievers.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: BEIR Benchmark. We report nDCG@10 on the test sets from the BEIR benchmark for bi-encoder methods without re-ranker. We also report the average and number of datasets where a method is the best ("Best on") over the entire BEIR benchmark (excluding three datasets because of their licence). Bold is the best overall. MS MARCO is excluded from the average. "CE" refers to cross-encoder.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">BM25+CE</th>
<th style="text-align: center;">DPR</th>
<th style="text-align: center;">ANCE</th>
<th style="text-align: center;">TAS-B</th>
<th style="text-align: center;">Gen-Q</th>
<th style="text-align: center;">ColBERT</th>
<th style="text-align: center;">Splade v2</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Ours+CE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MS MARCO</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">$\mathbf{4 7 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Trec-COVID</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">$\mathbf{7 5 . 7}$</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: left;">NFCorpus</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">$\mathbf{3 5 . 0}$</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">34.4</td>
</tr>
<tr>
<td style="text-align: left;">NQ</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">$\mathbf{5 7 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">HotpotQA</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">$\mathbf{7 1 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">FiQA</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">$\mathbf{3 6 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">ArguAna</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">$\mathbf{4 9 . 3}$</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">41.3</td>
</tr>
<tr>
<td style="text-align: left;">Touche-2020</td>
<td style="text-align: center;">$\mathbf{3 6 . 7}$</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: left;">CQADupStack</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">37.0 .</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">$\mathbf{3 7 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Quora</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">$\mathbf{8 6 . 5}$</td>
<td style="text-align: center;">82.4</td>
</tr>
<tr>
<td style="text-align: left;">DBPedia</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">$\mathbf{4 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Scidocs</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">$\mathbf{1 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">FEVER</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">$\mathbf{8 1 . 9}$</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">$\mathbf{8 1 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Climate-FEVER</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">$\mathbf{2 5 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Scifact</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">$\mathbf{6 9 . 3}$</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: left;">Avg. w/o CQA</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">51.2</td>
</tr>
<tr>
<td style="text-align: left;">Avg.</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">50.2</td>
</tr>
<tr>
<td style="text-align: left;">Best on</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">9</td>
</tr>
</tbody>
</table>
<p>Table 3: Few-shot retrieval. Test nDCG@10 after training on a small in-domain training set. We compare BERT and our model, with and without an intermediate fine-tuning step on MS MARCO. Note that our unsupervised pre-training alone outperforms BERT with intermediate MS MARCO fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Additional data</th>
<th style="text-align: center;">SciFact</th>
<th style="text-align: center;">NFCorpus</th>
<th style="text-align: center;">FiQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># queries</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">729</td>
<td style="text-align: center;">2,590</td>
<td style="text-align: center;">5,500</td>
</tr>
<tr>
<td style="text-align: left;">BM25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">23.6</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: left;">Contriever</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">MS MARCO</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: left;">Contriever</td>
<td style="text-align: center;">MS MARCO</td>
<td style="text-align: center;">$\mathbf{8 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{3 5 . 8}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 1}$</td>
</tr>
</tbody>
</table>
<p>Next, we report nDCG@10 on the BEIR benchmark for different retrievers trained on MS MARCO in Table 2 (recall@100 can be found in Table 10 of appendix). We individually report results on each dataset as well as the average over 14 datasets of the BEIR Benchmark (excluding 3 for license reasons). We observe that when used as pre-training, contrastive learning leads to strong performance: contriever obtains the best results among dense bi-encoder methods for the nDCG@10, and is state-of-the-art for the recall@100 (improving the average recall@100 from 65.0 to 67.1 ). This strong recall@100 performance can be further exploited by using a cross-encoder ${ }^{2}$ to re-rank the retrieved documents: this leads to the state-of-the-art on 8 datasets of the BEIR benchmark for the nDCG@10, as well as on average. It should be noted that our fine-tuning procedure on MS MARCO is simpler than for other retrievers, as we use a simple strategy for negative mining and do not use distillation. Our model would probably also benefits from improvements proposed by these retrievers, but this is beyond the scope of this paper.</p>
<p>Finally, we illustrate the benefit of our retriever compared to BM25 in a few-shot setting, where we have access to a small number of in-domain retrieval examples. This setting is common in practice, and lexical based methods, like BM25, cannot leverage the small training sets to adapt its weights. In Table 3, we report nDCG@10 on three datasets from BEIR associated with the smallest training sets, ranging from 729 to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>5,500 queries. We observe that on these small datasets, our pre-training leads to better results than BERT pre-training, even when BERT is fine-tuned on MS MARCO as an intermediate step. Our pre-trained model also outperforms BM25, showing the advantage of dense retriever over lexical methods in the few-shot setting. More details are given in Appendix A.3.</p>
<h1>5 Multilingual retrieval</h1>
<p>In this section, we illustrate another advantage of learning unsupervised dense retrievers, when performing multi-lingual retrieval. While large labeled datasets are available in English, allowing to train strong dense retrievers (as shown in previous sections), this is unfortunately not the case for lower resources languages. Here, we show how unsupervised training is a promising direction. First, we show that our approach leads to strong performance, either in a full unsupervised setting, or by fine-tuning a multi-lingual model on English data such as MS MARCO. Second, we demonstrate that our model can also perform cross-lingual retrieval, by retrieving English documents from other languages queries. Unsupervised retrievers based on lexical matching, such as BM25, would obtain poor performance, especially for pairs of languages with different scripts such as English and Arabic.</p>
<h3>5.1 Multilingual pre-training</h3>
<p>Our multilingual model, called $m$ Contriever, is jointly pre-trained on 29 languages. The multilingual pretraining largely follows the method discussed in previous sections, but it differs by few particularities related to the pre-training data and the hyperparameters used. The model is initialized with the multilingual version of BERT, mBERT, trained on 104 languages. For the pre-training data, we consider the languages contained in CCNet (Wenzek et al., 2020) that are also part of our evaluation datasets. This results in a training set containing the CCNet data for 29 languages detailed in Table 12. During pre-training, examples are uniformly sampled over languages, i.e. the probability that a training sample comes from a specific language is the same for all languages. We observed that increasing the number of languages considered for pre-training generally deteriorates performance as reported in Appendix B. 3 similarly to what has been observed for multilingual masked language models (Conneau et al., 2019). We pre-trained our multilingual mContriever with a queue size of 32768 . This generally improves stability, and is able to compensate for the additional instabilities observed in the multilingual setting. More detailed hyperparameters are given in Appendix B.1.</p>
<h3>5.2 Fine-tuning</h3>
<p>Large labeled datasets for information retrieval are generally available only in English. It is therefore natural to investigate whether large monolingual datasets can be leveraged for multilingual retrieval. We consider fine-tuning our pre-trained mContriever model on MS MARCO. This generally improves performance in all languages. The model trained on MS MARCO can be further fine-tuned on Mr. TyDi achieving state-of-the-art performance on this dataset. Further details regarding fine-tuning are given in Appendix B.2.</p>
<h3>5.3 Evaluation</h3>
<p>We evaluate the performance of our pre-trained model with and without fine-tuning on English data on two different benchmarks. First, we consider Mr. TyDi (Zhang et al., 2021), a multilingual information retrieval benchmark derived from TyDi QA (Clark et al., 2020). Given a question, the goal is to find relevant documents in a pool of Wikipedia documents in the same language.</p>
<p>In Mr. TyDi the pool of documents is restricted to the language of the query. Being able to retrieve relevant documents from another language is desirable to leverage large source of information that may no be available in all languages. In order to evaluate cross-lingual retrieval performance we derive an evaluation setting from MKQA (Longpre et al., 2020). Given a question in a specific language, we perform retrieval in English Wikipedia, and evaluate if the English answer is in the retrieved documents. The MKQA dataset makes this possible by providing the same questions and answers in 26 languages. We remove unanswerable questions, questions accepting a binary yes/no answer and questions with long answers from the original MKQA dataset.</p>
<p>Table 4: Multilingual retrieval on Mr. TyDi. We report MRR@100 and Recall@100 on the test sets of Mr. TyDi. mContriever fine-tuned on MS MARCO is compared against its counterparts without contrastive pre-training using a similar fine-tuning recipe, referred to as $m B E R T+M S M A R C O$, as well as a model initialized with XLM-R referred to as $X L M-R+M S M A R C O$. We also report the results after fine-tuning on Mr. TyDi for the model trained on MS MARCO.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ar</th>
<th style="text-align: center;">bn</th>
<th style="text-align: center;">en</th>
<th style="text-align: center;">fi</th>
<th style="text-align: center;">id</th>
<th style="text-align: center;">ja</th>
<th style="text-align: center;">ko</th>
<th style="text-align: center;">ru</th>
<th style="text-align: center;">sw</th>
<th style="text-align: center;">te</th>
<th style="text-align: center;">th</th>
<th style="text-align: center;">avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MRR@100</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BM25 (Zhang et al., 2021)</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: center;">mDPR (Zhang et al., 2021)</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">16.7</td>
</tr>
<tr>
<td style="text-align: center;">Hybrid (Zhang et al., 2021)</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">41.7</td>
</tr>
<tr>
<td style="text-align: center;">mBERT + MS MARCO</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">31.3</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R + MS MARCO</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;">mContriever</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">25.0</td>
</tr>
<tr>
<td style="text-align: center;">+ MS MARCO</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">38.4</td>
</tr>
<tr>
<td style="text-align: center;">+ Mr. Tydi</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;">Recall@100</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BM25 (Zhang et al., 2021)</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;">mDPR (Zhang et al., 2021)</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">47.3</td>
</tr>
<tr>
<td style="text-align: center;">Hybrid (Zhang et al., 2021)</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">80.9</td>
</tr>
<tr>
<td style="text-align: center;">mBERT + MS MARCO</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">76.8</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R + MS MARCO</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">79.7</td>
</tr>
<tr>
<td style="text-align: center;">mContriever</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">77.2</td>
</tr>
<tr>
<td style="text-align: center;">+ MS MARCO</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">87.0</td>
</tr>
<tr>
<td style="text-align: center;">+ Mr. Tydi</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">93.6</td>
</tr>
</tbody>
</table>
<p>This results in an evaluation set of 6619 queries. It should be noted that methods based on term matching such as BM25 are intrinsically limited in this cross-lingual retrieval setting because similar terms in different languages may not match.</p>
<h1>5.4 Baselines</h1>
<p>On Mr. TyDi (Zhang et al., 2021) we report results from the original paper. This includes a BM25 baseline, a model fine-tuned on NaturalQuestions using the DPR pipeline, and an hybrid model combining the two.
On our cross-lingual evaluation benchmark derived from MKQA, we consider the retriever of the CORA question answering pipeline (Asai et al., 2021), trained on a combination of datasets containing the English NaturalQuestions and the cross-lingual XOR-TyDi QA, with data augmentation based on translation.</p>
<p>Additionally, to isolate the effect of contrastive pre-training, we also compare mContriever fine-tuned on MS MARCO to its counterparts without contrastive pre-training, initialized from mBERT. This model is referred as $m B E R T+M S M A R C O$ in tables. We also report results obtained by fine-tuning XLM-R (Conneau et al., 2019) on MS-MARCO. For both models we use the same hyper-parameters used to fine-tune mContriever on MS MARCO except for the temperature, additional details are reported in Appendix B.2.</p>
<h3>5.5 Results</h3>
<p>We report results on Mr. TyDi in Table 4. The effectiveness of the multilingual pre-training appears clearly as the pre-trained model fine-tuned on MS MARCO achieve significantly better performance than its counterparts without pre-training when fine-tuned using the same pipeline. Interestingly fine-tuning on these English-only datasets improves performance on all languages. Moreover our unsupervised mContriever outperforms BM25 for the Recall@100, and after fine-tuning on MS MARCO it achieves state-of-the-art performance for this metric. Performance can be further improved by fine-tuning on the train set of Mr. TyDi. This appears to be</p>
<p>Table 5: Cross-lingual retrieval on MKQA. We report the average on all languages included in MKQA for the Recall@100 and the Recall@20, and report the Recall@100 for a subset of languages. Complete results are reported in Table 13 and Table 14 of appendix.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Avg. R@20</th>
<th style="text-align: center;">Avg. R@100</th>
<th style="text-align: center;">en</th>
<th style="text-align: center;">ar</th>
<th style="text-align: center;">ja</th>
<th style="text-align: center;">ko</th>
<th style="text-align: center;">es</th>
<th style="text-align: center;">he</th>
<th style="text-align: center;">de</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CORA (Asai et al., 2021)</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">$\mathbf{7 5 . 6}$</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">$\mathbf{6 8 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">mBERT + MS MARCO</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">59.6</td>
</tr>
<tr>
<td style="text-align: left;">XLM-R + MS MARCO</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">61.1</td>
</tr>
<tr>
<td style="text-align: left;">mContriever</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">49.0</td>
</tr>
<tr>
<td style="text-align: left;">+ MS MARCO</td>
<td style="text-align: center;">$\mathbf{5 3 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 6}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 6}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 3}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 6}$</td>
<td style="text-align: center;">66.6</td>
</tr>
</tbody>
</table>
<p>Table 6: MoCo vs. in-batch negatives. In this table, we report nDCG@10 on the BEIR benchmark for in-batch negatives and MoCo, without fine-tuning on the MS MARCO dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">NFCorpus</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">FiQA</th>
<th style="text-align: center;">ArguAna</th>
<th style="text-align: center;">Quora</th>
<th style="text-align: center;">DBPedia</th>
<th style="text-align: center;">SciDocs</th>
<th style="text-align: center;">FEVER</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MoCo</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: left;">In-batch negatives</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">31.9</td>
</tr>
</tbody>
</table>
<p>particularly important for the MRR@100 metric which put emphasis on the quality of the first documents retrieved. For this metric our unsupervised model is still lagging behind BM25.</p>
<p>Results on the cross-lingual retrieval benchmark derived from MKQA are reported in Table 5, with per language details for the Recall@100 and Recall@20 reported in Table 13 and Table 14 of appendix. Interestingly using only supervised training data in English, our mContriever fine-tuned on MS MARCO outperforms the CORA retriever. Also, similarly to the results reported on Mr. TyDi, adding multilingual contrastive pre-training before fine-tuning on MS MARCO improves performance over its counterpart without pre-training. On MKQA, evaluation is performed by lowercasing both queries and documents, we observed that this improves performance. This does not impact the CORA retriever which is based on an uncased version of mBERT.</p>
<h1>6 Ablation studies</h1>
<p>In this section, we investigate the influence of different design choices on our method. In these ablations, all the models are pre-trained on English Wikipedia for 200k gradient steps, with a batch size of 2,048 (on 32 GPUs). Each fine-tuning on MS MARCO takes 20k gradient steps with a batch size of 512 (on 8 GPUs), using AdamW and no hard negatives.</p>
<p>MoCo vs. in-batch negatives. First, we compare the two contrastive pre-training methods: MoCo and in-batch negatives. As in in-batch negatives, the number of negative examples is equal to the batch size, we train models with a batch size of 4,096 and restrict the queue in MoCo to the same number of elements. This experiment measures the effect of using of momentum encoder for the keys instead of the same network as for the queries. Using a momentum also prevents from backpropagating the gradient through the keys. We report results, without fine-tuning on MS MARCO in Table 6. We observe that the difference of performance between the two methods is small, especially after fine-tuning on MS MARCO. We thus propose to use MoCo as our contrastive learning framework, since it scales to a larger number of negative examples without the need to increase the batch size.</p>
<p>Number of negative examples. Next, we study the influence of the number of negatives used in the contrastive loss, by varying the queue size of the MoCo algorithm. We consider values ranging from 2,048 to 131,072, and report results in Figure 2. We see that on average over the BEIR benchmark, increasing the number of negatives leads to better retrieval performance, especially in the unsupervised setting. However, we note that this effect is not equally strong for all datasets.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Impact of the number of negatives. We report nDCG@10 as a function of the queue size, with and without fine-tuning on MS MARCO. We report numbers using the MoCo framework where the keys for the negatives are computed with the momentum encoder and stored in a queue.</p>
<p>Data augmentations. Third, we compare different ways to generate pairs of positive examples from a single document or chunk of text. In particular, we compare random cropping, which leads to pairs with overlap, and the inverse cloze task, which was previously considered to pre-train retrievers. Interestingly, as shown in Table 7, the random cropping strategy outperforms the inverse cloze task in our setting. We believe that random cropping, leading to the identical distributions of keys and queries, leads to more stable training with MoCo compared to ICT. This might explains part of the difference of performance between the two methods. We also investigate whether additional data perturbations, such as random word deletion or replacement, are beneficial for retrieval.</p>
<p>Training data. Finally, we study the impact of the pre-training data on the performance of our retriever, by training on Wikipedia, CCNet or a mix of both sources of data. We report results in Table 8, and observe that there is no clear winner between the two data sources. Unsurprisingly, training on the more diverse CCNet data leads to strong improvements on datasets from different domains than Wikipedia, such as FiQA or Quora. On the other hand, on a dataset like FEVER, training on Wikipedia leads to better results. To get the best of both worlds, we consider two strategies to mix the two data sources. In the " $50 / 50 \%$ " strategy, examples are sampled uniformly across domain, meaning that half the batches are from Wikipedia and the other half from CCNet. In the "uniform" strategy, examples are sampled uniformly over the union of the dataset. Since CCNet is significantly larger than Wikipedia, this means that most of the batches are from CCNet.</p>
<p>Impact of fine-tuning on MS MARCO. To isolate the impact of pre-training from the impact of fine-tuning on MS MARCO, we apply the same fine-tuning to the BERT base uncased model. We report results in Table 9, and observe that when applied to BERT, our fine-tuning leads to results that are lower than the state-of-the-art. Hence, we believe that most of the improvement compared to the state-of-the-art retrievers can be attributed to our contrastive pre-training strategy.</p>
<p>Table 7: Impact of data augmentions. We report nDCG@10 without fine-tuning on MS MARCO.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">NFCorpus</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">ArguAna</th>
<th style="text-align: center;">Quora</th>
<th style="text-align: center;">DBPedia</th>
<th style="text-align: center;">SciDocs</th>
<th style="text-align: center;">FEVER</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ICT</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">25.9</td>
</tr>
<tr>
<td style="text-align: left;">Crop</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">32.2</td>
</tr>
<tr>
<td style="text-align: left;">Crop + delete</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">33.8</td>
</tr>
<tr>
<td style="text-align: left;">Crop + replace</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">32.9</td>
</tr>
</tbody>
</table>
<p>Table 8: Training data. We report nDCG@10 without fine-tuning on MS MARCO.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">NFCorpus</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">FiQA</th>
<th style="text-align: center;">ArguAna</th>
<th style="text-align: center;">Quora</th>
<th style="text-align: center;">DBPedia</th>
<th style="text-align: center;">SciDocs</th>
<th style="text-align: center;">FEVER</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Wiki</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">33.0</td>
</tr>
<tr>
<td style="text-align: left;">CCNet</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">34.9</td>
</tr>
<tr>
<td style="text-align: left;">Uniform</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: left;">$50 / 50 \%$</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">34.7</td>
</tr>
</tbody>
</table>
<p>Table 9: Fine-tuning. We report nDCG@10 after fine-tuning BERT and our model on MS MARCO.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">NFCorpus</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">FiQA</th>
<th style="text-align: center;">ArguAna</th>
<th style="text-align: center;">Quora</th>
<th style="text-align: center;">DBPedia</th>
<th style="text-align: center;">SciDocs</th>
<th style="text-align: center;">FEVER</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">42.0</td>
</tr>
<tr>
<td style="text-align: left;">Contriever</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">46.5</td>
</tr>
</tbody>
</table>
<h1>7 Discussion</h1>
<p>In this work, we propose to explore the limits of contrastive pre-training to learn dense text retrievers. We use the MoCo technique, which allows to train models with a large number of negative examples. We make several interesting observations: first, we show that neural networks trained without supervision using contrastive learning exhibit good retrieval performance, which are competitive with BM25 (albeit not state-of-the-art). These results can be further improved by fine-tuning on the supervised MS MARCO dataset, leading to strong results, in particular for recall@100. Based on that observation, we use a cross-encoder to re-rank documents retrieved with our model, leading to new state-of-the-art on the competitive BEIR benchmark. We also performed extensive ablation experiments, and observed that independent random cropping seems to be a strong alternative to the inverse Cloze task for training retrievers.</p>
<h2>References</h2>
<p>Akari Asai, Xinyan Yu, Jungo Kasai, and Hannaneh Hajishirzi. One question answering model for many languages with cross-lingual dense passage retrieval. CoRR, abs/2107.11976, 2021. URL https://arxiv. org/abs/2107.11976. 10, 11</p>
<p>Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. 3, 6</p>
<p>Adam Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu Mittal. Bridging the lexical chasm: statistical approaches to answer-finding. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 192-199, 2000. 1</p>
<p>Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021. 2</p>
<p>Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for embedding-based large-scale retrieval. arXiv preprint arXiv:2002.03932, 2020. 2, 3</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain questions. In Proc. ACL, 2017a. 1</p>
<p>Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong. On sampling strategies for neural network-based collaborative filtering. arXiv preprint arXiv:1706.07881, 2017b. 5</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020. $2,4,5$</p>
<p>Xilun Chen, Kushal Lakhotia, Barlas Oğuz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one?, 2021. URL https://arxiv.org/abs/2110.06918. 3</p>
<p>Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. Tydi QA: A benchmark for information-seeking question answering in typologically diverse languages. CoRR, abs/2003.05002, 2020. URL https://arxiv.org/abs/2003.05002. 9</p>
<p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. CoRR, abs/1911.02116, 2019. URL http://arxiv.org/abs/1911.02116. $9,10,19$</p>
<p>Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391-407, 1990. 2</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL, 2019. 3, 4</p>
<p>Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. Cert: Contrastive self-supervised learning for language understanding. arXiv preprint arXiv:2005.12766, 2020. 3</p>
<p>Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. Splade v2: Sparse lexical and expansion model for information retrieval. arXiv preprint arXiv:2109.10086, 2021. 7</p>
<p>Luyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540, 2021. 3</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021. 5, 7</p>
<p>Daniel Gillick, Alessandro Presta, and Gaurav Singh Tomar. End-to-end retrieval in continuous space. arXiv preprint arXiv:1811.08008, 2018. 3</p>
<p>John M Giorgi, Osvald Nitski, Gary D Bader, and Bo Wang. Declutr: Deep contrastive learning for unsupervised textual representations. arXiv preprint arXiv:2006.03659, 2020. 3</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020. 3, 5, 7</p>
<p>Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729-9738, 2020. 2, 6, 17</p>
<p>Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. Efficiently teaching an effective dense retriever with balanced topic aware sampling. arXiv preprint arXiv:2104.06967, 2021. 7</p>
<p>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information $\mathcal{E}$ Knowledge Management, pp. 2333-2338, 2013. 2, 4</p>
<p>Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv:1905.01969, 2019. 3</p>
<p>Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering, 2020a. URL https://arxiv.org/abs/2012.04584. 3, 7</p>
<p>Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282, 2020b. 6</p>
<p>Gautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola De Cao, Sebastian Riedel, and Edouard Grave. A memory efficient baseline for open domain question answering. arXiv preprint arXiv:2012.15156, 2020. 3</p>
<p>Yacine Jernite, Samuel R Bowman, and David Sontag. Discourse-based objectives for fast unsupervised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017. 3</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 2019. 3</p>
<p>Karen Sparck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 1972. 2</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proc. ACL, 2017. 6, 7</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. 3, 4, 5,7</p>
<p>Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with colbert, 2020. 3,7</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019. 1, 6, 7</p>
<p>Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. arXiv preprint arXiv:2102.11600, 2021. 17</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019. 3</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proc. $A C L, 2019.2,3,4,5,6$</p>
<p>Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. Pre-training via paraphrasing. arXiv preprint arXiv:2006.15020, 2020. 3</p>
<p>Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. Pretrained transformers for text ranking: Bert and beyond. arXiv preprint arXiv:2010.06467, 2020. 2</p>
<p>Shayne Longpre, Yi Lu, and Joachim Daiber. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. CoRR, abs/2007.15207, 2020. URL https://arxiv.org/abs/2007.15207. 9</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. 17, 18
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and attentional representations for text retrieval. arXiv preprint arXiv:2005.00181, 2020. 3</p>
<p>Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin. A replication study of dense passage retriever, 2021. 7</p>
<p>Christopher D Manning, Hinrich Schütze, and Prabhakar Raghavan. Introduction to information retrieval. Cambridge university press, 2008. 2</p>
<p>Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia Song. Coco-lm: Correcting and contrasting text sequences for language model pretraining. arXiv preprint arXiv:2102.08473, 2021. 3</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. $3111-3119,2013.3$</p>
<p>Bhaskar Mitra, Nick Craswell, et al. An introduction to neural information retrieval. Foundations and Trends® in Information Retrieval, 13(1):1-126, 2018. 2</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human generated machine reading comprehension dataset. In CoCo@ NIPS, 2016. 1</p>
<p>Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019. 2,4</p>
<p>Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(4):694-707, 2016. 2</p>
<p>Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5835-5847, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.466. URL https://aclanthology.org/2021.naacl-main.466. 5</p>
<p>Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. Learning to retrieve passages without supervision, 2021. URL https://arxiv.org/abs/2112.07708. 3</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. 3, 4</p>
<p>Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc, 2009. 1</p>
<p>Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at TREC-3. NIST Special Publication Sp, 1995. 2</p>
<p>Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering, 2021. 2, 7</p>
<p>Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. Learning semantic representations using convolutional neural networks for web search. In Proceedings of the 23rd international conference on world wide web, pp. 373-374, 2014. 2</p>
<p>Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021. $2,6,18,19$</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018. 1</p>
<p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference, 2020. 6, 9, 17</p>
<p>Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3733-3742, 2018. 2, 4, 6</p>
<p>Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive learning for sentence representation. arXiv preprint arXiv:2012.15466, 2020. 3</p>
<p>Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. 3, 4, 7</p>
<p>Sohee Yang and Minjoon Seo. Is retriever merely an approximator of reader? arXiv preprint arXiv:2010.10999, 2020. 3</p>
<p>Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. tydi: A multi-lingual benchmark for dense retrieval. CoRR, abs/2108.08787, 2021. URL https://arxiv.org/abs/2108.08787. 9, 10</p>
<h1>A Technical details for Contriever</h1>
<h2>A. 1 Contrastive pre-training</h2>
<p>For the model with fine-tuning on MS MARCO, we use the MoCo algorithm He et al. (2020) with a queue of size 131,072, a momentum value of 0.9995 and a temperature of 0.05 . We use the random cropping data augmentation, with documents of 256 tokens and span sizes sampled between $5 \%$ and $50 \%$ of the document length. Documents are simply random piece of text sampled from a mix between Wikipedia and CCNet data (Wenzek et al., 2020), where half the batches are sampled from each source. We also apply token deletion with a probability of $10 \%$. We optimize the model with the AdamW (Loshchilov \&amp; Hutter, 2019) optimizer, with learning rate of $5 \cdot 10^{-5}$, batch size of 2,048 and 500,000 steps. We initialize the network with the publicly available BERT base uncased model.</p>
<h2>A. 2 Fine-tuning on MS MARCO</h2>
<p>For the fine-tuning on MS MARCO we do not use the MoCo algorithm and simply use in-batch negatives. We use the ASAM optimizer (Kwon et al., 2021), with a learning rate of $10^{-5}$ and a batch size of 1024 with a temperature of 0.05 , also used during pre-training. We train an initial model with random negative examples for 20000 steps, mine hard negatives with this first model, and re-train a second model with those. Each query is associated with a gold document and a negative document, which is a random document in the first phase and a hard negative $10 \%$ of the time in the second phase. For each query, all documents from the current batch aside of the gold document are used as negatives.</p>
<h2>A. 3 Few-shot training</h2>
<p>For the few-shot evaluation presented in Table 3, we train for 500 epochs on each dataset with a batch size of 256 with in-batch random negatives. We evaluate performance performance on the development set every 100 gradient updates and perform early stopping based on this metric. For SciFact, we hold out randomly $10 \%$ of the training data and use them as development set, leading to a train set containing 729 samples.</p>
<p>Table 10: BEIR Benchmark. We report the recall@100 on the test sets from the BEIR benchmark for bi-encoder methods. We report the capped recall@100 on Trec-COVID following the original BEIR setup. Note that using a cross-encoder to re-rank the top-100 documents do not change the recall@100, hence, we do not include these methods in this table. We also report the average and number of datasets where a method is the best ("Best on") over the entire BEIR benchmark (excluding three datasets because of their licence). Bold is the best overall. On Trec-COVID we report the capped Recall@100, see <em>Thakur et al. (2021)</em> for more details. MS MARCO is excluded from the average.</p>
<table>
<thead>
<tr>
<th></th>
<th>BM25</th>
<th>DPR</th>
<th>ANCE</th>
<th>TAS-B</th>
<th>Gen-Q</th>
<th>ColBERT</th>
<th>Splade v2</th>
<th>Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td>MS MARCO</td>
<td>65.8</td>
<td>55.2</td>
<td>85.2</td>
<td>88.4</td>
<td>88.4</td>
<td>86.5</td>
<td>-</td>
<td>89.1</td>
</tr>
<tr>
<td>Trec-COVID</td>
<td>49.8</td>
<td>21.2</td>
<td>45.7</td>
<td>38.7</td>
<td>45.6</td>
<td>46.4</td>
<td>12.3</td>
<td>40.7</td>
</tr>
<tr>
<td>NFCorpus</td>
<td>25.0</td>
<td>20.8</td>
<td>23.2</td>
<td>28.0</td>
<td>28.0</td>
<td>25.4</td>
<td>27.7</td>
<td>30.0</td>
</tr>
<tr>
<td>NQ</td>
<td>76.0</td>
<td>88.0</td>
<td>83.6</td>
<td>90.3</td>
<td>86.2</td>
<td>91.2</td>
<td>93.0</td>
<td>92.5</td>
</tr>
<tr>
<td>HotpotQA</td>
<td>74.0</td>
<td>59.1</td>
<td>57.8</td>
<td>72.8</td>
<td>67.3</td>
<td>74.8</td>
<td>82.0</td>
<td>77.7</td>
</tr>
<tr>
<td>FiQA</td>
<td>53.9</td>
<td>34.2</td>
<td>58.1</td>
<td>59.3</td>
<td>61.8</td>
<td>60.3</td>
<td>62.1</td>
<td>65.6</td>
</tr>
<tr>
<td>ArguAna</td>
<td>94.2</td>
<td>75.1</td>
<td>93.7</td>
<td>94.2</td>
<td>97.8</td>
<td>91.4</td>
<td>97.2</td>
<td>97.7</td>
</tr>
<tr>
<td>Touche-2020</td>
<td>53.8</td>
<td>30.1</td>
<td>45.8</td>
<td>43.1</td>
<td>45.1</td>
<td>43.9</td>
<td>35.4</td>
<td>29.4</td>
</tr>
<tr>
<td>CQADupStack</td>
<td>60.6</td>
<td>40.3</td>
<td>57.9</td>
<td>62.2</td>
<td>65.4</td>
<td>62.4</td>
<td>-</td>
<td>66.3</td>
</tr>
<tr>
<td>Quora</td>
<td>97.3</td>
<td>47.0</td>
<td>98.7</td>
<td>98.6</td>
<td>98.8</td>
<td>98.9</td>
<td>98.7</td>
<td>99.3</td>
</tr>
<tr>
<td>DBPedia</td>
<td>39.8</td>
<td>34.9</td>
<td>31.9</td>
<td>49.9</td>
<td>43.3</td>
<td>46.1</td>
<td>57.5</td>
<td>54.1</td>
</tr>
<tr>
<td>Scidocs</td>
<td>35.6</td>
<td>21.9</td>
<td>26.9</td>
<td>33.5</td>
<td>33.2</td>
<td>34.4</td>
<td>36.4</td>
<td>37.8</td>
</tr>
<tr>
<td>Fever</td>
<td>93.1</td>
<td>84.0</td>
<td>90.0</td>
<td>93.7</td>
<td>92.8</td>
<td>93.4</td>
<td>95.1</td>
<td>94.9</td>
</tr>
<tr>
<td>Climate-fever</td>
<td>43.6</td>
<td>39.0</td>
<td>44.5</td>
<td>53.4</td>
<td>45.0</td>
<td>44.4</td>
<td>52.4</td>
<td>57.4</td>
</tr>
<tr>
<td>Scifact</td>
<td>90.8</td>
<td>72.7</td>
<td>81.6</td>
<td>89.1</td>
<td>89.3</td>
<td>87.8</td>
<td>92.0</td>
<td>94.7</td>
</tr>
<tr>
<td>Avg. w/o CQA</td>
<td>63.6</td>
<td>48.3</td>
<td>60.1</td>
<td>65.0</td>
<td>64.2</td>
<td>64.5</td>
<td>64.8</td>
<td>67.1</td>
</tr>
<tr>
<td>Avg.</td>
<td>63.4</td>
<td>47.7</td>
<td>60.0</td>
<td>64.8</td>
<td>64.2</td>
<td>64.3</td>
<td>-</td>
<td>67.0</td>
</tr>
<tr>
<td>Best on</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>4</td>
<td>7</td>
</tr>
</tbody>
</table>
<h2>B Multilingual retrieval with mContriever</h2>
<h3>B.1 Hyperparameters for multilingual contrastive pre-training</h3>
<p>The pre-trained mContriever model is pre-trained for 500,000 steps with a queue of size 32768, and temperature of 0.05 and a momentum value of 0.999. We optimize the model with the AdamW (Loshchilov &amp; Hutter, 2019) optimizer, with learning rate of 5 · 10⁻⁵. The learning rate follows a linear warmup for 20,000 steps followed by linear decay until the end of training. Languages used for pre-training are detailed in Table 12.</p>
<h3>B.2 Hyperparameters for multilingual fine-tuning</h3>
<p>We fine-tune mContriever using in-batch negatives, AdamW optimizer (Loshchilov &amp; Hutter, 2019), a learning rate of 10⁻⁵, and a batch size of 1024 samples with a temperature τ of 0.05. On MS MARCO and Mr. TyDi the model is trained for 20k gradient steps. We notice overfitting on NaturalQuestions, and thus reduced the training to 1k gradient steps. We use a warmup of 1000 gradient steps with linear decay afterwards in all cases. Hard negatives are mined on Mr. TyDi with the model trained on MS MARCO. We did not observe significant improvements using hard negatives on MS MARCO and NaturalQuestions.</p>
<p>For the fine-tuning on MS MARCO of the models initiliazed from mBERT (resp. XLM-R) without contrastive pre-training, we use a temperature τ of 1 (resp. 5). We tried temperatures in {10, 5, 2, 1, 0.1, 0.05} and chose the one leading to the best performance. We observed a decrease in performance for lower temperatures. We fine-tuned mContriever with τ = 0.05 following the temperature used during pre-training. We followed the temperature τ = 0.05 used for the training of Contriever, and did not test other temperatures for the contrastive pre-training of the multilingual model, mContriever.</p>
<p>Table 11: Unsupervised retrieval. Performance of unsupervised methods on the BEIR datasets. We report the capped recall@100 on Trec-COVID following the original BEIR setup. For SimCSE we report results of the model using RoBERTa large. REALM uses annotated entity recognition data for training. On Trec-COVID we report the capped Recall@100, see <em>Thakur et al. (2021)</em> for more details.</p>
<table>
<thead>
<tr>
<th>Model $(\rightarrow)$</th>
<th>BM25</th>
<th>BERT</th>
<th>SimCSE</th>
<th>REALM</th>
<th>Contriever</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset $(\downarrow)$</td>
<td>Recall@100</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MS MARCO</td>
<td>65.8</td>
<td>3.5</td>
<td>33.6</td>
<td>52.6</td>
<td>67.2</td>
</tr>
<tr>
<td>Trec-COVID</td>
<td>49.8</td>
<td>10.6</td>
<td>26.8</td>
<td>8.1</td>
<td>17.2</td>
</tr>
<tr>
<td>NFCorpus</td>
<td>25.0</td>
<td>6.7</td>
<td>18.2</td>
<td>23.0</td>
<td>29.4</td>
</tr>
<tr>
<td>NQ</td>
<td>76.0</td>
<td>14.3</td>
<td>42.9</td>
<td>58.1</td>
<td>77.1</td>
</tr>
<tr>
<td>HotpotQA</td>
<td>74.0</td>
<td>15.8</td>
<td>42.7</td>
<td>56.1</td>
<td>70.4</td>
</tr>
<tr>
<td>FiQA-2018</td>
<td>53.9</td>
<td>6.9</td>
<td>41.0</td>
<td>28.0</td>
<td>56.2</td>
</tr>
<tr>
<td>ArguAna</td>
<td>94.2</td>
<td>59.1</td>
<td>95.2</td>
<td>73.1</td>
<td>90.1</td>
</tr>
<tr>
<td>Tôuche-2020</td>
<td>53.8</td>
<td>3.0</td>
<td>18.6</td>
<td>11.5</td>
<td>22.5</td>
</tr>
<tr>
<td>CQADupStack</td>
<td>60.6</td>
<td>11.0</td>
<td>48.9</td>
<td>35.5</td>
<td>61.4</td>
</tr>
<tr>
<td>Quora</td>
<td>97.3</td>
<td>74.6</td>
<td>97.9</td>
<td>92.7</td>
<td>98.7</td>
</tr>
<tr>
<td>DBPedia</td>
<td>39.8</td>
<td>7.1</td>
<td>21.5</td>
<td>33.0</td>
<td>45.3</td>
</tr>
<tr>
<td>SCIDOCS</td>
<td>35.6</td>
<td>11.3</td>
<td>23.0</td>
<td>23.1</td>
<td>36.0</td>
</tr>
<tr>
<td>Fever</td>
<td>93.1</td>
<td>13.6</td>
<td>50.8</td>
<td>82.6</td>
<td>93.6</td>
</tr>
<tr>
<td>Climate-fever</td>
<td>43.6</td>
<td>12.8</td>
<td>44.8</td>
<td>42.3</td>
<td>44.1</td>
</tr>
<tr>
<td>SciFact</td>
<td>90.8</td>
<td>35.2</td>
<td>75.3</td>
<td>83.8</td>
<td>92.6</td>
</tr>
<tr>
<td>Avg.</td>
<td>63.6</td>
<td>19.0</td>
<td>45.4</td>
<td>46.9</td>
<td>60.1</td>
</tr>
<tr>
<td>Best on</td>
<td>3</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>10</td>
</tr>
<tr>
<td></td>
<td>NDCG@10</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MS MARCO</td>
<td>22.8</td>
<td>0.6</td>
<td>8.8</td>
<td>15.2</td>
<td>20.6</td>
</tr>
<tr>
<td>Trec-COVID</td>
<td>65.6</td>
<td>16.6</td>
<td>38.6</td>
<td>20.1</td>
<td>27.4</td>
</tr>
<tr>
<td>NFCorpus</td>
<td>32.5</td>
<td>2.5</td>
<td>14.0</td>
<td>24.1</td>
<td>31.7</td>
</tr>
<tr>
<td>NQ</td>
<td>32.9</td>
<td>2.7</td>
<td>12.6</td>
<td>15.2</td>
<td>25.4</td>
</tr>
<tr>
<td>HotpotQA</td>
<td>60.3</td>
<td>4.9</td>
<td>23.3</td>
<td>40.5</td>
<td>48.1</td>
</tr>
<tr>
<td>FiQA-2018</td>
<td>23.6</td>
<td>1.4</td>
<td>14.8</td>
<td>9.7</td>
<td>24.5</td>
</tr>
<tr>
<td>ArguAna</td>
<td>31.5</td>
<td>23.1</td>
<td>45.6</td>
<td>22.8</td>
<td>37.9</td>
</tr>
<tr>
<td>Tôuche-2020</td>
<td>36.7</td>
<td>3.4</td>
<td>11.6</td>
<td>7.3</td>
<td>19.3</td>
</tr>
<tr>
<td>CQADupStack</td>
<td>29.9</td>
<td>2.5</td>
<td>20.2</td>
<td>13.5</td>
<td>28.4</td>
</tr>
<tr>
<td>Quora</td>
<td>78.9</td>
<td>3.9</td>
<td>81.5</td>
<td>71.6</td>
<td>83.5</td>
</tr>
<tr>
<td>DBPedia</td>
<td>31.3</td>
<td>3.9</td>
<td>13.7</td>
<td>22.7</td>
<td>29.2</td>
</tr>
<tr>
<td>SCIDOCS</td>
<td>15.8</td>
<td>2.7</td>
<td>7.4</td>
<td>9.0</td>
<td>14.9</td>
</tr>
<tr>
<td>FEVER</td>
<td>75.3</td>
<td>4.9</td>
<td>20.1</td>
<td>42.9</td>
<td>68.2</td>
</tr>
<tr>
<td>Climate-fever</td>
<td>21.3</td>
<td>4.1</td>
<td>17.6</td>
<td>14.3</td>
<td>15.5</td>
</tr>
<tr>
<td>SciFact</td>
<td>66.5</td>
<td>9.8</td>
<td>38.5</td>
<td>47.1</td>
<td>64.9</td>
</tr>
<tr>
<td>Avg.</td>
<td>41.7</td>
<td>8.7</td>
<td>24.6</td>
<td>25.1</td>
<td>36.0</td>
</tr>
<tr>
<td>Best on</td>
<td>12</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3>B.3 Curse of multilinguality</h3>
<p>We tried to pre-train models on different sets of languages. We generally observed performance deterioration when scaling to more languages similarly to what has been observed for general multilingual masked language models <em>Conneau et al. (2019)</em>. In Table 15 we report results on Mr. TyDi with a model pre-trained on the 11 languages of Mr. TyDi versus the model used in the rest of the paper which has been pre-trained on 29 languages including the 11 languages of Mr. TyDi as detailed in Table 12. We also report performance of these models after training on MS MARCO, eventually followed by further fine-tuning on Mr. TyDi. It appears that the performance of the unsupervised model and the performance after fine-tuning on MS MARCO are better for the model pre-trained only on 11 languages. The difference is mitigated after fine-tuning on Mr. TyDi.</p>
<p>Table 12: List of languages used for multilingual retrieval.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language <br> Pre-training <br> Mr. TyDi <br> MKQA</th>
<th style="text-align: center;">ar <br> Arabic <br> ✓</th>
<th style="text-align: center;">bn <br> Bengali <br> ✓</th>
<th style="text-align: center;">da <br> Danish <br> ✓</th>
<th style="text-align: center;">de <br> German <br> ✓</th>
<th style="text-align: center;">en <br> English <br> ✓</th>
<th style="text-align: center;">es <br> Spanish <br> ✓</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fi</td>
<td style="text-align: center;">fr</td>
<td style="text-align: center;">he</td>
<td style="text-align: center;">hu</td>
<td style="text-align: center;">it</td>
<td style="text-align: center;">id</td>
</tr>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">Finnish</td>
<td style="text-align: center;">French</td>
<td style="text-align: center;">Hebrew</td>
<td style="text-align: center;">Hungarian</td>
<td style="text-align: center;">Italian</td>
<td style="text-align: center;">Indonesian</td>
</tr>
<tr>
<td style="text-align: center;">Pre-training</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;">Mr. TyDi</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ja</td>
<td style="text-align: center;">km</td>
<td style="text-align: center;">ko</td>
<td style="text-align: center;">ms</td>
<td style="text-align: center;">nl</td>
<td style="text-align: center;">no</td>
</tr>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">Japanese</td>
<td style="text-align: center;">Khmer</td>
<td style="text-align: center;">Korean</td>
<td style="text-align: center;">Malay</td>
<td style="text-align: center;">Dutch</td>
<td style="text-align: center;">Norwegian</td>
</tr>
<tr>
<td style="text-align: center;">Pre-training</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;">Mr. TyDi</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">pl</td>
<td style="text-align: center;">pt</td>
<td style="text-align: center;">ru</td>
<td style="text-align: center;">sv</td>
<td style="text-align: center;">sw</td>
<td style="text-align: center;">te</td>
</tr>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">Polish</td>
<td style="text-align: center;">Portugese</td>
<td style="text-align: center;">Russian</td>
<td style="text-align: center;">Swedish</td>
<td style="text-align: center;">Swahili</td>
<td style="text-align: center;">Telugu</td>
</tr>
<tr>
<td style="text-align: center;">Pre-training</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;">Mr. TyDi</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">th</td>
<td style="text-align: center;">tr</td>
<td style="text-align: center;">vi</td>
<td style="text-align: center;">zh-cn</td>
<td style="text-align: center;">zh-hk</td>
<td style="text-align: center;">zh-tw</td>
</tr>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">Thai</td>
<td style="text-align: center;">Turkish</td>
<td style="text-align: center;">Vietnamese</td>
<td style="text-align: center;">Chinese (Simplified)</td>
<td style="text-align: center;">Chinese (Hong Kong)</td>
<td style="text-align: center;">Chinese (Traditional)</td>
</tr>
<tr>
<td style="text-align: center;">Pre-training</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✓</td>
</tr>
<tr>
<td style="text-align: center;">Mr. TyDi</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
<td style="text-align: center;">✗</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
<td style="text-align: center;">✓</td>
</tr>
</tbody>
</table>
<p>Table 13: Recall@100 on MKQA for cross-lingual retrieval in the setting described in Section 5.3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">avg</th>
<th style="text-align: center;">en</th>
<th style="text-align: center;">ar</th>
<th style="text-align: center;">fi</th>
<th style="text-align: center;">ja</th>
<th style="text-align: center;">ko</th>
<th style="text-align: center;">ru</th>
<th style="text-align: center;">es</th>
<th style="text-align: center;">sv</th>
<th style="text-align: center;">he</th>
<th style="text-align: center;">th</th>
<th style="text-align: center;">da</th>
<th style="text-align: center;">de</th>
<th style="text-align: center;">fr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CORA</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: center;">mBERT + MS MARCO</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R + MS MARCO</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;">Contriever</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">50.2</td>
</tr>
<tr>
<td style="text-align: center;">+ MS MARCO</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">it</td>
<td style="text-align: center;">nl</td>
<td style="text-align: center;">pl</td>
<td style="text-align: center;">pt</td>
<td style="text-align: center;">hu</td>
<td style="text-align: center;">vi</td>
<td style="text-align: center;">ms</td>
<td style="text-align: center;">km</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">tr</td>
<td style="text-align: center;">zh-cn</td>
<td style="text-align: center;">zh-hk</td>
<td style="text-align: center;">zh-tw</td>
</tr>
<tr>
<td style="text-align: center;">CORA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">52.8</td>
</tr>
<tr>
<td style="text-align: center;">mBERT + MS MARCO</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">59.3</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R + MS MARCO</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">55.7</td>
</tr>
<tr>
<td style="text-align: center;">Contriever</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">52.5</td>
</tr>
<tr>
<td style="text-align: center;">+ MS MARCO</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">64.3</td>
</tr>
</tbody>
</table>
<p>Table 14: Recall@20 on MKQA for cross-lingual retrieval in the setting described in Section 5.3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">avg</th>
<th style="text-align: right;">en</th>
<th style="text-align: right;">ar</th>
<th style="text-align: right;">fi</th>
<th style="text-align: right;">ja</th>
<th style="text-align: right;">ko</th>
<th style="text-align: right;">ru</th>
<th style="text-align: right;">es</th>
<th style="text-align: right;">sv</th>
<th style="text-align: right;">he</th>
<th style="text-align: right;">th</th>
<th style="text-align: right;">da</th>
<th style="text-align: right;">de</th>
<th style="text-align: right;">fr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CORA</td>
<td style="text-align: right;">49.0</td>
<td style="text-align: right;">$\mathbf{6 8 . 5}$</td>
<td style="text-align: right;">31.7</td>
<td style="text-align: right;">49.7</td>
<td style="text-align: right;">34.1</td>
<td style="text-align: right;">33.1</td>
<td style="text-align: right;">46.5</td>
<td style="text-align: right;">$\mathbf{6 0 . 3}$</td>
<td style="text-align: right;">58.1</td>
<td style="text-align: right;">36.8</td>
<td style="text-align: right;">33.6</td>
<td style="text-align: right;">$\mathbf{5 9 . 4}$</td>
<td style="text-align: right;">$\mathbf{5 8 . 5}$</td>
<td style="text-align: right;">$\mathbf{6 1 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">mBERT + MS MARCO</td>
<td style="text-align: right;">45.3</td>
<td style="text-align: right;">65.5</td>
<td style="text-align: right;">30.2</td>
<td style="text-align: right;">38.9</td>
<td style="text-align: right;">41.7</td>
<td style="text-align: right;">34.5</td>
<td style="text-align: right;">44.3</td>
<td style="text-align: right;">52.4</td>
<td style="text-align: right;">50.5</td>
<td style="text-align: right;">32.6</td>
<td style="text-align: right;">38.5</td>
<td style="text-align: right;">52.5</td>
<td style="text-align: right;">46.6</td>
<td style="text-align: right;">53.8</td>
</tr>
<tr>
<td style="text-align: left;">XLM-R + MS MARCO</td>
<td style="text-align: right;">46.7</td>
<td style="text-align: right;">64.5</td>
<td style="text-align: right;">29.0</td>
<td style="text-align: right;">45.1</td>
<td style="text-align: right;">39.7</td>
<td style="text-align: right;">34.9</td>
<td style="text-align: right;">45.9</td>
<td style="text-align: right;">51.4</td>
<td style="text-align: right;">56.1</td>
<td style="text-align: right;">32.5</td>
<td style="text-align: right;">49.4</td>
<td style="text-align: right;">55.8</td>
<td style="text-align: right;">48.3</td>
<td style="text-align: right;">50.5</td>
</tr>
<tr>
<td style="text-align: left;">Contriever</td>
<td style="text-align: right;">31.4</td>
<td style="text-align: right;">50.2</td>
<td style="text-align: right;">26.6</td>
<td style="text-align: right;">26.7</td>
<td style="text-align: right;">29.4</td>
<td style="text-align: right;">27.9</td>
<td style="text-align: right;">32.7</td>
<td style="text-align: right;">20.7</td>
<td style="text-align: right;">37.6</td>
<td style="text-align: right;">22.2</td>
<td style="text-align: right;">31.1</td>
<td style="text-align: right;">31.2</td>
<td style="text-align: right;">31.2</td>
<td style="text-align: right;">30.7</td>
</tr>
<tr>
<td style="text-align: left;">+ MS MARCO</td>
<td style="text-align: right;">$\mathbf{5 3 . 9}$</td>
<td style="text-align: right;">67.2</td>
<td style="text-align: right;">$\mathbf{4 0 . 1}$</td>
<td style="text-align: right;">$\mathbf{5 5 . 1}$</td>
<td style="text-align: right;">$\mathbf{4 6 . 2}$</td>
<td style="text-align: right;">$\mathbf{4 1 . 7}$</td>
<td style="text-align: right;">$\mathbf{5 2 . 3}$</td>
<td style="text-align: right;">59.3</td>
<td style="text-align: right;">$\mathbf{6 0 . 0}$</td>
<td style="text-align: right;">$\mathbf{4 5 . 6}$</td>
<td style="text-align: right;">$\mathbf{5 2 . 0}$</td>
<td style="text-align: right;">62.0</td>
<td style="text-align: right;">54.8</td>
<td style="text-align: right;">59.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;">it</td>
<td style="text-align: right;">nl</td>
<td style="text-align: right;">pl</td>
<td style="text-align: right;">pt</td>
<td style="text-align: right;">hu</td>
<td style="text-align: right;">vi</td>
<td style="text-align: right;">ms</td>
<td style="text-align: right;">km</td>
<td style="text-align: right;">no</td>
<td style="text-align: right;">tr</td>
<td style="text-align: right;">zh-cn</td>
<td style="text-align: right;">zh-hk</td>
<td style="text-align: right;">zh-tw</td>
</tr>
<tr>
<td style="text-align: left;">CORA</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">58.2</td>
<td style="text-align: right;">$\mathbf{6 3 . 5}$</td>
<td style="text-align: right;">54.3</td>
<td style="text-align: right;">$\mathbf{5 8 . 4}$</td>
<td style="text-align: right;">47.6</td>
<td style="text-align: right;">49.8</td>
<td style="text-align: right;">57.6</td>
<td style="text-align: right;">24.8</td>
<td style="text-align: right;">58.8</td>
<td style="text-align: right;">49.1</td>
<td style="text-align: right;">38.6</td>
<td style="text-align: right;">40.5</td>
<td style="text-align: right;">39.6</td>
</tr>
<tr>
<td style="text-align: left;">mBERT + MS MARCO</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">52.1</td>
<td style="text-align: right;">55.3</td>
<td style="text-align: right;">45.6</td>
<td style="text-align: right;">49.5</td>
<td style="text-align: right;">44.6</td>
<td style="text-align: right;">46.9</td>
<td style="text-align: right;">49.9</td>
<td style="text-align: right;">21.5</td>
<td style="text-align: right;">51.3</td>
<td style="text-align: right;">42.7</td>
<td style="text-align: right;">44.6</td>
<td style="text-align: right;">45.3</td>
<td style="text-align: right;">45.5</td>
</tr>
<tr>
<td style="text-align: left;">XLM-R + MS MARCO</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">45.4</td>
<td style="text-align: right;">54.5</td>
<td style="text-align: right;">48.5</td>
<td style="text-align: right;">49.6</td>
<td style="text-align: right;">47.3</td>
<td style="text-align: right;">49.7</td>
<td style="text-align: right;">54.0</td>
<td style="text-align: right;">$\mathbf{3 3 . 4}$</td>
<td style="text-align: right;">53.7</td>
<td style="text-align: right;">48.7</td>
<td style="text-align: right;">42.4</td>
<td style="text-align: right;">42.4</td>
<td style="text-align: right;">42.0</td>
</tr>
<tr>
<td style="text-align: left;">Contriever</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">38.6</td>
<td style="text-align: right;">45.1</td>
<td style="text-align: right;">25.1</td>
<td style="text-align: right;">37.6</td>
<td style="text-align: right;">28.3</td>
<td style="text-align: right;">27.3</td>
<td style="text-align: right;">39.6</td>
<td style="text-align: right;">15.7</td>
<td style="text-align: right;">33.2</td>
<td style="text-align: right;">26.5</td>
<td style="text-align: right;">35.0</td>
<td style="text-align: right;">32.7</td>
<td style="text-align: right;">32.5</td>
</tr>
<tr>
<td style="text-align: left;">+ MS MARCO</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$\mathbf{5 9 . 4}$</td>
<td style="text-align: right;">60.9</td>
<td style="text-align: right;">$\mathbf{5 8 . 1}$</td>
<td style="text-align: right;">56.9</td>
<td style="text-align: right;">$\mathbf{5 5 . 2}$</td>
<td style="text-align: right;">$\mathbf{5 5 . 9}$</td>
<td style="text-align: right;">$\mathbf{6 0 . 9}$</td>
<td style="text-align: right;">26.2</td>
<td style="text-align: right;">$\mathbf{6 1 . 0}$</td>
<td style="text-align: right;">$\mathbf{5 6 . 7}$</td>
<td style="text-align: right;">$\mathbf{5 0 . 9}$</td>
<td style="text-align: right;">$\mathbf{5 1 . 9}$</td>
<td style="text-align: right;">$\mathbf{5 1 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 15: Performance dilution for multilingual retrievers. We report MRR@100 and R@100 on the test set of Mr. TyDi after pre-training on two different sets of languages, one containing the 11 languages of Mr. TyDi which is included in the set of 29 languages used to train mContriever used in the rest of the paper. We also report results of these models after fine-tuning on MS MARCO, potentially followed by a final fine-tuning stage on Mr. TyDi.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ar</th>
<th style="text-align: center;">bn</th>
<th style="text-align: center;">en</th>
<th style="text-align: center;">fi</th>
<th style="text-align: center;">id</th>
<th style="text-align: center;">ja</th>
<th style="text-align: center;">ko</th>
<th style="text-align: center;">ru</th>
<th style="text-align: center;">sw</th>
<th style="text-align: center;">te</th>
<th style="text-align: center;">th</th>
<th style="text-align: center;">avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MRR@100</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11 languages</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">26.8</td>
</tr>
<tr>
<td style="text-align: center;">29 languages</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">25.0</td>
</tr>
<tr>
<td style="text-align: center;">+ MS MARCO</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11 languages</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">39.7</td>
</tr>
<tr>
<td style="text-align: center;">29 languages</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">38.4</td>
</tr>
<tr>
<td style="text-align: center;">+ Mr. TyDi</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11 languages</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;">29 languages</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;">R@100</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11 languages</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">78.9</td>
</tr>
<tr>
<td style="text-align: center;">29 languages</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">77.2</td>
</tr>
<tr>
<td style="text-align: center;">+ MS MARCO</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11 languages</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: center;">29 languages</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">87.0</td>
</tr>
<tr>
<td style="text-align: center;">+ Mr. TyDi</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11 languages</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">93.9</td>
</tr>
<tr>
<td style="text-align: center;">29 languages</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">93.6</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We use the existing ms-marco-MiniLM-L-6-v2 cross-encoder model to perform the re-ranking.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7971 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7971</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7971</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-cb92a7f9d9dbcf9145e32fdfa0e70e2a6b828eb1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cb92a7f9d9dbcf9145e32fdfa0e70e2a6b828eb1" target="_blank">The Semantic Scholar Open Data Platform</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper combines public and proprietary data sources using state-of-theart techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date.</p>
                <p><strong>Paper Abstract:</strong> The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-theart techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7971",
    "paper_id": "paper-cb92a7f9d9dbcf9145e32fdfa0e70e2a6b828eb1",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004448,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Semantic Scholar Open Data Platform</h1>
<p>Rodney Kinney, Chloe Anastasiades, Russell Authur<em>, Iz Beltagy; Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola</em>, Stefan Candra, Yoganand Chandrasekhar; Arman Cohan<em>, Miles Crawford</em>, Doug Downey, Jason Dunkelberger, Oren Etzioni; Rob Evans, Sergey Feldman, Christopher Fiorelli; Daniel King; Joseph Gorney; David Graham, Dany Haddad, Fangzhou Hu, Regan Huff, Daniel King<em>, Sebastian Kohlmeier</em>, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu<em>, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi</em>, Paul Sayre, Zejiang Shen<em>, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian</em>, Amber Tanaka, Alex D. Wade<em>, Linda Wagner</em>, Lucy Lu Wang<em>, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine Van Zuylen</em>, Daniel S. Weld<br>Allen Institute for Artificial Intelligence<br>Seattle, Washington<br>{rodneyk, dougd, danw}@allenai.org</p>
<h4>Abstract</h4>
<p>The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-the-art techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 225M+ papers, 100M+ authors, 650M+ paper-authorship edges, and 2.8B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will periodically update this document to reflect improvements and new data offerings.</p>
<h2>1 Introduction</h2>
<p>Semantic Scholar ${ }^{1}$ (S2), was launched in 2015 by the Allen Institute for Artificial Intelligence ${ }^{2}$ (AI2) to help scholars combat information overload and more efficiently discover and understand the most relevant research literature. Through a growing number of partnerships with scientific publishers and preprint services, Semantic Scholar has built a comprehensive and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>open corpus of scientific publications as a public service. While the Semantic Scholar website provides many features, such as automatically-generated author pages, personalized libraries, and paper recommendations, most of the site's data and functionality is also available via data download, open-source libraries, and API services.</p>
<p>In this paper, we overview the primary technology used to build the corpus, and the API services and downloads we provide to access it. We hope that the resources described in this article can further accelerate a variety of work that depends critically on highquality scholarly data. Research into scientific NLP and science of science benefit directly from such resources. More generally, we are enabling the development of new applications that help scientists discover and understand the literature of their field. The need for timely and comprehensive scholarly data has become more imperative since the 2021 sunsetting of the Microsoft Academic Graph (MAG) (Sinha et al., 2015), which was long a standard source for scholarly data in the community.</p>
<p>The remainder of the paper proceeds as follows. In Section 2, we give an overview of the data and services that comprise the platform. In Section 3, we describe the platform's data processing pipeline. In Section 4, we describe our public APIs and downloadable datasets. In Section 5, we discuss related work. In Section 6, we conclude and discuss future work.</p>
<h2>2 Platform Overview</h2>
<p>The purpose of the Semantic Scholar Open Data Platform is to build and distribute the Semantic Scholar Academic Graph, or S2AG (pronounced "stag"). S2AG is a disambiguated, high-quality, bibliographic knowledge graph. The nodes of S2AG represent papers, authors, venues, and academic institutions. The</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the Semantic Scholar platform</p>
<p>edges represent papers written by an author, papers cited by another paper, papers published in a venue, and authors affiliated with an institution. S2AG is built by ingesting a variety of data sources into our data processing pipeline, and is distributed via a variety of publicly-available APIs and datasets, as illustrated in Figure 1.</p>
<p>The work we describe here is an update of a previous version of the Semantic Scholar data pipeline described in Ammar et al. (2018). The core structure of the graph (authors, papers and their relationships) is the same, although the components that build the knowledge graph have been refined. The entity extraction and linking described in Ammar et al. (2018) have been deprecated in favor of the semantic features described below.</p>
<p>Table 1 summarizes the overall size of S2AG as of January 2023.<sup>3</sup> S2AG covers many natural, physical, and social sciences. Table 2 summarizes the number of paper records in different academic fields of study. Some papers are assigned to multiple fields, whereas many are unclassified ("n/a"), due to lack of sufficient information.</p>
<h2>3 Data Processing Pipeline</h2>
<p>At the core of Semantic Scholar lies a sophisticated data processing pipeline that continually ingests documents and metadata from numerous sources, extracting full text and metadata from PDFs, normalizing and disambiguating authors, institutions, and venues, classifying each paper's field of study, generating a textual summary of its key results, and more.</p>
<p>The pipeline builds S2AG by ingesting academic paper metadata and PDF content from a variety of <em>Data Sources</em>. A <em>PDF Content Extraction</em> system extracts structured data from unstructured PDFs. The extracted content, along with structured metadata from the input sources, is processed by a series of <em>Knowledge Graph Construction</em> systems that build S2AG. A set of models add <em>Semantic Features</em> to the graph, such as such as paper summarization and vector embeddings.</p>
<p>Many components of our pipeline are available as open software or models. Table 3 provides a high-level summary alongside links to publicly available code, models and datasets, where applicable. Many of these also have an associated published research article, to which we refer readers seeking further details.</p>
<h3>3.1 Data Sources</h3>
<p>The pipeline has more than 50 input sources. They include non-profit organizations such as Crossref, preprint servers such as arXiv, academic publishers through negotiated agreements, and our own internet web crawler. Sources may provide metadata in the JATS<sup>4</sup> format, or a variety of proprietary formats. Data may be pushed or pulled via FTP, fetched via an API, or downloaded in bulk via HTTP. Most sources are updated daily. An important additional source is humancreated data. Users of the web site can claim the identity of an author in our corpus and manually curate the papers associated with that author. Our team also makes manual corrections in response to email requests from users. The first task of the pipeline is to fetch the latest data from each source and parse it into a normalized format. Sources typically provide limited information about a paper in structured form: typically the title, author names, venue, and date, often linked to a PDF file.</p>
<h3>3.2 PDF Content Extraction</h3>
<p>We augment the structured metadata by parsing the unstructured information in a paper's PDF into structured</p>
<p><sup>3</sup>We are populating the author-affiliation links at the time of writing, so we have given the expected eventual number.</p>
<p><sup>4</sup>https://jats.nlm.nih.gov/</p>
<table>
<thead>
<tr>
<th>Nodes</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>paper</td>
<td>225M</td>
</tr>
<tr>
<td>author</td>
<td>105M</td>
</tr>
<tr>
<td>publication venue</td>
<td>195k</td>
</tr>
<tr>
<td>Edges</td>
<td></td>
</tr>
<tr>
<td>citation</td>
<td>2.8B</td>
</tr>
<tr>
<td>paper-author</td>
<td>650M</td>
</tr>
<tr>
<td>paper-venue</td>
<td>65M</td>
</tr>
</tbody>
</table>
<p>Table 1: Approximate size of S2AG as of May 2025.
form, resulting in fine-grained information about the complete text. A critical output of our PDF content extraction is a structured bibliography from which we can construct the citation graph. Other important outputs include section headers, paragraphs, figures and tables, and inline references to figures, tables, and bibliography entries. PDFs are fundamentally a printformat specification, and extracting structured information from them is difficult and subject to errors. Nevertheless, they are the de-facto representation of a paper's official content, and we have invested heavily in our extraction technology. There are three steps to PDF content extraction: Text Extraction, Visual Region Annotation, and Text Span Annotation.</p>
<p>Text Extraction is the process of converting the set of commands that indicate where characters should appear on the page into plain text, i.e., inferring word boundaries and word order. For this, we use existing open-source toolkits, including pdfalto, PDFPlumber, ${ }^{6}$ and/or PDFMiner. ${ }^{7}$</p>
<p>It is possible to extract decent-quality content using only the plain text from the Text Extraction step. For this, we have used Grobid ${ }^{8}$ as well as our own extractor called ScienceParse. ${ }^{9,10}$ Our latest pipeline uses visual recognition, described below, to improve the quality of the extraction.</p>
<p>The Visual Region Annotation step first generates a visual image for each page, using poppler. ${ }^{11}$. Within each page, we run an object detection model ${ }^{12}$ from the LayoutParser (Shen et al., 2021) library which identifies bounding boxes and labels each one with visual layout categories such as figure, table, paragraph. The code, model, and data are available at https://github.com/Layout-Parser/layout-parser.</p>
<p>Text Span Annotation is the process of giving semantic labels to tokens from the Text Extraction step.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Paper record counts in S2AG for different academic fields, as of May 2025</p>
<p>For this, we use the I-VILA model trained on the S2VL dataset, both introduced in Shen et al. (2022), to tag tokens with categories such as title, author name, section header, body text, figure caption, bibliography, etc. The code, model, and data are available at https://github.com/allenai/VILA.</p>
<p>Visual Region and Text Span annotations are converted into structured data (e.g., section headers are associated with section content, figures are associated with their captions, and bibliography entries are decomposed into their constituent elements) using the PaperMage library, which makes its code, models and data available at https://github.com/allenai/papermage. The final output of the PDF extraction pipeline is a structured data object suitable for encoding in JSON format. It includes core metadata such as title, authors, and abstract, as well as the full body text with detailed structural information about the text.</p>
<h3>3.3 Knowledge Graph Construction</h3>
<p>The output of the PDF processing above, like the structured data in our input sources, consists of plain-string names for real world entities. Those entities include authors, the institutions with which those authors are affiliated, the venues in which the papers were published, and of course, the papers themselves. To build S2AG, we must assign an ID to each real-world entity and associate each plain-string name with the appropriate ID.</p>
<p>Paper Deduplication is necessary because we process data from many independent sources. Paper titles are not unique, and can vary in how they are expressed. Authors can release updated versions of a</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Model</th>
<th>Datasets</th>
<th>GitHub</th>
</tr>
</thead>
<tbody>
<tr>
<td>Visual Region</td>
<td>EfficientDet (Tan et al., 2020) via</td>
<td>PubLayNet</td>
<td>Layout-Parser/layout-parser</td>
</tr>
<tr>
<td>Annotation</td>
<td>LayoutParser (Shen et al., 2021)</td>
<td>(Zhong et al., 2019)</td>
<td></td>
</tr>
<tr>
<td>Text Span</td>
<td>I-VILA</td>
<td>S2-VL</td>
<td>allenai/VILA</td>
</tr>
<tr>
<td>Annotation</td>
<td>(Shen et al., 2022)</td>
<td>(Shen et al., 2022)</td>
<td></td>
</tr>
<tr>
<td>Paper</td>
<td>S2APLER</td>
<td>S2APLER</td>
<td>allenai/S2APLER</td>
</tr>
<tr>
<td>Deduplication</td>
<td>S2APLER</td>
<td>S2APLER</td>
<td>allenai/S2APLER</td>
</tr>
<tr>
<td>Author</td>
<td>S2AND</td>
<td>S2AND</td>
<td>allenai/S2AND</td>
</tr>
<tr>
<td>Disambiguation</td>
<td>(Subramanian et al., 2021)</td>
<td>(Subramanian et al., 2021)</td>
<td></td>
</tr>
<tr>
<td>Affiliation</td>
<td>S2AFF</td>
<td>S2AFF</td>
<td>allenai/S2AFF</td>
</tr>
<tr>
<td>Normalization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>TLDR</td>
<td>BART (Lewis et al., 2019) with</td>
<td>SciTLDR</td>
<td>allenai/SciTLDR</td>
</tr>
<tr>
<td>Summarization</td>
<td>CATTS (Cachola et al., 2020)</td>
<td>(Cachola et al., 2020)</td>
<td></td>
</tr>
<tr>
<td>Citation Intent</td>
<td>Cohan et al. (2019)</td>
<td>SciCite</td>
<td>allenai/SciCite</td>
</tr>
<tr>
<td>Classification</td>
<td></td>
<td>(Cohan et al., 2019)</td>
<td></td>
</tr>
<tr>
<td>Field of Study</td>
<td>S2FOS</td>
<td>S2FOS</td>
<td>allenai/s2_fos</td>
</tr>
<tr>
<td>Classification</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Influential Citation</td>
<td>Valenzuela et al. (2015)</td>
<td>meaningful-citations</td>
<td>-</td>
</tr>
<tr>
<td>Classification</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Paper</td>
<td>SPECTER (Cohan et al., 2020)</td>
<td>SciDocs (Cohan et al., 2020) &amp;</td>
<td>allenai/SPECTER</td>
</tr>
<tr>
<td>Embedding</td>
<td>SPECTER2 (Singh et al., 2023)</td>
<td>SciRepEval (Singh et al., 2023)</td>
<td>allenai/SciRepEval</td>
</tr>
</tbody>
</table>
<p>Table 3: Selected models and datasets used by the pipeline.
paper with a slightly modified, or almost completely different, title. Our latest paper deduplication model is named S2APLER. It works by grouping papers into blocks using title, such that papers with similar but nonidentical titles end up in the same block, then scoring the pairwise similarity of papers in each block with a trained model. The model uses string-similarity features for title, abstract, author names, venue name, etc. The synthetic training dataset uses paper data from authoritative sources that provide either a PDF or a $\mathrm{DOL}^{13}$ Paper pairs with matching DOI/PDFs from different sources are used as positive training examples. Pairs with similar titles but non-matching DOI/PDFs are used as negative examples. We make S2APLER available at https://github.com/allenai/S2APLER.</p>
<p>Citation Linking is the system for finding references to one paper in another paper's bibliography. We also use the Text Span Annotation output to associate each citation link with the text of the sentence containing the citation. Citation Linking is a very similar problem to paper deduplication, except that instead of scoring the similarity between two papers, we score the similarity between a paper and a bibliography entry produced by the PDF Extraction system. We are using fuzzy text-matching heuristics on title and authors for citation linking, but anticipate that the S2APLER model can eventually be adapted to this problem.</p>
<p>For Publication Venue Normalization, we combine</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>data from Fatcat ${ }^{14}$ and MAG to build a comprehensive set of normalized venues. To match normalized venues, we index all known variant titles for the venue, including ISO-4 normalization, ${ }^{15}$ in a direct lookup table. We apply regular-expression-based rules to the unnormalized venue strings we obtain from extracted or input sources and look for exact matches in the knowledge base.</p>
<p>In Author Disambiguation, we use a system named S2AND, introduced in Subramanian et al. (2021), to assign an ID to each author mention (a name of an author appearing in a particular paper). S2AND operates in three stages: (1) Grouping author mentions into candidate blocks, (2) Scoring similarity between records within a block using a LightGBM model (Ke et al., 2017), and (3) Clustering mentions within a block. The similarity scoring is a trained on a large dataset for author disambiguation also introduced in Subramanian et al. (2021). The code, model and dataset for S2AND is available at https://github.com/allenai/S2AND.</p>
<p>For Author Affiliation Normalization, we link to ROR, ${ }^{16}$ a registry of persistent identifiers for research organizations. Our linking model is named S2AFF. It first parses unnormalized affiliation strings with a trained NER model into main institute, child institute, and address components. It then fetches the top 100 candidates from a Jaccard-overlap retrieval index and</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>ranks them using a pairwise LightGBM model (Ke et al., 2017) trained using internally-gathered human annotations. We make the code and data for S2AFF available at https://github.com/allenai/S2AFF, but the links are not currently included in our datasets.</p>
<h3>3.4 Semantic Features</h3>
<p>We now turn to describing the models that provide semantic features on top of the knowledge graph.</p>
<h3>3.4.1 TLDR Summarization</h3>
<p>To facilitate faster understanding and decision making when scanning lists of papers, we distribute short summaries of scientific papers, or TLDRs, as introduced in Cachola et al. (2020).</p>
<p>Generating TLDRs of scientific papers can be a challenging task that involves high source compression and requires domain-specific expertise. We use a BART (Lewis et al., 2019) model trained with CATTS (Cachola et al., 2020), training on paper titles as a scaffolding task to overcome the problem of limited annotated training data. We trained this model on a combined dataset consisting of examples from SciTLDR (Cachola et al., 2020) and a separately collected set of summaries for biomedical papers from the Semantic Scholar corpus. The code, model, and data are available at https://github.com/allenai/SciTLDR.</p>
<h3>3.4.2 Citation Intent and Influence Classification</h3>
<p>Citations play a critical role in scientific papers, and understanding the intent of a citation is helpful for automated analysis of scholarly literature. We use the model and dataset, called SciCite, introduced in Cohan et al. (2019) to classify each citation into one of three categories: background information, use of methods, or comparing results. The code, model, and data for SciCite are available at https://github.com/allenai/SciCite.</p>
<p>We also classify whether each citation is Highly Influential. Based on the dataset and findings from Valenzuela et al. (2015), ${ }^{17}$ we use a feature-based heuristic: (1) Only citations between papers with no overlapping authors are considered eligible, (2) A citation is classified as Highly Influential if it appears at least three times in a sentence in which no other papers are cited, if the citing sentence contains terms such as "build upon" "following" or "inspired by", or if the citing sentence has references to tables or figures (indicating a direct comparison with the cited work).</p>
<h3>3.4.3 Fields-of-Study Classification</h3>
<p>Prior to the discontinuation of MAG, Semantic Scholar made use of the fields-of-study classifications that MAG provided, using their level 0 taxonomy. After MAG's deprecation, we deployed our own classification model, adding Education, Law, and Linguistics to the existing MAG list. These additions were based on user feedback and comparison to other popular academic data sources such as Dimensions.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We trained our own fields-of-study classifier, named S2FOS, ${ }^{18}$ using a multilabel linear SVM using character n-gram TF-IDF representations (the 300k most common character unigrams to 5 -grams). For training data, we manually labeled a number of publication venues, and then propagated those labels to all papers published in their respective venues. The code, model, and data are available at https://github.com/ allenai/s2_fos</p>
<h3>3.4.4 Paper Embeddings</h3>
<p>Vector representations (embeddings) of papers can be useful in a variety of downstream applications. Our pipeline uses them for author disambiguation and recommendations, and we publish embeddings so that others may use them in their own applications. We produce embeddings using SPECTER (Cohan et al., 2020), which generates document-level embeddings from SciBERT (Beltagy et al., 2019). SPECTER takes the paper title and abstract as input, and is trained to minimize a triplet margin loss that encourages paper pairs with a citation relationship to have more similar embeddings than those without. Its successor, SPECTER2 (Singh et al., 2023), provides improved performance by training on 10x more data and introducing a new method that learns adapters (Houlsby et al., 2019) tailored to specific task formats. We evaluated on the SciDocs benchmark, also introduced in Cohan et al. (2020), as well as a newer benchmark SciRepEval (Singh et al., 2023), which increases the number and difficulty of tasks. The code, models, and data are publicly available for SPECTER https://github.com/allenai/SPECTER, SPECTER2 https://github.com/allenai/SPECTER2, and SciRepEval https://github.com/allenai/SciRepEval.</p>
<h3>3.4.5 Recent Paper Recommendations</h3>
<p>We dynamically train recommendation models to surface relevant new papers to users. Our recommender takes a set of positively- and negatively-annotated papers, and outputs a ranked list of recommended papers. The model generates recommendations in three steps: Ranker Training, Candidate Selection, and Candidate Ranking.</p>
<p>Ranker Training is an extension of Arxiv Sanity. ${ }^{19,20}$ From each user's positive/negative annotations, it trains two linear Support Vector Machine models: one from the TF-IDF representations of the annotated papers and one from the SPECTER embeddings. We augment negative user annotations with randomly selected negative examples, the latter having less weight. During Candidate Selection, we use FAISS $^{21}$ to search an approximate k-nearest neighbor index of the SPECTER embeddings of $\sim 1 \mathrm{M}$ papers published in the last 60</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Field of Study</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>Medicine</td>
<td>2.9M</td>
</tr>
<tr>
<td>Biology</td>
<td>2.2M</td>
</tr>
<tr>
<td>Physics</td>
<td>1.2M</td>
</tr>
<tr>
<td>Computer Science</td>
<td>810k</td>
</tr>
<tr>
<td>Mathematics</td>
<td>580k</td>
</tr>
<tr>
<td>Psychology</td>
<td>540k</td>
</tr>
<tr>
<td>Chemistry</td>
<td>430k</td>
</tr>
<tr>
<td>Materials Science</td>
<td>400k</td>
</tr>
<tr>
<td>Environmental Science</td>
<td>400k</td>
</tr>
<tr>
<td>Engineering</td>
<td>390k</td>
</tr>
<tr>
<td>Agricultural And Food Sciences</td>
<td>380k</td>
</tr>
<tr>
<td>Education</td>
<td>260k</td>
</tr>
<tr>
<td>Business</td>
<td>230k</td>
</tr>
<tr>
<td>Economics</td>
<td>210k</td>
</tr>
<tr>
<td>Political Science</td>
<td>150k</td>
</tr>
<tr>
<td>Geology</td>
<td>110k</td>
</tr>
<tr>
<td>Art</td>
<td>50k</td>
</tr>
<tr>
<td>Sociology</td>
<td>50k</td>
</tr>
<tr>
<td>History</td>
<td>40k</td>
</tr>
<tr>
<td>Linguistics</td>
<td>30k</td>
</tr>
<tr>
<td>Philosophy</td>
<td>30k</td>
</tr>
<tr>
<td>Law</td>
<td>20k</td>
</tr>
<tr>
<td>Geography</td>
<td>20k</td>
</tr>
</tbody>
</table>
<p>Table 4: Full-text availability of papers in S2ORC for different academic fields
days (refreshed nightly). Finally, for Candidate Ranking we select $\sim 500$ papers nearest the centroid of the positively-annotated papers and rank them using the average of the two model scores.</p>
<h2>4 APIs and Datasets</h2>
<p>The outputs of our data processing pipeline and semantic models are made available through a suite of APIs and datasets described below. Because we develop and refine our models over time, the data served by the APIs may shift, or it may come from a mixture of models as we migrate from one system to its successor. Where appropriate, we will update our live documentation ${ }^{22}$ to reflect any changes.</p>
<p>For unauthenticated users, we offer a low volume of API requests and samples of the datasets. For full datasets and high request volumes, we ask that users obtain an authentication key, at no charge, subject to terms of use. ${ }^{23}$ To date, we have issued over 3000 authentication keys to various partners, for uses varying from student research projects to non-profit organizations to commercial products. The API served over 1.8 billion requests in 2024, with over 100 clients making more than 1 k requests/day and the heaviest users making over 100k/day. The most popular types of request are paper metadata lookup by ID (60\%) and keyword search ( $16 \%$ ).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.1 Graph and Search</h3>
<p>The Graph API ${ }^{24}$ provides the most current data from the Semantic Scholar Academic Graph, combining IDbased lookup with a variety of search endpoints Papers can be retrieved by our internal ID, or by using identifiers from arXiv, PubMed, DOI, and others. Papers can also be retrieved via their bidirectional citation relationship to other papers, or by author. For users needing to download large volumes of data, we recommend using the bulk dataset downloads described below.</p>
<p>The "relevance search" endpoint is for traditional search engine queries, using keyword matching and metadata-based filters. It uses an elasticsearch index of paper titles, abstracts, and author names. Results are limited to 1000 matches, which are reranked using a trained LightGBM model. The reranking model emphasizes direct title matches and highly-cited papers with recent publication dates.</p>
<p>The "title search" endpoint is for locating a paper by title. If no matching paper is found, then an empty result is returned.</p>
<p>The "bulk search" endpoint is for retrieving large numbers of papers from broad queries, where ordering is not critical. Queries can be expressed using boolean logic, which is applied as a filter in addition to metadata-based filters. Matching papers can be sorted by citation count or publication date. The endpoint will paginate results, and can return up to 10 M matching papers in total.</p>
<p>The "snippet search" endpoint is for retrieving matching text passages from the S2ORC dataset (see below). Titles, abstracts, and body text are loaded into a Vespa cluster, along with paper metadata for filtering. The index includes over 275M passages from over 12M papers across various fields of study. Each passage is limited to 480 tokens and truncated at sentence and section boundaries where possible, having an overlap of one sentence (up to 64 tokens) with the preceding and following passages. Passage text is embedded using mxbai-embed-large-v1 (Lee et al., 2024) with binary quantization, and placed into both a dense (approximate nearest neighbor) and sparse (keyword) index. Matching results are retrieved using the union of embedding and keyword-based matches, which are then ranked with a weighted sum of query-snippet embedding similarity and bm25 scores.</p>
<h3>4.2 Datasets</h3>
<p>Download links to monthly snapshots of our knowledge graph can be obtained via our Datasets API. ${ }^{25}$ We also publish incremental diffs between sequential releases. Each dataset is a collection of gzipped JSON files, where records in one dataset refer to records in other datasets by ID. The datasets are:</p>
<ul>
<li>papers: Core metadata of papers</li>
</ul>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<ul>
<li>abstracts: Abstract text for papers, where allowed by licensing</li>
<li>authors: Core metadata of authors</li>
<li>citations: Citation links between papers, with citation context and intent and influential classifications</li>
<li>embeddings: SPECTER embeddings of papers</li>
<li>paper-ids: Mapping between different IDs used to identify a paper. Useful for tracking deduplication between releases.</li>
<li>tldrs: TLDRs for papers</li>
<li>publication-venues: Core metadata for publication venues</li>
<li>S2ORC: Introduced in Lo et al. (2020), S2ORC is the largest publicly-available collection of full text for open-access scientific papers. S2ORC's full text is annotated with automatically-identified structural and semantic elements of the paper: section headings, paragraphs, bibliography entries, inline citation mentions, table/figure references, etc. Table 4 shows the number of openaccess full-text papers broken down by academic field. Since its original release as a static collection, S2ORC has grown in size and is being kept up-to-date as part of our PDF processing pipeline. For further details on S2ORC, we refer the reader to Lo et al. (2020) and documentation at https://github.com/allenai/s2orc.</li>
</ul>
<h3>4.3 Recommendations</h3>
<p>The Recommendations API ${ }^{26}$ generates recommendations, selected from papers published within the past 60 days, based on positive/negative paper annotations. The caller provides at least one paper ID as a positive example, and any number of paper IDs as negative examples. The response is a relevance-ordered list of recently-published papers and their metadata.</p>
<h2>5 Related Work</h2>
<p>Table 5 summarizes major providers of scholarly data along three key dimensions: comprehensiveness, access, and services offered. Some providers, such as Google Scholar, do not offer any programmatic services at all. Others, such as MAG, have been discontinued. The major open provider of parsed content, PubMed Central, is not cross-disciplinary. Other providers require a subscription. Semantic Scholar is unique in providing a comprehensive and open knowledge base with the widest array of services.</p>
<h2>6 Conclusion and Future Work</h2>
<p>We have described the Semantic Scholar data platform, which offers code bases, data sets, and APIs covering</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>scientific literature. The Semantic Scholar Academic Graph (S2AG) consists of hundreds of millions of papers and billions of citation links, created by a state-of-the-art PDF extraction and knowledge graph normalization pipeline described in this paper. The platform also offers semantic features such as summarization, vector embeddings and recommendations.</p>
<p>In the future, we hope to expose selected semantic features as services, for users to apply to their own data. We hope to add richer semantic labels to our full-text annotations. We plan to add more personalized functionality, such as access to library content and reading history. We will expand our tools for collecting human data corrections, and possibly collect automated annotations from external collaborators. Of course, we will continue to improve our existing knowledge graph construction and semantic feature models. We hope that providing these resources will enable application development and research using scholarly data to promote the advancement of science globally.</p>
<h2>Acknowledgements</h2>
<p>The Semantic Scholar Open Data Platform, including S2AG and various dataset and API offerings, is the product of years of work by members of the Semantic Scholar team. The authors of this paper have all contributed directly to the creation and continued maintenance of this platform, including software and model development, data curation, evaluation, design, product management, and more.</p>
<p>This work was supported in part by NSF Grant CNS2213656.</p>
<h2>References</h2>
<p>Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu A. Ha, Rodney Michael Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler C. Murray, HsuHan Ooi, Matthew E. Peters, Joanna L. Power, Sam Skjonsberg, Lucy Lu Wang, Christopher Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni. 2018. Construction of the Literature Graph in Semantic Scholar. In NAACL. https://doi.org/10. 18653/v1/N18-3011</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 3615-3620. https://doi.org/10.18653/v1/ D19-1371</p>
<p>Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel S. Weld. 2020. TLDR: Extreme Summarization of Scientific Documents. In FINDINGS. https://doi.org/ 10.18653/v1/2020.findings-emnlp. 428</p>
<table>
<thead>
<tr>
<th>Resource</th>
<th>URL</th>
<th>Article Count</th>
<th>Access</th>
<th>Services</th>
</tr>
</thead>
<tbody>
<tr>
<td>Aminer</td>
<td>aminer.org</td>
<td>321.5 M</td>
<td>open</td>
<td>$\mathrm{D}^{*}$</td>
</tr>
<tr>
<td>arXiv</td>
<td>arxiv.org</td>
<td>2 M</td>
<td>open</td>
<td>$\mathrm{D}^{<em> </em>}, \mathrm{~F}, \mathrm{~S}$</td>
</tr>
<tr>
<td>BASE</td>
<td>base-search.net</td>
<td>180.5 M</td>
<td>open</td>
<td>S</td>
</tr>
<tr>
<td>CORE</td>
<td>core.ac.uk</td>
<td>207.3 M</td>
<td>open</td>
<td>$\mathrm{D}^{*}, \mathrm{~S}$</td>
</tr>
<tr>
<td>Dimensions</td>
<td>app.dimensions.ai</td>
<td>123.8 M</td>
<td>subscription</td>
<td>D, F, M, S</td>
</tr>
<tr>
<td>Google Scholar</td>
<td>scholar.google.com</td>
<td>?</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>The Lens</td>
<td>lens.org</td>
<td>240.4 M</td>
<td>subscription</td>
<td>D, M, S</td>
</tr>
<tr>
<td>Meta</td>
<td>-</td>
<td>-</td>
<td>terminated 3/31/22</td>
<td>-</td>
</tr>
<tr>
<td>Microsoft Academic</td>
<td>-</td>
<td>-</td>
<td>terminated 12/31/21</td>
<td>-</td>
</tr>
<tr>
<td>OpenAlex</td>
<td>openalex.org</td>
<td>205.2 M</td>
<td>open</td>
<td>D, F, M</td>
</tr>
<tr>
<td>PubMed Central</td>
<td>ncbi.nlm.nih.gov/pmc/</td>
<td>7.5 M</td>
<td>open</td>
<td>$\mathrm{D}^{<em> </em>}, \mathrm{~F}, \mathrm{P}, \mathrm{S}$</td>
</tr>
<tr>
<td>ResearchGate</td>
<td>researchgate.net</td>
<td>135.0 M</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Scopus</td>
<td>scopus.com</td>
<td>84.0 M</td>
<td>subscription</td>
<td>F, M, S</td>
</tr>
<tr>
<td>Semantic Scholar</td>
<td>semanticscholar.org</td>
<td>225 M</td>
<td>open</td>
<td>D, F, M, P, S, T</td>
</tr>
<tr>
<td>Web of Science Core</td>
<td>webofknowledge.com</td>
<td>83.2 M</td>
<td>subscription</td>
<td>F, M, S</td>
</tr>
</tbody>
</table>
<p>Key: D=data download; F=field-of-study classification; M=advanced metadata;
P=semantically parsed text; S=title and abstract search; T=natural language summarization
<em>=data more than a year stale; </em>*=restricted fields of study
Article count does not include patents or datasets.
Table 5: Comparison of leading scholarly data providers</p>
<p>Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural Scaffolds for Citation Intent Classification in Scientific Publications. NAACL (2019).</p>
<p>Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. 2020. SPECTER: Document-level Representation Learning using Citation-informed Transformers. arXiv 2004.07180 (2020). https://doi.org/10.18653/v1/2020.acl-main. 207
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. In Proceedings of the 36th International Conference on Machine Learning.</p>
<p>Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In NIPS.</p>
<p>Sean Lee, Aamir Shakir, Darius Koenig, and Julius Lipp. 2024. Open Source Strikes Bread - New Fluffy Embeddings Model. https://www.mixedbread.ai/ blog/mxbai-embed-large-v1
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Michael Kinney, and Daniel S. Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In ACL. https://doi.org/10.18653/V1/2020. ACL-MAIN. 447
Zejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl, Daniel S Weld, and Doug Downey. 2022. VILA: Improving structured content extraction from scientific PDFs using visual layout groups. Transactions of the Association for Computational Linguistics 10 (2022), 376-392.</p>
<p>Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, and Weining Li. 2021. LayoutParser: A unified toolkit for deep learning based document image analysis. In International Conference on Document Analysis and Recognition. Springer, 131-146.</p>
<p>Amanpreet Singh, Mike D'Arcy, Arman Cohan, Doug Downey, and Sergey Feldman. 2023. SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 5548-5566. https://doi.org/10. 18653/v1/2023.emnlp-main. 338</p>
<p>Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Paul Hsu, and Kuansan Wang. 2015. An Overview of Microsoft Academic Service (MAS) and Applications. Proceedings of the 24th International Conference on World Wide Web (2015). https://doi.org/10.1145/2740908.2742839</p>
<p>Shivashankar Subramanian, Daniel King, Doug Downey, and Sergey Feldman. 2021. S2AND: A Benchmark and Evaluation System for Author Name Disambiguation. ArXiv abs/2103.07534 (2021).</p>
<p>Mingxing Tan, Ruoming Pang, and Quoc V Le. 2020. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10781-10790.</p>
<p>Marco Valenzuela, Vu A. Ha, and Oren Etzioni. 2015. Identifying Meaningful Citations. In AAAI Workshop: Scholarly Big Data.</p>
<p>Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019. PubLayNet: largest dataset ever for document layout analysis. In 2019 International Conference on Document Analysis and Recognition (ICDAR). IEEE, 1015-1022. https://doi.org/10.1109/ ICDAR.2019.00166</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{26}$ https://api.semanticscholar.org/api-docs/ recommendations&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{24}$ https://api.semanticscholar.org/api-docs/graph
${ }^{25}$ https://api.semanticscholar.org/api-docs/datasets&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
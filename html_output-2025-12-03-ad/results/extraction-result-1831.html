<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1831 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1831</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1831</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-28cbd1f2a472ba9a0330c97e7e6820b29883a69e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/28cbd1f2a472ba9a0330c97e7e6820b29883a69e" target="_blank">True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments.</p>
                <p><strong>Paper Abstract:</strong> Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1831.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1831.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B (TWOSOME)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-7B (used as the TWOSOME agent backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only large language model (LLaMA-7B) pretrained on large text corpora and deployed as the frozen backbone in the TWOSOME framework; LoRA adapters are trained with PPO to align the language model to embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLaMA: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LLaMA-7B (as TWOSOME agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Decoder-only LLM (LLaMA-7B) kept frozen for generation logits; actor implemented by the frozen LLaMA-7B augmented with low-rank adapters (LoRA) and updated by PPO; critic implemented as small MLPs attached to the last transformer block (trained). At inference the LoRA module (actor) alone is used as the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Web-scale text corpora / language data (LLM text pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; the LLaMA-7B model is used as provided (cited to Touvron et al., 2023). This paper only states LLMs are pretrained on text data (unsupervised next-token modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Overcooked and VirtualHome</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Overcooked: partially-observable 7x7 grid kitchen with macro-actions (e.g., Chop, Get-Tomato, Go-Cutting-Board, Deliver); objectives are to prepare and deliver recipes (tomato salad, tomato-lettuce salad) with rewards for correct chopping and delivery and sparse penalties. VirtualHome: partially-observable simulated household with many objects and high-level macro-actions (Walk, Grab, PutBack, SwitchOn, Sit, etc.); tasks include Food Preparation (heat pancake) and Entertainment (bring chips & milk, turn on TV, sit), with sparse task-completion reward.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Discrete language token sequences / semantic action prompts (human-readable action prompts such as 'pick up the tomato' or 'walk to the kitchen') used as the model's candidate outputs during scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>High-level discrete macro-actions mapped to environment routines (e.g., Chop, Get-Tomato, Walk-Kitchen, Grab-Milk, Putback-Milk-Coffeetable). Macro-actions may execute several underlying primitive steps; action space is symbolic and discrete.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Each valid macro-action is associated with a unique action prompt (token sequence). At each step the observation prompt is concatenated with each action prompt; the LLaMA-7B provides token log-likelihoods which are multiplied (joint token probability) to yield P_token(a_k | s). A softmax over these joint log-probabilities yields a policy over valid actions. To correct bias versus longer prompts the paper introduces token normalization (divide log-prob by number of tokens) and word normalization (divide by number of words). During finetuning, LoRA parameters (actor) and the critic MLP are trained with PPO using environment rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Textual / symbolic observations only in the experiments: raw symbolic environment state is converted to observation prompts (no raw RGB used in the paper). The paper notes conversion from images/vectors to text could be done by VLMs but in these experiments symbolic observations are used (partial observability).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>TWOSOME (LLaMA-7B + LoRA + PPO) with word normalization achieved optimal or near-optimal performance across tasks: final success rates reported as 1.0 (100%) ± 0.0 on Overcooked Tomato Salad, Tomato-Lettuce Salad, and VirtualHome Food Preparation and Entertainment (Table 2); in unseen-task generalization TWOSOME attained 100% on many substituted-food tasks and 97% on Entertainment when tested cross-task (Table 3/4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines: PPO (tabular/MLP actor-critic) and non-finetuned LLM scoring (TWOSOME w/o finetuning / SayCan-like) performed worse. Reported examples: TWOSOME without finetuning success rates: Tomato Salad 0.36 ± 0.04, Tomato-Lettuce Salad 0.00 ± 0.00, Food Preparation 0.81 ± 0.01, Entertainment 0.45 ± 0.05 (Table 2). PPO: e.g., Entertainment 0.33 ± 0.47; PPO struggled in VirtualHome without action masking and in some Overcooked tasks got stuck in suboptimal policies.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Paper reports TWOSOME with word normalization learned optimal policies in the two Overcooked tasks using ~10k and ~80k sampled data respectively (explicitly stated). Training hyperparameters show total optimization budgets up to 5e5 steps in some runs but the empirical convergence points highlighted are 10k and 80k for the two Overcooked tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>PPO required much larger amounts of data: the paper states PPO needed >500k steps to converge in the harder Overcooked task and vanilla PPO failed in VirtualHome unless action masking was applied; non-finetuned LLMs were only evaluated zero-shot / without environment-driven finetuning (100-episode evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Substantial reduction reported: e.g., TWOSOME reached optimal policy in the harder Overcooked task in ~80k samples versus PPO taking >500k steps (≈6x fewer samples); for the easier Overcooked task TWOSOME reached optimum in ≈10k samples (PPO was much slower or got stuck). Exact multiplicative gains vary by task but the paper emphasizes markedly better sample efficiency for TWOSOME.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key contributing factors reported: (1) strong language priors from LLM pretraining that bias exploration toward semantically sensible actions (open-vocabulary/semantic action prompts); (2) scoring of whole action prompts by token joint probability (ensures action validity); (3) word-normalization to correct length bias in joint token probabilities; (4) parameter-efficient finetuning (LoRA + PPO) that preserves base LLM capability while aligning to environment; (5) prompt-design principles (cohesive prompts, use of articles, repeating preferred nouns in observation prompts) that steer probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Reported limitations and failure modes: (1) length bias of joint token probability (longer prompts get smaller raw joint probabilities) which, if uncorrected, causes instability — addressed via normalization; (2) computational overhead: must score all valid actions every step which increases compute and reduces batch size; (3) LLMs' prior common-sense may conflict with specific environment dynamics (misalignment) until RL finetuning; (4) smaller LLMs may lack reasoning ability and need stronger finetuning; (5) perception gap when raw visual inputs require an intermediate VLM to convert to text (not directly evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A language-pretrained LLM (LLaMA-7B) can be repurposed as an embodied agent by scoring language-encoded macro-actions and then aligning those scores to true environment returns via PPO on small LoRA modules: this yields large gains in sample efficiency and generalization (open-vocabulary transfer to unseen tasks) compared to vanilla RL and prompt-only baselines. Crucial engineering choices are action-prompt scoring, word-level normalization to correct token-length bias, parameter-efficient LoRA finetuning under PPO, and careful prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1831.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1831.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that combines LLM action probabilities with an affordance/value function to select actions (LLM suggests possible actions; a learned value/affordance model scores feasibility); used as a related baseline (TWOSOME without finetuning is described as similar to SayCan).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as I can, not as I say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>SayCan (conceptual baseline; non-finetuned LLM scoring with affordance)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Uses pretrained LLM to assign probabilities to candidate language actions and multiplies those probabilities with an affordance/value estimator to select feasible actions; typically does not finetune the LLM on environment interactions (as described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained on language/text corpora (not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper (cited external work).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used as baseline for Overcooked / VirtualHome style tasks in comparisons in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>In this paper SayCan-like behavior is represented by TWOSOME without finetuning (non-finetuned LLM scoring action prompts and using affordance values equal to 1 in experiments). Evaluations include success rates on the same Overcooked/VirtualHome tasks and eight unseen tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language action prompts (semantic descriptions of macro-actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>High-level macro-actions in Overcooked and VirtualHome, same as TWOSOME's action set.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LLM token probabilities over action prompts are combined (in SayCan original formulation) with an affordance/value function to rank/actions; in this paper the TWOSOME w/o finetuning variant approximates this behavior (affordance value set to 1).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Symbolic/textual observations used in this paper's baselines; original SayCan used real robot perception pipelines (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When evaluated (as 'SayCan' / TWOSOME without finetuning) in eight unseen tasks, reported success rates (Table 3) were: Cheese 0.66, Hamburger 0.79, Applepie 0.85, Pizza 0.64, Washing Plate 0.65, Laundry 0.35, Food Preparation 0.66, Entertainment 0.04. Total returns shown in Table 4 show substantially lower returns than finetuned TWOSOME for most tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported in this paper for SayCan; non-finetuned runs were evaluated zero-shot over 100 episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Leverages LLM semantic priors to propose plausible actions and requires an affordance/feasibility model to ground proposals; benefits from open-vocabulary action naming.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Without environment-driven finetuning the LLM can fail on tasks with specific dynamics or long-horizon dependencies (e.g., extremely low success on Entertainment in these evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>As a prompt-only / non-finetuned approach, LLM scoring (SayCan-like) provides moderate zero-shot capability on some tasks but is substantially outperformed by TWOSOME finetuning (LoRA+PPO), especially on complex or cross-task generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1831.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1831.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLAM (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLAM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concurrent work that uses RL finetuning to ground LLMs in interactive environments; evaluated on toy environments with primitive actions and a smaller encoder-decoder LLM (Flan-T5-780M).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grounding large language models in interactive environments with online reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GLAM (Flan-T5-780M in BabyAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Applies RL finetuning to a pre-trained LLM (Flan-T5-780M) to ground language in actions, evaluated in toy environments with primitive actions of similar token length.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained language model (Flan-T5 family) trained on instruction/data mixtures (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; referenced only at high level.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>BabyAI (toy grid-world) in GLAM's evaluation as described in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Toy environments with primitive actions (turn left, turn right, go forward, etc.) where actions have similar numbers of tokens and limited semantics compared to macro-actions used in TWOSOME's settings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Short token sequences corresponding to primitive actions (similar token length across actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Primitive discrete navigation actions in BabyAI.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>GLAM uses RL finetuning to ground LLM outputs to environment actions (specific scoring/mapping details are in the GLAM paper; this paper notes that GLAM focuses on primitive actions and equal-length prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Toy symbolic observations in BabyAI (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>GLAM demonstrates RL grounding is possible but focused on primitive actions with similar token lengths (which avoids the token-length bias problem highlighted by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Because GLAM evaluated primitive actions with similar token lengths and simpler semantics, it may underutilize LLM semantic abilities and does not expose prompt-design or action-length imbalance issues.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as concurrent work that grounds LLMs with RL but in simpler toy settings; contrasts with TWOSOME which tackles semantic macro-actions and addresses action-prompt length bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1831.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1831.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied agent approach that leverages a large LLM (GPT-4 in the cited work) to discover and learn skills continually during interaction with environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Voyager (GPT-4 based in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Uses a powerful LLM (GPT-4 in the cited work) to plan, discover, and bootstrap skills during open-ended embodied interaction (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained LLM (GPT-4) on large-scale language data (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Referenced as applied to open-ended embodied learning (not directly used in experiments in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Open-ended embodied discovery and skill accumulation in simulated environments (details in the Voyager paper; not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>LLM-generated plans/commands represented in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>High-level actions/skills discovered and executed in simulation (details not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Cited work uses LLM planning and then executes/learns skills; mapping details are in the Voyager paper and not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Demonstrates the utility of very large LLM priors (GPT-4) to bootstrap skill learning in embodied agents (as referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as related work showing LLMs can be used to discover skills during embodied learning; detailed claims are in the Voyager paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1831.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1831.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal embodied language model applying large pretrained language models to robot perception and control (referenced in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaLM-E: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>PaLM-E integrates language and perception to support embodied tasks; cited here as an example of deploying large language models in robot learning and real-world interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal pretraining (language + vision / embodied data) in the original PaLM-E work; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic real-world interaction tasks (referenced; not used in this paper's experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Robotic manipulation/navigation tasks leveraging multimodal inputs; specifics in the PaLM-E paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions and plan-like outputs in the original PaLM-E work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Robot control commands / high-level instructions (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>PaLM-E uses multimodal conditioning and mapping to robot actions as described in its own paper; not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multimodal (vision + language) in the PaLM-E work; not used directly in the experiments of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Shows integration of perception with LLM priors can enable embodied robot tasks (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example of applying large pre-trained language models to embodied robotics and multimodal control; details in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as I can, not as I say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Grounding large language models in interactive environments with online reinforcement learning <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>LLaMA: Open and efficient foundation language models <em>(Rating: 2)</em></li>
                <li>Language models meet world models: Embodied experiences enhance language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1831",
    "paper_id": "paper-28cbd1f2a472ba9a0330c97e7e6820b29883a69e",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "LLaMA-7B (TWOSOME)",
            "name_full": "LLaMA-7B (used as the TWOSOME agent backbone)",
            "brief_description": "A decoder-only large language model (LLaMA-7B) pretrained on large text corpora and deployed as the frozen backbone in the TWOSOME framework; LoRA adapters are trained with PPO to align the language model to embodied environments.",
            "citation_title": "LLaMA: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_agent_name": "LLaMA-7B (as TWOSOME agent)",
            "model_agent_description": "Decoder-only LLM (LLaMA-7B) kept frozen for generation logits; actor implemented by the frozen LLaMA-7B augmented with low-rank adapters (LoRA) and updated by PPO; critic implemented as small MLPs attached to the last transformer block (trained). At inference the LoRA module (actor) alone is used as the policy.",
            "pretraining_data_type": "Web-scale text corpora / language data (LLM text pretraining)",
            "pretraining_data_details": "Not specified in this paper; the LLaMA-7B model is used as provided (cited to Touvron et al., 2023). This paper only states LLMs are pretrained on text data (unsupervised next-token modeling).",
            "embodied_task_name": "Overcooked and VirtualHome",
            "embodied_task_description": "Overcooked: partially-observable 7x7 grid kitchen with macro-actions (e.g., Chop, Get-Tomato, Go-Cutting-Board, Deliver); objectives are to prepare and deliver recipes (tomato salad, tomato-lettuce salad) with rewards for correct chopping and delivery and sparse penalties. VirtualHome: partially-observable simulated household with many objects and high-level macro-actions (Walk, Grab, PutBack, SwitchOn, Sit, etc.); tasks include Food Preparation (heat pancake) and Entertainment (bring chips & milk, turn on TV, sit), with sparse task-completion reward.",
            "action_space_text": "Discrete language token sequences / semantic action prompts (human-readable action prompts such as 'pick up the tomato' or 'walk to the kitchen') used as the model's candidate outputs during scoring.",
            "action_space_embodied": "High-level discrete macro-actions mapped to environment routines (e.g., Chop, Get-Tomato, Walk-Kitchen, Grab-Milk, Putback-Milk-Coffeetable). Macro-actions may execute several underlying primitive steps; action space is symbolic and discrete.",
            "action_mapping_method": "Each valid macro-action is associated with a unique action prompt (token sequence). At each step the observation prompt is concatenated with each action prompt; the LLaMA-7B provides token log-likelihoods which are multiplied (joint token probability) to yield P_token(a_k | s). A softmax over these joint log-probabilities yields a policy over valid actions. To correct bias versus longer prompts the paper introduces token normalization (divide log-prob by number of tokens) and word normalization (divide by number of words). During finetuning, LoRA parameters (actor) and the critic MLP are trained with PPO using environment rewards.",
            "perception_requirements": "Textual / symbolic observations only in the experiments: raw symbolic environment state is converted to observation prompts (no raw RGB used in the paper). The paper notes conversion from images/vectors to text could be done by VLMs but in these experiments symbolic observations are used (partial observability).",
            "transfer_successful": true,
            "performance_with_pretraining": "TWOSOME (LLaMA-7B + LoRA + PPO) with word normalization achieved optimal or near-optimal performance across tasks: final success rates reported as 1.0 (100%) ± 0.0 on Overcooked Tomato Salad, Tomato-Lettuce Salad, and VirtualHome Food Preparation and Entertainment (Table 2); in unseen-task generalization TWOSOME attained 100% on many substituted-food tasks and 97% on Entertainment when tested cross-task (Table 3/4).",
            "performance_without_pretraining": "Baselines: PPO (tabular/MLP actor-critic) and non-finetuned LLM scoring (TWOSOME w/o finetuning / SayCan-like) performed worse. Reported examples: TWOSOME without finetuning success rates: Tomato Salad 0.36 ± 0.04, Tomato-Lettuce Salad 0.00 ± 0.00, Food Preparation 0.81 ± 0.01, Entertainment 0.45 ± 0.05 (Table 2). PPO: e.g., Entertainment 0.33 ± 0.47; PPO struggled in VirtualHome without action masking and in some Overcooked tasks got stuck in suboptimal policies.",
            "sample_complexity_with_pretraining": "Paper reports TWOSOME with word normalization learned optimal policies in the two Overcooked tasks using ~10k and ~80k sampled data respectively (explicitly stated). Training hyperparameters show total optimization budgets up to 5e5 steps in some runs but the empirical convergence points highlighted are 10k and 80k for the two Overcooked tasks.",
            "sample_complexity_without_pretraining": "PPO required much larger amounts of data: the paper states PPO needed &gt;500k steps to converge in the harder Overcooked task and vanilla PPO failed in VirtualHome unless action masking was applied; non-finetuned LLMs were only evaluated zero-shot / without environment-driven finetuning (100-episode evaluations).",
            "sample_complexity_gain": "Substantial reduction reported: e.g., TWOSOME reached optimal policy in the harder Overcooked task in ~80k samples versus PPO taking &gt;500k steps (≈6x fewer samples); for the easier Overcooked task TWOSOME reached optimum in ≈10k samples (PPO was much slower or got stuck). Exact multiplicative gains vary by task but the paper emphasizes markedly better sample efficiency for TWOSOME.",
            "transfer_success_factors": "Key contributing factors reported: (1) strong language priors from LLM pretraining that bias exploration toward semantically sensible actions (open-vocabulary/semantic action prompts); (2) scoring of whole action prompts by token joint probability (ensures action validity); (3) word-normalization to correct length bias in joint token probabilities; (4) parameter-efficient finetuning (LoRA + PPO) that preserves base LLM capability while aligning to environment; (5) prompt-design principles (cohesive prompts, use of articles, repeating preferred nouns in observation prompts) that steer probabilities.",
            "transfer_failure_factors": "Reported limitations and failure modes: (1) length bias of joint token probability (longer prompts get smaller raw joint probabilities) which, if uncorrected, causes instability — addressed via normalization; (2) computational overhead: must score all valid actions every step which increases compute and reduces batch size; (3) LLMs' prior common-sense may conflict with specific environment dynamics (misalignment) until RL finetuning; (4) smaller LLMs may lack reasoning ability and need stronger finetuning; (5) perception gap when raw visual inputs require an intermediate VLM to convert to text (not directly evaluated here).",
            "key_findings": "A language-pretrained LLM (LLaMA-7B) can be repurposed as an embodied agent by scoring language-encoded macro-actions and then aligning those scores to true environment returns via PPO on small LoRA modules: this yields large gains in sample efficiency and generalization (open-vocabulary transfer to unseen tasks) compared to vanilla RL and prompt-only baselines. Crucial engineering choices are action-prompt scoring, word-level normalization to correct token-length bias, parameter-efficient LoRA finetuning under PPO, and careful prompt design.",
            "uuid": "e1831.0",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SayCan (referenced)",
            "name_full": "SayCan: Grounding language in robotic affordances",
            "brief_description": "A method that combines LLM action probabilities with an affordance/value function to select actions (LLM suggests possible actions; a learned value/affordance model scores feasibility); used as a related baseline (TWOSOME without finetuning is described as similar to SayCan).",
            "citation_title": "Do as I can, not as I say: Grounding language in robotic affordances",
            "mention_or_use": "mention",
            "model_agent_name": "SayCan (conceptual baseline; non-finetuned LLM scoring with affordance)",
            "model_agent_description": "Uses pretrained LLM to assign probabilities to candidate language actions and multiplies those probabilities with an affordance/value estimator to select feasible actions; typically does not finetune the LLM on environment interactions (as described in this paper).",
            "pretraining_data_type": "Pretrained on language/text corpora (not specified in this paper).",
            "pretraining_data_details": "Not specified in this paper (cited external work).",
            "embodied_task_name": "Used as baseline for Overcooked / VirtualHome style tasks in comparisons in this paper",
            "embodied_task_description": "In this paper SayCan-like behavior is represented by TWOSOME without finetuning (non-finetuned LLM scoring action prompts and using affordance values equal to 1 in experiments). Evaluations include success rates on the same Overcooked/VirtualHome tasks and eight unseen tasks.",
            "action_space_text": "Language action prompts (semantic descriptions of macro-actions).",
            "action_space_embodied": "High-level macro-actions in Overcooked and VirtualHome, same as TWOSOME's action set.",
            "action_mapping_method": "LLM token probabilities over action prompts are combined (in SayCan original formulation) with an affordance/value function to rank/actions; in this paper the TWOSOME w/o finetuning variant approximates this behavior (affordance value set to 1).",
            "perception_requirements": "Symbolic/textual observations used in this paper's baselines; original SayCan used real robot perception pipelines (not detailed here).",
            "transfer_successful": null,
            "performance_with_pretraining": "When evaluated (as 'SayCan' / TWOSOME without finetuning) in eight unseen tasks, reported success rates (Table 3) were: Cheese 0.66, Hamburger 0.79, Applepie 0.85, Pizza 0.64, Washing Plate 0.65, Laundry 0.35, Food Preparation 0.66, Entertainment 0.04. Total returns shown in Table 4 show substantially lower returns than finetuned TWOSOME for most tasks.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Not reported in this paper for SayCan; non-finetuned runs were evaluated zero-shot over 100 episodes.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Leverages LLM semantic priors to propose plausible actions and requires an affordance/feasibility model to ground proposals; benefits from open-vocabulary action naming.",
            "transfer_failure_factors": "Without environment-driven finetuning the LLM can fail on tasks with specific dynamics or long-horizon dependencies (e.g., extremely low success on Entertainment in these evaluations).",
            "key_findings": "As a prompt-only / non-finetuned approach, LLM scoring (SayCan-like) provides moderate zero-shot capability on some tasks but is substantially outperformed by TWOSOME finetuning (LoRA+PPO), especially on complex or cross-task generalization.",
            "uuid": "e1831.1",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GLAM (referenced)",
            "name_full": "GLAM",
            "brief_description": "Concurrent work that uses RL finetuning to ground LLMs in interactive environments; evaluated on toy environments with primitive actions and a smaller encoder-decoder LLM (Flan-T5-780M).",
            "citation_title": "Grounding large language models in interactive environments with online reinforcement learning",
            "mention_or_use": "mention",
            "model_agent_name": "GLAM (Flan-T5-780M in BabyAI)",
            "model_agent_description": "Applies RL finetuning to a pre-trained LLM (Flan-T5-780M) to ground language in actions, evaluated in toy environments with primitive actions of similar token length.",
            "pretraining_data_type": "Pretrained language model (Flan-T5 family) trained on instruction/data mixtures (details not provided in this paper).",
            "pretraining_data_details": "Not specified in this paper; referenced only at high level.",
            "embodied_task_name": "BabyAI (toy grid-world) in GLAM's evaluation as described in this paper",
            "embodied_task_description": "Toy environments with primitive actions (turn left, turn right, go forward, etc.) where actions have similar numbers of tokens and limited semantics compared to macro-actions used in TWOSOME's settings.",
            "action_space_text": "Short token sequences corresponding to primitive actions (similar token length across actions).",
            "action_space_embodied": "Primitive discrete navigation actions in BabyAI.",
            "action_mapping_method": "GLAM uses RL finetuning to ground LLM outputs to environment actions (specific scoring/mapping details are in the GLAM paper; this paper notes that GLAM focuses on primitive actions and equal-length prompts).",
            "perception_requirements": "Toy symbolic observations in BabyAI (not detailed in this paper).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "GLAM demonstrates RL grounding is possible but focused on primitive actions with similar token lengths (which avoids the token-length bias problem highlighted by this paper).",
            "transfer_failure_factors": "Because GLAM evaluated primitive actions with similar token lengths and simpler semantics, it may underutilize LLM semantic abilities and does not expose prompt-design or action-length imbalance issues.",
            "key_findings": "Mentioned as concurrent work that grounds LLMs with RL but in simpler toy settings; contrasts with TWOSOME which tackles semantic macro-actions and addresses action-prompt length bias.",
            "uuid": "e1831.2",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Voyager (referenced)",
            "name_full": "Voyager",
            "brief_description": "An embodied agent approach that leverages a large LLM (GPT-4 in the cited work) to discover and learn skills continually during interaction with environments.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "model_agent_name": "Voyager (GPT-4 based in cited work)",
            "model_agent_description": "Uses a powerful LLM (GPT-4 in the cited work) to plan, discover, and bootstrap skills during open-ended embodied interaction (cited as related work).",
            "pretraining_data_type": "Pretrained LLM (GPT-4) on large-scale language data (not detailed in this paper).",
            "pretraining_data_details": "Not specified in this paper.",
            "embodied_task_name": "Referenced as applied to open-ended embodied learning (not directly used in experiments in this paper).",
            "embodied_task_description": "Open-ended embodied discovery and skill accumulation in simulated environments (details in the Voyager paper; not provided here).",
            "action_space_text": "LLM-generated plans/commands represented in natural language.",
            "action_space_embodied": "High-level actions/skills discovered and executed in simulation (details not provided here).",
            "action_mapping_method": "Cited work uses LLM planning and then executes/learns skills; mapping details are in the Voyager paper and not described here.",
            "perception_requirements": "Not specified in this paper.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Demonstrates the utility of very large LLM priors (GPT-4) to bootstrap skill learning in embodied agents (as referenced).",
            "transfer_failure_factors": "Not discussed here.",
            "key_findings": "Referenced as related work showing LLMs can be used to discover skills during embodied learning; detailed claims are in the Voyager paper.",
            "uuid": "e1831.3",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PaLM-E (referenced)",
            "name_full": "PaLM-E",
            "brief_description": "A multimodal embodied language model applying large pretrained language models to robot perception and control (referenced in related work).",
            "citation_title": "PaLM-E: An embodied multimodal language model",
            "mention_or_use": "mention",
            "model_agent_name": "PaLM-E (referenced)",
            "model_agent_description": "PaLM-E integrates language and perception to support embodied tasks; cited here as an example of deploying large language models in robot learning and real-world interactions.",
            "pretraining_data_type": "Multimodal pretraining (language + vision / embodied data) in the original PaLM-E work; details not provided in this paper.",
            "pretraining_data_details": "Not specified in this paper.",
            "embodied_task_name": "Robotic real-world interaction tasks (referenced; not used in this paper's experiments).",
            "embodied_task_description": "Robotic manipulation/navigation tasks leveraging multimodal inputs; specifics in the PaLM-E paper.",
            "action_space_text": "Natural language instructions and plan-like outputs in the original PaLM-E work (not detailed here).",
            "action_space_embodied": "Robot control commands / high-level instructions (not detailed here).",
            "action_mapping_method": "PaLM-E uses multimodal conditioning and mapping to robot actions as described in its own paper; not detailed here.",
            "perception_requirements": "Multimodal (vision + language) in the PaLM-E work; not used directly in the experiments of this paper.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Shows integration of perception with LLM priors can enable embodied robot tasks (cited as related work).",
            "transfer_failure_factors": "Not discussed here.",
            "key_findings": "Mentioned as an example of applying large pre-trained language models to embodied robotics and multimodal control; details in the cited work.",
            "uuid": "e1831.4",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as I can, not as I say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Grounding large language models in interactive environments with online reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "LLaMA: Open and efficient foundation language models",
            "rating": 2
        },
        {
            "paper_title": "Language models meet world models: Embodied experiences enhance language models",
            "rating": 2
        }
    ],
    "cost": 0.02180325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments VIA REINFORCEMENT LEARNING</h1>
<p>Weihao Tan ${ }^{1}$, Wentao Zhang ${ }^{1}$, Shanqi Liu ${ }^{2}$, Longtao Zheng ${ }^{1}$, Xinrun Wang ${ }^{1,}$ Bo An ${ }^{1,3 *}$ ${ }^{1}$ Nanyang Technological University, Singapore ${ }^{2}$ Zhejiang University ${ }^{3}$ Skywork AI<br>{weihao001, wt.zhang, longtao001, xinrun.wang, boan}@ntu.edu.sg shanqiliu@zju.edu.cn</p>
<h4>Abstract</h4>
<p>Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decisionmaking environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>LLMs have demonstrated remarkable success in natural language generation and understanding (Brown et al., 2020; OpenAI, 2023). Recent studies show that LLMs can manage other AI models and tools to address complex multimodal tasks (Shen et al., 2023; Lu et al., 2023), assist or play sophisticated games, such as TextWorld (Yao et al., 2023), Handbi (Hu \&amp; Sadigh, 2023), and MineCraft (Wang et al., 2023a), or be deployed on robots for real-world interactions (Brohan et al., 2023; Driess et al., 2023). While LLMs can provide insightful suggestions for complex tasks, they often fail in solving simple decision-making tasks due to misalignment issues (Brohan et al., 2023).
There are two main misalignment issues leading to the failures of LLMs in decision-making
<img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Only tomato, lettuce (b) LLMs guide the agent and onion are provided in to put the lettuce on the the game. LLMs may cutting board, which choose to pick up addi- already contains tomato, tional ingredients, such as without knowing that each cucumber and pepper to cutting board can only cook the dish. contain one item at a time.</p>
<p>Figure 1: Two misalignment examples</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>tasks. i) LLMs may generate invalid actions. As the example shown in Figure 1a, LLMs may keep adding cucumber and pepper when asked to make a tomato and lettuce salad, while these ingredients are not provided. ii) LLMs may not know accurately the dynamic transitions of the environments, especially when some specific constraints are introduced in the environments. This incorrect estimation will make LLMs tend to choose actions that fit their learned common sense, resulting in the failure to solve domain-specific tasks. As the example shown in Figure 1b, LLMs may keep trying to put both tomato and lettuce on the same cutting board, without knowing that only one item can be placed on the board in this environment. Addressing these two misalignment issues requires careful alignment between LLMs and environments.</p>
<p>On the contrary, reinforcement learning (RL) learns agents' policies from scratch through trial and error in environments (Sutton \&amp; Barto, 2018), which ensures that RL agents are well aligned with environments. Most RL methods start from random policies, updated according to the return from the environments, which leads to poor sample efficiency as most policies have poor performance at the early stage of learning. One way to improve the sample efficiency is to incorporate the prior knowledge with the initialization of the policy and the exploration during training (Kumar et al., 2023). LLMs are ideal sources of prior knowledge for RL agents as LLMs are trained with enormous data from the corpus. Therefore, by leveraging RL to align LLMs with embodied environments to solve decision-making tasks, we can address the misalignment issues in LLMs and the sample efficiency issue in RL simultaneously.</p>
<p>Motivated by this idea, we propose True knoWledge cOmeS frOM practicE (TWOSOME), a general online framework that deploys LLMs as embodied agents to efficiently interact and align with environments via RL to solve decision-making tasks without requiring any prepared datasets or prior knowledge of the environments. Instead of letting LLMs directly generate actions, we use the loglikelihood scores of each token provided by LLMs to calculate the joint probabilities of each action and form valid behavior policies. This process eliminates the misalignment caused by invalid actions. Moreover, the LLM agents are optimized with proximal policy optimization (PPO) (Schulman et al., 2017) using rewards from the environments, which eliminates the misalignment caused by dynamic transitions. We observe that the formed behavior policies suffer a severe issue that longer actions tend to have lower joint probabilities, resulting in an unreasonable unbalance over the action distribution. To overcome this issue, we propose token normalization and word normalization in terms of the number of tokens and words in actions to rectify the unbalance. Furthermore, we design a novel architecture for efficient training, where both the actor and critic in RL methods share the same frozen LLaMA-7B model (Touvron et al., 2023), updated by parameter efficient finetuning methods, e.g., LoRA (Hu et al., 2022). During training, we observe that prompts for observations and actions can greatly influence the initial policies of LLM agents, therefore, we also summarize four principles for designing efficient prompts to enhance the reasoning ability of LLMs.</p>
<p>We first evaluate our methods on a classical RL decision-making environment, Overcooked, and a simulated physical environment, VirtualHome, with various tasks to show that our proposed word normalization method can remarkably improve stability and accelerate convergence during the training process. TWOSOME with word normalization exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in all tasks. Then, we test our trained TWOSOME agents in eight new unseen tasks and find that TWOSOME has superior generalization ability across unseen tasks. Finally, we evaluate our trained TWOSOME agents on traditional NLP benchmarks to demonstrate that under our framework, there is no significant loss of the LLMs' original ability during online PPO fine-tuning.</p>
<h1>2 Related Work</h1>
<p>In this section, we present a brief overview of related work. More discussions are in Appendix A.
Embodied Agents with LLMs. Recent methods use LLMs to assist planning and reasoning in robot learning (Brohan et al., 2023; Liang et al., 2022; Zeng et al., 2022) and simulation environments (Fan et al., 2022; Wang et al., 2023a; Yao et al., 2023). LLMs are also applied to help robot navigation (Parisi et al., 2022; Majumdar et al., 2020) and manipulation (Jiang et al., 2022; Ren et al., 2023; Khandelwal et al., 2022). Among them, ReAct (Yao et al., 2023) uses chain-of-thought prompting by generating both reasoning traces and action plans with LLMs. SayCan (Brohan et al., 2023) leverages the ability of LLMs to understand human instructions to make plans for completing</p>
<p>tasks without finetuning LLMs. Voyager (Wang et al., 2023a) leverages GPT-4 to learn and continually discover skills during learning. While these works exhibit promising results, they rely too heavily on the inherent capabilities of powerful LLMs, which are difficult to apply to smaller LLMs with weaker reasoning abilities. Concurrent to our work, GLAM (Carta et al., 2023) uses RL to ground LLMs. However, they focus on simple primitive actions in toy environments without rich semantics, resulting in underutilizing the capabilities of LLMs, and failing to observe the impact of prompt design and address the unbalance over action space.</p>
<p>Finetuning LLMs. Parameter-efficient finetuning (PEFT) can significantly reduce the number of parameters to tune LLMs while with minor loss of the performance (Ding et al., 2023). Prompt tuning (Lester et al., 2021) and prefix tuning (Li \&amp; Liang, 2021) prepend additional tunable tokens to the input or hidden layers, and adapter tuning (Houlsby et al., 2019) introduces the adapter to LLMs, where only the introduced tokens and adapters are finetuned. Recently, low-rank adaption (LoRA) (Hu et al., 2022) introduces low-rank matrices to approximate parameter updates. Reinforcement learning from human feedback (RLHF) is effective in finetuning LLMs (Ouyang et al., 2022), where the reward for RL is learned from human feedback. Another concurrent work (Xiang et al., 2023) leverages embodied environments to provide feedback to finetune LLMs. Different from our method, they use supervised learning to finetune LLMs with pre-collected embodied experiences by random sampling in environments instead of doing decision-making tasks from scratch.</p>
<h1>3 Preliminaries</h1>
<p>In this section, we provide a brief background on LLM and the RL problem formulation.
LLMs learn from text data using unsupervised learning. LLMs optimize the joint probabilities of variable-length symbol sequences as the product of conditional probabilities by $P(x)=$ $\prod_{i=1}^{n} P\left(s_{i} \mid s_{1}, \ldots, s_{i-1}\right)$, where $\left(s_{1}, s_{2}, \ldots, s_{n}\right)$ is variable length sequence of symbols.
LoRA is a parameter- and compute-efficient finetuning method that incorporates trainable rank decomposition matrices into each layer of an LLM. It allows indirect training of dense layers with weight matrix, $W_{0} \in \mathbb{R}^{d \times k}$ by optimizing the rank-decomposition matrices by $W_{0}+\Delta W=$ $W_{0}+B A$, where $B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}$, and the rank $r$ is much smaller than $d$ and $k$.
RL formulates decision-making problems as Markov Decision Processes (MDPs). An MDP is defined by the tuple $(S, A, T, R, \gamma)$, where $S$ is the state space, $A$ is the action space, $T$ is the transition dynamics, $R$ is the reward function and $\gamma$ is the discount factor. Agents select actions based on observations, aiming to maximize the expected discounted accumulative reward.
PPO is a state-of-the-art actor-critic RL method that optimizes the policy based on the accumulative reward with advantage function $A\left(s_{t}, a_{t}\right)=Q\left(s_{t}, a_{t}\right)-V\left(s_{t}\right)$, where $V\left(s_{t}\right)$ is the value function and $Q\left(s_{t}, a_{t}\right)$ is the action-value function. The objective of PPO can be expressed as:</p>
<p>$$
J(\theta)=\mathbb{E}<em _theta="\theta">{s, a}\left[\min \left(\frac{\pi</em>(s, a)\right)\right]
$$}(a \mid s)}{\pi_{\theta_{o l d}}(a \mid s)} A_{\pi_{\theta_{o l d}}}(s, a), \operatorname{clip}\left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{o l d}}(a \mid s)}, 1 \pm \epsilon\right) A_{\pi_{\theta_{o l d}}</p>
<p>where $A_{\pi_{\theta_{o l d}}}$ represents the advantage function of the policy before updating, and the clip operator controls the size of policy updates.
Aligning LLMs with Embodied Environments. We intend to deploy LLMs as interactive agents in embodied environments, where LLMs receive textual observations and generate textual actions executed in the environments. The textual observations and actions can be transferred from images or vectors by vision-language modes (VLMs) or scripts. Compared to primitive actions, high-level actions, such as macro-actions (Theocharous \&amp; Kaelbling, 2003) and learned skills (Konidaris et al., 2011) usually have richer semantics, benefiting LLMs to leverage their prior knowledge.</p>
<h2>4 TWOSOME</h2>
<p>In this section, we present TWOSOME, a general online framework to align LLMs with embodied environments via RL. We first describe how to deploy LLMs as embodied agents to generate valid actions. Secondly, we investigate the influence of lengths of action prompts on the generated policies and propose two normalization techniques, i.e., token normalization and word normalization, to al-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of how TWOSOME generates a policy using joint probabilities of actions. The color areas in the token blocks indicate the probabilities of the corresponding token in the actions.
leviate the unbalance over actions. Then, we design a parameter-efficient training method under the PPO framework. Finally, we summarize four principles for efficient prompt design for TWOSOME.</p>
<h1>4.1 VALID Policy GENERATION</h1>
<p>Actions directly generated by LLMs could be invalid in environments. This issue can be partially resolved by prompt design, i.e., adding the restrictions of actions in the prompt. However, most of the current LLMs cannot exactly follow the restrictions, especially for small and medium LLMs, i.e., less than 65B. Therefore, novel methods are required to let LLMs generate valid policies.</p>
<p>Instead of letting LLMs generate actions directly, TWOSOME queries the scores of all available actions from LLMs. These scores are used to determine the probabilities of executing the actions, which is similar to SayCan (Brohan et al., 2023). The process of the generation of policy is illustrated in Figure 2. Specifically, we associate a unique semantic prompt, e.g., pick up the tomato, to each action in the embodied environments, named action prompt, and observation prompt for the raw observation. The action prompt $a_{k} \in \mathcal{A}$ is a sequence of tokens $a_{k}=\left{w_{k}^{1}, \ldots, w_{k}^{N_{k}}\right}$ where $N_{k}$ is the length of the action prompt $k$. We note that the action prompts are not necessarily of the same length. At each step, the observation prompt $s \in \mathcal{S}$ is concatenated with each of the valid action prompts $a_{k}$, which forms the input of the LLM. We use the probabilities of tokens generated by the LLMs to compute the probability of the action prompt. Note that this probability differs from the probability of executing the action in the environment. For convenience, we call this probability token-level probability. The token-level probability of $a_{k}$ is</p>
<p>$$
P_{\text {token }}\left(a_{k} \mid s\right)=P\left(w_{k}^{1}, \ldots, w_{k}^{N_{k}} \mid s\right)=\prod_{i=1}^{N_{k}} P\left(w_{k}^{i} \mid s, w_{k}^{1}, \ldots, w_{k}^{i-1}\right)
$$</p>
<p>Normally, scores provided by LLMs are the loglikelihood of each token, namely the logits, $\log P_{\text {token }}$. We use softmax to normalize token-level probabilities over actions to get the policy:</p>
<p>$$
P\left(a_{k} \mid s\right)=\frac{\exp \left(\log P_{\text {token }}\left(a_{k} \mid s\right)\right)}{\sum_{a \in \mathcal{A}} \exp \left(\log P_{\text {token }}(a \mid s)\right)}
$$</p>
<p>There are two main advantages of this method: i) the generated policy is always valid for execution in the embodied environments and ii) leveraging the compositionability, our method can be applied to enormous actions representable by the vocabulary.</p>
<h3>4.2 ACTION PROMPT NORMALIZATION</h3>
<p>In this subsection, we will illustrate the issue in the method presented in the above section and present two normalization techniques to alleviate them.</p>
<p>Issue of Eq. (1). One key issue in Eq. (1) is that the probability of each token $P\left(w_{k}^{i} \mid\right)$ is always less than 1. Therefore, longer action prompts tend to have lower token-level probabilities, even though the longer action prompts may be more reasonable in the environment. For example, in Figure 2, pick up the tomato has more tokens than serve the dish but is the optimal action in terms of the given observation. A simple remedy for this issue is forcing all action prompts to have similar lengths, which is the case in GLAM (Carta et al., 2023) where all primitive actions have similar lengths. However, this will harm the applicability of the methods which makes the design of the</p>
<p>action prompts difficult, even impossible sometimes. Therefore, we propose two normalization techniques, token normalization and word normalization, to address this issue.</p>
<p>Token and Word Normalization. A simple idea to solve this issue is to normalize the token-level probabilities of the actions with the number of tokens, which can be defined as:</p>
<p>$$
\log P_{\text {token }}^{t n}\left(a_{k} \mid s\right)=\log P_{\text {token }}\left(a_{k} \mid s\right) / N_{k}
$$</p>
<p>We note that $\log P_{\text {token }}\left(a_{k} \mid s\right)$ will always be negative, the longer action prompts will have smaller negative values, therefore, dividing by the number of tokens will make the token-level probabilities of longer action prompts fall in the same magnitude with short action prompts. Another option is that instead of dividing the number of tokens, we can normalize the token-level probability by dividing the number of words, i.e.,</p>
<p>$$
\log P_{\text {token }}^{w n}\left(a_{k} \mid s\right)=\log P_{\text {token }}\left(a_{k} \mid s\right) / W_{k}
$$</p>
<p>where $W_{k}$ is the number of words in the action prompt $k$. For the example of pick up the tomato, $N_{k}=5$ while $W_{k}=4$.</p>
<p>Comparing the Two Normalizations. Though token normalization can eliminate the influence brought by the length of action prompts, it is slightly excessive. We observe that if a word is divided into several tokens, the first token usually has a relatively low probability, while the probabilities of the rest of the tokens tend to be remarkably high, which are often almost close to $100 \%$. For example, another important action, chop, in the Overcooked environment, is made up of two tokens, $c h$ and $o p$. The probabilities of $c h$ are usually in the range of $0-20 \%$, depending on the priority given by the LLM agent according to the observation, however, once $c h$ appears, the probability of $o p$ will boost to $90-99 \%$ in the next word since there are no other words starting with $c h$ in the observation prompts. The same phenomenon is also observed in tomao and dish as shown in Figure 2. LLMs discovered and learned this statistical law during the autoregressive training process. Thus, instead of normalizing the probabilities of actions according to the number of tokens, it is more reasonable to regard the several tokens made up of one word as an integrated symbol. Therefore, word normalization is a more suitable normalization, where the log probabilities of action get averaged according to the number of words in the action prompts.</p>
<h1>4.3 PARAMETER-EFFICIENT PPO FINETUNING</h1>
<p>In this section, we present the parameter-efficient finetuning method to align LLMs and embodied environments with the generated policies under the PPO framework. We first introduce the network design and then the training procedure.</p>
<p>Architecture. As shown in Figure 3, we add additional MLP layers to the last transformer block of LLaMA-7B model as the critic. The critic's MLPs use the last token of the observation prompt as input and output the estimated value of the observation prompt. On the other hand, the actor is formed by the frozen LLaMA-7B model with the augmentation of LoRA parameters. We also note that the dropout layers would bring additional training instabilities, because the randomness of the dropout may violate the KL divergence constraint in PPO, therefore, we do not use dropout in our LoRA modules. During training, only the MLPs for
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Parameter-efficient architecture the critic and the LoRA parameters for the actor are updated, which makes the training efficient. The LLaMA-7B model can also serve as the reference model to regularize the update of the parameters, which is not explored in this work. Though we focus on the decoder-only models, our method can be seamlessly extended to the encoder-decoder architecture, e.g., T5 (Raffel et al., 2020).</p>
<p>Training. The training procedure generally follows PPO (Schulman et al., 2017), which is demonstrated to be effective in finetuning LLMs with human feedback (Ouyang et al., 2022). We observe the training instability when updating the actor multiple times with the same data, which is also</p>
<p>observed in DeepSpeed Chat blog ${ }^{2}$. Therefore, every sampled data is discarded after training once. Given the fact that the newly added MLPs in the critic are initialized randomly, while the LoRA parameters in the actor are initialized as zero, i.e., the output of the actor is exactly the same as the LLaMA-7B model, which is reasonable, therefore, a larger learning rate for the critic and a smaller learning rate for the actor is preferred for stable training and fast convergence.</p>
<p>Inference. During inference, the critic is discarded and only the actor is needed. Furthermore, the alignment of the LLMs and embodied environments is fully encoded in the LoRA parameters, which is normally 20 times less than the LLMs, e.g., 4.2 M for LLaMA-7B. Therefore, the LoRA parameters can be a plug-and-play module of LLMs for generalizability across different environments.</p>
<h1>4.4 Prompt Design</h1>
<p>The prompts of observations and actions will significantly influence the generated policies by LLMs, which is a path orthogonal way to the finetuning for the alignment between LLMs and embodied environments. We summarize four principles for designing efficient prompts:</p>
<ul>
<li>Prompts of observations and actions should be cohesive for concatenation. Observation prompts end with you should and the next step is to, indicating the start of action prompts.</li>
<li>Articles, i.e., the, $a$, and an, are important for action prompts. Most action prompts consist of a verb and a noun, e.g., pick up the tomato. As LLMs are trained with high-quality corpus, they are sensitive to the articles. Therefore, pick up the tomato is better than pick up tomato, where the latter leads to the extremely low probability on tomato.</li>
<li>Preferred actions should appear in the observation prompt. As observed in (Xu et al., 2022; Fu et al., 2021), LLMs tend to assign higher probabilities to the repetitive tokens. Therefore, we can encourage LLMs to assign higher probability on preferred actions by emphasizing the nouns several times in the observation prompts. For example, if the observation prompt is I see a tomato. My task is to make a tomato salad. I should, then the tomato will have a relatively high probability.</li>
<li>The same action can have different action prompts under different observations. For example, in Overcooked, when the agent carries a bowl in hand, pick up the toamto can be replaced with put the tomato in the plate, where both action prompts have the same function in the environment, the latter one better fits with the context and thus has higher probability.</li>
</ul>
<p>The objective of the prompt design is to represent the observations and actions in the understandable manner of LLMs, thus improving the alignment between LLMs and embodied environments.</p>
<h2>5 EXPERIMENTS</h2>
<h3>5.1 EXPERIMENTS SETUP</h3>
<p>We deploy our methods on the LLaMA-7B model with half-precision and compare the performance among five methods: PPO (adapted from CleanRL (Huang et al., 2022a)), TWOSOME without finetuning (similar to SayCan (Brohan et al., 2023) with affordance value set to 1), TWOSOME without action prompt normalization (similar to GLAM (Carta et al., 2023) under decoder-only architecture and high-level actions) and TWOSOME with token normalization or word normalization. We mainly evaluate these methods in a typical decision-making environment, Overcooked, and a simulated physical household environment, VirtualHome.</p>
<p>Overcooked An agent is placed in the $7 \times 7$ Overcooked kitchen, aiming to make and serve a tomato salad and tomato-lettuce salad with the provided ingredients and tools in two tasks shown in Figure 4a and 4b. In the second task, we add an additional ingredient, onion as a disruptor, to show the robustness of our method. The agent needs to explore and learn the correct order to cook the dish with the provided macro-actions, such as Chop, Get-Tomato, and Go-Cutting-Board. The environment is partially observable. The agent only observes the objects within $5 \times 5$ square centered on the agent. The reward involves +0.2 for chopping a correct ingredient, +1 terminal reward for delivering the correct dish, -0.1 for delivering any wrong item, and -0.001 for every time step. The environment is adapted from Xiao et al. (2022) and Wu et al. (2021).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Experimental environments. Figure 4a and 4b show two tasks in Overcooked. Figure 4c and 4 d show two tasks in VirtualHome.</p>
<p>VirtualHome An agent is placed in a fully furnished household with various rich objects. Compared to Overcooked, this environment is more complex with a larger action space. The agent uses macro-actions to interact with the environment such as walk to the living room, turn on the TV and sit on the sofa. As shown in Figure 4c and 4d, we design two tasks. For the first task in Figure 4c, the agent needs to find the cold pancake on the table and heat it with the microwave in the kitchen. For the second task shown in Figure 4d, the agent plans to have some entertainment so it wants to find something to enjoy while watching TV. Thus, it needs to pick up chips and milk in the kitchen, bring them to the living room, turn on the TV, sit on the sofa and enjoy. The challenge arises when the agent already holding both the milk and chips, lacks an additional hand to turn on the TV. Consequently, it needs to learn to put at least one item on the nearby coffee table before operating the TV. Both tasks adopt a sparse reward setting, only when the task is finished, will the agent receive +1 reward. The environment is also partially observable. The agent can only see the objects in the current room. The environment is adapted from (Puig et al., 2018). A more detailed introduction of these two environments can be found in Appendix C. Our parameter-efficient framework enables us to complete all the experiments in one NVIDIA Tesla A100 40GB GPU. All the hyperparameters can be found in Appendix D.2. Finally, we provide the policy visualization and a detailed analysis of prompt design for all tasks in Appendix E and F.</p>
<h1>5.2 The Impact of Different Normalizations in TWOSOME</h1>
<p>Figure 5a shows the performance of finetuned TWOSOMEs. TWOSOME with word normalization succeeds in learning the optimal policy among all tasks, exhibiting great performance, and high sample efficiency over other methods, which is consistent with the analysis in Section 4.2. Except for the second Overcooked task, which is more difficult, TWOSOME without normalization learns quickly at the beginning but suffers a severe sudden drop in the the other three tasks. Without normalization, the unbalance over action distribution can accelerate the training process if the shorter action prompts happen to be reasonable actions but introduce extra training instability. TWOSOME with token normalization alleviates this instability slightly excessively, resulting in less data efficiency and slow convergence, as shown in the two overcooked tasks.</p>
<h3>5.3 TWOSOME vs. BASELINES</h3>
<p>Figure 5b shows the performance among the typical RL method, PPO, prompt tuning method, TWOSOME without finetuning, and our best method, TWOSOME with word normalization.</p>
<p>Comparison with Prompt Tuning. TWOSOME without finetuning fully relies on the capabilities of pre-trained LLMs with the designed prompts, demonstrating the LLM's decision-making ability</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of performance among PPO and four TWOSOME variants: i) without finetuning, ii) without normalization, iii) with token normalization, and iv) with word normalization. PPO masks unrelated actions in both VirtualHome tasks. The results are averaged over 3 seeds. The non-trained method is averaged over 100 episodes.
and the difficulty of the task. For the second task, which is the most difficult task, the non-finetuned LLM agent fails to finish the task completely. For the other three tasks, the low total returns with large variations indicate that the non-finetuned LLM agent can only finish these tasks with low probabilities. Prompt tuning can improve LLMs in solving easy decision-making tasks but difficult to solve complex and long-horizontal tasks with more interferents in the environment. We conclude that all training methods including PPO and all TWOSOME variants surpass the performance of the prompt tuning method, emphasizing the necessity of aligning LLMs with environments via RL.</p>
<p>Comparison with PPO. TWOSOME with word normalization succeeds in learning the optimal policies in both Overcooked tasks using 10k and 80k sampled data, respectively. In contrast, PPO fails to learn the optimal policy and gets stuck in the suboptimal due to partial observability. For the more complex second task, PPO even needs more than 500K steps to converge. As for the two tasks in VirtualHome, we find that vanilla PPO cannot deal with the large action space and learns nothing, as shown in Appendix C.3.1. So we mask all the unrelated actions in VirtualHome for PPO. The masked PPO manages to learn the optimal policy in the task of Food Preparation but still fails in the task of Entertainment. The results demonstrate that by leveraging the power of LLM, our method can defeat the traditional RL methods in solving decision-making tasks with large action spaces.</p>
<p>All finetuned TWOSOME methods, no matter whether they are normalized, manage to overcome partial observability and achieve optimal performance in at least two seeds in the two Overcooked tasks. Even though the agent does not see the target object, e.g., tomato, it can still appear in the goal part of the observation prompt, such as your task is to make a salad consisting of tomato, maintaining the high probabilities of tomato related actions, while other unrelated objects like onion, not appeared in neither the observation nor the observation prompt, remain relatively low probabilities. This feature greatly facilitates the exploration process and helps agents to sample good actions.</p>
<h1>5.4 Open-VOCAbULARY TASK GENERALIZATION</h1>
<p>We also evaluate the generalization ability of TWOSOME in eight new unseen tasks. LLMs' openvocabulary feature enables TWOSOME to transfer learned skills to different tasks, while traditional RL agents do not have such ability. We compare the performance between our finetuned TWOSOME and non-tuned TWOSOME. Figure 6 shows that for the first four tasks, which are similar to the agent's original trained task, though non-tuned TWOSOME can somehow finish the tasks, fine-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Task Generalization Tests. TWOSOME without Finetuning and TWOSOME with word normalization are tested in eight unseen tasks. Each task is tested over 100 episodes. For the first four tasks, Cheese, Hamburger, Apple Pie and Pizza, we replace the pancake in the original Food Preparation task with the corresponding food to see whether the agent can still finish the task. The following Washing Plate and Laundry tasks are more different but still have similar procedures to Food Preparation (agent needs to put dishes or clothes into the dishwasher or washing machine). For the last two crossover tests, TWOSOME agent trained in Food Preparation is tested in Entertainment and TWOSOME trained in Entertainment is tested in Food Preparation.
tuned TWOSOME achieves perfect performance. Replacing objects has little impact on finetuned TWOSOME. For the following two more different tasks, Washing Plate and Laundry, there is an obvious drop in the success rate of non-tuned TWOSOME, while finetuned TWOSOME still manages to complete the task. For the last two crossover tasks, where agents are tested in a completely different task, TWOSOME still exhibits remarkable performance. Especially in the Entertainment task, where non-tuned TWOSOME can barely finish the task and finetuned TWOSOME still maintains a $97 \%$ success rate. Total returns of each task are provided in Appendix B.2.</p>
<h1>5.5 IMPACT OF ONLINE FinETUNING</h1>
<p>To investigate the impacts of online finetuning on the LLM's abilities, we evaluate the models trained by TWOSOME with word normalization in Virtualhome on widely used NLP benchmarks (Gao et al., 2021). The models trained in Food Preparation and Entertainment are named TWOSOME-FP and TWOSOMEE, respectively. The four benchmarks are</p>
<p>Table 1: Zero-shot performance on Language Model Evaluation Harness (Gao et al., 2021).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ARC_C</th>
<th style="text-align: center;">HellaSwag</th>
<th style="text-align: center;">PIQA</th>
<th style="text-align: center;">MMLU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaMA-7B-hf</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">TWOSOME-FP</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: left;">TWOSOME-E</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.30</td>
</tr>
</tbody>
</table>
<p>ARC_C, HellaSwag, PIQA and MMLU, which are also reported in Touvron et al. (2023). Results of the zero-shot performance are displayed in Table 1, which demonstrates that there is no significant loss of the ability of the language understanding of LLMs after the aligning with embodied environments, and even sometimes brings minor improvements. The full results are in Appendix B.3.</p>
<h2>6 DISCUSSION AND CONCLUSION</h2>
<p>In this paper, we propose TWOSOME, a general online framework for efficiently aligning LLMs with embodied environments via RL to solve decision-making tasks without requiring any prepared datasets or prior knowledge of environments and without significant loss of LLMs' original ability. Instead of letting LLMs generate actions directly, TWOSOME is more controllable and feasible with better interpretability, since the impact of tuning prompts can be clearly reflected by the change of action probabilities. TWOSOME with word normalization exhibits significantly better sample efficiency and performance compared to traditional RL methods and prompt tuning methods. Moreover, TWOSOME exhibits a remarkable generalization ability to transfer learned skills and knowledge to unseen tasks. However, our method suffers a major limitation that training a PPO agent from scratch is far faster and cheaper than finetuning an LLM. TWOSOME needs to feed all the valid actions to the LLMs for every action sampling, resulting in multiple times the amount of computation and a small batch size. We hope this work can provide a step toward the general autonomous agent, where LLMs can self-improve by interacting with the world and harvesting true knowledge from practice.</p>
<h1>REPRODUCIbILITY STATEMENT</h1>
<p>This work does not use any dataset. All the TWOSOME experimental code and environment code of Overcooked and VirtualHome are included in the Supplementary Materials. We also provide videos for each task recorded by our best agent in the Supplementary Materials. All the hyperparameters and network architecture we use can be found in Appendix D. 1 and D.2. We provide the policy visualization and a detailed analysis of prompt design for all tasks in Appendix E and F. Our parameter-efficient framework enables us to complete all the experiments in one NVIDIA Tesla A100 40GB GPU. Additional experimental results such as success rate and NLP benchmarks can be found in Appendix B. A detailed introduction to the Overcooked and VirtualHome environments, including observation space, action space, and macro actions we use can be found in Appendix C.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISGAward No: AISG2-GC-2023-009). This research is also supported by the National Research Foundation, Singapore under its Industry Alignment Fund - Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. The computational work for this work was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).</p>
<h2>REFERENCES</h2>
<p>Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, et al. Imitating interactive intelligence. arXiv preprint arXiv:2012.05672, 2020.</p>
<p>Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as I can, not as I say: Grounding language in robotic affordances. In Conference on Robot Learning, pp. 287-318, 2023.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pp. 1877-1901, 2020.</p>
<p>Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. arXiv preprint arXiv:2302.02662, 2023.</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. Collaborating with language models for embodied reasoning. In Second Workshop on Language and Reinforcement Learning, 2022.</p>
<p>Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220-235, 2023.</p>
<p>Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. MineDojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.</p>
<p>Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei Shi. A theoretical analysis of the repetition problem in text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. $12848-12856,2021$.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo. 5371628 .</p>
<p>Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. Vln-bert: A recurrent vision-and-language bert for navigation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1643-1653, 2021.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pp. 2790-2799, 2019.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.</p>
<p>Hengyuan Hu and Dorsa Sadigh. Language instructed reinforcement learning for human-AI coordination. arXiv preprint arXiv:2304.07297, 2023.</p>
<p>Shengyi Huang, Rousslan Fernand JulienDossa Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and João GM Araújo. CleanRL: High-quality single-file implementations of deep reinforcement learning algorithms. The Journal of Machine Learning Research, 23(1):1258512602, 2022a.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147, 2022b.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, 2022c.</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li FeiFei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.</p>
<p>Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. LILA: Languageinformed latent actions. In Conference on Robot Learning, pp. 1379-1390, 2021.</p>
<p>Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective: CLIP embeddings for embodied AI. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 14809-14818, 2022.</p>
<p>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.</p>
<p>George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew Barto. Autonomous skill acquisition on a mobile manipulator. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1468-1473, 2011.</p>
<p>Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline Qlearning on diverse multi-task data both scales and generalizes. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, 2021.</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. Pre-trained language models for interactive decision-making. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, 2021.
J. Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. ArXiv, abs/2209.07753, 2022.</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023.</p>
<p>Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving vision-and-language navigation with image-text pairs from the web. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI 16, pp. 259-274. Springer, 2020.</p>
<p>OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Kumar Gupta. The unsurprising effectiveness of pre-trained vision models for control. In International Conference on Machine Learning, 2022.</p>
<p>Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8494-8502, 2018.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.</p>
<p>Allen Z Ren, Bharat Govil, Tsung-Yen Yang, Karthik R Narasimhan, and Anirudha Majumdar. Leveraging language for accelerated learning of tool manipulation. In Conference on Robot Learning, pp. 1531-1541, 2023.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with ChatGPT and its friends in Huggingface. arXiv preprint arXiv:2303.17580, 2023.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: An autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pp. 894-906. PMLR, 2022.</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022.</p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.
Georgios Theocharous and Leslie Kaelbling. Approximate planning in pomdps with macro-actions. Advances in neural information processing systems, 16, 2003.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023b.</p>
<p>Sarah A Wu, Rose E Wang, James A Evans, Joshua B Tenenbaum, David C Parkes, and Max Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent collaboration. Topics in Cognitive Science, 13(2):414-432, 2021.</p>
<p>Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. arXiv preprint arXiv:2305.10626, 2023.</p>
<p>Yuchen Xiao, Weihao Tan, and Christopher Amato. Asynchronous actor-critic for multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. arXiv preprint arXiv:2206.02369, 2022.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Andy Zeng, Adrian S. Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Peter R. Florence. Socratic models: Composing zero-shot multimodal reasoning with language. ArXiv, abs/2204.00598, 2022.</p>
<p>Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. 2023.</p>
<h1>APPENDIX</h1>
<h2>A MORE Related Work</h2>
<p>Embodied Agent with LLMs. The successful integration of language as a semantically rich input for interactive decision-making highlights the crucial role of LLMs in facilitating interaction and decision-making processes (Abramson et al., 2020; Karamcheti et al., 2021; Li et al., 2022). LLMs are also applied in various environments to aid robot navigation (Parisi et al., 2022; Hong et al., 2021; Majumdar et al., 2020) and manipulation (Jiang et al., 2022; Ren et al., 2023; Khandelwal et al., 2022). Recently, there have been a large number of methods that utilize LLMs to enhance planning and reasoning capabilities in embodied agents. SayCan (Brohan et al., 2023) assesses the affordance of candidate actions by multiplying their probabilities under LLMs with a value function. Zeng et al. (2022) combine the LLM with a visual-language model and a pre-trained language-conditioned policy (Shridhar et al., 2022) to enable open vocabulary robotic tasks. Huang et al. (2022b) demonstrate that LLMs can be employed for planning and executing simple household tasks. They ground LLM-generated actions by comparing their embeddings with a predefined list of acceptable actions. To incorporate environment feedback, Inner Monologue (Huang et al., 2022c) extends SayCan using a closed-loop principle. This principle is also applied in related works such as (Yao et al., 2023; Huang et al., 2022c; Kim et al., 2023; Singh et al., 2022; Liang et al., 2022; Shinn et al., 2023; Wang et al., 2023b;a) to continuously monitor agent behaviors and refine and adjust plans accordingly for tasks such as computer automation, Minecraft, etc. Furthermore, there are approaches that prompt LLMs to generate temporal-abstracted actions (Zheng et al., 2023). Dasgupta et al. (2022) employ the LLM as a planner and success detector for an agent with their actor module necessitates pre-training with RL to enable the agent to follow natural language instructions. While these works demonstrate impressive results, they rely too heavily on the inherent capabilities of powerful LLMs, like GPT4 and PaLM (Chowdhery et al., 2022), which are difficult to apply to smaller LLMs with weaker reasoning abilities, like LLaMA-7B.</p>
<p>Concurrent to our work, GLAM (Carta et al., 2023) utilizes RL finetuning to achieve functional grounding of LLMs. However, they focus on simple primitive actions (turn left, turn right, go forward, etc.) evaluated in toy environments, BabyAI (Chevalier-Boisvert et al., 2018) with a much smaller encoder-decoder LLM, Flan-T5-780M. These primitive actions have a similar number of tokens and less meaningful semantics, resulting in underutilizing the capabilities of LLMs, and failing to observe the impact of prompt design and address the unbalance over action space, resulting in additional instability and poor robustness.</p>
<p>Finetuning LLMs. Parameter-efficient finetuning (PEFT) can significantly reduce the number of parameters to tune LLMs while with minor loss of the performance (Ding et al., 2023). Prompt tuning (Lester et al., 2021) and prefix tuning (Li \&amp; Liang, 2021) prepend additional tunable tokens to the input or hidden layers, and adapter tuning (Houlsby et al., 2019) introduces the adapter, i.e., the bottleneck layers, to LLMs, where only the introduced tokens and adapters are finetuned. Recently, low-rank adaption (LoRA) (Hu et al., 2022) introduces low-rank matrices to approximate parameter updates.</p>
<p>Reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) proves that RL is an effective way to make alignments with human preferences and proposes a novel perspective regarding LLMs as actors under actor-critic framework. Our work is also motivated by this method. While RLHF needs to train a reward model to simulate human preferences, these signals are naturally present in most RL environments, which makes it possible to align LLMs with certain environments.</p>
<p>Another concurrent work (Xiang et al., 2023) leverages VirtualHome to collect embodied experiences by random sampling and use supervised learning to finetune LLMs to these embodied knowledge. They finally apply a simple RL method, MCTS, to plan without training, instead of directly generating plans. It is worth pointing out that our method also applies their supervised learning process to learn the embodied knowledge and get more familiar with the environments. Our online PPO sampling is much more efficient and feasible than their random sampling, e.g. in the overcooked tasks, it is almost impossible to complete the task by random sampling. In addition, an online learning PPO is obviously better than a simple MCTS without training. Compared with this work, our method is a simpler end-to-end online framework, which can automatically acquire new knowledge and solve decision-making tasks by interacting with environments without any prepared datasets.</p>
<h1>B ADDITIONAL EXPERIMENTAL RESULTS</h1>
<h2>B. 1 SUCCESS Rate in OVERCOOKED and VIRTUALHOME</h2>
<p>Besides the total reward provided in 5b, here we also provide the final success rates of all methods in Table 2, which tells almost the same story. Our TWOSOME with word normalization method achieves a $100 \%$ success rate across all tasks, exhibiting great stability and remarkable performance.</p>
<p>Table 2: Final success Rate of different methods in Overcooked and VirtualHome. Results are averaged over three seeds. Each seed is run for 100 episodes. Norm is short for normalization.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">PPO</th>
<th style="text-align: center;">TWOSOME</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W/O Finetuning</td>
<td style="text-align: center;">W/O Norm</td>
<td style="text-align: center;">Token Norm</td>
<td style="text-align: center;">Word Norm</td>
</tr>
<tr>
<td style="text-align: center;">Tomato Salad</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
<td style="text-align: center;">$0.67 \pm 0.47$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$\mathbf{1} \pm \mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: center;">Tomato-lettuce Salad</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$\mathbf{1} \pm \mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: center;">Food Preparation</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$0.81 \pm 0.01$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$\mathbf{1} \pm \mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: center;">Entertainment</td>
<td style="text-align: center;">$0.33 \pm 0.47$</td>
<td style="text-align: center;">$0.45 \pm 0.05$</td>
<td style="text-align: center;">$0.67 \pm 0.47$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$\mathbf{1} \pm \mathbf{0}$</td>
</tr>
</tbody>
</table>
<h2>B. 2 Performance of TWOSOME and SayCan in Eight Unseen tasks</h2>
<p>Table 3 and 4 show the final success rate and total return of our best method TWOSOME with word normalization and our baseline, SayCan(non-finetuned TWSOME) in eight unseen tasks. Each task is run for 100 episodes. TWOSOME exhibits superior generalization across all tasks.</p>
<p>Table 3: Final success rate of TWOSOME and SayCan in eight unseen tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Cheese</th>
<th style="text-align: center;">Hamburger</th>
<th style="text-align: center;">Applepie</th>
<th style="text-align: center;">Pizza</th>
<th style="text-align: center;">Washing Plate</th>
<th style="text-align: center;">Laundry</th>
<th style="text-align: center;">Food Preparation</th>
<th style="text-align: center;">Entertainment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TWOSOME</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.04</td>
</tr>
</tbody>
</table>
<p>Table 4: Total return of TWOSOME and SayCan in eight unseen tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Cheese</th>
<th style="text-align: center;">Hamburger</th>
<th style="text-align: center;">Applepie</th>
<th style="text-align: center;">Pizza</th>
<th style="text-align: center;">Washing Plate</th>
<th style="text-align: center;">Laundry</th>
<th style="text-align: center;">Food Preparation</th>
<th style="text-align: center;">Entertainment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TWOSOME</td>
<td style="text-align: center;">$0.77 \pm 0.01$</td>
<td style="text-align: center;">$0.77 \pm 0.01$</td>
<td style="text-align: center;">$0.77 \pm 0.01$</td>
<td style="text-align: center;">$0.77 \pm 0.01$</td>
<td style="text-align: center;">$0.77 \pm 0.02$</td>
<td style="text-align: center;">$0.76 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.24$</td>
<td style="text-align: center;">$0.43 \pm 0.14$</td>
</tr>
<tr>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">$0.18 \pm 0.17$</td>
<td style="text-align: center;">$0.21 \pm 0.17$</td>
<td style="text-align: center;">$0.23 \pm 0.17$</td>
<td style="text-align: center;">$0.17 \pm 0.18$</td>
<td style="text-align: center;">$0.18 \pm 0.17$</td>
<td style="text-align: center;">$0.08 \pm 0.13$</td>
<td style="text-align: center;">$0.20 \pm 0.16$</td>
<td style="text-align: center;">$0.01 \pm 005$</td>
</tr>
</tbody>
</table>
<h2>B. 3 Evaluation on NLP Benchmarks</h2>
<p>To investigate the impacts of the PPO online finetuning on the LLM's abilities, we evaluate the models trained by TWOSOME with word normalization in the virtual home tasks on widely used NLP benchmasrks (Gao et al., 2021). The models trained in Food Preparation and Entertainment are named TWOSOME-FP and TWOSOME-E, respectively. The four benchmarks are ARC_C, HellaSwag, PIQA and MMLU, which are also reported in (Touvron et al., 2023). The results on ARC_Challenge, HellaSwag and PIQA are displayed in Table 5 and the results of MMLU are displayed in Table 6. All results are calculated following the default configurations in (Gao et al., 2021).</p>
<p>Table 5: Zero-shot performance on Common Sense Reasoning tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLaMA</th>
<th style="text-align: center;">TWOSOME-FP</th>
<th style="text-align: center;">TWOSOME-E</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ARC_C</td>
<td style="text-align: center;">$0.42 \pm 0.01$</td>
<td style="text-align: center;">$0.43 \pm 0.01$</td>
<td style="text-align: center;">$0.43 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag</td>
<td style="text-align: center;">$0.57 \pm 0.00$</td>
<td style="text-align: center;">$0.58 \pm 0.00$</td>
<td style="text-align: center;">$0.58 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">PIQA</td>
<td style="text-align: center;">$0.79 \pm 0.01$</td>
<td style="text-align: center;">$0.79 \pm 0.01$</td>
<td style="text-align: center;">$0.79 \pm 0.01$</td>
</tr>
</tbody>
</table>
<p>Table 6: Zero-shot performance on Massive Multitask Language Understanding (MMLU).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLaMA</th>
<th style="text-align: center;">TWOSOME-FP</th>
<th style="text-align: center;">TWOSOME-E</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">abstract_algebra</td>
<td style="text-align: center;">$0.33 \pm 0.05$</td>
<td style="text-align: center;">$0.30 \pm 0.05$</td>
<td style="text-align: center;">$0.28 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">anatomy</td>
<td style="text-align: center;">$0.38 \pm 0.04$</td>
<td style="text-align: center;">$0.39 \pm 0.04$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">astronomy</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
<td style="text-align: center;">$0.24 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">business_ethics</td>
<td style="text-align: center;">$0.37 \pm 0.05$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
<td style="text-align: center;">$0.32 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">clinical_knowledge</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
<td style="text-align: center;">$0.35 \pm 0.03$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">college_biology</td>
<td style="text-align: center;">$0.37 \pm 0.04$</td>
<td style="text-align: center;">$0.35 \pm 0.04$</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">college_chemistry</td>
<td style="text-align: center;">$0.28 \pm 0.05$</td>
<td style="text-align: center;">$0.30 \pm 0.05$</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">college_computer_science</td>
<td style="text-align: center;">$0.25 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">college_mathematics</td>
<td style="text-align: center;">$0.34 \pm 0.05$</td>
<td style="text-align: center;">$0.35 \pm 0.05$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">college_medicine</td>
<td style="text-align: center;">$0.31 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">college_physics</td>
<td style="text-align: center;">$0.25 \pm 0.04$</td>
<td style="text-align: center;">$0.25 \pm 0.04$</td>
<td style="text-align: center;">$0.20 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">computer_security</td>
<td style="text-align: center;">$0.38 \pm 0.05$</td>
<td style="text-align: center;">$0.40 \pm 0.05$</td>
<td style="text-align: center;">$0.40 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">conceptual_physics</td>
<td style="text-align: center;">$0.32 \pm 0.03$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">econometrics</td>
<td style="text-align: center;">$0.23 \pm 0.04$</td>
<td style="text-align: center;">$0.20 \pm 0.04$</td>
<td style="text-align: center;">$0.21 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">electrical_engineering</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">elementary_mathematics</td>
<td style="text-align: center;">$0.24 \pm 0.02$</td>
<td style="text-align: center;">$0.23 \pm 0.02$</td>
<td style="text-align: center;">$0.23 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">formal_logic</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">global_facts</td>
<td style="text-align: center;">$0.32 \pm 0.05$</td>
<td style="text-align: center;">$0.29 \pm 0.05$</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_biology</td>
<td style="text-align: center;">$0.32 \pm 0.03$</td>
<td style="text-align: center;">$0.32 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_chemistry</td>
<td style="text-align: center;">$0.21 \pm 0.03$</td>
<td style="text-align: center;">$0.21 \pm 0.03$</td>
<td style="text-align: center;">$0.19 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_computer_science</td>
<td style="text-align: center;">$0.24 \pm 0.04$</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
<td style="text-align: center;">$0.28 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_european_history</td>
<td style="text-align: center;">$0.40 \pm 0.04$</td>
<td style="text-align: center;">$0.38 \pm 0.04$</td>
<td style="text-align: center;">$0.41 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_geography</td>
<td style="text-align: center;">$0.28 \pm 0.03$</td>
<td style="text-align: center;">$0.29 \pm 0.03$</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_government_and_politics</td>
<td style="text-align: center;">$0.30 \pm 0.03$</td>
<td style="text-align: center;">$0.30 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_macroeconomics</td>
<td style="text-align: center;">$0.28 \pm 0.02$</td>
<td style="text-align: center;">$0.26 \pm 0.02$</td>
<td style="text-align: center;">$0.24 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_mathematics</td>
<td style="text-align: center;">$0.24 \pm 0.03$</td>
<td style="text-align: center;">$0.25 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_microeconomics</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
<td style="text-align: center;">$0.24 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_physics</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
<td style="text-align: center;">$0.28 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_psychology</td>
<td style="text-align: center;">$0.33 \pm 0.02$</td>
<td style="text-align: center;">$0.32 \pm 0.02$</td>
<td style="text-align: center;">$0.27 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_statistics</td>
<td style="text-align: center;">$0.22 \pm 0.03$</td>
<td style="text-align: center;">$0.19 \pm 0.03$</td>
<td style="text-align: center;">$0.17 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_us_history</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
<td style="text-align: center;">$0.41 \pm 0.03$</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_world_history</td>
<td style="text-align: center;">$0.38 \pm 0.03$</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
<td style="text-align: center;">$0.42 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">human_aging</td>
<td style="text-align: center;">$0.43 \pm 0.03$</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
<td style="text-align: center;">$0.41 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">human_sexuality</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">international_law</td>
<td style="text-align: center;">$0.47 \pm 0.05$</td>
<td style="text-align: center;">$0.43 \pm 0.05$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">jurisprudence</td>
<td style="text-align: center;">$0.36 \pm 0.05$</td>
<td style="text-align: center;">$0.35 \pm 0.05$</td>
<td style="text-align: center;">$0.35 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">logical_fallacies</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
<td style="text-align: center;">$0.35 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">machine_learning</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
<td style="text-align: center;">$0.28 \pm 0.04$</td>
<td style="text-align: center;">$0.33 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">management</td>
<td style="text-align: center;">$0.28 \pm 0.04$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
<td style="text-align: center;">$0.22 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">marketing</td>
<td style="text-align: center;">$0.38 \pm 0.03$</td>
<td style="text-align: center;">$0.38 \pm 0.03$</td>
<td style="text-align: center;">$0.39 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">medical_genetics</td>
<td style="text-align: center;">$0.42 \pm 0.05$</td>
<td style="text-align: center;">$0.38 \pm 0.05$</td>
<td style="text-align: center;">$0.36 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">miscellaneous</td>
<td style="text-align: center;">$0.45 \pm 0.02$</td>
<td style="text-align: center;">$0.45 \pm 0.02$</td>
<td style="text-align: center;">$0.42 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">moral_disputes</td>
<td style="text-align: center;">$0.35 \pm 0.03$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
<td style="text-align: center;">$0.29 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">moral_scenarios</td>
<td style="text-align: center;">$0.24 \pm 0.01$</td>
<td style="text-align: center;">$0.24 \pm 0.01$</td>
<td style="text-align: center;">$0.24 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">nutrition</td>
<td style="text-align: center;">$0.33 \pm 0.03$</td>
<td style="text-align: center;">$0.33 \pm 0.03$</td>
<td style="text-align: center;">$0.30 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">philosophy</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">prehistory</td>
<td style="text-align: center;">$0.39 \pm 0.03$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">professional_accounting</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
<td style="text-align: center;">$0.28 \pm 0.03$</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">professional_law</td>
<td style="text-align: center;">$0.28 \pm 0.01$</td>
<td style="text-align: center;">$0.28 \pm 0.01$</td>
<td style="text-align: center;">$0.29 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">professional_medicine</td>
<td style="text-align: center;">$0.25 \pm 0.03$</td>
<td style="text-align: center;">$0.24 \pm 0.03$</td>
<td style="text-align: center;">$0.25 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">professional_psychology</td>
<td style="text-align: center;">$0.34 \pm 0.02$</td>
<td style="text-align: center;">$0.31 \pm 0.02$</td>
<td style="text-align: center;">$0.30 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">public_relations</td>
<td style="text-align: center;">$0.35 \pm 0.05$</td>
<td style="text-align: center;">$0.33 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">security_studies</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
<td style="text-align: center;">$0.23 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">sociology</td>
<td style="text-align: center;">$0.41 \pm 0.03$</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">us_foreign_policy</td>
<td style="text-align: center;">$0.42 \pm 0.05$</td>
<td style="text-align: center;">$0.41 \pm 0.05$</td>
<td style="text-align: center;">$0.41 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">virology</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
<td style="text-align: center;">$0.33 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">world_religions</td>
<td style="text-align: center;">$0.46 \pm 0.04$</td>
<td style="text-align: center;">$0.48 \pm 0.04$</td>
<td style="text-align: center;">$0.44 \pm 0.04$</td>
</tr>
</tbody>
</table>
<h1>C Details of Embodied Environments</h1>
<p>In this section, we present the details of the embodied environments Overcooked and VirtualHome.</p>
<h2>C. 1 OVERCOOKED</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Overcooked environments.</p>
<p>Goal. An agent is placed in the Overcooked kitchen and aims to cook a certain dish with the provided ingredients and tools and deliver it to the 'star' cell as soon as possible. Agents have to learn the correct procedure in terms of picking up raw vegetables, chopping them, and merging them in a bowl before delivering. Figure 7a and 7b show the recipes for making tomato salad and tomato-lettuce salad.</p>
<p>Observation Space. The environment is a $7 \times 7$ grid world involving ingredients, bowls, cutting boards, and a delivery counter. For the task of making tomato salad, there is only one tomato, one bowl, one cutting board, and one delivery counter available in the environment. For the task of making a tomato-lettuce salad, one tomato, one lettuce, one onion, one bowl, two cutting boards, and one delivery cell are provided in the environment. The onion serves as an interferent, though it does not appear in the recipe. The global state information consists of the positions of the agent and the above objects, and the status of each ingredient: chopped or unchopped. The environment is partially observable. For the observation, the agent only observes the positions and status of the entities within a $5 \times 5$ square centered on the agent. Other unseen objects are masked in the observation. And the initial positions of all the objects are known to agents. The symbolic raw observations are directly fed into PPO as input and converted to prompts with scripts to serve as LLMs' input.</p>
<p>Primitive-Action Space. Agents use five primitive actions: up, down, left, right, and stay to interact with the environment. Agents can move around and achieve picking, placing, chopping, and delivering by standing next to the corresponding cell and moving against it. All the macro-actions are based on these primitive actions.</p>
<p>Macro-Action Space. In this work, we mainly focus on using high-level macro-actions, since they usually have richer semantics. Every macro-action may take several time steps to complete. The main function of each macro-action and the corresponding termination conditions is exactly the same as the setting in (Xiao et al., 2022) despite that we only have one agent. Here we list and briefly describe all the macro-actions we use: Chop, chops a raw ingredient into pieces when the agent stands next to a cutting board with an unchopped ingredient on it. Get-Tomato, Get-Lettuce, Get-Onion, Get-Bowl, Go-Cutting-Board-1/2 and Deliver, which navigate the agent to the location of the corresponding object and execute the corresponding action.</p>
<p>In the first task of making tomato salad, Get-Tomato, Get-Bowl, Go-Cutting-Board-1 and Deliver are available. In the second task of making tomato-lettuce salad all the macro-actions are valid.</p>
<p>Dynamics: The transition in this task is deterministic. If an agent delivers any wrong item, the item will be reset to its initial position.</p>
<p>Reward: +0.2 for chopping a correct ingredient, +1 terminal reward for delivering the correct dish, -0.1 for delivering any wrong dish, and -0.001 for every timestep.</p>
<p>Episode Termination: Each episode terminates either when the agent successfully delivers the target dish to the delivery counter or reaches the maximal time steps, 200.</p>
<h1>C. 2 VirtualHome</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: VirtualHome environments.</p>
<p>Goal. An agent, represented as a humanoid avatar, is placed in a fully furnished household with various rich objects to interact with. Figure 8a and 8b show two tasks we design for the agent to complete. For the first task of food preparation, the agent needs to find the pancake on the table in the kitchen and put it in the microwave to heat. For the second task of entertainment, the agent needs to find and bring chips and milk, from the kitchen to the living room and succeed sitting on the sofa with the TV open and the milk and chips are nearby. The challenge arises when the agent, already holding both the milk and chips, lacks an additional hand to turn on the TV. Consequently, it needs to learn to put at least one item on the nearby coffee table before operating the TV.
Observation Space. The environment is also partially observable. The agent can only see the objects in the current room and does not the objects in the other room. The observation consists of a set of bool values, representing whether the agent sees the relative object, whether these objects are close to the agent and the status of the objects, such as whether the TV is turned on and whether the milk is on the coffee table. The symbolic raw observations are directly fed into PPO as input and converted to prompts with scripts to serve as LLMs' input.</p>
<h2>C. 3 Action SPace</h2>
<p>To simulate the daily activities, all the actions in VirtualHome are macro-actions. Every macroaction takes only time step for execution. Here, we list and briefly describe all the macro-actions as Table 7 .</p>
<p>Table 7: Actions in VirtualHome and their corresponding descriptions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Action</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[STANDUP]</td>
<td style="text-align: center;">Stand up</td>
</tr>
<tr>
<td style="text-align: center;">[SIT] <Object> <Object> (object_id)</td>
<td style="text-align: center;">Sit on the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <Object> (object_id)</td>
<td style="text-align: center;">Walk to the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[GRAB] <Object> (object_id)</td>
<td style="text-align: center;">Grab the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[OPEN] <Object> (object_id)</td>
<td style="text-align: center;">Open the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[CLOSE] <Object> (object_id)</td>
<td style="text-align: center;">Close the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[SWITCHON] <Object> (object_id)</td>
<td style="text-align: center;">Switch/Turn on the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[SWITCHOFF] <Object> (object_id)</td>
<td style="text-align: center;">Switch/Turn off the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[PUTIN] <Object1> (object1_id) <Object2> (object2_id)</td>
<td style="text-align: center;">Put the Object1(object1_id) in the Object2(object2_id)</td>
</tr>
<tr>
<td style="text-align: center;">[PUTBACK] <Object1> (object1_id) <Object2> (object2_id)</td>
<td style="text-align: center;">Put the Object1(object1_id) on the Object2(object2_id)</td>
</tr>
</tbody>
</table>
<p>For the first task of heating pancake, there are four rooms and two items in the environment and their corresponding object id are as follows: livingroom(267), kitchen(11), bathroom(172), bedroom(210), pancake(62), microwave(109). For the second task of watching TV, there are four rooms and five items in the environment and their corresponding object id are as follows: livingroom(267), kitchen(11), bathroom(172), bedroom(210), chips(61), milk(46), TV(297), sofa(276), coffeetable(268). For their actions, corresponding action prompts and abbreviated labels, please refer to Table 8 and Table 9.</p>
<p>Only when the agent is close to the object, can the agent operate the object. For example, the agent needs to walk to the TV before turning it on.</p>
<p>Table 8: Actions, action prompts and abbreviated labels in the heating pancake task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Action</th>
<th style="text-align: center;">Action Prompt</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[WALK] <livingroom> (267)</td>
<td style="text-align: center;">walk to the living room</td>
<td style="text-align: center;">Walk-Livingroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <kitchen> (11)</td>
<td style="text-align: center;">walk to the kitchen</td>
<td style="text-align: center;">Walk-Kitchen</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <bathroom> (172)</td>
<td style="text-align: center;">walk to the bathroom</td>
<td style="text-align: center;">Walk-Bathroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <bedroom> (210)</td>
<td style="text-align: center;">walk to the bedroom</td>
<td style="text-align: center;">Walk-Bedroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <pancake> (62)</td>
<td style="text-align: center;">reach for the pancake</td>
<td style="text-align: center;">Walk-Pancake</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <microwave> (109)</td>
<td style="text-align: center;">move to the microwave</td>
<td style="text-align: center;">Walk-Microwave</td>
</tr>
<tr>
<td style="text-align: center;">[GRAB] <pancake> (62)</td>
<td style="text-align: center;">grab the pancake</td>
<td style="text-align: center;">Grab-Pancake</td>
</tr>
<tr>
<td style="text-align: center;">[OPEN] <microwave> (109)</td>
<td style="text-align: center;">open the microwave</td>
<td style="text-align: center;">Open-Microwave</td>
</tr>
<tr>
<td style="text-align: center;">[CLOSE] <microwave> (109)</td>
<td style="text-align: center;">close the microwave</td>
<td style="text-align: center;">Close-Microwave</td>
</tr>
<tr>
<td style="text-align: center;">[PUTIN] <pancake> (62) <microwave> (109)</td>
<td style="text-align: center;">put the pancake in the microwave</td>
<td style="text-align: center;">Putin-Pancake-Microwave</td>
</tr>
</tbody>
</table>
<p>Table 9: Actions, action prompts and abbreviated labels in the watching TV task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Action</th>
<th style="text-align: center;">Action Prompt</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[WALK] <livingroom> (267)</td>
<td style="text-align: center;">walk to the living room</td>
<td style="text-align: center;">Walk-Livingroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <kitchen> (11)</td>
<td style="text-align: center;">walk to the kitchen</td>
<td style="text-align: center;">Walk-Kitchen</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <bathroom> (172)</td>
<td style="text-align: center;">walk to the bathroom</td>
<td style="text-align: center;">Walk-Bathroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <bedroom> (210)</td>
<td style="text-align: center;">walk to the bedroom</td>
<td style="text-align: center;">Walk-Bedroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <chips> (61)</td>
<td style="text-align: center;">reach for the chips</td>
<td style="text-align: center;">Walk-Chips</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <milk> (46)</td>
<td style="text-align: center;">reach for the milk</td>
<td style="text-align: center;">Walk-Milk</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <coffeetable> (268)</td>
<td style="text-align: center;">move to the coffee table</td>
<td style="text-align: center;">Walk-Coffeetable</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <TV> (297)</td>
<td style="text-align: center;">move to the TV</td>
<td style="text-align: center;">Walk-TV</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <sofa> (276)</td>
<td style="text-align: center;">move to the sofa</td>
<td style="text-align: center;">Walk-Sofa</td>
</tr>
<tr>
<td style="text-align: center;">[GRAB] <chips> (61)</td>
<td style="text-align: center;">grab the chips</td>
<td style="text-align: center;">Grab-Chips</td>
</tr>
<tr>
<td style="text-align: center;">[GRAB] <milk> (46)</td>
<td style="text-align: center;">grab the milk</td>
<td style="text-align: center;">Grab-Milk</td>
</tr>
<tr>
<td style="text-align: center;">[SWITCHON] <TV> (297)</td>
<td style="text-align: center;">turn on the TV</td>
<td style="text-align: center;">Switchon-TV</td>
</tr>
<tr>
<td style="text-align: center;">[SWITCHOFF] <TV> (297)</td>
<td style="text-align: center;">turn off the TV</td>
<td style="text-align: center;">Switchoff-TV</td>
</tr>
<tr>
<td style="text-align: center;">[SIT] <sofa> (276)</td>
<td style="text-align: center;">take a seat on the sofa</td>
<td style="text-align: center;">Sit-Sofa</td>
</tr>
<tr>
<td style="text-align: center;">[STANDUP]</td>
<td style="text-align: center;">stand up from the sofa</td>
<td style="text-align: center;">Standup-Sofa</td>
</tr>
<tr>
<td style="text-align: center;">[PUTBACK] <chips> (61) <coffeetable> (268)</td>
<td style="text-align: center;">put the chips on the coffee table</td>
<td style="text-align: center;">Putback-Chips-Coffeetable</td>
</tr>
<tr>
<td style="text-align: center;">[PUTBACK] <milk> (46) <coffeetable> (268)</td>
<td style="text-align: center;">put the milk on the coffee table</td>
<td style="text-align: center;">Putback-Milk-Coffeetable</td>
</tr>
</tbody>
</table>
<p>Dynamics: The transition in this task is deterministic.
Reward: We adopt a sparse reward setting, where only when the task is finished will the agent receive +1 reward.</p>
<p>Episode Termination: Each episode terminates either when the agent successfully finishes the task or reaches the maximal time steps, 50 since every macro-action takes only time step for execution. For the task of heating pancake, the agent succeeds when it places the pancake in the microwave and closes the microwave. For the task of watching TV, the agent succeeds when it sits on the sofa with TV turned on and chips and milk are on the coffee table or hold in hand.</p>
<h1>C.3.1 Vanilla PPO VS PPO With Action Mask</h1>
<p>As mentioned in Section 5.3, in VirtualHome environment, vanilla PPO without action mask cannot learn anything for the two tasks due to the large action space. Figure 9 shows the performance of vanilla PPO without action mask and PPO with action mask.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Performance of PPO without action mask and PPO with action mask in VirtualHome tasks.</p>
<h1>D Training Details</h1>
<p>Our results are mainly generated on a single NVIDIA Tesla A100 and NVIDIA RTX A6000 GPU. For all domains, we use the same architecture, which is shown in Figure 10. Throughout all the experiments, we conducted training and testing using the half-precision float16.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Parameter efficient architecture</p>
<h2>D. 1 Network Architecture</h2>
<p>The same neural network architecture is applied to both the actor and critic networks across TWOSOME method. In the LLaMA-7B model, additional MLP layers are incorporated into the last transformer block to serve as the critic. The critic's MLPs take the last token of the observation prompt as input and output the estimated value of the observation prompt. The critic utilizes a 3 layer MLP structure, with the number of neurons in each layer being 1024, 512, and 1, respectively. ReLU is employed as the activation function. On the other hand, the actor consists of the frozen LLaMA-7B model with the augmentation of LoRA parameters. The policy optimizer uses AdamW with an epsilon value of $1 \mathrm{e}-5$ and a weight decay of 0 . The critic optimizer uses Adam with an epsilon value of $1 \mathrm{e}-5$ and the default weight decay of 0 . In our experiments, we found that the default epsilon value of $1 \mathrm{e}-8$ in the policy optimizer's AdamW can lead to unstable training and potential "nan" outputs in Lora.</p>
<p>In the PPO method, the critic network remains a 3-layer MLP architecture with 64 neurons in each layer and a single neuron as the final output. However, the main distinction from TWOSOME lies in the choice of activation function, which is Tanh. Meanwhile, the actor in the PPO method also adopts a 3-layer MLP architecture. The number of neurons in the first two layers is the same as the critic, which is 64 . The activation function used is Tanh. The final layer's output depends on the number of actions in the task. For example, in the Entertainment task, there are 17 actions, while in the Food Preparation task, there are 10 actions. The optimizer uses Adam with an epsilon value of $1 \mathrm{e}-5$ and a weight decay of 0 .</p>
<h2>D. 2 Hyperparameters for TWOSOME and PPO</h2>
<p>In following subsections, we first list the hyper-parameter candidates used for training TWOSOME via grid search in the corresponding task, and then show the hyper-parameter table with the parameters used by TWOSOME and PPO achieving the best performance. We choose the best performance of each method depending on its final convergence value and convergence efficiency.</p>
<p>Table 10: Overcooked hyper-parameter candidates of TWOSOME for grid search training</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning rate pair (policy,critic)</th>
<th style="text-align: center;">$\begin{gathered} \text { (1e-6, 5e-5), (5e-7, 5e-5), (5e-7, 1e-5) } \ \text { (3e-6, 5e-5), (1e-6, 1e-5) } \ (200,0.99),(50,0.95) \ 1 \mathrm{e} 5,5 \mathrm{e} 5 \end{gathered}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Max episode steps and gamma</td>
<td style="text-align: center;">First group parameters</td>
<td style="text-align: center;">Second group parameters</td>
</tr>
<tr>
<td style="text-align: center;">Total time steps</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of environments</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Number of steps per environment policy rollout</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of policy</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of critic</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Number of epochs to update policy</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Gradient checkpoint steps</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">Surrogate clipping coefficient</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the entropy</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the value function</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">Maximum norm for the gradient clipping</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Target KL divergence threshold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: Overcooked hyper-parameter used of TWOSOME in Tomato Salad task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning rate pair (policy,critic)</th>
<th style="text-align: center;">$(5 \mathrm{e}-7,1 \mathrm{e}-5)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Max episode steps and gamma</td>
<td style="text-align: center;">$(200,0.99)$</td>
</tr>
<tr>
<td style="text-align: center;">Total time steps</td>
<td style="text-align: center;">5 e 5</td>
</tr>
<tr>
<td style="text-align: center;">Number of environments</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of steps per environment policy rollout</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of policy</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of critic</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of epochs to update policy</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Gradient checkpoint steps</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Surrogate clipping coefficient</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the entropy</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the value function</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Maximum norm for the gradient clipping</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Target KL divergence threshold</td>
<td style="text-align: center;">0.02</td>
</tr>
</tbody>
</table>
<p>Table 12: Overcooked hyper-parameter used of TWOSOME in Tomato Lettuce Salad task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning rate pair (policy,critic)</th>
<th style="text-align: center;">$(5 \mathrm{e}-7,1 \mathrm{e}-5)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Max episode steps and gamma</td>
<td style="text-align: center;">$(200,0.99)$</td>
</tr>
<tr>
<td style="text-align: center;">Total time steps</td>
<td style="text-align: center;">5 e 5</td>
</tr>
<tr>
<td style="text-align: center;">Number of environments</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of steps per environment policy rollout</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of policy</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of critic</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of epochs to update policy</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Gradient checkpoint steps</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Surrogate clipping coefficient</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the entropy</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the value function</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Maximum norm for the gradient clipping</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Target KL divergence threshold</td>
<td style="text-align: center;">0.02</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/microsoft/DeepSpeedExamples/blob/master/ applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/README.md&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
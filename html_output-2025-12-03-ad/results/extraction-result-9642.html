<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9642 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9642</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9642</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-276579678</p>
                <p><strong>Paper Title:</strong> A large language model framework for literature-based disease–gene association prediction</p>
                <p><strong>Paper Abstract:</strong> Abstract With the exponential growth of biomedical literature, leveraging Large Language Models (LLMs) for automated medical knowledge understanding has become increasingly critical for advancing precision medicine. However, current approaches face significant challenges in reliability, verifiability, and scalability when extracting complex biological relationships from scientific literature using LLMs. To overcome the obstacles of LLM development in biomedical literature understating, we propose LORE, a novel unsupervised two-stage reading methodology with LLM that models literature as a knowledge graph of verifiable factual statements and, in turn, as semantic embeddings in Euclidean space. LORE captured essential gene pathogenicity information when applied to PubMed abstracts for large-scale understanding of disease–gene relationships. We demonstrated that modeling a latent pathogenic flow in the semantic embedding with supervision from the ClinVar database led to a 90% mean average precision in identifying relevant genes across 2097 diseases. This work provides a scalable and reproducible approach for leveraging LLMs in biomedical literature analysis, offering new opportunities for researchers to identify therapeutic targets efficiently.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9642.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9642.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Open Relation extraction and Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage literature-semantics framework that uses LLMs to (1) extract atomic, verifiable relation triplets from scientific articles (LLM-ORE) and (2) embed aggregated relations for each entity pair into dense vectors (LLM-EMB) to enable large-scale downstream modeling and ranking of disease-gene associations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LORE framework (uses GPT-3.5 / OpenAI embeddings; optionally Llama-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework combining prompt-driven open relation extraction (LLM-ORE) to produce structured subject-predicate-object triplets per article, followed by an embedding stage (LLM-EMB) that encodes all relations for an entity pair into a single 512-dimensional vector; embeddings are used with an ML ranker (lambda-GBDT) for disease-wise pathogenicity prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>PubMed abstracts selected via pubmedKB and a bootstrapped literature subset (top genes per disease, top diseases per gene, and top DGs) to form a focused literature corpus for disease-gene relation extraction; also extended to all pubmedKB abstracts with DG co-occurrence for scaling experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1745538</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Synthesize literature knowledge about disease-gene (DG) relationships to produce a verifiable knowledge graph and dense literature-semantic embeddings for predicting gene pathogenicity per disease.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Two-stage LLM-driven pipeline: (1) LLM-ORE: demonstration-based text-continuation prompting of an LLM to produce concrete relational triplets (subject, predicate, object) per article for specified entity pairs; (2) LLM-EMB: concatenate relations per entity pair into a document (split if exceeding context), then produce a 512-d embedding for that pair using an embedding model. Extracted relations are lemmatized and tagged by a curated taxonomy of key semantics; downstream ML ranking (LambdaRank objective with GBDT) models disease-wise pathogenic flow.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature-scale knowledge graph (atomic relation triplets) + 512-d literature-semantic embeddings per disease-gene pair + PMKB-CV dataset with predicted pathogenicity scores and evidence links.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>'e1', 'causes', 'e2'. 'e1 mutations', 'are frequently encountered in', 'e2 patients'. (example atomic triplets); aggregated document: 'e1 causes e2.e1 mutations are frequently encountered in e2 patients.e1 haploinsufficiency results in e2.'; embedding: 512-d vector per DG.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison against ClinVar curated pathogenic DGs using leave-one-disease-out cross-validation; ranking evaluated with Average Precision (AP) per disease and Mean Average Precision (MAP) overall; baselines included naive paper co-occurrence counting, relation-counting using curated key semantics, linear regression on embeddings, and direct-questioning of GPT-3.5/GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using embeddings + ML-Ranker: MAP = 79.9% across 2097 diseases (all DGs). For DGs with LLM-ORE annotations, ML-Ranker MAP = 90.0%. Co-occurrence paper counting MAP = 69.4%. LLM-EMB linear regression MAP = 87.7%. Direct question to GPT-4o MAP = 31.7%. LLM-ORE (GPT-3.5) extraction produced ~11.3M relations from ~1.745M articles; Llama-8B extraction on all 3,997,496 abstracts produced 74,132,940 relations and enabled ML-Ranker MAP = 81.6% on that larger corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scalable literature synthesis into a concise verifiable knowledge graph; links extracted facts to source articles (verifiability); captures latent cross-disease pathogenic 'flow' enabling robust disease-wise ranking; works with smaller/open models (Llama-8B) enabling wider reproducibility; produces dense embeddings amenable to supervised models and visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on article abstracts (not full texts) so misses associations absent from abstracts; LLM extraction coverage not complete (LLM-ORE covered 71.4% of ClinVar DGs on the subset vs. 94.8% coverage by simple co-occurrence); taxonomy curation used ClinVar (potential bias); LLMs can hallucinate or produce noisy relations requiring downstream filtering and verification; embedding/regression axes capture only part of variance (top PCA axes explained limited variance).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Directly asking GPT-4o for DG pathogenicity produced poor rankings (MAP 31.7%), demonstrating that naive conversational queries to LLMs are unreliable for literature-scale synthesis. LLM-ORE misses some ClinVar associations that don't appear in abstracts. Regions in embedding space where pathogenic and non-pathogenic DGs mix require disease-wise modeling; point-wise modeling can fail without sufficient data. The curated key-semantics method's evaluation is not directly comparable because it used ClinVar for curation (introduces circularity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9642.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9642.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-ORE (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Open Relation Extraction using gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt-driven open relation extraction that asks an LLM to read an article (title + abstract) and emit concise factual relation triplets (subject, predicate, object) between target entities; used as the first-stage of LORE to create atomic statements for a literature knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A conversational/text-continuation LLM used with demonstration-based prompts (task-agnostic ORE demonstration followed by target-article application); prompts enforce structured list output of relational triplets and include examples/counterexamples to discourage unwanted behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A bootstrapped subset of PubMed abstracts selected from pubmedKB focusing on most relevant articles for DG analysis (top-three genes per disease, top-three diseases per gene, and top 15K DGs), totaling ~1.745M articles in the subset used for LLM-ORE extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1745538</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Extract atomic, verifiable disease-gene relations (including mutation, causation, association, cohort evidence) from each article for building a literature knowledge graph and for tagging by pathogenic semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Text-continuation prompting with a demonstration block that defines desired behavior (examples/counterexamples) and a target block applying ORE to the article and entity pair; each article+entity pair is mapped to a list of relational triplets; lemmatization of predicates and automatic tagging via curated key semantics follow extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>List of relation triplets per article for each entity pair; aggregated into a literature knowledge graph (11.285M relations reported from the subset).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example triplet: <'BRCA1', 'mutation observed in', 'breast cancer patient'>; demonstration-style output ensured concise predicates like 'causes', 'is associated with', 'co-segregates with'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Coverage and utility assessed by (a) fraction of ClinVar DGs covered by extracted relations (71.4% on the PMKB-CV subset) and (b) contribution to downstream ranking performance (ML-Ranker MAP improvements when using relations/embeddings); also compared to naive paper co-occurrence counts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM-ORE produced ~11.3M relations across ~1.745M articles; relations covered 71.4% of ClinVar DGs in the PMKB-CV subset; using relations (tag counts) provided improved interpretability but lower MAP than the full ML-Ranker pipeline; when used with embeddings and ML-Ranker, contributed to MAP up to 90.0% for DGs with LLM-ORE annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Generates atomic, source-linked, verifiable factual statements rather than free-form summaries; flexible/open relation space (not constrained to predefined relation types); demonstration prompts mitigate some undesirable outputs and encourage structured output.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extraction limited by abstract content; LLM may still produce noisy or hallucinated relations requiring downstream filtering/curation; extraction coverage less than simple co-occurrence in some cases; prompt-engineered extraction depends on prompt quality and LLM reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Missed ~28.6% of ClinVar DGs (in subset) that were present in abstracts via co-occurrence but not captured as relations. Some relations required abstract-level inference and could be incorrectly paraphrased or over-generalized by the LLM; long relation documents may need splitting due to context window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9642.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9642.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-EMB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Embedding of Aggregated Relations (LLM-EMB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encodes all extracted relations for a given entity pair into a single dense numerical embedding (512 dimensions) representing the literature-semantic knowledge about that pair, enabling downstream visualization and supervised ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-embedding-3-large (OpenAI) for embeddings; embedding vectors aggregated per entity pair</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An embedding model (OpenAI text-embedding-3-large) used to convert concatenated relation documents for each entity pair into 512-dimensional vectors; if documents exceed context, they are split and averaged weighted by token length.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>All relations produced by LLM-ORE for each disease-gene pair from the literature subset (11.285M relations across ~1.745M articles).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>11285095</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Represent literature knowledge for each disease-gene pair numerically to reveal latent structures (e.g., pathogenicity manifolds) and to provide features for ML ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Concatenate all sentence-level relations for an entity pair into a document (split into sub-documents if necessary), produce embeddings per sub-document via text-embedding-3-large, and aggregate (token-length-weighted average) into a single 512-d vector per DG.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>512-dimensional literature-semantic embedding per disease-gene pair; used for UMAP/PCA visualizations and as features for ML-Ranker.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A 512-d vector representing aggregated relations for <BRCA1, breast cancer>. Visual analyses (UMAP, PCA) show clustering of pathogenic DGs and a smooth 'pathogenic flow' manifold.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Used as features in linear regression and ML-Ranker evaluations against ClinVar labels; visual validation via UMAP/PCA showing clusters aligned with known pathogenic DGs and semantic lemmas.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM-EMB linear regression alone achieved MAP = 87.7% (on PMKB-CV). Visualizations showed pathogenic DGs cluster in subspaces correlated with semantics like 'mutation' and 'cause'.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Condenses literature across many articles into compact, analyzable numeric representations; reveals latent semantic structures (pathogenic flow) useful for cross-disease modeling; robust as features for downstream supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Embedding quality depends on the embedding model and the completeness/quality of extracted relations; averaging/summing sub-document embeddings may blur rare but important evidence; embeddings capture only part of variance (top PCA axes limited).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>DGs without any relations are assigned zero embeddings (handled but limits information); mixed regions in embedding space where pathogenic and non-pathogenic DGs coexist require disease-wise modeling—point-wise classifiers may struggle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9642.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9642.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-8B application</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B-Instruct (referred to as Llama-8B) used for LLM-ORE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 8B parameter instruct-tuned LLM used as a drop-in replacement for GPT-3.5 to perform article-level open relation extraction at literature scale, enabling processing of ~4M abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct (Llama-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An 8-billion-parameter instruction-tuned LLaMA-family model used with the same demonstration-based ORE prompts as GPT-3.5 to extract relations; chosen for accessibility and cost-effectiveness to scale extraction across millions of abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>All pubmedKB abstracts containing disease-gene co-occurrence (3,997,496 abstracts processed in the large-scale run).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>3997496</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Perform open relation extraction (LLM-ORE) across the full set of pubmedKB abstracts with DG co-occurrence to maximize coverage and produce a literature-scale relation set for embedding and ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Same text-continuation ORE prompting pipeline applied with Llama-8B instead of GPT-3.5; outputs were aggregated and embedded similarly to produce DG embeddings and downstream ML ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Large-scale set of relation triplets (74,132,940 relations reported) and corresponding embeddings enabling ML-Ranker over a much larger corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Same triplet format as GPT-3.5 outputs; aggregated to produce embeddings and PMKB-CV-style outputs at much larger scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Coverage of ClinVar DGs and MAP of ML-Ranker when using Llama-extracted relations and embeddings; compared to GPT-3.5-based run on the literature subset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Replacing GPT-3.5 with Llama-8B caused marginal performance loss in the subset experiment (Llama-based ML-Ranker MAP = 79.5% vs GPT-3.5-based 79.9%). When processing all 3.997M abstracts with Llama-8B, curated 74,132,940 relations covering 91.3% of ClinVar DGs and achieved ML-Ranker MAP = 81.6% on the larger corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Enables open, large-scale extraction due to accessibility and lower compute/costs; achieved comparable performance to GPT-3.5 with marginal loss while vastly expanding coverage (relations and ClinVar coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller/fewer parameters than large proprietary models may slightly reduce extraction quality in some contexts; still constrained by abstract-only inputs and by prompt quality; may require infrastructure for large-scale processing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Marginal degradation relative to GPT-3.5 in the subset experiment; potential for missing nuanced relations that larger models might capture; still subject to hallucination or noisy extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9642.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9642.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct LLM Q/A baseline (GPT-4o / GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct question-answering of DG pathogenicity using GPT-4o and GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach where LLMs are directly asked about disease-gene pathogenicity (without structured ORE/embedding pipeline), used to benchmark the effectiveness of LORE versus naive conversational querying of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (ver.2024-05-13) and GPT-3.5 (ver.2023-06-13)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large conversational LLMs queried in a direct fashion per DG to report pathogenicity or relevance, without restricting evidence sources or constructing a knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not an explicit corpus-driven approach; the models rely on their parametric memory (pretraining) and the immediate prompt; used as black-box baselines against the literature-curation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Directly ask the LLM whether a gene is pathogenic for a disease (i.e., plain conversational assessment of DG pathogenicity) rather than synthesizing from an explicit set of retrieved articles.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Direct Q/A with no explicit retrieval-augmented generation or constrained evidence source; used to estimate how well standalone LLM responses align with ClinVar-labeled pathogenic DGs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Single-shot judgments/answers per DG (narrative or categorical outputs), not a structured knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Direct answer such as 'Gene X is (likely) pathogenic for Disease Y because ...' (free-text, often without traceable citations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Converted outputs into rankings and measured MAP against ClinVar labels across PMKB-CV diseases; compared to LORE pipeline results.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4o baseline MAP = 31.7% (performed poorly). GPT-3.5 as direct Q/A was also not competitive compared to the LORE pipeline (specific direct-Q/A MAP not reported beyond GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Simplicity and immediacy; uses LLM's parametric knowledge without engineering an extraction pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Low reliability and verifiability (hallucination risk and opaque parametric memory); poor ranking performance for literature-scale DG pathogenicity prediction; outputs lack traceable evidence to sources.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performed poorly in ranking pathogenic genes (GPT-4o MAP 31.7%), demonstrating that direct conversational queries are insufficient for literature-scale, evidence-backed synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: a survey <em>(Rating: 1)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 2)</em></li>
                <li>Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine <em>(Rating: 2)</em></li>
                <li>The impact of large language models on scientific discovery: a preliminary study using GPT-4 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9642",
    "paper_id": "paper-276579678",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LORE",
            "name_full": "LLM-based Open Relation extraction and Embedding",
            "brief_description": "A two-stage literature-semantics framework that uses LLMs to (1) extract atomic, verifiable relation triplets from scientific articles (LLM-ORE) and (2) embed aggregated relations for each entity pair into dense vectors (LLM-EMB) to enable large-scale downstream modeling and ranking of disease-gene associations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LORE framework (uses GPT-3.5 / OpenAI embeddings; optionally Llama-8B)",
            "model_description": "Framework combining prompt-driven open relation extraction (LLM-ORE) to produce structured subject-predicate-object triplets per article, followed by an embedding stage (LLM-EMB) that encodes all relations for an entity pair into a single 512-dimensional vector; embeddings are used with an ML ranker (lambda-GBDT) for disease-wise pathogenicity prediction.",
            "model_size": null,
            "input_corpus_description": "PubMed abstracts selected via pubmedKB and a bootstrapped literature subset (top genes per disease, top diseases per gene, and top DGs) to form a focused literature corpus for disease-gene relation extraction; also extended to all pubmedKB abstracts with DG co-occurrence for scaling experiments.",
            "input_corpus_size": 1745538,
            "topic_query_description": "Synthesize literature knowledge about disease-gene (DG) relationships to produce a verifiable knowledge graph and dense literature-semantic embeddings for predicting gene pathogenicity per disease.",
            "distillation_method": "Two-stage LLM-driven pipeline: (1) LLM-ORE: demonstration-based text-continuation prompting of an LLM to produce concrete relational triplets (subject, predicate, object) per article for specified entity pairs; (2) LLM-EMB: concatenate relations per entity pair into a document (split if exceeding context), then produce a 512-d embedding for that pair using an embedding model. Extracted relations are lemmatized and tagged by a curated taxonomy of key semantics; downstream ML ranking (LambdaRank objective with GBDT) models disease-wise pathogenic flow.",
            "output_type": "Literature-scale knowledge graph (atomic relation triplets) + 512-d literature-semantic embeddings per disease-gene pair + PMKB-CV dataset with predicted pathogenicity scores and evidence links.",
            "output_example": "'e1', 'causes', 'e2'. 'e1 mutations', 'are frequently encountered in', 'e2 patients'. (example atomic triplets); aggregated document: 'e1 causes e2.e1 mutations are frequently encountered in e2 patients.e1 haploinsufficiency results in e2.'; embedding: 512-d vector per DG.",
            "evaluation_method": "Comparison against ClinVar curated pathogenic DGs using leave-one-disease-out cross-validation; ranking evaluated with Average Precision (AP) per disease and Mean Average Precision (MAP) overall; baselines included naive paper co-occurrence counting, relation-counting using curated key semantics, linear regression on embeddings, and direct-questioning of GPT-3.5/GPT-4o.",
            "evaluation_results": "Using embeddings + ML-Ranker: MAP = 79.9% across 2097 diseases (all DGs). For DGs with LLM-ORE annotations, ML-Ranker MAP = 90.0%. Co-occurrence paper counting MAP = 69.4%. LLM-EMB linear regression MAP = 87.7%. Direct question to GPT-4o MAP = 31.7%. LLM-ORE (GPT-3.5) extraction produced ~11.3M relations from ~1.745M articles; Llama-8B extraction on all 3,997,496 abstracts produced 74,132,940 relations and enabled ML-Ranker MAP = 81.6% on that larger corpus.",
            "strengths": "Scalable literature synthesis into a concise verifiable knowledge graph; links extracted facts to source articles (verifiability); captures latent cross-disease pathogenic 'flow' enabling robust disease-wise ranking; works with smaller/open models (Llama-8B) enabling wider reproducibility; produces dense embeddings amenable to supervised models and visualization.",
            "limitations": "Relies on article abstracts (not full texts) so misses associations absent from abstracts; LLM extraction coverage not complete (LLM-ORE covered 71.4% of ClinVar DGs on the subset vs. 94.8% coverage by simple co-occurrence); taxonomy curation used ClinVar (potential bias); LLMs can hallucinate or produce noisy relations requiring downstream filtering and verification; embedding/regression axes capture only part of variance (top PCA axes explained limited variance).",
            "failure_cases": "Directly asking GPT-4o for DG pathogenicity produced poor rankings (MAP 31.7%), demonstrating that naive conversational queries to LLMs are unreliable for literature-scale synthesis. LLM-ORE misses some ClinVar associations that don't appear in abstracts. Regions in embedding space where pathogenic and non-pathogenic DGs mix require disease-wise modeling; point-wise modeling can fail without sufficient data. The curated key-semantics method's evaluation is not directly comparable because it used ClinVar for curation (introduces circularity).",
            "uuid": "e9642.0",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-ORE (GPT-3.5)",
            "name_full": "LLM-based Open Relation Extraction using gpt-3.5-turbo-0613",
            "brief_description": "Prompt-driven open relation extraction that asks an LLM to read an article (title + abstract) and emit concise factual relation triplets (subject, predicate, object) between target entities; used as the first-stage of LORE to create atomic statements for a literature knowledge graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613 (GPT-3.5)",
            "model_description": "A conversational/text-continuation LLM used with demonstration-based prompts (task-agnostic ORE demonstration followed by target-article application); prompts enforce structured list output of relational triplets and include examples/counterexamples to discourage unwanted behaviors.",
            "model_size": null,
            "input_corpus_description": "A bootstrapped subset of PubMed abstracts selected from pubmedKB focusing on most relevant articles for DG analysis (top-three genes per disease, top-three diseases per gene, and top 15K DGs), totaling ~1.745M articles in the subset used for LLM-ORE extraction.",
            "input_corpus_size": 1745538,
            "topic_query_description": "Extract atomic, verifiable disease-gene relations (including mutation, causation, association, cohort evidence) from each article for building a literature knowledge graph and for tagging by pathogenic semantics.",
            "distillation_method": "Text-continuation prompting with a demonstration block that defines desired behavior (examples/counterexamples) and a target block applying ORE to the article and entity pair; each article+entity pair is mapped to a list of relational triplets; lemmatization of predicates and automatic tagging via curated key semantics follow extraction.",
            "output_type": "List of relation triplets per article for each entity pair; aggregated into a literature knowledge graph (11.285M relations reported from the subset).",
            "output_example": "Example triplet: &lt;'BRCA1', 'mutation observed in', 'breast cancer patient'&gt;; demonstration-style output ensured concise predicates like 'causes', 'is associated with', 'co-segregates with'.",
            "evaluation_method": "Coverage and utility assessed by (a) fraction of ClinVar DGs covered by extracted relations (71.4% on the PMKB-CV subset) and (b) contribution to downstream ranking performance (ML-Ranker MAP improvements when using relations/embeddings); also compared to naive paper co-occurrence counts.",
            "evaluation_results": "LLM-ORE produced ~11.3M relations across ~1.745M articles; relations covered 71.4% of ClinVar DGs in the PMKB-CV subset; using relations (tag counts) provided improved interpretability but lower MAP than the full ML-Ranker pipeline; when used with embeddings and ML-Ranker, contributed to MAP up to 90.0% for DGs with LLM-ORE annotations.",
            "strengths": "Generates atomic, source-linked, verifiable factual statements rather than free-form summaries; flexible/open relation space (not constrained to predefined relation types); demonstration prompts mitigate some undesirable outputs and encourage structured output.",
            "limitations": "Extraction limited by abstract content; LLM may still produce noisy or hallucinated relations requiring downstream filtering/curation; extraction coverage less than simple co-occurrence in some cases; prompt-engineered extraction depends on prompt quality and LLM reliability.",
            "failure_cases": "Missed ~28.6% of ClinVar DGs (in subset) that were present in abstracts via co-occurrence but not captured as relations. Some relations required abstract-level inference and could be incorrectly paraphrased or over-generalized by the LLM; long relation documents may need splitting due to context window limits.",
            "uuid": "e9642.1",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-EMB",
            "name_full": "LLM-based Embedding of Aggregated Relations (LLM-EMB)",
            "brief_description": "Encodes all extracted relations for a given entity pair into a single dense numerical embedding (512 dimensions) representing the literature-semantic knowledge about that pair, enabling downstream visualization and supervised ranking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-embedding-3-large (OpenAI) for embeddings; embedding vectors aggregated per entity pair",
            "model_description": "An embedding model (OpenAI text-embedding-3-large) used to convert concatenated relation documents for each entity pair into 512-dimensional vectors; if documents exceed context, they are split and averaged weighted by token length.",
            "model_size": null,
            "input_corpus_description": "All relations produced by LLM-ORE for each disease-gene pair from the literature subset (11.285M relations across ~1.745M articles).",
            "input_corpus_size": 11285095,
            "topic_query_description": "Represent literature knowledge for each disease-gene pair numerically to reveal latent structures (e.g., pathogenicity manifolds) and to provide features for ML ranking.",
            "distillation_method": "Concatenate all sentence-level relations for an entity pair into a document (split into sub-documents if necessary), produce embeddings per sub-document via text-embedding-3-large, and aggregate (token-length-weighted average) into a single 512-d vector per DG.",
            "output_type": "512-dimensional literature-semantic embedding per disease-gene pair; used for UMAP/PCA visualizations and as features for ML-Ranker.",
            "output_example": "A 512-d vector representing aggregated relations for &lt;BRCA1, breast cancer&gt;. Visual analyses (UMAP, PCA) show clustering of pathogenic DGs and a smooth 'pathogenic flow' manifold.",
            "evaluation_method": "Used as features in linear regression and ML-Ranker evaluations against ClinVar labels; visual validation via UMAP/PCA showing clusters aligned with known pathogenic DGs and semantic lemmas.",
            "evaluation_results": "LLM-EMB linear regression alone achieved MAP = 87.7% (on PMKB-CV). Visualizations showed pathogenic DGs cluster in subspaces correlated with semantics like 'mutation' and 'cause'.",
            "strengths": "Condenses literature across many articles into compact, analyzable numeric representations; reveals latent semantic structures (pathogenic flow) useful for cross-disease modeling; robust as features for downstream supervised learning.",
            "limitations": "Embedding quality depends on the embedding model and the completeness/quality of extracted relations; averaging/summing sub-document embeddings may blur rare but important evidence; embeddings capture only part of variance (top PCA axes limited).",
            "failure_cases": "DGs without any relations are assigned zero embeddings (handled but limits information); mixed regions in embedding space where pathogenic and non-pathogenic DGs coexist require disease-wise modeling—point-wise classifiers may struggle.",
            "uuid": "e9642.2",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama-8B application",
            "name_full": "Llama-3.1-8B-Instruct (referred to as Llama-8B) used for LLM-ORE",
            "brief_description": "An open-source 8B parameter instruct-tuned LLM used as a drop-in replacement for GPT-3.5 to perform article-level open relation extraction at literature scale, enabling processing of ~4M abstracts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct (Llama-8B)",
            "model_description": "An 8-billion-parameter instruction-tuned LLaMA-family model used with the same demonstration-based ORE prompts as GPT-3.5 to extract relations; chosen for accessibility and cost-effectiveness to scale extraction across millions of abstracts.",
            "model_size": "8B",
            "input_corpus_description": "All pubmedKB abstracts containing disease-gene co-occurrence (3,997,496 abstracts processed in the large-scale run).",
            "input_corpus_size": 3997496,
            "topic_query_description": "Perform open relation extraction (LLM-ORE) across the full set of pubmedKB abstracts with DG co-occurrence to maximize coverage and produce a literature-scale relation set for embedding and ranking.",
            "distillation_method": "Same text-continuation ORE prompting pipeline applied with Llama-8B instead of GPT-3.5; outputs were aggregated and embedded similarly to produce DG embeddings and downstream ML ranking.",
            "output_type": "Large-scale set of relation triplets (74,132,940 relations reported) and corresponding embeddings enabling ML-Ranker over a much larger corpus.",
            "output_example": "Same triplet format as GPT-3.5 outputs; aggregated to produce embeddings and PMKB-CV-style outputs at much larger scale.",
            "evaluation_method": "Coverage of ClinVar DGs and MAP of ML-Ranker when using Llama-extracted relations and embeddings; compared to GPT-3.5-based run on the literature subset.",
            "evaluation_results": "Replacing GPT-3.5 with Llama-8B caused marginal performance loss in the subset experiment (Llama-based ML-Ranker MAP = 79.5% vs GPT-3.5-based 79.9%). When processing all 3.997M abstracts with Llama-8B, curated 74,132,940 relations covering 91.3% of ClinVar DGs and achieved ML-Ranker MAP = 81.6% on the larger corpus.",
            "strengths": "Enables open, large-scale extraction due to accessibility and lower compute/costs; achieved comparable performance to GPT-3.5 with marginal loss while vastly expanding coverage (relations and ClinVar coverage).",
            "limitations": "Smaller/fewer parameters than large proprietary models may slightly reduce extraction quality in some contexts; still constrained by abstract-only inputs and by prompt quality; may require infrastructure for large-scale processing.",
            "failure_cases": "Marginal degradation relative to GPT-3.5 in the subset experiment; potential for missing nuanced relations that larger models might capture; still subject to hallucination or noisy extractions.",
            "uuid": "e9642.3",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Direct LLM Q/A baseline (GPT-4o / GPT-3.5)",
            "name_full": "Direct question-answering of DG pathogenicity using GPT-4o and GPT-3.5",
            "brief_description": "Baseline approach where LLMs are directly asked about disease-gene pathogenicity (without structured ORE/embedding pipeline), used to benchmark the effectiveness of LORE versus naive conversational querying of LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (ver.2024-05-13) and GPT-3.5 (ver.2023-06-13)",
            "model_description": "Large conversational LLMs queried in a direct fashion per DG to report pathogenicity or relevance, without restricting evidence sources or constructing a knowledge graph.",
            "model_size": null,
            "input_corpus_description": "Not an explicit corpus-driven approach; the models rely on their parametric memory (pretraining) and the immediate prompt; used as black-box baselines against the literature-curation pipeline.",
            "input_corpus_size": null,
            "topic_query_description": "Directly ask the LLM whether a gene is pathogenic for a disease (i.e., plain conversational assessment of DG pathogenicity) rather than synthesizing from an explicit set of retrieved articles.",
            "distillation_method": "Direct Q/A with no explicit retrieval-augmented generation or constrained evidence source; used to estimate how well standalone LLM responses align with ClinVar-labeled pathogenic DGs.",
            "output_type": "Single-shot judgments/answers per DG (narrative or categorical outputs), not a structured knowledge graph.",
            "output_example": "Direct answer such as 'Gene X is (likely) pathogenic for Disease Y because ...' (free-text, often without traceable citations).",
            "evaluation_method": "Converted outputs into rankings and measured MAP against ClinVar labels across PMKB-CV diseases; compared to LORE pipeline results.",
            "evaluation_results": "GPT-4o baseline MAP = 31.7% (performed poorly). GPT-3.5 as direct Q/A was also not competitive compared to the LORE pipeline (specific direct-Q/A MAP not reported beyond GPT-4o).",
            "strengths": "Simplicity and immediacy; uses LLM's parametric knowledge without engineering an extraction pipeline.",
            "limitations": "Low reliability and verifiability (hallucination risk and opaque parametric memory); poor ranking performance for literature-scale DG pathogenicity prediction; outputs lack traceable evidence to sources.",
            "failure_cases": "Performed poorly in ranking pathogenic genes (GPT-4o MAP 31.7%), demonstrating that direct conversational queries are insufficient for literature-scale, evidence-backed synthesis.",
            "uuid": "e9642.4",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: a survey",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 2,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine",
            "rating": 2,
            "sanitized_title": "benefits_limits_and_risks_of_gpt4_as_an_ai_chatbot_for_medicine"
        },
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using GPT-4",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        }
    ],
    "cost": 0.01584225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Problem Solving Protocol A large language model framework for literature-based disease-gene association prediction</p>
<p>Peng-Hsuan Li 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Yih-Yun Sun 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Hsueh-Fen Juan 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Department of Life Science
National Taiwan University
No. 1, Sec. 4, Roosevelt Rd10617TaipeiTaiwan</p>
<p>Center for Computational and Systems Biology
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Center for Advanced Computing and Imaging in Biomedicine
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Chien-Yu Chen 0000-0002-6940-6389
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Center for Computational and Systems Biology
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Center for Advanced Computing and Imaging in Biomedicine
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Department of Biomechatronics Engineering
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Huai-Kuang Tsai 0000-0002-4200-8137
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Institute of Information Science
Academia Sinica
No. 128, Academia Road, Section 211529NankangTaipeiTaiwan</p>
<p>Jia-Hsin Huang jiahsin.huang@gmail.com 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Problem Solving Protocol A large language model framework for literature-based disease-gene association prediction
AED27303585B563AB05DD7EDBA7B860210.1093/bib/bbaf070Received: November 15, 2024. Revised: January 9, 2025. Accepted: February 6, 2025literature miningbiomedical relation extractionNLPknowledge graphlarge language model
This study explores biomedical informatics and artificial intelligence, leveraging large language models and knowledge graphs to advance precision medicine and enhance the discovery of disease-gene relationships.</p>
<p>Introduction</p>
<p>Knowledge curation from scientific literature is fundamental to the advancement of research across disciplines [1][2][3].Traditionally, domain-specific knowledge accumulates incrementally and relies heavily on human expert review processes.The biomedical literature, for instance, is a key resource for identifying causal genetic elements associated with diseases and offering insights into clinical practice.Several expert-curated databases, such as ClinVar [4], COSMIC [5], OMIM [6], and PharmGKB [7], provide invaluable assessments of literature evidence.However, such resources are limited in scale because of the broad scope and the rapid expansion of scientific publications [8,9].Many computational approaches have been applied to enhance automation in biomedical literature-based discovery for different tasks, such as gene-disease association prediction [10][11][12], text mining and curation [13][14][15][16][17], and biomedical entity relation extraction [18][19][20][21].However, these methods primarily focus on extracting isolated sentences or paragraphs containing entities of interest rather than synthesizing comprehensive information across multiple sources and contexts.Thus, substantial efforts are required to create task-specific datasets and train models for each literature domain.</p>
<p>On the other hand, Machine Reading Comprehension (MRC) [22], wherein machines answer questions based on textual context, serves as a promising complement to human expertise in reading vast amounts of literature.Recent advancements in natural language processing, particularly the development of Large Language Models (LLMs), have significantly enhanced MRC capabilities to potentially accelerate knowledge synthesis [23][24][25].Recent LLMs, such as GPT-4, have demonstrated remarkable capabilities in textual comprehension across diverse domains; however, LLMs face challenges when it comes to reliability and verifiability [23,26].Specifically, LLMs are prone to hallucination, a phenomenon whereby plausible but factually incorrect information is generated.Moreover, the opaque parametric memory of LLMs poses a substantial obstacle to traceabilitysources of evidence supporting their statements are often unclear.To mitigate these concerns, researchers have applied Retrieval Augmented Generation (RAG) in LLM-based chatbots [27,28].RAG restricts the information source of an LLM to an explicit but small set of retrieved texts per user query.However, owing to scalability constraints, fast but shallow sentence similarity-based retrieval is required, leading to incomplete information capture as nuanced content and relevant articles are often missing.Figure 1.Overview of the literature-semantics framework.Given a large body of literature containing expert domain knowledge, the proposed framework creates a comprehensive unsupervised knowledge graph and numerical embeddings between entities.This enables large-scale supervised modeling of downstream tasks, where model predictions are accompanied by verifiable evidence of relations that can be traced back to the original articles.(a) The framework is applied to PubMed literature, and a knowledge graph containing semantic relations between diseases and genes and their mutations is created.(b) An embedding for disease-gene relationships, where each point in space contains the literature-semantic knowledge of a DG, is created.(c) The embedding is shown to contain a latent structure of DG pathogenicity.(d) We further analyze the disease-wise pathogenic f low and find that it is consistent across diseases and even smoother than the point-wise distribution.(e) An ML-ranker is trained to model the f low and to predict pathogenic genes for each disease, where the prediction scope is 200× larger than expert-curated supervision.(f) We curated 105 key semantics about DG pathogenicity with linguistic lemmas to automatically tag relations.(G) With the proposed framework, we facilitate future research on DG pathogenicity with a literature-scale knowledge base of predicted DG scores and supporting evidence.</p>
<p>In essence, scalable knowledge curation calls for a computational method that is inductive across domains and is capable of capturing nuanced textual context for knowledge synthesis, all while maintaining essential reliability and verifiability.To this end, we introduce LORE (LLM-based Open Relation extraction and Embedding), a novel literature semantics framework that encompasses the best of both worlds and is tested true in capturing disease-gene relationships across the PubMed literature (Fig. 1).</p>
<p>LORE leverages a two-stage reading methodology comprising LLM-based Open Relation Extraction (LLM-ORE) and LLM-based embedding (LLM-EMB) (Fig. 1a).First, LORE employs LLM-ORE to comprehend each article and generate atomic statements about entity relations therein.Curated knowledge is explicitly derived from individual articles, making the generated relations reliable and verifiable.Furthermore, this operation creates a comprehensive unsupervised knowledge graph of the literature, the conciseness of which makes it possible for LORE to then use LLM-EMB to encode the full relation knowledge between each entity pair and create numerical embeddings for downstream task-specific applications.In this work, we applied LORE to curate disease-gene relationship knowledge in the PubMed literature, using disease, gene, and variant annotations from pubmedKB [2].</p>
<p>The ClinVar [4] database is an important annotation repository of the relationships between genes and diseases.Using key disease-gene pairs from ClinVar as references, we evaluated the effectiveness of LORE to explore the latent space of gene-wise pathogenicity and disease-wise pathogenic f low across genes (Fig. 1b).In addition, we constructed machine learning models with the supervision of pathogenic genes from ClinVar to rank the relevance of gene pathogenicity using the semantic embeddings (Fig. 1b).</p>
<p>Finally, we curated a taxonomy of key semantics to use as tags for relations (Fig. 1c).We created PMKB-CV (pubmedKB-ClinVar) dataset, a novel resource that expands the scope of disease-gene relationship data.Notably, PMKB-CV encompasses more than 2097 diseases and covers disease-gene pairs (DGs) at a scale 200 times larger than that covered by ClinVar.Moreover, PMKB-CV provides rich annotations, including semantic embeddings, predicted DG scores, and verifiable knowledge graph relations with tags and source article IDs (Fig. 1c).In summary, our LORE framework harnesses the power of LLM-based MRC and enables literaturescale knowledge graph construction and downstream modeling.Importantly, PMKB-CV further bridges the gap between largescale computational analysis and human assessment, helping to advance our understanding of disease genetics and potential therapeutic targets.</p>
<p>Results</p>
<p>LLM-ORE curates semantic relations knowledge from literature</p>
<p>We applied LLM-ORE to PubMed abstracts for annotating diseasegene relationships and created a comprehensive unsupervised knowledge graph (Fig. 1a).A total of 11 million relations across 1.7 million abstracts were obtained by prompting GPT-3.5 [29].Using text-continuation prompts [30], we employed LLMs to analyze individual articles and extract atomic statements describing relations between pairs of entities.Figure 2 illustrates the prompt structure used to guide GPT-3.5 in this task.This prompt is composed of two key sections.The first section demonstrates a domain-agnostic Open Relation Extraction (ORE) [31].The text serves as a primer, independent of the specific biomedical context, to establish the expected format and level of detail for the extraction (see more details in Methods).Following the same structure as the demonstration, the second section applies the ORE process to the target article and entities under investigation.Notably, this approach allows for a generalized ORE from the literature, which is not constrained to predefined relation types or entity pairs.A total of 358,888 distinct semantic lemmas are present across the 11 million disease-gene relations.We reviewed 282 highcoverage lemmas in the knowledge graph and curated a taxonomy of 105 key semantics about pathogenicity (Fig. 3).The key semantics, automatically tagged to relations, served as a set of indexes to access the knowledge graph.We grouped the key pathogenicity semantics into four main classes.</p>
<p>Class 1, Relation, includes key semantics that directly describe the relation between a gene and a disease.For example, the key semantic 'mutation cosegregates with disease' in Class 1.3 describes the correlation between occurrence of a certain genetic mutation and whether the genome is from a patient of a certain disease.</p>
<p>Class 2, Mutation, conveys information about genetic mutations.This type of information implicates that the corresponding gene has a role in a certain disease.</p>
<p>Class 3, Disease, conveys information on the genetic aspects of diseases.This semantic implies that a certain gene is at play.Class 4 contains cohort and miscellaneous information that entails or hints at pathogenicity.</p>
<p>LLM-EMB embeddings capture underlying gene pathogenicity</p>
<p>In the second stage of LORE, we applied LLM-EMB to the PubMed DG knowledge graph created in the first stage using LLM-ORE.For each pair of entities, all their relations across all articles were encoded to a single vector, which contained the literature knowledge about their relationship.A dense 512-dimensional representation was created for each DG.</p>
<p>To analyze the latent pathogenicity structure within the literature-semantic embedding, we first displayed the embedding space using 2D UMAP [32] (Fig. 4a-d).Each point represented a DG.When points were colored by co-occurrence frequency in PubMed abstracts, high-frequency DGs were observed to be distributed throughout the space (Fig. 4a).When points were colored by ClinVar pathogenicity labels, pathogenic DGs were observed to cluster in a subspace (Fig. 4b).This subspace was well-captured by the pathogenic score prediction of ML-Ranker (See more details about the ML-Ranker in the following sections and in Methods).With an optimal pathogenic score threshold to split the DGs by color into red or gray, the distribution was close to that of the ClinVar labels (Fig. 4d).In contrast to the ClinVar labels, which were human-curated and sparse, the stratified pathogenic score predicted by ML-Ranker delineated a smooth landscape of pathogenicity (Fig. 4c).Thus, high-score DGs not curated yet in ClinVar will be of high interest to the biomedical community.</p>
<p>Next, we displayed the 3D linear subspaces of the LLM-EMB embedding and observed the DG pathogenicity distribution (Fig. 4e-g).The axes were calculated using Principal Component Analysis (PCA) or ridge regression, a regularized version of ordinary least squares regression, against ClinVar pathogenicity labels.For the PCA subspace (Fig. 4e), the top three dimensions explained 12.8% of the variance of the embedding.A latent structure of two manifolds was observed-a dense spherical cluster of gray DG points without ClinVar annotations and a curved pattern of gradual transition from gray to red (i.e., known pathogenic DGs annotated in ClinVar).For the subspaces spanned by a combination of PCA axes and regression axes (Fig. 4f,g), clearer manifolds were observed when more regression axes were included.With two regression axes, a distribution pattern aligned with ClinVar annotations, signifying an association with pathogenicity, was revealed.We noted that the regression axes spanned a smaller subspace of the embedding compared with the PCA axes.Nevertheless, they provided an analytical pathogenicity perspective of the unsupervised embedding.</p>
<p>We further analyzed the literature-semantics structure by visualizing the distribution of important semantic lemmas in the 3D PCA subspace (Fig. 4h-j).The frequency of each semantic lemma, such as 'cause,' was represented in log scale per DG using a cool-blue-to-warm-red color gradient.Distinct patterns emerged for various lemmas, particularly those lemmas with a f latter distribution, such as 'associate,' 'mutation,' and 'cause.' The semantic 'associate' (Fig. 4h) was observed to increase along a linear axis but was more prevalent (indicated by greener colors) in the curved arm than in the sparse parts (bluer).The semantic 'mutation' (Fig. 4i) was primarily distributed along the curved arm.Similarly, the semantic 'cause' (Fig. 4j) was concentrated along the curved arm, with a significant presence at the space correlated with ClinVar annotations.This visualization of semantic lemmas revealed a connection between the intrinsic semantic structure of LLM-EMB and the latent DG point-wise pathogenicity structure.</p>
<p>In short, our analyses revealed the connection between the literature semantics captured by LORE and the known pathogenic DGs in the ClinVar database, as illustrated in Fig. 4. Using both UMAP and PCA visualization techniques, we examined how these known pathogenic DGs are distributed within the semantic LLM-EMB embedding space.Notably, these pathogenic DGs clustered in specific subspaces that aligned with disease-related semantic concepts -particularly terms like 'mutation' and 'cause'.This clustering pattern demonstrates the potential value of LLM-EMB embeddings as robust features for developing our ML-Ranker system for pathogenicity prediction.</p>
<p>Pathogenic flow in the literature-semantic space</p>
<p>In this section, we explore the embedding space underlying the DG pathogenicity distribution (Fig. 5).First, to model pathogenicity, a straightforward approach would be to model the distribution of pathogenic DGs (i.e., red points) in the embedding space (Fig. 5a,b).Clear subspaces of pathogenicity and nonpathogenicity were evident in the literature-semantic space visualized using 3D linear axes (Fig. 4e-g).These low-rank subspaces can be seen in the gray balls and the ends of the gray-to-red arm.However, the gray and red dots co-occur in some places, such as the center of the arm.To distinguish between gray and red dots, high-dimensional hyperplanes or complex nonlinear subspaces are needed, requiring more data and hampering generalization.</p>
<p>However, the fundamental task is to predict the most relevant pathogenic genes for each disease.If the gray and red dots for each disease are distributed separately to the two ends of a linear axis or, more generally, a smooth curve, pathogenic and nonpathogenic genes can be distinguished by splitting the curve.Furthermore, if the curves are consistent across diseases, the relevant pathogenic genes for every disease can be identified by monotonically raising the predicted relevance along the curves.Under this condition, perfect modeling can be achieved even if nonpathogenic and pathogenic DGs from different diseases are mixed in the embedding space (Fig. 5c).</p>
<p>In this study, we defined pathogenic f low as the direction and magnitude of nonpathogenicity to pathogenicity at each location in space.We started by calculating the disease-wise f low direction at each DG.Then, we quantized the f low direction at each location in space, using f low magnitude to ref lect consistency.We observed a smooth, cross-disease consistent field of pathogenic f low in the literature-semantic embedding (Fig. 5d).A consistent f low of nonpathogenicity to pathogenicity was noted along the arm-shaped manifold of the gray-to-red transition, implying that although DG point-wise modeling was difficult in the central mixture of the arm, disease-wise modeling was smoother and much more linear.</p>
<p>Literature-scale pathogenicity prediction by ML-ranker</p>
<p>We built an ML-Ranker that can model disease-wise pathogenic f low and score DGs that co-occur in PubMed abstracts.We applied the lambda objective with Gradient-Boosted Decision Trees (GBDT) [33] to directly model disease-wise pathogenic f low.</p>
<p>We compiled the PMKB-CV dataset to validate the proposed approach (Fig. 6a).PMKB-CV contains 2097 diseases that are present in both pubmedKB and ClinVar.For these diseases, 652 701 DGs co-occurred in PubMed abstracts, whereas only 3004 DGs were human-curated by ClinVar.Paper abstract co-occurrence covered 94.8% of the 3004 known pathogenic DGs, whereas LLM-ORE relations covered 71.4% (Fig. 6b).We included pubmedKB annotations, such as the number of paper abstracts in which a DG co-occurs in the dataset, and also used them as model features.Moreover, we used zero embedding for DGs without relations.Consequently, all DGs in the dataset can be uniformly used for training and prediction.We used leave-one-disease-out cross-validation to evaluate the performance of ML-Ranker for predicting pathogenic genes for each disease iteratively.An average precision (AP) score was calculated for each disease and Mean Average Precision (MAP) was used to evaluate the overall performance of the ranker.As a baseline, we directly asked GPT-3.5 (ver.2023-06-13) and GPT-4o (ver.2024-05-13) about the pathogenicity of each DG.In addition, to put into perspective the effectiveness of the curated key semantics in identifying crucial literature evidence, we tested a DG pathogenicity prediction method of simply counting the number of tagged relations per DG.Of note, the curated key semantics were only used in the experiment 'LLM-ORE (key semantics)' (Fig. 6b).</p>
<p>For all DGs in PMKB-CV, ML-Ranker achieved an MAP of 79.9%, which was a significant enhancement over the 69.4% MAP of co-occurrence paper counting and the 31.7%MAP of GPT-4o (Fig. 6b).Similar performance (MAP = 79.2%) was observed when applying 5-fold cross-validation of disjoint genes subsets (Supplementary Fig. 1).</p>
<p>When focusing specifically on those DGs with LLM-ORE annotations, ML-Ranker yielded a remarkable MAP of 90.0% (Fig. 6b).The predictive performance (AP) of ML-Ranker was statistically significantly better than other methods including co-occurrence paper counting (LLM-#paper), literature-semantic relation counting (LLM-ORE), and latent pathogenic f low modeling (LLM-EMB) (Fig. 6c).For the highly prevalent diseases that co-occurred with the highest number of genes in pubmedKB, ML-Ranker achieved a robust MAP of 81%, compared with the 67% MAP of co-occurrence paper counting (Fig. 6d).GPT-4o does not perform well.In comparison, LLM-EMB linear regression alone achieves 87.7% performance, whereas the full-f ledged MLranker provides higher performance at 90.0% or higher coverage at 94.8%.(c) Dispersion of ranking performance across diseases for each method and the p-values of distribution differences by one-sided Wilcoxon signed-rank test.LLM-ranker significantly outperformed baseline methods.(d) Performance across diseases of different scopes.The naive paper counting method encountered difficulties while ranking DGs for diseases that co-occurred with many genes in PMKB, but our semantic embedding approach remained robust.(e) Extending the scope of LORE using the public llama-8B model.Replacing GPT-3.5 with llama-8B resulted in marginal performance loss.The smaller model was further applied to all 3.9 million papers with DG co-occurrence.The final extracted relations covered 91.3% of ClinVar DGs and achieved 81.6% ranking performance.(f) Top-ranked DGs, seven out of 586 in PMKB, for Tourette syndrome accompanied by their literature relation evidence.</p>
<p>Furthermore, we extended LORE to use the open-source Llama-8B model (ver.Llama-3.1-8B-Instruct).Using a much smaller and accessible model, ML-Ranker achieved a comparable 79.5% MAP to the 79.9% MAP of using GPT-3.5.Leveraging Llama-8B, we processed all 3,997,496 pubmedKB abstracts with DG co-occurrence and curated 74,132,940 relations.The resulting relations covered 91.3% of ClinVar DGs, enabling ML-Ranker to achieve a notable ranking performance of 81.6% MAP (Fig. 6e).</p>
<p>Finally, the DG scores and ranking provided by ML-Ranker are accompanied by literature evidence (Fig. 6f).Our approach facilitates future expert assessment of DG pathogenicity by a quick grasp of literature knowledge with key semantics relations and relevant articles.</p>
<p>Discussion</p>
<p>Recent advancements in LLMs aim to automate complex sensemaking as human endeavors in reading and connecting information across large collections of scientific literature [34].Our study introduces LORE, a novel literature semantics framework that fundamentally reframes how we leverage LLMs to extract and use knowledge from scientific literature.</p>
<p>The LORE framework offers several key advantages.First, knowledge synthesis using the LORE approach constructs a literature knowledge graph of verifiable factual statements linked to the sources.Second, LORE offers a scalable framework for knowledge synthesis from large amounts of article texts.LORE extracts original article texts and transforms them into a concise knowledge graph.The approach is more efficient than traditional retrieval augmented generation approaches that select only a small set of articles for an LLM to read.The knowledge graph is much more concise compared with the original articles.This reduction in size and complexity allows for a more efficient representation of information; all the relevant knowledge can then be embedded for downstream tasks.In addition, LORE allows new publications to be annotated, thereby continually expanding the knowledge graph.Third, this approach places much less demand on the capability of LLMs, compared with directly asking LLMs expert domain questions.Using LORE, we have captured gene pathogenicity with GPT-3.5 and Llama-8B (Fig. 6e), a feat far from being achieved by directly asking GPT-4o (Fig. 6b).Indeed, small and open-source LLMs have been demonstrated to be competent for article-level comprehension [35,36], hence the methodology is not constrained to enterprise LLMs.Finally, our framework demonstrates remarkable efficacy in capturing disease-gene relationships through unsupervised relationship extraction and embedding, and users can also employ LORE with prompt engineering and fine-tuning to annotate task-specific knowledge across various domains of scientific inquiry [37,38].</p>
<p>When applying LORE to the complex landscape of DG relationships, we demonstrated the presence of a latent smooth field of cross-disease consistent pathogenic f low in the unsupervised literature-semantic embeddings.This discovery reveals that although pathogenic and nonpathogenic DGs from different diseases may occupy similar locations in the embedding space, a consistent directional f low of pathogenicity exists in terms of semantics.To illustrate, consider a simplified one-dimensional embedding axis where rare and common diseases coexist (Supplementary Fig. 2).Suppose that D1 is a rare disease reported in a few studies and that D2 is a common disease whose association with many genes is discussed in a multitude of papers; in this scenario, the following literature annotations, DG locations, and pathogenicity labels are possible:</p>
<p>G1 is not related to D1. (x = 0, y = non-pathogenic).</p>
<p>G2 mutation is found in a D1 patient. (x = 1, y = pathogenic). G3 mutation is found in a D2 patient; G3 is associated with D2. (x = 2, y = non-pathogenic). G4 mutation is found in a D2 patient; G4 causes D2. (x = 3, y = pathogenic).</p>
<p>Although the pathogenic D1G2 and the non-pathogenic D2G3 are mixed in the center of the axis, literature evidence about pathogenicity consistently increases along the axis.As a result, even when pathogenic and non-pathogenic associations are interspersed due to inter-disease differences such as popularity, the literature-semantic axis provides for a cross-disease consistent linear f low, enabling accurate pathogenicity modeling across diseases.</p>
<p>Our initial analyses of this pathogenic f low revealed clusters of disease-specific pathogenic curves.Notably, we found that these clusters often form a continuum, with the endpoint of one cluster serving as the starting point for another.This observation suggests a broader, interconnected field of pathogenic relationships across diseases, offering new perspectives on the complex landscape of genetic pathogenicity.</p>
<p>LORE curates knowledge for entity pairs that co-occur in literature articles.For the study of disease-gene pathogenicity, we noted that the potential curation scope was larger than the PMKB-CV dataset.PMKB-CV contained 2097 diseases that had both pubmedKB DG co-occurrence and known ClinVar pathogenic DGs (Fig. 6a); the full pubmedKB contained 3 128 402 DGs co-occurred in abstracts, spanning 8894 diseases (Supplementary Fig. 3).In this study, we focused on those 2097 diseases that could be validated by ClinVar, but the potential curation scope was as large as the 3 128 402 DGs.On the other hand, we also noted that the full ClinVar contained 4311 known pathogenic DGs, 1307 of which had no pubmedKB abstract co-occurrence.This was the inherent limitation to article abstract-based MRC.</p>
<p>Conclusion</p>
<p>In summary, our study makes three significant contributions to the field.First, it presents a novel literature semantics framework that addresses the long-standing challenges of comprehensiveness, reliability, and verifiability in machine reading comprehension.Second, it demonstrates the efficacy of LORE in capturing complex pathogenic relationships across diseases to reveal new insights into pathogenic f low.Finally, it provides a literature-scale dataset that not only complements existing resources such as ClinVar but also offers a knowledge graph of DG relationships with graded pathogenicity scores for genetic prioritization in clinical practice.The methodology of LORE is a general improvement on LLM-based machine reading comprehension, paving the way for bridging vast literature resources and actionable scientific knowledge to realize accelerated discoveries across scientific disciplines.</p>
<p>Methods</p>
<p>Two-stage reading comprehension of LORE</p>
<p>In the first stage of LORE, we applied LLM-ORE by prompting LLMs. Figure 2 shows the actual prompt we used.The demonstration consists of an article with a topic as Martin Likes Fish, target entity pair 'Martin' and 'fish', along with lists of unwanted and desired annotations.This section implicitly specifies the ORE task and the following required properties.</p>
<p>(1) The annotations should be concrete statements of fact implied by the article.</p>
<p>(2) The annotations should follow a structured format, specifically a list of relational triplets.</p>
<p>(3) Each relational triplet should be &lt; 'subject', 'predicate', 'object' &gt; .</p>
<p>(4) The subject should contain the first entity, and the first entity should only appear in the subject.</p>
<p>(5) The object should contain the second entity, and the second entity should only appear in the object.</p>
<p>(6) The predicate should be a concise, self-contained description of the relation between the subject and the object.</p>
<p>(7) Abstract understanding of the article is allowed beyond sentencelevel syntax.</p>
<p>The approach allows for abstract understanding beyond sentence-level syntax.The requirements are better understood through demonstration rather than explicit definitions.For instance, instead of explaining the terms 'subject,' 'predicate,' and 'object,' the examples and counterexamples in the demonstration make the concept clear.In addition, the examples illustrate behaviors that are easier to grasp instinctively rather than by complex rules.Examples include the removal of 'also' from 'Martin also loves eating fish', the rewriting of 'large fish scare him' to 'scare of', and the digested understanding of 'Martin dreamed about fishing' from 'he has a dream.The dream is about fishing'.Finally, we note that the ending '-' is important in ensuring that LLM follows the desired list format.</p>
<p>Formally, the desired generation
g = f LLM−ORE p, e 1 , e 2 ; m
where p is the target article, e 1 and e 2 are the names of the target entities, and m is the LLM model.In this work, we use the paper title and abstract as p.For m, gpt-3.5-turbo-0613 is used.The function maps &lt; p, e 1 , e 2 &gt; to g, the list of parsed relational triplets, using the text-continuation prompt shown in Fig. 2.</p>
<p>In the second stage of LORE, LLM reads all relations between an entity pair and produces a numerical representation of knowledge about their relationships.For example, the following relations between the entity e 1 and the entity e 2 are read by LLM as the following document.'e1', 'causes', 'e2'.'e1 mutations', 'are frequently encountered in', 'e2 patients'.'e1 haploinsufficiency', 'results in', 'e2'.Document: 'e1 causes e2.e1 mutations are frequently encountered in e2 patients.e1 haploinsufficiency results in e2.'.</p>
<p>If the document is larger than the allowed context of an LLM, it is split into multiple sub-documents, each of which contains as many relations as possible.</p>
<p>Formally, the embedding vector of an entity pair is given by
v = f LLM−EMB D e1,e2 ; m =</p>
<p>Modeling the pathogenic flow with ML-ranker</p>
<p>To visualize the disease-wise pathogenic f low, we define the f low as a unit vector at each DG that points to the pathogenic direction of disease D at the embedding location of DG.Formally, the f low vector is given by
u DG = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ v DG − 1 |S 0 D | DG ∈S 0 D v DG if P(DG) = 1 1 |S 1 D | DG ∈S 1 D v DG − v DG if P(DG) = 0 u DG = u DG | u DG |
where v denotes the embedding vector by LLM-EMB, S 0 D and S 1 D denote the sets of non-pathogenic and pathogenic diseasegene pairs of D respectively, and P maps non-pathogenic and pathogenic pairs to 0 and 1, respectively.In other words, each non-pathogenic DG has a unit f low vector directed at the average embedding of the pathogenic DGs of the same disease, and each pathogenic DG has a unit f low vector directed from the average embedding of the non-pathogenic DGs of the same disease.Then, we quantize the f low vectors for each cube in space by averaging them.Formally, a quantized f low vector is given by
u L = 1 | L | DG∈L u DG
where L is the set of DGs in a cube subspace.Finally, we show the field of pathogenic f low in Fig. 5d, where each arrow corresponds to a quantized f low vector.The arrow direction is the aggregated disease-wise f low direction from non-pathogenicity to pathogenicity, and the arrow length, proportional to | u L |, reflects the degree of cross-disease consistency at that location.</p>
<p>As shown in Fig. 5c, the essence of modeling the pathogenic f low is to model the difference between non-pathogenic and pathogenic genes per disease.Specifically, suppose DG1 and DG2 correspond to the same disease but P(DG1) = 1 and P(DG2) = 0. Let s denote the predicted pathogenicity score.Then one would want to maximize the probability Pr (P(DG1) &gt; P(DG2)) by minimizing the following RankNet [39]  To make sure the most relevant genes are ranked on top for each disease, the final gradient, λ, is the gradient of the RankNet loss multiplied by the change in Normalized Discounted Cumulative Gain (NDCG) [40].Formally, this LambdaRank [41] gradient is given by λ DG1,DG2 = ∂L ∂s DG1</p>
<p>• NDCG (DG1, DG2)</p>
<p>In this work, we use λGBDT, the lambda objective combined with GBDT [33], as the ML-Ranker to explicitly model the pathogenic f low in the literature-semantic space.</p>
<p>To evaluate the ranking performance for each disease, AP is used to see if the known pathogenic genes for a disease are ranked on top.Formally, for each disease,
AP = 1 R N k=1 known(k) × precision(k)
where N is the number of ranked genes for that disease, and R is the number of ranked known pathogenic genes for that disease.</p>
<p>known(k) = 1 if the gene ranked at k is a known pathogenic gene for that disease; otherwise known(k) = 0. precision(k) is the percentage of known pathogenic genes among genes ranked top-k for that disease.</p>
<p>Key semantics curation</p>
<p>The curation process consists of three main steps: linguistic lemma extraction, important lemma identification, and manual taxonomy construction.In the first step, the sentential relations extracted by LLM-ORE are tokenized to bags of words, and word inf lections are lemmatized to dictionary form.For example, cause, causes, caused, and causing all correspond to the same linguistic lemma.At this stage, entity names are also filtered out.</p>
<p>In the second step, important lemmas are identified using a coverage filter and a precision filter.The coverage filter demands a lemma to appear in LLM-ORE relations of at least n DGs.The precision filter requires that a certain proportion r of all the relations involving a lemma be from known pathogenic DGs, as indicated by ClinVar.The parameters can be adjusted according to the desired scope of key semantics.In this work, we selected a coverage parameter n = 100 and a precision parameter r = 50% and resulted in 282 important lemmas.</p>
<p>The final step involves manually curating a taxonomy of 105 key semantics by examining the LLM-ORE relations associated with these important lemmas During curation, we sampled 10 relations from known pathogenic DGs and 10 relations from other DGs to inspect for each lemma.The resulting key semantics were then used to tag all relevant relations in the respective lemmas.As a result, the LLM-ORE knowledge graph contains sentential relations linked to the semantic taxonomy of pathogenicity.</p>
<p>Furthermore, we experimented with a DG pathogenicity prediction method where the number of tagged relations for each DG is directly used as its pathogenic score.We note that the curation process has used the ClinVar information, so the generalizability of the ranking performance of this method is not directly comparable to other methods.Nevertheless, we put the effectiveness of the curation method into perspective, showing what performance and scope DG pathogenicity researchers can expect when using the key semantics tags to grasp the literature knowledge and identify relevant relations and articles for their DGs of interest.</p>
<p>Constructing the PMKB-CV dataset</p>
<p>We constructed the PMKB-CV dataset as a large-scale complement of ClinVar and an evaluation benchmark for our proposed methodology.The scope of the dataset is defined using disease IDs from Medical Subject Headings (MeSH), a vocabulary thesaurus maintained by the Nation Library of Medicine (NLM), and Homo sapiens protein-coding gene IDs from the National Center for Biotechnology Information (NCBI).</p>
<p>The PMKB-CV dataset comprises two main components including literature-based and expert database parts.For the literature part, we utilized the annotations from pubmedKB [2].The gene and variant mentions are both indexed by NCBI gene IDs, and we considered the occurrence of either a gene or its variant as an occurrence.This approach yielded 3,128,402 DG pairs with co-occurrence within abstracts, encompassing 8894 diseases and 18,393 genes.In addition, we extracted a subset of PubMed articles as the most relevant literature for the study of diseasegene relationships via a bootstrapping iteration with pubmedKB annotations as features.Using leave-one-disease-out training, we predicted a bootstrap score for each DG.Then, the literature subset is formed by the articles associated with the top three genes for each disease, the top three diseases for each gene, and the top 15 K DGs.The resulting subset contains 1,745,538 articles, for which we applied LORE and curated 11,285,095 relations.</p>
<p>The expert database part was derived from ClinVar [4], which provides gene-disease relationship data.As ClinVar uses OMIM (Online Mendelian Inheritance in Man) [6] numbers for disease indexing, we applied UMLS (Unified Medical Language System) [42], a vocabulary alignment dataset maintained by NLM, to map OMIM numbers to MeSH IDs.This process resulted in 4311 known pathogenic DGs, spanning 3175 distinct diseases and 2416 distinct genes.</p>
<p>The final PMKB-CV dataset was created by including those diseases that have both pubmedKB co-occurrence DGs and Clin-Var known pathogenic DGs.Statistics of the resulting dataset are shown in Fig. 6a.</p>
<p>Key Points</p>
<p>• We present a scalable framework that achieves 90% mean AP in identifying pathogenic gene associations across 2097 diseases, demonstrating remarkable accuracy in automated literature interpretation while effectively mitigating LLM hallucination risks.• Our analysis of literature-based semantic embeddings revealed a consistent directional pattern in how pathogenic genes are represented across different diseases.While both pathogenic and non-pathogenic disease-gene pairs cluster similarly in the embedding space, we discovered a distinct semantic f low that indicates pathogenicity.This pattern could help automate the identification of disease-causing genes from scientific literature.• The framework provides a reproducible methodology for leveraging LLMs in biomedical literature analysis, offering a valuable tool for researchers and clinicians in understanding disease mechanisms and identifying potential therapeutic targets.</p>
<p>Figure 2 .
2
Figure 2. Annotating articles with LLM-ORE (open relation extraction).Text-continuation prompts are used to make LLM write down its understanding of an article in the form of atomic factual statements.The ORE task is crafted to extract concise relations between entities at the article level.For example, 'Martin dreamed about fishing' requires comprehension and rewriting of several sentences.Also, common unwanted behaviors are avoided by providing examples of bad relations.We applied the task-agnostic prompt to extract an open set of diverse and comprehensive entity relationships for academic literature.</p>
<p>Figure 3 .
3
Figure 3. Curated key semantics.We created a taxonomy of four main classes, 15 subclasses, and 105 key semantics about disease-gene pathogenicity.With their corresponding linguistic lemmas, tags are added to relations automatically.Here, two key semantics and sample relations are shown for every subclass.</p>
<p>Figure 4 .
4
Figure 4. LLM-EMB literature-semantic embedding visualization.(a-d) Visualization with UMAP to analyze the latent pathogenicity structure within the literature-semantic embedding.Points are colored by the number of papers (#paper) (a), ClinVar pathogenicity labels (b), graded ML-ranker prediction (c), and binary ML-ranker prediction (d).The sparse ClinVar-curated red pathogenic DGs are seen clustering toward a subspace, captured well by ML-ranker, which also provides a smooth landscape of graded predictions for uncurated DGs.(e-g) Visualization with linear axes calculated using PCA (e), ridge (g), and their combination (f).A point is colored red if it is a known pathogenic DG in ClinVar, and DGs with unknown pathogenicity are colored gray.A latent structure of two manifolds-a gray ball of nonpathogenicity and a curved arm of transition from nonpathogenicity to pathogenicity-resides in the semantic space.(h-j) PCA visualization colored by distributions of literature semantics 'associate' (h), 'mutation' (i), and 'cause' (j).The connection between the smooth semantics distribution and the sparsely-curated ClinVar pathogenicity distribution can be seen.</p>
<p>Figure 5 .
5
Figure 5. Pathogenic f low in the literature-semantic space.(a) Suppose in the semantic embedding, nine DGs from three different diseases reside on a curve.(b) For the DG point-wise objective, the disease group information is not used, and absolute zero-one labels are the prediction target.As a result, a non-linear function along the curve must be learned.(c) For the disease-wise f low objective, DGs are grouped by disease, and relative f low directions are the prediction target.Because of the cross-disease consistency of the f low, a linear function along the curve will be learned to rank DGs for every disease perfectly.(d) Visualization of the actual pathogenic f low.A smooth, cross-disease consistent field of pathogenic f low is seen residing in the literature-semantic space.</p>
<p>Figure 6 .
6
Figure 6.PMKB-CV dataset and ranking performance.(a) Statistics of the PMKB-CV dataset.For 2097 diseases, the literature semantics framework has a 200× prediction scope against curated DGs.(b) Mean average precision (MAP) of the ranking performance and the ClinVar coverage of different methods.GPT-4o does not perform well.In comparison, LLM-EMB linear regression alone achieves 87.7% performance, whereas the full-f ledged MLranker provides higher performance at 90.0% or higher coverage at 94.8%.(c) Dispersion of ranking performance across diseases for each method and the p-values of distribution differences by one-sided Wilcoxon signed-rank test.LLM-ranker significantly outperformed baseline methods.(d) Performance across diseases of different scopes.The naive paper counting method encountered difficulties while ranking DGs for diseases that co-occurred with many genes in PMKB, but our semantic embedding approach remained robust.(e) Extending the scope of LORE using the public llama-8B model.Replacing GPT-3.5 with llama-8B resulted in marginal performance loss.The smaller model was further applied to all 3.9 million papers with DG co-occurrence.The final extracted relations covered 91.3% of ClinVar DGs and achieved 81.6% ranking performance.(f) Top-ranked DGs, seven out of 586 in PMKB, for Tourette syndrome accompanied by their literature relation evidence.</p>
<p>d∈De 1 ,e 2
2
emb d; m × | d | d∈De 1 ,e 2 | d | where e 1 and e 2 are the target entities, D e1,e2 is the set of all sub-documents, usually just one full document, containing the relations between e 1 and e 2 , and m is the LLM model.In this work, we employed the text-embedding-3-large from OpenAI for m, and the length of each sub-document | d | is its number of tokens according to m.</p>
<p>loss.L DG1,DG2 = − log Pr (P(DG1) &gt; P(DG2)) = log 1 + e −σ (sDG1−sDG2)</p>
<p>AcknowledgementsThis manuscript was edited by Wallace Academic Editing.The authors also thank Dau-Ming Niu and Yun-Ru Chen at the Taipei Veterans General Hospital in Taiwan for their support (NSTC 113-2634-F-A49-003).Code availabilityThe code supporting the conclusions of this study is available on GitHub at https://github.com/ailabstw/LORE.Data availabilityThe PMKB-CV datasets supporting the findings of this study are available at https://doi.org/10.5281/zenodo.14607639.FundingThis work was supported in part by the Center for Advanced Computing and Imaging in Biomedicine (NTU-113 L900701) from The Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education in Taiwan.Author contributionsSupplementary dataSupplementary data are available at Briefings in Bioinformatics online.Conf lict of interest: None declared.
How user intelligence is improving PubMed. N Fiorini, R Leaman, D J Lipman, 10.1038/nbt.4267Nat Biotechnol. 362018</p>
<p>pubmedKB: an interactive web server for exploring biomedical entity relations in the biomedical literature. P H Li, T F Chen, J Y Yu, 10.1093/nar/gkac310Nucleic Acids Res. 502022</p>
<p>PubMed and beyond: biomedical literature search in the age of artificial intelligence. Q Jin, R Leaman, Z Lu, 10.1016/j.ebiom.2024.104988EBioMedicine. 1001049882024</p>
<p>ClinVar: public archive of interpretations of clinically relevant variants. M J Landrum, J M Lee, M Benson, 10.1093/nar/gkv1222Nucleic Acids Res. 442016</p>
<p>COSMIC: the catalogue of somatic mutations In cancer. J G Tate, S Bamford, H C Jubb, 10.1093/nar/gky1015Nucleic Acids Res. 472019</p>
<p>OMIM.org: leveraging knowledge across phenotype-gene relationships. J S Amberger, C A Bocchini, A F Scott, 10.1093/nar/gky1151Nucleic Acids Res. 472019</p>
<p>An evidence-based framework for evaluating pharmacogenomics knowledge for personalized medicine. M Whirl-Carrillo, R Huddart, L Gong, 10.1002/cpt.2350Clin Pharmacol Ther. 1102021</p>
<p>Biomedical language processing: what's beyond PubMed?. L Hunter, K B Cohen, 10.1016/j.molcel.2006.02.012Mol Cell. 212006</p>
<p>Manual curation is not sufficient for annotation of genomic databases. W A Baumgartner, Jr, K B Cohen, L M Fox, 10.1093/bioinformatics/btm229Bioinformatics. 232007</p>
<p>Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research. A Bravo, J Pinero, N Queralt-Rosinach, 10.1186/s12859-015-0472-9BMC Bioinform. 16552015</p>
<p>DisGeNET: a comprehensive platform integrating information on human diseaseassociated genes and variants. J Pinero, À Bravo, N Queralt-Rosinach, 10.1093/nar/gkw943Nucleic Acids Res. 452017</p>
<p>RENET: a deep learning approach for extracting gene-disease associations from literature. Y Wu, R Luo, Hcm Leung, 10.1007/978-3-030-17083-7_17Research in Computational Molecular Biology. 114672019</p>
<p>PGxCorpus, a manually annotated corpus for pharmacogenomics. J Legrand, R Gogdemir, C Bousquet, 10.1038/s41597-019-0342-9Sci Data. 732020</p>
<p>ACE2 expression is increased in the lungs of patients with comorbidities associated with severe COVID-19. Bgg Pinto, Aer Oliveira, Y Singh, 10.1093/infdis/jiaa332J Infect Dis. 2222020</p>
<p>Molecular and networklevel mechanisms explaining individual differences in autism spectrum disorder. A M Buch, P E Vértes, J Seidlitz, 10.1038/s41593-023-01259-xNat Neurosci. 262023</p>
<p>Graph embedding-based link prediction for literature-based discovery in Alzheimer's disease. Y Pu, D Beck, K Verspoor, 10.1016/j.jbi.2023.104464J Biomed Inform. 1451044642023</p>
<p>The STRING database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest. D Szklarczyk, R Kirsch, M Koutrouli, 10.1093/nar/gkac1000Nucleic Acids Res. 512023</p>
<p>DTMiner: identification of potential disease targets through biomedical literature mining. D Xu, M Zhang, Y Xie, 10.1093/bioinformatics/btw503201632Bioinformatics</p>
<p>A global network of biomedical relationships derived from text. B Percha, R B Altman, 10.1093/bioinformatics/bty114Bioinformatics. 342018</p>
<p>BioREx: improving biomedical relation extraction by leveraging heterogeneous datasets. P T Lai, C H Wei, L Luo, 10.1016/j.jbi.2023.104487J Biomed Inform. 1461044872023</p>
<p>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge. C H Wei, A Allot, P T Lai, 10.1093/nar/gkae235Nucleic Acids Res. 522024</p>
<p>Neural machine reading comprehension: methods and trends. S Liu, X Zhang, S Zhang, 10.3390/app9183698Appl Sci. 936982019</p>
<p>Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine. P Lee, S Bubeck, J Petro, 10.1056/NEJMsr2214184N Engl J Med. 3882023</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, 10.1038/s41586-023-06291-2Nature. 6202023</p>
<p>Assessing GPT-4 for cell type annotation in singlecell RNA-seq analysis. W Hou, Ji Z , 10.1038/s41592-024-02235-4Nat Methods. 212024</p>
<p>Retrieve, summarize, and Verify: how will ChatGPT affect information seeking from the medical literature?. Q Jin, R Leaman, Z Lu, 10.1681/ASN.0000000000000166J Am Soc Nephrol. 342023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, F Petroni, V Karpukhin, Adv Neural Inf Process Syst. 332020</p>
<p>Retrieval-augmented generation for large language models: a survey. Y Gao, Y Xiong, X Gao, 10.48550/arXiv.2312.10997arXiv:2312.109972023arXiv Preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, Adv Neural Inf Process Syst. 352022</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Adv Neural Inf Process Syst. 332020</p>
<p>Effectiveness and efficiency of open relation extraction. F Mesquita, J Schmidek, D Barbosa, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational Linguistics2013</p>
<p>UMAP: uniform manifold approximation and projection. L Mcinnes, J Healy, N Saul, 10.21105/joss.00861J Open Source Softw. 38612018</p>
<p>Adapting boosting for information retrieval measures. Q Wu, Cjc Burges, K M Svore, 10.1007/s10791-009-9112-1Inf Retr. 132009</p>
<p>The impact of large language models on scientific discovery: a preliminary study using. M Research Ai4science, Azure Quantum, M , 10.48550/arXiv.2311.07361arXiv:2311.073612023GPT-4. arXiv Preprint</p>
<p>Benchmarking large language models for news summarization. T Zhang, F Ladhak, E Durmus, 10.1162/tacl_a_00632Trans Assoc Comput Linguist. 122024</p>
<p>G Team, T Mesnard, C Hardin, 10.48550/arXiv.2403.08295arXiv:2403.08295open models based on Gemini research and technology. 2024arXiv Preprint</p>
<p>QLoRA: efficient Finetuning of quantized LLMs. T Dettmers, A Pagnoni, A Holtzman, Adv Neural Inf Process Syst. 362023</p>
<p>MEDITRON-70B: scaling medical Pretraining for large language models. Z Chen, A H Cano, A Romanou, 10.48550/arXiv.2311.16079arXiv:2311.160792023arXiv Preprint</p>
<p>Learning to rank using gradient descent. C Burges, T Shaked, E Renshaw, Proceedings of the 22nd international conference on Machine learning -ICML '05. the 22nd international conference on Machine learning -ICML '05New York, NY, USAAssociation for Computing Machinery2005</p>
<p>Cumulated gain-based evaluation of IR techniques. K Järvelin, J Kekäläinen, 10.1145/582415.582418ACM Trans Inf Syst. 202002</p>
<p>Learning to rank with nonsmooth cost functions. C Burges, R Ragno, Q Le, 10.7551/mitpress/7503.003.0029Adv Neural Inf Process Syst. 192006</p>
<p>The unified medical language system (UMLS): integrating biomedical terminology. O Bodenreider, 10.1093/nar/gkh061Nucleic Acids Res. 322004</p>
<p>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup. Author The, 10.1093/bib/bbaf070Briefings in Bioinformatics. 26120252025Oxford University PressThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License</p>
<p>. Problem Solving Protocol. </p>            </div>
        </div>

    </div>
</body>
</html>
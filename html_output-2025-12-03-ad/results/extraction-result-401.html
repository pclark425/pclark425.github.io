<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-401 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-401</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-401</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-260063157</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.17591v1.pdf" target="_blank">Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training</a></p>
                <p><strong>Paper Abstract:</strong> Feature attribution methods highlight the important input tokens as explanations to model predictions, which have been widely applied to deep neural networks towards trustworthy AI. However, recent works show that explanations provided by these methods face challenges of being faithful and robust. In this paper, we propose a method with Robustness improvement and Explanation Guided training towards more faithful EXplanations (REGEX) for text classification. First, we improve model robustness by input gradient regularization technique and virtual adversarial training. Secondly, we use salient ranking to mask noisy tokens and maximize the similarity between model attention and feature attribution, which can be seen as a self-training procedure without importing other external information. We conduct extensive experiments on six datasets with five attribution methods, and also evaluate the faithfulness in the out-of-domain setting. The results show that REGEX improves fidelity metrics of explanations in all settings and further achieves consistent gains based on two randomization tests. Moreover, we show that using highlight explanations produced by REGEX to train select-then-predict models results in comparable task performance to the end-to-end method.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-401",
    "paper_id": "paper-260063157",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00471175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training
29 Dec 2023</p>
<p>Dongfang Li 
Harbin Institute of Technology (Shenzhen)
ShenzhenChina</p>
<p>Shan He 
Harbin Institute of Technology (Shenzhen)
ShenzhenChina</p>
<p>Baotian Hu 
Harbin Institute of Technology (Shenzhen)
ShenzhenChina</p>
<p>Qingcai Chen 
Harbin Institute of Technology (Shenzhen)
ShenzhenChina</p>
<p>Peng Cheng Laboratory
ShenzhenChina</p>
<p>Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training
29 Dec 2023DE894DDF2EECC5A43D5499C525958A30arXiv:2312.17591v1[cs.CL]
Feature attribution methods highlight the important input tokens as explanations to model predictions, which have been widely applied to deep neural networks towards trustworthy AI.However, recent works show that explanations provided by these methods face challenges of being faithful and robust.In this paper, we propose a method with Robustness improvement and Explanation Guided training towards more faithful EXplanations (REGEX) for text classification.First, we improve model robustness by input gradient regularization technique and virtual adversarial training.Secondly, we use salient ranking to mask noisy tokens and maximize the similarity between model attention and feature attribution, which can be seen as a self-training procedure without importing other external information.We conduct extensive experiments on six datasets with five attribution methods, and also evaluate the faithfulness in the out-of-domain setting.The results show that REGEX improves fidelity metrics of explanations in all settings and further achieves consistent gains based on two randomization tests.Moreover, we show that using highlight explanations produced by REGEX to train selectthen-predict models results in comparable task performance to the end-to-end method.</p>
<p>Introduction</p>
<p>As the broad adoption of Pre-trained Language Models (PLMs) requires humans to trust their output, we need to understand the rationale behind the output and even ask questions regarding how the model comes to its decision (Lipton, 2018).Recently, explanation methods for interpreting why a model makes certain decisions are proposed and become more crucial.For example, feature attribution methods assign scores to tokens and highlight the important ones as explanations (Sundararajan et al., 2017;Jain et al., 2020;DeYoung et al., 2020).</p>
<p>However, recent studies show that these explanations face challenges of being faithful and ro-Figure 1: Visualization of positive and negative highlights produced by post-hoc explanation methods (e.g., feature attribution).However, these explanations suffer from unfaithfulness problems (e.g., same model framework A and A' with different attributions) and can be further fooled by adversarial manipulation without changing model output (Ghorbani et al., 2019) (see §4.4).</p>
<p>bust (Yeh et al., 2019;Sinha et al., 2021;Ivankay et al., 2022), illustrated in Figure 1.The faithfulness means the explanation accurately represents the reasoning behind model predictions (Jacovi and Goldberg, 2020).Though some works are proposed to use higher-order gradient information (Smilkov et al., 2017), by incorporating game-theoretic notions (Hsieh et al., 2021) and learning from priors (Chrysostomou and Aletras, 2021a), how to improve the faithfulness of highlight explanations remains an open research problem.Besides, the explanation should be stable between functionally equivalent models trained from different initializations (Zafar et al., 2021).Intuitively, the potential causes of these challenges could be (i) the model is not robust and mostly leads to unfaithful and fragile explanations (Alvarez-Melis and Jaakkola, 2018;Li et al., 2022) and (ii) those explanation methods themselves also lack robustness to imperceptible perturbations of the input (Ghorbani et al., 2019); hence we need to develop better explanation methods.In this paper, we focus on the former and argue that there are connections between model robustness and explainability; any progress in one part may represent progress in both.</p>
<p>To this end, we propose a method with Robustness improvement and Explanation Guided training to improve the faithfulness of EXplanations (REGEX) while preserving the task performance for text classification.First, we apply the input gradient regularization technique and virtual adversarial training to improve model robustness.While previous works found that these mechanisms can improve the adversarial robustness and interpretability of deep neural networks (Ross and Doshi-Velez, 2018;Li et al., 2022), to the best of our knowledge, the faithfulness of model explanations by applying them has not been explored.Secondly, our method leverages token attributions aggregated by the explanation method, which provides a local linear approximation of the model's behaviour (Baehrens et al., 2010).We mask input tokens with low feature attribution scores to generate perturbed text and then maximize the similarity between new attention and attribution scores.Furthermore, we minimize the Kullback-Leibler (KL) divergence between model attention of original input and attributions.The main idea is to allow attention distribution of the model to learn from input importance during training to reduce the effect of noisy information.</p>
<p>To verify the effectiveness of REGEX, we consider a variety of classification tasks across six datasets with five attribution methods.Additionally, we conduct extensive empirical studies to examine the faithfulness of five feature attribution approaches in out-of-domain settings.The results show that REGEX improves the faithfulness of the highlight explanations measured by sufficiency and comprehensiveness (DeYoung et al., 2020) in all settings while outperforming or performing comparably to the baseline, and further achieves consistent gains based on two randomization tests.Moreover, we show that using the explanations output from REGEX to train select-then-predict models results in comparable task performance to the endto-end method, where the former trains an independent classifier using only the rationales extracted by the pre-trained extractor (Jain et al., 2020).Considering neural network models may be the primary source of fragile explanations (Ju et al., 2022;Tang et al., 2022), our work can be seen as a step towards understanding the connection between explainability and robustness -the desiderata in trustworthy AI.The main contributions of this paper can be summarized as:</p>
<p>• We explore how to improve the faithfulness of highlight explanations generated by feature attributions in text classification tasks.</p>
<p>• We propose an explanation guided training mechanism towards faithful attributions, which encourages the model to learn from input importance during training to reduce the effect of noisy tokens.(Hendrycks et al., 2020;Wang et al., 2021).However, as the debug tools for black-box models, explanation methods also lack robustness to imperceptible and targeted perturbations of the input (Heo et al., 2019;Camburu et al., 2019;Meister et al., 2021;Hsieh et al., 2021).While significantly different explanations are provided for similar models (Zafar et al., 2021), how to elicit more reliable explanations is a promising direction towards interpretation robustness.Different from Camburu et al. (2020) that addresses the inconsistent phenomenon of explanations, we investigate the connection between model robustness and faithfulness of the explanations.</p>
<p>Explanation Faithfulness</p>
<p>The faithfulness of explanations is important for NLP tasks, especially when humans refer to model decisions (Kindermans et al., 2017;Girardi et al., 2018).Jacovi and Goldberg (2020) first propose to evaluate the faithfulness of Natural Language Processing (NLP) Figure 2: The overall framework of proposed REGEX method.REGEX consists of two components for robustness improvement and explanations guided training respectively.For latter, we iteratively mask input tokens with low attribution scores and then minimize the KL divergence between attention of masked input and feature attributions.</p>
<p>methods by separating the two definitions between faithfulness and plausibility and provide guidelines on how evaluation of explanations methods should be conducted.Recently, some works have focused on faithfulness measurements of NLP model explanations and improve the faithfulness of specific explanations (Wiegreffe et al., 2021;Yin et al., 2021;Chrysostomou and Aletras, 2021b;Bastings et al., 2022).Among them, Ding and Koehn (2021) propose two specific consistency tests intending to measure if the post-hoc explanations remain consistent with similar models.</p>
<p>Incorporate Explanations into Learning While most previous explanation methods have been developed for explaining deep neural networks, some works explore the potential to leverage these explanations to help build better models (Liu and Avci, 2019;Rieger et al., 2020;Jayaram and Allaway, 2021;Ju et al., 2021;Bhat et al., 2021;Han and Tsvetkov, 2021;Ismail et al., 2021;Chrysostomou and Aletras, 2021a;Stacey et al., 2022;Ye and Durrett, 2022).For example, Hase and Bansal (2021) propose a framework to understand the role of explanations in learning, and find that explanations are suitably used in a retrieval-based modeling approach.Similarly, Adebayo et al. (2022) investigate whether post-hoc explanations effectively detect model reliance on spurious training signals, but the answer seems to be negative.While effectively incorporating explanations remains an open problem, we focus on using model explanations in a self-training way to improve its faithfulness.</p>
<p>Method</p>
<p>Problem Formulation</p>
<p>First, we consider the setting of multi-label text classification problem with n input examples
{(x i , y i )} n i=1 .
The input space embedded into vectors is x ⊆ R l×d and the output space is Y.A neural classifier is f θ : X → Y where f θ (x) parameterized by θ which denotes the output class for one example x = (x 1 , • • • , x l ) ∈ X , where l represents the length of the sequence.The optimization of the network is to minimize the cross-entropy loss L over the training set as follows:
L classif y = − n i=1 log p θ (y i |x i ).(1)
Then, given an input
x i = (x 1 , • • • , x l )
and its particular prediction f θ (x i ) = y i , the goal of feature attribution is to assign each token with a normalized score that then can be used to extract a compact set of relevant sub-sequences with respect to the prediction.Formally, an attribution of the prediction at input x i is a vector
a i = (a i1 , • • • , a il )
and a ij is defined as the attribution of x ij .After that, we denote the set of extracted tokens (i.e., highlight explanations or rationales) provided by taking top-k values from x i as r i , and use r i = x i \ r i , as the complementary set of r i to denote the set of irrelevant tokens.</p>
<p>Robustness Improvement</p>
<p>Adversarial attacks are inputs that are intentionally constructed to mislead neural networks (Szegedy et al., 2013;Goodfellow et al., 2015).Given the f θ and an input x ∈ X with the label y ∈ Y, an adversarial example x adv satisfies
x adv = x + ϵ, f (x) = y ∧ f (x adv ) ̸ = y (2)
where ϵ is the worst-case perturbation.Several defense methods have been proposed to increase the robustness of deep neural networks to adversarial attacks.We adopt two popular methods: virtual adversarial training (Miyato et al., 2015) which lever-ages a regularization loss to promote the smoothness of the model distribution, and input gradient regularization (Ross and Doshi-Velez, 2018) which regularizes the gradient of the cross-entropy loss.</p>
<p>Note that the methods used to improve the robustness are not limited to these techniques.As shown in Figure 2, we aim to improve the robustness of deep neural networks intrinsically.Instead of adopting adversarial training objective, we follow Jiang et al. (2019) to regularize the standard objective using virtual adversarial training (Miyato et al., 2018):
L at (x, y, θ) = max δ l(f (x + δ; θ), f (x; θ)). (3)
The goal of this approach is the enhancement of label smoothness in the embedding neighborhood.Specially, we run additional projected gradient steps to find the perturbation δ with violation of local smoothness to maximize the adversarial loss.On the other hand, input gradient regularization trains neural networks by minimizing not just the "energy" of the network but the rate of change of that energy with respect to the input features (Drucker and LeCun, 1992).The goal of this approach is to ensure that if any input changes slightly, the KL divergence between the predictions and the labels will not change significantly.Formally, it takes the original loss term and penalizes the ℓ 2 norm of its gradient and parameters:
L gr (x, y, θ) = ∥ ∂ ∂x L(x, y, θ)∥ 2 + ∥θ∥ 2 . (4)
It can also be interpreted as applying a particular projection to the Jacobian of the logits and regularizing it (Ross and Doshi-Velez, 2018).</p>
<p>Explanation Guided Training</p>
<p>If post-hoc explanations faithfully quantify the model predictions, the irrelevant tokens should have low feature attribution scores (Ismail et al., 2021).Based on this intuition, we leverage the existing explanations to guide the model for reducing feature attribution scores of irrelevant tokens without sacrificing the model performance.Concretely, we propose the Explanation Guided Training (EGT) mechanism.Instead of using the saliency method (i.e., gradient of the target class with respect to the input) (Simonyan et al., 2014), we apply the Integrated Gradients (IG) method (Sundararajan et al., 2017) that is more faithful via axiomatic proofs to calculate the token importance.We do not assume the IG is totally faithful, and we also experiment with other attribution methods in §5.1.It integrates the gradient along the path from an uninformative baseline to the original input.This baseline input is used to make a high-entropy prediction that represents uncertainty.As it takes a straight path between baseline and input, it requires computing gradients several times.The motivation for using path integral rather than vanilla gradient is that the gradient might have been saturated around the input while the former can alleviate this problem.Formally, given an input x and baseline x ′ , the integrated gradient along the i th dimension is defined as follows:
IGi(x) ::= (xi − x ′ i) 1 α=0 ∂f θ (x ′ +α×(x−x ′ )) ∂x i dα,(5)
where ∂f θ (x) ∂x i represents the gradient of f along the i th dimension at x which is the concatenated embedding of the input sequence, and the attribution of each token is the sum of the attributions of its embedding.Note that we attribute the output of the model with ground-truth labels during training.We also test other feature attribution methods in §5.1.</p>
<p>After calculating the token's importance score by ℓ 2 aggregation over embedding dimensions, we sort tokens of x based on these scores and mask the bottom K% words according to that sorting.We define the sorting function as s(•) and the masking function as m(•).For example, s i (x) is the i th smallest element in x, and m k (s(x), x) replaces all x i ∈ {s i (x)} ceil(1,Kl) e=0 with a mask distribution, i.e., m k (s(x), x) removes the K% lowest features from x based on the order provided by s(x).During training, we generate a new input x for each example x by masking the features with low attribution scores as follows:
x = m k (s IG (x), x).(6)
x is then passed through the network which results in an attention scores att( x).Following Jain et al. (2020), the attention scores are taken as the mean self-attention weights induced from the first token index to all other indices.Then we maximize the similarity between att (x) and att( x) to ensure that the model produces similar output probability distributions over labels for both masked and unmasked inputs.The optimization objective for the EGT is:
L kl (x, y, θ) = DKL (att(x); IG(x)) + DKL (att( x); IG(x)) ,(7)
where D KL is the KL divergence function between two distributions.The motivation behind two KL divergence terms is to encourage the model to focus on high salient words and ignore low salient words during training, and generate similar outputs for the original input x and masked input x, which can be seen as a special adversarial example.On the other hand, as the calculation of the mask input is batchwise, the model should learn to assign low gradient values to irrelevant tokens for the predicted label in an iterative way.</p>
<p>Training</p>
<p>We define the final weighted loss as follows,
L = λ1L classif y + λ2Lgr + λ3Lat + λ4L kl ,(8)
where</p>
<p>Erasure-based Faithfulness Evaluation</p>
<p>To evaluate post-hoc explanations, we adopt sufficiency that measures the degree to which the highlight explanation is adequate for a model to make predictions, and comprehensiveness that measures the influence of explanations to predictions (DeYoung et al., 2020).These two metrics are usually used to evaluate faithfulness as it does not require re-training and the main idea is to estimate the effect of changing parts of inputs on model output.Let p θ (y j |x i ) be the output probability of the j-th class for the i-th example, and rationale r i extracted according to attribution scores.Formally, the sufficiency we used is as follows:
S(xi, y j , ri) = 1 − max(0, p θ (y j |xi) − p θ (y j |ri)), (9) sufficiency(xi, y j , ri) = S(xi, y j , ri) − S(xi, y j , 0) 1 − S(xi, y j , 0) ,(10)
where higher sufficiency values are better as we normalize and reverse it between 0 and 1, and S(x i , y j , 0) is the sufficiency of the input where no token is erased.Similarly, we define the comprehensiveness as follows:
C(xi, y j , ri) = max(0, p θ (y j |xi) − p θ (y j |ri)), (11) comprehensiveness(xi, y j , ri) = C(xi, y j , ri) 1 − S(xi, y j , 0) ,(12)
where higher comprehensiveness values are better.As choosing the appropriate rationale length is dataset dependent, we use the Area Over the Perturbation Curve (AOPC) metrics for sufficiency and comprehensiveness.It defines bins of tokens to be erased and calculates the average measures across bins.Here, we keep the top 1%, 5%, 10%, 20%, 50% tokens into bins in the order of decreasing attribution scores.</p>
<p>Experiments</p>
<p>We conduct the experiments in six datasets under the in-domain/out-of-the-domain settings: SST (Socher et al., 2013), IMDB (Maas et al., 2011), Yelp (Zhang et al., 2015), and AmazDigiMu/AmazPantry/AmazInstr (Ni et al., 2019) (See details in Appendix A).The baseline is a text classification model fine-tuned on the training set while the same pre-trained language model is applied to REGEX.In other words, the baseline is optimized by Eqn. 1 without robustness improvement and explanation guided training mechanisms.</p>
<p>Post-hoc Explanation Methods</p>
<p>We consider five feature attribution methods and a random attribution method:
Random (RAND) (Chrysostomou</p>
<p>Post-hoc Explanations Faithfulness</p>
<p>We conduct experiments on the faithfulness metrics (i.e., normalized sufficiency and normalized comprehensive) to compare the fidelity of different post-hoc explanation methods between the baseline and REGEX models.We extract rationale r from a model by selecting the top-k most important tokens measured by these post-hoc explanation methods.Following Chrysostomou and Aletras (2022), we also evaluate explanation faithfulness in out-of-domain settings without retraining models (i.e., zero-shot), and we follow their settings with six dataset pairs and a random attribution baseline.</p>
<p>Especially the model has first trained on the source datasets, and then we evaluate its performance on the test set of the target datasets.</p>
<p>As shown in Table 1, REGEX improves the explanation faithfulness with all five attribution methods by a large gap under most in-and outof-domain settings.Among them, scaled attention and DeepLift perform better than others.For example, REGEX surpasses the baseline in the sufficiency metric for the explanation extracted by DeepLift under all scenarios, while the comprehensiveness decreases when the model is trained in the AmazDigiMu dataset and tested in the AmazInstr dataset.It shows that REGEX improves the fidelity of post-hoc explanations measured by sufficiency and comprehensiveness.Nevertheless, we observe a decrease in the comprehensiveness metrics for attention and IG on specific datasets.For example, considering the uncertainty of attention as an interpretable method (Serrano and Smith, 2019), the fidelity metrics of attention attribution are inferior to the baseline on all three Amazon Reviews datasets.</p>
<p>Overall, feature attribution approaches outperform random attributions of in-and out-of-domain settings in most cases.Moreover, results show that post-hoc explanation sufficiency and comprehensiveness are higher in in-domain test sets than in out-of-domain except for the Yelp dataset.On the other hand, as shown in Table 2, REGEX improves performance or achieves similar task performance to the baseline on most out-of-domain datasets.</p>
<p>Quantitative Evaluation by FRESH Method</p>
<p>We further compare the average macro F1 of the FRESH classifier (Jain et al., 2020) across five random seeds in the in-and out-of-domain settings.In short, FRESH is a select-then-predict framework, and the general process is that an extractor is first trained where the labels are induced by arbitrary feature importance scores over token inputs; then, an independent classifier is trained exclusively on rationales provided by the extractor which are assumed to be inherently faithful.Here, rationales extracted by the top-k most important tokens are used as input to the classifier for training and test.</p>
<p>As shown in Table 2, the best two methods are DeepLift and scaled attention, which achieve a similar performance as the original text input model in the in-and out-of-domain settings and is consistent with the faithfulness evaluation.For example, the FRESH classifier applying the DeepLift attribution method is higher than the baseline and outperforms the model with the full text input (97.1 vs. 96.9) on the Yelp dataset.It also illustrates that the performance depends on the choice of the feature attribution method.</p>
<p>Explanation Robustness</p>
<p>Following Zafar et al. ( 2021  attributions generated by the post-hoc explanation method.We use Jaccard similarity for explanations extracted by top 25% important tokens using the scaled attention method.If the two attributions are more similar, the Jaccard metric is higher.We compare the REGEX and baseline by comparing two identical models trained from different initializations.The #Untrained is a untrained model which randomly initialize the fully connected layers attached on top of the Transformer encoders.As shown in Table 3, REGEX achieves an improved performance than baseline.For example, REGEX gets 0.56 while baseline gets 0.36 for Init#3 and Init#4.As we expected, the similarity between explanations of the trained and untrained models is low, e.g., 0.12 between Init#4 and #Untrained.It shows that improving faithfulness of explanations can strengthen interpretation robustness.However, the overall results between the two feature attributions are still low as 50% of similarity comparisons are less than 0.5.</p>
<p>Analysis</p>
<p>Ablation Study</p>
<p>We perform ablation studies to explore the effect of robustness improvement and explanation guided training for faithfulness evaluations shown in Table 4 (all results in The performance of the attention method varies more across different hyper-parameters.In Figure 3, we compare different λ 4 in Eqn. 8 and observe that all methods achieve best sufficiency at 0.01 and best comprehensiveness at 0.001.In Figure 4, we compare different mask ratios in §3.3 and find that the mask ratio between 0.15 and 0.2 is useful as larger values can bring noise.</p>
<p>The choice of aggregation method and feature attribution method in §3.3 has a large effect on the faithfulness evaluation.We find that for most attribution methods, ℓ 2 aggregation has higher fidelity performance.For example, Saliency with ℓ 2 aggregation is better than Saliency with mean aggregation with more sufficiency improvement (0.70 vs. 0.55).Though there is no best method for explanation guided training, gradient-based methods (e.g., IG, 0.71) may be good choices in line with Atanasova et al. (2020).</p>
<p>…,is the fact that the wonderful RAYMOND MASSEY is relegated to the last twenty or so minutes in the trial scene.… David NIVEN and KIM HUNTER are wonderfully cast as the young lovers….French accented MARIUS GORING is a delight (he even gets in a remark about Technicolor) as the heavenly messenger sent to reclaim Niven when his wartime death goes unreported due to an oversight.Seeing this tonight on TCM for the first time in twenty or so years, I think it's a supreme example of what a wonderful year 1946 was for films.The Technicolor photography, somewhat subdued and not garish at all, is excellent and the way it shifts into B&amp;W for the heavenly sequences is done with great imagination and effectiveness….</p>
<p>Label</p>
<p>Qualitative Analysis</p>
<p>Table 5 presents two randomly-chosen examples of the test set of the IMDB dataset.For example, the top-k important tokens returned by REGEX are wonderfully, wonderful, wonderful, excellent and great in the first example.We observe that these highlight explanations seem intuitive to humans and reasonably plausible.Though faithfulness and plausibility are not necessarily correlative (Jacovi and Goldberg, 2020), we find that the highlights extracted by REGEX contain more sentiment-related words, which should be helpful for review-based text classification.</p>
<p>Conclusion</p>
<p>We explore whether the fidelity of explanations can be further optimized and propose an explanation guided training mechanism.Extensive empirical studies are conducted on six datasets in both in-and out-of-domain settings.Results show that our method REGEX improves both fidelity metrics and performance of select-then-predict models.The analysis of explanation robustness further shows that the consistency of explanations has been improved.The observation suggests that considering model robustness yields more faithful explanations.In the future, we would like to investigate more PLMs architectures and faithfulness metrics under the standard evaluation protocol.</p>
<p>Possible limitations include the limited PLM architecture/size (although we include additional results with RoBERTa in the Appendix D) and faithfulness evaluation metrics are not necessarily comprehensive.And we only focus on text classification tasks.</p>
<p>As a result, we do not investigate other language classification (e.g., natural language inference and question answering) and text generation tasks.If we can intrinsically know or derive the golden faithful explanations (Bastings et al., 2022;Lindner et al., 2023), the exploration of model robustness and explainability will be alternatively investigated for revealing the internal reasoning processes.And future work could include human study (e.g., evaluation about whether explanations help users choose the more robust of different models) and improve the robustness by more diverse ways (e.g., model distillation and data augmentation).</p>
<p>Our findings are also in line with Tang et al. (2022) and Logic Trap 3 (Ju et al., 2022) which claims the model reasoning process is changed rather than the attribution method is unreliable.Different from this two works -output probability perturbation or changing information flow, we view our results as complementary to their conclusion via sourcing the improvement of faithfulness.Although we show the link between robustness and faithfulness empirically, future work can strengthen the conclusions by discussion on a more conceptual and theoretical level.From a theoretical perspective, one possible reason is that the gradient of the model is more aligned with the normal direction to the close decision boundaries (Wang et al., 2022).In the future, we would like to analyze the relationship between robustness and explainability from geometric dimension.</p>
<p>Furthermore, we do not exhaustively experiment with all possible evaluation settings of interest even with the scale of our experiments.For example, saliency guided training methods (Ismail et al., 2021) could have been used as another baseline.We hope this work inspires more future work that develops more effective strategies to make explanations reliable and investigate how our findings translate to large language models, such as GPT-3 model family2 , as with the emergent capabilities of these models, fidelity to their explanations or rationale will have societal impacts on accountability of NLP systems.</p>
<p>A Dataset</p>
<p>We consider six datasets to evaluate explanations and the data statistics are as follows.</p>
<p>SST: The Stanford Sentiment Treebank (SST) dataset (Socher et al., 2013)  Yelp: The Yelp dataset (Zhang et al., 2015) includes highly polar movie reviews and is transformed to a binary classification task (positive/negative).The training set, development set, and test set consist of 476k, 84k, and 38k examples.AmazDigiMu/AmazPantry/AmazInstr: The amazon reviews dataset (Ni et al., 2019) is constructed by personalized justification from existing from Amazon review data.We choose the 3-class review and product metadata for three categories: Digital Music, Prime Pantry and Musical Instruments (Chrysostomou and Aletras, 2022).These examples are then divided into three subsets: AmazDigiMu (122k/21k/25k examples), AmazPantry (99k/17k/20k examples) and AmazInstr (16k/29k/3k examples).</p>
<p>B Experiment Settings</p>
<p>We use Spacy3 to pre-tokenize the sentence and apply the BERT-base model to encode text (Devlin et al., 2019).We use AdamW optimizer with batch sizes of 8, 16, 32, 64 for model training.The initial learning rate is 1 × 10 −5 for fine-tuning BERT parameters and 1 × 10 −4 for the classification layer.The maximum sequence length, the dropout rate, the gradient accumulation steps, the training epoch and the hidden size d are set to 256, 0.1, 10%, 10, 768 respectively.We clip the gradient norm within 1.0.The learning parameters are selected based on the best performance on the development set.Our model is trained with NVIDIA Tesla A100 40GB GPUs (PyTorch &amp; Huggingface/Transformers 4 &amp; Captum 5 ).Following Jiang et al. (2019), we set the perturbation size ϵ = 1 × 10 −5 , the step size η = 1 × 10 −3 , ascent iteration step C = 2 and the variance of normal distribution σ = 1 × 10 −5 .The weight parameters λ 1 , λ 2 , λ 3 , λ 4 are set to 1.0, 0.01, 0.5, 0.01 respectively.The mask ration K is set to 0.15.The number of steps used by the approximation method in IG is 50, and we use zero scalar corresponding to each input tensor as IG baselines.The parameters are selected based on the development set.For the baseline and FRESH model, we use the same transformer-based models as mentioned previously to encode tokens and we choose rationale length by following Chrysostomou and Aletras (2022).The model is trained for 10 epochs, and we keep the best models with respect to macro F1 scores on the development sets.</p>
<p>C Text Classification to Attacks</p>
<p>We conduct the behavioral testing with CHECKLIST (Ribeiro et al., 2020) and TextAttack (Morris et al., 2020)  the attack success rate which is used to evaluate the effectiveness of the attacks is 3.23%.</p>
<p>D UIT and DIT with Larger Pre-trained Language Model</p>
<p>To further verify the effect of model scale on the results, we conducted experiments on the robustness of explanations under the pre-trained language model RoBERTa (Liu et al., 2019), including UIT and DIT.The experimental results are shown in the Table 8.We have two findings: (1) the size of the model has a certain positive effect on the stability of explanations, with the Jaccard similarity improved under REGEX and Baseline, although the improvement is not significant.(2) REGEX can still improve performance under larger pre-trained models which further strengths our findings.</p>
<p>E Full Results</p>
<p>Table 7 presents the Full-text F1 of variants in ablation study.Table 9 lists the full results for FRESH (select-then-predict) models.Table 10 lists the full results of ablation study.From these results, we further found that sufficiency of the extracted explanations when using one robustness training method (either virtual adversarial training or input gradient regularization) is inferior to the sufficiency when using no robustness training.We speculate that there are several reasons: (1) the two mechanisms are related, i.e., removing one has a more significant impact than removing both simultaneously;</p>
<p>(2) the results have variance despite the adoption of the AOPC metric, not to mention that the sufficiency metrics suffer from out-of-distribution challenges; (3) these ablation experiments are on models trained on SST and tested on SST; future works could perform a more detailed ablation analysis on other datasets (such as in out-of-domain settings).</p>
<p>Table 1 :
1
.38) .68(.51) .48(.42) .71(.42) .49(.40) .49(.41) .22(.19) .56(.39) .41(.22) .52(.25) .43(.26).43(.26)IMDB.25(.31).54(.53) .45(.39) .46(.32) .40(.31) .40(.32) .19(.23) .75(.54) .66(.34) .61(.27) .58(.27) .58(.28) Yelp .24(.32) .51(.56) .38(.40) .45(.35) .35(.33) .36(.34) .22(.21) .70(.48) .57(.28) .59(.24) .48(.24) .47(.25) IMDB IMDB .34(.32) .82(.55) .51(.46) .80(.36) .54(.36) .53(.36) .17(.16) .71(.48) .39(.31) .62(.25) .31(.23) .32(.24) SST .30(.24) .72(.35) .42(.28) .68(.28) .46(.27) .45(.27) .21(.27) .59(.46) .28(.32) .51(.33) .32(.33) .33(.33) Yelp .32(.35) .81(.48) .53(.41) .79(.36) .48(.36) .47(.36) .20(.21) .71(.45) .42(.32) .64(.26) .33(.26) .34(.26) AmazDigiMu .56(.21) .82(.38) .58(.21) .82(.22) .60(.24) .59(.22) .12(.23) .48(.46) .16(.20) .46(.22) .15(.24) .15(.25) AmazPantry .56(.22) .81(.39) .58(.21) .81(.23) .59(.24) .58(.23) .13(.27) .50(.51) .16(.22) .47(.25) .16(.27) .17(.29) Normalized sufficiency and comprehensiveness in the in-and out-of-domain settings for five feature attribution approaches and a random attribution.REGEX vs. baseline (shown in brackets).For example, a value of .30(.38) represents the result of Normalized Sufficiency on the SST test set with the RAND method, .30means the score of our method, and .38 means the baseline.
Scaled Attention (α∇α) (Serrano and Smith,2019):Normalized attention scores α i scaled by the corresponding gradients ∇α i = ∂ ŷ ∂α i .InputXGrad (x∇x) (Shrikumar et al., 2016;Kindermans et al., 2016): The input attributionimportance is generated by multiplying the gradient∇x i = ∂ ∂x i with the input. ŷIntegrated Gradients (IG) (Sundararajan et al.,2017): See  §3.3 for details.
(Shrikumar et al., 2017)n importance is assigned at random.Attention (α)(Jain et al., 2020): Normalized attention scores are used to calculate token importance.DeepLift(Shrikumar et al., 2017): The difference between each neuron activation and a reference vector is used to rank words.</p>
<p>Table 2 :
2
Average macro F1 results of Full-text and FRESH models with a prescribed rationale length.REGEX vs. baseline (shown in brackets, averaged across 5 seeds).The reference performance (Full-text F1) is from the BERT-base model fine-tuned on the full text.Full results are in Appendix E. The bold numbers represent the results of the best FRESH model trained with rationales from REGEX model among five attribution methods.</p>
<p>), we test implementation invariance of feature attributions by Untrained Model Test (UIT) and Different Initialization Test (DIT).The UIT and DIT measure the consistency and calculate the Jaccard similarity between feature
Jaccard@25% Init#1 Init#2 Init#3 Init#4 #UntrainedInit#11.0.44(.33).54(.34).56(.34).28(.27)Init#2.44(.33)1.0.45(.44).41(.34).16(.17)Init#3.54(.34).45(.44)1.0.56(.36).22(.21)Init#4.56(.34).41(.34).56(.36)1.0.12(.16)#Untrained.28(.27).16(.17).22(.21).12(.16)1.0</p>
<p>Table 3 :
3
Jaccard@25% between the feature attributions (REGEX vs. baseline, here we use scaled attention) for models with same architecture, with same data, and same learning schedule, except for randomly initial parameters.</p>
<p>Table 10
10), and investigate the</p>
<p>Table 4 :
4
Ablation study with different aggregation methods and feature attribution methods in §3.3.</p>
<p>:</p>
<p>Positive Prediction: Positive Dataset: IMDB ID: Test 1364 …but pompous horror icon Christopher Lee squirming in the midst of it all (the gracefully-aged star has pathetically asserted a number of times in interviews that he hasn't appeared in horror-oriented fare since his last picture for Hammer Films back in 1976!).Anyway, this film should have borne the subtitle "Your Movie Is A Turd" being astoundingly inept in all departments (beginning with the allimportant werewolf make-up)!The plot (and dialogue) is not only terrible, but it has the limpest connection with Dante's film strangely enough, the author of the original novel Gary Brandner co-wrote this himself!Still, one of the undeniable highlights (er...low points) of the film is the pointless elliptical editing</p>
<p>Label: Negative Prediction: Negative Dataset: IMDB ID: Test 1373</p>
<p>Table 5 :
5
We randomly pick two examples from test set of IMDB dataset, and highlight the Top-k important tokens using DeepLift method (REGEX vs. Baseline).</p>
<p>Table 6 :
6
Attack results of REGEX and baseline by CHECKLIST attack recipe.
MethodsFull-text F1Saliency (Mean)87.81±3.64InputXGrad (Mean)91.21±0.23DeepLift (Mean)87.99±0.48IG (Mean)91.60±0.08Saliency (ℓ2)83.52±1.29InputXGrad (ℓ2)90.83±0.29DeepLift (ℓ2)87.62±0.53REGEX89.73±0.05w/o robustness improvement90.57±0.52w/o explanation guided training 85.19±2.80</p>
<p>Table 7 :
7
Macro F1 and standard deviations with different aggregation methods and feature attribution methods in §3.3.</p>
<p>Table 9 :
9
AmazDigiMu 67.87±0.462.53±0.967.52±1.048.30±2.248.30±2.2AmazDigiMu AmazInstr 60.95±0.1 49.98±0.860.92±0.5 39.02±0.239.02±0.2AmazPantry 60.05±0.346.27±0.959.01±1.038.83±1.038.83±1.0AmazPantry 67.83±1.059.62±0.867.99±1.6 50.33±1.250.33±1.2AmazPantry AmazDigiMu 58.49±0.851.48±1.058.40±0.5 42.71±0.842.71±0.8AmazInstr 64.91±0.5 54.92±1.7 65.55±1.043.31±0.943.31±0.9AmazInstr 69.52±0.7 63.06±0.6 70.73±0.247.47±1.047.47±1.0AmazInstr AmazDigiMu 58.59±0.851.64±0.458.93±0.5 43.68±0.7 43.68±0.7 AmazPantry 64.95±0.955.82±0.6 65.58±0.245.24±0.845.24±0.8Macro F1 and standard deviations of FRESH models with Top-k explanations.RED means REGEX outperforms the baseline.DeepLift x∇x IG RAND α∇α α DeepLift x∇x IG
TrainTestα∇ααDeepLiftx∇xIGSST88.88±0.7 83.00±0.3 87.31±0.5 77.84±0.5 77.84±0.5SSTIMDB86.27±0.2 65.32±1.9 81.18±0.6 53.22±0.6 53.22±0.6Yelp90.15±0.1 76.45±0.6 80.35±2.1 64.38±0.5 64.38±0.5IMDB88.88±0.3 79.16±0.2 87.60±0.2 59.14±1.0 59.14±1.0IMDBSST80.60±1.6 71.75±0.3 72.91±0.6 65.68±2.2 65.68±2.2Yelp90.37±0.5 72.71±1.0 86.51±0.4 70.54±0.9 70.54±0.9Yelp96.27±0.1 87.13±0.1 97.05±0.0 71.22±0.1 71.22±0.1YelpSST82.03±0.5 58.13±0.6 69.89±0.4 67.58±0.6 67.58±0.6IMDB83.68±0.4 51.51±0.4 79.10±1.2 47.99±1.8 47.99±1.8</p>
<p>Table 10 :
10
Full results of ablation study with different aggregation methods and feature attribution methods in §3.3.</p>
<p>We will publicly release the code, pre-trained models and all experimental setups.
https://beta.openai.com/playground
https://spacy.io/models/en
https://github.com/huggingface/transformers
https://captum.ai/</p>
<p>Post hoc explanations may be ineffective for detecting unknown spurious correlation. Julius Adebayo, Michael Muelly, Harold Abelson, Been Kim, Proc. of ICLR. of ICLR2022</p>
<p>David Alvarez, -Melis , Tommi S Jaakkola, arXiv:1806.08049On the robustness of interpretability methods. 2018arXiv preprint</p>
<p>A diagnostic study of explainability techniques for text classification. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein, Proc. of EMNLP. of EMNLP2020</p>
<p>How to explain individual classification decisions. David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus-Robert Müller, J. Mach. Learn. Res. 2010</p>
<p>A protocol for evaluating the faithfulness of input salience methods for text classification. Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, Katja Filippova, Proc. of EMNLP. of EMNLP2022will you find these shortcuts?</p>
<p>Self-training with few-shot rationalization: Teacher explanations aid student in few-shot NLU. Moorthy Meghana, Alessandro Bhat, Subhabrata Sordoni, Mukherjee, Proc. of EMNLP. of EMNLP2021</p>
<p>Oana-Maria Camburu, Eleonora Giunchiglia, Jakob Foerster, Thomas Lukasiewicz, Phil Blunsom, arXiv:1910.02065Can I trust the explainer? verifying post-hoc explanatory methods. 2019arXiv preprint</p>
<p>Make up your mind! adversarial generation of inconsistent natural language explanations. Oana-Maria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, Phil Blunsom, Proc. of ACL. of ACL2020</p>
<p>Enjoy the salience: Towards better transformer-based faithful explanations with word salience. George Chrysostomou, Nikolaos Aletras, Proc. of EMNLP. of EMNLP2021a</p>
<p>Improving the faithfulness of attention-based explanations with task-specific information for text classification. George Chrysostomou, Nikolaos Aletras, Proc. of ACL. of ACL2021b</p>
<p>An empirical study on explanations in out-of-domain settings. George Chrysostomou, Nikolaos Aletras, Proc. of ACL. of ACL2022</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proc. of NAACL. of NAACL2019</p>
<p>ERASER: A benchmark to evaluate rationalized NLP models. Jay Deyoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C Wallace, Proc. of ACL. of ACL2020</p>
<p>Evaluating saliency methods for neural language models. Shuoyang Ding, Philipp Koehn, Proc. of NAACL. of NAACL2021</p>
<p>Improving generalization performance using double backpropagation. Harris Drucker, Yann Lecun, IEEE Trans. Neural Networks. 1992</p>
<p>Interpretation of neural networks is fragile. Amirata Ghorbani, Abubakar Abid, James Y Zou, Proc. of AAAI. of AAAI2019</p>
<p>Patient risk assessment and warning symptom detection using deep attentionbased neural networks. Ivan Girardi, Pengfei Ji, An-Phi Nguyen, Nora Hollenstein, Adam Ivankay, Lorenz Kuhn, EMNLP workshop. 2018. 2018Chiara Marchiori, and Ce Zhang</p>
<p>Explaining and harnessing adversarial examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, Proc. of ICLR. of ICLR2015</p>
<p>Influence tuning: Demoting spurious correlations via instance attribution and instance-driven updates. Xiaochuang Han, Yulia Tsvetkov, Proc. of EMNLP Findings. of EMNLP Findings2021</p>
<p>When can models learn from explanations? A formal framework for understanding the roles of explanation data. Peter Hase, Mohit Bansal, arXiv:2102.022012021arXiv preprint</p>
<p>Pretrained transformers improve out-of-distribution robustness. Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song, Proc. of ACL. of ACL2020</p>
<p>Fooling neural network interpretations via adversarial model manipulation. Juyeon Heo, Sunghwan Joo, Taesup Moon, Proc. of NeurIPS. of NeurIPS2019</p>
<p>Evaluations and methods for explanation through robustness analysis. Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Kumar Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh, Proc. of ICLR. of ICLR2021</p>
<p>Improving deep learning interpretability by saliency guided training. Aya Abdelsalam Ismail, Héctor Corrada Bravo, Soheil Feizi, Proc. of NeurIPS. of NeurIPS2021</p>
<p>Fooling explanations in text classifiers. Adam Ivankay, Ivan Girardi, Chiara Marchiori, Pascal Frossard, Proc. of ICLR. of ICLR2022</p>
<p>Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?. Alon Jacovi, Yoav Goldberg, Proc. of ACL. of ACL2020</p>
<p>Human rationales as attribution priors for explainable stance detection. Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C Wallace, Proc. of ACL. Sahil Jayaram and Emily Allaway. of ACL. Sahil Jayaram and Emily Allaway2020. 2021Proc. of EMNLP</p>
<p>Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Tuo Zhao, arXiv:1911.034372019arXiv preprint</p>
<p>Enhancing multiple-choice machine reading comprehension by punishing illogical interpretations. Yiming Ju, Yuanzhe Zhang, Zhixing Tian, Kang Liu, Xiaohuan Cao, Wenting Zhao, Jinlong Li, Proc. of EMNLP. of EMNLPJun Zhao. 2021</p>
<p>Logic traps in evaluating attribution scores. Yiming Ju, Yuanzhe Zhang, Zhao Yang, Zhongtao Jiang, Kang Liu, Proc. of ACL. of ACLJun Zhao. 2022</p>
<p>Dumitru Erhan, and Been Kim. 2017. The (un)reliability of saliency methods. Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, T Kristof, Sven Schütt, Dähne, Explainable AI. 2019</p>
<p>Pieter-Jan Kindermans, Kristof Schütt, Klaus-Robert Müller, Sven Dähne, arXiv:1611.07270Investigating the influence of noise and distractors on the interpretation of neural networks. 2016arXiv preprint</p>
<p>Unifying model explainability and robustness for joint text classification and rationale extraction. Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, Yunan Zhang, Proc. of AAAI. of AAAI2022</p>
<p>Tracr: Compiled Transformers as a Laboratory for Interpretability. David Lindner, János Kramár, Matthew Rahtz, Thomas Mcgrath, Vladimir Mikulik, arXiv:2301.050622023arXiv preprint</p>
<p>The mythos of model interpretability. Zachary C Lipton, ACM Queue. 2018</p>
<p>Incorporating priors with feature attribution on text classification. Frederick Liu, Besim Avci, Proc. of ACL. of ACL2019</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Learning word vectors for sentiment analysis. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, Christopher Potts, Proc. of ACL. of ACL2011</p>
<p>Is sparse attention more interpretable?. Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell, Proc. of ACL. of ACL2021</p>
<p>Takeru Miyato, Masanori Shin Ichi Maeda, Ken Koyama, Shin Nakae, Ishii, arXiv:1507.00677Distributional smoothing with virtual adversarial training. 2015arXiv preprint</p>
<p>Virtual adversarial training: a regularization method for supervised and semisupervised learning. Takeru Miyato, Shin-Ichi Maeda, Masanori Koyama, Shin Ishii, 2018</p>
<p>Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, Yanjun Qi, Proc. of EMNLP: System Demonstrations. of EMNLP: System Demonstrations2020</p>
<p>Justifying recommendations using distantly-labeled reviews and fine-grained aspects. Jianmo Ni, Jiacheng Li, Julian J Mcauley, Proc. of EMNLP. of EMNLP2019</p>
<p>Beyond accuracy: Behavioral testing of NLP models with checklist. Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh, Proc. of ACL. of ACL2020</p>
<p>Interpretations are useful: Penalizing explanations to align neural networks with prior knowledge. Laura Rieger, Chandan Singh, W James Murdoch, Bin Yu, Proc. of ICML. of ICML2020</p>
<p>Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. Andrew Slavin, Ross , Finale Doshi-Velez, Proc. of AAAI. of AAAI2018</p>
<p>Is attention interpretable?. Sofia Serrano, Noah A Smith, Proc. of ACL. of ACL2019</p>
<p>Learning important features through propagating activation differences. Avanti Shrikumar, Peyton Greenside, Anshul Kundaje, Proc. of ICML. of ICML2017</p>
<p>Not just a black box: Learning important features through propagating activation differences. Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje, arXiv:1605.017132016arXiv preprint</p>
<p>Deep inside convolutional networks: Visualising image classification models and saliency maps. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, Proc. of ICLR. of ICLR2014</p>
<p>Perturbing inputs for fragile interpretations in deep natural language processing. Sanchit Sinha, Hanjie Chen, Arshdeep Sekhon, Yangfeng Ji, Yanjun Qi, Proceedings of the Fourth Black-boxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Fourth Black-boxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP2021</p>
<p>Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B Viégas, Martin Wattenberg, arXiv:1706.03825Smoothgrad: removing noise by adding noise. 2017arXiv preprint</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, Proc. of EMNLP. of EMNLP2013</p>
<p>Supervising model attention with human explanations for robust natural language inference. Joe Stacey, Yonatan Belinkov, Marek Rei, Proc. of AAAI. of AAAI2022</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, Proc. of ICML. of ICML2017</p>
<p>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus, arXiv:1312.6199Intriguing properties of neural networks. 2013arXiv preprint</p>
<p>Identifying the source of vulnerability in explanation discrepancy: A case study in neural text classification. Ruixuan Tang, Hanjie Chen, Yangfeng Ji, EMNLP BlackboxNLP. 2022. 2022</p>
<p>Xuezhi Wang, Haohan Wang, Diyi Yang, arXiv:2112.08313Measure and improve robustness in NLP models: A survey. 2021arXiv preprint</p>
<p>Robust models are more interpretable because attributions look normal. Zifan Wang, Matt Fredrikson, Anupam Datta, Proc. of ICML. of ICML2022</p>
<p>Measuring association between labels and free-text rationales. Sarah Wiegreffe, Ana Marasovic, Noah A Smith, Proc. of EMNLP. of EMNLP2021</p>
<p>Can explanations be useful for calibrating black box models?. Xi Ye, Greg Durrett, Proc. of ACL. of ACL2022</p>
<p>On the (in)fidelity and sensitivity of explanations. Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Sai Suggala, David I Inouye, Pradeep Ravikumar, Proc. of NeurIPS. of NeurIPS2019</p>
<p>On the faithfulness measurements for model interpretations. Fan Yin, Zhouxing Shi, Cho-Jui Hsieh, Kai-Wei Chang, arXiv:2104.087822021arXiv preprint</p>
<p>On the lack of robust interpretability of neural text classifiers. Muhammad Bilal Zafar, Michele Donini, Dylan Slack, Cédric Archambeau, Sanjiv Das, Krishnaram Kenthapadi, Proc. of ACL Findings. of ACL Findings2021</p>
<p>Character-level convolutional networks for text classification. Xiang Zhang, Junbo , Jake Zhao, Yann Lecun, Proc. of NeurIPS. of NeurIPS2015</p>            </div>
        </div>

    </div>
</body>
</html>
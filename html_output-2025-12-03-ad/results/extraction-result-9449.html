<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9449 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9449</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9449</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-a75649771901a4881b44c0ceafa469fcc6e6f968</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a75649771901a4881b44c0ceafa469fcc6e6f968" target="_blank">How Can We Know What Language Models Know?</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts to provide a tighter lower bound on what LMs know.</p>
                <p><strong>Paper Abstract:</strong> Abstract Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a __ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a __ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9449.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9449.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Manual prompts (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manually designed cloze-style prompts (Petroni et al. baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Single, manually created cloze-style templates used to query masked LMs for relational facts (e.g., "x was born in _"). Serves as the baseline prompt format in the paper's LAMA/T‑REx experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base / BERT-large (also ERNIE, KnowBert evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset), LAMA-UHN, Google-RE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cloze-style factual knowledge retrieval: predict a single-token object y given subject x and relation-specific prompt template t_r; evaluated micro- and macro-averaged accuracy over Wikidata triplets (T-REx subset).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Single manually-crafted cloze template per relation (one prompt), masked-LM prediction of the blank (P@1). Examples: "x is affiliated with the y religion".</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Micro-averaged accuracy (T-REx, Table 2): BERT-base Man = 31.1%; BERT-large Man = 32.3%. Macro (Table 3): BERT-base Man = 22.8%; BERT-large Man = 25.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Manual prompts provide a reasonable lower bound but can be sub-optimal because the LM may have learned facts in different surface contexts; therefore single manual prompts can under-retrieve known facts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>T-REx (41 relations) single-token objects only; evaluation uses micro- and macro-averaged accuracy (P@1). Manual prompts originate from Petroni et al.; T-REx-train held-out data used for prompt learning in other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9449.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mining-based prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mining-based prompt generation (middle-word + dependency path)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatically mined prompts from Wikipedia sentences that contain both subject and object pairs, using (1) the middle words between x and y and (2) shortest dependency path spans as templates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base / BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same cloze-style factual retrieval task; mined prompts replace the single manual template with automatically extracted templates per relation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Automatically extracted templates from corpus occurrences: (a) middle-word templates (words between subject and object replaced with placeholders), (b) dependency-path spans (shortest dependency path between subject and object used as prompt). Up to T=40 frequent prompts kept.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to manual single-prompt baseline and to paraphrase-based prompts; also evaluated as single top-1 prompt, rank-based Top-K ensembles, and optimized-weight ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Single-prompt (Top1) micro-accuracy (Table 2): BERT-base Mine = 31.4% (vs Man 31.1%); BERT-large Mine = 37.0% (vs Man 32.3%). Optimized ensemble (Opti.) micro: BERT-base Mine Opti. = 38.9%; BERT-large Mine Opti. = 43.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Versus manual Opti.: Man Opti. not reported as separate, but Mine Opti. improves substantially over manual single prompt (e.g., BERT-base Man 31.1% vs Mine Opti. 38.9%, +7.8 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Best reported improvement from combining mined prompts (Opti.) vs manual single prompt: approx +7.8 percentage points on BERT-base micro-accuracy (31.1% -> 38.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Mined prompts expose diverse surface forms in which relations occur in the corpus; different mined prompts elicit facts that manual prompts miss because the LM may have seen the fact in varied syntactic/lexical contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Mined candidates limited to sentences in Wikipedia containing both entities via distant supervision; filtered prompts (<=10 words, non-stopword-only); T=40 prompts, dependency parser for path extraction; training set T-REx-train used to rank prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9449.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paraphrase-based prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paraphrase generation via back-translation (paraphrased prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate lexically diverse but semantically similar prompts by back-translating a seed prompt (manual or mined) using English↔German NMT and keeping top candidates by round-trip probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base / BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cloze-style factual retrieval where a seed prompt is paraphrased into multiple prompt variants used individually or ensembled.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Paraphrase candidates produced by back-translation (B=7 per direction, keep top T=40 after scoring by forward×backward roundtrip probability); prompts used singly (Top1) or ensembled (TopK or optimized weights).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to mined-only prompts and manual-only prompts; tested as Man+Para (paraphrase of manual), Mine+Para (paraphrase of top mined), and ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Single-prompt (Top1) micro-accuracy (Table 2): Man+Para BERT-base = 34.1% (vs Man 31.1%); Mine+Para BERT-base = 32.7%. Optimized ensemble (Opti.) micro: Man+Para = 37.3%, Mine+Para = 36.2% on BERT-base.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paraphrases often improve single-prompt performance over manual prompts (+3.0 pp for Man+Para on BERT-base). For rank-based ensembles paraphrases often outperform mined prompts; for optimized-weight ensembles mined prompts can outperform paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example: Man+Para Top1 BERT-base +3.0 percentage points vs Man (31.1% -> 34.1%). Paraphrase ensembling (Man+Para Opti.) raises micro-accuracy to 37.3% on BERT-base (+6.2 pp over Man single-prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Small lexical or function-word changes in prompts can substantially change LM predictions; paraphrases capture alternative surface realizations that are more effective for certain subject-object pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Back-translation uses WMT'19 English-German NMT; B=7 candidates each direction, keep up to T=40; paraphrases filtered similarly; evaluated with Top-K and optimized ensembles using T-REx-train for tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9449.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rank-based Top-K ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rank-based ensemble by averaging log-probabilities of top-K prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parameter-free ensembling that ranks prompts by training-set accuracy, then averages their log probabilities (equal weights) to compute object scores; tested with K=1,3,5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base / BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Factual retrieval using multiple prompts: combine top-K prompts' log-probabilities (mean) to form final prediction distribution (softmax over summed log-prob scores).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Use top-K prompts (ranked by training accuracy) and average log P_LM(y | x, t_{r,i}) across those prompts; K varied (Top1/Top3/Top5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to single-prompt Top1 and to optimized-weight ensemble (learned weights), as well as linear-average combination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (Table 2, BERT-base, Mine+Man): Top1 = 31.6% micro; Top3 = 35.9%; Top5 = 35.1%. For many prompt sets Top3/Top5 outperform Top1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Rank-based Top3/Top5 typically outperforms single Top1 by several percentage points (e.g., Mine+Man BERT-base: Top3 35.9% vs Top1 31.6% -> +4.3 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+~3–5 percentage points micro-accuracy gain from Top1 -> Top3/Top5 in representative cases.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different prompts elicit knowledge from different contexts observed by the LM; averaging predictions across diverse prompts increases recall of facts the LM knows in different surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts ranked by accuracy on T-REx-train; log-prob averaging used (preferred over linear averaging); K is a hyperparameter with best K varying by prompt type (often 2–3 for mined prompts, 5 for paraphrases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9449.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Optimized-weight ensemble (Opti.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Optimization-based ensemble with learned prompt weights</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learn a softmax distribution over candidate prompts per relation (vector theta_r) to weight log-probabilities from each prompt and maximize training likelihood of gold objects; used at test time to combine prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base / BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Factual retrieval where T candidate prompts per relation are combined by learned weights to maximize training-set object probability; weights optimized with Adam on T-REx-train.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Weighted log-linear combination: s(y|x,r)=sum_i P_theta(t_{r,i}|r) * log P_LM(y|x,t_{r,i}); P_theta is softmax over learned real-valued theta vector per relation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to rank-based Top-K (equal weights), Top1, oracle, and cross-model weight transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Micro-accuracy (Table 2): BERT-base Mine+Man Opti. = 39.6% (best non-oracle for BERT-base); BERT-large Mine+Man Opti. = 43.9%. These outperform rank-based ensembles by multiple points.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Example: Mine+Man BERT-base Top3 = 35.9% vs Opti. = 39.6% (+3.7 pp). Optimized ensembles outperform rank-based averaging consistently in micro-accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Optimized ensembles improve micro-accuracy by ~3–8 percentage points relative to single-prompt manual baselines and by several points relative to rank-based Top-K.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because mined prompts vary more than paraphrases, learning per-prompt weights concentrates mass on the most reliable prompts and complements them with diverse lower-weight prompts; equal-weight averaging is suboptimal when prompts have unequal reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>T up to 40 prompts per relation; theta_r learned per relation using Adam (default) on T-REx-train; total parameters = T * #relations; learned weights often concentrate on a single prompt with fast-decaying tail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9449.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle upper-bound</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle across prompt set (upper bound)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An upper-bound estimate that counts a fact as correct if any prompt in the candidate set leads the LM to predict the gold object (i.e., best-case over prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base / BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Upper-bound micro-accuracy computed as proportion of facts recoverable by at least one prompt in the prompt pool (mined/paraphrased/manual combined).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Oracle: treat a triple as retrieved if any prompt among the generated set yields the ground-truth object.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Top1, TopK, and optimized ensembles to show remaining headroom.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Oracle micro-accuracy (Table 2): Mine+Man Oracle BERT-base = 52.6%; BERT-large = 56.1%. For mined-only Oracle: 50.7% (base) and 54.4% (large).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Gap between Oracle and best Opti. indicates remaining potential: e.g., BERT-base Mine+Man Oracle 52.6% vs Opti. 39.6% -> +13.0 pp headroom.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large headroom: Oracle often >50% whereas best ensembles are in high-30s/low-40s, leaving ~10–16 percentage points of potential retrieval improvement by better prompt selection/ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different prompts tap into different contexts where the LM encoded the fact; the oracle shows that the LM in fact contains many facts that are retrievable in some surface form but not consistently across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Oracle computed over the same candidate prompt pool (T up to 40) used elsewhere (mined/paraphrased/manual); computed on T-REx test split.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9449.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Middle-word vs Dependency prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Middle-word-only vs Middle+Dependency prompt generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation comparing templates extracted by taking words between subject and object (middle-word) vs adding dependency-path-based templates; dependency-based prompts capture informative tokens not physically between entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cloze-style retrieval where prompt candidates are limited to middle-words only vs combined middle-word + dependency-based prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Two mining-derived prompt pools: (a) Mid = middle-word prompts only; (b) Mid+Dep = middle-word plus dependency-path prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison Mid vs Mid+Dep across Top1/Top3/Top5/Opti./Oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 7 (BERT-base): Mid Top1 = 30.7%, Mid+Dep Top1 = 31.4%; Opti. Mid = 36.9%, Mid+Dep Opti. = 38.9%; Oracle Mid = 45.1%, Mid+Dep Oracle = 50.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Adding dependency-based prompts gives consistent gains (e.g., Opti. +2.0 pp; Oracle +5.6 pp), showing dependency-based prompts capture additional useful patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Opti. micro-accuracy gain ~+2.0 pp (36.9% -> 38.9%); Oracle gain larger (~+5.6 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Words on the dependency path but not strictly between x and y often express the relation (e.g., 'capital of x is y'), so dependency-path prompts bring in syntactic variants that middle-word extraction misses.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Dependency paths computed with a dependency parser; prompt spans taken from leftmost to rightmost word on the shortest dependency path; same filtering rules (<=10 words, non-stopword-only).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9449.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Small lexical changes (prompt brittleness)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to small prompt modifications (paraphrase token edits)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation that small updates (insert/delete/change of a single word or function word) to a prompt can lead to large changes in retrieval accuracy for particular relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cloze-style retrieval; compare minimal edits in prompt wording and measure change in micro-accuracy for targeted relations.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Single-token lexical/function-word edits in paraphrase prompts (e.g., 'x plays in y position' -> 'x plays at y position').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompt vs small-modification paraphrase; paraphrases often generated by back-translation or manual small edits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 6 examples (BERT-base): P413 ('x plays in' -> 'at y position') +23.2 pp; P495 ('x was created' -> 'made in y') +10.8 pp; other small edits showing +2–10 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Single-word or small-phrase changes can increase micro-accuracy by up to ~23 percentage points for specific relations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large per-relation effect sizes (example: +23.2 pp for P413), demonstrating high brittleness to small wording changes.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Large pretrained LMs are brittle to exact prompt wording: small function/content-word differences change conditional distributions sharply, likely because training contexts match only certain surface realizations.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Case-study style examples across relations; paraphrases include insert/update/delete operations and back-translation candidates; evaluated on T-REx test pairs for the relation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9449.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Log-linear vs Linear averaging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Log-probability averaging (log-linear) vs linear probability interpolation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of two methods to combine multiple prompt predictions: average of log probabilities (preferred) versus linear averaging of probabilities; log-linear better penalizes objects unlikely under any prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Combine multiple prompt predictions into a single object distribution for P@1 retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Two combination formulas: (1) s(y)=mean_i log P(y|x,t_i) followed by softmax (log-linear), (2) P(y)=mean_i P(y|x,t_i) (linear interpolation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison plotted in Figure 5; log-linear had superior performance across K.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Figure 5 (qualitative): log-linear combination outperforms linear averaging; precise numeric deltas not tabulated in main tables but plotted (authors state log-linear is better).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Log-linear yields higher micro-accuracy than linear averaging across top-K ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Log probabilities penalize objects that are very unlikely under any particular prompt, avoiding dilution by a single prompt's high but spurious probability — thus giving more robust combined scores.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Comparison conducted on mined prompts with rank-based Top-K ensembles; evaluation shown in Figure 5; authors prefer log-linear by default in other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9449.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-aware prompt fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-aware prompt optimization (hill-climbing / mask-predict style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Attempted to directly optimize prompt text with respect to LM probabilities of gold objects by iteratively masking and replacing tokens (mask-predict-like hill-climb); produced mixed results (oracle up, ensembles down).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Find prompt text that maximizes P_LM(y|x,t) by iterative token replacement guided by LM masked-token probabilities computed over training triples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Start from initial prompt and iterate: mask token i, choose argmax token given other tokens and training distribution (averaged over training pairs); repeat left-to-right until convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compare prompt sets before vs after LM-aware fine-tuning across Top1/Top3/Top5/Opti./Oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 14 (BERT-base aggregate): 'before' Top1=31.9%, Top3=34.5%, Opti.=38.1%, Oracle=47.9%; 'after' Top1=30.2%, Top3=32.5%, Opti.=37.5%, Oracle=50.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LM-aware refinement increased Oracle but slightly decreased ensemble (TopK/Opti.) performance — suggests overfitting to training-set prompts: Oracle +2.9 pp (47.9 -> 50.8) but Opti. -0.6 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Mixed: Oracle increased (~+2.9 pp) while ensemble performance dropped slightly (Opti. -0.6 pp to -1.6 pp depending on column).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Gradient/guided prompt optimization can produce unnatural English or overfit to training set; refined prompts may be tailored to training triples so ensembles generalize worse even though some prompts became very strong on train.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Optimization via mask-and-replace per-token averaged over training triples; authors tried gradient-based approaches but found them unstable and used hill-climbing; results reported on T-REx test.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9449.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Backward-probability scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forward+backward scoring (approx. mutual information)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augment forward probability P(y|x,t) with backward probability P(x|y,t) (approximate mutual information) for scoring candidate objects to counter class imbalance; authors approximate backward score over top-B forward candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve ranking by combining forward and approximate backward probabilities to promote stronger subject-object alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Scoring function adds log P_LM(x|y,t) for top-B forward candidates (approximation due to large object space); used in optimized ensemble scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to optimized ensemble without backward term (baseline Opti.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report small improvement; Table 15 referenced for numbers but main text: "improvement resulting from backward probability is small."</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Small/neglegible gains over baseline optimized ensemble; not a major factor.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Including backward probability encourages stronger subject-object alignment and may counter class imbalance, but approximation and limited gain suggest limited practical benefit in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Backward probabilities computed only for top-B forward candidate objects at train and test; B chosen for efficiency (exact B not provided in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9449.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9449.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-model prompt transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-model generalization of optimized prompt weights</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of whether prompt weights learned (Opti.) on one LM generalize when applied to another LM (e.g., train on BERT-large, test on BERT-base or ERNIE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base / BERT-large / ERNIE</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA (T-REx subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use optimized prompt weight vectors learned on one model to combine prompts at test time on another model and measure performance drop, if any.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Optimized-weight ensemble parameters learned on model A and applied to model B; evaluated for several pairs (base<->large, base<->ERNIE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared same-model-trained weights vs cross-model-trained weights.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 12/13 examples: Test BERT-base, Train BERT-base Mine+Man Opti.=39.6%; Test BERT-base, Train BERT-large Mine+Man Opti.=40.1% (slightly higher). Test ERNIE, Train ERNIE Mine+Man Opti.=43.8%; Test ERNIE, Train BERT-base Mine+Man Opti.=40.5% (drop ~3.3 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Cross-model application results in small drops in many cases; sometimes weights trained on larger model transfer well to smaller (BERT-large->BERT-base gave slight improvement in one case).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Cross-model drops typically small (a few percentage points); in some cases weights from larger model slightly outperform same-model weights on smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prompts exploit similar learned contextual associations across same-architecture models; models with the same architecture benefit more from shared prompt weights than models of different architectures (e.g., BERT vs ERNIE).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Matrices reported in Tables 12 and 13 with test/training model axes; experiments used Mine, Mine+Man, Mine+Para, Man+Para prompt sets and Opti. weighting learned on T-REx-train.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Can We Know What Language Models Know?', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>BERT is not a knowledge base (yet): Factual knowledge vs. Name-based reasoning in unsupervised QA <em>(Rating: 2)</em></li>
                <li>Inducing relational knowledge from BERT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9449",
    "paper_id": "paper-a75649771901a4881b44c0ceafa469fcc6e6f968",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Manual prompts (baseline)",
            "name_full": "Manually designed cloze-style prompts (Petroni et al. baseline)",
            "brief_description": "Single, manually created cloze-style templates used to query masked LMs for relational facts (e.g., \"x was born in _\"). Serves as the baseline prompt format in the paper's LAMA/T‑REx experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base / BERT-large (also ERNIE, KnowBert evaluated)",
            "model_size": null,
            "task_name": "LAMA (T-REx subset), LAMA-UHN, Google-RE",
            "task_description": "Cloze-style factual knowledge retrieval: predict a single-token object y given subject x and relation-specific prompt template t_r; evaluated micro- and macro-averaged accuracy over Wikidata triplets (T-REx subset).",
            "presentation_format": "Single manually-crafted cloze template per relation (one prompt), masked-LM prediction of the blank (P@1). Examples: \"x is affiliated with the y religion\".",
            "comparison_format": null,
            "performance": "Micro-averaged accuracy (T-REx, Table 2): BERT-base Man = 31.1%; BERT-large Man = 32.3%. Macro (Table 3): BERT-base Man = 22.8%; BERT-large Man = 25.7%.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Manual prompts provide a reasonable lower bound but can be sub-optimal because the LM may have learned facts in different surface contexts; therefore single manual prompts can under-retrieve known facts.",
            "null_or_negative_result": false,
            "experimental_details": "T-REx (41 relations) single-token objects only; evaluation uses micro- and macro-averaged accuracy (P@1). Manual prompts originate from Petroni et al.; T-REx-train held-out data used for prompt learning in other methods.",
            "uuid": "e9449.0",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Mining-based prompts",
            "name_full": "Mining-based prompt generation (middle-word + dependency path)",
            "brief_description": "Automatically mined prompts from Wikipedia sentences that contain both subject and object pairs, using (1) the middle words between x and y and (2) shortest dependency path spans as templates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base / BERT-large",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Same cloze-style factual retrieval task; mined prompts replace the single manual template with automatically extracted templates per relation.",
            "presentation_format": "Automatically extracted templates from corpus occurrences: (a) middle-word templates (words between subject and object replaced with placeholders), (b) dependency-path spans (shortest dependency path between subject and object used as prompt). Up to T=40 frequent prompts kept.",
            "comparison_format": "Compared to manual single-prompt baseline and to paraphrase-based prompts; also evaluated as single top-1 prompt, rank-based Top-K ensembles, and optimized-weight ensembles.",
            "performance": "Single-prompt (Top1) micro-accuracy (Table 2): BERT-base Mine = 31.4% (vs Man 31.1%); BERT-large Mine = 37.0% (vs Man 32.3%). Optimized ensemble (Opti.) micro: BERT-base Mine Opti. = 38.9%; BERT-large Mine Opti. = 43.7%.",
            "performance_comparison": "Versus manual Opti.: Man Opti. not reported as separate, but Mine Opti. improves substantially over manual single prompt (e.g., BERT-base Man 31.1% vs Mine Opti. 38.9%, +7.8 pp).",
            "format_effect_size": "Best reported improvement from combining mined prompts (Opti.) vs manual single prompt: approx +7.8 percentage points on BERT-base micro-accuracy (31.1% -&gt; 38.9%).",
            "explanation_or_hypothesis": "Mined prompts expose diverse surface forms in which relations occur in the corpus; different mined prompts elicit facts that manual prompts miss because the LM may have seen the fact in varied syntactic/lexical contexts.",
            "null_or_negative_result": false,
            "experimental_details": "Mined candidates limited to sentences in Wikipedia containing both entities via distant supervision; filtered prompts (&lt;=10 words, non-stopword-only); T=40 prompts, dependency parser for path extraction; training set T-REx-train used to rank prompts.",
            "uuid": "e9449.1",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Paraphrase-based prompts",
            "name_full": "Paraphrase generation via back-translation (paraphrased prompts)",
            "brief_description": "Generate lexically diverse but semantically similar prompts by back-translating a seed prompt (manual or mined) using English↔German NMT and keeping top candidates by round-trip probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base / BERT-large",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Cloze-style factual retrieval where a seed prompt is paraphrased into multiple prompt variants used individually or ensembled.",
            "presentation_format": "Paraphrase candidates produced by back-translation (B=7 per direction, keep top T=40 after scoring by forward×backward roundtrip probability); prompts used singly (Top1) or ensembled (TopK or optimized weights).",
            "comparison_format": "Compared to mined-only prompts and manual-only prompts; tested as Man+Para (paraphrase of manual), Mine+Para (paraphrase of top mined), and ensembles.",
            "performance": "Single-prompt (Top1) micro-accuracy (Table 2): Man+Para BERT-base = 34.1% (vs Man 31.1%); Mine+Para BERT-base = 32.7%. Optimized ensemble (Opti.) micro: Man+Para = 37.3%, Mine+Para = 36.2% on BERT-base.",
            "performance_comparison": "Paraphrases often improve single-prompt performance over manual prompts (+3.0 pp for Man+Para on BERT-base). For rank-based ensembles paraphrases often outperform mined prompts; for optimized-weight ensembles mined prompts can outperform paraphrases.",
            "format_effect_size": "Example: Man+Para Top1 BERT-base +3.0 percentage points vs Man (31.1% -&gt; 34.1%). Paraphrase ensembling (Man+Para Opti.) raises micro-accuracy to 37.3% on BERT-base (+6.2 pp over Man single-prompt).",
            "explanation_or_hypothesis": "Small lexical or function-word changes in prompts can substantially change LM predictions; paraphrases capture alternative surface realizations that are more effective for certain subject-object pairs.",
            "null_or_negative_result": false,
            "experimental_details": "Back-translation uses WMT'19 English-German NMT; B=7 candidates each direction, keep up to T=40; paraphrases filtered similarly; evaluated with Top-K and optimized ensembles using T-REx-train for tuning.",
            "uuid": "e9449.2",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Rank-based Top-K ensemble",
            "name_full": "Rank-based ensemble by averaging log-probabilities of top-K prompts",
            "brief_description": "Parameter-free ensembling that ranks prompts by training-set accuracy, then averages their log probabilities (equal weights) to compute object scores; tested with K=1,3,5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base / BERT-large",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Factual retrieval using multiple prompts: combine top-K prompts' log-probabilities (mean) to form final prediction distribution (softmax over summed log-prob scores).",
            "presentation_format": "Use top-K prompts (ranked by training accuracy) and average log P_LM(y | x, t_{r,i}) across those prompts; K varied (Top1/Top3/Top5).",
            "comparison_format": "Compared to single-prompt Top1 and to optimized-weight ensemble (learned weights), as well as linear-average combination.",
            "performance": "Example (Table 2, BERT-base, Mine+Man): Top1 = 31.6% micro; Top3 = 35.9%; Top5 = 35.1%. For many prompt sets Top3/Top5 outperform Top1.",
            "performance_comparison": "Rank-based Top3/Top5 typically outperforms single Top1 by several percentage points (e.g., Mine+Man BERT-base: Top3 35.9% vs Top1 31.6% -&gt; +4.3 pp).",
            "format_effect_size": "+~3–5 percentage points micro-accuracy gain from Top1 -&gt; Top3/Top5 in representative cases.",
            "explanation_or_hypothesis": "Different prompts elicit knowledge from different contexts observed by the LM; averaging predictions across diverse prompts increases recall of facts the LM knows in different surface forms.",
            "null_or_negative_result": false,
            "experimental_details": "Prompts ranked by accuracy on T-REx-train; log-prob averaging used (preferred over linear averaging); K is a hyperparameter with best K varying by prompt type (often 2–3 for mined prompts, 5 for paraphrases).",
            "uuid": "e9449.3",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Optimized-weight ensemble (Opti.)",
            "name_full": "Optimization-based ensemble with learned prompt weights",
            "brief_description": "Learn a softmax distribution over candidate prompts per relation (vector theta_r) to weight log-probabilities from each prompt and maximize training likelihood of gold objects; used at test time to combine prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base / BERT-large",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Factual retrieval where T candidate prompts per relation are combined by learned weights to maximize training-set object probability; weights optimized with Adam on T-REx-train.",
            "presentation_format": "Weighted log-linear combination: s(y|x,r)=sum_i P_theta(t_{r,i}|r) * log P_LM(y|x,t_{r,i}); P_theta is softmax over learned real-valued theta vector per relation.",
            "comparison_format": "Compared to rank-based Top-K (equal weights), Top1, oracle, and cross-model weight transfer.",
            "performance": "Micro-accuracy (Table 2): BERT-base Mine+Man Opti. = 39.6% (best non-oracle for BERT-base); BERT-large Mine+Man Opti. = 43.9%. These outperform rank-based ensembles by multiple points.",
            "performance_comparison": "Example: Mine+Man BERT-base Top3 = 35.9% vs Opti. = 39.6% (+3.7 pp). Optimized ensembles outperform rank-based averaging consistently in micro-accuracy.",
            "format_effect_size": "Optimized ensembles improve micro-accuracy by ~3–8 percentage points relative to single-prompt manual baselines and by several points relative to rank-based Top-K.",
            "explanation_or_hypothesis": "Because mined prompts vary more than paraphrases, learning per-prompt weights concentrates mass on the most reliable prompts and complements them with diverse lower-weight prompts; equal-weight averaging is suboptimal when prompts have unequal reliability.",
            "null_or_negative_result": false,
            "experimental_details": "T up to 40 prompts per relation; theta_r learned per relation using Adam (default) on T-REx-train; total parameters = T * #relations; learned weights often concentrate on a single prompt with fast-decaying tail.",
            "uuid": "e9449.4",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Oracle upper-bound",
            "name_full": "Oracle across prompt set (upper bound)",
            "brief_description": "An upper-bound estimate that counts a fact as correct if any prompt in the candidate set leads the LM to predict the gold object (i.e., best-case over prompts).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base / BERT-large",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Upper-bound micro-accuracy computed as proportion of facts recoverable by at least one prompt in the prompt pool (mined/paraphrased/manual combined).",
            "presentation_format": "Oracle: treat a triple as retrieved if any prompt among the generated set yields the ground-truth object.",
            "comparison_format": "Compared to Top1, TopK, and optimized ensembles to show remaining headroom.",
            "performance": "Oracle micro-accuracy (Table 2): Mine+Man Oracle BERT-base = 52.6%; BERT-large = 56.1%. For mined-only Oracle: 50.7% (base) and 54.4% (large).",
            "performance_comparison": "Gap between Oracle and best Opti. indicates remaining potential: e.g., BERT-base Mine+Man Oracle 52.6% vs Opti. 39.6% -&gt; +13.0 pp headroom.",
            "format_effect_size": "Large headroom: Oracle often &gt;50% whereas best ensembles are in high-30s/low-40s, leaving ~10–16 percentage points of potential retrieval improvement by better prompt selection/ensembling.",
            "explanation_or_hypothesis": "Different prompts tap into different contexts where the LM encoded the fact; the oracle shows that the LM in fact contains many facts that are retrievable in some surface form but not consistently across prompts.",
            "null_or_negative_result": false,
            "experimental_details": "Oracle computed over the same candidate prompt pool (T up to 40) used elsewhere (mined/paraphrased/manual); computed on T-REx test split.",
            "uuid": "e9449.5",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Middle-word vs Dependency prompts",
            "name_full": "Middle-word-only vs Middle+Dependency prompt generation",
            "brief_description": "Ablation comparing templates extracted by taking words between subject and object (middle-word) vs adding dependency-path-based templates; dependency-based prompts capture informative tokens not physically between entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Cloze-style retrieval where prompt candidates are limited to middle-words only vs combined middle-word + dependency-based prompts.",
            "presentation_format": "Two mining-derived prompt pools: (a) Mid = middle-word prompts only; (b) Mid+Dep = middle-word plus dependency-path prompts.",
            "comparison_format": "Direct comparison Mid vs Mid+Dep across Top1/Top3/Top5/Opti./Oracle.",
            "performance": "Table 7 (BERT-base): Mid Top1 = 30.7%, Mid+Dep Top1 = 31.4%; Opti. Mid = 36.9%, Mid+Dep Opti. = 38.9%; Oracle Mid = 45.1%, Mid+Dep Oracle = 50.7%.",
            "performance_comparison": "Adding dependency-based prompts gives consistent gains (e.g., Opti. +2.0 pp; Oracle +5.6 pp), showing dependency-based prompts capture additional useful patterns.",
            "format_effect_size": "Opti. micro-accuracy gain ~+2.0 pp (36.9% -&gt; 38.9%); Oracle gain larger (~+5.6 pp).",
            "explanation_or_hypothesis": "Words on the dependency path but not strictly between x and y often express the relation (e.g., 'capital of x is y'), so dependency-path prompts bring in syntactic variants that middle-word extraction misses.",
            "null_or_negative_result": false,
            "experimental_details": "Dependency paths computed with a dependency parser; prompt spans taken from leftmost to rightmost word on the shortest dependency path; same filtering rules (&lt;=10 words, non-stopword-only).",
            "uuid": "e9449.6",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Small lexical changes (prompt brittleness)",
            "name_full": "Sensitivity to small prompt modifications (paraphrase token edits)",
            "brief_description": "Empirical observation that small updates (insert/delete/change of a single word or function word) to a prompt can lead to large changes in retrieval accuracy for particular relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Cloze-style retrieval; compare minimal edits in prompt wording and measure change in micro-accuracy for targeted relations.",
            "presentation_format": "Single-token lexical/function-word edits in paraphrase prompts (e.g., 'x plays in y position' -&gt; 'x plays at y position').",
            "comparison_format": "Original prompt vs small-modification paraphrase; paraphrases often generated by back-translation or manual small edits.",
            "performance": "Table 6 examples (BERT-base): P413 ('x plays in' -&gt; 'at y position') +23.2 pp; P495 ('x was created' -&gt; 'made in y') +10.8 pp; other small edits showing +2–10 pp.",
            "performance_comparison": "Single-word or small-phrase changes can increase micro-accuracy by up to ~23 percentage points for specific relations.",
            "format_effect_size": "Large per-relation effect sizes (example: +23.2 pp for P413), demonstrating high brittleness to small wording changes.",
            "explanation_or_hypothesis": "Large pretrained LMs are brittle to exact prompt wording: small function/content-word differences change conditional distributions sharply, likely because training contexts match only certain surface realizations.",
            "null_or_negative_result": false,
            "experimental_details": "Case-study style examples across relations; paraphrases include insert/update/delete operations and back-translation candidates; evaluated on T-REx test pairs for the relation.",
            "uuid": "e9449.7",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Log-linear vs Linear averaging",
            "name_full": "Log-probability averaging (log-linear) vs linear probability interpolation",
            "brief_description": "Comparison of two methods to combine multiple prompt predictions: average of log probabilities (preferred) versus linear averaging of probabilities; log-linear better penalizes objects unlikely under any prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base (representative)",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Combine multiple prompt predictions into a single object distribution for P@1 retrieval.",
            "presentation_format": "Two combination formulas: (1) s(y)=mean_i log P(y|x,t_i) followed by softmax (log-linear), (2) P(y)=mean_i P(y|x,t_i) (linear interpolation).",
            "comparison_format": "Direct comparison plotted in Figure 5; log-linear had superior performance across K.",
            "performance": "Figure 5 (qualitative): log-linear combination outperforms linear averaging; precise numeric deltas not tabulated in main tables but plotted (authors state log-linear is better).",
            "performance_comparison": "Log-linear yields higher micro-accuracy than linear averaging across top-K ensembles.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Log probabilities penalize objects that are very unlikely under any particular prompt, avoiding dilution by a single prompt's high but spurious probability — thus giving more robust combined scores.",
            "null_or_negative_result": false,
            "experimental_details": "Comparison conducted on mined prompts with rank-based Top-K ensembles; evaluation shown in Figure 5; authors prefer log-linear by default in other experiments.",
            "uuid": "e9449.8",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "LM-aware prompt fine-tuning",
            "name_full": "LM-aware prompt optimization (hill-climbing / mask-predict style)",
            "brief_description": "Attempted to directly optimize prompt text with respect to LM probabilities of gold objects by iteratively masking and replacing tokens (mask-predict-like hill-climb); produced mixed results (oracle up, ensembles down).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Find prompt text that maximizes P_LM(y|x,t) by iterative token replacement guided by LM masked-token probabilities computed over training triples.",
            "presentation_format": "Start from initial prompt and iterate: mask token i, choose argmax token given other tokens and training distribution (averaged over training pairs); repeat left-to-right until convergence.",
            "comparison_format": "Compare prompt sets before vs after LM-aware fine-tuning across Top1/Top3/Top5/Opti./Oracle.",
            "performance": "Table 14 (BERT-base aggregate): 'before' Top1=31.9%, Top3=34.5%, Opti.=38.1%, Oracle=47.9%; 'after' Top1=30.2%, Top3=32.5%, Opti.=37.5%, Oracle=50.8%.",
            "performance_comparison": "LM-aware refinement increased Oracle but slightly decreased ensemble (TopK/Opti.) performance — suggests overfitting to training-set prompts: Oracle +2.9 pp (47.9 -&gt; 50.8) but Opti. -0.6 pp.",
            "format_effect_size": "Mixed: Oracle increased (~+2.9 pp) while ensemble performance dropped slightly (Opti. -0.6 pp to -1.6 pp depending on column).",
            "explanation_or_hypothesis": "Gradient/guided prompt optimization can produce unnatural English or overfit to training set; refined prompts may be tailored to training triples so ensembles generalize worse even though some prompts became very strong on train.",
            "null_or_negative_result": true,
            "experimental_details": "Optimization via mask-and-replace per-token averaged over training triples; authors tried gradient-based approaches but found them unstable and used hill-climbing; results reported on T-REx test.",
            "uuid": "e9449.9",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Backward-probability scoring",
            "name_full": "Forward+backward scoring (approx. mutual information)",
            "brief_description": "Augment forward probability P(y|x,t) with backward probability P(x|y,t) (approximate mutual information) for scoring candidate objects to counter class imbalance; authors approximate backward score over top-B forward candidates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base (representative)",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Improve ranking by combining forward and approximate backward probabilities to promote stronger subject-object alignment.",
            "presentation_format": "Scoring function adds log P_LM(x|y,t) for top-B forward candidates (approximation due to large object space); used in optimized ensemble scoring.",
            "comparison_format": "Compared to optimized ensemble without backward term (baseline Opti.).",
            "performance": "Authors report small improvement; Table 15 referenced for numbers but main text: \"improvement resulting from backward probability is small.\"",
            "performance_comparison": "Small/neglegible gains over baseline optimized ensemble; not a major factor.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Including backward probability encourages stronger subject-object alignment and may counter class imbalance, but approximation and limited gain suggest limited practical benefit in this setting.",
            "null_or_negative_result": true,
            "experimental_details": "Backward probabilities computed only for top-B forward candidate objects at train and test; B chosen for efficiency (exact B not provided in main text).",
            "uuid": "e9449.10",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Cross-model prompt transfer",
            "name_full": "Cross-model generalization of optimized prompt weights",
            "brief_description": "Investigation of whether prompt weights learned (Opti.) on one LM generalize when applied to another LM (e.g., train on BERT-large, test on BERT-base or ERNIE).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base / BERT-large / ERNIE",
            "model_size": null,
            "task_name": "LAMA (T-REx subset)",
            "task_description": "Use optimized prompt weight vectors learned on one model to combine prompts at test time on another model and measure performance drop, if any.",
            "presentation_format": "Optimized-weight ensemble parameters learned on model A and applied to model B; evaluated for several pairs (base&lt;-&gt;large, base&lt;-&gt;ERNIE).",
            "comparison_format": "Compared same-model-trained weights vs cross-model-trained weights.",
            "performance": "Table 12/13 examples: Test BERT-base, Train BERT-base Mine+Man Opti.=39.6%; Test BERT-base, Train BERT-large Mine+Man Opti.=40.1% (slightly higher). Test ERNIE, Train ERNIE Mine+Man Opti.=43.8%; Test ERNIE, Train BERT-base Mine+Man Opti.=40.5% (drop ~3.3 pp).",
            "performance_comparison": "Cross-model application results in small drops in many cases; sometimes weights trained on larger model transfer well to smaller (BERT-large-&gt;BERT-base gave slight improvement in one case).",
            "format_effect_size": "Cross-model drops typically small (a few percentage points); in some cases weights from larger model slightly outperform same-model weights on smaller model.",
            "explanation_or_hypothesis": "Prompts exploit similar learned contextual associations across same-architecture models; models with the same architecture benefit more from shared prompt weights than models of different architectures (e.g., BERT vs ERNIE).",
            "null_or_negative_result": false,
            "experimental_details": "Matrices reported in Tables 12 and 13 with test/training model axes; experiments used Mine, Mine+Man, Mine+Para, Man+Para prompt sets and Opti. weighting learned on T-REx-train.",
            "uuid": "e9449.11",
            "source_info": {
                "paper_title": "How Can We Know What Language Models Know?",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "BERT is not a knowledge base (yet): Factual knowledge vs. Name-based reasoning in unsupervised QA",
            "rating": 2
        },
        {
            "paper_title": "Inducing relational knowledge from BERT",
            "rating": 1
        }
    ],
    "cost": 0.020322749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How Can We Know What Language Models Know?</h1>
<p>Zhengbao Jiang ${ }^{1 <em>}$ Frank F. Xu ${ }^{1 </em>}$ Jun Araki ${ }^{2}$ Graham Neubig ${ }^{1}$<br>${ }^{1}$ Language Technologies Institute, Carnegie Mellon University<br>${ }^{2}$ Bosch Research North America<br>{zhengbaj,fangzhex, gneubig}@cs.cmu.edu jun.araki@us.bosch.com</p>
<h4>Abstract</h4>
<p>Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as "Obama is a _ by profession". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as "Obama worked as a _ " may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from $31.1 \%$ to $39.6 \%$, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.</p>
<h2>1 Introduction</h2>
<p>Recent years have seen the primary role of language models (LMs) transition from generating or evaluating the fluency of natural text (Mikolov and Zweig, 2012; Merity et al., 2018; Melis et al., 2018; Gamon et al., 2005) to being a powerful tool for text understanding. This understanding has mainly been achieved through the use of language modeling as a pre-training task for feature extractors, where the hidden vectors learned through a language modeling objective are then used in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>down-stream language understanding systems (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018; Devlin et al., 2019).</p>
<p>Interestingly, it is also becoming apparent that $\mathrm{LMs}^{1}$ themselves can be used as a tool for text understanding by formulating queries in natural language and either generating textual answers directly (McCann et al., 2018; Radford et al., 2019), or assessing multiple choices and picking the most likely one (Zweig and Burges, 2011; Rajani et al., 2019). For example, LMs have been used to answer factoid questions (Radford et al., 2019), answer common sense queries (Trinh and Le, 2018; Sap et al., 2019), or extract factual knowledge about relations between entities (Petroni et al., 2019; Baldini Soares et al., 2019). Regardless of the end task, the knowledge contained in LMs is probed by providing a prompt, and letting the LM either generate the continuation of a prefix (e.g., "Barack Obama was born in _ "), or predict missing words in a cloze-style template (e.g., "Barack Obama is a _ by profession").</p>
<p>However, while this paradigm has been used to achieve a number of intriguing results regarding the knowledge expressed by LMs, they usually rely on prompts that were manually created based on the intuition of the experimenter. These manually created prompts (e.g., "Barack Obama was born in _") might be sub-optimal because LMs might have learned target knowledge from substantially different contexts (e.g., "The birth place of Barack Obama is Honolulu, Hawaii.") during their training. Thus it is quite possible that a fact that the LM does know cannot be retrieved due to the prompts not being effective queries for the fact. Thus, existing results are simply a lower bound on the extent of knowledge contained</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Top-5 predictions and their log probabilities using different prompts (manual, mined, and paraphrased) to query BERT. Correct answer is underlined.
in LMs, and in fact, LMs may be even more knowledgeable than these initial results indicate. In this paper we ask the question: "How can we tighten this lower bound and get a more accurate estimate of the knowledge contained in state-of-the-art LMs?" This is interesting both scientifically, as a probe of the knowledge that LMs contain, and from an engineering perspective, as it will result in higher recall when using LMs as part of a knowledge extraction system.</p>
<p>In particular, we focus on the setting of Petroni et al. (2019) who examine extracting knowledge regarding the relations between entities (definitions in $\S 2$ ). We propose two automatic methods to systematically improve the breadth and quality of the prompts used to query the existence of a relation (§ 3). Specifically, as shown in Figure 1, these are mining-based methods inspired by previous relation extraction methods (Ravichandran and Hovy, 2002), and paraphrasing-based methods that take a seed prompt (either manually created or automatically mined), and paraphrase it into several other semantically similar expressions. Further, because different prompts may work better when querying for different subjectobject pairs, we also investigate lightweight ensemble methods to combine the answers from different prompts together (§ 4).</p>
<p>We experiment on the LAMA benchmark (Petroniet al., 2019), which is an English-language benchmark devised to test the ability of LMs to retrieve relations between entities (§5). We first demonstrate that improved prompts significantly improve accuracy on this task, with the one-best prompt extracted by our method raising accuracy from $31.1 \%$ to $34.1 \%$ on BERT-base (Devlin et al., 2019), with similar gains being obtained with</p>
<p>BERT-large as well. We further demonstrate that using a diversity of prompts through ensembling further improves accuracy to $39.6 \%$. We perform extensive analysis and ablations, gleaning insights both about how to best query the knowledge stored in LMs and about potential directions for incorporating knowledge into LMs themselves. Finally, we have released the resulting LM Prompt And Query Archive (LPAQA) to facilitate future experiments on probing knowledge contained in LMs.</p>
<h2>2 Knowledge Retrieval from LMs</h2>
<p>Retrieving factual knowledge from LMs is quite different from querying standard declarative knowledge bases (KBs). In standard KBs, users formulate their information needs as a structured query defined by the KB schema and query language. For example, SELECT ?y WHERE {wd:Q76 wdt:P19 ?y} is a SPARQL query to search the birth place of Barack_Obama. In contrast, LMs must be queried by natural language prompts, such as '‘Barack Obama was born in _'', and the word assigned the highest probability in the blank will be returned as the answer. Unlike deterministic queries on KBs, this provides no guarantees of correctness or success.</p>
<p>While the idea of prompts is common to methods for extracting many varieties of knowledge from LMs, in this paper we specifically follow the formulation of Petroni et al. (2019), where factual knowledge is in the form of triples $\langle x, r, y\rangle$. Here $x$ indicates the subject, $y$ indicates the object, and $r$ is their corresponding relation. To query the LM, $r$ is associated with a cloze-style prompt $t_{r}$ consisting of a sequence of tokens, two of which are placeholders for subjects and objects (e.g., ' $x$ plays at $y$ position'). The existence of the fact in the LM is assessed by replacing $x$ with the surface form of the subject, and letting the model predict the missing object (e.g., 'LeBron James plays at _ position'): ${ }^{2}$</p>
<p>$$
\hat{y}=\arg \max <em _mathrm_LM="\mathrm{LM">{y^{\prime} \in V} P</em>\right)
$$}}\left(y^{\prime} \mid x, t_{r</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>where $\mathcal{V}$ is the vocabulary, and $P_{\mathrm{LM}}\left(y^{\prime} \mid x, t_{r}\right)$ is the LM probability of predicting $y^{\prime}$ in the blank conditioned on the other tokens (i.e., the subject and the prompt). ${ }^{3}$ We say that an LM has knowledge of a fact if $\hat{y}$ is the same as the groundtruth $y$. Because we would like our prompts to most effectively elicit any knowledge contained in the LM itself, a "good" prompt should trigger the LM to predict the ground-truth objects as often as possible.</p>
<p>In previous work (McCann et al., 2018; Radford et al., 2019; Petroni et al., 2019), $t_{r}$ has been a single manually defined prompt based on the intuition of the experimenter. As noted in the introduction, this method has no guarantee of being optimal, and thus we propose methods that learn effective prompts from a small set of training data consisting of gold subject-object pairs for each relation.</p>
<h2>3 Prompt Generation</h2>
<p>First, we tackle prompt generation: the task of generating a set of prompts $\left{t_{r, i}\right}_{i=1}^{T}$ for each relation $r$, where at least some of the prompts effectively trigger LMs to predict ground-truth objects. We employ two practical methods to either mine prompt candidates from a large corpus (§ 3.1) or diversify a seed prompt through paraphrasing (§ 3.2).</p>
<h3>3.1 Mining-based Generation</h3>
<p>Our first method is inspired by templatebased relation extraction methods (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), which are based on the observation that words in the vicinity of the subject $x$ and object $y$ in a large corpus often describe the relation $r$. Based on this intuition, we first identify all the Wikipedia sentences that contain both subjects and objects of a specific relation $r$ using the assumption of distant supervision, then propose two methods to extract prompts.</p>
<p>Middle-word Prompts Following the observation that words in the middle of the subject and object are often indicative of the relation, we</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>directly use those words as prompts. For example, "Barack Obama was born in Hawaii" is converted into a prompt ' $x$ was born in $y$ '' by replacing the subject and the object with placeholders.</p>
<p>Dependency-based Prompts Toutanova et al. (2015) note that in cases of templates where words do not appear in the middle (e.g., "The capital of France is Paris'), templates based on syntactic analysis of the sentence can be more effective for relation extraction. We follow this insight in our second strategy for prompt creation, which parses sentences with a dependency parser to identify the shortest dependency path between the subject and object, then uses the phrase spanning from the leftmost word to the rightmost word in the dependency path as a prompt. For instance, the dependency path in the above example is "France $\stackrel{p o b j}{\longleftrightarrow}$ of $\stackrel{\text { prep }}{\longleftarrow}$ capital $\stackrel{\text { nsubj }}{\longleftarrow}$ is $\stackrel{\text { attr }}{\longleftrightarrow}$ Paris', where the leftmost and rightmost words are "capital" and "Paris", giving a prompt of "capital of $x$ is $y^{\prime \prime}$.</p>
<p>Notably, these mining-based methods do not rely on any manually created prompts, and can thus be flexibly applied to any relation where we can obtain a set of subject-object pairs. This will result in diverse prompts, covering a wide variety of ways that the relation may be expressed in text. However, it may also be prone to noise, as many prompts acquired in this way may not be very indicative of the relation (e.g., ' $x, y$ '), even if they are frequent.</p>
<h3>3.2 Paraphrasing-based Generation</h3>
<p>Our second method for generating prompts is more targeted-it aims to improve lexical diversity while remaining relatively faithful to the original prompt. Specifically, we do so by performing paraphrasing over the original prompt into other semantically similar or identical expressions. For example, if our original prompt is ' $x$ shares a border with $y$ ", it may be paraphrased into ' $x$ has a common border with $y$ "' and ' $x$ adjoins $y$ '". This is conceptually similar to query expansion techniques used in information retrieval that reformulate a given query to improve retrieval performance (Carpineto and Romano, 2012).</p>
<p>Although many methods could be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple</p>
<p>method of using back-translation (Sennrich et al., 2016; Mallinson et al., 2017) to first translate the initial prompt into $B$ candidates in another language, each of which is then back-translated into $B$ candidates in the original language. We then rank $B^{2}$ candidates based on their roundtrip probability (i.e., $P_{\text {forward }}(\hat{t} \mid \hat{t}) \cdot P_{\text {backward }}(t \mid \hat{t})$, where $\hat{t}$ is the initial prompt, $\hat{t}$ is the translated prompt in the other language, and $t$ is the final prompt), and keep the top $T$ prompts.</p>
<h2>4 Prompt Selection and Ensembling</h2>
<p>In the previous section, we described methods to generate a set of candidate prompts $\left{t_{r, i}\right}_{i=1}^{T}$ for a particular relation $r$. Each of these prompts may be more or less effective at eliciting knowledge from the LM, and thus it is necessary to decide how to use these generated prompts at test time. In this section, we describe three methods to do so.</p>
<h3>4.1 Top-1 Prompt Selection</h3>
<p>For each prompt, we can measure its accuracy of predicting the ground-truth objects (on a training dataset) using:
$A\left(t_{r, i}\right)=\frac{\sum_{|x, y| \in \mathcal{R}} \delta\left(y=\arg \max <em _mathrm_LM="\mathrm{LM">{y^{\prime}} P</em>$,
where $\mathcal{R}$ is a set of subject-object pairs with relation $r$, and $\delta(\cdot)$ is Kronecker's delta function, returning 1 if the internal condition is true and 0 otherwise. In the simplest method for querying the LM, we choose the prompt with the highest accuracy and query using only this prompt.}}\left(y^{\prime} \mid x, t_{r, i}\right)\right)}{|\mathcal{R}|</p>
<h3>4.2 Rank-based Ensemble</h3>
<p>Next we examine methods that use not only the top-1 prompt, but combine together multiple prompts. The advantage to this is that the LM may have observed different entity pairs in different contexts within its training data, and having a variety of prompts may allow for elicitation of knowledge that appeared in these different contexts.</p>
<p>Our first method for ensembling is a parameterfree method that averages the predictions of the top-ranked prompts. We rank all the prompts based on their accuracy of predicting the objects on the training set, and use the average log
probabilities ${ }^{4}$ from the top $K$ prompts to calculate the probability of the object:</p>
<p>$$
\begin{aligned}
s(y \mid x, r) &amp; =\sum_{i=1}^{K} \frac{1}{K} \log P_{\mathrm{LM}}\left(y \mid x, t_{r, i}\right) \
P(y \mid x, r) &amp; =\operatorname{softmax}(s(\cdot \mid x, r))_{y}
\end{aligned}
$$</p>
<p>where $t_{r, i}$ is the prompt ranked at the $i$-th position. Here, $K$ is a hyper-parameter, where a small $K$ focuses on the few most accurate prompts, and a large $K$ increases diversity of the prompts.</p>
<h3>4.3 Optimized Ensemble</h3>
<p>The above method treats the top $K$ prompts equally, which is sub-optimal given some prompts are more reliable than others. Thus, we also propose a method that directly optimizes prompt weights. Formally, we re-define the score in Equation 1 as:</p>
<p>$$
s(y \mid x, r)=\sum_{i=1}^{T} P_{\boldsymbol{\theta}<em i="i" r_="r,">{r}}\left(t</em>\right)
$$} \mid r\right) \log P_{\mathrm{LM}}\left(y \mid x, t_{r, i</p>
<p>where $P_{\boldsymbol{\theta}<em i="i" r_="r,">{r}}\left(t</em>} \mid r\right)=\operatorname{softmax}\left(\boldsymbol{\theta<em r="r">{r}\right)$ is a distribution over prompts parameterized by $\boldsymbol{\theta}</em>$ is optimized to maximize the probability of the gold-standard objects $P(y \mid x, r)$ over training data.}$, a $T$-sized realvalue vector. For every relation, we learn to score a different set of $T$ candidate prompts, so the total number of parameters is $T$ times the number of relations. The parameter $\boldsymbol{\theta}_{r</p>
<h2>5 Main Experiments</h2>
<h3>5.1 Experimental Settings</h3>
<p>In this section, we assess the extent to which our prompts can improve fact prediction performance, raising the lower bound on the knowledge we discern is contained in LMs.</p>
<p>Dataset As data, we use the T-REx subset (ElSahar et al., 2018) of the LAMA benchmark (Petroni et al., 2019), which has a broader set of 41 relations (compared with the Google-RE subset, which only covers 3). Each relation is associated with at most 1000 subject-object pairs from Wikidata, and a single manually designed</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>prompt. To learn to mine prompts (§ 3.1), rank prompts (§ 4.2), or learn ensemble weights (§ 4.3), we create a separate training set of subject-object pairs also from Wikidata for each relation that has no overlap with the T-REx dataset. We denote the training set as T-REx-train. For consistency with the T-REx dataset in LAMA, T-REx-train also is chosen to contain only single-token objects. To investigate the generality of our method, we also report the performance of our methods on the Google-RE subset, ${ }^{5}$ which takes a similar form to T-REx but is relatively small and only covers three relations.</p>
<p>Pörner et al. (2019) note that some facts in LAMA can be recalled solely based on surface forms of entities, without memorizing facts. They filter out those easy-to-guess facts and create a more difficult benchmark, denoted as LAMAUHN. We also conduct experiments on the T-REx subset of LAMA-UHN (i.e., T-REx-UHN) to investigate whether our methods can still obtain improvements on this harder benchmark. Dataset statistics are summarized in Table 1.</p>
<p>Models As for the models to probe, in our main experiments we use the standard BERT-base and BERT-large models (Devlin et al., 2019). We also perform some experiments with other pretrained models enhanced with external entity representations, namely, ERNIE (Zhang et al., 2019) and KnowBert (Peters et al., 2019), which we believe may do better on recall of entities.</p>
<p>Evaluation Metrics We use two metrics to evaluate the success of prompts in probing LMs. The first evaluation metric, micro-averaged accuracy, follows the LAMA benchmark ${ }^{6}$ in calculating the accuracy of all subject-object pairs for relation $r$ :</p>
<p>$$
\frac{1}{|\mathcal{R}|} \sum_{\langle x, y\rangle \in \mathcal{R}} \delta(\hat{y}=y)
$$</p>
<p>where $\hat{y}$ is the prediction and $y$ is the ground truth. Then we average across all relations. However, we found that the object distributions</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Dataset statistics. All the values are averaged across 41 relations.
of some relations are extremely skewed (e.g., more than half of the objects in relation native_language are French). This can lead to deceptively high scores, even for a majorityclass baseline that picks the most common object for each relation, which achieves a score of $22.0 \%$. To mitigate this problem, we also report macroaveraged accuracy, which computes accuracy for each unique object separately, then averages them together to get the relation-level accuracy:</p>
<p>$$
\frac{1}{|\text { uni_obj }(\mathcal{R})|} \sum_{y^{\prime} \in \text { uni_obj }(\mathcal{R})} \frac{\sum_{\langle x, y\rangle \in \mathcal{R}, y=y^{\prime}} \delta(\hat{y}=y)}{|{y \mid\langle x, y\rangle \in \mathcal{R}, y=y^{\prime}}|}
$$</p>
<p>where uni_obj $(\mathcal{R})$ returns a set of unique objects from relation $r$. This is a much stricter metric, with the majority-class baseline only achieving a score of $2.2 \%$.</p>
<p>Methods We attempted different methods for prompt generation and selection/ensembling, and compare them with the manually designed prompts used in Petroni et al. (2019). Majority refers to predicting the majority object for each relation, as mentioned above. Man is the baseline from Petroni et al. (2019) that only uses the manually designed prompts for retrieval. Mine (§ 3.1) uses the prompts mined from Wikipedia through both middle words and dependency paths, and Mine+Man combines them with the manual prompts. Mine+Para (§ 3.2) paraphrases the highest-ranked mined prompt for each relation, while Man+Para uses the manual one instead.</p>
<p>The prompts are combined either by averaging the log probabilities from the TopK highestranked prompts (§ 4.2) or the weights after optimization (§ 4.3; Opti.). Oracle represents the upper bound of the performance of the generated prompts, where a fact is judged as correct if any one of the prompts allows the LM to successfully predict the object.</p>
<p>Implementation Details We use $T=40$ most frequent prompts either generated through mining</p>
<p>or paraphrasing in all experiments, and the number of candidates in back-translation is set to $B=7$. We remove prompts only containing stopwords/ punctuations or longer than 10 words to reduce noise. We use the round-trip English-German neural machine translation models pre-trained on WMT'19 (Ng et al., 2019) for back-translation, as English-German is one of the most highly resourced language pairs. ${ }^{7}$ When optimizing ensemble parameters, we use Adam (Kingma and $\mathrm{Ba}, 2015$ ) with default parameters and batch size of 32 .</p>
<h3>5.2 Evaluation Results</h3>
<p>Micro- and macro-averaged accuracy of different methods are reported in Tables 2 and 3, respectively.</p>
<p>Single Prompt Experiments When only one prompt is used (in the first Top1 column in both tables), the best of the proposed prompt generation methods increases micro-averaged accuracy from $31.1 \%$ to $34.1 \%$ on BERT-base, and from $32.3 \%$ to $39.4 \%$ on BERT-large. This demonstrates that the manually created prompts are a somewhat weak lower bound; there are other prompts that further improve the ability to query knowledge from LMs. Table 4 shows some of the mined prompts that resulted in a large performance gain compared with the manual ones. For the relation religion, ' $x$ who converted to $y$ "' improved $60.0 \%$ over the manually defined prompt of ' $x$ is affiliated with the $y$ religion'', and for the relation subclass_of, ' $x$ is a type of $y$ '' raised the accuracy by $22.7 \%$ over ' $x$ is a subclass of $y^{\prime \prime}$. It can be seen that the largest gains from using mined prompts seem to occur in cases where the manually defined prompt is more complicated syntactically (e.g., the former), or when it uses less common wording (e.g., the latter) than the mined prompt.</p>
<p>Prompt Ensembling Next we turn to experiments that use multiple prompts to query the LM. Comparing the single-prompt results in column 1 to the ensembled results in the following three columns, we can see that ensembling multiple prompts almost always leads to better performance. The simple average used in Top3 and</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Prompts | Top1 | Top3 | Top5 | Opti. | Oracle |
| :--: | :--: | :--: | :--: | :--: | :--: |
|  | BERT-base (Man=31.1) |  |  |  |  |
| Mine | 31.4 | 34.2 | 34.7 | 38.9 | 50.7 |
| Mine+Man | 31.6 | 35.9 | 35.1 | 39.6 | 52.6 |
| Mine+Para | 32.7 | 34.0 | 34.5 | 36.2 | 48.1 |
| Man+Para | 34.1 | 35.8 | 36.6 | 37.3 | 47.9 |
|  | BERT-large (Man=32.3) |  |  |  |  |
| Mine | 37.0 | 37.0 | 36.4 | 43.7 | 54.4 |
| Mine+Man | 39.4 | 40.6 | 38.4 | 43.9 | 56.1 |
| Mine+Para | 37.8 | 38.6 | 38.6 | 40.1 | 51.8 |
| Man+Para | 35.9 | 37.3 | 38.0 | 38.8 | 50.0 |</p>
<p>Table 2: Micro-averaged accuracy of different methods (\%). Majority gives us $22.0 \%$. Italic indicates best single-prompt accuracy, and bold indicates the best non-oracle accuracy overall.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompts</th>
<th style="text-align: center;">Top1</th>
<th style="text-align: center;">Top3</th>
<th style="text-align: center;">Top5</th>
<th style="text-align: center;">Opti.</th>
<th style="text-align: center;">Oracle</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT-base (Man=22.8)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mine</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">36.2</td>
</tr>
<tr>
<td style="text-align: center;">Mine+Man</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;">Mine+Para</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: center;">Man+Para</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">34.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT-large (Man=25.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mine</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">40.7</td>
</tr>
<tr>
<td style="text-align: center;">Mine+Man</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: center;">Mine+Para</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;">Man+Para</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">39.3</td>
</tr>
</tbody>
</table>
<p>Table 3: Macro-averaged accuracy of different methods (\%). Majority gives us $2.2 \%$. Italic indicates best single-prompt accuracy, and bold indicates the best non-oracle accuracy overall.</p>
<p>Top5 outperforms Top1 across different prompt generation methods. The optimized ensemble further raises micro-averaged accuracy to $38.9 \%$ and $43.7 \%$ on BERT-base and BERT-large respectively, outperforming the rank-based ensemble by a large margin. These two sets of results demonstrate that diverse prompts can indeed query the LM in different ways, and that the optimizationbased method is able to find weights that effectively combine different prompts together.</p>
<p>We list the learned weights of top-3 mined prompts and accuracy gain over only using the top-1 prompt in Table 5. Weights tend to concentrate on one particular prompt, and the other prompts serve as complements. We also depict the performance of the rank-based ensemble method</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">Relations</th>
<th style="text-align: left;">Manual Prompts</th>
<th style="text-align: center;">Mined Prompts</th>
<th style="text-align: right;">Acc. Gain</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">P140</td>
<td style="text-align: left;">religion</td>
<td style="text-align: left;">$x$ is affiliated with the $y$ religion</td>
<td style="text-align: center;">$x$ who converted to $y$</td>
<td style="text-align: right;">+60.0</td>
</tr>
<tr>
<td style="text-align: left;">P159</td>
<td style="text-align: left;">headquarters location</td>
<td style="text-align: left;">The headquarter of $x$ is in $y$</td>
<td style="text-align: center;">$x$ is based in $y$</td>
<td style="text-align: right;">+4.9</td>
</tr>
<tr>
<td style="text-align: left;">P20</td>
<td style="text-align: left;">place of death</td>
<td style="text-align: left;">$x$ died in $y$</td>
<td style="text-align: center;">$x$ died at his home in $y$</td>
<td style="text-align: right;">+4.6</td>
</tr>
<tr>
<td style="text-align: left;">P264</td>
<td style="text-align: left;">record label</td>
<td style="text-align: left;">$x$ is represented by music label $y$</td>
<td style="text-align: center;">$x$ recorded for $y$</td>
<td style="text-align: right;">+17.2</td>
</tr>
<tr>
<td style="text-align: left;">P279</td>
<td style="text-align: left;">subclass of</td>
<td style="text-align: left;">$x$ is a subclass of $y$</td>
<td style="text-align: center;">$x$ is a type of $y$</td>
<td style="text-align: right;">+22.7</td>
</tr>
<tr>
<td style="text-align: left;">P39</td>
<td style="text-align: left;">position held</td>
<td style="text-align: left;">$x$ has the position of $y$</td>
<td style="text-align: center;">$x$ is elected $y$</td>
<td style="text-align: right;">+7.9</td>
</tr>
</tbody>
</table>
<p>Table 4: Micro-averaged accuracy gain (\%) of the mined prompts over the manual prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">Relations</th>
<th style="text-align: left;">Prompts and Weights</th>
<th style="text-align: right;">Acc. Gain</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">P127</td>
<td style="text-align: left;">owned by</td>
<td style="text-align: left;">$x$ is owned by $y_{.485} x$ was acquired by $y_{.151} x$ division of $y_{.151}$</td>
<td style="text-align: right;">+7.0</td>
</tr>
<tr>
<td style="text-align: left;">P140</td>
<td style="text-align: left;">religion</td>
<td style="text-align: left;">$x$ who converted to $y_{.615} y$ tirthankara $x_{.190} y$ dedicated to $x_{.110}$</td>
<td style="text-align: right;">+12.2</td>
</tr>
<tr>
<td style="text-align: left;">P176</td>
<td style="text-align: left;">manufacturer</td>
<td style="text-align: left;">$y$ introduced the $x_{.594} y$ announced the $x_{.286} x$ attributed to the $y_{.111}$</td>
<td style="text-align: right;">+7.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Weights of top-3 mined prompts, and the micro-averaged accuracy gain (\%) over using the top-1 prompt.
with respect to the number of prompts in Figure 2. For mined prompts, top-2 or top-3 usually gives us the best results, while for paraphrased prompts, top-5 is the best. Incorporating more prompts does not always improve accuracy, a finding consistent with the rapidly decreasing weights learned by the optimization-based method. The gap between Oracle and Opti. indicates that there is still space for improvement using better ensemble methods.</p>
<p>Mining vs. Paraphrasing For the rank-based ensembles (Top1, 3, 5), prompts generated by paraphrasing usually perform better than mined prompts, while for the optimization-based ensemble (Opti.), mined prompts perform better. We conjecture this is because mined prompts exhibit more variation compared to paraphrases, and proper weighting is of central importance. This difference in the variation can be observed in the average edit distance between the prompts of each class, which is 3.27 and 2.73 for mined and paraphrased prompts respectively. However, the improvement led by ensembling paraphrases is still significant over just using one prompt (Top1 vs. Opti.), raising microaveraged accuracy from $32.7 \%$ to $36.2 \%$ on BERT-base, and from $37.8 \%$ to $40.1 \%$ on BERTlarge. This indicates that even small modifications to prompts can result in relatively large changes in predictions. Table 6 demonstrates cases where modification of one word (either function or content word) leads to significant accuracy
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance for different top- $K$ ensembles.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">Modifications</th>
<th style="text-align: right;">Acc. Gain</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">P413</td>
<td style="text-align: left;">$x$ plays in $\rightarrow$ at $y$ position</td>
<td style="text-align: right;">+23.2</td>
</tr>
<tr>
<td style="text-align: left;">P495</td>
<td style="text-align: left;">$x$ was created $\rightarrow$ made in $y$</td>
<td style="text-align: right;">+10.8</td>
</tr>
<tr>
<td style="text-align: left;">P495</td>
<td style="text-align: left;">$x$ was $\rightarrow$ is created in $y$</td>
<td style="text-align: right;">+10.0</td>
</tr>
<tr>
<td style="text-align: left;">P361</td>
<td style="text-align: left;">$x$ is a part of $y$</td>
<td style="text-align: right;">+2.7</td>
</tr>
<tr>
<td style="text-align: left;">P413</td>
<td style="text-align: left;">$x$ plays in $y$ position</td>
<td style="text-align: right;">+2.2</td>
</tr>
</tbody>
</table>
<p>Table 6: Small modifications (update, insert, and delete) in paraphrase lead to large accuracy gain (\%).</p>
<p>improvements, indicating that large-scale LMs are still brittle to small changes in the ways they are queried.</p>
<p>Middle-word vs. Dependency-based We compare the performance of only using middleword prompts and concatenating them with dependency-based prompts in Table 7. The</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompts</th>
<th style="text-align: center;">Top1</th>
<th style="text-align: center;">Top3</th>
<th style="text-align: center;">Top5</th>
<th style="text-align: center;">Opti.</th>
<th style="text-align: center;">Oracle</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mid</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">45.1</td>
</tr>
<tr>
<td style="text-align: left;">Mid+Dep</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">50.7</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study of middle-word and dependency-based prompts on BERT-base.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Man</th>
<th style="text-align: center;">Mine</th>
<th style="text-align: center;">Mine <br> +Man</th>
<th style="text-align: center;">Mine <br> +Para</th>
<th style="text-align: center;">Man <br> +Para</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">37.3</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">41.1</td>
</tr>
<tr>
<td style="text-align: left;">KnowBert</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">32.1</td>
</tr>
</tbody>
</table>
<p>Table 8: Micro-averaged accuracy (\%) of various LMs
improvements confirm our intuition that words belonging to the dependency path but not in the middle of the subject and object are also indicative of the relation.</p>
<p>Micro vs. Macro Comparing Tables 2 and 3, we can see that macro-averaged accuracy is much lower than micro-averaged accuracy, indicating that macro-averaged accuracy is a more challenging metric that evaluates how many unique objects LMs know. Our optimizationbased method improves macro-averaged accuracy from $22.8 \%$ to $25.7 \%$ on BERT-base, and from $25.7 \%$ to $30.1 \%$ on BERT-base. This again confirms the effectiveness of ensembling multiple prompts, but the gains are somewhat smaller. Notably, in our optimization-based methods, the ensemble weights are optimized on each example in the training set, which is more conducive to optimizing micro-averaged accuracy. Optimization to improve macroaveraged accuracy is potentially an interesting direction for future work that may result in prompts more generally applicable to different types of objects.</p>
<p>Performance of Different LMs In Table 8, we compare BERT with ERNIE and KnowBert, which are enhanced with external knowledge by explicitly incorporating entity embeddings. ERNIE outperforms BERT by 1 point even with the manually defined prompts, but our prompt generation methods further emphasize the difference between the two methods, with the highest accuracy numbers differing by 4.2 points using the Mine+Man method. This</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Man</th>
<th style="text-align: center;">Mine</th>
<th style="text-align: center;">Mine <br> +Man</th>
<th style="text-align: center;">Mine <br> +Para</th>
<th style="text-align: center;">Man <br> +Para</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-base</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">27.0</td>
</tr>
<tr>
<td style="text-align: left;">BERT-large</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">29.8</td>
</tr>
</tbody>
</table>
<p>Table 9: Micro-averaged accuracy (\%) on LAMA-UHN.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Man</th>
<th style="text-align: center;">Mine</th>
<th style="text-align: center;">Mine <br> +Man</th>
<th style="text-align: center;">Mine <br> +Para</th>
<th style="text-align: center;">Man <br> +Para</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-base</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: left;">BERT-large</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">10.7</td>
</tr>
</tbody>
</table>
<p>Table 10: Micro-averaged accuracy (\%) on Google-RE.
indicates that if LMs are queried effectively, the differences between highly performant models may become more clear. KnowBert underperforms BERT on LAMA, which is opposite to the observation made in Peters et al. (2019). This is probably because that multi token subjects/objects are used to evaluate KnowBert in Peters et al. (2019), while LAMA contains only single-token objects.</p>
<p>LAMA-UHN Evaluation The performances on LAMA-UHN benchmark are reported in Table 9. Although the overall performances drop dramatically compared to the performances on the original LAMA benchmark (Table 2), optimized ensembles can still outperform manual prompts by a large margin, indicating that our methods are effective in retrieving knowledge that cannot be inferred based on surface forms.</p>
<h3>5.3 Analysis</h3>
<p>Next, we perform further analysis to better understand what type of prompts proved most suitable for facilitating retrieval of knowledge from LMs.</p>
<p>Prediction Consistency by Prompt We first analyze the conditions under which prompts will yield different predictions. We define the divergence between predictions of two prompts $t_{r, i}$ and $t_{r, j}$ using the following equation:
$\operatorname{Div}\left(t_{r, i}, t_{r, j}\right)=\frac{\sum_{(x, y) \in \mathcal{R}} \delta\left(C\left(x, y, t_{r, i}\right) \neq C\left(x, y, t_{r, j}\right)\right)}{|\mathcal{R}|}$,
where $C\left(x, y, t_{r, i}\right)=1$ if prompt $t_{r, i}$ can successfully predict $y$ and 0 otherwise, and $\delta(\cdot)$ is</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Correlation of edit distance between prompts and their prediction divergence.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$x / y \vee y / x$</th>
<th style="text-align: center;">$x / y \vee \mathrm{P} y / x$</th>
<th style="text-align: center;">$x / y \vee \mathrm{~W}^{*} \mathrm{P} y / x$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathrm{V}=$ verb particle? adv?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{W}=$ (noun | adj | adv | pron | det)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{P}=$ (prep | particle |inf. marker)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: Three part-of-speech-based regular expressions used in ReVerb to identify relational phrases.</p>
<p>Kronecker's delta. For each relation, we normalize the edit distance of two prompts into $[0,1]$ and bucket the normalized distance into five bins with intervals of 0.2 . We plot a box chart for each bin to visualize the distribution of prediction divergence in Figure 3, with the green triangles representing mean values and the green bars in the box representing median values. As the edit distance becomes larger, the divergence increases, which confirms our intuition that very different prompts tend to cause different prediction results. The Pearson correlation coefficient is 0.25 , which shows that there is a weak correlation between these two quantities.</p>
<p>Performance on Google-RE We also report the performance of optimized ensemble on the Google-RE subset in Table 10. Again, ensembling diverse prompts improves accuracies for both the BERT-base and BERT-large models. The gains are somewhat smaller than those on the T-REx subset, which might be caused by the fact that there are only three relations and one of them (predicting the birth_date of a person) is particularly hard to the extent that only one prompt yields non-zero accuracy.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ranking position distribution of prompts with different patterns. Lower is better.</p>
<p>POS-based Analysis Next, we try to examine which types of prompts tend to be effective in the abstract by examining the part-of-speech (POS) patterns of prompts that successfully extract knowledge from LMs. In open information extraction systems (Banko et al., 2007), manually defined patterns are often leveraged to filter out noisy relational phrases. For example, ReVerb (Fader et al., 2011) incorporates three syntactic constraints listed in Table 11 to improve the coherence and informativeness of the mined relational phrases. To test whether these patterns are also indicative of the ability of a prompt to retrieve knowledge from LMs, we use these three patterns to group prompts generated by our methods into four clusters, where the "other" cluster contains prompts that do not match any pattern. We then calculate the rank of each prompt within the extracted prompts, and plot the distribution of rank using box plots in Figure 4. ${ }^{8}$ We can see that the average rank of prompts matching these patterns is better than those in the "other" group, confirming our intuitions that good prompts should conform with those patterns. Some of the best performing prompts' POS signatures are " $x$ VBD VBN IN $y$ " (e.g., " $x$ was born in $y$ ") and " $x$ VBZ DT NN IN $y$ " (e.g., " $x$ is the capital of $y$ ").</p>
<p>Cross-model Consistency Finally, it is of interest to know whether the prompts that we are extracting are highly tailored to a</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test <br> Train</th>
<th style="text-align: center;">BERT-base <br> base</th>
<th style="text-align: center;">BERT-large <br> large</th>
<th style="text-align: center;">base</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mine</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: left;">Mine+Man</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: left;">Mine+Para</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">39.0</td>
</tr>
<tr>
<td style="text-align: left;">Man+Para</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">37.5</td>
</tr>
</tbody>
</table>
<p>Table 12: Cross-model micro-averaged accuracy (\%). The first row is the model to test, and the second row is the model on which prompt weights are learned.
specific model, or whether they can generalize across models. To do so, we use two settings: One compares BERT-base and BERT-large, the same model architecture with different sizes; the other compares BERT-base and ERNIE, different model architectures with a comparable size. In each setting, we compare when the optimization-based ensembles are trained on the same model, or when they are trained on one model and tested on the other. As shown in Tables 12 and 13, we found that in general there is usually some drop in performance in the cross-model scenario (third and fifth columns), but the losses tend to be small, and the highest performance when querying BERTbase is actually achieved by the weights optimized on BERT-large. Notably, the best accuracies of $40.1 \%$ and $42.2 \%$ (Table 12) and $39.5 \%$ and $40.5 \%$ (Table 13) with the weights optimized on the other model are still much higher than those obtained by the manual prompts, indicating that optimized prompts still afford large gains across models. Another interesting observation is that the drop in performance on ERNIE (last two columns in Table 13) is larger than that on BERT-large (last two columns in Table 12) using weights optimized on BERT-base, indicating that models sharing the same architecture benefit more from the same prompts.</p>
<p>Linear vs. Log-linear Combination As mentioned in $\S 4.2$, we use log-linear combination of probabilities in our main experiments. However, it is also possible to calculate probabilities through regular linear interpolation:</p>
<p>$$
P(y \mid x, r)=\sum_{i=1}^{K} \frac{1}{K} P_{\mathrm{LM}}\left(y \mid x, t_{r, i}\right)
$$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test <br> Train</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ERNIE</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">ERNIE</td>
<td style="text-align: center;">ERNIE</td>
<td style="text-align: center;">BERT</td>
</tr>
<tr>
<td style="text-align: left;">Mine</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: left;">Mine+Man</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">40.5</td>
</tr>
<tr>
<td style="text-align: left;">Mine+Para</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">39.0</td>
</tr>
<tr>
<td style="text-align: left;">Man+Para</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">40.3</td>
</tr>
</tbody>
</table>
<p>Table 13: Cross-model micro-averaged accuracy (\%). The first row is the model to test, and the second row is the model on which prompt weights are learned.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance of two interpolation methods.</p>
<p>We compare these two ways to combine predictions from multiple mined prompts in Figure 5 (§ 4.2). We assume that log-linear combination outperforms linear combination because log probabilities make it possible to penalize objects that are very unlikely given any certain prompt.</p>
<h2>6 Omitted Design Elements</h2>
<p>Finally, in addition to the elements of our main proposed methodology in $\S 3$ and $\S 4$, we experimented with a few additional methods that did not prove highly effective, and thus were omitted from our final design. We briefly describe these below, along with cursory experimental results.</p>
<h3>6.1 LM-aware Prompt Generation</h3>
<p>We examined methods to generate prompts by solving an optimization problem that maximizes the probability of producing the ground-truth objects with respect to the prompts:</p>
<p>$$
t_{r}^{*}=\arg \max <em r="r">{t</em>\right)
$$}} P_{\mathrm{LM}}\left(y \mid x, t_{r</p>
<p>where $P_{\mathrm{LM}}\left(y \mid x, t_{r}\right)$ is parameterized with a pretrained LM. In other words, this method directly searches for a prompt that causes the LM to assign ground-truth objects the highest probability.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompts</th>
<th style="text-align: center;">Top1</th>
<th style="text-align: center;">Top3</th>
<th style="text-align: center;">Top5</th>
<th style="text-align: center;">Opti.</th>
<th style="text-align: center;">Oracle</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">before</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">47.9</td>
</tr>
<tr>
<td style="text-align: left;">after</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">50.8</td>
</tr>
</tbody>
</table>
<p>Table 14: Micro-averaged accuracy (\%) before and after LM-aware prompt fine-tuning.</p>
<p>Solving this problem of finding text sequences that optimize some continuous objective has been studied both in the context of end-to-end sequence generation (Hoang et al., 2017), and in the context of making small changes to an existing input for adversarial attacks (Ebrahimi et al., 2018; Wallace et al., 2019). However, we found that directly optimizing prompts guided by gradients was unstable and often yielded prompts in unnatural English in our preliminary experiments. Thus, we instead resorted to a more straightforward hillclimbing method that starts with an initial prompt, then masks out one token at a time and replaces it with the most probable token conditioned on the other tokens, inspired by the mask-predict decoding algorithm used in non-autoregressive machine translation (Ghazvininejad et al., 2019): ${ }^{9}$</p>
<p>$$
P_{\mathrm{LM}}\left(w_{i} \mid t_{r} \backslash i\right)=\frac{\sum_{\langle x, y\rangle \in \mathcal{R}} P_{\mathrm{LM}}\left(w_{i} \mid x, t_{r} \backslash i, y\right)}{|\mathcal{R}|}
$$</p>
<p>where $w_{i}$ is the $i$-th token in the prompt and $t_{r} \backslash i$ is the prompt with the $i$-th token masked out. We followed a simple rule that modifies a prompt from left to right, and this is repeated until convergence.</p>
<p>We used this method to refine all the mined and manual prompts on the T-REx-train dataset, and display their performance on the T-REx dataset in Table 14. After fine-tuning, the oracle performance increased significantly, while the ensemble performances (both rank-based and optimizationbased) dropped slightly. This indicates that LM-aware fine-tuning has the potential to discover better prompts, but some portion of the refined prompts may have over-fit to the training set upon which they were optimized.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 15: Performance (\%) of using forward and backward features with BERT-base.</p>
<h3>6.2 Forward and Backward Probabilities</h3>
<p>Finally, given class imbalance and the propensity of the model to over-predict the majority object, we examine a method to encourage the model to predict subject-object pairs that are more strongly aligned. Inspired by the maximum mutual information objective used in Li et al. (2016a), we add the backward log probability $\log P_{\mathrm{LM}}\left(x \mid y, t_{r, i}\right)$ of each prompt to our optimization-based scoring function in Equation 3. Due to the large search space for objects, we turn to an approximation approach that only computes backward probability for the most probable $B$ objects given by the forward probability at both training and test time. As shown in Table 15, the improvement resulting from backward probability is small, indicating that a diversity-promoting scoring function might not be necessary for knowledge retrieval from LMs.</p>
<h2>7 Related Work</h2>
<p>Much work has focused on understanding the internal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017). For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a; Tenney et al., 2019b; Jawahar et al., 2019; Goldberg, 2019).</p>
<p>Different from analyses probing the representations themselves, our work follows Petroni et al. (2019); Pörner et al. (2019) in probing for factual</p>
<p>knowledge. They use manually defined prompts, which may be under-estimating the true performance obtainable by LMs. Concurrently to this work, Bouraoui et al. (2020) made a similar observation that using different prompts can help better extract relational knowledge from LMs, but they use models explicitly trained for relation extraction whereas our methods examine the knowledge included in LMs without any additional training.</p>
<p>Orthogonally, some previous works integrate external knowledge bases so that the language generation process is explicitly conditioned on symbolic knowledge (Ahn et al., 2016; Yang et al., 2017; Logan et al., 2019; Hayashi et al., 2020). Similar extensions have been applied to pre-trained LMs like BERT, where contextualized representations are enhanced with entity embeddings (Zhang et al., 2019; Peters et al., 2019; Pörner et al., 2019). In contrast, we focus on better knowledge retrieval through prompts from LMs as-is, without modifying them.</p>
<h2>8 Conclusion</h2>
<p>In this paper, we examined the importance of the prompts used in retrieving factual knowledge from language models. We propose mining-based and paraphrasing-based methods to systematically generate diverse prompts to query specific pieces of relational knowledge. Those prompts, when combined together, improve factual knowledge retrieval accuracy by $8 \%$, outperforming manually designed prompts by a large margin. Our analysis indicates that LMs are indeed more knowledgeable than initially indicated by previous results, but they are also quite sensitive to how we query them. This indicates potential future directions such as (1) more robust LMs that can be queried in different ways but still return similar results, (2) methods to incorporate factual knowledge in LMs, and (3) further improvements in optimizing methods to query LMs for knowledge. Finally, we have released all our learned prompts to the community as the LM Prompt and Query Archive (LPAQA), available at: https://github.com/jzbjyb/LPAQA.</p>
<h2>Acknowledgments</h2>
<p>This work was supported by a gift from Bosch Research and NSF award no. 1815287. We would like to thank Paul Michel, Hiroaki Hayashi,</p>
<p>Pengcheng Yin, and Shuyan Zhou for their insightful comments and suggestions.</p>
<h2>References</h2>
<p>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plaintext collections. In Proceedings of the Fifth ACM Conference on Digital Libraries, June 2-7, 2000, San Antonio, TX, USA, pages 85-94. ACM.</p>
<p>Sungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, and Yoshua Bengio. 2016. A neural knowledge language model. CoRR, abs/1608.00318v2.</p>
<p>Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2895-2905, Florence, Italy. Association for Computational Linguistics.</p>
<p>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pages 2670-2676.</p>
<p>Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. 2017. What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861-872, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Yonatan Belinkov and James R. Glass. 2019. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72.</p>
<p>Rahul Bhagat and Deepak Ravichandran. 2008. Large scale acquisition of paraphrases for learning surface patterns. In Proceedings</p>
<p>of ACL-08: HLT, pages 674-682, Columbus, Ohio. Association for Computational Linguistics.</p>
<p>Zied Bouraoui, Jose Camacho-Collados, and Steven Schockaert. 2020. Inducing relational knowledge from BERT. In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), New York, USA.</p>
<p>Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval. ACM, Computing Surveys, 44(1):1:1-1:50.</p>
<p>Andrew M. Dai and Quoc V. Le. 2015. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3079-3087.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186.</p>
<p>Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-box adversarial examples for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31-36, Melbourne, Australia, Association for Computational Linguistics.</p>
<p>Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018. T-REx: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018.</p>
<p>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open
information extraction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1535-1545.</p>
<p>Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference translations: Beyond language modeling. In Proceedings of EAMT, pages 103-111.</p>
<p>Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6114-6123, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yoav Goldberg. 2019. Assessing BERT's syntactic abilities. CoRR, abs/1901.05287v1.</p>
<p>Hiroaki Hayashi, Zecong Hu, Chenyan Xiong, and Graham Neubig. 2020. Latent relation language models. In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), New York, USA.</p>
<p>John Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4129-4138.</p>
<p>Cong Duy Vu Hoang, Gholamreza Haffari, and Trevor Cohn. 2017. Towards decoding as continuous optimisation in neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 146-156, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Robert L. Logan IV, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh.</p>
<ol>
<li>Barack's wife Hillary: Using knowledge graphs for fact-aware language modeling. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 5962-5971.</li>
</ol>
<p>Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. 2019. What does BERT learn about the structure of language? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 3651-3657.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016a. A diversitypromoting objective function for neural conversation models. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages 110-119.</p>
<p>Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Understanding neural networks through representation erasure. CoRR, abs/1612.08220v3.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521-535.</p>
<p>Jonathan Mallinson, Rico Sennrich, and Mirella Lapata. 2017. Paraphrasing revisited with neural machine translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 881-893, Valencia, Spain. Association for Computational Linguistics.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. CoRR, abs/1806.08730v1.</p>
<p>Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pages 51-61.</p>
<p>Gábor Melis, Chris Dyer, and Phil Blunsom. 2018. On the state of the art of evaluation in neural language models. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.</p>
<p>Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. Regularizing and optimizing LSTM language models. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.</p>
<p>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In 2012 IEEE Spoken Language Technology Workshop (SLT), pages 234-239. IEEE.</p>
<p>Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook FAIR's WMT19 news translation task submission. In Proceedings of the Fourth Conference on Machine Translation, WMT 2019, Florence, Italy, August 1-2, 2019 Volume 2: Shared Task Papers, Day 1, pages 314-319.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 2227-2237.</p>
<p>Matthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of the 2019 Conference</p>
<p>on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 43-54, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Nina Pörner, Ulli Waltinger, and Hinrich Schütze. 2019. BERT is not a knowledge base (yet): Factual knowledge vs. Namebased reasoning in unsupervised QA. CoRR, abs/1911.03681v1.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8).</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! Leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942, Florence, Italy. Association for Computational Linguistics.</p>
<p>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 41-47. Association for Computational Linguistics.</p>
<p>Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido Dagan, and Alberto Lavelli. 2006. Investigating a generic paraphrasebased approach for relation extraction. In 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Association for Computational Linguistics.</p>
<p>Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for if-then reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3027-3035.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers.</p>
<p>Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based neural MT learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526-1534, Austin, Texas. Association for Computational Linguistics.</p>
<p>Noah A. Smith, Chris Dyer, Miguel Ballesteros, Graham Neubig, Lingpeng Kong, and Adhiguna Kuncoro. 2017. What do recurrent neural network grammars learn about syntax? In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 1: Long Papers, pages 1249-1258.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019a. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4593-4601.</p>
<p>Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019b. What do you learn from context? Probing for sentence structure in contextualized word representations. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.</p>
<p>Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael</p>
<p>Gamon. 2015. Representing text for joint embedding of text and knowledge bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1499-1509.</p>
<p>Trieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning. CoRR, abs/1806.02847v2.</p>
<p>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153-2162, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Zichao Yang, Phil Blunsom, Chris Dyer, and Wang Ling. 2017. Reference-aware language models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 1850-1859.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 1441-1451.</p>
<p>Geoffrey Zweig and Christopher J. C. Burges. 2011. The Microsoft Research sentence completion challenge. Microsoft Research, Redmond, WA, USA, Technical Report MSR-TR-2011-129.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ In theory, this algorithm can be applied to both masked LMs like BERT and traditional left-to-right LMs, since the masked probability can be computed using Bayes' theorem for traditional LMs. However, in practice, due to the large size of vocabulary, it can only be approximated with beam search, or computed with more complicated continuous optimization algorithms (Hoang et al., 2017).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Some models we use in this paper, e.g., BERT (Devlin et al., 2019), are bi-directional, and do not directly define probability distribution over text, which is the underlying definition of an LM. Nonetheless, we call them LMs for simplicity.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-717 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-717</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-717</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-2ab44880e1763baf3d8753ccb43ad3bd5f122b70</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2ab44880e1763baf3d8753ccb43ad3bd5f122b70" target="_blank">Adversarial examples for models of code</a></p>
                <p><strong>Paper Venue:</strong> Proc. ACM Program. Lang.</p>
                <p><strong>Paper TL;DR:</strong> The main idea of the approach is to force a given trained model to make an incorrect prediction, as specified by the adversary, by introducing small perturbations that do not change the program’s semantics, thereby creating an adversarial example.</p>
                <p><strong>Paper Abstract:</strong> Neural models of code have shown impressive results when performing tasks such as predicting method names and identifying certain kinds of bugs. We show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code using adversarial examples. The main idea of our approach is to force a given trained model to make an incorrect prediction, as specified by the adversary, by introducing small perturbations that do not change the program’s semantics, thereby creating an adversarial example. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs (DAMP). DAMP works by deriving the desired prediction with respect to the model’s inputs, while holding the model weights constant, and following the gradients to slightly modify the input code. We show that our DAMP attack is effective across three neural architectures: code2vec, GGNN, and GNN-FiLM, in both Java and C#. Our evaluations demonstrate that DAMP has up to 89% success rate in changing a prediction to the adversary’s choice (a targeted attack) and a success rate of up to 94% in changing a given prediction to any incorrect prediction (a non-targeted attack). To defend a model against such attacks, we empirically examine a variety of possible defenses and discuss their trade-offs. We show that some of these defenses can dramatically drop the success rate of the attacker, with a minor penalty of 2% relative degradation in accuracy when they are not performing under attack. Our code, data, and trained models are available at https://github.com/tech-srl/adversarial-examples .</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e717.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e717.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>embedding_index_vs_onehot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding Lookup: index-based lookup versus explicit one-hot representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mismatch between common natural-language descriptions/tutorials (which describe embedding lookup as a dot-product with a one-hot vector) and actual model implementations (which perform direct index-based row lookup), with concrete consequences for computing gradients w.r.t. discrete inputs in adversarial example generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural models of code / DAMP adversarial attack pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline that computes gradients of a code-classification model w.r.t. input tokens/names to find discrete semantic-preserving perturbations (variable renames or dead-code names) that change model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>tutorials / guides on embedding lookup (and general descriptions in methods sections)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model implementation detail (embedding matrix lookup vs one-hot based API)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation-description mismatch (index lookup vs one-hot derivation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Many guides describe embedding lookup as taking the dot product of an embedding matrix with a one-hot vector, implying a differentiable input representation; however, practical implementations typically perform an index-based row lookup (no explicit one-hot), so deriving the loss with respect to the discrete index yields zero gradients. This prevents direct gradient-based computation on discrete inputs unless the input is explicitly represented as a differentiable distribution (one-hot) over the vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input representation / embedding lookup (first layer of the neural network)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>analytic inspection of gradient flow and implementation semantics (manual code/architecture analysis) combined with conceptual reasoning about gradients of index-based lookup</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>The paper reasons that deriving the loss w.r.t. an index is zero almost everywhere; the authors therefore switch to a one-hot distribution representation and demonstrate attack feasibility. Quantitative effect is shown indirectly by the success rates of DAMP once one-hot derivation is used (see impact).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Without representing inputs as differentiable one-hot distributions, gradient-based adversarial methods cannot compute nonzero gradients for discrete tokens, effectively preventing DAMP-style attacks; after adopting one-hot derivation, the authors achieved high attack success (targeted attacks up to 89% success and non-targeted up to 94% on models of code). Thus the mismatch determines whether gradient-based discrete attacks are possible.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as common: the paper notes 'some common guides and tutorials' recommend the dot-product / one-hot view, while practical implementations typically use index-based lookup. The mismatch is therefore widespread between documentation/tutorial descriptions and optimized code implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Difference between high-level pedagogical descriptions (which emphasize conceptual one-hot formulations) and low-level implementation optimizations (direct index-based embedding lookup) leading to differing differentiability properties.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Represent discrete inputs as differentiable distributions (explicit one-hot vectors or distributions over vocabulary) when computing gradients for adversarial manipulation; after gradient computation, project back to discrete tokens via argmax / top-k search.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Highly effective for enabling DAMP: using one-hot derivation allowed the authors to compute gradients and select candidate replacements, enabling targeted and non-targeted attacks with reported success rates (e.g., up to 89% targeted, up to 94% non-targeted across evaluated models).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning for source code (neural models of code, adversarial examples)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial examples for models of code', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e717.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e717.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>continuous-to-discrete_metric_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch of continuous-domain adversarial metrics and methods when applied to discrete code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual gap where metrics and adversarial-generation techniques developed for continuous domains (images/audio) — e.g., imperceptible L_p perturbations, FGSM-style additive noise — do not map to discrete program inputs because any change in code is perceptible and must preserve semantics (compilability), requiring different notions of minimality and different attack mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adversarial evaluation pipeline for neural models of code</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental framework that attempts to generate adversarial examples for code by applying semantic-preserving discrete transformations and measuring model robustness (targeted and non-targeted).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>prior literature assumptions / methods sections from image and NLP adversarial work</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>adversarial generation code and experimental attack procedures (DAMP implementation versus image/NLP adversarial code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>metric and method mismatch (continuous-domain assumptions not valid for discrete code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Standard adversarial definitions (small, imperceptible continuous perturbations measured by L_p norms) and techniques (FGSM, additive noise) assume continuous inputs and gradual, imperceptible changes; code is discrete and must preserve exact semantics (compile/run equivalence), so those metrics and perturbation models do not apply. Consequently, notions such as 'imperceptibility' and minimal L_p change do not meaningfully quantify similarity between original and perturbed programs; the paper therefore adopts atomic semantic-preserving transformations (single variable rename or single dead-code insertion) as a proxy for minimal perturbation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>attack/perturbation model and similarity metrics used for defining adversarial examples</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Conceptual analysis contrasting continuous adversarial literature with properties of code; empirical observation that continuous techniques are inapplicable and that existing NLP perturbation methods only permit non-targeted attacks or break semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>The paper operationalizes similarity by restricting to single semantic-preserving transformations and measures attack success rates (e.g., DAMP targeted up to 89% and non-targeted up to 94% on some models) under that discrete similarity constraint. It also reports robustness (percent of examples not changed) under attacks and under defenses to quantify impact.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Because continuous-domain metrics are inapplicable, existing image/NLP adversarial methods are insufficient; the authors developed DAMP, tailored for discrete code, and demonstrated substantial vulnerability. The change in metric/definition directly affects what counts as an adversarial example and consequently the measured robustness of models.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Intrinsic to discrete-program domains: the gap applies broadly whenever adversarial methods from continuous domains are directly ported to code; the paper treats this as a general, domain-level mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Fundamental data-type difference (continuous vs discrete) and semantic constraints of programs (compilability, exact behavior), together with prior work's implicit assumptions about imperceptibility and continuous perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Redefine similarity and perturbation models for code (semantic-preserving discrete transformations); develop discrete-gradient methods (derive w.r.t. one-hot distributions) and constrained search (BFS over top-k candidates, depth limits); evaluate defenses such as Outlier Detection and Adversarial Training tuned for discrete perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Mitigations (Outlier Detection, Adversarial Training) notably reduced attack success: Outlier Detection achieved high robustness (e.g., ~74.35% robustness for non-targeted VarName while incurring ~2% F1 degradation), and Adversarial Training also achieved a favorable trade-off (F1 ~94.98 and robustness up to ~99% for some attacks against DeadCode). These numbers show that adopting domain-specific defenses is effective.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>adversarial machine learning; discrete domains (source code models)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial examples for models of code', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e717.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e717.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>transform_spec_vs_safe_impl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specification of semantic-preserving transformations versus safe implementability in code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gap between natural-language claims that many semantic-preserving program transformations exist (e.g., statement re-ordering) and the paper's practical constraint that only a small subset (variable renaming, dead-code insertion) can be safely and reliably applied without deeper program analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adversarial program transformation subsystem (transformation selection in DAMP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Component that enumerates and applies semantic-preserving program transformations to generate candidate inputs for model evaluation and adversarial search; in practice limited to transformations guaranteed safe without heavy analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper claims / literature statements about semantic-preserving transformations</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>program transformation code (rename, insert dead-code; other transforms omitted due to semantic-safety requirements)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / implementation restriction (claim vs safe implementability)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>While many transformations are described in natural language as semantic-preserving (e.g., re-ordering independent statements, operator swapping), their safe automated application requires program analysis to ensure semantics are preserved; the authors therefore restrict the implemented perturbations to two transformations (variable renaming and dead-code insertion) that can be applied without deep analysis. This represents a pragmatic gap between the broader set of transformations often mentioned in text and the small, implementable subset.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>input perturbation/transformation stage (transformation catalog and application logic)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Design-time reasoning and conservative implementation choice: authors explicitly discuss which transforms require deeper analysis and therefore are excluded from the implemented attack set.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Measured implicitly via attack success when constrained to the safe transform set (single-variable rename or single-dead-code insertion). Attack success rates reported under these constraints quantify the practical power of the restricted transform set (e.g., DAMP achieves high success even with just one rename).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Limiting transforms reduces the adversary's search space and simplifies correctness guarantees (preservation of semantics), but DAMP remains highly effective even under this restriction (targeted up to 89%, non-targeted up to 94% in reported settings). The restriction also makes defense evaluation tractable and meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common in practice: many papers and descriptions list many semantic-preserving transformations, but practical automated pipelines often implement only a safe subset unless full program analysis is available.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or high-level natural-language descriptions of permissible transformations omit the complexity of guaranteeing semantic preservation automatically; implementing the full set requires program-analysis infrastructure that may be costly or undecidable in general.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt a conservative, well-specified subset of transformations that are provably semantics-preserving without deep analysis (variable renames, dead-code insertions); when broader transforms are desired, invest in program-analysis tooling to ensure safety.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in practice: using only the conservative transforms still allowed the authors to generate many successful adversarial examples; the approach trades off breadth of attack for semantic safety and implementability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>program analysis / automated program transformation applied to machine-learning-based code tools</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial examples for models of code', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explaining and Harnessing Adversarial Examples <em>(Rating: 2)</em></li>
                <li>Intriguing properties of neural networks <em>(Rating: 2)</em></li>
                <li>HotFlip: White-Box Adversarial Examples for Text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-717",
    "paper_id": "paper-2ab44880e1763baf3d8753ccb43ad3bd5f122b70",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "embedding_index_vs_onehot",
            "name_full": "Embedding Lookup: index-based lookup versus explicit one-hot representation",
            "brief_description": "A mismatch between common natural-language descriptions/tutorials (which describe embedding lookup as a dot-product with a one-hot vector) and actual model implementations (which perform direct index-based row lookup), with concrete consequences for computing gradients w.r.t. discrete inputs in adversarial example generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Neural models of code / DAMP adversarial attack pipeline",
            "system_description": "A pipeline that computes gradients of a code-classification model w.r.t. input tokens/names to find discrete semantic-preserving perturbations (variable renames or dead-code names) that change model predictions.",
            "nl_description_type": "tutorials / guides on embedding lookup (and general descriptions in methods sections)",
            "code_implementation_type": "model implementation detail (embedding matrix lookup vs one-hot based API)",
            "gap_type": "implementation-description mismatch (index lookup vs one-hot derivation)",
            "gap_description": "Many guides describe embedding lookup as taking the dot product of an embedding matrix with a one-hot vector, implying a differentiable input representation; however, practical implementations typically perform an index-based row lookup (no explicit one-hot), so deriving the loss with respect to the discrete index yields zero gradients. This prevents direct gradient-based computation on discrete inputs unless the input is explicitly represented as a differentiable distribution (one-hot) over the vocabulary.",
            "gap_location": "model input representation / embedding lookup (first layer of the neural network)",
            "detection_method": "analytic inspection of gradient flow and implementation semantics (manual code/architecture analysis) combined with conceptual reasoning about gradients of index-based lookup",
            "measurement_method": "The paper reasons that deriving the loss w.r.t. an index is zero almost everywhere; the authors therefore switch to a one-hot distribution representation and demonstrate attack feasibility. Quantitative effect is shown indirectly by the success rates of DAMP once one-hot derivation is used (see impact).",
            "impact_on_results": "Without representing inputs as differentiable one-hot distributions, gradient-based adversarial methods cannot compute nonzero gradients for discrete tokens, effectively preventing DAMP-style attacks; after adopting one-hot derivation, the authors achieved high attack success (targeted attacks up to 89% success and non-targeted up to 94% on models of code). Thus the mismatch determines whether gradient-based discrete attacks are possible.",
            "frequency_or_prevalence": "Described as common: the paper notes 'some common guides and tutorials' recommend the dot-product / one-hot view, while practical implementations typically use index-based lookup. The mismatch is therefore widespread between documentation/tutorial descriptions and optimized code implementations.",
            "root_cause": "Difference between high-level pedagogical descriptions (which emphasize conceptual one-hot formulations) and low-level implementation optimizations (direct index-based embedding lookup) leading to differing differentiability properties.",
            "mitigation_approach": "Represent discrete inputs as differentiable distributions (explicit one-hot vectors or distributions over vocabulary) when computing gradients for adversarial manipulation; after gradient computation, project back to discrete tokens via argmax / top-k search.",
            "mitigation_effectiveness": "Highly effective for enabling DAMP: using one-hot derivation allowed the authors to compute gradients and select candidate replacements, enabling targeted and non-targeted attacks with reported success rates (e.g., up to 89% targeted, up to 94% non-targeted across evaluated models).",
            "domain_or_field": "machine learning / deep learning for source code (neural models of code, adversarial examples)",
            "reproducibility_impact": true,
            "uuid": "e717.0",
            "source_info": {
                "paper_title": "Adversarial examples for models of code",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "continuous-to-discrete_metric_gap",
            "name_full": "Mismatch of continuous-domain adversarial metrics and methods when applied to discrete code",
            "brief_description": "A conceptual gap where metrics and adversarial-generation techniques developed for continuous domains (images/audio) — e.g., imperceptible L_p perturbations, FGSM-style additive noise — do not map to discrete program inputs because any change in code is perceptible and must preserve semantics (compilability), requiring different notions of minimality and different attack mechanics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Adversarial evaluation pipeline for neural models of code",
            "system_description": "Experimental framework that attempts to generate adversarial examples for code by applying semantic-preserving discrete transformations and measuring model robustness (targeted and non-targeted).",
            "nl_description_type": "prior literature assumptions / methods sections from image and NLP adversarial work",
            "code_implementation_type": "adversarial generation code and experimental attack procedures (DAMP implementation versus image/NLP adversarial code)",
            "gap_type": "metric and method mismatch (continuous-domain assumptions not valid for discrete code)",
            "gap_description": "Standard adversarial definitions (small, imperceptible continuous perturbations measured by L_p norms) and techniques (FGSM, additive noise) assume continuous inputs and gradual, imperceptible changes; code is discrete and must preserve exact semantics (compile/run equivalence), so those metrics and perturbation models do not apply. Consequently, notions such as 'imperceptibility' and minimal L_p change do not meaningfully quantify similarity between original and perturbed programs; the paper therefore adopts atomic semantic-preserving transformations (single variable rename or single dead-code insertion) as a proxy for minimal perturbation.",
            "gap_location": "attack/perturbation model and similarity metrics used for defining adversarial examples",
            "detection_method": "Conceptual analysis contrasting continuous adversarial literature with properties of code; empirical observation that continuous techniques are inapplicable and that existing NLP perturbation methods only permit non-targeted attacks or break semantics.",
            "measurement_method": "The paper operationalizes similarity by restricting to single semantic-preserving transformations and measures attack success rates (e.g., DAMP targeted up to 89% and non-targeted up to 94% on some models) under that discrete similarity constraint. It also reports robustness (percent of examples not changed) under attacks and under defenses to quantify impact.",
            "impact_on_results": "Because continuous-domain metrics are inapplicable, existing image/NLP adversarial methods are insufficient; the authors developed DAMP, tailored for discrete code, and demonstrated substantial vulnerability. The change in metric/definition directly affects what counts as an adversarial example and consequently the measured robustness of models.",
            "frequency_or_prevalence": "Intrinsic to discrete-program domains: the gap applies broadly whenever adversarial methods from continuous domains are directly ported to code; the paper treats this as a general, domain-level mismatch.",
            "root_cause": "Fundamental data-type difference (continuous vs discrete) and semantic constraints of programs (compilability, exact behavior), together with prior work's implicit assumptions about imperceptibility and continuous perturbations.",
            "mitigation_approach": "Redefine similarity and perturbation models for code (semantic-preserving discrete transformations); develop discrete-gradient methods (derive w.r.t. one-hot distributions) and constrained search (BFS over top-k candidates, depth limits); evaluate defenses such as Outlier Detection and Adversarial Training tuned for discrete perturbations.",
            "mitigation_effectiveness": "Mitigations (Outlier Detection, Adversarial Training) notably reduced attack success: Outlier Detection achieved high robustness (e.g., ~74.35% robustness for non-targeted VarName while incurring ~2% F1 degradation), and Adversarial Training also achieved a favorable trade-off (F1 ~94.98 and robustness up to ~99% for some attacks against DeadCode). These numbers show that adopting domain-specific defenses is effective.",
            "domain_or_field": "adversarial machine learning; discrete domains (source code models)",
            "reproducibility_impact": true,
            "uuid": "e717.1",
            "source_info": {
                "paper_title": "Adversarial examples for models of code",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "transform_spec_vs_safe_impl",
            "name_full": "Specification of semantic-preserving transformations versus safe implementability in code",
            "brief_description": "A gap between natural-language claims that many semantic-preserving program transformations exist (e.g., statement re-ordering) and the paper's practical constraint that only a small subset (variable renaming, dead-code insertion) can be safely and reliably applied without deeper program analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Adversarial program transformation subsystem (transformation selection in DAMP)",
            "system_description": "Component that enumerates and applies semantic-preserving program transformations to generate candidate inputs for model evaluation and adversarial search; in practice limited to transformations guaranteed safe without heavy analysis.",
            "nl_description_type": "paper claims / literature statements about semantic-preserving transformations",
            "code_implementation_type": "program transformation code (rename, insert dead-code; other transforms omitted due to semantic-safety requirements)",
            "gap_type": "incomplete specification / implementation restriction (claim vs safe implementability)",
            "gap_description": "While many transformations are described in natural language as semantic-preserving (e.g., re-ordering independent statements, operator swapping), their safe automated application requires program analysis to ensure semantics are preserved; the authors therefore restrict the implemented perturbations to two transformations (variable renaming and dead-code insertion) that can be applied without deep analysis. This represents a pragmatic gap between the broader set of transformations often mentioned in text and the small, implementable subset.",
            "gap_location": "input perturbation/transformation stage (transformation catalog and application logic)",
            "detection_method": "Design-time reasoning and conservative implementation choice: authors explicitly discuss which transforms require deeper analysis and therefore are excluded from the implemented attack set.",
            "measurement_method": "Measured implicitly via attack success when constrained to the safe transform set (single-variable rename or single-dead-code insertion). Attack success rates reported under these constraints quantify the practical power of the restricted transform set (e.g., DAMP achieves high success even with just one rename).",
            "impact_on_results": "Limiting transforms reduces the adversary's search space and simplifies correctness guarantees (preservation of semantics), but DAMP remains highly effective even under this restriction (targeted up to 89%, non-targeted up to 94% in reported settings). The restriction also makes defense evaluation tractable and meaningful.",
            "frequency_or_prevalence": "Common in practice: many papers and descriptions list many semantic-preserving transformations, but practical automated pipelines often implement only a safe subset unless full program analysis is available.",
            "root_cause": "Ambiguous or high-level natural-language descriptions of permissible transformations omit the complexity of guaranteeing semantic preservation automatically; implementing the full set requires program-analysis infrastructure that may be costly or undecidable in general.",
            "mitigation_approach": "Adopt a conservative, well-specified subset of transformations that are provably semantics-preserving without deep analysis (variable renames, dead-code insertions); when broader transforms are desired, invest in program-analysis tooling to ensure safety.",
            "mitigation_effectiveness": "Effective in practice: using only the conservative transforms still allowed the authors to generate many successful adversarial examples; the approach trades off breadth of attack for semantic safety and implementability.",
            "domain_or_field": "program analysis / automated program transformation applied to machine-learning-based code tools",
            "reproducibility_impact": true,
            "uuid": "e717.2",
            "source_info": {
                "paper_title": "Adversarial examples for models of code",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explaining and Harnessing Adversarial Examples",
            "rating": 2
        },
        {
            "paper_title": "Intriguing properties of neural networks",
            "rating": 2
        },
        {
            "paper_title": "HotFlip: White-Box Adversarial Examples for Text",
            "rating": 1
        }
    ],
    "cost": 0.015262,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Adversarial Examples for Models of Code</h1>
<p>NOAM YEFET, Technion, Israel<br>URI ALON, Technion, Israel<br>ERAN YAHAV, Technion, Israel</p>
<p>Neural models of code have shown impressive results when performing tasks such as predicting method names and identifying certain kinds of bugs. We show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code using adversarial examples. The main idea of our approach is to force a given trained model to make an incorrect prediction, as specified by the adversary, by introducing small perturbations that do not change the program's semantics, thereby creating an adversarial example. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs (DAMP). DAMP works by deriving the desired prediction with respect to the model's inputs, while holding the model weights constant, and following the gradients to slightly modify the input code.</p>
<p>We show that our DAMP attack is effective across three neural architectures: CODE2vec, GGNN, and GNN-FiLM, in both Java and C#. Our evaluations demonstrate that DAMP has up to $89 \%$ success rate in changing a prediction to the adversary's choice (a targeted attack) and a success rate of up to $94 \%$ in changing a given prediction to any incorrect prediction (a non-targeted attack). To defend a model against such attacks, we empirically examine a variety of possible defenses and discuss their trade-offs. We show that some of these defenses can dramatically drop the success rate of the attacker, with a minor penalty of $2 \%$ relative degradation in accuracy when they are not performing under attack.</p>
<p>Our code, data, and trained models are available at https://github.com/tech-srl/adversarial-examples .
Additional Key Words and Phrases: Adversarial Attacks, Targeted Attacks, Neural Models of Code</p>
<h2>1 INTRODUCTION</h2>
<p>Neural models of code have achieved state-of-the-art performance on various tasks such as the prediction of variable names and types [Allamanis et al. 2018; Alon et al. 2018; Bielik et al. 2016; Raychev et al. 2015], code summarization [Allamanis et al. 2016; Alon et al. 2019a; Fernandes et al. 2019], code generation [Alon et al. 2019b; Brockschmidt et al. 2019; Murali et al. 2017], code search [Cambronero et al. 2019; Liu et al. 2019; Sachdev et al. 2018], and bug finding [Pradel and Sen 2018; Rice et al. 2017; Scott et al. 2019].</p>
<p>In other domains such as computer vision, deep models have been shown to be vulnerable to adversarial examples [Goodfellow et al. 2014b; Szegedy et al. 2013]. Adversarial examples are inputs crafted by an adversary to force a trained neural model to make a certain (incorrect) prediction. The generation of adversarial examples was demonstrated for image classification [Goodfellow et al. 2014a,b; Kurakin et al. 2016; Mirza and Osindero 2014; Moosavi-Dezfooli et al. 2016; Nguyen et al. 2015; Papernot et al. 2017, 2016; Szegedy et al. 2013] and for other domains [Alzantot et al. 2018a,b; Belinkov and Bisk 2017; Carlini and Wagner 2018; Ebrahimi et al. 2017; Pruthi et al. 2019; Taori et al. 2019]. The basic idea underlying many of the techniques is to add specially-crafted noise to a correctly labeled input, such that the model under attack yields a desired incorrect label when presented with the modified input (i.e., with the addition of noise). Adding noise to a continuous object to change the prediction of a model is relatively easy to achieve mathematically. For example, for an image, this can be achieved by changing the intensity of pixel values [Goodfellow et al. 2014b; Szegedy et al. 2013]. Unfortunately, this does not carry over to the domain of programs, since a program is a discrete object that must maintain semantic properties.</p>
<p>Authors' addresses: Noam Yefet, Technion, Israel, snyefet@cs.technion.ac.il; Uri Alon, Technion, Israel, urialon@cs.technion. ac.il; Eran Yahav, Technion, Israel, yahave@cs.technion.ac.il.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. A Java snippet $f 1$ is classified correctly as sort by the model of code2vec.org. Given $f 1$ and the target contains, our approach generates $f 2$ by renaming array to ttypes. Given the target escape, our approach generates $f 3$ by adding an unused variable declaration of int upperhexdigits. Additional examples can be found in Appendix A.</p>
<p>In this paper, we present a novel approach for generating adversarial examples for neural models of code. More formally:</p>
<h1>1.1 Goal</h1>
<p>Given a program $\mathcal{P}$ and a correct prediction $y$ made by a model $\mathcal{M}$, such that: $\mathcal{M}(\mathcal{P})=y$, our goal is to find a semantically equivalent program $\mathcal{P}^{\prime}$ such that $\mathcal{M}$ makes a given adversarial prediction $y_{\text {bad }}$ of the adversary's choice: $\mathcal{M}\left(\mathcal{P}^{\prime}\right)=y_{\text {bad }}$.</p>
<p>The main challenge in tackling the above goal lies in exploring the vast space of programs that are semantically equivalent to $\mathcal{P}$, and finding a program for which $\mathcal{M}$ will predict $y_{\text {bad }}$.</p>
<p>Generally, we can define a set of semantic-preserving transformations, which in turn induce a space of semantically equivalent programs. For example, we can: (i) rename variables and (ii) add dead code. There are clearly many other semantic preserving transformations (e.g., re-ordering independent statements), but their application would require a deeper analysis of the program to guarantee that they are indeed semantic preserving. In this paper, therefore, we focus on the above two semantic-preserving transformations, which can be safely applied without any semantic analysis.</p>
<p>One naïve approach for exploring the space of equivalent programs is to randomly apply transformations using brute-force. We can apply transformations randomly to generate new programs and use the model to make a prediction for each generated program. However, the program space to be explored is exponentially large, making exhaustive exploration prohibitively expensive.</p>
<h3>1.2 Our Approach</h3>
<p>We present a new technique called Discrete Adversarial Manipulation of Programs (DAMP). The main idea in DAMP is to select semantic preserving perturbations by deriving the output distribution of the model with respect to the model's input and following the gradient to modify the input, while keeping the model weights constant. Given a desired adversarial label $y_{b a d}$ and an existing variable name, we derive the loss of the model with $y_{b a d}$ as the correct label, with respect to the one-hot vector of the input variable. We then take the argmax of the resulting gradient to select</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Perturbing a variable name: the original variable name is represented as a one-hot vector over the variable-name vocabulary. After perturbation, the vector is no longer one-hot. We apply argmax to find the most likely adversarial name, resulting with another one-hot vector over the variable-name vocabulary.
an alternative variable name, rename the original variable to the alternative name, check whether this modification changes the output label to the desired adversarial label, and continue iteratively. This process is illustrated in Figure 2, and detailed in Section 4.</p>
<p>This iterative process allows DAMP to modify the program in a way that preserves its semantics but will cause a model to make adversarial predictions. We show that models of code are susceptible to targeted attacks that force a model to make a specific incorrect prediction chosen by the adversary, as well as to simpler non-targeted attacks that force a model to make any incorrect prediction without a specific target prediction in mind. Our approach is a "white-box" approach, since it assumes the attacker has access to either the model under attack or to a similar model. ${ }^{1}$ Under this assumption, our approach is general and is applicable to any model that can be derived with respect to its inputs i.e., any neural model. We do not make any assumptions about the internal details or specific architecture of the model under attack.</p>
<p>To mitigate these attacks, we evaluate and compare a variety of defensive approaches. Some of these defenses work by re-training the model using another loss function or a modified version of the same dataset. Other defensive approaches are "modular", in the sense that they can be placed in front of an already-trained model, identify perturbations in the input, and feed a masked version of the input into the vulnerable model. These defense mechanisms allow us to trade off the accuracy of the original model for improved robustness.
Main Contributions The contributions of this paper are:</p>
<ul>
<li>The first technique for generating targeted adversarial examples for models of code. Our technique, called Discrete Adversarial Manipulation of Programs (DAMP), is general and only requires that the attacker is able to compute gradients in the model under attack (or in a similar model). DAMP is effective in generating both targeted and non-targeted attacks.</li>
<li>An experimental evaluation of attacks on three neural architectures: code2vec [Alon et al. 2019c], GGNN [Allamanis et al. 2018], and GNN-FiLM [Brockschmidt et al. 2019] in two</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. A C# VARMisuse example which is classified correctly as DestinationType in the method Equals by the GGNN model of Allamanis et al. [2018]. Given the code in Figure 3a and the target SourceType, our approach renames a local variable destination in another method to the specific name scsqbhj, making the model predict the wrong variable in the method Equals, thus ("maliciously") introducing a real bug in the method Equals. Additional examples are shown in Appendix A.
languages: Java and C#. Our evaluation shows that our adversarial technique can change a prediction according to the adversary's will ("targeted attack") up to $89 \%$ of the time, and is successful in changing a given prediction to an incorrect prediction ("non-targeted attack") $94 \%$ of the time.</p>
<ul>
<li>A thorough evaluation of techniques for defending models of code against attacks that perturb names, and an analysis of their trade-offs. When some of these defenses are used, the success rate of the attack drops drastically for both targeted and non-targeted attacks, with a minor penalty of $2 \%$ in accuracy.</li>
</ul>
<h1>2 OVERVIEW</h1>
<p>In this section, we provide an informal overview.</p>
<h3>2.1 Motivating Examples</h3>
<p>We begin by demonstrating our technique on two examples, which address two different tasks, using two different neural models, and in two programming languages (Java and C#).
Bypass Semantic Labeling (code2vec - Java) We demonstrate how our approach can force the CODE2VEC [Alon et al. 2019c] model to predict a label of our choice. Consider the code snippet $f 1$ of Figure 1. This code snippet sorts a given array. The code2vec model [Alon et al. 2019c] applied to this code snippet predicts the correct name, sort, with a probability of $98.54 \%$.</p>
<p>Given the code snippet $f 1$ and the arbitrarily chosen adversarial target contains, our approach finds that renaming the original variable array to ttypes in $f 2$ forces the model to predict the</p>
<p>label contains with an even higher probability, although the functionality remained the same. We denote this as a VarName attack.</p>
<p>Given the code snippet $f 1$ and the adversarial target escape, our approach finds that adding an unused variable (i.e., dead code) named upperhexdigits to the end of f1 forces the model to predict the label escape with a probability of $100 \%$ (see f 3 in Figure 1). We denote this attack as a DeadCode targeted attack.</p>
<p>In general, bypassing semantic labeling models can allow a variety of malicious behaviors. For example, a malicious developer can spread malicious code to users by having the code classified as "benign" by a malware detection model [Arp et al. 2014; David et al. 2019; Grosse et al. 2016; Saxe and Berlin 2015; Wang et al. 2017]. In this paper, we demonstrate the ability to bypass semantic labeling by applying our attack on the code2vec model (Figure 1), forcing the model to predict a label of our choice.
Bypass Bug Detection (VarMisuse - C#) As another example, we demonstrate how our approach can force a Graph Neural Network (GNN) model to choose an obvious bug as the correct completion. In Figure 3a, a GNN model trained on the VarMisuse task [Allamanis et al. 2018; Brockschmidt 2019] in C# correctly chooses to "fill the blank" using the field DestinationType inside the method Equals. By renaming a local variable called destination in another method to the specific name scsqbhj (Figure 3b), the model chooses the incorrect field SourceType in the method Equals. The fields DestinationType (correct) and SourceType (incorrect) both have the same type; thus, the code still compiles and the attack causes a real bug in Equals.</p>
<p>More generally, bypassing a bug detection model [Pradel and Sen 2018; Rice et al. 2017; Scott et al. 2019] can allow a malicious developer inside an organization or inside an open-source project to intentionally introduce bugs. In this paper, we demonstrate this ability using the VarMisuse on Graph Neural Networks (GNNs) (Figure 20), forcing the model to choose an incorrect (but type-correct) variable.</p>
<p>In addition to the code2vec and VarMisuse tasks that we address in this paper, we believe adversarial examples can be applied to neural code search [Cambronero et al. 2019; Liu et al. 2019; Sachdev et al. 2018]. A developer can attract users to a specific library or an open-source project by introducing code that will be disproportionately highly ranked by a neural code search model.</p>
<h1>2.2 Discrete Adversarial Manipulation of Programs (DAMP)</h1>
<p>Consider the code snippet $f 1$ of Figure 1 that sorts a given array. The code2vec model [Alon et al. 2019c] applied to this code snippet predicts the correct name, sort. Our goal is to find semantically equivalent snippets that will cause an underlying model to yield an incorrect target prediction of our choice.
Gradient-Based Exploration of the Program Space We need a way to guide exploration of the program space towards a specific desired target label (in a targeted attack), or away from the original label (in a non-targeted attack).</p>
<p>In standard stochastic gradient descent (SGD)-based training of neural networks, the weights of the network are updated to minimize the loss function. The gradient is used to guide the update of the network weights to minimize the loss. However, what we are trying to determine is not an update of the network's weights, but rather an "update" of the network's inputs. A natural way to obtain such guidance is to derive the desired prediction with respect to the model's inputs while holding the model weights constant and follow the gradient to modify the inputs.</p>
<p>In settings where the input is continuous (e.g., images), modifying the input can be done directly by adding a small noise value and following the direction of the gradient towards the desired target label (targeted), or away from the original label (non-targeted). A common technique used</p>
<p>for images is the Fast Gradient Signed Method (FGSM) [Goodfellow et al. 2014b] approach, which modifies the input using a small fixed $\epsilon$ value.
Deriving with Respect to a Discrete Input In settings where the input is discrete, the first layer of a neural network is typically an embedding layer that embeds discrete objects, such as names and tokens, into a continuous space [Allamanis et al. 2016; Alon et al. 2019a; Iyer et al. 2016]. The input is the index of the symbol, which is used to look up its embedding in the embedding matrix. The question for discrete inputs is therefore: what does it mean to derive with respect to the model's inputs?</p>
<p>One approach is to derive with respect to the embedding vector, which is the result of the embedding layer. In this approach, after the gradient is obtained, we need to reflect the update of the embedding vector back to discrete-input space. This can be done by looking for the nearestneighbors of the updated embedding vector in the original embedding space, and finding a nearby vector that has a corresponding discrete input. In this approach, there is no guarantee that following the gradient is the best step.</p>
<p>In contrast, our Discrete Adversarial Manipulation of Programs (DAMP) approach derives with respect to a one-hot vector that represents the distribution over discrete values (e.g., over variable names). Instead of deriving by the input itself, the gradient is taken with respect to the distribution over the inputs. Intuitively, this allows us to directly obtain the best discrete value for following the gradient.
Targeted Gradient-based Attack Using our gradient-based method, we explore the space of semantically equivalent programs directly toward a desired adversarial target. For example, given the code snippet $f 1$ of Figure 1 and the desired target label contains, our approach for generating adversarial examples automatically infers the snippet $f 2$ of Figure 1. Similarly, given the target label escape, our approach automatically infers the snippet $f 3$ of Figure 1.</p>
<p>All code snippets of Figure 1 are semantically equivalent. The only difference between $f 1$ and f 2 is the name of the variables. Specifically, these snippets differ only in the name of a single variable, which is named array in f 1 and ttypes in f 2 . Nevertheless, when array is renamed to ttypes, the prediction made by code2vec changes to the desired (adversarial) target label contains. The difference between $f 1$ and $f 3$ is the addition of a single variable declaration int upperhexdigits, which is never used in the code snippet. Nevertheless, adding this declaration changes the prediction made by the model to the desired (adversarial) target label escape.</p>
<h1>3 BACKGROUND</h1>
<p>In this section we provide fundamental background on neural networks and adversarial examples.</p>
<h3>3.1 Training Neural Networks</h3>
<p>A Neural Network (NN) model can be viewed as a function $f_{\theta}: X \rightarrow Y$, where $X$ is the input domain (image, text, code, etc.) and $Y$ is usually a finite set of labels. Assuming a perfect classifier $h^{<em>}: X \rightarrow Y$, the goal of the function $f_{\theta}$ is to assign the correct label $y \in Y$ (which is determined by $h^{</em>}$ ) for each input $x \in X$. In order to accomplish that, $f_{\theta}$ contains a set of trainable weights, denoted by $\theta$, which can be adjusted to fit a given labeled training set $T=\left{(x, y) \mid x \in \bar{X} \subset X, y \in Y, y=h^{*}(x)\right}$. The process of adjusting $\theta$ (i.e., training) is done by solving an optimization problem defined by a certain loss function $J(\theta, x, y)$; this is usually mean squared error (MSE) or cross entropy and is used to estimate the model's generalization ability:</p>
<p>$$
\theta^{*}=\underset{\theta}{\operatorname{argmin}} \sum_{(x, y) \in T} J(\theta, x, y)
$$</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Illustration of gradient descent, subscripts denote different time steps: in each step, the gradient is computed w.r.t. $\theta_{t}$ for calculating a new $\theta_{t+1}$ by updating $\theta_{t}$ towards the opposite direction of the gradient, until we reach a (possibly local) minimum value of $J$.</p>
<p>One of the most common algorithms to approximate the above problem is gradient descent [Cauchy [n.d.]] using backpropagation [Kelley 1960]. When gradient descent is used for training, the following update rule is applied repeatedly to update the model's weights:</p>
<p>$$
\theta_{t+1}=\theta_{t}-\eta \cdot \nabla_{\theta} J\left(\theta_{t}, x, y\right)
$$</p>
<p>where $\eta \in \mathcal{R}$ is a small scalar hyperparameter called learning rate, for example, 0.001 . Intuitively, the gradient descent algorithm can be viewed as taking small steps in the direction of the steepest descent, until a (possibly local) minimum is reached. This process is illustrated in Figure 4, where $\theta$ contains a single trainable variable $\theta \in \mathcal{R}$.</p>
<h1>3.2 Adversarial Examples</h1>
<p>Neural network models are very popular and have been applied in many domains, including computer vision [He et al. 2016; Krizhevsky et al. 2012; Simonyan and Zisserman 2014; Szegedy et al. 2015], natural language [Cho et al. 2014; Hochreiter and Schmidhuber 1997; Mikolov et al. 2010], and source code [Allamanis et al. 2018, 2016; Alon et al. 2019a, 2018; Bavishi et al. 2018; Bielik et al. 2016; Brockschmidt et al. 2019; Liu et al. 2019; Lu et al. 2017; Murali et al. 2017; Pradel and Sen 2018; Raychev et al. 2015; Rice et al. 2017; Sachdev et al. 2018].</p>
<p>Although neural networks have shown astonishing results in many domains, they were found to be vulnerable to adversarial examples. An adversarial example is an input which intentionally forces a given trained model to make an incorrect prediction. For neural networks that are trained on continuous objects like images and audio, the adversarial examples are usually achieved by applying a small perturbation on a given input image [Carlini and Wagner 2018; Goodfellow et al. 2014b; Szegedy et al. 2013]. This perturbation is found using gradient-based methods: usually by deriving the desired loss with respect to the neural network's inputs.</p>
<p>In discrete problem domains such as natural language or programs, generating adversarial examples is markedly different. Specifically, the existing techniques and metrics do not hold, because discrete symbols cannot be perturbed with imperceptible adversarial noise. Additionally, in discrete domains, deriving the loss with respect to its discrete input results in a gradient of zeros.</p>
<p>Recently, attempts were made to find adversarial examples in the domain of Natural Language Processing (NLP). Although adversarial examples on images are easy to generate, the generation of adversarial text is harder. This is due to the discrete nature of text and the difficulty of generating semantic-preserving perturbations. One of the approaches to overcome this problem is to replace a word with a synonym [Alzantot et al. 2018b]. Another approach is to insert typos into words by replacing a few characters in the text [Belinkov and Bisk 2017; Ebrahimi et al. 2017]. However, these NLP approaches allow only non-targeted attacks.</p>
<p>Additionally, NLP approaches cannot be directly applied to code. NLP models don't take into account the unique properties of code, such as: multiple occurrences of variables, semantic and syntactic patterns in code, the relation between different parts of the code, the readability of the entire code, and whether or not the code still compiles after the adversarial mutation. Therefore, applying NLP methods on the code domain makes the hard problem even harder.</p>
<h1>4 ADVERSARIAL EXAMPLES FOR MODELS OF CODE</h1>
<p>In this section we describe the process of generating adversarial examples using our DAMP approach.</p>
<h3>4.1 Definitions</h3>
<p>Suppose we are given a trained model of code. The given model can be described as a function $f_{\theta}: \mathcal{C} \rightarrow \mathcal{Y}$, where $\mathcal{C}$ is the set of all code snippets and $\mathcal{Y}$ is a set of labels.</p>
<p>Given a code snippet $c \in \mathcal{C}$ that the given trained model predicts as $y \in \mathcal{Y}$, i.e., $f_{\theta}(c)=y$, we denote by $y_{\text {bad }} \in \mathcal{Y}$ the adversarial label chosen by the attacker. Following the definitions of Bielik and Vechev [2020]: let $\Delta(c)$ be a set of valid modifications of the code snippet $c$, and let $\delta(c)$ be a new input obtained by applying a modification $\delta: \mathcal{C} \rightarrow \mathcal{C}$ to $c$, such that $\delta \in \Delta(c)$. For instance, if $c$ is the code snippet in Figure 3a, then Figure 3b shows $\delta_{\text {destination } \rightarrow \text { scsqbhj }}(c)$ - the same code snippet where $\delta_{\text {destination } \rightarrow \text { scsqbhj }}$ is the modification that renames the variable destination to scsqbhj. As a result, the prediction of the model changes from $y=$ DestinationType to $y_{\text {bad }}=\operatorname{SourceType}$, and $y=f_{\theta}(c) \neq f_{\theta}\left(\delta_{\text {destination } \rightarrow \text { scsqbhj }}(c)\right)=y_{\text {bad }}$.</p>
<p>In this paper, we focus on perturbations that rename local variables or add dead code. For brevity, in this section we focus only on generating adversarial examples by renaming of the original variables (VARName). Thus, we define the possible perturbations that we consider as $\Delta(c)=\left{\delta_{v \rightarrow v^{\prime}} \mid \forall c \in \mathcal{C}: \delta_{v \rightarrow v^{\prime}}(c)=c_{v \rightarrow v^{\prime}}\right}$, where $c_{v \rightarrow v^{\prime}}$ is the code snippet $c$ in which the variable $v$ was renamed to $v^{\prime}$.</p>
<p>Our approach can also be applied to a code snippet without changing the existing names, and instead adding a redundant variable declaration (DeadCode insertion, see Section 6.1.2). In such a case, our approach can be similarly applied by choosing an initial random name for the new redundant variable, and selecting this variable as the variable we wish to rename.</p>
<p>We define $\operatorname{Var}(c)$ as the set of all local variables existing in $c$. The adversary's objective is to thus select a single variable $v \in \operatorname{Var}(c)$ and an alternative name $v^{\prime}$, such that renaming $v$ to $v^{\prime}$ will make the model predict the adversarial label. The most challenging step is to find the right alternative name $v^{\prime}$ such that $f_{\theta}\left(\delta_{v \rightarrow v^{\prime}}(c)\right)=y_{\text {bad }}$.</p>
<p>Minimality of Perturbation. In addition to semantic equivalence, we also require that the programs "before" and "after" the perturbation be as similar as possible, i.e., $c \approx \delta(c)$. While this constraint is well-defined formally and intuitively in continuous domains such as images and audio [Carlini and Wagner 2018; Szegedy et al. 2013], in discrete domains (e.g., programs) the existing definitions do not hold. The main reason is that every change is perceptible in code. It is possible to compose a series of perturbations $\delta_{1}, \delta_{2}, \ldots, \delta_{k}$ and apply them to a given code snippet: $\delta_{1} \circ \delta_{2} \ldots \circ \delta_{k}(c)$. In this paper, to satisfy the similarity requirement, we focus on selecting and applying only a single perturbation.</p>
<p>Semantic-preserving Transformations. The advantages of variable renaming as the form of semantic-preserving transformation are that (i) each variable appears in several places in the code, so a single renaming can induce multiple perturbations; (ii) the adversarial code can still be compiled and therefore stays in the code domain; and (iii) some variables do not affect the readability of the code and hence, renaming them creates unnoticed unobserved adversarial examples.</p>
<p>We focus on two distinct types of adversarial examples:</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Gradient is computed for $y_{\text {bad }}$ loss function w.r.t. $v$. By moving toward the opposite direction of the gradient (and replacing $v$ with $v^{\prime}$ ), we decrease the loss of $y_{\text {bad }}$.</p>
<ul>
<li>Targeted attack - forces the model to output a specific prediction, which is not the correct prediction.</li>
<li>Non-targeted attack - forces the model to make any incorrect prediction.</li>
</ul>
<p>From a high-level perspective, the main idea in both kinds of attacks lies in the difference between the standard approach of training a neural network using back-propagation and generating adversarial examples. While training a neural network, we derive the loss with respect to the learned parameters and update each learned parameter. In contrast, when generating adversarial examples, we derive the loss with respect to the inputs while holding the learned parameters constant, and updating the inputs.</p>
<h1>4.2 Targeted Attack</h1>
<p>In this kind of attack, our goal is to make the model predict an incorrect desired label $y_{b a d}$ by renaming a given variable $v$ to $v^{\prime}$. Let $\theta$ be the learned parameters of a model, $c$ be the input code snippet to the model, $y$ be the target label, and $J(\theta, c, y)$ be the loss function used to train the neural model. As explained in Section 3.1, when training using gradient descent, the following rule is used to update the model parameters and minimize $J$ :</p>
<p>$$
\theta_{t+1}=\theta_{t}-\eta \cdot \nabla_{\theta} J\left(\theta_{t}, c, y\right)
$$</p>
<p>We can apply a gradient descent step with $y_{b a d}$ as the desired label in the loss function, and derive with respect to any given variable $v$ :</p>
<p>$$
v^{\prime}=\bar{v}-\eta \cdot \nabla_{\bar{v}} J\left(\theta, c, y_{b a d}\right)
$$</p>
<p>where $\bar{v}$ is the one-hot vector representing $v$. The above action can be viewed intuitively as taking a step toward the steepest descent, where the direction is determined by the loss function that considers $y_{b a d}$ as the correct label. This is illustrated in Figure 5</p>
<p>The loss $J\left(\theta, c, y_{b a d}\right)$ in Equation (4) is differentiable with respect to $\bar{v}$ because $J$ is the same loss function that the model was originally trained with, e.g., cross entropy loss. The main difference from the standard gradient descent training (Equation (2)) is that Equation (4) takes the gradient with respect to the one-hot representation of the input variable $\left(\nabla_{\bar{v}}\right)$, rather than taking the gradient with respect to the learnable weights $\left(\nabla_{\theta}\right)$ in Equation (2).</p>
<p>In fact, the result of the above action does not produce a desired new variable name $v^{\prime}$. Instead it produces a distribution over all possible variable names. To concretize the name of $v^{\prime}$, we choose the argmax over the resulting distribution, as illustrated in Figure 7 and detailed in Section 4.4.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Gradient ascent illustration: Gradient is computed with respect to $v$. By moving towards the gradient's direction (and replacing $v$ with $v^{\prime}$ ), we increase the loss.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Perturbing a variable name: the original variable name is represented as a one-hot vector over the variable-name vocabulary. After perturbation, the vector is no longer one-hot. We apply argmax to find the most likely adversarial name, resulting with another one-hot vector over the variable-name vocabulary. This figure is identical to Figure 2 and repeated here for clarity.</p>
<h1>4.3 Non-targeted Attack</h1>
<p>In a non-targeted attack, our goal is to update $v$ to $v^{\prime}$ in a way that will increase the loss in any direction, instead of decreasing it, as in the training process. Thus, we compute the gradient with respect to $v$ and use Gradient Ascent:</p>
<p>$$
v^{\prime}=\bar{v}+\eta \cdot \nabla_{\bar{v}} J(\theta, x, y)
$$</p>
<p>This rule can be illustrated as taking a step toward the steepest ascent in the loss function (Figure 6).
Targeted vs. Non-targeted Attacks. Notice the difference between Equation (4) and Equation (5). In Equation (4), the goal of the targeted attack is to find the direction of the lowest loss with respect to the adversarial label. So, we take a step toward the negative gradient.</p>
<p>In Equation (5), our goal for the non-targeted attack is to find the direction of the higher loss with respect to the original label. This is exactly what the gradient gives us.</p>
<p>These equations differ on two cardinal axes: the target label (original or adversarial), and direction of progress (towards or away from), which are the main axes of the equations.</p>
<h1>4.4 Deriving by Integer Indices</h1>
<p>The operation of looking-up a row vector in an embedding matrix using its index is simple. Some common guides and tutorials describe this as taking the dot product of the embedding matrix and a one-hot vector representing the index. In contrast with these guides, when implementing neural networks, there is usually no real need to use one-hot vectors at all. All word embeddings can be stored in a matrix such that each row in the matrix corresponds to a word vector. Looking up a specific vector is then performed by simply looking up a row in the matrix using its index.</p>
<p>Nevertheless, in the context of adversarial examples, deriving the loss with respect to a single variable name is equivalent to deriving with respect to an index, which is zero almost everywhere. Thus, instead of using indices, we have to represent variable names using one-hot vectors, because these can be derived. Looking up a vector in a matrix can then be performed by taking the dot product of the embedding matrix with the one-hot vector. Deriving the loss by a one-hot vector instead of an index is thus equivalent to deriving by the (differentiable) distribution over indices, rather than deriving by the index itself. The result of each adversarial step is thus a distribution over all variable names, in which we select the $\operatorname{argmax}$ (Figure 7).</p>
<h3>4.5 Search</h3>
<p>Sometimes, adversarial examples can be found by applying the adversarial step of Equation (4) once. At other times, multiple steps are needed, i.e., replacing $v$ with $v^{\prime}$ and computing the gradient again with respect to $v^{\prime}$. We limit the number of times we apply the adversarial step by a depth hyperparameter. Additionally, instead of taking the argmax from the distribution over candidate names, we can try all "top-k" candidates. These define a Breadth-First Search (BFS), where the width parameter is defined by the "top-k". Checking whether a choice of variable name results in the desired output is low cost; it just requires computing the output of the model given the modified input. Thus, it is conveniently feasible to perform multiple adversarial steps and check multiple top-k candidates.</p>
<p>The depth and width hyperparameters fine tune the trade-off between the effectiveness of the attack and its computation cost. Small values will define a search that is computationally inexpensive but may lead to early termination without any result. Higher values can increase the probability of finding an adversarial example at the cost of a longer search.</p>
<h3>4.6 Convergence</h3>
<p>In general, gradient descent is guaranteed to converge to a global minimum only when optimizing convex functions [Nesterov 2013]. The loss functions of most neural networks are not convex, and thus, our gradient descent process of multiple adversarial steps is not guaranteed to succeed. Furthermore, even if we could hypothetically find a global minimum to the loss function $J\left(\theta, c, y_{b a d}\right)$, it is possible that for a given adversarial target $y_{b a d}$ there is no perturbation that can force the model to predict $y_{b a d}$. In fact, it is rather intriguing that the success rate for our targeted attack is so high (as show in Section 6) when we restrict the perturbation to a single variable.</p>
<p>As discussed in Section 4.5, we limit the number of steps by a depth hyperparameter. Empirically, in most cases this process ends successfully, by finding a new variable name $v^{\prime}$ that forces the adversarial label $y_{b a d}$. If no adversarial $v^{\prime}$ was found after depth steps, we consider this attack to have failed.</p>
<p>While recent work provides proofs and guarantees for continuous domains [Balunovic et al. 2019; Singh et al. 2019], we are not aware of any work that provides guarantees for discrete domains. Even in continuous domains, guarantees have only been provided for small networks with just 88 K learnable parameters [Singh et al. 2019] and 45 K parameters [Balunovic et al. 2019]; these</p>
<p>networks often restrict the use of non-linearities. In contrast, the models we experiment with are state-of-the-art, realistic, models with several orders of magnitude more parameters.</p>
<h1>5 DEFENSE AGAINST ADVERSARIAL EXAMPLES</h1>
<p>To defend against adversarial attacks, we consider two broad classes of defense techniques. The first class contains techniques that serve as a "gatekeeper" for incoming input and can be plugged in on top of existing trained models, without having to re-train them. The second class contains techniques that require the model to be re-trained, possibly using a modified loss function or a modified version of the original training set.</p>
<h3>5.1 Defense Without Re-training</h3>
<p>Techniques that do not require re-training are appealing because they allow us to separate the optimization of the model from the optimization of the defense, and easily tune the balance between them. Moreover, training neural models is generally computationally expensive; thus, these approaches enable us to perform the expensive training step only once.</p>
<p>Approaches that do not require retraining can be generalized as placing a defensive-model $g$ before the model of code $f_{\theta}$, which is independent of $g$. The goal of $g$ is to fix the given input, if necessary, in a way that will allow $f_{\theta}$ to predict correctly. Mathematically, the new model can be defined as being composed of two models: $f_{\theta} \circ g$. We assume that the adversary has access to the model $f_{\theta}$ being attacked, but not to the defense model $g$.</p>
<p>We evaluated the following approaches that do not require re-training:
No Vars - a conservative defensive approach that replaces all variables to an UNK ("unknown") symbol, only at test time. This approach is $100 \%$ robust by construction, but does not leverage variable names for prediction.
Outlier Detection - tries to identify an outlier variable name and filter it out by replacing only this variable with UNK. The main idea is that the adversarial variable name is likely to have a low contextual relation to the other, existing, identifiers and literals in code. We detect outliers by finding an outlier variable in terms of $L_{2}$ distance among the vectors of the existing variable names. Given a code snippet $c \in C$, we define $\operatorname{Sym}(c)$ as the set of all identifiers and literals existing in $c$. We select the variable $z^{*}$, which is the most distant from the average of the other symbols:</p>
<p>$$
z^{*}=\underset{z \in \operatorname{Var}(c)}{\operatorname{argmax}}\left|\frac{\sum_{v \in S y m(c), v \neq z} \operatorname{vec}(v)}{|\operatorname{Sym}(c)|}-\operatorname{vec}(z)\right|_{2}
$$</p>
<p>We then define a threshold $\sigma$ that determines whether $z^{*}$ is an outlier. If the $L_{2}$ distance between the vector of $z$ and the average of the rest of the symbols is greater than $\sigma$, then $z$ is replaced with an UNK symbol; otherwise, the code snippet is left untouched. Practically, the threshold $\sigma$ is tuned on a validation set. It determines the trade-off between the effectiveness of the defense and the accuracy while not under attack, as we evaluate and discuss in Section 6.3.3.</p>
<h3>5.2 Defense With Re-training</h3>
<p>Ideally, re-training the original model allows us to train the model to be robust while, at the same time, training the model to perform as well as the non-defensive model. Re-training allows the model to be less vulnerable from the beginning, rather than patching a vulnerable model using a separate defense.</p>
<p>We evaluated the following techniques that do require re-training:
Train Without Vars - replaces all variables with an unK symbol both at training and test time. This approach is also $100 \%$ robust by construction, as No Vars does not require re-training. It is</p>
<p>expected to perform better than No Vars in terms of F1 because it is trained not to rely on variable names, and to use other signals instead. The downside is that it requires training a model from scratch, while No Vars can be applied to an already-trained model.
Adversarial Training - follows Goodfellow et al. [2014b] and trains a model on the original training set, while learning to perform the original predictions and training on adversarial examples at the same time. Instead of minimizing only the expected loss of the original distribution, every example from the training set $(x, y) \in T$ contributes both to the original loss $J(\theta, x, y)$ and to an adversarial loss $J_{\text {adv }}\left(\theta, x^{\prime}, y\right)$. Here, $x^{\prime}$ is a perturbed version of $x$, which was created using a single BFS step of our non-targeted attack (Section 4.3). During training, we minimized $J+J_{\text {adv }}$ simultaneously. Note, this method doesn't change the model complexity but it does increase the training time, making it about three times slower.
Adversarial Fine-Tuning - follows Hosseini et al. [2017] and trains a model for several epochs with the original examples from the training set using the original loss $J(\theta, x, y)$. Once the model is trained for optimal performance, it is fine-tuned on adversarial examples. During fine-tuning, the model is trained for a single iteration over the training set, and only on adversarial versions of each training example, using the adversarial loss $J_{\text {adv }}\left(\theta, x^{\prime}, y\right)$. The expectation in this approach is to establish the model's high performance first, and then ensure the model's robustness to adversarial examples because of the recent fine-tuning.
No Defense, $|\operatorname{vocab}|={10 \mathrm{k}, 50 \mathrm{k}, 100 \mathrm{k}}-$ trains the original model with a smaller vocabulary. Limiting the vocabulary size has the potential to improve robustness by ignoring rare variable names. Names that are observed in the training or test data but are outside the limited vocabulary are replaced with a special UNK symbol. This way, the model is expected to consider only frequent names. Because of their frequency, these names will be observed enough times during training such that their vector representation is more stable.</p>
<h1>6 EVALUATION</h1>
<p>Our set of experiments comprises two parts: (a) evaluating the ability of DAMP to change the prediction of the downstream classifier for targeted and non-targeted attacks; and (b) evaluating a variety of defense techniques and their ability to mitigate the attacks.</p>
<p>We note that our goal is not to perform a robustness evaluation of the attacked models themselves. For more on evaluating the robustness for models of code, we refer the reader to Ramakrishnan et al. [2020] and Bielik and Vechev [2020]. Instead, the goal of this section is to evaluate the effectiveness of our proposed targeted and non-targeted attacks, and to evaluate the robustness that different defense techniques provide to a given model.</p>
<p>Our code, data, and trained models are available at https://github.com/tech-srl/adversarialexamples .</p>
<h3>6.1 Setup</h3>
<p>6.1.1 Downstream Models. We evaluated our DAMP attack using three popular architectures as downstream models. We obtained the trained models from their original authors, who trained the models themselves.
code2vec was introduced by Alon et al. [2019c] as a model that predicts a label for a code snippet. The main idea is to decompose the code snippet into AST paths, and represent the entire snippet as a set of its paths. Using large scale training, the model was demonstrated to predict a method name conditioned on the method body. The goal of the attack is to thus change the predicted method name by perturbing the method body. This model takes Java methods as its inputs, and represents variable names using a vocabulary of learned embeddings.</p>
<p>Gated Graph Neural Networks (GGNNs) were introduced by Li et al. [2016] as an extension to Graph Neural Networks (GNNs) [Scarselli et al. 2008]. Their aim is to represent the problem as a graph, and to aggregate the incoming messages to each vertex with the current state of the vertex using a GRU [Cho et al. 2014] cell. GGNNs were later adapted to source code tasks by Allamanis et al. [2018], who applied them to the VarMisuse task of predicting the correct variable in a given blank slot among all variables in a certain scope.
GNN-FiLM is a GNN architecture that was recently introduced by Brockschmidt [2019]. It differs from prior GNN models in its message passing functions, which compute the "neural message" based on both the source and target of each graph edge, rather than just the source as done in previous architectures. GNN-FiLM was shown to perform well for both the VarMisuse task as well as other graph-based tasks such as protein-protein interaction and quantum chemistry molecule property prediction. The goal of the attack here and in GGNNs is to make the model predict the incorrect output variable name; this is done by changing an unrelated input variable name, which is neither the correct variable nor the adversarial variable.</p>
<p>Vocabulary. In code2vec, we use the original trained model's own vocabulary to search for a new adversarial variable name. We derive the adversarial loss with respect to the distribution over all variable names. Thus, the chosen adversarial variable name can be any name from the vocabulary that was used to train the original code2vec model. The GNN-FiLM and the GGNN models take C# methods as their inputs and represent variable names using a character-level convolution. Thus, in our DAMP attack we derive the loss with respect to the distribution over all characters in a given name; the chosen adversarial input name can be any combination of characters.
6.1.2 Adversarial Strategies. While there are a variety of possible adversarial perturbations, we focus on two main adversarial strategies:</p>
<ul>
<li>Variable Renaming (VarName): choose a single variable, and iteratively change its name until the model's prediction is changed, using a BFS (as explained in Section 4). Eventually, the "distance" between the original code and the adversarial code is a single variable name at most.</li>
<li>Dead-Code Insertion (DeadCode): insert a new unused variable declaration and derive the model with respect to its name. The advantage of this strategy is that the existing code remains unchanged, which might make this attack more difficult to notice. Seemingly, this kind of attack can be mitigated by removing unused variables before feeding the code into the trained model. Nonetheless, in the general case, detecting an unreachable code is undecidable. In all cases, we arbitrarily placed the dead code at the end of the input method, and used our attack to find a new name for the new (unused) declared variable. In our preliminary experiments we observed that placing the dead code anywhere else works similarly. In this attack strategy, a single variable declaration is inserted. Thus, the "distance" between the original code and the adversarial code is a single variable declaration statement.
Other semantic-preserving transformations such as statement swapping and operator swapping, are not differentiable and thus do not enable targeted attacks.</li>
</ul>
<p>When renaming variables or introducing new variables, we verified that the new variable name does not collide with an existing variable that has the same name. In the code2vec experiments, we used the adversarial step to run a BFS with width $=2$ and depth $=2$. In GGNN we used width $=1$ and depth $=3$, and GNN-FiLM required depth $=10$ to achieve attack success that is close to that of the other models. We discuss these differences in Section 6.2.3. Increasing the width and depth can definitely improve the adversary's success but at the cost of a longer search, although the entire BFS takes a few seconds at most.</p>
<p>6.1.3 Dataset. code2vec - Java We evaluated our proposed attack and defense on code2vec using the Java-large dataset [Alon et al. 2019a]. This dataset contains more than 16M Java methods and their labels, taken from 9500 top-starred Java projects in GitHub that have been created since January 2007. It contains 9000 projects for training, 200 distinct projects for validation, and 300 distinct projects for test. We filtered out methods with no local variables or arguments, since they cannot be perturbed by variable renaming. To evaluate the effectiveness of our DAMP attack, we focus on the examples that the model predicted correctly out of the test set of Java-large.</p>
<p>That is, from the original test set, we used a subset that the original code2vec predicted accurately. On this filtered test set, the accuracy of the original code2vec model is $100 \%$ by construction.
GGNN and GNN-FILM - C# We evaluated DAMP with the GGNN and GNN-FILM C# models on the dataset of Allamanis et al. [2018]. This dataset consists of about 220,000 graphs (examples) from 29 top-starred C# projects on GitHub. For all examples, there is at least one type-correct replacement variable other than the correct variable, and a maximum of up to 5 candidates. DAMP always attacks by modifying an unrelated variable, not the correct variable or the adversarial target. In targeted attacks, we randomly pick one out of the five candidates as the target for attack. In non-targeted attacks, our attacker tries to make the model predict any incorrect candidate. Similar to the Java dataset, we focus on the examples that each original model predicts correctly out of the test set.</p>
<h1>6.2 Attack</h1>
<p>We focus on two main attack tasks: targeted and non-targeted attacks. For targeted attacks, we used Equation (4) as the adversarial step. For non-targeted attacks, we used Equation (5) as the adversarial step. For the desired adversarial labels, we randomly sampled labels that occurred at least 10 K times in the training set.
6.2.1 Metrics. We measured the robustness of each setting for each of the different attack approaches. The lower model robustness, the higher effectiveness of the attack.</p>
<p>In targeted attacks, the goal of the adversary is to change the prediction of the model to a label of the attacker's desire. We thus define robustness as the percentage of examples in which the correctly predicted label was not changed to the adversary's desired label. If the predicted label was changed to a label that is not the adversarial label, we consider the model as robust to the targeted attack.</p>
<p>In non-targeted attacks, the goal of the adversary is to change the prediction of the model to any label other than the correct label. We thus define robustness as the percentage of examples in which the correctly predicted label was not changed to any label other than the correct label.
6.2.2 Baselines. Since our task is new, we are not aware of existing baselines. We thus compare DAMP to different approaches in targeted and non-targeted attacks for CODE2vec and GNN models.
TFIDF is a statistical baseline for attacking CODE2vec: for every pair of a label and a variable name it computes the number of times the variable appears in the training set under this label, divided by the total number of occurrences of the variable in the training set. Then, given a desired adversarial label $y_{b a d}$, TFIDF outputs the variable name $v$ that has the highest score with $y_{b a d}$ : $\operatorname{TFIDF}\left(y_{b a d}\right)=\operatorname{argmax}<em a="a" b="b" d="d">{v} \frac{#\left(y</em>$.
CopyTarget attacks CODE2VEC with targeted attacks. CopyTarget replaces a variable with the desired adversarial label. For example, if the adversarial label is badPrediction, CopyTarget renames a variable to badPrediction.}, v\right)}{#(v)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">VARNAME (robustness \%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DEADCODE (robustness \%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RandomVar</td>
<td style="text-align: center;">TFIDF</td>
<td style="text-align: center;">DAMP (this work)</td>
<td style="text-align: center;">RandomVar</td>
<td style="text-align: center;">TFIDF</td>
<td style="text-align: center;">DAMP (this work)</td>
</tr>
<tr>
<td style="text-align: center;">Non-targeted</td>
<td style="text-align: center;">34.10</td>
<td style="text-align: center;">53.53</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">54.90</td>
<td style="text-align: center;">84.00</td>
<td style="text-align: center;">21.83</td>
</tr>
<tr>
<td style="text-align: center;">Targeted:</td>
<td style="text-align: center;">CopyTarget</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CopyTarget</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">init</td>
<td style="text-align: center;">84.47</td>
<td style="text-align: center;">74.33</td>
<td style="text-align: center;">48.44</td>
<td style="text-align: center;">96.79</td>
<td style="text-align: center;">96.67</td>
<td style="text-align: center;">87.62</td>
</tr>
<tr>
<td style="text-align: center;">mergeFrom</td>
<td style="text-align: center;">72.79</td>
<td style="text-align: center;">9.01</td>
<td style="text-align: center;">10.39</td>
<td style="text-align: center;">99.82</td>
<td style="text-align: center;">29.34</td>
<td style="text-align: center;">22.65</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
<td style="text-align: center;">99.47</td>
<td style="text-align: center;">77.91</td>
<td style="text-align: center;">78.27</td>
<td style="text-align: center;">99.97</td>
<td style="text-align: center;">98.98</td>
<td style="text-align: center;">95.96</td>
</tr>
<tr>
<td style="text-align: center;">isEmpty</td>
<td style="text-align: center;">88.61</td>
<td style="text-align: center;">98.11</td>
<td style="text-align: center;">79.04</td>
<td style="text-align: center;">99.98</td>
<td style="text-align: center;">99.72</td>
<td style="text-align: center;">87.63</td>
</tr>
<tr>
<td style="text-align: center;">clear</td>
<td style="text-align: center;">89.56</td>
<td style="text-align: center;">89.00</td>
<td style="text-align: center;">82.80</td>
<td style="text-align: center;">99.07</td>
<td style="text-align: center;">98.55</td>
<td style="text-align: center;">97.89</td>
</tr>
<tr>
<td style="text-align: center;">remove</td>
<td style="text-align: center;">84.94</td>
<td style="text-align: center;">84.83</td>
<td style="text-align: center;">63.15</td>
<td style="text-align: center;">99.29</td>
<td style="text-align: center;">99.24</td>
<td style="text-align: center;">80.02</td>
</tr>
<tr>
<td style="text-align: center;">value</td>
<td style="text-align: center;">99.77</td>
<td style="text-align: center;">71.81</td>
<td style="text-align: center;">76.75</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">98.16</td>
<td style="text-align: center;">98.33</td>
</tr>
<tr>
<td style="text-align: center;">load</td>
<td style="text-align: center;">86.75</td>
<td style="text-align: center;">85.46</td>
<td style="text-align: center;">55.65</td>
<td style="text-align: center;">99.03</td>
<td style="text-align: center;">97.22</td>
<td style="text-align: center;">86.65</td>
</tr>
<tr>
<td style="text-align: center;">add</td>
<td style="text-align: center;">92.88</td>
<td style="text-align: center;">86.72</td>
<td style="text-align: center;">68.60</td>
<td style="text-align: center;">99.93</td>
<td style="text-align: center;">95.29</td>
<td style="text-align: center;">93.75</td>
</tr>
<tr>
<td style="text-align: center;">run</td>
<td style="text-align: center;">95.11</td>
<td style="text-align: center;">40.92</td>
<td style="text-align: center;">51.52</td>
<td style="text-align: center;">99.36</td>
<td style="text-align: center;">62.87</td>
<td style="text-align: center;">77.63</td>
</tr>
<tr>
<td style="text-align: center;">Count best:</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>Table 1. Robustness of code2vec to our adversarial attacks, targeted and non-targeted, using VarName and DeadCode, compared to the baselines (the lower robustness, the more effective the attack). DAMP is more effective than the baselines in 6 out of 10 VARNAME randomly sampled targets and in 8 out of 10 DeadCODE targets.</p>
<p>CharBruteForce attacks GGNN and GNN-FiLM, which address the VarMisuse task. CharBruteForce changes the name of the attacked variable by randomly changing every character iteratively up to a limited number of iterations.
RandomVar is used in non-targeted attacks on code2vec: RandomVar replaces the given variable with a randomly selected variable name from the training set vocabulary.</p>
<p>In all experiments, the baselines were given the same number of trials as our attack. In RandomVar, we randomly sampled the same number of times. In TFIDF, we used the top-k TFIDF candidates as additional trials. In CopyTarget, we took the target as the new variable name and its k-nearest neighbors in the embedding space.
6.2.3 Attack - Results. code2vec Table 1 summarizes the results of DAMP on code2vec. The main result is that DAMP outperforms the baselines by a large margin in both targeted and non-targeted attacks.</p>
<p>In non-targeted attacks, DAMP attacks are much more effective than the baseline: for VarName, code2vec is $6 \%$ robust to DAMP, $34.10 \%$ robust to RandomVar and $53.53 \%$ robust to TFIDF. For DeadCode, code2vec is $21.83 \%$ robust to DAMP, $54.90 \%$ robust to RandomVar, and $84.00 \%$ robust to TFIDF. Thus, DAMP's attack is more effective than that of the baselines.</p>
<p>In targeted attacks, DAMP performs better than CopyTarget for each of the randomly sampled adversarial labels. For example, code2vec is only $10.39 \%$ robust to DAMP attacks that change the prediction to the label mergeFrom, and $72.79 \%$ robust to the CopyTarget attack of the same target adversarial label.</p>
<p>However, in some of the randomly sampled targets TFIDF was more effective than DAMP. In VarName, DAMP was the most effective in 6 out of the 10 randomly sampled adversarial targets, while TFIDF was the most effective in the remaining 4. In DeadCode attacks, DAMP was the most effective in 8 out of 10 randomly sampled adversarial targets, while TFIDF was the most effective in the remaining 2. Although TFIDF performed better than DAMP for a few of the targets, the</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GGNN Robustness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GNN-FiLM Robustness</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CharBruteForce</td>
<td style="text-align: center;">DAMP (this work)</td>
<td style="text-align: center;">CharBruteForce</td>
<td style="text-align: center;">DAMP (this work)</td>
</tr>
<tr>
<td style="text-align: left;">Non-targeted</td>
<td style="text-align: center;">96.24</td>
<td style="text-align: center;">$\mathbf{5 7 . 9 9}$</td>
<td style="text-align: center;">95.56</td>
<td style="text-align: center;">$\mathbf{8 3 . 5 5}$</td>
</tr>
<tr>
<td style="text-align: left;">targeted</td>
<td style="text-align: center;">97.84</td>
<td style="text-align: center;">$\mathbf{6 9 . 0 0}$</td>
<td style="text-align: center;">96.19</td>
<td style="text-align: center;">$\mathbf{8 7 . 6 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2. Robustness of GGNN and GNN-FiLM to our adversarial attacks (targeted and non-targeted), compared to CharBruteForce (the lower the robustness, the more efficient the attack). Our attack is more effective than brute-force given an equal number of trials. Every successful attack results in a type-safe VArMisuse bug.
differences were usually small. Overall, DAMP's targeted attack is more effective than that of the baselines.</p>
<p>Non-targeted attacks generally yield lower model robustness than targeted attacks. This is expected, since non-targeted attacks try to change the label to any label other than the correct one, while targeted attacks count as successful only if they change the prediction to the desired label.</p>
<p>In general, the VarName attack is more effective than DeadCode. We hypothesize that this is because the inserted unused variable impacts only a small part of the code. Hence, it may have a smaller numerical effect on the computation of the model. In contrast, renaming an existing variable changes multiple occurrences in the code and thus has a wider effect.
GGNN and GNN-FiLM Table 2 summarizes the results of the DAMP attack and the baseline on the GGNN and GNN-FiLM models. The main result is that DAMP is much more effective than the CharBruteForce baseline. The GGNN model is $69.00 \%$ robust to the DAMP targeted attack, and $98.84 \%$ robust to the baseline attack. The GNN-FiLM is $87.62 \%$ robust to the DAMP attack and $96.19 \%$ robust to the CharBruteForce attack.</p>
<p>In addition, we see that DAMP is more effective on code2vec than on both GNN architectures. CODE2vEC is $6 \%$ robust to non-targeted VARNAME attacks and GGNNs is $57.99 \%$ robust to non-targeted attacks. There are several reasons for this difference between the attacked models:
(1) CODE2VEC is simpler and more "neurally shallow", while GGNN uses 6 layers of message propagation steps, and GNN-FiLM uses 10 layers.
(2) The models' tasks are very different: CODE2VEC classifies a given code snippet to 1 out of about 200,000 possible target method names, while both GNN architectures address the VArMisuse task and need to choose 1 out of only 2 to 5 possible variables.
(3) CODE2VEC has orders of magnitude more trainable parameters (about 350M) than GGNN (about 1.6 M ) and GGNN (about 11.5 M ), making CODE2VEC more sparse, its loss hyperspace more complex, and thus more vulnerable.</p>
<p>Finally, we see that the GNN-FiLM model is more robust than GGNN. Even though we used a more aggressive BFS with 10 gradient steps for GNN-FiLM and only 3 gradient steps for GGNN, the GNN-FiLM robustness to DAMP targeted attack is $87.62 \%$, while the GGNN robustness is $69.00 \%$. We hypothesize that this is primarily because the message passing function in GNN-FiLM computes the sent message between nodes based on both the source and target of each graph edge, rather than just the source as in the GGNN. This makes it more robust to an attack on a single one of these nodes. The higher results of GNN-FiLM over GGNN in other graph-based benchmarks [Brockschmidt 2019] hint that GNN-FiLM may be using the graph topology, i.e., the program structure, to a larger degree; the GGNN focuses mainly on the names and is thus more vulnerable to name-based attacks. We leave further investigation of the differences between different GNN architectures for future work.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Performance (not under attack)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">VarName <br> Robustness (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DeadCode Robustness (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prec</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Non-targeted</td>
<td style="text-align: center;">Target: "run"</td>
<td style="text-align: center;">Non-targeted</td>
<td style="text-align: center;">Target: "run"</td>
</tr>
<tr>
<td style="text-align: center;">No Defense</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">51.52</td>
<td style="text-align: center;">22.83</td>
<td style="text-align: center;">77.63</td>
</tr>
<tr>
<td style="text-align: center;">No Vars</td>
<td style="text-align: center;">78.78</td>
<td style="text-align: center;">80.83</td>
<td style="text-align: center;">79.98</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Outlier Detection</td>
<td style="text-align: center;">98.18</td>
<td style="text-align: center;">97.75</td>
<td style="text-align: center;">97.92</td>
<td style="text-align: center;">74.35</td>
<td style="text-align: center;">96.49</td>
<td style="text-align: center;">96.73</td>
<td style="text-align: center;">95.76</td>
</tr>
<tr>
<td style="text-align: center;">Train Without Vars</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">90.86</td>
<td style="text-align: center;">90.40</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Adversarial Training</td>
<td style="text-align: center;">98.09</td>
<td style="text-align: center;">92.96</td>
<td style="text-align: center;">94.98</td>
<td style="text-align: center;">68.80</td>
<td style="text-align: center;">94.48</td>
<td style="text-align: center;">99.33</td>
<td style="text-align: center;">99.98</td>
</tr>
<tr>
<td style="text-align: center;">Adversarial Fine-Tuning</td>
<td style="text-align: center;">85.51</td>
<td style="text-align: center;">84.86</td>
<td style="text-align: center;">85.12</td>
<td style="text-align: center;">13.59</td>
<td style="text-align: center;">68.94</td>
<td style="text-align: center;">31.07</td>
<td style="text-align: center;">56.64</td>
</tr>
<tr>
<td style="text-align: center;">No Defense, $</td>
<td style="text-align: center;">\operatorname{vocab}</td>
<td style="text-align: center;">=10 k$</td>
<td style="text-align: center;">90.50</td>
<td style="text-align: center;">94.26</td>
<td style="text-align: center;">92.66</td>
<td style="text-align: center;">6.56</td>
<td style="text-align: center;">45.78</td>
</tr>
<tr>
<td style="text-align: center;">No Defense, $</td>
<td style="text-align: center;">\operatorname{vocab}</td>
<td style="text-align: center;">=50 k$</td>
<td style="text-align: center;">94.51</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.00</td>
<td style="text-align: center;">3.18</td>
<td style="text-align: center;">40.48</td>
</tr>
<tr>
<td style="text-align: center;">No Defense, $</td>
<td style="text-align: center;">\operatorname{vocab}</td>
<td style="text-align: center;">=100 k$</td>
<td style="text-align: center;">96.84</td>
<td style="text-align: center;">99.97</td>
<td style="text-align: center;">98.99</td>
<td style="text-align: center;">2.61</td>
<td style="text-align: center;">32.48</td>
</tr>
</tbody>
</table>
<p>Table 3. Precision, recall, F1, and robustness percentage of different models. The higher the robustness, the more effective the defense. Scores that are above $95 \%$ are marked in bold. Precision, recall, and F1 are measured on the subset of the test set that the vanilla model predicts accurately (as explained in Section 6.1.3). Outlier Detection and Adversarial Training are sweet-spots: they perform almost as good as No Defense in terms of precision, recall, and F1, and they are almost as robust as the extreme No Vars.</p>
<h1>6.3 Defense</h1>
<p>We experimented with all defense techniques as described in Section 5, in a code2vec model. No Defense is the vanilla, unmodified, trained model that was trained by its original authors. No Defense, $|\operatorname{vocab}|={10 k, 50 k, 100 k}$ are vanilla code2vec models that we trained using the authors' code and the default settings, without any special defense or modification, except for limiting the vocabulary sizes (where vocabulary size is mentioned); this allowed us to examine whether the limited vocabulary size can serve as some sort of defense.
6.3.1 Metrics. We measured the success rate of the different defense approaches in preventing the adversarial attack and increasing robustness. When evaluating alternative defenses, it is also important to measure the performance of the original model while using the defense, but not under attack. An overly defensive approach can lead to $100 \%$ robustness at the cost of reduced prediction performance.</p>
<p>To tune the threshold $\sigma$ of the Outlier Detection defense, we balanced the following factors: (1) the robustness of the model using this defense; and (2) the F1 score of the model using this defense, while not under attack. We tuned the threshold on the validation set, and chose $\sigma=2.7$ since it led to $75 \%$ robustness against non-targeted attack at the cost of $2 \%$ degradation in F1 score while not under attack. However, this threshold can be tuned according to the desired needs in the trade-off between performance and defense (see Section 6.3.3).
6.3.2 Defense - Results. The effectiveness of the different defense techniques is presented in Table 3. The main results are as follows: Outlier Detection provides the best performance and highest robustness among the techniques that do not require re-training; it achieves an F1 of 97.02 and above $75.35 \%$ robustness for targeted and non-targeted attacks. Among the techniques that do require re-training, Adversarial Training achieves the highest performance and robustness; Adversarial Training achieves an F1 score of 94.98 and above $68.80 \%$ robustness for targeted and non-targeted attacks.</p>
<p>The penalty of using Outlier Detection is only $2.18 \%$ degradation in F1 score compared to No Defense. At the other extreme, Train Without Vars is $100 \%$ robust to VarName and DeadCode attacks, but its F1 is degraded by $9.6 \%$ compared to No Defense. That is, Outlier Detection is a sweet spot in</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">VarName (robustness \%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DeadCode (robustness \%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAMP $^{\dagger}$</td>
<td style="text-align: center;">DAMP + Outlier Detection</td>
<td style="text-align: center;">DAMP + Adversarial Training</td>
<td style="text-align: center;">DAMP $^{\dagger}$</td>
<td style="text-align: center;">DAMP + <br> Outlier <br> Detection</td>
<td style="text-align: center;">DAMP + Adversarial Training</td>
</tr>
<tr>
<td style="text-align: center;">init</td>
<td style="text-align: center;">48.44</td>
<td style="text-align: center;">74.32</td>
<td style="text-align: center;">78.17</td>
<td style="text-align: center;">87.62</td>
<td style="text-align: center;">91.41</td>
<td style="text-align: center;">99.97</td>
</tr>
<tr>
<td style="text-align: center;">mergeFrom</td>
<td style="text-align: center;">10.39</td>
<td style="text-align: center;">99.98</td>
<td style="text-align: center;">98.91</td>
<td style="text-align: center;">22.65</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
<td style="text-align: center;">78.27</td>
<td style="text-align: center;">99.58</td>
<td style="text-align: center;">99.39</td>
<td style="text-align: center;">95.96</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">99.99</td>
</tr>
<tr>
<td style="text-align: center;">isEmpty</td>
<td style="text-align: center;">79.04</td>
<td style="text-align: center;">99.22</td>
<td style="text-align: center;">98.91</td>
<td style="text-align: center;">87.63</td>
<td style="text-align: center;">97.03</td>
<td style="text-align: center;">99.99</td>
</tr>
<tr>
<td style="text-align: center;">clear</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">98.77</td>
<td style="text-align: center;">85.09</td>
<td style="text-align: center;">97.89</td>
<td style="text-align: center;">99.56</td>
<td style="text-align: center;">99.99</td>
</tr>
<tr>
<td style="text-align: center;">remove</td>
<td style="text-align: center;">63.15</td>
<td style="text-align: center;">94.50</td>
<td style="text-align: center;">89.29</td>
<td style="text-align: center;">80.02</td>
<td style="text-align: center;">99.33</td>
<td style="text-align: center;">99.99</td>
</tr>
<tr>
<td style="text-align: center;">value</td>
<td style="text-align: center;">76.75</td>
<td style="text-align: center;">90.87</td>
<td style="text-align: center;">99.47</td>
<td style="text-align: center;">98.33</td>
<td style="text-align: center;">99.72</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">load</td>
<td style="text-align: center;">55.65</td>
<td style="text-align: center;">60.27</td>
<td style="text-align: center;">88.29</td>
<td style="text-align: center;">86.65</td>
<td style="text-align: center;">85.28</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">add</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">88.97</td>
<td style="text-align: center;">95.90</td>
<td style="text-align: center;">93.75</td>
<td style="text-align: center;">97.69</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">run</td>
<td style="text-align: center;">51.52</td>
<td style="text-align: center;">96.49</td>
<td style="text-align: center;">94.48</td>
<td style="text-align: center;">77.63</td>
<td style="text-align: center;">95.76</td>
<td style="text-align: center;">99.98</td>
</tr>
</tbody>
</table>
<p>Table 4. Robustness of code2vec to adversarial attacks with the Outlier Detection and the Adversarial Training defenses, across different adversarial targets (the higher the robustness - the more effective the defense). DAMP ${ }^{\dagger}$ results are the same as in Table 1.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. The performance of each defense technique compared to its average robustness. Average robustness is calculated as the average of the four robustness scores in Table 3. Points marked with a bullet $(\bullet)$ require re-training; points marked with a star $(\star)$ do not require re-training. Outlier Detection provides the highest performance and robustness among the techniques that do not require re-training; Adversarial Training provides the highest performance and robustness among the techniques that do require re-training.
the trade-off between performance and robustness: it is $98 \%$ as accurate as the original model (No Defense) and $74-96 \%$ as robust as the model that ignores variables completely.</p>
<p>Adversarial Training provides a sweet-spot among the techniques that do require re-training: its penalty is $5.02 \%$ degradation in F1 score compared to No Defense, and it achieves over $99 \%$ robustness to DeadCode and $94.48 \%$ robustness to targeted VarName. Adversarial training allows the model to leverage variable names to achieve high performance while not under attack, but also to not overly rely on variable names; it is thus robust to adversarial examples. Adversarial</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. The Outlier Detection defense: the trade-off between robustness and performance-while-not-underattack, with respect to the similarity threshold $\sigma$ on the validation set. The dashed vertical line at $\sigma=2.7$ denotes the value that we chose for $\sigma$ according to the validation set. A lower threshold leads to perfect robustness and lower performance; a higher threshold leads to performance that is equal to the original model's, but the model is also as vulnerable as the original model. The robustness score is the robustness against non-targeted VarName attacks.</p>
<p>Fine-Tuning performs surprisingly worse than Adversarial Training, both in F1 score and in its robustness.</p>
<p>No Vars is $100 \%$ robust as Train Without Vars, but is about 10 F1 points worse than Train Without Vars. The only benefit of No Vars over Train Without Vars is that No Vars can be applied to a model without re-training it.</p>
<p>Reducing the vocabulary size to 100 K and 50 K , results in a negligible decrease in performance while not under attack, compared to not defending at all. However, it also results in roughly the same poor robustness as no defense at all. Reducing the vocabulary size to 10 K hurts performance while not under attack and does not provide much robustness.</p>
<p>These results are visualized in Figure 8.
Table 4 shows the performance of the Outlier Detection and the Adversarial Training defenses across randomly sampled adversarial labels.
6.3.3 Robustness - Performance Trade-off. One of the advantages of the Outlier Detection defense, which we found to be one of the most effective defense techniques, is the ability to tune its trade-off between robustness and performance. Figure 9 shows the trade-off with respect to the similarity threshold $\sigma$. Small values of $\sigma$ make the model with Outlier Detection defense more robust. It becomes almost as robust as No Vars, but perform worse in terms of F1 score. As the value of $\sigma$ increases, the model with Outlier Detection defense becomes less robust, but performs better while not under attack.</p>
<div class="codehilite"><pre><span></span><code><span class="n">void</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="nc">int</span><span class="err">[]</span><span class="w"> </span><span class="k">array</span><span class="p">)</span><span class="err">{</span>
<span class="w">    </span><span class="k">boolean</span><span class="w"> </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">true</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="k">array</span><span class="p">.</span><span class="n">length</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">swapped</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">false</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="k">array</span><span class="p">.</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="k">array</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="k">array</span><span class="o">[</span><span class="n">j+1</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">                </span><span class="nc">int</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">array</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="k">array</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">array</span><span class="o">[</span><span class="n">j+1</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="k">array</span><span class="o">[</span><span class="n">j+1</span><span class="o">]=</span><span class="w"> </span><span class="n">temp</span><span class="p">;</span>
<span class="w">            </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">true</span><span class="p">;</span>
<span class="w">        </span><span class="err">}</span>
<span class="w">    </span><span class="err">}</span>
<span class="err">}</span>
<span class="w">    </span><span class="nl">Prediction</span><span class="p">:</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="p">(</span><span class="mf">98.54</span><span class="o">%</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">void</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="nc">int</span><span class="err">[]</span><span class="w"> </span><span class="n">mstyleids</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">    </span><span class="k">boolean</span><span class="w"> </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">true</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">mstyleids</span><span class="p">.</span><span class="n">length</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">swapped</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">    </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">false</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">mstyleids</span><span class="p">.</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">mstyleids</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">mstyleids</span><span class="o">[</span><span class="n">j+1</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="nc">int</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mstyleids</span><span class="o">[</span><span class="n">J</span><span class="o">]</span><span class="p">;</span>
<span class="w">        </span><span class="n">mstyleids</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mstyleids</span><span class="o">[</span><span class="n">j+1</span><span class="o">]</span><span class="p">;</span>
<span class="w">        </span><span class="n">mstyleids</span><span class="o">[</span><span class="n">j+1</span><span class="o">]=</span><span class="w"> </span><span class="n">temp</span><span class="p">;</span>
<span class="w">        </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">true</span><span class="p">;</span>
<span class="w">    </span><span class="err">}</span>
<span class="err">}</span>
<span class="w">    </span><span class="nl">Prediction</span><span class="p">:</span><span class="w"> </span><span class="k">get</span><span class="w"> </span><span class="p">(</span><span class="mf">99.99</span><span class="o">%</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">void</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="nc">int</span><span class="err">[]</span><span class="w"> </span><span class="n">possiblematches</span><span class="p">)</span><span class="w"> </span><span class="err">\{</span>
<span class="w">    </span><span class="k">boolean</span><span class="w"> </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">true</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">possiblematches</span><span class="p">.</span><span class="n">length</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">swapped</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">\{</span>
<span class="w">        </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">false</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">possiblematches</span><span class="p">.</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">\{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">possiblematches</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">possiblematches</span><span class="o">[</span><span class="n">j+1</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="err">\{</span>
<span class="w">                </span><span class="nc">int</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">possiblematches</span><span class="o">[</span><span class="n">J</span><span class="o">]</span><span class="p">;</span>
<span class="w">                    </span><span class="n">possiblematches</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">possiblematches</span><span class="o">[</span><span class="n">j+1</span><span class="o">]</span><span class="p">;</span>
<span class="w">                    </span><span class="n">possiblematches</span><span class="o">[</span><span class="n">j+1</span><span class="o">]=</span><span class="w"> </span><span class="n">temp</span><span class="p">;</span>
<span class="w">                    </span><span class="n">swapped</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">true</span><span class="p">;</span>
<span class="w">    </span><span class="err">}</span>
<span class="err">}</span>
</code></pre></div>

<p>Fig. 10. A snippet classified correctly as sort by the model of code2vec.org. The same example is classified as get by renaming array to mstyleids and is classified as indexOf by renaming array to possiblematches.</p>
<h1>6.4 Additional Examples</h1>
<p>All examples that are shown in this paper and in appendix A can be experimented with on their original models at http://code2vec.org and https://github.com/microsoft/tf-gnn-samples.</p>
<p>Figure 10 shows additional targeted attacks against the "sort" example from Figure 1. Renaming the variable array to mstyleids changes the prediction to get with a probability of $99.99 \%$; renaming array to possiblematches changes the prediction to indexOf with a probability of $86.99 \%$. The predicted adversarial labels (get and indexOf) were chosen arbitrarily before finding the variable name replacements.
Transferability Occasionally, a dead code attack is transferable across examples, and has the same effect even in different examples. This is demonstrated in Figure 11, where adding the unused variable declaration int introsorter $=0$; to each of the multiple snippets changes their prediction to sort with probability of $100 \%$. This effect is reminiscent of the Adversarial Patch [Brown et al. 2017], that was shown to force an image classifier to predict a specific label, regardless of the input example. However, except for a few cases, we found that adversarial examples generally do not transfer across examples. We also did not find significant evidence that adversarial examples transfer across models that were trained on the same dataset, e.g., from GNN-FILM to GGNN. The question of whether adversarial examples are transferable in discrete domains such as code remains open.
GNN Examples Figure 12 shows A C# VarMisuse example that is classified correctly as _getterBuilder in the method GetGetter by the GGNN model. Given the code and the target</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ As recently shown by Wallace et al. [2020], this is a reasonable assumption. An attacker can imitate the model under attack by: training an imitation model using labels achieved by querying the original model; crafting adversarial examples using the imitation model; and transferring these adversarial examples back to the original model.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8793 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8793</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8793</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-275906943</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.14427v2.pdf" target="_blank">GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better</a></p>
                <p><strong>Paper Abstract:</strong> The success of Large Language Models (LLMs) in various domains has led researchers to apply them to graph-related problems by converting graph data into natural language text. However, unlike graph data, natural language inherently has sequential order. We observe a counter-intuitive fact that when the order of nodes or edges in the natural language description of a graph is shuffled, despite describing the same graph, model performance fluctuates between high performance and random guessing. Additionally, due to LLMs' limited input context length, current methods typically randomly sample neighbors of target nodes as representatives of their neighborhood, which may not always be effective for accurate reasoning. To address these gaps, we introduce GraphSOS (Graph Sampling and Order Selection). This novel model framework features an Order Selector Module to ensure proper serialization order of the graph and a Subgraph Sampling Module to sample subgraphs with better structure for better reasoning. Furthermore, we propose Graph CoT obtained through distillation, and enhance LLM's reasoning and zero-shot learning capabilities for graph tasks through instruction tuning. Experiments on multiple datasets for node classification and graph question-answering demonstrate that GraphSOS improves LLMs' performance and generalization ability on graph tasks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8793.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8793.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node-Edge List Serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feature List + Edge List Natural Language Serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text serialization that converts a graph's node features into a Feature List and its edges into an Edge List (pairs or triples) and presents both as natural-language lists for input to LLMs; used as the primary graph-to-text encoding in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Feature List + Edge List (node-edge list serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert node set V and node feature matrix X into a Feature List (e.g., '[Node 0: <text> ...]') and convert edge set E into an Edge List of ordered pairs or triples (e.g., '[(0,1), (1,2), ...]' or knowledge triples). The two lists are concatenated in some sequence to form the single natural-language sequence g(G) consumed by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAG) and knowledge graphs (graph QA)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Explicit construction of two lists: (1) Feature List: textual attributes for each node (from X) listed as entries 'Node i: <text>'; (2) Edge List: edges written as ordered pairs or triples; for knowledge-graph QA edges can be encoded as (subject, predicate, object) triples. The paper denotes this overall conversion function as g(G) (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>TAG node classification and graph question-answering (graph QA) — used for both supervised fine-tuning and zero-shot inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used within GraphSOS (serialization + SSM + OSM + GraphCoT) with LLaMA 3-8B: TAG node classification examples — Citeseer 77.0 ± 0.5%, Cora 70.5 ± 0.8%, Texas 76.5 ± 0.7% (Table 2). Graph QA example — MetaQA accuracy 89.6 ± 1.0% (Table 3). Ablation shows serialization + OSM/SSM outperforms random-ordered serializations (see Table 4 and Figure 10).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Baseline Graph-LLM encodings that also serialize graphs are outperformed by GraphSOS' pipeline: e.g., GraphGPT and GraphWiz (which also rely on serialized descriptions) show greater order sensitivity and lower or less-stable zero-shot accuracy in many settings (see Table 2 and Table 3). The paper compares random-permutation serializations vs. OSM-selected serializations and finds OSM yields higher accuracy and less fluctuation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, direct mapping from graph structure to text that is compatible with off-the-shelf LLMs; interpretable (Feature List and Edge List are human-readable); enables modular improvements (SSM/OSM/GraphCoT). When combined with SSM and OSM, yields state-of-the-art or strong results on the studied TAG and graph-QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Inherently sequential — different valid permutations describe the same graph but produce dramatically different LLM behavior (strong order sensitivity); naive serialization can be too long for LLM context windows, forcing subgraph sampling; factorial number of valid orderings (impractical to enumerate) leads to the need for heuristics (OSM) and introduces runtime overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When element order in Feature List/Edge List is unfavorable, LLMs often collapse to near-random predictions (observed in zero-shot order-sensitivity experiments); overly long serializations (large n_max) can degrade reasoning; random neighbor sampling (without SSM) can mix heterophilous neighbors and harm node classification, especially on heterophily datasets (e.g., Texas) as shown in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8793.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8793.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Order Selector Module (OSM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Order Selector Module — multiple-order serialization with selection via cross-attention + Gumbel-Softmax</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module that generates multiple candidate permutations (m candidates) of the serialized Feature List and Edge List, encodes each candidate and the question with a pre-trained encoder, scores them with cross-attention, and selects a single (relatively) optimal ordering using Gumbel-Softmax for downstream LLM consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Order-selector-driven serialization (multiple permutations + selection)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given a subgraph, OSM creates m random permutations of the Feature List and Edge List; encodes question and each candidate via a pre-trained encoder (BERT [CLS]); computes cross-attention weights between the question and candidates; applies Gumbel-Softmax to produce a differentiable one-hot selection of the best candidate order to feed to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applies to serialized subgraphs from TAGs and knowledge graphs (i.e., any graph materialized as Feature/Edge/Triple lists)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Enumerate m permutations (m << factorial of list length). For each candidate sequence, produce an embedding with a pre-trained encoder. Use the question embedding as query and candidates' embeddings as keys in cross-attention to produce selection weights; use Gumbel-Softmax to pick one candidate ordering as the final serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification and graph QA (used to stabilize and improve LLM performance across orderings)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>OSM improves accuracy and reduces variance. Example: LLaMA 3-8B GraphSOS-2stage-SSM-OSM on MetaQA: 92.7 ± 0.7% vs GraphSOS-2stage-SSM (no OSM) 89.8 ± 1.3% (Table 3). Ablation (Table 4): Cora w/o OSM = 68.5 ± 1.1% vs GraphSOS = 70.5 ± 0.8%. Table 7 (MetaQA) shows accuracy increasing with m: m=4 => 84.5% (0.81s), m=10 => 89.6% (1.02s), m=20 => 90.2% (1.37s).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to random permutations (no selection), OSM-selected ordering yields higher and more stable performance (smaller fluctuations across 10 random-permutation trials, Figure 10). The paper contrasts OSM with naive serializations and shows lower variance and higher mean accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces LLM order sensitivity by selecting relatively better serializations for a given question; improves mean accuracy and reduces performance variance across different textual orders; differentiable selection enables end-to-end training with frozen LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Enumerating permutation candidates is approximate (m must be small for tractability), so OSM only finds a relatively optimal order within the sampled candidates; increases inference time proportional to m; choice of m trades off accuracy and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If m is too small, the true optimal order may not be sampled, limiting gains. Increasing m improves accuracy but increases inference time (Table 7). OSM selection is limited by the representational power of the encoder used to score permutations and by training signal when LLM is frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8793.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8793.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subgraph Sampling Module (SSM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-weighted Random-Walk Subgraph Sampling Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned subgraph-sampling procedure that uses node textual encodings (BERT [CLS]) and multi-head scaled dot-product attention to define transition probabilities for random walks, preferentially retaining neighbors more correlated with the target node; trained with a separate Scoring Model using preference-style objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Attention-guided random-walk subgraph sampling + serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode target node and k-hop neighbors with a pre-trained text encoder (BERT [CLS]). Compute scaled dot-product multi-head cross-attention between target (query) and neighbor embeddings to produce attention weights. Use the attention weights as transition probabilities P(u_j | v) for random-walk sampling until a maximum sample size n_max is reached; sampled nodes + target form the subgraph to be serialized.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAG), applicable in principle to any graph where node attributes can be embedded as text</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>1) Extract k-hop neighbors (k=2 in experiments) and encode nodes with a pre-trained encoder (BERT). 2) Compute multi-head attention (h=4) between target node embedding and neighbor embeddings. 3) Use the attention vector as the random-walk transition probabilities to sample neighbors until |V_Gv| ≤ n_max. 4) Serialize the sampled subgraph with the Feature List + Edge List representation g(G_v).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>TAG node classification and graph QA (as subgraph input to LLM pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SSM yields consistent gains vs. random sampling. Ablation (Table 4): w/o SSM (random neighbor sampling) with LLaMA3 gives Citeseer 74.9 ± 1.2% vs GraphSOS (with SSM+OSM+GraphCoT) 77.0 ± 0.5%; Texas (heterophily) w/o SSM 69.6 ± 1.4% vs GraphSOS 76.5 ± 0.7%. Figure 9 shows SSM samples a higher proportion of same-class neighbors compared to random sampling (quantitative proportions shown in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to the mainstream random-n-neighbor sampling used in prior Graph-LLM predictors, SSM selects neighbors more correlated to the target node (by attention) and is trained to prefer homophilous subgraphs (or, by construction, any desired structural property). The paper demonstrates SSM outperforms random sampling in node-classification tasks, especially on heterophilous graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Learns to sample task-relevant subgraphs rather than relying on uniform/random sampling; reduces mixing of dissimilar neighbors in heterophily scenarios; improves downstream classification accuracy and yields subgraphs with higher proportion of same-class neighbors.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires a separate Scoring Model and training data with constructed positive/negative examples (computational and annotation cost); depends on text-encoder quality (BERT [CLS]); introduces additional complexity and hyperparameters (n_max, k, number of heads).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If SSM is trained with poorly constructed positives/negatives or limited data, it may not generalize; n_max too low or too high degrades performance (sensitivity shown in Appendix C.3 and Figure 11); SSM as implemented lacks explicit structure-aware components (authors note this in Future Work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8793.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8793.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple / Knowledge Triple List</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Triple List Serialization (subject-predicate-object triples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text encoding for knowledge graphs where edges are converted to (subject, predicate, object) triples and presented as a list for LLM input; used for graph QA tasks such as MetaQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Knowledge Triple List (triple serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent each fact/edge as a textual triple '(subject, predicate, object)' and list these triples (Knowledge Triple List) as the serialized graph input. For graph-QA tasks, the triple list is provided (often many triples per question) and then optionally sampled/pruned before feeding the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / structured QA graphs (e.g., MetaQA movie knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Retrieve 1-hop and 2-hop triples centered on the target entity; write them as textual triples; optionally apply SSM or sampling strategies to reduce to context window size; feed triple list as part of g(G).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph question-answering (MetaQA and graph reasoning QA datasets), path reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Within GraphSOS, using triple-list serialization plus OSM/SSM/GraphCoT, LLaMA3 achieved MetaQA accuracy up to 92.7 ± 0.7% (GraphSOS-2stage-OSM) in supervised settings (Table 3). Other complex reasoning tasks (cycle, connect, bipartite, topology, shortest, triangle, flow, hamilton, subgraph) show improved performance with GraphSOS compared to many baselines (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Triple-list serializations are standard for QA tasks; GraphSOS's contribution is sampling and ordering of triples (via SSM and OSM) plus GraphCoT for reasoning, which together outperform naive triple-list feeding and many baselines (including several GNNs and prior Graph LLM approaches) in the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Aligns naturally with knowledge-graph format; compatible with chain-of-thought reasoning; when combined with SSM/OSM, leads to strong supervised and zero-shot QA results.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Large numbers of triples (50–1,198 per question in MetaQA) exceed LLM context windows, requiring sampling/selection; naive ordering of triples is subject to the same order-sensitivity issues and can produce unstable LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If irrelevant or misleading triples are sampled (random sampling), LLM performance can drop; long triple lists without careful sampling or ordering can produce inferior reasoning and accuracy; some graph-reasoning tasks (e.g., Hamiltonian path) remain challenging in zero-shot for many LLMs unless combined with GraphCoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8793.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8793.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Talk-Like-A-Graph encodings (GPT-Adjacency / GPT-Incident / GPT-Expert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encodings from 'Talk like a graph: Encoding graphs for large language models' (GPT-Adjacency, GPT-Incident, GPT-Expert)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior graph-to-text encoding schemes described in Fatemi et al. (2023) that format graphs for LLMs using adjacency lists, incidence representations, or 'expert' formats to communicate graph structure to language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GPT-Adjacency / GPT-Incident / GPT-Expert encodings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Alternative graph serializations: adjacency-based textual encoding (adjacency lists), incidence-based encodings, and heuristic 'expert' encodings that aim to represent graph structure compactly and in LLM-friendly text form. In the paper these are referenced as representative prior encodings evaluated in baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs / text-attributed graphs / knowledge graphs (depending on encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not implemented in this paper; referenced as prior work. Each encoding defines a deterministic textual mapping (adjacency lists, incidence lists, or customized expert text) from graph structure to text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Referenced as baselines for node classification and graph QA in the paper's experiments (Table 2 and Table 3 reports their performance as baselines using GPT-3.5-turbo-16k in the original Talk-Like-A-Graph work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in this paper's baselines (Table 2/3): examples include on TAG node classification (zero-shot) GPT-Adjacency GPT-Incident GPT-Expert produce low accuracies on some datasets (e.g., Citeseer: ~17.8–18.6% in Table 2), while on some homophilic datasets they can show high accuracy (e.g., Texas/Wisconsin in Table 2 where closed-source methods sometimes have larger numbers). These baseline numbers reflect the performance reported or reproduced in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper contrasts these prior encodings' baseline performance with GraphSOS; GraphSOS (with SSM/OSM/GraphCoT) yields higher and more stable performance on many datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Designed specifically to encode graph structural information for LLMs; some encodings may be compact or intuitive (adjacency/incidence).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Prior encodings still suffer from order sensitivity and context-window limitations; in the paper's experimental reproduction they often underperform GraphSOS, especially in heterophily or zero-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>As with any serialized representation, poor ordering or presence of many irrelevant neighbors/triples leads to degraded LLM reasoning; baselines based on these encodings exhibited large performance variability depending on ordering and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8793.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8793.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Chain-of-Thought (Graph CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Chain-of-Thought Distillation (Graph CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled chain-of-thought answer format for graph tasks obtained from a large model (GPT-4o) that explicitly demonstrates graph-structure-aware reasoning steps; used to instruction-tune the LLM to follow graph reasoning patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Chain-of-Thought (Graph CoT) answer format</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generate training answers that contain explicit stepwise reasoning about graph structure (e.g., identify neighbors, aggregate evidence, deduce category) using GPT-4o; use distilled Graph CoT answers as winning responses during Direct Preference Optimization (DPO) and as instruction-tuning targets so the model learns to produce structured reasoning text alongside final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applies to TAG and graph-QA tasks where reasoning about structure is required</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not a graph-to-text encoding of the graph itself but a textual representation of the reasoning process over serialized graphs. Construct Graph CoT examples by prompting GPT-4o to produce analysis steps and final answers given g(G) and Q, then use these as supervised and preference targets in two-stage tuning (instruction tuning + DPO).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification and graph QA, particularly for enabling zero-shot reasoning and improving LLM logical steps over graph inputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Graph CoT improves downstream performance and zero-shot generalization: Table 4 shows LLaMA3 GraphSOS (two-stage with Graph CoT) outperforms the 'w/o Graph CoT' variant; e.g., on MetaQA subgraph task GraphSOS = 72.9 ± 0.8% vs w/o Graph CoT = 64.9 ± 0.4% (other datasets also show consistent gains).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Two-stage tuning (instruction tuning then DPO using Graph CoT as preference signal) consistently outperforms single-stage instruction tuning (SFT only) in experiments; Graph CoT-equipped models perform nearer or above specialized supervised models on some zero-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Teaches the LLM to follow multi-step graph-reasoning procedures, improving interpretability and zero-shot transfer; reduces tendency to 'skip' structural aggregation step and directly guess an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on generation from a very large reference model (GPT-4o) which can be expensive; distilled reasoning can still carry hallucinations or incorrect logical steps; quality depends on the quality of distilled Graph CoT examples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If Graph CoT demonstrations contain incorrect reasoning or are biased, DPO can push the model toward flawed chains; the paper warns about residual hallucination risk and recommends validation mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models. <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models. <em>(Rating: 2)</em></li>
                <li>Graphwiz: An instruction-following language model for graph computational problems. <em>(Rating: 2)</em></li>
                <li>Can large language models understand graph structured data ? an empirical evaluation and benchmarking. <em>(Rating: 2)</em></li>
                <li>G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8793",
    "paper_id": "paper-275906943",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Node-Edge List Serialization",
            "name_full": "Feature List + Edge List Natural Language Serialization",
            "brief_description": "A text serialization that converts a graph's node features into a Feature List and its edges into an Edge List (pairs or triples) and presents both as natural-language lists for input to LLMs; used as the primary graph-to-text encoding in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Feature List + Edge List (node-edge list serialization)",
            "representation_description": "Convert node set V and node feature matrix X into a Feature List (e.g., '[Node 0: &lt;text&gt; ...]') and convert edge set E into an Edge List of ordered pairs or triples (e.g., '[(0,1), (1,2), ...]' or knowledge triples). The two lists are concatenated in some sequence to form the single natural-language sequence g(G) consumed by the LLM.",
            "graph_type": "Text-Attributed Graphs (TAG) and knowledge graphs (graph QA)",
            "conversion_method": "Explicit construction of two lists: (1) Feature List: textual attributes for each node (from X) listed as entries 'Node i: &lt;text&gt;'; (2) Edge List: edges written as ordered pairs or triples; for knowledge-graph QA edges can be encoded as (subject, predicate, object) triples. The paper denotes this overall conversion function as g(G) (see Table 1).",
            "downstream_task": "TAG node classification and graph question-answering (graph QA) — used for both supervised fine-tuning and zero-shot inference",
            "performance_metrics": "When used within GraphSOS (serialization + SSM + OSM + GraphCoT) with LLaMA 3-8B: TAG node classification examples — Citeseer 77.0 ± 0.5%, Cora 70.5 ± 0.8%, Texas 76.5 ± 0.7% (Table 2). Graph QA example — MetaQA accuracy 89.6 ± 1.0% (Table 3). Ablation shows serialization + OSM/SSM outperforms random-ordered serializations (see Table 4 and Figure 10).",
            "comparison_to_others": "Baseline Graph-LLM encodings that also serialize graphs are outperformed by GraphSOS' pipeline: e.g., GraphGPT and GraphWiz (which also rely on serialized descriptions) show greater order sensitivity and lower or less-stable zero-shot accuracy in many settings (see Table 2 and Table 3). The paper compares random-permutation serializations vs. OSM-selected serializations and finds OSM yields higher accuracy and less fluctuation.",
            "advantages": "Simple, direct mapping from graph structure to text that is compatible with off-the-shelf LLMs; interpretable (Feature List and Edge List are human-readable); enables modular improvements (SSM/OSM/GraphCoT). When combined with SSM and OSM, yields state-of-the-art or strong results on the studied TAG and graph-QA benchmarks.",
            "disadvantages": "Inherently sequential — different valid permutations describe the same graph but produce dramatically different LLM behavior (strong order sensitivity); naive serialization can be too long for LLM context windows, forcing subgraph sampling; factorial number of valid orderings (impractical to enumerate) leads to the need for heuristics (OSM) and introduces runtime overhead.",
            "failure_cases": "When element order in Feature List/Edge List is unfavorable, LLMs often collapse to near-random predictions (observed in zero-shot order-sensitivity experiments); overly long serializations (large n_max) can degrade reasoning; random neighbor sampling (without SSM) can mix heterophilous neighbors and harm node classification, especially on heterophily datasets (e.g., Texas) as shown in ablations.",
            "uuid": "e8793.0",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Order Selector Module (OSM)",
            "name_full": "Order Selector Module — multiple-order serialization with selection via cross-attention + Gumbel-Softmax",
            "brief_description": "A module that generates multiple candidate permutations (m candidates) of the serialized Feature List and Edge List, encodes each candidate and the question with a pre-trained encoder, scores them with cross-attention, and selects a single (relatively) optimal ordering using Gumbel-Softmax for downstream LLM consumption.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Order-selector-driven serialization (multiple permutations + selection)",
            "representation_description": "Given a subgraph, OSM creates m random permutations of the Feature List and Edge List; encodes question and each candidate via a pre-trained encoder (BERT [CLS]); computes cross-attention weights between the question and candidates; applies Gumbel-Softmax to produce a differentiable one-hot selection of the best candidate order to feed to the LLM.",
            "graph_type": "Applies to serialized subgraphs from TAGs and knowledge graphs (i.e., any graph materialized as Feature/Edge/Triple lists)",
            "conversion_method": "Enumerate m permutations (m &lt;&lt; factorial of list length). For each candidate sequence, produce an embedding with a pre-trained encoder. Use the question embedding as query and candidates' embeddings as keys in cross-attention to produce selection weights; use Gumbel-Softmax to pick one candidate ordering as the final serialization.",
            "downstream_task": "Node classification and graph QA (used to stabilize and improve LLM performance across orderings)",
            "performance_metrics": "OSM improves accuracy and reduces variance. Example: LLaMA 3-8B GraphSOS-2stage-SSM-OSM on MetaQA: 92.7 ± 0.7% vs GraphSOS-2stage-SSM (no OSM) 89.8 ± 1.3% (Table 3). Ablation (Table 4): Cora w/o OSM = 68.5 ± 1.1% vs GraphSOS = 70.5 ± 0.8%. Table 7 (MetaQA) shows accuracy increasing with m: m=4 =&gt; 84.5% (0.81s), m=10 =&gt; 89.6% (1.02s), m=20 =&gt; 90.2% (1.37s).",
            "comparison_to_others": "Compared to random permutations (no selection), OSM-selected ordering yields higher and more stable performance (smaller fluctuations across 10 random-permutation trials, Figure 10). The paper contrasts OSM with naive serializations and shows lower variance and higher mean accuracy.",
            "advantages": "Reduces LLM order sensitivity by selecting relatively better serializations for a given question; improves mean accuracy and reduces performance variance across different textual orders; differentiable selection enables end-to-end training with frozen LLM.",
            "disadvantages": "Enumerating permutation candidates is approximate (m must be small for tractability), so OSM only finds a relatively optimal order within the sampled candidates; increases inference time proportional to m; choice of m trades off accuracy and latency.",
            "failure_cases": "If m is too small, the true optimal order may not be sampled, limiting gains. Increasing m improves accuracy but increases inference time (Table 7). OSM selection is limited by the representational power of the encoder used to score permutations and by training signal when LLM is frozen.",
            "uuid": "e8793.1",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Subgraph Sampling Module (SSM)",
            "name_full": "Attention-weighted Random-Walk Subgraph Sampling Module",
            "brief_description": "A learned subgraph-sampling procedure that uses node textual encodings (BERT [CLS]) and multi-head scaled dot-product attention to define transition probabilities for random walks, preferentially retaining neighbors more correlated with the target node; trained with a separate Scoring Model using preference-style objectives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Attention-guided random-walk subgraph sampling + serialization",
            "representation_description": "Encode target node and k-hop neighbors with a pre-trained text encoder (BERT [CLS]). Compute scaled dot-product multi-head cross-attention between target (query) and neighbor embeddings to produce attention weights. Use the attention weights as transition probabilities P(u_j | v) for random-walk sampling until a maximum sample size n_max is reached; sampled nodes + target form the subgraph to be serialized.",
            "graph_type": "Text-Attributed Graphs (TAG), applicable in principle to any graph where node attributes can be embedded as text",
            "conversion_method": "1) Extract k-hop neighbors (k=2 in experiments) and encode nodes with a pre-trained encoder (BERT). 2) Compute multi-head attention (h=4) between target node embedding and neighbor embeddings. 3) Use the attention vector as the random-walk transition probabilities to sample neighbors until |V_Gv| ≤ n_max. 4) Serialize the sampled subgraph with the Feature List + Edge List representation g(G_v).",
            "downstream_task": "TAG node classification and graph QA (as subgraph input to LLM pipeline)",
            "performance_metrics": "SSM yields consistent gains vs. random sampling. Ablation (Table 4): w/o SSM (random neighbor sampling) with LLaMA3 gives Citeseer 74.9 ± 1.2% vs GraphSOS (with SSM+OSM+GraphCoT) 77.0 ± 0.5%; Texas (heterophily) w/o SSM 69.6 ± 1.4% vs GraphSOS 76.5 ± 0.7%. Figure 9 shows SSM samples a higher proportion of same-class neighbors compared to random sampling (quantitative proportions shown in paper).",
            "comparison_to_others": "Compared to the mainstream random-n-neighbor sampling used in prior Graph-LLM predictors, SSM selects neighbors more correlated to the target node (by attention) and is trained to prefer homophilous subgraphs (or, by construction, any desired structural property). The paper demonstrates SSM outperforms random sampling in node-classification tasks, especially on heterophilous graphs.",
            "advantages": "Learns to sample task-relevant subgraphs rather than relying on uniform/random sampling; reduces mixing of dissimilar neighbors in heterophily scenarios; improves downstream classification accuracy and yields subgraphs with higher proportion of same-class neighbors.",
            "disadvantages": "Requires a separate Scoring Model and training data with constructed positive/negative examples (computational and annotation cost); depends on text-encoder quality (BERT [CLS]); introduces additional complexity and hyperparameters (n_max, k, number of heads).",
            "failure_cases": "If SSM is trained with poorly constructed positives/negatives or limited data, it may not generalize; n_max too low or too high degrades performance (sensitivity shown in Appendix C.3 and Figure 11); SSM as implemented lacks explicit structure-aware components (authors note this in Future Work).",
            "uuid": "e8793.2",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Triple / Knowledge Triple List",
            "name_full": "Knowledge Triple List Serialization (subject-predicate-object triples)",
            "brief_description": "A graph-to-text encoding for knowledge graphs where edges are converted to (subject, predicate, object) triples and presented as a list for LLM input; used for graph QA tasks such as MetaQA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Knowledge Triple List (triple serialization)",
            "representation_description": "Represent each fact/edge as a textual triple '(subject, predicate, object)' and list these triples (Knowledge Triple List) as the serialized graph input. For graph-QA tasks, the triple list is provided (often many triples per question) and then optionally sampled/pruned before feeding the LLM.",
            "graph_type": "Knowledge graphs / structured QA graphs (e.g., MetaQA movie knowledge graph)",
            "conversion_method": "Retrieve 1-hop and 2-hop triples centered on the target entity; write them as textual triples; optionally apply SSM or sampling strategies to reduce to context window size; feed triple list as part of g(G).",
            "downstream_task": "Graph question-answering (MetaQA and graph reasoning QA datasets), path reasoning tasks",
            "performance_metrics": "Within GraphSOS, using triple-list serialization plus OSM/SSM/GraphCoT, LLaMA3 achieved MetaQA accuracy up to 92.7 ± 0.7% (GraphSOS-2stage-OSM) in supervised settings (Table 3). Other complex reasoning tasks (cycle, connect, bipartite, topology, shortest, triangle, flow, hamilton, subgraph) show improved performance with GraphSOS compared to many baselines (Table 3).",
            "comparison_to_others": "Triple-list serializations are standard for QA tasks; GraphSOS's contribution is sampling and ordering of triples (via SSM and OSM) plus GraphCoT for reasoning, which together outperform naive triple-list feeding and many baselines (including several GNNs and prior Graph LLM approaches) in the evaluated benchmarks.",
            "advantages": "Aligns naturally with knowledge-graph format; compatible with chain-of-thought reasoning; when combined with SSM/OSM, leads to strong supervised and zero-shot QA results.",
            "disadvantages": "Large numbers of triples (50–1,198 per question in MetaQA) exceed LLM context windows, requiring sampling/selection; naive ordering of triples is subject to the same order-sensitivity issues and can produce unstable LLM outputs.",
            "failure_cases": "If irrelevant or misleading triples are sampled (random sampling), LLM performance can drop; long triple lists without careful sampling or ordering can produce inferior reasoning and accuracy; some graph-reasoning tasks (e.g., Hamiltonian path) remain challenging in zero-shot for many LLMs unless combined with GraphCoT.",
            "uuid": "e8793.3",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Talk-Like-A-Graph encodings (GPT-Adjacency / GPT-Incident / GPT-Expert)",
            "name_full": "Encodings from 'Talk like a graph: Encoding graphs for large language models' (GPT-Adjacency, GPT-Incident, GPT-Expert)",
            "brief_description": "Prior graph-to-text encoding schemes described in Fatemi et al. (2023) that format graphs for LLMs using adjacency lists, incidence representations, or 'expert' formats to communicate graph structure to language models.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models.",
            "mention_or_use": "mention",
            "representation_name": "GPT-Adjacency / GPT-Incident / GPT-Expert encodings",
            "representation_description": "Alternative graph serializations: adjacency-based textual encoding (adjacency lists), incidence-based encodings, and heuristic 'expert' encodings that aim to represent graph structure compactly and in LLM-friendly text form. In the paper these are referenced as representative prior encodings evaluated in baselines.",
            "graph_type": "General graphs / text-attributed graphs / knowledge graphs (depending on encoding)",
            "conversion_method": "Not implemented in this paper; referenced as prior work. Each encoding defines a deterministic textual mapping (adjacency lists, incidence lists, or customized expert text) from graph structure to text.",
            "downstream_task": "Referenced as baselines for node classification and graph QA in the paper's experiments (Table 2 and Table 3 reports their performance as baselines using GPT-3.5-turbo-16k in the original Talk-Like-A-Graph work).",
            "performance_metrics": "Reported in this paper's baselines (Table 2/3): examples include on TAG node classification (zero-shot) GPT-Adjacency GPT-Incident GPT-Expert produce low accuracies on some datasets (e.g., Citeseer: ~17.8–18.6% in Table 2), while on some homophilic datasets they can show high accuracy (e.g., Texas/Wisconsin in Table 2 where closed-source methods sometimes have larger numbers). These baseline numbers reflect the performance reported or reproduced in the experiments.",
            "comparison_to_others": "Paper contrasts these prior encodings' baseline performance with GraphSOS; GraphSOS (with SSM/OSM/GraphCoT) yields higher and more stable performance on many datasets.",
            "advantages": "Designed specifically to encode graph structural information for LLMs; some encodings may be compact or intuitive (adjacency/incidence).",
            "disadvantages": "Prior encodings still suffer from order sensitivity and context-window limitations; in the paper's experimental reproduction they often underperform GraphSOS, especially in heterophily or zero-shot tasks.",
            "failure_cases": "As with any serialized representation, poor ordering or presence of many irrelevant neighbors/triples leads to degraded LLM reasoning; baselines based on these encodings exhibited large performance variability depending on ordering and dataset.",
            "uuid": "e8793.4",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Graph Chain-of-Thought (Graph CoT)",
            "name_full": "Graph Chain-of-Thought Distillation (Graph CoT)",
            "brief_description": "A distilled chain-of-thought answer format for graph tasks obtained from a large model (GPT-4o) that explicitly demonstrates graph-structure-aware reasoning steps; used to instruction-tune the LLM to follow graph reasoning patterns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph Chain-of-Thought (Graph CoT) answer format",
            "representation_description": "Generate training answers that contain explicit stepwise reasoning about graph structure (e.g., identify neighbors, aggregate evidence, deduce category) using GPT-4o; use distilled Graph CoT answers as winning responses during Direct Preference Optimization (DPO) and as instruction-tuning targets so the model learns to produce structured reasoning text alongside final answers.",
            "graph_type": "Applies to TAG and graph-QA tasks where reasoning about structure is required",
            "conversion_method": "Not a graph-to-text encoding of the graph itself but a textual representation of the reasoning process over serialized graphs. Construct Graph CoT examples by prompting GPT-4o to produce analysis steps and final answers given g(G) and Q, then use these as supervised and preference targets in two-stage tuning (instruction tuning + DPO).",
            "downstream_task": "Node classification and graph QA, particularly for enabling zero-shot reasoning and improving LLM logical steps over graph inputs",
            "performance_metrics": "Graph CoT improves downstream performance and zero-shot generalization: Table 4 shows LLaMA3 GraphSOS (two-stage with Graph CoT) outperforms the 'w/o Graph CoT' variant; e.g., on MetaQA subgraph task GraphSOS = 72.9 ± 0.8% vs w/o Graph CoT = 64.9 ± 0.4% (other datasets also show consistent gains).",
            "comparison_to_others": "Two-stage tuning (instruction tuning then DPO using Graph CoT as preference signal) consistently outperforms single-stage instruction tuning (SFT only) in experiments; Graph CoT-equipped models perform nearer or above specialized supervised models on some zero-shot tasks.",
            "advantages": "Teaches the LLM to follow multi-step graph-reasoning procedures, improving interpretability and zero-shot transfer; reduces tendency to 'skip' structural aggregation step and directly guess an answer.",
            "disadvantages": "Relies on generation from a very large reference model (GPT-4o) which can be expensive; distilled reasoning can still carry hallucinations or incorrect logical steps; quality depends on the quality of distilled Graph CoT examples.",
            "failure_cases": "If Graph CoT demonstrations contain incorrect reasoning or are biased, DPO can push the model toward flawed chains; the paper warns about residual hallucination risk and recommends validation mechanisms.",
            "uuid": "e8793.5",
            "source_info": {
                "paper_title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models.",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models.",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "Graphwiz: An instruction-following language model for graph computational problems.",
            "rating": 2,
            "sanitized_title": "graphwiz_an_instructionfollowing_language_model_for_graph_computational_problems"
        },
        {
            "paper_title": "Can large language models understand graph structured data ? an empirical evaluation and benchmarking.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering.",
            "rating": 1,
            "sanitized_title": "gretriever_retrievalaugmented_generation_for_textual_graph_understanding_and_question_answering"
        }
    ],
    "cost": 0.0175675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better
12 Feb 2025</p>
<p>Xu Chu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#104;&#117;&#120;&#117;&#64;&#115;&#116;&#117;&#46;&#112;&#107;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#99;&#104;&#117;&#120;&#117;&#64;&#115;&#116;&#117;&#46;&#112;&#107;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Hanlin Xue 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Zhijie Tan 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Bingce Wang 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Tong Mo 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Weiping Li 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>Robbin Cindy 
School of Software and Microelectronics
Peking University
BeijingChina</p>
<p>GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better
12 Feb 202511E69D96EC518F508DE047F0C99986A3arXiv:2501.14427v3[cs.LG]Exp 1: Nodes: [Robin: A boyCindy: A girl]Edges: [(Robinis_friend_ofCindy)(Cindyis_friend_ofRobin)] Attribute: A boy Attribute: A boy
The success of Large Language Models (LLMs) in various domains has led researchers to apply them to graph-related problems by converting graph data into natural language text.However, unlike graph data, natural language inherently has sequential order.We observe a counter-intuitive fact that when the order of nodes or edges in the natural language description of a graph is shuffled, despite describing the same graph, model performance fluctuates between high performance and random guessing.Additionally, due to LLMs' limited input context length, current methods typically randomly sample neighbors of target nodes as representatives of their neighborhood, which may not always be effective for accurate reasoning.To address these gaps, we introduce Graph-SOS (Graph Sampling and Order Selection).This novel model framework features an Order Selector Module to ensure proper serialization order of the graph and a Subgraph Sampling Module to sample subgraphs with better structure for better reasoning.Furthermore, we propose Graph CoT obtained through distillation, and enhance LLM's reasoning and zero-shot learning capabilities for graph tasks through instruction tuning.Experiments on multiple datasets for node classification and graph question-answering demonstrate that GraphSOS improves LLMs' performance and generalization ability on graph tasks.</p>
<p>Introduction</p>
<p>The recent success of Large Language Models (LLMs) (Touvron et al., 2023;Bai et al., 2023) motivates researchers to explore their potential in handling tasks across various modalities, including vision, speech, and tabular data (Li et al., 2023;Fang et al., 2024;Xia et al., 2024), as well as graph data.As a non-Euclidean geometric structure, graphs are indispensable in representing and solving numerous applications, including social network analysis (Kumar et al., 2022;Liu et al., 2024), recommendation systems (Fan et al., 2019;Luo et al., 2024), and spatiotemporal prediction (Pareja et al., 2020;Zhu et al., 2023).Many Graph LLM studiess (Fatemi et al., 2023;Guo et al., 2023;Tang et al., 2024;Chen et al., 2024) focus on converting graph data into natural language text and inputting it along with questions into closed-source LLMs or LLMs fine-tuned with graph tasks.LLMs complete graph tasks based on their inherent knowledge and reasoning capabilities, such as node classification (Tang et al., 2024) and graph questionanswering (Chen et al., 2024).</p>
<p>Despite promising results, we identify two concerning issues and raise questions accordingly.Question I: Do LLMs truly understand and process graph topological structures correctly?In graph learning using LLM as predictor (Chen et al., 2023), the mainstream paradigm serializes graphs using natural language descriptions of nodes and edges (Ren et al., 2024), as shown in Figure 1.Since natural language sequences are one-dimensional, the same graph can have multiple equivalent descriptive orders.However, research on both LLMs and Multimodal Large Language Models (MLLMs) indicates that LLMs/MLLMs have better prompt orders (though no universal optimal order exists for models or tasks) (Lu et al., 2022;Tan et al., 2024).Models perform well when the ordering is correct, while other orders lead to near-random performance, which may affect model performance on graph tasks.We conduct node classification tasks on the text-attributed graph dataset Cora (Chen et al., 2023) and graph question-answering tasks on the movie knowledge graph dataset MetaQA (Zhang et al.,  2018).We test LLMs of different architectures and scales (Qwen 2.5 (Team, 2024), LLaMA 3 (Dubey et al., 2024)) and two advanced Graph LLMs (GraphGPT (Tang et al., 2024), GraphWiz (Chen et al., 2024)) under zero-shot learning settings (Kojima et al., 2022).For each dataset, we randomly permute the order of nodes and edges in the serialized graphs and conduct 10 independent experiments to analyze performance variations across different orderings, leading to some counter-intuitive findings as shown in Figure 2. Furthermore, another question for Graph LLM is Question II: Does context based on randomly sampled partial neighborhoods limit model effectiveness?The current mainstream paradigm of LLM as predictor obtains subgraph representations by randomly sampling n neighbors centered on target nodes and feeding them to LLMs (Ren et al., 2024), to accommodate LLMs' limited context length.Although studies on RAG make efforts on graph data compression (He et al., 2024;Hu et al., 2024), they focus on retrieving external knowledge and enhancing LLM.Different from these studies, to our best knowledge, no work discusses how to determine optimal or relatively better input subgraphs from the provided graph rather than relying on random sampling.Random sampling may sample graph structures that are detrimental to graph learning, such as heterophily (Zhu et al., 2020;Pei et al., 2020).This occurs because sampling nodes of different categories than the target node from its neighbors can lead to incorrect mixing of node features (Zhu et al., 2020), making nodes indistinguishable and resulting in incorrect answers.</p>
<p>To address these issues, we propose a novel framework called GraphSOS (Graph Sampling and Order Selection).</p>
<p>For Question I, GraphSOS introduces an Order Selector Module that selects better sequence order for serialized graphs, which is then fed into the LLM along with the question.Order Selector Module ensures that LLMs receive relatively better-ordered inputs for any graph, thus maintaining performance.For Question II, we introduce a Subgraph Sampling Module before the graph enters the Order Selector Module, which samples subgraphs of target nodes from the graph.We train the random walk process of the Subgraph Sampling Module using concepts from reinforcement learning and preference learning (Brown et al., 2020;Rafailov et al., 2024) to sample better subgraphs.Additionally, to ensure the model follows instructions and derives answers through analysis and logical reasoning of input graphs, we propose Graph Chain of Thought (Graph CoT) obtained through distillation.We use Graph CoT to enhance LLM's reasoning and zero-shot capabilities for graph tasks through instruction tuning.Our contributions can be summarized as:</p>
<p>• We identify current Graph LLMs are sensitive to graph serialization order, and random subgraph sampling can mix node features from different categories incorrectly, affecting model performance.</p>
<p>• We propose GraphSOS, a novel framework that improves LLM graph processing via two key components: a Subgraph Sampling Module for optimal subgraph extraction and an Order Selector Module for better graph serialization order.</p>
<p>• We introduce Graph CoT obtained through distillation and employ instruction tuning to teach models to reason correctly about graph structures.</p>
<p>• We evaluate our model on node classification and graph question-answering tasks and analyze the impact of its components.We demonstrate GraphSOS's superior performance in supervised and zero-shot graph learning settings.</p>
<p>Preliminaries</p>
<p>LLMs for graph data tasks can categorized into "LLM as enhancer" and "LLM as predictor" based on the role of LLM (Chen et al., 2023).This paper focuses on LLM as predictor, which leverages LLM's reasoning capabilities to solve graph tasks, including graph Question-Answering (graph QA) and node classification on Text- Attributed Graph (TAG).A graph can formally represented as G(V, E, X), where V and E represent the sets of nodes and edges respectively.X denotes the feature matrix, where each row vector represents a node's attributes or feature information.For TAG node classification, each node corresponds to a text attribute in X.For graph QA, G may not include meaningful X, such as in shortest path problems.</p>
<p>LLM processes graph inputs in sequence form, thus requiring a graph encoding function to convert the graph into a sequence suitable for language models (Fatemi et al., 2023).The process of LLM obtaining answers can be represented as:
A = f (g(G), Q),(1)
where f (•) formalizes the process of LLM obtaining answers from inputs, Q represents the user's question, A represents LLM's answer, and g(•) denotes the graph encoding function that converts graph G into a sequence, including serialization and possible subgraph sampling processes.</p>
<p>As shown in Table 1, for TAG node classification tasks, we define g(G) as the process of converting the node set V and node feature matrix X into a Feature List and converting the edge set E into an Edge List represented as pairs.For graph QA tasks, we convert the edge set E into pairs or triples based on specific task requirements.Since the graph QA tasks we study do not include node attributes, g(G) for graph QA tasks does not contain a Feature List.</p>
<p>Our objectives are twofold.First, we optimize g(•) to enable LLM to obtain better-sampled subgraphs of target node v in graph G and select relatively better serialization order representations for these subgraphs (in Sections 3.1 and 3.2).Secondly, we train and tune the LLM parameters to enhance the model's graph understanding and reasoning capabilities, to optimize f (•) (in Section 3.3).</p>
<p>Methodology</p>
<p>In this section, we detail the proposed GraphSOS framework, with the overall architecture shown in Figure 3. GraphSOS inputs both graph and question and generates natural language answers as output.The Subgraph Sampling Module takes graph G as input and aims to extract a subgraph of target node v from G. The Order Selector  Module takes the subgraph and question as input, aiming to generate a natural language description of the subgraph and determine the sequence order of elements from both the Feature List and Edge List within the description.Finally, the instruction-tuned LLM generates answers in the Graph CoT answer format.</p>
<p>Subgraph Sampling Module</p>
<p>In the Introduction, we address Question II: Does context based on randomly sampled partial neighborhoods limit model effectiveness?To improve this limitation, we design the Subgraph Sampling Module (SSM) to construct a subgraph G v for target node v from graph G, rather than relying purely on random sampling.As shown in Figure 4, SSM obtains text attributes (i.e., features) of target node v and its k-hop (k = 2) neighbors, using a pre-trained encoder (PTE) to generate encoded features for each node.In our implementation, we use Bert (Devlin et al., 2019) as the PTE, utilizing the [CLS] token embedding from Bert's last layer output as the representation for each node's features.</p>
<p>Next, we aim to use the [CLS] token embedding of target node v as a query to compute its correlation with the [CLS] token embeddings of each neighbor node, guiding random walks to retain highly correlated neighbors in the sampled subgraph G v .We introduce scaled dot-product attention to compute correlation weights between v and its neighbors, which is defined as (Vaswani et al., 2017):
Attention(Q, K, V ) = softmax( QK T √ d k )V,(2)
where d k is the embedding dimension, Q, K, and V represent the embedding matrices for query, key, and value respectively.Here, Q is the [CLS] token embedding vector of target node v, K = V is the matrix composed of [CLS] token embeddings of v's neighbors, and QK T √ d k represents the correlation vector between query vector Q and all vectors in key matrix K, defined as attention weights.We then introduce multi-head attention based on dot-product attention.The [CLS] token embeddings of the target node v and its neighbors are linearly mapped into h (h=4) splits for multi-head cross-attention computation.In the (multihead) cross-attention process, target node v's embedding is mapped to queries, while neighbor nodes' embeddings are mapped to keys.Attention weights are computed based on query and key embeddings.Let µ be the split embeddings, the multi-head cross-attention process can be expressed as:
o attn = 1 h h i=1 head i (µ v i , (µ u1 i , µ u2 i , . . . , µ un i )),(3)
where head i represents attention weights from the i-th attention head, h is the number of attention heads, µ v i is the i-th embedding component of target node v's split embedding, µ un i is the i-th embedding component of neighbor node u n 's split embedding, and n is the total number of v's k-hop neighbors.In our experiments, we use 4 attention heads (h=4) for multi-head cross-attention computation.The attention weights from multi-head attention guide random walks to retain highly correlated neighbors in sampled subgraph G v .Given maximum sample node count n max , the random walk sampling process can be defined as:
P (u j |v) = o attn [j], |V Gv | ≤ n max ,(4)
where o attn [j] represents the j-th value in the attention weight vector, indicating the correlation between neighbor node u j and target node v, P (u j |v) is the probability of transitioning from node v to neighbor node u j in the random walk process, V Gv represents the node set in the sampled subgraph, and |V Gv | represents its node count.The neighbor nodes sampled by random walk together with node v form the subgraph G v .</p>
<p>Scoring Model.To train SSM, we draw inspiration from reinforcement learning and preference learning (Brown et al., 2020;Rafailov et al., 2024).We first construct 500 data instances from the text-attributed graph datasets Citeseer and Cornell (see Section 4.1), where each instance contains a target node and its 2-hop neighbors, and convert them into text descriptions using g(G) as defined in Table 1.Then, we construct positive and negative subgraph examples for each target node data point and train a Scoring Model using crossentropy loss to score subgraphs.Based on task requirements, since we mainly focus on heterophily (Zhu et al., 2020;Pei et al., 2020), positive examples are constructed as subgraphs with strong homophily, while negative examples are constructed as subgraphs with strong heterophily.We use Qwen 2.5-0.5B(Team, 2024) as the Scoring Model, requiring it to output 1 for positive examples and 0 for negative examples.</p>
<p>To obtain continuous scores from the Scoring Model, we compute the softmax values of the probabilities that Qwen 2.5-0.5Bpredicts 0 or 1 for the first token, using the probability of predicting 1 from the softmax values as the model's score.This process can be described as:
P (y = 1|g(G)) = e hsc(g(G)
)1,y=1 e hsc(g(G))1,y=0 + e hsc(g(G))1,y=1 , (5) where h sc represents the output logits from the Scoring Model's last layer, y ∈ {0, 1} indicates the preference label, (•) 1,y=i subscript represents taking the probability value of the first new token corresponding to digit i, and P (y = 1|g(G)) is the score given by the Scoring Model.We use this score to update SSM gradients, defining SSM's loss function as:
L SSM = 1 T (1 − P (y = 1|g(G))) 2 , (6)
where T is the temperature coefficient that adjusts the loss magnitude, and we set T = 5 for smoother loss.It is worth noting that we use a scoring model to train the SSM, rather than directly using an LLM to replace SSM, because LLMs have limited input window sizes, while random walks in SSM have unlimited input windows.Moreover, constructing positive and negative example subgraphs for all target nodes for training would be computationally intensive.Through a Scoring Model trained on limited data, we can score any subgraph processed by SSM, greatly reducing training and data annotation costs.Frozen components are highlighted in blue and marked with snowflake icons.</p>
<p>Order Selector Module</p>
<p>In the Introduction, we also address Question I: Do LLMs truly understand and process graph topological structures correctly?We demonstrate that both LLMs and Graph LLMs are sensitive to the order of elements in Feature List and Edge List, which deviates from ideal Graph LLMs, as a graph LLM should maintain high performance regardless of the sequence order in which its nodes and edges are described.To address this gap, we design the Order Selector Module (OSM).This module generates serialized representations of subgraphs produced by SSM and arranges the elements in Feature List and Edge List in optimized orders according to the user's questions.OSM ensures that for any input subgraph, its serialized representation is order-optimal or relatively optimal for model responses.</p>
<p>As shown in Figure 5, OSM receives a subgraph and serializes it into a natural language sequence consisting of Feature List and Edge List.Then, it generates m order representations for this sequence, each being a random permutation of elements in the Feature List and Edge List.When m can enumerate all possible permutations, OSM theoretically selects the optimal order, however, this would result in factorial computational complexity, which is unacceptable in terms of time cost.Therefore, in our experiments, we select a subset of all possible orders, setting m = 10, allowing OSM to produce a fixed number of order permutations and obtain the relatively optimal order among them.</p>
<p>Next, the m sequences and the user's question are input to PTE for encoding.We also use Bert (Devlin et al., 2019) as the encoder, utilizing the [CLS] token embedding from its last layer as the representation for both the question and each ordered sequence.The cross-attention design is almost identical to SSM's, except that query Q becomes the question's embedding vector, and key K becomes the matrix composed of embeddings from m ordered sequences.Cross-attention outputs attention weights, where each element represents the correlation weight between that ordered sequence and the question.We apply Gumbel Softmax (Jang et al., 2016) to the attention weights to obtain a (one-hot) mask of length m, which selects a single optimal order from the m ordered sequences as output.Gumbel Softmax is used to ensure model differentiability.</p>
<p>Training OSM.We train OSM and the LLM in Figure 2 as an end-to-end system, updating OSM parameters using the loss between the language model's output and target answers.Specifically, before training OSM, we first train the LLM through two-stage tuning (in Section 3.3).Then, when training OSM, we freeze the LLM parameters and focus on updating OSM parameters.The loss function is constructed as:
L OSM (π θ ) = − N i=1 log π θ (y i |x i ),(7)
where π θ (y i |x i ) is the conditional probability of LLM generating target output y i given input x i , and N is the number of training samples.During training, LLM parameters are frozen, and only OSM parameters are updated.</p>
<p>Graph Chain-of-Thought Distillation</p>
<p>Traditional graph learning, such as GNN, typically completes graph tasks in two steps (Li et al., 2015;Kipf &amp; Welling, 2016):</p>
<p>Step 1: Aggregate and update node features, Step 2: Make predictions based on aggregated features.However, although many studies input graphs along with questions to LLMs, LLMs often skip Step 1, ignoring graph structure and directly predicting answers, for example in node classification tasks (Tang et al., 2024) and graph question-answering tasks (Chen et al., 2024).This indicates that LLMs often complete graph tasks following incorrect graph reasoning steps.</p>
<p>Inspired by Chain-of-Thought (CoT) research in language models (Wei et al., 2022) (Guo et al., 2023;Fatemi et al., 2023).We aim to distill the capabilities of large closed-source models into our small-parameter model through knowledge distillation.Following general steps for distilling knowledge from LLMs (Tang et al., 2024;Chen et al., 2024), we use GPT-4o to construct answers for the training dataset and construct prompts that make GPT-4o first analyze graph structure, then generate answers in CoT format, with examples shown in Figure 6.</p>
<p>Two-Stage Tuning for LLMs.To enable language models to follow human instructions, instruction tuning is commonly used (Ouyang et al., 2022b).However, to further enable models to think and generate responses using Graph CoT, we define a two-stage tuning approach.Stage 1: Instruction tuning.Stage 2: Direct Preference Optimization (DPO) (Rafailov et al., 2024).In the instruction tuning stage, the LLM is trained directly using the Question and SFT Answer format as shown in Figure 6.We use LoRA (Hu et al., 2022) to improve training efficiency.In the DPO phase, preference data is constructed using Graph CoT answers as winning responses and SFT answers as losing responses, encouraging the LLM to generate responses based on Graph CoT.Loss functions for the two stages are defined as:
L SF T (π θ ) = − N i=1 log π θ (y i |x i ),(8)L DP O (π θ ) = −E (x,yw,y l )∼D log σ[β log( π θ (y w |x) π ref (y w |x) ) −β log( π θ (y l |x) π ref (y l |x) )]
(9) where π θ is LLM parameters, π θ (y i |x i ) is the conditional probability of generating target output y i given input x i , π ref represents the reference policy, which is the LLM parameter state after the first stage instruction tuning and before the second stage DPO training.(x, y w , y l ) is a triplet consisting of input question and its corresponding winning and losing answers, β (β = 1) is the temperature coefficient, σ is the sigmoid function, and D is the training dataset.</p>
<p>Experiments</p>
<p>In this section, we validate the proposed GraphSOS by addressing the following questions: RQ1: How does Graph-SOS perform in supervised and zero-shot learning settings?RQ2: What are the contributions of each key component in GraphSOS to the overall performance?RQ3: How do the hyperparameters affect model performance?The detailed analysis of RQ3 is provided in Appendix C.3.</p>
<p>Experimental Setup</p>
<p>Dataset.We focus on Text-Attributed Graph (TAG) node classification and graph Question-Answering (graph QA) tasks.For TAG node classification, we use three homophily citation TAG datasets provided by Chen et al. (Chen et al., 2023): Cora, Citeseer, and Pubmed, following their training and test set splits.Additionally, following Pei et al. (Pei et al., 2020), we construct three high-heterophily text graph datasets: Texas, Wisconsin, and Cornell, from WebKB.In these datasets, nodes represent webpages, and edges represent hyperlinks between them.Node features are webpage descriptions.Webpages are categorized into four classes: student, course, staff, and faculty, with training, validation, and test sets split in a 1:1:8 ratio.</p>
<p>For graph QA tasks, MetaQA (Zhang et al., 2018) is a movie knowledge graph QA dataset.We select 1,000 samples from the provided 2-hop test set (avoiding training set to prevent data contamination) and split them into training, validation, and test sets in a 1:1:8 ratio.For each question, we retrieve 1-hop and 2-hop triples centered around the target entity from the knowledge graph, with the number of triples per question ranging from 50 to 1,198.Furthermore, we use the graph reasoning QA dataset provided by Chen et al. (Chen et al., 2024), which includes 9 tasks with a total of 18.1k training samples and 400 test samples per task.</p>
<p>Baseline Methods.In our performance comparison, we consider various advanced methods for comprehensive evaluation: (i) The first category includes Graph Neural Networks (GNNs): We use two-layer GNNs, including GCN (Kipf &amp; Welling, 2016), GAT (Veličković et al., 2018), Graph-SAGE (Hamilton et al., 2017), NodeFormer (Wu et al., 2023), etc.As well as GNNs with knowledge distillation: GKD (Yang et al., 2022), GLNN (Zhang et al., 2022), and AdaGMLP (Lu et al., 2024).More baselines and results can be seen in Appendix C.1.For TAG node classification tasks, node text attributes are encoded using BERT (Devlin et al., 2019), and the [CLS] token embedding is used as the node feature vector.For graph QA tasks, we sample one-dimensional features from a normal distribution as node features.(ii) The second category showcases state-of-the-art multi-task capable Graph LLMs, including GraphGPT-7B (Tang et al., 2024), GraphWiz-LLaMa2-7B (Chen et al., 2024), and three variants of TALK-LIKE-A-GRAPH (Fatemi et al., 2023) (GPT-Adjacency, GPT-Incident, and GPT-Expert), which utilize the closed-source GPT-3.5-turbo-16k(Ouyang et al., 2022a) and thus do not participate in any training in our experiments.(iii) The third category consists of simple fine-tuned open-source LLMs, including Qwen 2.5-7B (Team, 2024) and LLaMA 3-8B (Dubey et al., 2024).These models are fine-tuned using the SFT answer format shown in Figure 6 for comparison with our approach.</p>
<p>Implementation Details.We construct GraphSOS using LLaMA 3-8B and Qwen 2.5-7B as backbone models.The experimental configuration details are in Appendix B. We evaluate the models on TAG node classification and graph QA tasks to assess their performance in both supervised and zero-shot settings.The overall performance is shown in Table 2 and Table 3  Graph QA.In Table 3, GNNs are supervised trained except for MetaQA, topology, and hamilton, as GNNs do not support path reasoning.Datasets labeled as SFT indicate supervised fine-tuning using their respective training sets.Specifically, as the most challenging tasks in graph reasoning QA (Chen et al., 2024), hamilton and subgraph use zero-shot settings for all LLM-based methods except Graph-Wiz to test zero-shot learning capabilities, while GraphWiz, designed and trained specifically for graph reasoning QA, is fine-tuned on training sets of all graph QA datasets for comparison.Experimental results show that in SFT settings, GraphSOS leads performance on almost all datasets compared to all baselines while maintaining stable performance.Additionally, "2stage" models typically perform better than "1stage" indicating Graph CoT benefits graph reasoning tasks.In zero-shot learning settings (hamilton and subgraph), despite no supervised training, GraphSOS achieves performance close to or even surpassing supervised-trained GraphWiz, demonstrating GraphSOS's ability to understand and reason about graphs learned from other tasks.</p>
<p>Module Ablation Study (RQ2)</p>
<p>We conduct ablation studies to explore the individual contributions of different components in our proposed GraphSOS.Using LLaMA 3-8B as the base model, results are shown in Table 4.</p>
<p>Effect of Subgraph Sampling.We study the benefits of Subgraph Sampling Module using the "w/o SSM" variant.</p>
<p>In this variant, we directly randomly sample neighbors of target nodes for node classification across three datasets, without using SSM for neighbor sampling.Results in Table 4 show that GraphSOS with SSM outperforms its variant using random sampling.This indicates SSM's ability to select task-relevant subgraphs, especially for heterophily graphs (e.g., Texas), where SSM can reduce the selection of dissimilar neighbors to target nodes.As shown in Figure 9 in Appendix C.2, compared to the "w/o SSM" variant, SSM samples a notably higher proportion of same-class neighbors.Research in graph learning suggests this benefits node classification tasks (McPherson et al., 2001;Battaglia et al., 2018).</p>
<p>Effect of Order Selection.We study the benefits of Order Selector Module in selecting element order in Feature List and Edge List of serialized graphs using the "w/o OSM" variant.In this variant, we randomly arrange elements in Feature List and Edge List and conduct experiments on node classification and graph QA tasks.The results in Effect of Graph CoT.We study the benefits of using DPO to teach models to generate Graph CoT answers during two-stage tuning using the "w/o Graph CoT" variant.In this variant, we use instruction tuning with SFT answers from Figure 6 and conduct experiments on node classification and graph QA tasks.Results in Table 4 show that GraphSOS with two-stage tuning demonstrates consistent performance advantages over single-stage instruction tuning models.Notably, this performance advantage is also evident in zero-shot learning settings on the subgraph dataset, indicating Graph CoT helps models transfer reasoning abilities learned from other tasks to specific tasks, demonstrating zero-shot reasoning and generalization capabilities.</p>
<p>Conclusion</p>
<p>This paper introduces GraphSOS to address design deficiencies in Graph LLMs.We focus on two major issues: order sensitivity to serialized graphs and random subgraph sampling as input.We propose the Order Selector Module and Subgraph Sampling Module as solutions.We also introduce Graph CoT to enhance LLMs' graph reasoning.GraphSOS shows improved performance on graph tasks in both supervised and zero-shot learning settings.</p>
<p>Impact Statement</p>
<p>This work advances graph learning capabilities of Large Language Models through improved sampling, ordering, and reasoning mechanisms.While our proposed Graph Chain of Thought (Graph CoT) approach enhances model interpretability and performance by making reasoning steps explicit, we acknowledge several important considerations regarding its societal and ethical implications.</p>
<p>The primary ethical consideration stems from the inherent limitations of base LLMs that GraphSOS builds upon.Despite the structured reasoning paths, LLMs may still exhibit hallucinations or generate incorrect logical steps, which could propagate through the graph analysis process.This is particularly concerning in high-stakes applications like social network analysis or recommendation systems, where faulty reasoning could lead to biased or harmful decisions.</p>
<p>To mitigate these risks, we recommend:</p>
<ol>
<li>
<p>Using more robust base models with demonstrated reliability.</p>
</li>
<li>
<p>Creating diverse and carefully curated Graph CoT training data that emphasizes accurate reasoning.</p>
</li>
<li>
<p>Implementing validation mechanisms to verify the logical consistency of generated reasoning chains.</p>
</li>
<li>
<p>Maintaining human oversight in critical applications.</p>
</li>
</ol>
<p>Additionally, while Graph CoT improves transparency, it should not be considered a complete solution for model interpretability.The reasoning chains, while human-readable, may still reflect underlying biases present in the training data or model architecture.</p>
<p>We believe the benefits of enhanced graph understanding and explicit reasoning outweigh these risks when proper precautions are taken.Our work makes LLMs for graph tasks more reliable and interpretable, crucial for their responsible deployment in real-world applications.</p>
<p>A. Analyzing Order Sensitivity in LLMs and LLMs for Graph Tasks</p>
<p>Many studies find that LLMs are highly sensitive to prompt order in zero-shot and few-shot settings (Jiang et al., 2019;Lu et al., 2022;Tan et al., 2024).As shown in Figure 7, taking a sorting problem as an example, Qwen2.5-7B-Instruct (Team, 2024) produces different results for the same question when presented in two different orders.Counter to intuition, one order leads to the correct answer while the other leads to an incorrect answer.Experiments by Lu et al. (Lu et al., 2022) further demonstrate that there is no universally optimal order across different LLMs or tasks.</p>
<p>Similar to the order sensitivity studies on LLMs, we find that Graph LLMs are also order-sensitive.As shown in Figure 8, taking node classification on the Cora dataset as an example, when we describe the same citation graph using two different natural language orders (i.e., changing the order of elements in Node features and Edge list), GraphGPT (Tang et al., 2024), which is trained on large-scale Text-Attributed Graph data, produces different results for the two orders.One order leads to the correct answer while the other leads to an incorrect answer.This indicates that there exist relatively better natural language description orders for graphs that enable LLMs or Graph LLMs to perform better on graph-related tasks.The central idea is to apply the rules to a target problem to get a first approximation to the answer; but if the problem is judged to be compellingly similar to a known exception of the rules in any aspect of its behavior, then that aspect is modelled after the exception rather than the rules.The architecture is implemented for the full-scale task of pronouncing surnames.Preliminary results suggest that the system performs almost as well as the best commercial systems.However, of more interest than the absolute performance of the system is the result that this performance was better than what could have been achieved with the rules alone.This illustrates the capacity of the architecture to improve on the rule-based system it starts with.The results also demonstrate a beneficial interaction in the system, in that improving the rules speeds up the case-based component.Paper 454.The title is Data-oriented methods for grapheme-to-phoneme conversion.The abstract of this paper: It is traditionally assumed that various sources of linguistic knowledge and their interaction should be formalised in order to be able to convert words into their phonemic representations with reasonable accuracy.We show that using supervised learning techniques, based on a corpus of transcribed words, the same and even better performance can be achieved, without explicit modeling of linguistic knowledge.In this paper we present two instances of this approach.A first model implements a variant of instance-based learning, in which a weighed similarity metric and a database of prototypical exemplars are used to predict new mappings.In the second model, grapheme-to-phoneme mappings are looked up in a compressed text-tospeech lexicon (table lookup) enriched with default mappings.We compare performance and accuracy of these approaches to a connectionist (backpropagation) approach and to the linguistic knowledge based approach..A case study in evaluating a case-based system: This paper presents a case study in evaluating a case-based system.It describes the evaluation of Anapron, a system that pronounces names by a combination of rule-based and case-based reasoning.Three sets of experiments were run on Anapron: a set of exploratory measurements to profile the system's operation; a comparison between Anapron and other name-pronunciation systems; and a set of studies that modified various parts of the system to isolate the contribution of each.Lessons learned from these experiments for CBR evaluation methodology and for CBR theory are discussed.This work may not be copied or reproduced in whole or in part for any commercial purpose.Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories of Cambridge, Massachusetts; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice.Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories.All rights reserved.Paper 454.The title is Data-oriented methods for grapheme-to-phoneme conversion.The abstract of this paper: It is traditionally assumed that various sources of linguistic knowledge and their interaction should be formalised in order to be able to convert words into their phonemic representations with reasonable accuracy.We show that using supervised learning techniques, based on a corpus of transcribed words, the same and even better performance can be achieved, without explicit modeling of linguistic knowledge.In this paper we present two instances of this approach.A first model implements a variant of instancebased learning, in which a weighed similarity metric and a database of prototypical exemplars are used to predict new mappings.In the second model, grapheme-to-phoneme mappings are looked up in a compressed text-to-speech lexicon (table lookup) enriched with default mappings.We compare performance and accuracy of these approaches to a connectionist (backpropagation) approach and to the linguistic knowledge based approach.. Paper 1307.The title is planning in an open-textured domain.Paper 38.The title is Improving rule-based systems through case-based reasoning.The abstract of this paper: A novel architecture is presented for combining rule-based and case-based reasoning.The central idea is to apply the rules to a target problem to get a first approximation to the answer; but if the problem is judged to be compellingly similar to a known exception of the rules in any aspect of its behavior, then that aspect is modelled after the exception rather than the rules.The architecture is implemented for the full-scale task of pronouncing surnames.Preliminary results suggest that the system performs almost as well as the best commercial systems.However, of more interest than the absolute performance of the system is the result that this performance was better than what could have been achieved with the rules alone.This illustrates the capacity of the architecture to improve on the rule-based system it starts with.The results also demonstrate a beneficial interaction in the system, in that improving the rules speeds up the case-based component.</p>
<p>C.2. Module Ablation Results</p>
<p>C.3. Parameter Sensitivity (RQ3)</p>
<p>We analyze the impact of two hyperparameters on GraphSOS: the number of sampled neighbors n max in SSM and the number of order candidates m in OSM. Figure 11 shows the performance of GraphSOS with LLaMA 3-8B as the base model under different settings of sampled neighbors n max .Results indicate that both too high and too low values of n max affect model performance.A low n max leads to limited graph structural information for LLM, restricting model reasoning; a high n max results in overly long context input to LLM, making reasoning difficult (An et al., 2024).Table 7 shows the performance of LLaMA 3-8B-based GraphSOS under different order candidate numbers m.The results indicate that the model performance improves steadily as m increases.This suggests that appropriately increasing m can enhance performance by including more candidate order samples.However, increasing the order leads to growth in inference time overhead.To balance performance and time overhead, we only choose to set m = 10.Nevertheless, we point out that increasing m brings performance improvements.</p>
<p>D. Future Work</p>
<p>In this paper, when discussing graph structures, we focus on homophily and heterophily.However, other structural properties (such as degree, connectivity, symmetry) are worth exploring.We demonstrate that with properly constructed training data, subgraphs with any graph structure that is more beneficial for tasks can be sampled by training GraphSOS's SSM, rather than only homophily and heterophily.Additionally, ensuring accurate Graph CoT steps is crucial for graph reasoning.We encourage introducing more powerful models to generate Graph CoT data or using manually constructed Graph CoT data, and emphasize the effectiveness of constructing larger training datasets to improve model performance.Moreover, the Subgraph Sampling Module constructed in this paper lacks explicit structure-aware components.We encourage new methods to incorporate structure-awareness capabilities to discover, evaluate, and preserve critical paths or paths that play important connecting roles in the topological structure.</p>
<p>Figure 1 .
1
Figure1.Converting a graph into natural language description.Elements in both node and edge lists can be arranged in any order to represent the same graph.</p>
<p>(a) Node classification accuracy of models.(b) Accuracy of models on graph QA tasks.(c) Node classification accuracy of Qwen models with different parameter counts.(d) Accuracy of Qwen models with different parameter counts on graph QA tasks.</p>
<p>Figure 2 .
2
Figure 2. Zero-shot performance of models with different orders of node and edge.</p>
<p>Figure 2(a) and Figure 2(b) show that merely changing the order of nodes and edges in the questions causes performance fluctuations across all these models (both LLMs and Graph LLMs that have undergone embedding alignment and graph task fine-tuning).Additionally, as shown in Figure 2(c) and Figure 2(d), this phenomenon persists across models of the same architecture with different parameter scales, with no significant improvement as model parameters increase.Therefore, regarding Question I, it seems that current LLMs and Graph LLMs do not understand and process graphs well, as an ideal Graph LLM should maintain high performance regardless of the ordering used to represent the same graph.More detailed analysis and examples can be seen in Appendix A.</p>
<p>[Node 2 is …, Node 1 … Node 3 … ] Edge List: [(1, 2), (3, 2), (2, 3) … ] Question: "What is the category of target node 2</p>
<p>Figure 3 .
3
Figure 3.The overall framework of GraphSOS.Trainable components are highlighted in yellow and marked with flame icons.(a) Subgraph Sampling Module: samples and outputs a subgraph of a target node from graph G. (b) Order Selector Module: takes the subgraph and user question, converts the subgraph into a text sequence and selects the sequence order.(c) LLM: generates answers based on question and serialized text representation of the graph.</p>
<p>Figure 4 .
4
Figure 4. Internal details of the Subgraph Sampling Module (SSM).Frozen components are highlighted in blue and marked with snowflake icons.</p>
<p>Figure 5 .
5
Figure 5. Internal details of the Order Selector Module (OSM).Frozen components are highlighted in blue and marked with snowflake icons.</p>
<p>, representing the proportion of edges connecting nodes of the same class, where Z u,: = Z v,: indicates nodes u and v belong to the same category.Each dataset's H edge (G) is labeled in the Edge Hom.row, with values closer to 0 indicating stronger heterophily in G.All models are supervised trained only on Citeseer and Cornell training sets, with zero-shot learning on the remaining datasets.Results show that in SFT settings, LLaMA 3-GraphSOS-2stage-SSM-OSM outperforms GNNs on Citeseer and Cornell, particularly on heterophily graph Cornell, demonstrating strong heterophily graph learning capabilities.In the zero-shot learning setting, all variants of GraphSOS demonstrate consistent performance advantages, achieving 1-5 times higher accuracy compared to other baselines.</p>
<p>Figure 7 .
7
Figure 7. Examples of Qwen2.5-7B-Instruct'sresponses to the same question with different sequential orderings.</p>
<p>Paper 1307.The title is planning in an open-textured domain.Edge list: 1-hop-edges: ['(38,1802)', '(38,90)', '(38,454)', '(38,1349)', '(38,1307)'].Based on node features and edge list above, which of the following categories does the paper 38 most likely belong to? [Rule Learning, Neural Networks, Case Based, Genetic Algorithms, Theory, Reinforcement Learning, Paper 1349.The title is Pronouncing Names by a Combination of Rule-Based and Case-Based Reasoning.Paper 1802.The title is Case-based Acquisition of User Preferences for Solution Improvement in Ill-Structured Domains,.The abstract of this paper: 1 We have developed an approach to acquire complicated user optimization criteria and use them to guide.Paper 90.The title is The evaluation of Anapron.The abstract of this paper:</p>
<p>Figure 8 .
8
Figure8.Examples of GraphGPT's responses to node classification tasks where the same graph is described in natural language using different sequential orderings.</p>
<p>Figure 9 .
9
Figure 9. Proportion of same-class neighbors in random sampling and SSM sampling.</p>
<p>Figure 10 .
10
Figure 10.Performance fluctuation comparison between random order and OSM-selected order.</p>
<p>Figure 11 .
11
Figure 11.Performance of GraphSOS with different numbers of sampled neighbors nmax.</p>
<p>Table 1 .
1
Construction of serialized text input for the graph.</p>
<p>Task g(G) TAG Node Feature List: [Node 0 ... ], Classification Edge List: [(0, 1) (0, 2) ... ] Graph QA Edge List: [(0, 1) (0, 2) ... ] Knowledge Triple List: [(Lore, release year, Graph QA 2012) ... ]</p>
<p>GraphCoT Answer: This is a node classification task.Paper 0 has a direct connection with paper 1260 and 2-hop connections to papers 3038, 2243, 818, and 2921.Paper 0 discusses "Argument in Multi-Agent Systems" … Both the target paper and its neighbors heavily focus on agent-based systems … which clearly indicates this paper belongs to the "Agents" category.### Agents.This is a node classification task.Paper 0 has a direct connection with paper 1260 and 2-hop connections to papers 3038, 2243, 818, and 2921.Paper 0 discusses "Argument in Multi-Agent Systems" … Both the target paper and its neighbors heavily focus on agent-based systems … which clearly indicates this paper belongs to the "Agents" category.### Agents.[ Paper 0: Argument in Multi-Agent Systems Multi-agent systems research … Paper 1260 … ] Edge List: [1-hop-edges: ['(0,1260)' … ], 2-hop-edges: ['(0,3038)' … ]].Based on the feature and edge list above, which of the following categories does Paper 0 most likely belong to? [Agents, AI, DB, IR, ML, HCI].Answer: Question: Feature List: [ Paper 0: Argument in Multi-Agent Systems Multi-agent systems research … Paper 1260 … ] Edge List: [1-hop-edges: ['(0,1260)' … ], 2-hop-edges: ['(0,3038)' … ]].Based on the feature and edge list above, which of the following categories does Paper 0 most likely belong to? [Agents, AI, DB, IR, ML, HCI].Answer: Figure 6.Constructing answers in Graph CoT format.Yellow highlights indicate analysis steps, while blue highlights show reasoning processes.
Graph CoT Answer: SFT Answer: Agents. SFT Answer: Agents.
Question: Feature List:</p>
<p>Table 2 .
2
Performance comparison (accuracy) on TAG node classification tasks under supervised and zero-shot settings.
DatasetCiteseerCoraPubmedCornellTexasWisconsinEdge Hom.0.780.810.800.260.250.33Training MethodSFT0-shot0-shotSFT0-shot0-shotGCN70.7±0.413.9±3.226.3±2.8 47.4±3.98.9±7.721.2±21.4GAT71.2±0.813.4±5.627.5±3.3 50.5±2.7 37.6±4.9 22.9±19.2GraphSage70.9±0.6 24.2±14.1 25.8±3.0 48.9±3.2 29.5±6.8 23.5±20.1GNNNodeFormer70.5±0.814.5±4.026.1±3.1 75.5±1.4 30.1±6.6 22.8±20.3GKD72.0±0.514.1±4.024.5±3.2 48.2±3.19.7±7.212.3±10.0GLNN73.1±0.313.8±3.721.0±2.8 51.7±3.4 19.4±7.4 21.6±19.772.8±0.414.1±3.511.5±7.9 71.2±1.3 20.1±7.2 22.0±19.5GraphWiz74.9±0.70.1±0.91.5±1.150.0±0.8 48.6±1.260.6±0.6GraphGPT53.2±1.39.1±0.570.1±1.4 49.8±0.7 52.3±0.960.0±1.1Graph LLMGPT-Adjacency17.8±0.564.2±0.720.1±0.6 77.8±0.1 72.9±0.179.1±0.2GPT-Incident18.6±0.465.4±0.320.2±0.7 78.2±0.0 73.1±0.180.2±0.0GPT-Expert18.5±0.265.9±0.320.8±0.8 78.1±0.0 73.2±0.179.9±0.11stage38.4±0.836.8±1.220.2±0.6 71.9±1.5 64.4±0.479.1±0.9Qwen 2.5GraphSOS-2stage GraphSOS-2stage-SSM64.2±1.1 65.3±0.664.2±0.7 65.4±1.470.8±1.3 75.7±0.5 75.8±1.4 72.3±0.9 77.3±1.2 76.9±0.382.1±0.8 83.5±1.0GraphSOS-2stage-SSM-OSM 69.7±0.866.3±0.673.9±1.2 80.1±0.6 78.6±0.984.9±0.51stage74.5±1.29.7±0.87.6±0.576.5±1.3 68.5±0.779.5±1.4LLaMA 3GraphSOS-2stage GraphSOS-2stage-SSM74.9±0.9 75.3±0.367.3±1.5 68.5±1.175.9±0.4 76.5±0.6 72.6±1.0 76.1±1.4 78.9±0.9 74.3±0.581.8±1.2 83.0±0.8GraphSOS-2stage-SSM-OSM 77.0±0.570.5±0.877.6±0.7 79.5±0.9 76.5±0.785.2±0.64.2. Overall Performance of GraphSOS (RQ1)</p>
<p>Table 3 .
3
Performance comparison (accuracy) on graph QA tasks under supervised and zero-shot settings.
DatasetMetaQAcycleconnectbipartitetopologyshortesttriangleflowhamiltonsubgraphTraining MethodSFTSFTSFTSFTSFTSFTSFTSFTSFT/0-shot SFT/0-shotGCN-82.5±1.3 73.0±0.8 81.3±1.1-5.3±0.77.0±1.4 10.3±0.9-62.0±1.2GAT-84.5±0.6 79.8±1.4 83.8±0.5-7.3±1.27.3±0.8 11.8±1.5-64.5±0.4GraphSAGE-83.8±1.1 78.5±1.2 82.9±0.8-7.0±0.97.5±1.1 11.2±1.2-63.8±0.8GNNNodeFormer-83.5±0.9 79.0±1.0 82.7±1.0-6.8±1.08.7±0.9 13.0±1.3-58.2±0.7GKD-84.2±0.8 79.5±1.1 83.1±0.5-7.2±0.87.2±1.2 11.6±1.0-64.2±0.9GLNN-84.7±1.0 80.8±0.7 83.9±0.4-8.2±0.37.1±0.8 11.3±1.4-63.5±1.1AdaGMLP-84.5±0.9 80.5±0.8 83.6±0.5-8.0±0.47.3±0.9 11.5±1.3-63.8±1.0GraphWiz35.3±1.9 70.0±1.1 89.8±0.3 73.3±1.4 16.3±0.7 12.8±1.0 24.0±0.6 28.3±1.339.0±0.870.3±1.5GraphGPT32.9±1.2 72.8±0.4 83.5±1.5 66.8±0.7 0.0±0.09.3±0.5 23.3±1.3 13.3±0.831.8±1.459.8±0.6Graph LLMGPT-Adjacency86.9±1.1 81.2±0.9 89.5±1.0 77.8±0.8 70.1±0.7 24.2±0.7 34.8±1.2 36.2±0.941.5±1.165.1±0.7GPT-Incident86.8±1.1 84.9±0.8 90.3±0.9 79.1±0.7 72.3±0.8 14.5±0.9 25.7±0.8 37.1±1.141.2±0.966.8±1.0GPT-Expert86.9±1.9 81.8±1.0 89.8±1.1 78.2±0.9 70.0±0.7 24.9±0.6 35.1±1.0 36.5±0.841.1±1.266.8±0.81stage40.8±3.5 79.3±1.4 88.0±0.7 73.3±1.2 0.0±0.0 10.3±1.5 24.0±0.9 27.3±0.431.8±1.159.8±0.8Qwen 2.5GraphSOS-2stage80.3±6.8 79.5±0.6 88.0±1.1 74.8±0.9 15.8±1.4 16.8±0.7 21.8±1.2 20.0±1.532.8±0.568.8±1.0GraphSOS-2stage-OSM 86.9±3.5 80.4±1.2 89.3±0.6 77.1±0.9 16.7±0.8 16.9±1.1 24.2±0.9 26.8±1.236.3±1.268.9±0.51stage46.4±3.4 83.5±0.7 90.0±1.2 78.5±0.5 0.0±0.0 14.8±0.8 35.3±1.1 25.8±0.631.8±1.362.5±0.9LLaMA 3GraphSOS-2stage83.3±1.1 89.8±1.3 92.8±0.8 79.3±1.4 17.3±0.4 24.5±1.1 37.5±0.5 38.5±1.241.0±0.769.5±1.5GraphSOS-2stage-OSM 89.6±1.0 92.7±0.7 93.4±1.2 80.2±0.9 18.8±0.8 26.3±0.5 41.2±1.3 40.7±1.042.7±0.972.9±0.8</p>
<p>Table 4 .
4
Ablation study on SSM, OSM, and Graph CoT of Graph-SOS.
CiteseerCoraTexasMetaQAcyclesubgraphw/o SSM74.9±1.2 67.5±0.8 69.6±1.4---w/o OSM75.3±0.3 68.5±1.1 74.3±0.5 83.3±1.1 89.8±1.3 69.5±1.5w/o Graph CoT 71.6±1.4 9.3±0.6 69.3±1.2 47.7±0.8 85.2±1.5 64.9±0.4GraphSOS77.0±0.5 70.5±0.8 76.5±0.7 89.6±1.0 92.7±0.7 72.9±0.8</p>
<p>Table 4
4demonstrate that GraphSOS with OSM outperforms its vari-ants, indicating that the context ordering selected by OSMhelps LLMs understand and reason about graphs, therebyensuring high performance. Additionally, as shown in Fig-ure 10 in Appendix C.2, in statistical results of 10 indepen-dent experiments with randomly permuted element orders inFeature List and Edge List for each dataset, GraphSOS withOSM shows smaller performance fluctuation, indicatingOSM's ability to suppress LLM's order sensitivity.</p>
<p>Table 7 .
7
Accuracy and inference time of baselines and GraphSOS with different numbers of order candidates m on MetaQA.
ModelsAcc(%)time(s)GraphWiz35.3±1.90.43GraphGPT32.9±1.20.51LLaMa 346.4±3.40.65GraphSoS (m =84.5±0.50.81GraphSoS (m = 10) 89.6±1.01.02GraphSoS (m = 15) 89.8±1.21.19GraphSoS (m = 20) 90.2±0.91.37
B. Experimental SetupWe set the learning rate, epoch, batch size, and maximum length for fine-tuning all models to 5e-5, 3, 8, and 1024 respectively.Each experiment is repeated 3 times, with means and standard deviations reported.All experiments use an Intel(R) Xeon(R) Silver 4316 processor as CPU and a single 80G Nvidia A100 GPU.The system memory is 256GB, with Ubuntu 22.03.3 as the operating system, CUDA version 12.4, Python version 3.10.4,and torch version 2.0.1.For Graph CoT distillation in Section 3.3, we use GPT-4o to generate Graph CoT format answers with a temperature of 0.9 and maximum output tokens of 512.We employ LoRA (Low-Rank Adaptation) for fine-tuning.The training dataset is preprocessed using 16 workers with a maximum sequence length of 1024 tokens.The LoRA hyperparameters are set as follows: rank = 8, alpha = 16, and dropout = 0, targeting all model layers.For optimization, we use the AdamW optimizer with a learning rate of 5e-5 and cosine learning rate scheduling.We employ mixed-precision training bfloat16 format.The batch size is set to 2 with a gradient accumulation of 8 steps.Gradient clipping is applied with a maximum norm of 1.0.The model checkpoints are saved every 100 steps, with loss logging occurring every 5 steps.For DPO training, we use identical infrastructure settings but configure preference learning with beta = 0.1 and sigmoid-based preference loss.The preprocessing is handled by 16 workers with a maximum sequence length of 2048 tokens.The LoRA parameters remain the same (rank = 8, targeting all layers), while the learning rate is reduced to 5e-6 with cosine scheduling and 10% warmup ratio.Training uses bfloat16 format over 3 epochs, with batch size = 1 and gradient accumulation steps = 8.Checkpoints are saved every 500 steps, and loss logging occurs every 10 steps.C. Additional ResultsC.1. Complete Experimental ResultsWe also compare with the following baselines, including ChebNet(Defferrard et al., 2016), SGC(Wu et al., 2019), and LW-GNN(Dai et al., 2022).results are shown in Table5and Table6.
Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing. S Abu-El-Haija, B Perozzi, A Kapoor, International Conference on Machine Learning. 2019</p>
<p>Make your llm fully utilize the context. S An, Z Ma, Z Lin, N Zheng, J.-G Lou, ArXiv, abs/2404.168112024</p>
<p>. J Bai, S Bai, Y Chu, ArXiv, abs/2309.16609Qwen technical report. 2023</p>
<p>Relational inductive biases, deep learning, and graph networks. P W Battaglia, J B Hamrick, V Bapst, A Sanchez-Gonzalez, V F Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, ¸aglar Gülc ¸ehre, H F Song, A J Ballard, J Gilmer, G E Dahl, A Vaswani, K R Allen, C Nash, V Langston, C Dyer, N M O Heess, D Wierstra, P Kohli, M M Botvinick, O Vinyals, Y Li, R Pascanu, ArXiv, abs/1806.01261201846935302</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, ArXiv, abs/2005.141652020</p>
<p>Graphwiz: An instruction-following language model for graph computational problems. N Chen, Y Li, J Tang, J Li, Knowledge Discovery and Data Mining. 2024</p>
<p>Exploring the potential of large language models (llms)in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, J Tang, ACM SIGKDD Explorations Newsletter. 252023</p>
<p>Label-wise graph convolutional network for heterophilic graphs. E Dai, S Zhou, Z Guo, S Wang, 21. PMLR, 09- 12Proceedings of the First Learning on Graphs Conference. B Rieck, R Pascanu, the First Learning on Graphs ConferenceDec 2022198of Proceedings of Machine Learning Research</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems. M Defferrard, X Bresson, P Vandergheynst, 201629</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, North American Chapter. the Association for Computational Linguistics2019</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, ArXiv, abs/2407.217832024</p>
<p>Graph neural networks for social recommendation. W Fan, Y Ma, Q Li, The world wide web conference. 2019</p>
<p>Llama-omni: Seamless speech interaction with large language models. Q Fang, S Guo, Y Zhou, Z Ma, S Zhang, Y Feng, ArXiv, abs/2409.066662024</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J J Halcrow, B Perozzi, ArXiv, abs/2310.045602023</p>
<p>Can large language models understand graph structured data ? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, W Hamilton, Z Ying, J Leskovec, ArXiv, abs/2305.15066Advances in neural information processing systems. 302023. 2017Inductive representation learning on large graphs</p>
<p>G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. X He, Y Tian, Y Sun, N V Chawla, T Laurent, Y Lecun, X Bresson, B Hooi, arXiv:2402.076302024arXiv preprint</p>
<p>LoRA: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, International Conference on Learning Representations. 2022</p>
<p>Grag: Graph retrieval-augmented generation. Y Hu, Z Lei, Z Zhang, B Pan, C Ling, L Zhao, arXiv:2405.165062024arXiv preprint</p>
<p>Categorical reparameterization with gumbel-softmax. E Jang, S S Gu, B Poole, ArXiv, abs/1611.011442016</p>
<p>How can we know what language models know? Transactions of the. Z Jiang, F F Xu, J Araki, G Neubig, 2019Association for Computational Linguistics8</p>
<p>Semi-supervised classification with graph convolutional networks. T Kipf, M Welling, T Kojima, S S Gu, M Reid, ArXiv, abs/1609.02907Advances in neural information processing systems. 2016. 202235Large language models are zero-shot reasoners</p>
<p>Influence maximization in social networks using graph embedding and graph neural network. S Kumar, A Mallik, A Khetarpal, Information Sciences. 6072022</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S C H Hoi, International Conference on Machine Learning. 2023</p>
<p>Y Li, D Tarlow, M Brockschmidt, R Zemel, arXiv:1511.05493Gated graph sequence neural networks. 2015arXiv preprint</p>
<p>Rumor detection with a novel graph neural network approach. T Liu, Q Cai, C Xu, Academic Journal of Science and Technology. 1012024</p>
<p>Adagmlp: Adaboosting gnn-to-mlp knowledge distillation. W Lu, Z Guan, W Zhao, Y Yang, 10.1145/3637528.3671699Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24New York, NY, USA2024Association for Computing Machinery. ISBN 9798400704901</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Y Lu, M Bartolo, A Moore, S Riedel, P Stenetorp, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Collaborative sequential recommendations via multi-view gnn-transformers. T Luo, Y Liu, S J Pan, ACM Transactions on Information Systems. 2024</p>
<p>Birds of a feather: Homophily in social networks. M Mcpherson, L Smith-Lovin, J M Cook, Review of Sociology. 272001</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 2022a35</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, ArXiv, abs/2203.021552022b</p>
<p>Evolvegcn: Evolving graph convolutional networks for dynamic graphs. A Pareja, G Domeniconi, J Chen, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Geom-gcn: Geometric graph convolutional networks. H Pei, B Wei, K C Chang, .-C Lei, Y Yang, B , ArXiv, abs/2002.052872020</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, Advances in Neural Information Processing Systems. 362024</p>
<p>A survey of large language models for graphs. X Ren, J Tang, D Yin, N V Chawla, C Huang, Knowledge Discovery and Data Mining. 2024</p>
<p>Order matters: Exploring order sensitivity in multimodal large language models. Z Tan, X Chu, W Li, T Mo, arXiv:2410.169832024arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. J Tang, Y Yang, W Wei, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>5: A party of foundation models. Q Team, Qwen2, September 2024. </p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, ArXiv, abs/2302.139712023</p>
<p>Attention is all you need. A Vaswani, N M Shazeer, N Parmar, Neural Information Processing Systems. 2017</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, International Conference on Learning Representations. 2018accepted as poster</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, Advances in neural information processing systems. 202235</p>
<p>Simplifying graph convolutional networks. F Wu, A Souza, T Zhang, C Fifty, T Yu, K Weinberger, International conference on machine learning. PMLR2019</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Q Wu, W Zhao, Z Li, D P Wipf, J Yan, ArXiv, abs/2306.083852023258509408</p>
<p>Chartx &amp; chartvlm: A versatile benchmark and foundation model for complicated chart reasoning. R Xia, B Zhang, H Ye, 2024CoRR</p>
<p>Geometric knowledge distillation: Topology compression for graph neural networks. C Yang, Q Wu, J Yan, Advances in Neural Information Processing Systems. 202235</p>
<p>Graph-less neural networks: Teaching old mlps new tricks via distillation. S Zhang, Y Liu, Y Sun, N Shah, Y Zhang, H Dai, Z Kozareva, A Smola, L Song, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2022. 201832International Conference on Learning Representations</p>
<p>Beyond homophily in graph neural networks: Current limitations and effective designs. J Zhu, Y Yan, L Zhao, Advances in neural information processing systems. 202033</p>
<p>Answer from Qwen2.5-7B-Instruct Here's a math problem involving sorting numbers from smallest to largest: Arrange these 20 numbers in ascending order (from smallest to largest): -847. Y Zhu, F Cong, D Zhang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023. -1002, -100, -847.5, -8, -7, -0.451, 0, 0.0036, 0.125, 0.15, 0.25, 1.618, 2.3333, 2.718, 6, 16, 16, 3.1416, 455. .5, 0.0036, -1002, 7/3, π, 455.89, -0.451, 2⁴, -√49, 0, 1.618, 15%, 2.7188999Wingnn: Dynamic graph neural networks with random gradient aggregation window. 4² , 0.125, -10² , 1/4, 9</p>
<p>Answer from Qwen2.5-7B-Instruct Here's a math problem involving sorting numbers from smallest to largest: Arrange these 20 numbers in ascending order. from smallest to largest</p>
<p>. Llm Graph, Graphwiz, 35.3±1.9 70.0±1.1 89.8±0.3 73.3±1.4 16.3±0.7 12.8±1.0 24.0±0.6 28.3±1.3 39.0±0.8 70.3±1.5</p>
<p>. 4±3.4 83.5±0.7 90.0±1.2 78.5±0.5 0.0±0.0 14.8±0.8 35.3±1.1 25.8±0.6 31.8±1.3 62.5±0.9LLaMA. 3</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1871 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1871</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1871</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-36.html">extraction-schema-36</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <p><strong>Paper ID:</strong> paper-277633954</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.06525v1.pdf" target="_blank">The Power of the Pareto Front: Balancing Uncertain Rewards for Adaptive Experimentation in scanning probe microscopy</a></p>
                <p><strong>Paper Abstract:</strong> Automated experimentation has the potential to revolutionize scientific discovery, but its effectiveness depends on well-defined optimization targets, which are often uncertain or probabilistic in real-world settings. In this work, we demonstrate the application of Multi-Objective Bayesian Optimization (MOBO) to balance multiple, competing rewards in autonomous experimentation. Using scanning probe microscopy (SPM) imaging, one of the most widely used and foundational SPM modes, we show that MOBO can optimize imaging parameters to enhance measurement quality, reproducibility, and efficiency. A key advantage of this approach is the ability to compute and analyze the Pareto front, which not only guides optimization but also provides physical insights into the trade-offs between different objectives. Additionally, MOBO offers a natural framework for human-in-the-loop decision-making, enabling researchers to fine-tune experimental trade-offs based on domain expertise. By standardizing high-quality, reproducible measurements and integrating human input into AI-driven optimization, this work highlights MOBO as a powerful tool for advancing autonomous scientific discovery.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1871.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1871.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOBO-SPM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Objective Bayesian Optimization workflow for Scanning Probe Microscopy tapping-mode optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-loop autonomous optimization system that uses Gaussian process surrogate models and a q-Noisy Expected Hypervolume Improvement acquisition function to optimize multiple heuristic/physics-derived reward metrics (trace-retrace height agreement, tip-sample distance, phase) for tapping-mode SPM by proposing control parameters and validating them with real scans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Objective Bayesian Optimization (MOBO) with Gaussian processes and q-Noisy Expected Hypervolume Improvement (q-NEHI)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>instrument optimization / materials characterization (scanning probe microscopy)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Reward functions computed from real-time scan data (Height difference / Trace‚ÄìRetrace difference, Tip‚ÄìSample distance, Phase reward; optionally Similarity/Pearson correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>expert-heuristic data-derived metrics (computed from measured instrument channels) modeled with data-driven surrogates (Gaussian processes)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>At each experimental iteration the system computes rewards from real-time trace and retrace scan lines: (1) Height difference = negative log of normalized absolute difference between trace and retrace height lines; (2) Tip‚Äìsample distance = negative log of the minimum probe height relative to global minimum; (3) Phase reward = negative log of fraction of phase pixels below free-air phase (measures attractive vs repulsive mode); (4) Similarity = Pearson correlation between trace and retrace (used for comparison). These measured rewards are used as targets to train separate Gaussian process models (gpytorch) across the 3D control parameter space (drive amplitude, setpoint, I gain). The acquisition function q-NEHI (from BOTorch) uses GP predicted mean and uncertainty to propose next experimental parameters maximizing expected hypervolume improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric_name</strong></td>
                            <td>Full topography and phase scans acquired on the sample using the optimized control parameters (experimentally measured height and phase maps)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_validation_method</strong></td>
                            <td>Experimental execution of the suggested control parameters on the actual SPM instrument (AESPM interface) to acquire real trace/retrace scan lines and full topography/phase maps; safe seeding uses force-distance (FD) measurements to exclude repulsive-mode setpoints prior to optimization. Final validation is a full-area topography/phase scan performed with the selected optimal parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explicit_gap_measurement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performed</strong></td>
                            <td>computational + experimental measurement (per-iteration scans during MOBO and final full topography scans for validation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_predictions_made</strong></td>
                            <td>Approximately 60 parameter proposals executed (10 random seeding points + 50 active learning iterations); each proposal produced measured scan lines used to compute rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>number_experimentally_validated</strong></td>
                            <td>Approximately 60 measurements (one per proposed parameter set), with final optimized-parameter full topography scans also acquired for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_novelty</strong></td>
                            <td>incremental (application of established MOBO/GP techniques to automated tuning of SPM with new reward design and Pareto-analysis-driven validation).</td>
                        </tr>
                        <tr>
                            <td><strong>extrapolation_distance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging ML-driven experimental control applied to a mature instrument domain (SPM); GP-based MOBO is an established computational method but its application to real-time SPM tuning is an emerging workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_failure_modes</strong></td>
                            <td>Discussed: (1) Phase reward is weak/low-resolution on samples lacking step edges because only ‚âà3% of pixels contribute, reducing its influence; (2) Tip‚Äìsample distance reward can be confounded by sample tilt and thus may not reflect overall scan quality; (3) similarity (Pearson correlation) metric has a narrow dynamic range (clustered values), making it less useful for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>computational modeling with GP ‚Üí per-iteration experimental acquisition of scan lines (used as both proxy measurement and data to update GP) ‚Üí final full-area experimental topography/phase scan for validating the selected optimal parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Qualitative comparison to human heuristics: MOBO choices (e.g., sacrificing tip‚Äìsample distance for better trace‚Äìretrace alignment) agree with human expert heuristics; no quantitative baseline error rates or head-to-head metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Sample-specific factors that affect proxy-to-ground-truth fidelity include sample tilt (affecting tip‚Äìsample distance metric), scarcity of step edges (reducing phase reward signal), measurement noise and the instrument's safe operating setpoint thresholds determined by force‚Äìdistance curves.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1871.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1871.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pareto-Reward-Validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pareto-front based validation and selection of reward functions (height difference vs similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology using Pareto scatter plots and hypervolume contributions from MOBO to evaluate and validate alternative proxy reward definitions by comparing their dynamic range and influence on optimization and confirming via experimental scans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pareto-front analysis for reward definition validation within MOBO</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>instrument optimization / materials characterization (scanning probe microscopy)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Alternative reward definitions for trace‚Äìretrace alignment: (A) Height difference (normalized absolute difference) and (B) Similarity (Pearson correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>expert-heuristic data-derived metrics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Both metrics are computed from measured trace and retrace height lines: Height difference uses normalized sum of absolute differences (then negative-log transformed); Similarity uses Pearson correlation between trace and retrace. The paper computes both in the same experimental MOBO workflow and examines their distributions and span on Pareto scatter plots and contributions to hypervolume.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric_name</strong></td>
                            <td>Qualitative/quantitative scan quality assessed from actual measured trace/retrace scan lines and full topography/phase maps</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_validation_method</strong></td>
                            <td>Experimental scans collected during MOBO iterations and representative characteristic scans (trace/retrace lines and phase maps) taken at selected points (including those on Pareto front) to visually and quantitatively assess which reward better distinguishes good vs bad scans.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explicit_gap_measurement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performed</strong></td>
                            <td>computational + experimental measurement (direct comparison of reward distributions/predicted Pareto front to actual scan line quality measured experimentally)</td>
                        </tr>
                        <tr>
                            <td><strong>number_predictions_made</strong></td>
                            <td>The same MOBO experimental campaign (~10 seed + 50 active iterations) where both reward metrics were computed on the acquired scans; specific count of comparisons not separately enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>number_experimentally_validated</strong></td>
                            <td>Multiple experimentally acquired scans across the MOBO trajectory (‚âà60) were used to evaluate the metrics; representative scans highlighted in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_novelty</strong></td>
                            <td>incremental (methodological validation of reward definitions using Pareto analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>extrapolation_distance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging practice to validate reward/metric design via Pareto-front analysis within autonomous experimental workflows</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_failure_modes</strong></td>
                            <td>Similarity (Pearson) metric failure: narrow dynamic range, clustering near -1 across diverse scan qualities, underrepresented in hypervolume-driven acquisition due to low span; Height difference success: broader dynamic range and smooth distribution enabling better discrimination of scan quality.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Compute multiple reward proxies from each experimental scan ‚Üí compare their distribution and Pareto contributions using GP predictions and measured rewards ‚Üí select/retain reward definitions that show broad dynamic range and experimental discriminative power.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Height difference shown experimentally and in Pareto analysis to outperform Pearson similarity metric as a proxy for trace‚Äìretrace alignment (qualitative: larger span and smoother variation across MOBO trajectory); no numerical performance metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Metric usefulness depends on sample morphology (e.g., presence/absence of step edges), signal sparsity (phase pixels), and sample tilt which differentially affect reward statistics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Machine Learning-Based Reward-Driven Tuning of Scanning Probe Microscopy: Towards Fully Automated Microscopy <em>(Rating: 2)</em></li>
                <li>Integration of scanning probe microscope with high-performance computing: Fixed-policy and reward-driven workflows implementation <em>(Rating: 2)</em></li>
                <li>A self-driving laboratory advances the Pareto front for material properties <em>(Rating: 2)</em></li>
                <li>Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1871",
    "paper_id": "paper-277633954",
    "extraction_schema_id": "extraction-schema-36",
    "extracted_data": [
        {
            "name_short": "MOBO-SPM",
            "name_full": "Multi-Objective Bayesian Optimization workflow for Scanning Probe Microscopy tapping-mode optimization",
            "brief_description": "A closed-loop autonomous optimization system that uses Gaussian process surrogate models and a q-Noisy Expected Hypervolume Improvement acquisition function to optimize multiple heuristic/physics-derived reward metrics (trace-retrace height agreement, tip-sample distance, phase) for tapping-mode SPM by proposing control parameters and validating them with real scans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Multi-Objective Bayesian Optimization (MOBO) with Gaussian processes and q-Noisy Expected Hypervolume Improvement (q-NEHI)",
            "domain": "instrument optimization / materials characterization (scanning probe microscopy)",
            "proxy_metric_name": "Reward functions computed from real-time scan data (Height difference / Trace‚ÄìRetrace difference, Tip‚ÄìSample distance, Phase reward; optionally Similarity/Pearson correlation)",
            "proxy_metric_type": "expert-heuristic data-derived metrics (computed from measured instrument channels) modeled with data-driven surrogates (Gaussian processes)",
            "proxy_metric_description": "At each experimental iteration the system computes rewards from real-time trace and retrace scan lines: (1) Height difference = negative log of normalized absolute difference between trace and retrace height lines; (2) Tip‚Äìsample distance = negative log of the minimum probe height relative to global minimum; (3) Phase reward = negative log of fraction of phase pixels below free-air phase (measures attractive vs repulsive mode); (4) Similarity = Pearson correlation between trace and retrace (used for comparison). These measured rewards are used as targets to train separate Gaussian process models (gpytorch) across the 3D control parameter space (drive amplitude, setpoint, I gain). The acquisition function q-NEHI (from BOTorch) uses GP predicted mean and uncertainty to propose next experimental parameters maximizing expected hypervolume improvement.",
            "ground_truth_metric_name": "Full topography and phase scans acquired on the sample using the optimized control parameters (experimentally measured height and phase maps)",
            "ground_truth_validation_method": "Experimental execution of the suggested control parameters on the actual SPM instrument (AESPM interface) to acquire real trace/retrace scan lines and full topography/phase maps; safe seeding uses force-distance (FD) measurements to exclude repulsive-mode setpoints prior to optimization. Final validation is a full-area topography/phase scan performed with the selected optimal parameters.",
            "proxy_performance": null,
            "ground_truth_performance": null,
            "explicit_gap_measurement": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "has_both_proxy_and_ground_truth": true,
            "validation_performed": "computational + experimental measurement (per-iteration scans during MOBO and final full topography scans for validation)",
            "number_predictions_made": "Approximately 60 parameter proposals executed (10 random seeding points + 50 active learning iterations); each proposal produced measured scan lines used to compute rewards.",
            "number_experimentally_validated": "Approximately 60 measurements (one per proposed parameter set), with final optimized-parameter full topography scans also acquired for validation.",
            "discovery_novelty": "incremental (application of established MOBO/GP techniques to automated tuning of SPM with new reward design and Pareto-analysis-driven validation).",
            "extrapolation_distance": null,
            "proxy_bias_correction": false,
            "proxy_bias_correction_method": null,
            "validation_cost_time": null,
            "domain_maturity": "emerging ML-driven experimental control applied to a mature instrument domain (SPM); GP-based MOBO is an established computational method but its application to real-time SPM tuning is an emerging workflow.",
            "proxy_failure_modes": "Discussed: (1) Phase reward is weak/low-resolution on samples lacking step edges because only ‚âà3% of pixels contribute, reducing its influence; (2) Tip‚Äìsample distance reward can be confounded by sample tilt and thus may not reflect overall scan quality; (3) similarity (Pearson correlation) metric has a narrow dynamic range (clustered values), making it less useful for optimization.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": null,
            "multiple_proxy_types": true,
            "validation_cascade": "computational modeling with GP ‚Üí per-iteration experimental acquisition of scan lines (used as both proxy measurement and data to update GP) ‚Üí final full-area experimental topography/phase scan for validating the selected optimal parameters.",
            "comparison_to_baseline": "Qualitative comparison to human heuristics: MOBO choices (e.g., sacrificing tip‚Äìsample distance for better trace‚Äìretrace alignment) agree with human expert heuristics; no quantitative baseline error rates or head-to-head metrics reported.",
            "domain_specific_factors": "Sample-specific factors that affect proxy-to-ground-truth fidelity include sample tilt (affecting tip‚Äìsample distance metric), scarcity of step edges (reducing phase reward signal), measurement noise and the instrument's safe operating setpoint thresholds determined by force‚Äìdistance curves.",
            "uuid": "e1871.0"
        },
        {
            "name_short": "Pareto-Reward-Validation",
            "name_full": "Pareto-front based validation and selection of reward functions (height difference vs similarity)",
            "brief_description": "A methodology using Pareto scatter plots and hypervolume contributions from MOBO to evaluate and validate alternative proxy reward definitions by comparing their dynamic range and influence on optimization and confirming via experimental scans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Pareto-front analysis for reward definition validation within MOBO",
            "domain": "instrument optimization / materials characterization (scanning probe microscopy)",
            "proxy_metric_name": "Alternative reward definitions for trace‚Äìretrace alignment: (A) Height difference (normalized absolute difference) and (B) Similarity (Pearson correlation)",
            "proxy_metric_type": "expert-heuristic data-derived metrics",
            "proxy_metric_description": "Both metrics are computed from measured trace and retrace height lines: Height difference uses normalized sum of absolute differences (then negative-log transformed); Similarity uses Pearson correlation between trace and retrace. The paper computes both in the same experimental MOBO workflow and examines their distributions and span on Pareto scatter plots and contributions to hypervolume.",
            "ground_truth_metric_name": "Qualitative/quantitative scan quality assessed from actual measured trace/retrace scan lines and full topography/phase maps",
            "ground_truth_validation_method": "Experimental scans collected during MOBO iterations and representative characteristic scans (trace/retrace lines and phase maps) taken at selected points (including those on Pareto front) to visually and quantitatively assess which reward better distinguishes good vs bad scans.",
            "proxy_performance": null,
            "ground_truth_performance": null,
            "explicit_gap_measurement": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "has_both_proxy_and_ground_truth": true,
            "validation_performed": "computational + experimental measurement (direct comparison of reward distributions/predicted Pareto front to actual scan line quality measured experimentally)",
            "number_predictions_made": "The same MOBO experimental campaign (~10 seed + 50 active iterations) where both reward metrics were computed on the acquired scans; specific count of comparisons not separately enumerated.",
            "number_experimentally_validated": "Multiple experimentally acquired scans across the MOBO trajectory (‚âà60) were used to evaluate the metrics; representative scans highlighted in figures.",
            "discovery_novelty": "incremental (methodological validation of reward definitions using Pareto analysis)",
            "extrapolation_distance": null,
            "proxy_bias_correction": false,
            "proxy_bias_correction_method": null,
            "validation_cost_time": null,
            "domain_maturity": "emerging practice to validate reward/metric design via Pareto-front analysis within autonomous experimental workflows",
            "proxy_failure_modes": "Similarity (Pearson) metric failure: narrow dynamic range, clustering near -1 across diverse scan qualities, underrepresented in hypervolume-driven acquisition due to low span; Height difference success: broader dynamic range and smooth distribution enabling better discrimination of scan quality.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": null,
            "multiple_proxy_types": true,
            "validation_cascade": "Compute multiple reward proxies from each experimental scan ‚Üí compare their distribution and Pareto contributions using GP predictions and measured rewards ‚Üí select/retain reward definitions that show broad dynamic range and experimental discriminative power.",
            "comparison_to_baseline": "Height difference shown experimentally and in Pareto analysis to outperform Pearson similarity metric as a proxy for trace‚Äìretrace alignment (qualitative: larger span and smoother variation across MOBO trajectory); no numerical performance metrics reported.",
            "domain_specific_factors": "Metric usefulness depends on sample morphology (e.g., presence/absence of step edges), signal sparsity (phase pixels), and sample tilt which differentially affect reward statistics.",
            "uuid": "e1871.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Machine Learning-Based Reward-Driven Tuning of Scanning Probe Microscopy: Towards Fully Automated Microscopy",
            "rating": 2
        },
        {
            "paper_title": "Integration of scanning probe microscope with high-performance computing: Fixed-policy and reward-driven workflows implementation",
            "rating": 2
        },
        {
            "paper_title": "A self-driving laboratory advances the Pareto front for material properties",
            "rating": 2
        },
        {
            "paper_title": "Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement",
            "rating": 1
        }
    ],
    "cost": 0.01080775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Power of the Pareto Front: Balancing Uncertain Rewards for Adaptive Experimentation in scanning probe microscopy</p>
<p>Yu Liu 
Institute for Advanced Materials and Manufacturing
Department of Materials Science and Engineering
University of Tennessee
37996KnoxvilleTennesseeUSA</p>
<p>Sergei V Kalinin sergei2@utk.edu 
Institute for Advanced Materials and Manufacturing
Department of Materials Science and Engineering
University of Tennessee
37996KnoxvilleTennesseeUSA</p>
<p>Physical Sciences Division
Pacific Northwest National Laboratory
99354Richland, WashingtonUSA</p>
<p>The Power of the Pareto Front: Balancing Uncertain Rewards for Adaptive Experimentation in scanning probe microscopy
6F3A79ECC43B23F06778F68FA875A813</p>
<p>I.</p>
<p>Introduction</p>
<p>Automated scientific discovery is rapidly emerging as a transformative research paradigm, reshaping experimental methodologies through the integration of automated instrumentation, AIdriven decision-making, and multi-tool workflows [1,2].This evolution ranges from the automation of individual scientific instruments [3][4][5] to the full-scale integration of experimental platforms within self-driving laboratories [6][7][8][9][10][11][12][13][14][15][16][17].By enabling autonomous hypothesis testing, adaptive experimentation, and real-time optimization, these systems have the potential to significantly accelerate discoveries across various scientific domains [18][19][20][21].</p>
<p>A fundamental requirement for active discovery workflows is the definition of optimization targets or reward functions that drive the iterative learning process [18].These reward functions form the foundation of autonomous workflows, guiding experimental decisions and facilitating interoperability among multiple tools in complex research environments.Designing effective reward functions is particularly critical in applications that involve multi-objective trade-offs, such as materials synthesis [7,14,18,22,23], high-throughput screening [24], and imaging [5,25].</p>
<p>However, in many cases, reward functions are inherently uncertain or probabilistic.For instance, in thin-film growth, Raman line intensities may be converted into a scoring metric [26].In automated experimentation involving scanning probe microscopy (SPM) and scanning transmission electron microscopy, uncertainty is introduced through complex measurement processes [27][28][29][30][31][32].In most cases, rather than using a single reward function to define the desired experimental outcome, multiple rewards provide a more natural and intuitive framework for human operators.Typically, each reward function captures a distinct aspect of the desired result, making multi-objective optimization particularly valuable for complex, expensive black-box optimization problems.These problems arise in diverse fields, including scientific experimentation [33], materials discovery [15,34], and hyperparameter tuning in machine learning [35][36][37].</p>
<p>Multi-Objective Bayesian Optimization (MOBO) provides an effective approach to optimizing multiple, potentially conflicting objectives while efficiently navigating the parameter space.It leverages Gaussian processes (GP) to model both the mean and uncertainty of expensive objective functions.An acquisition function is then used to balance exploration and exploitation, guiding the selection of subsequent experimental parameters to maximize overall performance.</p>
<p>Rather than identifying a single optimal solution, MOBO seeks to determine a set of tradeoff solutions, forming a Pareto front [35,36,38].On Pareto front, no solution can be improved in one objective without incurring a cost in another.When two reward functions have overlapping optimal parameter regions, as illustrated in Figure 1a-b, their Pareto front collapses to a single point in parameter space, resulting in a trivial Pareto solution as shown in Figure 1e.Conversely, if the optimal parameters for two rewards differ, trade-offs become necessary, leading to a nontrivial Pareto front, as depicted in Figure 1f-g.Notably, even if two rewards share an overlapping optimal solution, introducing a third reward with distinct optimal parameters necessitates tradeoffs among all three rewards in the joint Pareto front.</p>
<p>SPM is an essential tool for investigating materials and nanoscale phenomena.Among its various imaging and functional spectroscopy modes, tapping mode (TM) is the most widely used [39][40][41][42][43][44].However, achieving high-quality SPM images requires extensive tuning of microscope parameters, demanding significant time and expertise [45][46][47].Automating the optimization of SPM at an expert level not only ensures standardized and reproducible high-quality imaging but also aligns with the principles of Findability, Accessibility, Interoperability, and Reusability (FAIR), making high-quality SPM data more broadly available to the scientific community.</p>
<p>In this work, we explore the application of multi-objective Bayesian optimization (MOBO) to autonomous experimentation in SPM.We demonstrate that MOBO effectively balances multiple uncertain reward functions, ensuring FAIR (Findable, Accessible, Interoperable, and Reproducible) access to high-quality experimental data.Furthermore, MOBO provides insights into the interdependencies among different reward functions and facilitates human-in-the-loop decision-making by enabling researchers to tailor trade-offs along the Pareto front for specific experimental goals.We illustrate these concepts through the automated optimization of tapping mode in SPM, where three reward functions are derived from either the underlying physics of the system or heuristic criteria.Our results show that MOBO rapidly optimizes control parameters, yielding high-quality and reproducible scans.Additionally, analysis of the Pareto front offers deeper insight into the relationships between competing rewards and provides a framework for integrating human expertise into the decision-making process.</p>
<p>Figure 1.Rewards design for MOBO of tapping mode in SPM | a, reward of traces is designed to quantify how well the trace and retrace scan lines are aligned.This reward is expected to favor large drive amplitude and small setpoint parameters.b, reward of distance is designed to assess how close the tip is to the sample surface.This one is computed based on the absolute height of the probe and usually favors small setpoins.c, reward of phase is defined to reflect how well the scanning is kept in the attractive mode.It's computed by counting how many phase pixels are in the attractive mode with values above the free-air phase and usually favors small drive amplitudes and large setpoints.Here the expected distribution of rewards is only shown in the drive amplitude vs. setpoint space for easier visualization.d, phase as a function of tip-sample distance in theory.When the tip is very far from the sample surface, the phase stays close to the free-air phase (indicated by the red horizontal dashed line).When the probe gets closer and interacts with sample surface in the attractive mode, the phase remains above the free-air phase.When the probe is so close to the sample surface that it's working in repulsive mode, the phase will be below the freeair phase.e, expected Pareto scatter plot for reward of trace and reward of distance.The control parameters that give good alignment of trace and retrace are expected to give small tip-sample distance.Therefore, the expected Pareto front collapses into a trivial Pareto point.f-g, the reward of phase is competing with the reward of trace and reward of distance in the parameter space.Thus, there are expected trade-offs between them.The cartoons on the axes of e-g show the physical meaning of the extremes of each reward.</p>
<p>II. Define multiple objectives/rewards for the tapping mode SPM</p>
<p>To define reward functions suitable for optimizing tapping mode across a diverse range of materials, environments, and scales, we focus on identifying common features of high-quality TM scans.First, the trace and retrace of the height channel should be in close agreement and ideally follow the sample's true height profile.However, since the ground-truth sample height profile is generally inaccessible, we define a second key characteristic: the probe should be positioned as close to the sample as possible to minimize the tip-sample distance.Finally, the phase values should consistently remain above the free-air phase (Figure 1d) to ensure scanning occurs in the attractive regime [47][48][49].Operating in this regime reduces tip-sample interactions, thereby lowering the risk of tip and sample damage.
Reward 1 = ‚àílog (Œ£ ùëñ |‚Ñé ùë°ùëüùëéùëêùëí ‚àí ‚Ñé ùëüùëíùë°ùëüùëéùëêùëí | (‚Ñé ùë°ùëüùëéùëêùëí + ‚Ñé ùëüùëíùë°ùëüùëéùëêùëí )ùëÅ )(1)Reward 2 = ‚àílog (Œ£ ùëñ ùëÅ ùúÉ&lt;ùúÉ ùëìùëüùëíùëí ùëÅ )(2)Reward 3 = ‚àílog(‚Ñé ùëöùëñùëõ ‚àí ‚Ñé ùëîùëôùëúùëèùëéùëô ùëöùëñùëõ )(3)Reward 4 = ‚àílog(Œ£ ùëñ (2 ‚àí ùëÉ(‚Ñé ùë°ùëüùëéùëêùëí , ‚Ñé ùëüùëíùë°ùëüùëéùëêùëí )) N )(4)
where ‚Ñé  and ‚Ñé  are the trace and retrace height scan lines,  is the total number of pixels in the trace and retrace scan lines,  &lt;  is the number of pixels in the phase scan lines below (repulsive mode) the free phase angles, ‚Ñé  is the lowest probe position of  ‚Ñé trace and retrace height lines while ‚Ñé   is the global lowest probe position computed based on all the acquired scan lines, (‚Ñé  , ‚Ñé  ) is Pearson correlation between ‚Ñé  and ‚Ñé  to quantify the similarity between them.All the components inside log operator are capped minimally to the logarithmic constant e.</p>
<p>Building on these considerations, we define three rewards to quantify these characteristics based on real-time scanning data.Reward 1 (height difference) measures the agreement between the trace and retrace lines, computed as the normalized absolute difference between them.Reward 2 (distance) quantifies probe-sample proximity using the absolute height of the probe.Reward 3 (phase) detects the transition between attractive and repulsive scanning modes, calculated as the fraction of time the phase remains above the free-air phase.Reward 4 is a different way of quantifying the agreement between trace and retrace and we will use MOBO to evaluate which of the reward 1 and 4 is better at distinguishing good scans from bad ones.</p>
<p>Each of these rewards plays a distinct role in optimizing TM scans.The distance reward encourages the probe to remain close to the sample surface rather than oscillating in free space.The height difference reward further refines this proximity by ensuring that the probe closely follows the surface topography.Meanwhile, the phase reward prevents excessive probe-sample contact that could push the system into the repulsive mode.Consequently, the height difference reward favors a combination of high drive amplitude and low setpoint, as illustrated in Figure 1a.The distance reward assigns higher values to configurations with a low setpoint, as shown in Figure 1b.In contrast, the phase reward promotes lower drive amplitudes and higher setpoints, as depicted in Figure 1c.</p>
<p>As a result, the height difference and distance rewards are expected to share an overlapping optimal solution, leading to a trivial Pareto point (Figure 1e).However, the phase reward competes with both the height difference and distance rewards, necessitating trade-offs among them, as demonstrated in Figure 1f-g.</p>
<p>Parameter space -Drive amplitude, Setpoint amplitude, and I Gain.</p>
<p>Here we choose the drive amplitude, setpoint amplitude, and integral (I) gain as the controlling parameters to optimize.The range of drive amplitude is chosen according to the safe-seeding routine to be 0 to 100 nm as shown in Figure 2, with setpoint ranging from 0.1 % to 90 % of the free-air amplitudes.I gain in the PI loop extents from 30 to 200, chosen according to human heuristic about the SPM system used in this work.Here the drive amplitude, setpoint amplitude, and I Gain are chosen to be the controlling parameters to optimize.There are 10 initial random seeding points and 50 explorative active learning steps in this experiment, and the acquisition function of q-Noisy Expected Hypervolume Improvement (qNEHI) is used to determine the next parameters to try at each step.A fresh Tap300Al-G is used to optimize scan at 1 Hz in a 20 m area.a-b, the resulting a height and b phase maps taken with the optimal parameters given by the MOBO workflow.c-d, safe seeding with force-distance measurement.c, the threshold setpoints at different drive amplitudes are derived from corresponding FD curves as the intersection setpoint between phase-setpoint curve and the free-air phase in d.The measured threshold setpoints are interpolated in the full drive amplitude range and only setpoints greater than the threshold setpoints are considered as safe parameters.e, trajectory of MOBO exploration projected from 3D parameter space to the drive amplitude vs. setpoint 2D space with the I gain set to be its optimal value of 66.78.Color map represents the iteration number of steps connected by dashed lines with corresponding colors.f, hypervolume gain curve at different iteration steps for the MOBO workflow.The dashed gay lines separate the initial seeding steps from the active learning steps.</p>
<p>III. Optimization process</p>
<p>The effectiveness of MOBO and its corresponding Pareto front analysis are first demonstrated on a silicon calibration grating sample from Oxford Instruments Asylum Research, as shown in Figure 2a-b.Before optimization, a safe seeding process is implemented to ensure robust exploration of the parameter space.This process begins with a series of force-distance measurements to identify the threshold setpoints at which the system transitions from the attractive to the repulsive regime.The transition is characterized by a sudden phase drop from above to below the free-air phase.Only setpoints exceeding this threshold-corresponding to the attractive mode-are included in the optimization, as illustrated in Figure 2c-d.</p>
<p>To initialize the MOBO model, 10 random seeding points are sampled from the safe parameter space to train the Gaussian process (GP) models.During active learning iterations, the three reward functions are computed and averaged over five neighboring real-time trace and retrace scan lines.Subsequently, three separate GPs are trained to model these reward functions as a function of the control parameters: drive amplitude, setpoint, and integral gain (I gain).From these models, the acquisition function q-Noisy Expected Hypervolume Improvement (q-NEHI) [50,51] is computed to guide the optimization process.A reference point is selected in the hyperparameter reward space at a fixed distance below the maximum of each reward, allowing q-NEHI to quantify the potential improvement in hypervolume across the parameter space.</p>
<p>At each iteration, a Pareto front is computed with respect to the reference point, and the next experimental parameters are selected by maximizing q-NEHI.The optimization process continues until either the maximum number of iterations is reached or the hypervolume improvement converges.Post-analysis of the MOBO trajectory (Figure 2e) and the hypervolume gain curve (Figure 2f) suggest that optimal solutions are typically identified within approximately 10 exploration steps.Upon completion of the optimization, a full topography scan is conducted using the optimized parameters to validate the quality of the final scan.</p>
<p>IV. Analysis of the Pareto front</p>
<p>The Pareto scatter plots in Figure 3a-c confirm that the height difference reward exhibits a trivial Pareto point with the probe-sample distance reward, as indicated by the green arrow.The competition between the phase reward and the other two rewards appears less pronounced, as evidenced by the relatively small variations in the phase reward across most of the scattered points in Figure 3g-i.This limited variation arises because only a small fraction of pixels (~3%) contribute to the phase reward, primarily at step edges, which significantly diminishes its influence in the optimization process.</p>
<p>Although there is a set of controlling parameters that can maximize both the reward of height difference and the reward of tip-sample distance as labeled by the green arrow in Figure 3a, it does not maximize the reward of phase.Instead, the final optimal solution pointed by the red arrow has slightly better reward of phase as shown in Figure 3b-c and verified by less phase pixels below the free-air phase in Figure 3f compared to Figure 2e.The Pareto front scatter plots further illustrate that to reach this optimal solution, indicated by the red arrow, MOBO sacrifices the tip-sample distance reward in the trade-off between the reward of height difference and the reward of tipsample distance, as indicated by the red arrow laying in the lower right of Figure 3a.This choice agrees with human heuristics that the tip-sample distance could be affected by the tilt of the sample and thus less relevant to the overall scan quality compared to the reward of height difference.</p>
<p>Such conclusion is further supported by the predicted distribution of the three rewards shown in Figure 3g-i.The reward of phase in Figure 3i shows broader maxima compared to the other two rewards, agreeing with our conclusion from Pareto front analysis that the reward of phase is less selective due to the lack of step edge pixels in the calibration grating sample.Notably, the maxima of the height difference reward (Figure 3g) and the phase reward (Figure 3i) are in closer proximity within the parameter space, whereas the optimal solution for the tip-sample distance reward (Figure 3h) is located in a different region under the selected I gain parameter.This spatial separation reinforces the observed trade-offs in the MOBO process.</p>
<p>Finally, Figure 3d-f presents three representative trace and retrace scan lines of height and phase, with their corresponding control parameters (Figure 2e) and reward values (Figure 3g-i) highlighted by arrows in matching colors.The height difference shows collaborative behavior with the reward of tip-sample distance.In contrast, the reward of phase shows competing behavior with the reward of height difference and reward of tip-sample distance.d-f, characteristic scan trace and retrace lines of height and phase for d pressing too hard, e pressing slightly too hard, and f the optimal solution.The colored outline matches the colored arrows in a-c to show their corresponding location on the Pareto scatter plot.g-i, the predicted distribution of g height difference, h tip-sample distance, and i phase rewards in the drive amplitude vs. setpoint space at the 50 th step in the MOBO of the calibration grating sample in Figure 2. Detailed controlling parameters used in this figure are summarized in Table S1.</p>
<p>V. Pareto front tests the definition of the reward</p>
<p>In Figure 4, instead of employing the three rewards introduced in Figures 1 and 2 within the MOBO process, a fourth reward is introduced to demonstrate how the Pareto front can be leveraged to validate the definition of rewards.In addition to absolute height difference, the Pearson correlation between trace and retrace scan lines-referred to as the similarity reward-can be used to quantify their alignment.</p>
<p>The resulting Pareto scatter plot between the height difference reward and the similarity reward in Figure 4c highlights that height difference is a more effective metric for quantifying the alignment of trace and retrace scan lines.This conclusion is based on two key observations.First, the height difference reward exhibits a larger span compared to the similarity reward, allowing it to contribute more significantly to hypervolume gain and thus carry greater weight in trade-offs with other rewards.In contrast, the similarity reward's narrow range of variation makes it less influential in the acquisition function, leading to its underrepresentation in the optimization process.Second, the similarity reward is clustered predominantly around a value of -1, despite the MOBO process exploring a broad range of scan qualities.In contrast, the height difference reward is smoothly distributed across its full range, further demonstrating its superior ability to capture variations in trace-retrace alignment.</p>
<p>A well-defined reward function for scan quality should exhibit both a broad dynamic range, ensuring its contribution to hypervolume improvement, and a well-separated, continuous distribution, allowing for effective differentiation of scan qualities.The Pareto scatter plot thus serves as a valuable tool for evaluating the effectiveness of different reward definitions.</p>
<p>The Pareto scatter plots of the remaining three rewards (Figure 4d-f) further illustrate tradeoffs in the water droplet system.Notably, the height difference reward shares similar optimal parameters with the phase reward, as evidenced by their collective behavior in Figure 4e-where height difference can be maximized with minimal compromise in phase reward.This relationship is further corroborated by Figure S2c and S2e.Consequently, the tip-sample distance reward is sacrificed in the optimization trade-off to maximize overall hypervolume improvement, as indicated in Figure 3d.S2a.c, the Pareto scatter plot between the reward of height difference and the reward of similarity, which are defined to quantify the same consideration of how well the trace is aligned with the retrace scan line.The reward of height difference is better because it shows more significant reward improvement compared to the similarity, and it shows smoother change in the whole MOBO process which guarantees a larger distinction between good and bad scans.d-f, Pareto scatter plot for the rewards of height difference, tip-sample distance, and phase.Compared to similar plots in Figure 3, the reward of phase shows better resolution.</p>
<p>VI. MOBO as a Human-in-the-Loop Optimization Framework</p>
<p>MOBO provides a mechanism for incorporating human decision-making into the optimization process, particularly in balancing trade-offs between competing rewards along the Pareto front.This capability is crucial in scientific experiments where human expertise is needed to prioritize specific experimental outcomes.This is particularly useful in the situation where some rewards are more important than others, or where it requires human intervention as there is no satisfactory optimal solutions under given circumstances.</p>
<p>For instance, in Figure 5, we first perform a MOBO-driven optimization of tapping mode on a water droplet sample, yielding three GP models that predict the distribution of the three rewards across the 3D parameter space.Typically, the acquisition function is computed based on the predictions and uncertainties of these GP models to determine the optimal control parameters.However, MOBO enables dynamic reweighting of rewards, allowing the acquisition function to favor particular rewards by adjusting their relative importance.</p>
<p>Figure 5c-d illustrates how modifying the weight of the phase reward and height difference reward influences the optimal solution in the 2D parameter space.The corresponding scan lines (Figure 5e-i) demonstrate that decreasing the weight of the phase reward has minimal impact on the height and phase scan lines.This is consistent with the observed clustering of solutions in the lower left of Figure 5c, suggesting that a steep gradient in the parameter space leads to substantial hypervolume changes with small parameter shifts.This effect arises from the saturation of the phase reward: once all phase pixels remain above the free-air phase, further improvements become marginal.In contrast, increasing the weight of the phase reward (Figure 5g) effectively lowers the phase at the expense of reduced alignment between the height trace and retrace.</p>
<p>Altering the weight of the height difference reward has a more pronounced effect on scan quality.As shown in Figure 5h, decreasing its weight lowers the phase value by increasing the probe-sample distance, but this comes at the cost of significantly worse alignment between trace and retrace.Conversely, increasing the height difference reward weight (Figure 5i) pushes the probe closer to the sample surface, resulting in only marginal improvements in trace-retrace alignment while negatively impacting the phase reward-evidenced by phase pixels transitioning from the attractive to the repulsive mode.Thus, MOBO allows for quantitative and controlled adjustments to optimization objectives, aligning outcomes with human preferences.More importantly, these modifications are made with explicit quantification of their impact on scan quality, providing an objective measure of tradeoffs between rewards.Traditionally, human operators adjust control parameters based on intuition and empirical experience to achieve desired scan properties.With MOBO, instead of manually tuning individual parameters, operators can now directly specify which aspect of the scan to improve by adjusting the corresponding reward weights in a systematic and data-driven manner.Here the drive amplitude, setpoint amplitude, and I Gain are chosen to be the controlling parameters.There are 20 initial random seeding points and 50 explorative active learning steps in this experiment, and the acquisition function of qNEHI is used.A Tap300Al-G is used to optimize scan at 1 Hz in a 5 m area.a-b, the resulting a height and b phase maps taken with the optimal parameters given by the MOBO process.c, after 20 seeding plus 50 explorative steps, different weights (0.1 to 2.0 with 0.1 increment) are multiplied to the reward of phase before retraining the MOBO GP models.The new optimal controlling parameter of drive amplitude, setpoint, and I gain are given by the qNEHI acquisition function at each added weight.Here the updated optimal solutions are plotted in the drive amplitude vs. setpoint space, showing how the optimal solution evolves with different emphasis on the phase reward.d, a similar plot for added weight for the reward of trace.After the extra weights are added, topography maps with new optimal controlling parameters are taken: e-i, the trace and retrace scan lines of height and phase taken along the red dashed lines in a and b with optimal controlling parameters predicted by e default weights on all the rewards, f weight of 0.5 for the reward of phase, g weight of 1.5 for the reward of phase, h weight of 0.5 for the reward of trace, and i weight of 1.5 for the trace.MOBO offers a way to optimize controlling parameters with control on how much each reward will be improved quantitatively.Detailed controlling parameters used in this figure are summarized in Table S1.</p>
<p>VII. Conclusion</p>
<p>We demonstrate that Multi-Objective Bayesian Optimization effectively balances multiple uncertain rewards, enabling FAIR (Findable, Accessible, Interoperable, and Reproducible) access to high-quality and reproducible experimental results.Beyond optimizing system performance, MOBO provides valuable insights into the interplay between competing rewards and incorporates human decision-making into the optimization process by allowing trade-offs within the Pareto front to be tailored for different scenarios.</p>
<p>We illustrate these principles through the automated optimization of tapping mode in Scanning Probe Microscopy (SPM), where three reward functions-derived either from the underlying physics of the system or from human heuristics-guide the optimization.Our results show that MOBO rapidly converges to optimal control parameters, producing high-quality, reproducible scans while quantifying the relationships between competing rewards.The Pareto front analysis not only helps to refine the design and validation of reward functions but also enables human operators to actively participate in the decision-making process.</p>
<p>By assigning weights to different rewards, MOBO allows human operators to precisely control the trade-offs between objectives, transforming each reward into a quantifiable tuning parameter.This framework shifts experimental optimization from intuitive, trial-and-error approaches to a systematic, data-driven strategy, enhancing both efficiency and reproducibility in complex scientific experiments.</p>
<p>MOBO implementation and instrument control</p>
<p>For all the MOBO workflows in this work, we limit the parameter space resolution to 100 x 100 x 50 so that the workflow is lightweight enough to run on a local computer with central processing unit (CPU) only.The surrogate Gaussian Process (GP) model was incorporated using gpytorch [52].MOBO is implemented in BOTorch [53].</p>
<p>The SPM control is achieved by our home built open-source Python interface library, AESPM [54].This library not only enables real-time operating the SPM system local or remotely with code the same way as human operators but also has access to the intermediate data like trace and retrace scan lines in all the channels which are essential for fast optimization presented in this work.We have also prepared a full MOBO workflow based on our SpmSimu simulator that readers can modify to build automated workflow on their own instruments: https://github.com/RichardLiuCoding/Publications/blob/main/AC%20MOBO%20based%20on%20SPM%20simulator_v5.ipynb</p>
<p>AESPM is an open</p>
<p>I. Introduction to Multi-objective Bayesian Optimization</p>
<p>The Pareto front is the set of non-dominated solutions in the objective space, meaning no other solution is strictly better in all objectives at the same time.For example, in the two-objective maximizing problem shown in Figure S1, a point in the hyperparameter (reward) space is a Pareto point, also called non-dominated point, if and only if there are no other points on its right horizontally and there are no other points on its top vertically.When the two rewards have overlapping optimal solutions, their Pareto front will collapse into a trivial Pareto point, as shown in Figure S1d.Once the two rewards have different optimal parameters, there will be trade-offs involved on the nontrivial Pareto front as shown in Figure S1e-f.</p>
<p>Reward 3 = ‚àíReward 1 (7) Here  1 and  2 both range from 0 to 1.</p>
<p>In the MOBO process, the measured Pareto front is computed based on all the measured reward values.In the meantime, multiple individual GPs are used to predict the distribution and uncertainty of each reward in the whole experiment parameter space, based on which the acquisition function can be calculated.In the example of qNEHI, the expected hypervolume gain is computed in the hyperparameter space, considering the expected improvement in each reward and their corresponding uncertainty in the parameter space.Therefore, as shown in Figure S1d-f, the next set of parameters to try is determined by maximizing such expected hypervolume improvements (rewards gain), or equivalently how far the new measurement can push Pareto front away from the reference point to the "upper right" according to GP predictions.</p>
<p>In Figure S1, we show the distribution of Pareto front in the hyperparameter (reward) space in panel e, and parameter space in panel g for reward 1 vs. reward 2 MOBO problem.Notably, the Pareto front appears as an arc connecting the maximums of reward 1 and 2 along their overlapping gradient direction.</p>
<p>We subsequently examine the effect of the two different ways of biasing the MOBO choice in the acquisition function.Here we take 50 random seeding points in the  1 ‚àí  2 parameter space and then modify the acquisition function to give different predictions on the optimal solutions.In Figure S1h, we show the consequence of shifting the reference point.When the reference point is along the diagonal line (ref 1 in Figure S1e), the resulting optimal solution locates in the middle of the maximums of reward 1 and 2 (blue point in Figure S1h), showing no preference on neither reward 1 nor reward 2. When we lower the reference point for reward 1 (the orange point in Figure S1e), the new optimal solution given by MOBO shifts away from the maximum of this reward along the Pareto front, indicated by the orange point in Figure S1h.Therefore, to favor a reward, one needs to increase the corresponding reference point closer to the maximum of that reward.</p>
<p>The other method is to multiply weights directly to the measured rewards before feeding them to the acquisition function.For example, in Figure S1i, we show the results of multiplying the reward 1 with weights ranging from 0.75 to 1.25 on the optimal solutions predicted by the same MOBO.When a weight of greater than 1 is multiplied to a reward, the optimal solution will be moved away from its maximum along the Pareto front.In other words, adding a larger weight to a reward will suppress it in the MOBO.</p>
<p>Figure S1</p>
<p>. Overview of MOBO and Pareto front | a, an example distribution of reward in the parameter space.b, reward 2 is 90-degree rotated counter-clockwise with respect to reward 1 so that it's orthogonal to the reward 1. c, reward 3 is 180-degree rotated to reward 1 and it's opposite to the reward 1. d, Pareto scatter plot for reward 1 and reward 1 itself.When the two rewards have optimal solution overlapping in the parameter space, the Pareto front collapses into a trivial Pareto point (the red cross mark) that optimizes both rewards at the same parameter.e, In the case that the optimal solution of reward 1 is different from that of reward 2, there will be trade-offs between reward 1 and 2 on the Pareto front.f, When the two rewards are opposite to each other, all the points in the Pareto scatter plot will be Pareto front points, meaning that at each point, there is no another point that can further improve any of the rewards without sacrificing the other reward.The green circles in d-f show an example choice of reference in the hyperparameter (rewards) space.g, the distribution of Pareto front in the parameter space.Here the reward 1 and reward 2 are plotted as solid and dashed contours together, with the red dots representing the same Pareto front in the hyperparameter space of panel e. h, the effect of shifting the reference point on the MOBO choice.Lowering the reference point for a reward will move the optimal solution away from it along the Pareto front.i, the effect of adding weight to the rewards in the acquisition function.Adding extra weight (greater than 1) to a reward will also move the optimal solution away from it along the Pareto front.</p>
<p>Figures</p>
<p>Drive Amplitude (nm) Setpoint (%) I Gain (a.u.) Figure 2-a,b</p>
<p>Figure 2 .
2
Figure 2. Safe seeding and stopping criteria | a-b, MOBO of tapping mode on silicon calibration grating sample with 100 nm deep trenches.Here the drive amplitude, setpoint amplitude, and I Gain are chosen to be the controlling parameters to optimize.There are 10 initial random seeding points and 50 explorative active learning steps in this experiment, and the acquisition function of q-Noisy Expected Hypervolume Improvement (qNEHI) is used to determine the next parameters to try at each step.A fresh Tap300Al-G is used to optimize scan at 1 Hz in a 20 m area.a-b, the resulting a height and b phase maps taken with the optimal parameters given by the MOBO workflow.c-d, safe seeding with force-distance measurement.c, the threshold setpoints at different drive amplitudes are derived from corresponding FD curves as the intersection setpoint between phase-setpoint curve and the free-air phase in d.The measured threshold setpoints are interpolated in the full drive amplitude range and only setpoints greater than the threshold setpoints are considered as safe parameters.e, trajectory of MOBO exploration projected from 3D parameter space to the drive amplitude vs. setpoint 2D space with the I gain set to be its optimal value of 66.78.Color map represents the iteration number of steps connected by dashed lines with corresponding colors.f, hypervolume gain curve at different iteration steps for the MOBO workflow.The dashed gay lines separate the initial seeding steps from the active learning steps.</p>
<p>Figure 3 .
3
Figure 3. Predicted rewards and Pareto front of MOBO on the calibration grating | a-c, Pareto scatter plots for a height difference vs. tip-sample distance, b height difference vs. phase,</p>
<p>Figure 4 .
4
Figure 4. MOBO on more challenging sample and validation of reward designs | MOBO of tapping mode on water droplet sample on mica substrate.Here the drive amplitude, setpoint amplitude, and I Gain are chosen to be the controlling parameters.There are 10 initial random seeding points and 50 explorative active learning steps in this experiment, and the acquisition function of qNEHI is used.A fresh Tap300Al-G is used to optimize scan at 1 Hz in a 5 m area.a-b, the training maps of a height and b phase are taken with the different parameters during the MOBO process.These maps correspond to the scan lines taken with 1-8 th iterations in the MOBO trajectory plot in FigureS2a.c, the Pareto scatter plot between the reward of height difference and the reward of similarity, which are defined to quantify the same consideration of how well the trace is aligned with the retrace scan line.The reward of height difference is better because it shows more significant reward improvement compared to the similarity, and it shows smoother change in the whole MOBO process which guarantees a larger distinction between good and bad scans.d-f, Pareto scatter plot for the rewards of height difference, tip-sample distance, and phase.Compared to similar plots in Figure3, the reward of phase shows better resolution.</p>
<p>Figure 5 .
5
Figure 5. Bring humans to decide the trade-offs on the Pareto front | MOBO of tapping mode on water droplet sample on mica substrate.Here the drive amplitude, setpoint amplitude, and I Gain are chosen to be the controlling parameters.There are 20 initial random seeding points and 50 explorative active learning steps in this experiment, and the acquisition function of qNEHI is used.A Tap300Al-G is used to optimize scan at 1 Hz in a 5 m area.a-b, the resulting a height and b phase maps taken with the optimal parameters given by the MOBO process.c, after 20 seeding plus 50 explorative steps, different weights (0.1 to 2.0 with 0.1 increment) are multiplied to the reward of phase before retraining the MOBO GP models.The new optimal controlling parameter of drive amplitude, setpoint, and I gain are given by the qNEHI acquisition function at each added weight.Here the updated optimal solutions are plotted in the drive amplitude vs. setpoint space, showing how the optimal solution evolves with different emphasis on the phase reward.d, a similar plot for added weight for the reward of trace.After the extra weights are added, topography maps with new optimal controlling parameters are taken: e-i, the trace and retrace scan lines of height and phase taken along the red dashed lines in a and b with optimal controlling parameters predicted by e default weights on all the rewards, f weight of 0.5 for the reward of phase, g weight of 1.5 for the reward of phase, h weight of 0.5 for the reward of trace, and i weight of 1.5 for the trace.MOBO offers a way to optimize controlling parameters with control on how much each reward will be improved quantitatively.Detailed controlling parameters used in this figure are summarized in TableS1.</p>
<p>-source SPM-Python interface library.It can be found in the following link with detailed examples and tutorial notebooks: https://github.com/RichardLiuCoding/aespmSpmSimu is an open-source SPM scanning simulator.It can be found in the following link with detailed examples and tutorial notebooks: https://github.com/RichardLiuCoding/spmsimuTo help readers understand and reproduce the results in this work, we have provided an opensource Python notebook for the tutorial presented in the Appendix 1: https://github.com/RichardLiuCoding/Publications/blob/main/MOBO%20Tutorial_v2.ipynb</p>
<p>Reward 1 = 5 )
15
exp(‚àí( 1 ‚àí 0.35) 2 ‚àí ( 2 ‚àí 0.65) 2 ) ‚àí exp(‚àí( 1 ‚àí 0.65) 2 ‚àí ( 2 ‚àí 0.35) 2 ) (Reward 2 = exp(‚àí( 1 ‚àí 0.35) 2 ‚àí ( 2 ‚àí 0.35) 2 ) ‚àí exp(‚àí( 1 ‚àí 0.65) 2 ‚àí ( 2 ‚àí 0.65) 2 )</p>
<p>Figure S2 .
S2
Figure S2.Supplementary plots for the MOBO of droplet sample in Figure 3 | a, trajectory of MOBO exploration projected from 3D parameter space to the setpoint vs. drive amplitude 2D space with the I gain set to be its optimal value.b, hypervolume gain curve at different iteration steps for the MOBO training workflow.The dashed gay lines separate the initial seeding steps from the active learning steps.c-f, the predicted distribution of c height difference, d tip-sample distance, e phase, and f similarity rewards in the setpointdrive amplitude space at 50 th in the MOBO.</p>
<p>Figure S3 .
S3
Figure S3.Full Pareto scatter plot for the MOBO of droplet sample in Figure 4 and predicted distribution of the three rewards at the final step of MOBO.</p>
<p>Figure S4 .
S4
Figure S4.Testing of MOBO on more samples | a-b, height and phase maps taken with optimal parameters given by MOBO for water droplet sample.Similar plots are shown for c-d Cs3Bi2I9 (CBI) microcrystals with a deep hole, e-f flat surface on CBI microcrystals, and g-h a grain boundary on CBI microcrystals.There are 10 initial random seeding points and 50 explorative active learning steps in this experiment, and the acquisition function of q-Noisy Expected Hypervolume Improvement (qNEHI) is used to determine the next parameters to try at each step.A fresh Tap300Al-G is used to optimize scan at 1 Hz.</p>
<p>Table S1 . Detailed controlling parameters for each plot.
S1,i30.1140.3266.78Figure 2-g105.1814.1830.00Figure 2-h36.1039.5873.92Figure 4-a,b,e29.9084.9967.56Figure 4-f30.0186.4366.64Figure 4-g30.0687.9169.43Figure 4-h30.0887.7969.70Figure 4-i29.8484.9462.73
AcknowledgementThis work was supported by the Center for Advanced Materials and Manufacturing (CAMM), the NSF MRSEC center.We thank Astita Dubey and Mahshid Ahmadi for providing us with CBI microcrystals for challenging our MOBO workflow, and Roger Proksch for discussing safe seeding.Supplementary Materials
Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, Nature. 62079722023</p>
<p>Nobel Turing Challenge: creating the engine for scientific discovery. npj Systems Biology and Applications. H Kitano, 2021729</p>
<p>Machine Learning-Based Reward-Driven Tuning of Scanning Probe Microscopy: Towards Fully Automated Microscopy. Y Liu, R Proksch, J Bemis, 10.48550/arXiv.2408.040552024</p>
<p>Automated Experiment in 4D-STEM: Exploring Emergent Physics and Structural Behaviors. K M Roccapriore, O Dyck, M P Oxley, ACS Nano. 1652022</p>
<p>Reward based optimization of resonanceenhanced piezoresponse spectroscopy. Y Liu, B Slautin, J Bemis, Applied Physics Letters. 12642025</p>
<p>Atlas: a brain for self-driving laboratories. R J Hickman, M Sim, S Pablo-Garc√≠a, Digital Discovery. 2025</p>
<p>A Self-Driving Lab for Nano-and Advanced Materials Synthesis. M Zaki, C Prinz, B Ruehle, ACS Nano. 1992025</p>
<p>Self-Driving Laboratories for Chemistry and Materials Science. G Tom, S P Schmid, S G Baird, Chemical Reviews. 124162024</p>
<p>The future of self-driving laboratories: from human in the loop interactive AI to gamification. H Hysmith, E Foadian, S P Padhy, Digital Discovery. 342024</p>
<p>Self-driving laboratories to autonomously navigate the protein fitness landscape. J T Rapp, B J Bremer, P A Romero, Nature Chemical Engineering. 112024</p>
<p>Engineering a Sustainable Future: Harnessing Automation, Robotics, and Artificial Intelligence with Self-Driving Laboratories. S Sadeghi, R B Canty, N Mukhin, ACS Sustainable Chemistry &amp; Engineering. 12342024</p>
<p>A dynamic knowledge graph approach to distributed self-driving laboratories. J Bai, S Mosbach, C J Taylor, Nature Communications. 1514622024</p>
<p>Self-Driving Laboratory for Polymer Electronics. A Vriza, H Chan, J Xu, Chemistry of Materials. 3582023</p>
<p>The rise of self-driving labs in chemical and materials sciences. M Abolhasani, E Kumacheva, Nature Synthesis. 262023</p>
<p>A self-driving laboratory advances the Pareto front for material properties. B P Macleod, F G L Parlane, C C Rupnow, Nature Communications. 1319952022</p>
<p>Autonomous Chemical Experiments: Challenges and Perspectives on Establishing a Self-Driving Lab. M Seifrid, R Pollice, A Aguilar-Granda, Accounts of Chemical Research. 55172022</p>
<p>Self-driving laboratory for accelerated discovery of thin-film materials. B P Macleod, F G L Parlane, T D Morrissey, Science Advances. 688672020. 20</p>
<p>Designing workflows for materials characterization. S V Kalinin, M Ziatdinov, M Ahmadi, Applied Physics Reviews. 1112024</p>
<p>Hypothesis Learning in Automated Experiment: Application to Combinatorial Materials Libraries. M A Ziatdinov, Y Liu, A N Morozovska, Adv Mater. 34e22013452022. 20</p>
<p>Off-the-shelf deep learning is not enough, and requires parsimony, Bayesianity, and causality. npj Computational Materials. R K Vasudevan, M Ziatdinov, L Vlcek, 2021716</p>
<p>Disentangling Ferroelectric Wall Dynamics and Identification of Pinning Mechanisms via Deep Learning. Y Liu, R Proksch, C Y Wong, Advanced Materials. 334321036802021</p>
<p>Combinatorial Synthesis and Screening of Mixed Halide Perovskite Megalibraries. M Lai, D Shin, L Jibril, Journal of the American Chemical Society. 144302022</p>
<p>Nanoparticle synthesis assisted by machine learning. H Tao, T Wu, M Aldeghi, Nature reviews materials. 62021</p>
<p>Automated Materials Discovery Platform Realized: Scanning Probe Microscopy of Combinatorial Libraries. Y Liu, R Pant, I Takeuchi, arXiv 2412.180672024</p>
<p>Y Liu, R Proksch, J Bemis, arXiv 2408.04055Machine Learning-Based Reward-Driven Tuning of Scanning Probe Microscopy: Towards Fully Automated Microscopy. 2024</p>
<p>Real-time experiment-theory closed-loop interaction for autonomous materials science. H Liang, C Wang, H Yu, 10.48550/arXiv.2410.174302024</p>
<p>Building Workflows for Interactive Human in the Loop Automated Experiment (hAE) in STEM-EELS. U Pratiush, K M Roccapriore, Y Liu, 10.48550/arXiv.2404.073812024</p>
<p>Learning the right channel in multimodal imaging: automated experiment in piezoresponse force microscopy. npj Computational Materials. Y Liu, R K Vasudevan, K P Kelley, 2023934</p>
<p>Experimental discovery of structureproperty relationships in ferroelectric materials via active learning. Y Liu, K P Kelley, R K Vasudevan, Nature Machine Intelligence. 442022</p>
<p>Physics-based reward driven image analysis in microscopy. K Barakati, H Yuan, A Goyal, Digital Discovery. 32024</p>
<p>Reward driven workflows for unsupervised explainable analysis of phases and ferroic variants from atomically resolved imaging data. K Barakati, Y Liu, C Nelson, arXiv:2411.126122024arXiv preprint</p>
<p>Unsupervised Reward-Driven Image Segmentation in Automated Scanning Transmission Electron Microscopy Experiments. K Barakati, U Pratiush, A C Houston, 10.48550/arXiv.2409.124622024</p>
<p>Multi-Objective Bayesian Optimization for Laminate-Inspired Mechanically Reinforced Piezoelectric Self-Powered Sensing Yarns. Z Yang, K Park, J Nam, Advanced Science. 113324024402024</p>
<p>A Multi-Objective Molecular Generation Method Based on Pareto Algorithm and Monte Carlo Tree Search. Y Liu, Y Zhu, J Wang, 2410640</p>
<p>Multi-Objective Hyperparameter Optimization in Machine Learning-An Overview. F Karl, T Pielok, J Moosbauer, ACM Trans. Evol. Learn. Optim. 34162023</p>
<p>A survey on multi-objective hyperparameter optimization algorithms for machine learning. A Morales-Hern√°ndez, I Van Nieuwenhuyse, S Rojas Gonzalez, Artificial Intelligence Review. 5682023</p>
<p>Multi-objective parameter configuration of machine learning algorithms using model-based optimization. D Horn, B Bischl, 2016 IEEE Symposium Series on Computational Intelligence (SSCI). 2016</p>
<p>Clustering Analysis for the Pareto Optimal Front in Multi-Objective Optimization. Computation. L A Bejarano, H E Espitia, C E Montenegro, 20221037</p>
<p>Scanning probe microscopy. K Bian, C Gerber, A J Heinrich, Nature Reviews Methods Primers. 11362021</p>
<p>Tapping mode atomic force microscopy in liquids. P K Hansma, J P Cleveland, M Radmacher, Applied Physics Letters. 64131994</p>
<p>Imaging Crystals, Polymers, and Processes in Water with the Atomic Force Microscope. B Drake, C B Prater, A L Weisenhorn, Science. 24348981989</p>
<p>True Atomic Resolution by Atomic Force Microscopy Through Repulsive and Attractive Forces. F Ohnesorge, G Binnig, Science. 26051131993</p>
<p>Atomic Force Microscope. G Binnig, C F Quate, C Gerber, Physical Review Letters. 5691986</p>
<p>Surface Studies by Scanning Tunneling Microscopy. G Binnig, H Rohrer, C Gerber, Physical Review Letters. 4911982</p>
<p>Automatic tuning of PI controller for atomic force microscope based on relay with hysteresis. X Zhou, X Dong, Y Zhang, CCA) &amp; Intelligent Control. 2009. 2009IEEEIEEE Control Applications</p>
<p>PID control system analysis, design, and technology. K H Ang, G Chong, Y Li, IEEE transactions on control systems technology. 1342005</p>
<p>Bistable behavior of a vibrating tip near a solid surface. P Gleyzes, P K Kuo, A C Boccara, Applied Physics Letters. 581991</p>
<p>Energy dissipation measurements in frequencymodulated scanning probe microscopy. R Proksch, S V Kalinin, Nanotechnology. 21454557052010</p>
<p>A Review of Feedforward Control Approaches in Nanopositioning for High-Speed SPM. G M Clayton, S Tien, K K Leang, Journal of Dynamic Systems, Measurement, and Control. 61312009</p>
<p>Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. S Daulton, M Balandat, E Bakshy, 10.48550/arXiv.2006.050782020</p>
<p>Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. S Daulton, M Balandat, E Bakshy, 10.48550/arXiv.2105.081952021</p>
<p>GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. J R Gardner, G Pleiss, D Bindel, 10.48550/arXiv.1809.111652018</p>
<p>BOTORCH: a framework for efficient monte-carlo Bayesian optimization. M Balandat, B Karrer, D R Jiang, Proceedings of the 34th International Conference on Neural Information Processing Systems. the 34th International Conference on Neural Information Processing SystemsVancouver, BC, CanadaCurran Associates Inc2020. 1807</p>
<p>Integration of scanning probe microscope with high-performance computing: Fixed-policy and reward-driven workflows implementation. Y Liu, U Pratiush, J Bemis, Review of Scientific Instruments. 9952024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5190 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5190</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5190</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-270559941</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.11357v2.pdf" target="_blank">Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the "lost-in-the-middle" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose Refiner , an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. Refiner leverages a single decoder-only LLM to adaptively extract query-relevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained Refiner (with 7B parameters) exhibits significant gain to down-stream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, Refiner achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best so-lution. Refiner is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5190.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5190.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Reflective Retrieval-Augmented Generation (Self-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Retrieval-Augmented Generation framework that trains a single language model to perform on-demand retrieval, generation, and self-critique by emitting special reflection tokens (retrieval and critique tokens) during generation to control retrieval and assess answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rag: Learning to retrieve, generate, and critique through self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-RAG (reported with 7B and 13B variants in this paper's baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that augments a generation model with reflection tokens; the paper reports baseline results for Self-RAG implemented with 7B and 13B-sized models (as listed in Table 1). The underlying LM(s) are the Self-RAG variants evaluated by prior work (Asai et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-reflection tokens / self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>During generation the model emits special reflection tokens: 'retrieval' tokens to indicate when to perform on-demand retrieval and 'critique' tokens to assess the quality of generated content and decide on further retrieval/generation steps; this enables selective retrieval and selection among candidate chunk-specific answers.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PopQA, TriviaQA (unfiltered), ARC Challenge, HotpotQA, 2WikiMultihop</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain QA and multi-hop QA benchmarks: PopQA (rare-entity factual QA), TriviaQA (large-scale reading-comprehension QA), ARC Challenge (multiple-choice science reasoning), HotpotQA and 2WikiMultihop (multi-hop question answering requiring synthesis across documents).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported baseline accuracies (from Table 1) — Self-RAG 7B: PopQA 54.9%, TriviaQA 66.4%, ARC-C 47.3%, HotpotQA 41.6%, 2WikiMultihop 39.1%; Self-RAG 13B: PopQA 55.8%, TriviaQA 69.3%, ARC-C 53.1%, HotpotQA 47.5%, 2WikiMultihop 45.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>The paper cites Self-RAG as an advanced RAG approach that uses self-reflection tokens to improve RAG performance (enabling on-demand retrieval and selecting optimal answers from chunks). In this paper Self-RAG is included as a baseline; Refiner-augmented systems outperform Self-RAG on multi-hop QA (the paper reports Refiner margins of up to 7.0% on HotpotQA and 6.4% on 2WikiMultihop vs. the next best solutions, which include advanced RAG methods like Self-RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This paper does not present an ablation of Self-RAG's internal reflection mechanism (no direct in-paper comparison of Self-RAG with vs. without its reflection tokens). The paper only reports Self-RAG as a baseline; reported results suggest Refiner's extract-and-restructure approach gives larger gains on multi-hop tasks, implying Self-RAG may be less effective than structured post-retrieval extraction for synthesizing scattered key facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5190.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5190.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>self-reflection tokens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-reflection tokens (retrieval and critique tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token-based control mechanism used in some RAG frameworks where the LM emits special tokens to trigger retrieval (retrieval tokens) and to perform self-critique/evaluation of its own generations (critique tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rag: Learning to retrieve, generate, and critique through self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n/a (mechanism described / attributed to Self-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A protocol/mechanism rather than a single model: special tokens are integrated into the LM's generation process to orchestrate retrieval and self-assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-reflection tokens (retrieval + critique)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The LM emits retrieval tokens to indicate the need for on-demand retrieval, and critique tokens to evaluate the quality of its own generated answers and guide further retrieval or selection among answers; used to make retrieval/generation more adaptive.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Described as applied within RAG for open-domain QA (referenced generally in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to enable adaptive retrieval/generation and internal critique for question-answering tasks in advanced RAG frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper states such mechanisms (citing Asai et al., 2023) have been proposed to enhance RAG by enabling on-demand retrieval and selecting optimal answers generated from each document chunk, but this paper does not experimentally isolate or measure the token-based reflection mechanism itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No within-paper evaluation; the authors do not report direct failure modes for reflection tokens here. The Refiner paper does note general RAG failure modes (contradictory/distracting chunks, 'lost-in-the-middle') that advanced RAG techniques aim to mitigate, implying token-based reflection alone may not fully resolve scattered-key-info synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5190.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5190.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (COT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative prompting technique where the model is induced to produce intermediate reasoning steps (a 'chain of thought') to improve multi-step reasoning and logical problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>chain-of-thought prompting (applied within RAG in cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompting/decoding style that elicits step-by-step intermediate reasoning traces from an LM; cited here as one of the reasoning methods that can be incorporated into RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative generate-then-reflect (as part of chain-of-thought style reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generates intermediate reasoning steps which can be used to guide subsequent generation or retrieval decisions; in the paper COT is cited as an approach incorporated into RAG to improve logical/difficult tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Logical / multi-step reasoning tasks (general; referenced in context of RAG enhancements)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step logical reasoning where intermediate chain-of-thought can aid final answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper notes that incorporating chain-of-thought into RAG has 'significantly enhanced performance' on some logic-demanding tasks (citing prior work), but does not present direct experimental numbers or ablations in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No direct analysis in this paper; general limitations of COT (e.g., sensitivity to prompt design, possible hallucinated chains) are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5190.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5190.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought (TOT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought (TOT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative search-style approach that lets a model explore multiple chains of thought in a tree structure and perform selection/pruning to find better solutions for difficult problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tree-of-Thought augmentation (referenced as an RAG enhancement)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An approach that branches possible reasoning traces (thoughts) into a tree and uses search/evaluation to select promising branches, applied to improve problem solving in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative generate-and-evaluate (tree search over generated thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iteratively generates multiple candidate reasoning continuations (branches), evaluates/prunes them (a form of self-critique/selection), and continues search to produce higher-quality final answers; cited as incorporable into RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Difficult multi-step reasoning / planning tasks (cited generally)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems that benefit from exploring multiple reasoning trajectories and selecting among them (e.g., complex reasoning benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>The paper cites that combining TOT (and COT) with RAG has been shown in prior work to enhance performance on logic-demanding tasks, but it presents no direct TOT experiments or quantitative comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; general trade-offs (higher compute, complexity of search and evaluation) are not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-rag: Learning to retrieve, generate, and critique through self-reflection <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Learning to refine queries for retrieval augmented generation (RQ-RAG) <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting (origin papers on chain-of-thought prompting) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5190",
    "paper_id": "paper-270559941",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Self-RAG",
            "name_full": "Self-Reflective Retrieval-Augmented Generation (Self-RAG)",
            "brief_description": "A Retrieval-Augmented Generation framework that trains a single language model to perform on-demand retrieval, generation, and self-critique by emitting special reflection tokens (retrieval and critique tokens) during generation to control retrieval and assess answer quality.",
            "citation_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
            "mention_or_use": "use",
            "model_name": "Self-RAG (reported with 7B and 13B variants in this paper's baselines)",
            "model_description": "Framework that augments a generation model with reflection tokens; the paper reports baseline results for Self-RAG implemented with 7B and 13B-sized models (as listed in Table 1). The underlying LM(s) are the Self-RAG variants evaluated by prior work (Asai et al., 2023).",
            "reflection_method_name": "self-reflection tokens / self-critique",
            "reflection_method_description": "During generation the model emits special reflection tokens: 'retrieval' tokens to indicate when to perform on-demand retrieval and 'critique' tokens to assess the quality of generated content and decide on further retrieval/generation steps; this enables selective retrieval and selection among candidate chunk-specific answers.",
            "num_iterations": null,
            "task_name": "PopQA, TriviaQA (unfiltered), ARC Challenge, HotpotQA, 2WikiMultihop",
            "task_description": "Open-domain QA and multi-hop QA benchmarks: PopQA (rare-entity factual QA), TriviaQA (large-scale reading-comprehension QA), ARC Challenge (multiple-choice science reasoning), HotpotQA and 2WikiMultihop (multi-hop question answering requiring synthesis across documents).",
            "performance_with_reflection": "Reported baseline accuracies (from Table 1) — Self-RAG 7B: PopQA 54.9%, TriviaQA 66.4%, ARC-C 47.3%, HotpotQA 41.6%, 2WikiMultihop 39.1%; Self-RAG 13B: PopQA 55.8%, TriviaQA 69.3%, ARC-C 53.1%, HotpotQA 47.5%, 2WikiMultihop 45.7%.",
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "The paper cites Self-RAG as an advanced RAG approach that uses self-reflection tokens to improve RAG performance (enabling on-demand retrieval and selecting optimal answers from chunks). In this paper Self-RAG is included as a baseline; Refiner-augmented systems outperform Self-RAG on multi-hop QA (the paper reports Refiner margins of up to 7.0% on HotpotQA and 6.4% on 2WikiMultihop vs. the next best solutions, which include advanced RAG methods like Self-RAG).",
            "limitations_or_failure_cases": "This paper does not present an ablation of Self-RAG's internal reflection mechanism (no direct in-paper comparison of Self-RAG with vs. without its reflection tokens). The paper only reports Self-RAG as a baseline; reported results suggest Refiner's extract-and-restructure approach gives larger gains on multi-hop tasks, implying Self-RAG may be less effective than structured post-retrieval extraction for synthesizing scattered key facts.",
            "uuid": "e5190.0",
            "source_info": {
                "paper_title": "Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "self-reflection tokens",
            "name_full": "Self-reflection tokens (retrieval and critique tokens)",
            "brief_description": "A token-based control mechanism used in some RAG frameworks where the LM emits special tokens to trigger retrieval (retrieval tokens) and to perform self-critique/evaluation of its own generations (critique tokens).",
            "citation_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
            "mention_or_use": "mention",
            "model_name": "n/a (mechanism described / attributed to Self-RAG)",
            "model_description": "A protocol/mechanism rather than a single model: special tokens are integrated into the LM's generation process to orchestrate retrieval and self-assessment.",
            "reflection_method_name": "self-reflection tokens (retrieval + critique)",
            "reflection_method_description": "The LM emits retrieval tokens to indicate the need for on-demand retrieval, and critique tokens to evaluate the quality of its own generated answers and guide further retrieval or selection among answers; used to make retrieval/generation more adaptive.",
            "num_iterations": null,
            "task_name": "Described as applied within RAG for open-domain QA (referenced generally in the paper)",
            "task_description": "Used to enable adaptive retrieval/generation and internal critique for question-answering tasks in advanced RAG frameworks.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper states such mechanisms (citing Asai et al., 2023) have been proposed to enhance RAG by enabling on-demand retrieval and selecting optimal answers generated from each document chunk, but this paper does not experimentally isolate or measure the token-based reflection mechanism itself.",
            "limitations_or_failure_cases": "No within-paper evaluation; the authors do not report direct failure modes for reflection tokens here. The Refiner paper does note general RAG failure modes (contradictory/distracting chunks, 'lost-in-the-middle') that advanced RAG techniques aim to mitigate, implying token-based reflection alone may not fully resolve scattered-key-info synthesis.",
            "uuid": "e5190.1",
            "source_info": {
                "paper_title": "Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Chain-of-Thought (COT)",
            "name_full": "Chain-of-Thought prompting / reasoning",
            "brief_description": "An iterative prompting technique where the model is induced to produce intermediate reasoning steps (a 'chain of thought') to improve multi-step reasoning and logical problem solving.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "chain-of-thought prompting (applied within RAG in cited prior work)",
            "model_description": "A prompting/decoding style that elicits step-by-step intermediate reasoning traces from an LM; cited here as one of the reasoning methods that can be incorporated into RAG.",
            "reflection_method_name": "iterative generate-then-reflect (as part of chain-of-thought style reasoning)",
            "reflection_method_description": "Generates intermediate reasoning steps which can be used to guide subsequent generation or retrieval decisions; in the paper COT is cited as an approach incorporated into RAG to improve logical/difficult tasks.",
            "num_iterations": null,
            "task_name": "Logical / multi-step reasoning tasks (general; referenced in context of RAG enhancements)",
            "task_description": "Tasks requiring multi-step logical reasoning where intermediate chain-of-thought can aid final answer correctness.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper notes that incorporating chain-of-thought into RAG has 'significantly enhanced performance' on some logic-demanding tasks (citing prior work), but does not present direct experimental numbers or ablations in this work.",
            "limitations_or_failure_cases": "No direct analysis in this paper; general limitations of COT (e.g., sensitivity to prompt design, possible hallucinated chains) are not detailed here.",
            "uuid": "e5190.2",
            "source_info": {
                "paper_title": "Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Tree-of-Thought (TOT)",
            "name_full": "Tree-of-Thought (TOT)",
            "brief_description": "A deliberative search-style approach that lets a model explore multiple chains of thought in a tree structure and perform selection/pruning to find better solutions for difficult problems.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "mention",
            "model_name": "Tree-of-Thought augmentation (referenced as an RAG enhancement)",
            "model_description": "An approach that branches possible reasoning traces (thoughts) into a tree and uses search/evaluation to select promising branches, applied to improve problem solving in LLMs.",
            "reflection_method_name": "iterative generate-and-evaluate (tree search over generated thoughts)",
            "reflection_method_description": "Iteratively generates multiple candidate reasoning continuations (branches), evaluates/prunes them (a form of self-critique/selection), and continues search to produce higher-quality final answers; cited as incorporable into RAG.",
            "num_iterations": null,
            "task_name": "Difficult multi-step reasoning / planning tasks (cited generally)",
            "task_description": "Problems that benefit from exploring multiple reasoning trajectories and selecting among them (e.g., complex reasoning benchmarks).",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "The paper cites that combining TOT (and COT) with RAG has been shown in prior work to enhance performance on logic-demanding tasks, but it presents no direct TOT experiments or quantitative comparisons here.",
            "limitations_or_failure_cases": "Not evaluated in this paper; general trade-offs (higher compute, complexity of search and evaluation) are not quantified here.",
            "uuid": "e5190.3",
            "source_info": {
                "paper_title": "Refiner : Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
            "rating": 2,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Learning to refine queries for retrieval augmented generation (RQ-RAG)",
            "rating": 1,
            "sanitized_title": "learning_to_refine_queries_for_retrieval_augmented_generation_rqrag"
        },
        {
            "paper_title": "Chain-of-thought prompting (origin papers on chain-of-thought prompting)",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_origin_papers_on_chainofthought_prompting"
        }
    ],
    "cost": 0.01619325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities
18 Jun 2024</p>
<p>Zhonghao Li zhonghaoli@hkust-gz.edu.cn 
Hongkong University of Science and Technology</p>
<p>Xuming Hu xuminghu@hkust-gz.edu.cn 
Hongkong University of Science and Technology</p>
<p>Aiwei Liu 
Tsinghua University</p>
<p>Kening Zheng 
Hongkong University of Science and Technology</p>
<p>Sirui Huang 
The Hong Kong Polytechnic University</p>
<p>Hui Xiong xionghui@hkust-gz.edu.cn 
Hongkong University of Science and Technology</p>
<p>Josh Openai 
Steven Achiam 
Sandhini Adler 
Lama Agarwal 
Ilge Ahmad 
Florencia Leoni Akkaya 
Diogo Aleman 
Janko Almeida 
Sam Altenschmidt 
Shyamal Altman 
Red Anadkat 
Igor Avila 
Suchir Babuschkin 
Va- Lerie Balaji 
Paul Balcom 
Haiming Baltescu 
Mohammad Bao 
Jeff Bavarian 
Irwan Belgum 
Jake Bello 
Gabriel Berdine 
Christopher Bernadett-Shapiro 
Lenny Berner 
Oleg Bogdonoff 
Madelaine Boiko 
Anna-Luisa Boyd 
Greg Brakman 
Tim Brockman 
Miles Brooks 
Kevin Brundage 
Trevor Button 
Rosie Cai 
Andrew Campbell 
Brittany Cann 
Chelsea Carey 
Rory Carlson 
Brooke Carmichael 
Che Chan 
Fotis Chang 
Derek Chantzis 
Sully Chen 
Ruby Chen 
Jason Chen 
Mark Chen 
Ben Chen 
Chester Chess 
Casey Cho 
HyungWon Chu 
Dave Chung 
Jeremiah Cummings 
Yunxing Currier 
Cory Dai 
Thomas Decareaux 
Noah Degry 
Damien Deutsch 
Arka Deville 
David Dhar 
Steve Do- Han 
Sheila Dowling 
Adrien Dunning 
Atty Ecoffet 
Tyna Eleti 
David Eloundou 
Liam Farhi 
Niko Fedus 
Simón Posada Fe- Lix 
Juston Fishman 
Isabella Forte 
Leo Fulford 
Elie Gao 
Christian Georges 
Vik Gibson 
Tarun Goel 
Gabriel Gogineni 
Rapha Goh 
Jonathan Gontijo-Lopes 
Morgan Gordon 
Scott Grafstein 
Ryan Gray 
Joshua Greene 
ShixiangShane Gross 
Yufei Gu 
Chris Guo 
Jesse Hal- Lacy 
Jeff Han 
Yuchen Harris 
Mike He 
Jo- Hannes Heaton 
Chris Heidecke 
Alan Hesse 
Wade Hickey 
Peter Hickey 
Brandon Hoeschele 
Kenny Houghton 
Shengli Hsu 
Xin Hu 
Joost Hu 
Shantanu Huizinga 
Shawn Jain 
Joanne Jain 
Angela Jang 
Roger Jiang 
Haozhun Jiang 
Denny Jin 
Shino Jin 
Billie Jomoto 
Heewoo Jonn 
Tomer Jun 
Łukasz Kaftan 
Ali Kaiser 
Ingmar Kamali 
Kanitscheider 
Shirish Nitish 
Tabarak Keskar 
Logan Khan 
Jong Wook Kilpatrick 
Christina Kim 
Yongjik Kim 
Jan Kim 
Jamie Hen- Drik Kirchner 
Matt Kiros 
Daniel Knight 
Łukasz Kokotajlo 
Andrew Kondraciuk 
Aris Kondrich 
Kyle Konstantini- Dis 
Gretchen Kosic 
Vishal Krueger 
Michael Kuo 
Ikai Lampe 
Teddy Lan 
Jan Lee 
Jade Leike 
Daniel Leung 
ChakMing Levy 
Rachel Li 
Molly Lim 
Stephanie Lin 
Mateusz Lin 
Theresa Litwin 
Ryan Lopez 
Patri- Cia Lowe 
Anna Lue 
Kim Makanju 
Sam Malfacini 
Todor Manning 
Yaniv Markov 
Bianca Markovski 
Katie Martin 
Andrew Mayer 
Bob Mayne 
Scott Mcgrew 
Mck- Inney Mayer 
Christine Mcleavey 
Paul Mcmillan 
Jake Mcneil 
David Medina 
Aalok Mehta 
Jacob Menick 
Luke Metz 
Andrey Mishchenko 
Pamela Mishkin 
Vinnie Monaco 
Evan Morikawa 
Daniel Mossing 
Tong Mu 
Mira Murati 
Oleg Murk 
David Mély 
Ashvin Nair 
Reiichiro Nakano 
Rajeev Nayak 
Arvind Neelakantan 
Richard Ngo 
Hyeon- Woo Noh 
Long Ouyang 
Cullen O'keefe 
Jakub Pachocki 
Alex Paino 
Joe Palermo 
Ashley Pantuliano 
Giambattista Parascandolo 
Joel Parish 
Emy Parparita 
Alex Passos 
Mikhail Pavlov 
Andrew Peng 
Adam Perelman 
Mario Ryder 
Ted Saltarelli 
Shibani Sanders 
Girish Santurkar 
Heather Sastry 
David Schmidt 
John Schnurr 
Daniel Schul- Man 
Kyla Selsam 
Toki Sheppard 
Jes- Sica Sherbakov 
Sarah Shieh 
Pranav Shoker 
Szymon Shyam 
Eric Sidor 
Maddie Sigler 
Jordan Simens 
Katarina Sitkin 
Ian Slama 
Benjamin Sohl 
Yang Sokolowsky 
Natalie Song 
Staudacher 
Dave Wiethoff 
Clemens Willner 
Samuel Winter 
Hannah Wolrich 
Lauren Wong 
Sherwin Workman 
Jeff Wu 
Michael Wu 
Kai Wu 
Tao Xiao 
Sarah Xu 
Kevin Yoo 
Qiming Yu 
Wojciech Yuan 
Rowan Zaremba 
Chong Zellers 
Marvin Zhang 
Shengjia Zhang 
Tianhao Zhao 
Jun Zheng </p>
<p>de Avila Belbute Peres
Michael Petrov
Henrique Ponde de Oliveira Pinto
Michael, Pokorny</p>
<p>Michelle Pokrass
Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron RaymondElizabeth Proehl</p>
<p>Francis Real
Kendra Rimbach
Carl Ross, Bob RotstedHenri Roussez, Nick</p>
<p>Felipe Petroski Such
Natalie Summers
Ilya Sutskever
Jie Tang</p>
<p>Nikolas Tezak
Madeleine B. Thomp-son, Phil Tillet, Elizabeth Tseng, Jerry TworekAmin Tootoonchian, Pre-ston Tuggle, Nick Turley</p>
<p>Juan Felipe Cerón Uribe
Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian WengAlvin Wang, Ben Wang, Matt</p>
<p>Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities
18 Jun 2024ABFC6A944A76024867B143CC5D678665arXiv:2406.11357v2[cs.CL]
Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks.To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge.Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance.Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the "lost-inthe-middle" syndrome.Therefore, we typically need to restructure the content for LLM to recognize the key information.We propose Refiner, an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG.Refiner leverages a single decoder-only LLM to adaptively extract queryrelevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively.Experiments show that a trained Refiner (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks.Notably, Refiner achieves a 80.5% tokens reduction and a 1.6-7.0%improvement margin in multi-hop tasks compared to the next best solution.Refiner is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks.</p>
<p>Introduction</p>
<p>Recent Large Language Models (LLMs) (Ope-nAI et al., 2024) have demonstrated their remarkable abilities in understanding query (Brown et al., 2020) (Ouyang et al., 2022) and generating human language texts.Nonetheless, LLMs still face significant limitations, particularly in domain-specific (Zhang et al., 2024) or knowledgeintensive (Kandpal et al., 2023) tasks, where they often hallucinate (Zhang et al., 2023) when handling queries beyond their parametric knowledge (Muhlgay et al., 2024).To address this issue, Retrieval-augmented generation (RAG) (Lewis et al., 2021) has been rapidly developed (Gao et al., 2024), leveraging external knowledge base to retrieve document chunks semantically similar to the query and incorporate them into LLMs, thereby generating more faithful (Khandelwal et al., 2020) and generalizable (Kamalloo et al., 2023) outputs.</p>
<p>However, generating answers from the top retrieved document chunks (Karpukhin et al., 2020) indiscriminately has various shortcomings.Due to the potential ambiguity or insufficiency of information in the query, the chunks could become contradictory and distracting, leading to downstream LLMs (Izacard et al., 2022) to hallucinate (Zhang et al., 2023) (Ji et al., 2023).To mitigate this issue, researchers have proposed advanced RAG approaches, such as adopting query rewriting mechanism (Chan et al., 2024) or self-reflection tokens (Asai et al., 2023) during the generation process, which enhances the performance of RAG by enabling on-demand retrieval and selecting optimal answer generated from each document chunk.Meanwhile, others focus on compressing input prompt by employing a language model (LM) (Anderson et al., 2022) (Yang et al., 2023) (Jiang et al., 2023a) to exclude irrelevant content, thereby reduces computational costs and alleviate "lost-inthe-middle" (Wang et al., 2023) syndrome.</p>
<p>Despite these advancements, LMs are still predisposed to neglect key information scattered across multiple chunks.Moreover, the document chunks, retrieved based on semantic proximity to a query, often pertain to identical subjects and occasionally contain similar yet distinguished information.Failing to maintain context and naively concatenat-Figure 1: The example shows that misleading document chunks are prone to impeding downstream LM from comprehension and leading to hallucination.While our Refiner mitigates the problem by extracting and structuring query-relevant contents by sectioning distinct information, making downstream LM disseminate them effortlessly.</p>
<p>ing these text chunks, can induce confusion and biased comprehension in downstream LM (See examples in Appendix F), exacerbating "lost-in-themiddle" syndrome.This is particularly significant in multi-hop question answering (QA) (Yang et al., 2018) (Ho et al., 2020) task, which requires LM to synthesize information from multiple contents to arrive at a correct answer.Therefore, we typically need to structure the information for downstream LM to differentiate the key information.</p>
<p>We propose Refiner, a novel end-to-end extractand-restructure paradigm that takes into account the relatedness of information across document chunks.The proposal is realized from two approaches: (1).In order for downstream LMs to align with the original context, we propose to keep query-relevant contents verbatim, while also maintain context around those contents on-demand.(2).To emphasize information relatedness among document chunks, we propose to structure the output, as illustrated in the bottom right of Figure 1: The Refiner servers as grouping extractive contents by different information using hierarchical section, wherein contents with similar information are grouped in the same section, while contents with disparate information are segregated into separate sections.This approach aims at further clarifying chunk-level information relatedness for downstream LMs to easily comprehend.A case study of how Refiner resolves the relatedness information in practice, is illustrated in Table 17.</p>
<p>The Refiner leverages an arbitrary pre-trained LLM to generate targeted content, on the assumption that LLM possesses a broad foundation of world knowledge (Zhou et al., 2024) (Li et al., 2024), enabling it to effectively section the intricate, nuanced content-wise relationships.The Refiner is trained with knowledge distillation technique (Jung et al., 2024) to adaptively determine the amount of context to retain around key contents, and output with the proposed format.</p>
<p>Empirical results on several QA tasks, including single-hop and multi-hop tasks, demonstrate that Refiner significantly improves the answer accuracy of downstream LMs, surpassing the previous stateof-the-art (SOTA) advanced RAG solution and concurrent prompt compressing work by a margin of 2.2%-7.0%on two multi-hop QA datasets, and attains comparable accuracy on three single-hop QA datasets with a token reduction of 77.8% on average, compared to the second-best solution.</p>
<p>The Refiner's plug-and-play nature make it ideal for API-based models without parameter access.Once trained, the Refiner can be applied across different upstream retrieval systems and downstream LLMs.The structured output, containing sections, titles and contents, could be further disassembled, leading to a easier source document retrospection or citation integration for industry application.</p>
<p>Related Work</p>
<p>Retrieval Augmented Generation The Retrieval Augmented Generation (RAG) (Lewis et al., 2021) (Gao et al., 2024) fundamentally enhances the capabilities of LLMs by retrieving pertinent documents from an external knowledge base, which is made possible through the calculation of semantic similarity between user's query and documents.Notably, the de facto approach is to segment documents into chunks and construct text representation embedding upon these chunks.By leveraging external knowledge sources like Wikipedia, RAG effectively mitigates the issue of generating factually inaccurate content, thereby greatly enhancing the reliability of generated text.RAG typically involves a retrieval system for query-relevant knowledge indexing from a knowledge base, and a LM that serves as question answering interface.</p>
<p>Advanced RAG Advanced RAG model incorporates multiple optimization strategies that span both pre-retrieval and post-retrieval processes.For example, Self-RAG (Asai et al., 2023) introduces a selective knowledge retrieval mechanism, which utilizes a retrieve token to determine the necessity of retrieval and employs multiple critic tokens to assess answer quality.Building upon this, RQ-RAG (Chan et al., 2024) is trained to dynamically improve query by integrating rewriting, decomposition, and disambiguation functionalities.Furthermore, chain-of-thought (COT) (Wang et al., 2024) and Tree-of-thought (TOT) (Yao et al., 2023) are incorporated to RAG to tackle certain logicaldemanding tasks, which proved to have significantly enhanced the performance of LLMs.</p>
<p>Concurrent Post-Retrieval Work Previous research has explored the application of language model-based approaches to further distill or extract query-relevant key information from retrieved document segments (Lewis et al., 2021).Some studies have developed summarization models (Lennox et al., 2023) (Jung et al., 2024) to derive crucial query-relevant information from retrieved document blocks, RECOMP (Xu et al., 2023) has designed a dual-encoder extractive compressor and a encoder-decoder abstractive model to expunge irrelevant context and summarize upon the remainder.Additionally, LongLLMLingua (Jiang et al., 2023a) proposes a question-aware compression method with a configurable compression rate, allowing them to reduce computational cost and latency, while LLMLingua-2 (Pan et al., 2024) formulates prompt compression as a token classification problem, thereby retains the faithfulness of the compressed prompt.</p>
<p>Compared with recent studies (Xu et al., 2023) (Yoran et al., 2024) (Jiang et al., 2023a) that are similar to our Refiner, major differences are worth mentioning: These methodologies aim at effective and efficient summarization/extraction without considering the semantic relatedness among them, while we dedicate to retain relatedness information by explicitly sectioning them to form a more structured content.Furthermore, we take contextual completeness into account for downstream LMs to align with the original context effortlessly.</p>
<p>Problem Formulation</p>
<p>Following previous work (Xu et al., 2023), we use X = {q 1 , D 1 , ...., q i , D i } to represent Refiner's input sequence set, where q i denotes query and
D i = d 1 i , ...., d K i
denotes the corresponding top-K retrieved documents at step i.The Refiner is designated to extract from D i with respect to q i to generate output sequence y i .</p>
<p>The output sequences of the model y i should satisfy the following principles: (1) Concise: The output should be verbatim and context-completed w.r.t retrieved documents, such that y i ⊆ D i .(2) Sectioned: contents extracted from different document chunks should all be numeric sectioned.Contextually related contents should be grouped into the same section.Conversely, contents with different information should be grouped to different sections.</p>
<p>(3) Succinct: The output length should be significantly shorter than total length of the retrieved documents D i , such that |y i | ≪ |D i |.All contents should be helpful to response to the query.If none of the documents d is relevant, then |y i | = 0.</p>
<p>Proposed Method: Refiner</p>
<p>The primary objective of our research is to develop a model that leverages intrinsic knowledge to structure content relevant to the question in the postretrieval process.Empirical study (Hu et al., 2024) shows that models with larger parameters tend to better conform their output format to instructions and exhibit enhanced capabilities in comprehension.However, larger models are accompanied by a quadratic increase in computational cost pro- portional to the length of the prompt.In our case, where the model is required to respond with brief content from a discursive, lengthy context, the computational cost necessitates particular attention.Therefore, we propose to fine-tune a smaller model in order to reign the computational costs and enhance its capabilities to conform to our proposed structured output format.</p>
<p>Knowledge Distillation</p>
<p>Accurate extraction of query-relevant content with pertinent context is crucial for the Refiner model.To achieve this, we develop a training dataset that teaches Refiner to determine the optimal scope of context.While a simple approach to maintain context might involve selecting a few sentences surrounding the key content from the original document chunk, this risks introducing irrelevant context.Instead, we rely on larger LLMs to adaptively determine the extent of context to retain.Specifically, we employ knowledge distillation (Jung et al., 2024) technique to train Refiner, where a simpler student model approximates the behavior of more intricate teacher models (Zuchniak, 2023).</p>
<p>However, single teacher model can be imperfect, occasionally producing unstructured output, forgetting to provide context or grouping with erroneous section.To address this, we utilize multi-teacher knowledge distillation (Zuchniak, 2023), where multiple teacher models generate outputs from the same input, and the optimal output segment is selected to create a curated dataset.</p>
<p>As depicted in Figure 2, we utilize five teacher models, which are prompted to section contextcompleted query-relevant content from provided document chunks.The generated sentences are rigorously verified against the original document chunks to ensure verbatimness.The section parts are voted on and rearranged, and only the sentences generated by a majority of the models are consid-ered relevant context.The Refiner is subsequently trained on concatenated sentences with section restoration.We design diverse prompts on various QA datasets for teacher models to consistently provide the proposed extraction.See detailed data preparation procedure in Figure 4, composition of teacher model and hyperparameters in Table 6, and task-specific prompt templates in Table 8.</p>
<p>Model Training</p>
<p>Now that we have constructed the training dataset, we use Supervised Fine-Tuning (SFT) (Sun, 2024) to train a pre-trained Llama-2-7B-Chat (Touvron et al., 2023) model.In particular, given a curated high-quality train dataset
S SF T = {X i , D i , Y i } N i=1
, where N denotes number of samples in the dataset, the goal is to gain a model parameters M θ with the following training objective function:
minimize θ L(θ; SSF T ) = −EX,Y∼S SF T [log M θ (Y|X )] (1)
where L denotes the negative likelihood objective function to minimize, M θ represents the model parameters.The expectation E X ,Y∼S SF T averages over our dataset.Note that negative likelihood is adopted to formulate the loss function.Following previous works (Lewis et al., 2021) (Asai et al., 2023) (Yan et al., 2024) (Xu et al., 2023), we train Refiner M θ to generate a textual sequence {y i } N i=1 that extracts core information and context in D relevant to q with fewer tokens than D. To ensure our model can effectively handle training datasets of varying sizes, we employ a progressive training approach (Wu et al., 2024) for the Refiner.More training details are in the Appendix B.</p>
<p>Experiments</p>
<p>We assess the effectiveness of our paradigm on two distinct criteria: its potential to enhance the performance of downstream LMs and its relative efficacy compared to other RAG approaches.We integrate it with various downstream LMs and open-domain question answering datasets, building upon the methodologies established in prior researches (Asai et al., 2023) (Yan et al., 2024) (Xu et al., 2023).</p>
<p>Evaluate Tasks and Datasets</p>
<p>We evaluate our Refiner and downstream LMs on a diverse open-domain QA tasks, including shortform QA, long-form QA and multi-hop QA tasks.Throughout these experiments, we conduct zeroshot evaluations, where the prompts only provide instructions on tasks and outputs in contrast to few-shot examples (Wei et al., 2022) (Sanh et al., 2022).Detailed experiment environments, including prompt templates for Refiner and downstream LMs, are listed in the Appendix C.</p>
<p>Following previous work (Asai et al., 2023), we use accuracy as evaluation metric on all test dataset.Notably, the applied accuracy metric is reported to not always reflects the performance of the RAG system, as it looks up the existence of answer string in the generated output, omitting the semantic coherence which results in biased evaluation.In ARC Challenge task, we construct a regular expression-based calculation, which may influence the results compared with other reported scores from advanced RAG solutions.See subsection C.6 for our motivation and methodology for evaluating ARC Challenge task specifically.</p>
<p>Single-Hop QA Dataset includes three datasets, PopQA (Mallen et al., 2023), TriviaQAunfiltered (Joshi et al., 2017) and Reasoning Challenge (ARC-Challenge) (Clark et al., 2018).The RAG system needs to answer arbitrary questions about factual knowledge for PopQA and TriviaQA, and multiple-choice questions task about scientific event that occurs in daily life for ARC Challenge.For PopQA, we utilize long-tail subset, consisting of 1,399 rare entity queries with monthly Wikipedia page views of less than 100.The ARC Challenge consists of 1,172 QA samples.For TriviaQA, we employ the established validation and test split methodology adopted in prior research (Asai et al., 2023) (Min et al., 2019) (Guu et al., 2020), using 11,313 test queries for evaluation.We employ Contriever-MS MARCO (Izacard et al., 2021) as our retriever and retrieve ten documents for each query by default.</p>
<p>Multi-hop QA Dataset includes HotpotQA-devdistractor (Yang et al., 2018) dataset with 7,405 samples, and 2Wiki-dev (Ho et al., 2020) dataset with 12,576 samples.In contrast to the singlehop datasets, where document chunks are retrieved using our default retriever, we directly utilize the relevant content provided by these datasets.</p>
<p>Advanced RAG and Concurrent Work</p>
<p>We compare Refiner-augmented LMs with other advanced RAG solutions, including Self-RAG (Asai et al., 2023) and RQ-RAG (Chan et al., 2024).Additionally, we assess the performance of concurrent compressors, including RECOMP abstractive compressor (Xu et al., 2023) and LongLLMLingua (Jiang et al., 2023a).Hyperparameter settings are detailed in the subsection C.1 and subsection C.2.To facilitate a fair comparison, we maintain consistency with the experimental frameworks, utilizing the same retriever framework and downstream LMs as those used in Refiner.</p>
<p>Results</p>
<p>Refiner consistently outperforms other approaches on multi-hop QA task</p>
<p>As shown in Table 1, our Refiner significantly outperforms existing advanced RAG variants and concurrent compressors on multi-hop QA tasks, while demonstrating competitive performance on singlehop QA tasks.Compared to the next best solutions, our Refiner-augmented system registers a performance margin of 7.0% on HotpotQA task, and 6.4% on 2WikiMultihop task.Furthermore, the Refiner-augmented LMs bridges the performance gap between small-scale and extreme-scale LLM on TriviaQA and HotpotQA tasks, surpasses GPT-3.5-Turbomodel on PopQA, HotpotQA and 2WikiMultihop task, highlighting the substantial contribution of our Refiner component to RAG.As anticipated, the performance gains fall short on single-hop tasks compared to LongLLMLingua, revealing that the structured output that distinguishes information disparity produced by Refiner has a less pronounced impact on single-hop tasks than on multi-hop tasks.</p>
<p>Refiner's output is highly utilized by LMs</p>
<p>To quantify the efficiency among compressors, we compare their compression rates by dividing the average number of output tokens generated by the compressors by the average number of input tokens in the document chunks, then subtracting the value from one.(see  achieves the highest compression rate, at 96.6%, followed by Refiner, which compresses 89.1% of input tokens.LongLLMLingua's compression rate, set as a hyperparameter to 50%, results in an actual compression rate of approximately 48.9%.</p>
<p>Our Refiner achieves a comparable performance to LongLLMLingua in single-hop QA tasks, with providing only 22.2% of the average number of tokens, and surpasses it in multi-hop QA tasks with 19.5% of tokens, demonstrating high utilization of Refiner's output by downstream LMs, attributable to its structured and context-completed output.</p>
<p>Refiner-augmented system is less susceptible to noisy and lengthy content</p>
<p>Recent study (Xu et al., 2023) reveals that appending irrelevant documents can negatively impact the performance of RAG, our Refiner has successfully helped downstream LMs to mitigate the limitation by significantly reducing prompt length while keeping the performance of downstream LMs.Notably, our experimental results show that even when five additional irrelevant document chunks are appended, the performance of downstream LMs remains steady.In fact, as illustrated in Table 3, the accuracy of downstream LMs in both single-hop and multi-hop tasks has actually experienced an upswing.This content length tolerance exhibited by Refiner-augmented system could alleviate the lost-in-the-middle (Wang et al., 2023) phenomenon, commonly observed in downstream LMs.Such tolerance can be attributed to our training data, which deliberately organizes input document chunks in random order, thereby forcing the model to search and extract key information from full context.</p>
<p>The Refiner is resilient to RAG system</p>
<p>To conduct a thorough investigation of the effectiveness coherency of Refiner within different RAG settings, we undertook evaluation tasks using a range of Refiner-augmented LMs and varied top retrieval settings, as illustrated in Table 3. Notwithstanding the variations in content lengths and downstream LMs, the in-task performance exhibits its stability.As shown in  demonstrating its ubiquitous effectiveness, with the exception of the ARC Challenge task.This is due to the retrieved content's limited assistance with challenging questions, which necessitates downstream LMs to utilize intrinsic knowledge for deduction and extrapolation, eventually highlights the superiority of more sophisticated models.</p>
<p>Ablation Study and Analysis</p>
<p>To assess the efficacy of the proposed structured output, an ablation study is implemented by eliminating the structural components from the Refiner's output, resulting in naively concatenating the extracted content without any structural organization.</p>
<p>As illustrated in Table 4, we evaluate the impact of the removed structure on downstream tasks by testing models equipped with the expunged outputs, This includes using Llama2-(7B, 13B, 70B)-Chat, Meta-Llama3-(8B, 70B)-Instruct models, with both the top 5 and top 10 retrieval configurations.More detailed table from which data in Table 4 is derived, are presented in Table 12 and Table 13.Furthermore, we have conducted various revisions of structure format, the format and corresponding performance statistics are attached in Appendix E.</p>
<p>Task-Level Analysis</p>
<p>Our analysis of the decay in accuracy resulting from the removal of sections reveals significant variations across tasks.As shown in Table 4, the mean accuracy decay calculated for the top 5 and top 10 retrieval blocks indicates that sections make a substantial contribution to accuracy in most tasks, with most accuracy decays being positive.One notable exception is the ARC Challenge task, which experiences a decline in accuracy when sections are added.Further investigation reveals that this phenomenon stems from the retrieval system's propensity to retrieve documents related to incorrect options, leading to confusion for Refiner and a bias towards incorrect information.Encouragingly, the importance of sections varies significantly across tasks, with the overall trend remaining consistent between top retrieval settings.</p>
<p>Model-Level Analysis</p>
<p>Another phenomenon has caught our attention: all downstream LMs under the top 10 retrieval setting have experienced a greater decay in accuracy compared to those under the top 5 retrieval settings.</p>
<p>Our explanation is that Refiner tends to extract more content from the extra document chunks that are remotely correlated to the query, and separates them from those that are closely related using sections.Without these sections, downstream LMs struggle to disseminate crucial information from the lengthy contents, leading to a degraded accuracy.More detailed statistical data in the  chunks.The ratio of authentic output generated by Refiner for each top retrieval setting and evaluation task dataset is calculated and illustrated in Figure 3. Overall, the model exhibits a range of conformity from 87.4% to 96.8%, whereupon we proceed to investigate its malformed output and discuss the underlying causes of this phenomenon.</p>
<p>Temptation to Answering</p>
<p>The major predicament that Refiner encounters, is its compulsion to answer to the query rather than extracting relevant information.This is often exacerbated when the query lacks challenge and the information in the document chunk directly answers to the query.Under such circumstances, the model is inclined to repeat the query and the information, rather than engaging in meaningful information extraction.</p>
<p>Temptation to Extract from Query Additionally, Refiner is also prone to extracting informative content from the query, a problem that is particularly evident in the ARC-Challenge task.This task is characterized by a query comprising a question and four or more potential answers.The model is more inclined to extract content from these options, which, although potentially incorrect, are nonetheless relevant to the question at hand, thus tempting the Refiner model to extract.</p>
<p>Conclusion</p>
<p>This work presents a novel paradigm for Retrieval-Augmented Generation (RAG), introducing Refiner, a novel end-to-end paradigm that extracts and restructures document chunks, organizing query-relevant context-completed content into sections, thereby enhancing the performance of downstream language models.Thorough evaluations on five open-domain question-answering tasks conclusively demonstrate that Refiner outperforms stateof-the-art RAG solutions that employ more complex enhancements, rivaling and surpassing concurrent prompt compress approaches with a significantly higher token compress rate.Our Refiner has been tested in collaboration with various downstream language models and retrieval settings, proving to be resilient to the challenges posed by lengthy and noisy document chunks and consistently effective.Finally, Refiner is a plug-andplay solution that can be seamlessly integrated with modular RAG systems, facilitating its application across diverse open-source frameworks.</p>
<p>Limitations</p>
<p>This study seeks to enhance the quality of Large Language Model (LLM) outputs, a deficiency that continues to have significant real-world consequences, including spread of misinformation and provision of incorrect and dangerous advice.Notwithstanding our method and evaluation have demonstrated efficacy, they are conducted in a relatively indirect way, as how to evaluate the correctness of generated structural output from the model still requires further study.Moreover, our model is trained to extract and section information from open domain document chunks, its robustness has not been tested on alternative input structures, such as table data or domain-specific documents, which raises our concerns about potential misuse.Additionally, due to the nature of Refiner being a probabilistic model, there is always a probability of extracting content that does not exactly derive from the document chunks.Therefore, when using them, it is still necessary to exercise caution when distinguishing and verifying the output.</p>
<p>10 Broader Impact</p>
<p>Current RAG system has been found to hinder LLMs from producing misleading (Kwon et al., 2023) or incorrect information (Yan et al., 2024).</p>
<p>Although there exist concurrent studies that adopt extractive and summarize models to distill information from retrieved documents in post-retrieval process, none have considered the relatedness of document-wise information, potentially resulting in a loss of contextual information that sometimes critical for downstream LMs to arrive at a correct answer.We believe our Refiner is an important step to fill this gap.Furthermore, in contrast to the pre-retrieval and retrieval processes, the postretrieval process has not been fully explored (Gao et al., 2024)</p>
<p>A Model Cards</p>
<p>Below illustrates models inclusive in our work, all usages of models are under the compliance of their intended use.</p>
<p>Model Name Description</p>
<p>GPT-3.5-Turbo</p>
<p>Developed by OpenAI, GPT-3.5-Turbo, or ChatGPT, is an advanced NLP model in the GPT series, featuring enhanced context understanding and text generation capabilities.Trained on a vast array of internet text data, it excels in tasks such as text generation, question answering, translation, and programming assistance, finding use in sectors like customer service, content creation, and education (URL: https://openai.com/chatgpt).</p>
<p>Meta-Llama3-Instruct (8B, 70B)</p>
<p>Meta-Llama3-Instruct, a cutting-edge language model designed to excel in understanding and responding to natural language instructions.This innovative AI model is capable of comprehending complex directives and generating human-like responses that are both accurate and informative.By leveraging advanced machine learning algorithms and large-scale training datasets, Meta-Llama3-Instruct has achieved unprecedented levels of language understanding and generation capabilities, making it an invaluable tool for a wide range of applications.</p>
<p>Llama2-Chat (7, 13, 70B)</p>
<p>Llama2-Chat (Touvron et al., 2023), developed and publicly released by Meta, is a refined version of LLMs, optimized specifically for conversational purposes.The Llama2-Chat belongs to the Llama 2 family, a series of pre-trained and fine-tuned generative text models with 7 to 70 billion parameters.Being a fine-tuned LLM, Llama2-Chat excels in dialogue scenarios.</p>
<p>Alpaca (7B)</p>
<p>Alpaca (Taori et al., 2023) is a language model that has been fine-tuned from Meta's LLaMA 7B model for instruction-following tasks.Using 52,000 self-instruct style demonstrations, it was trained with OpenAI's text-davinci-003 model for instruction-following tasks.In evaluations, Alpaca has shown behaviors akin to OpenAI's text-davinci-003.Notably, Alpaca is distinguished by its small size, ease of replication, and low cost, making it an efficient, accessible model.</p>
<p>RECOMP Abstractive Compressor</p>
<p>RECOMP (Xu et al., 2023) Abstractive Compressor is a part of the RECOMP (Retrieve, Compress, Prepend) method, which involving retrieving documents and compressing them into textual summaries before they are integrated into the context for the language model.This compression not only reduces computational costs but also makes it easier for the language model to identify relevant information in long retrieved documents.The RECOMP Abstractive Compressor is one of the two types of compressors used in this method, which generates summaries by synthesizing information from multiple documents.This allows the compressor to create a concise summary that contains the key information from the retrieved documents.The Abstractive Compressor is trained to improve the performance of the language model on end tasks when the generated summaries are prepended to the language model's input, while keeping the summary concise.</p>
<p>If the retrieved documents are irrelevant to the input or offer no additional information to the language model, the compressor can return an empty string.</p>
<p>LongLLMLingua</p>
<p>LongLLMLingua (Jiang et al., 2023a) is a method designed to compress prompts, thereby enhances the performance of LLMs, reduce computational and financial costs, and decrease latency.By compressing the prompt on token level, it reduces costs and boosts efficiency, improving performance by up to 21.4% using only 1/4 of the tokens.Built upon LLMLingua (Jiang et al., 2023b), the contributions of this paradigm are: (1) a question-aware coarse-to-fine compression method to enhance key information density in the prompt; (2) a document reordering mechanism to minimize information loss; (3) dynamic compression ratios to integrate coarse-grained and fine-grained compression for adaptive granular control; (4) a post-compression subsequence recovery strategy to improve key information integrity.</p>
<p>Self-RAG</p>
<p>Self-Reflective Retrieval-Augmented Generation, or Self-RAG (Asai et al., 2023), is a framework designed to enhance the capabilities of LLMs.It does this by integrating retrieval and selfcritique mechanisms into the model's generation process.The framework trains a single arbitrary language model to adaptively retrieve passages on-demand, generate text, and reflect on retrieved passages and its own generations using special tokens, called reflection tokens.Reflection tokens are categorized into retrieval and critique tokens.Retrieval tokens indicate the need for retrieval, while critique tokens assess the quality of the generation1.Generating reflection tokens makes the language model controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements.</p>
<p>RQ-RAG</p>
<p>Following Self-RAG, RQ-RAG, or Learning to Refine Queries for Retrieval Augmented Generation (Chan et al., 2024), is a paradigm to refine queries with capabilities for explicit rewriting, decomposition, and disambiguation.By regenerating responses based on search results from external search engine, RQ-RAG surpasses the previous state-of-the-art by an average of 1.9% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets.</p>
<p>B Training Details B.1 Dataset</p>
<p>Single-hop QA Dataset consists of ARC Challenge (Clark et al., 2018) train, a set of natural, gradeschool science questions, PubHealth (Kotonya and Toni, 2020) train, a fact-checking for claims provided contents that support or refute to the claims, and TriviaQA (Joshi et al., 2017) train, a challenging reading comprehension QA dataset.Since ARC Challenge train and TriviaQA train dataset do not contain document candidates, we use off-the-shelf Contriever-MS MARCO to retrieve top document chunks from 2020 English Wikipedia using official Wikipedia embeddings.</p>
<p>Multi-hop QA Dataset includes HotpotQA (Yang et al., 2018) train dataset.The dataset contains 90447 pairs of QA and a set of supporting contents randomly mingled with irrelevant or misleading contents.We further clean datasets by removing answers that are absent in the contents or in the outputs of teacher models.Please refer to Table 7 for dataset statistics.</p>
<p>B.2 Teacher Models</p>
<p>We employ five teacher models to generate the proposed structured output for each query-document pair, including Llama2-70B-Chat (Touvron et al., 2023), Meta-Llama3-(8B,70B)-Instruct and Alpaca-7B (Taori et al., 2023).We design four prompts to tackle with diverse QA tasks.For each output generated by each teacher model, we utilize regular expressions to parse and resolve sections, titles, and contents.We then exclude any contents that are not present in the provided document chunks.The remaining contents are subject to a voting process among the teacher models, whereby only those contents that receive a majority vote from the teacher models are retained.Finally, the filtered contents are concatenated to formulate the final training dataset.The detailed task-specific prompts are illustrated in Table 8, and hyperparameters of the teacher models are listed in Table 6.
Hyperparameters Meta-Llama3-70B-Instruct Meta-Llama3-8B-</p>
<p>B.3 Structure Correction</p>
<p>Once outputs are generated by teacher models, we make a series of cleaning and filtering procedures so as to satisfy the proposed principles.Firstly, we filter out contents that are absent in retrieved document chunks, store legitimate sections voted by teacher models, then remove sentences that are vetoed by teacher models.Finally, the selected sections are unified by re-aligning section numbers, titles and extractive contents to form the Refiner's training data as illustrated in Figure 4.</p>
<p>B.4 Environment and Hyperparameters</p>
<p>We use 32GB memory and 4 Nvidia A100 with 80GB memory to train our model.We adopt distributed data parallel (Li et al., 2020) and Deepspeed stage 3 (Rajbhandari et al., 2020) for multi-GPU distributed training.We apply parameter-efficient funetuning with LoRA (Hu et al., 2021) rank of 64, alpha of 16 on Llama2-7B-Chat.AdamW (Loshchilov and Hutter, 2019) is adopted as optimizer.All seeds are set to 633.The total train steps reaches 6 hours in the specified environment.</p>
<p>Input: Given n Teacher LM {Mj} n 1 , Refiner M θ , query set {qi} N 1 , where q represents the user's query, R denotes retrieval system by default, C denotes corpus set, D is a set of corresponding retrieved documents such that D ∈ C, V is a collection of parsed contents in sentence level generated by teacher LM.T denotes temporary content collection.Output: An updated Refiner M θ 1: S ← ∅ 2: for i ∈ {1, . . ., N } do 3:</p>
<p>Di = TopN(R(qi, C)) 4:</p>
<p>for j ∈ {1, . . ., n} do 5:
s j = Generate(Mj, [qi, Di]) 6: V = ParseGeneratedContent(s j ) 7:
for content ∈ V do 8:</p>
<p>if content ̸ ∈ Di then 9:</p>
<p>V.remove(content) 10:
T ← ∅ 11: c = Counter(V) 12:
for content, count ∈ c do 13:</p>
<p>// Iterate over result from counter to get // number of occurrence for each content 14:</p>
<p>if count &gt; n/2 then 15: content = assignSection(content) 16:</p>
<p>T ← T ∪ content 17:</p>
<p>Yi ← Concatenate(T ) 18:
S ← S ∪ [qi, Di; Yi] 19: M θ = Finetune(M θ , S)</p>
<p>B.5 Training Process</p>
<p>As shown in Table 7, the cleaned datasets are highly imbalanced in number of samples.Thus, in order to train the model effectively, we practice progressive training: First, we form training dataset with TriviaQA and HotpotQA dataset combined, the model is trained for 3 epochs with a batch size of 128, a peak learning rate of 4e-5 with 3% warmup steps, and linear decay afterward.Subsequently, we continue training the model combining Arc Challenge and PubHealth datasets, with 5 epoch and a batch size of 128, a peak learning rate of 2e-5 with 5% warmup steps.</p>
<p>Dataset</p>
<p>Dataset Prompts</p>
<p>TriviaQA &amp; HotpotQA</p>
<p>System: You are an expert research assistant.Your job is to find the quotes from the markdown documents that are relevant to a question.\nPlease mark quotes with sections and titles of documents to group quotes by different information only from relevant documents.\nRule of labelling sections: if the fact of the first dummy quote "aaa" from a document with title "## AAA" is consistent with that of the second dummy quote "bbb" from a document with title "## BBB", and they contradict to that of the third dummy quote "ccc" from a document with title "## CCC", then label them as:\n "'\n1.1.## AAA\n aaa\n\n1.2.## BBB\n bbb\n\n2.1.## CCC\n ccc\n\n"'\n Quotes should be verbatim and context completed.\nPlease respond without any explanation.</p>
<p>User: Question: {query}\n Document (multiple documents are separated by "-"):\n {context}</p>
<p>ARC</p>
<p>Challenge</p>
<p>System: You are an expert research assistant.Your job is to find the quotes from the markdown documents that are relevant to a question.\nPlease mark quotes with sections and titles of documents to group quotes by different information only from relevant documents that either support or contradict to the question.\nRule of labelling sections: if the fact of the first dummy quote "aaa" from a document with title "## AAA" is consistent with that of the second dummy quote "bbb" from a document with title "## BBB", and they contradict to that of the third dummy quote "ccc" from a document with title "## CCC", then label them as:\n "'\n1.1.## AAA\n aaa\n\n1.2.## BBB\n bbb\n\n2.1.## CCC\n ccc\n\n"'\n Quotes should be verbatim and context completed.\nPlease respond without any explanation.</p>
<p>User: Question: {query}\n Document (multiple documents are separated by "-"):\n {context} PubHealth System: You are an expert research assistant.Your job is to find the quotes from the markdown documents that either support or contradict to a statement.\nPlease mark quotes with sections and titles of documents to group quotes by different information only from relevant documents that are helpful with answering to the statement.\nRule of labelling sections: if the fact of the first dummy quote "aaa" from a document with title "## AAA" is consistent with that of the second dummy quote "bbb" from a document with title "## BBB", and they contradict to that of the third dummy quote "ccc" from a document with title "## CCC", then label them as:\n "'\n1.1.## AAA\n aaa\n\n1.2.## BBB\n bbb\n\n2.1.## CCC\n ccc\n\n"'\n Quotes should be verbatim and context completed.\nPlease respond without any explanation.</p>
<p>User: Statement: {query}\n Document (multiple documents are separated by "-"):\n {context}</p>
<p>C Evaluation Details</p>
<p>C.1 Hyperparameters for Adavanced RAG For Self-RAG, we adopt the default inference settings.Specifically, we set the weight terms IsRel, IsSup, IsUse values of 1.0, 1.0 and 0.5, respectively.The retrieval threshold is set to 0.2 to encourage frequent retrieval.</p>
<p>C.2 Hyperparameters for Concurrent Compressor</p>
<p>We assess the performance of concurrent compressors encompassing RECOMP abstractive compressor (Xu et al., 2023) and LongLLMLingua (Jiang et al., 2023a).Notably, the RECOMP abstractive compressor has two versions, one trained on TriviaQA dataset and the other on HotpotQA dataset.We evaluate the former version on the PopQA, TriviaQA, and ARC Challenge datasets, while the latter version was tested on the HotpotQA and 2WikiMultihop datasets.Regarding the hyperparameters of LongLLMLingua, we utilized a default compression rate of 0.5 and a dynamic context compression ratio of 0.3.</p>
<p>C.3 Environment and Hyperparameters</p>
<p>We use 32GB memory and 4 Nvidia A100 with 80GB memory for inference.we adopt vllm (Kwon et al., 2023) to speed up inference.All models, including downstream LMs and Refiner, use greedy algorithm to sample their next tokens, and max new tokens is set to 2048.</p>
<p>C.4 Prompt Template for Refiner</p>
<p>To efficiently train Refiner and accelerate convergence speed, we revise its base model, Llama2-Chat's chat template, which is illustrated in Table 9.</p>
<p>Prompt Template for Refiner</p>
<p>C.5 Prompt Template for Downstream LM</p>
<p>To simulate real-world application of Refiner, we prompt downstream LMs to work with Refiner's output as illustrated in the Table 10.</p>
<p>C.6 Evaluation Metric for ARC Challenge</p>
<p>C.6.1 Motivation</p>
<p>To deliver our motivation to revising evaluation metric for ARC Challenge task, we provide a case from the ARC Challenge test subset.In this case, the model is prompted to answer question based on the A, B, C and D choices given, while through reading the context we can easily find out that the model is answering with the choice B. However, the generated content contains all of the four choices, as previous work does not take such cases into account, the performance of the model could be over-evaluated.The given information states that the morning temperature is 41°F, which suggests that the day will be sunny and mild.As the temperature drops gradually throughout the day, it is unlikely that the temperature at 2:00 p.m. will be above 78°F or 98°F, as these temperatures are much higher than the morning temperature.Therefore, the most likely temperature for 2:00 p.m. is 41°F, which is the same as the morning temperature.</p>
<p>Method Case in</p>
<p>Answer choice (A) 32°F is too low, as it is below the morning temperature and unlikely to be the temperature at 2:00 p.m. in a sunny and mild day.Answer choice (C) 78°F is also unlikely, as it is higher than the morning temperature and may not be reached in the afternoon.Answer choice (D) 98°F is the highest temperature mentioned in the passage and is therefore unlikely to be the temperature at 2:00 p.m.</p>
<p>D Ablation Analysis Details</p>
<p>We present a comprehensive disclosure of all ablation analysis results, with specific sections omitted from the Refiner's output.The accuracy outcomes under the top 5 retrieval setting are detailed in Table 12, while the accuracy outcomes under the top 10 retrieval setting are presented in Table 13.</p>
<p>E More Analysis on Output Structure</p>
<p>Our Refiner comes with a structured output, this allows for seamless dissemble of section, titles and contents (see Figure 5) using regular expressions, enables us to easily convert them into numerous alternative structures.In this context, we will explore a total of 16 additional different structure options, as illustrated in Table 14.(1970) based on that story.The Ancient Greek playwright Aeschylus has a gadfly pursue and torment Io, a maiden associated with the moon, watched constantly by the eyes of the herdsman Argus, associated with all the stars: "Io: Ah! Hah! Again the prick, the stab of gadfly-sting!O earth, earth, hide, the hollow shape-Argus-that evil thing-the hundred-eyed."William Shakespeare, inspired by Aeschylus, has Tom o'Bedlam in "King Lear", "Whom the foul fiend hath led through fire and through flame, through ford and whirlpool, o'er bog and quagmire", driven mad by the constant Title: Dromomania Content: sometimes been clinical and pathologizing, and other times been descriptive of unusual enthusiasm without negative or medicalizing connotations, reflecting the diverse uses of the term mania itself.In the 17th century, that the term "mania" came to be used to describe any show of great enthusiasm for a specific activity or object.Later, it came to be used as a suffix for Greek words to refer to an irrational obsession, such as in the words "guanaikomania", "hippomania", and "nymphomania".At the same time emerged the French , such as in which was borrowed in English as "bibliomania".The original sense -Title: Erich Hoyt Content: entomologist Ted Schultz) broke new ground.In Hoyt's "delightful. . .multi-layered" "The Earth Dwellers", the "ant's eye view of life works spectacularly" as Hoyt "fashions the ants into enchanting characters" charting "an insect's course through sex, aggression and foreign policy"."Insect Lives", an American Library Association "Outstanding Book for the College Bound" is a "potpourri of fascinating excerpts written by some of the finest insect biologists and naturalists spanning many centuries."A deep sea book called "Creatures of the Deep" (Firefly, 2001)  Refiner 1.1.Erich Hoyt entomologist Ted Schultz) broke new ground.In Hoyt's "delightful. . .multi-layered" "The Earth Dwellers", the "ant's eye view of life works spectacularly" as Hoyt "fashions the ants into enchanting characters" charting "an insect's course through sex, aggression and foreign policy".1.2.Insects in literature "Io: Ah! Hah! Again the prick, the stab of gadfly-sting!O earth, earth, hide, the hollow shape-Argus-that evil thing-the hundred-eyed."2.1.Obsession (novel) Her mother told her that she did something terrible.No one believes that Tanya's mother Patty, who worked with Milo's partner at the hospital as a nurse, could have done anything terrible, but as the reader learns her past, it contains dark secrets.</p>
<p>Llama2-13B-Chat</p>
<p>The content provided does not directly mention an obsession with a specific creature related to the term "Apimania".However, it is possible to infer that the term "entomophagy" mentioned in the first paragraph refers to an interest or obsession with insects as a food source.The term "entomophagy" is derived from the Greek words "entomon" meaning insect and "phagein" meaning to eat.This practice has been explored in Pedro Reyes' art project "Entomofagia" which focuses on the advantages of shifting protein consumption to insects.Therefore, Apimania could refer to an obsession with insects.</p>
<p>Refiner + Llama2-13B-Chat Based on the content provided, I can answer your question precisely.</p>
<p>According to the content, Apimania is an obsession with bees.This information can be inferred from the text "Obsession (novel)" and the quote "Io: Ah! Hah! Again the prick, the stab of gadfly-sting!O earth, earth, hide, the hollow shape-Argus-that evil thing-the hundred-eyed."which mentions bees and their sting.</p>
<p>Therefore, the answer to your question is: Bees.</p>
<p>Figure 2 :
2
Figure 2: Knowledge distillation for Refiner.The curated training data is generated by multiple teacher models, which undergoes parsing, filtering, voting to improve section and context expediency.</p>
<p>Figure 3 :
3
Figure 3: The ratio of contents extracted by Refiner that are exactly originate from respective document chunks under various top retrieval settings and tasks.</p>
<p>Figure 4 :
4
Figure 4: Prepare curated train dataset for Refiner.</p>
<p>[</p>
<p>INST]«SYS»[MONITOR]{documents}«/SYS»{query}[/INST]</p>
<p>Figure 5 :
5
Figure 5: The structure of output generated by Refiner consists of three parts: section, Title and content, whereSection elucidates relatedness information among extracted contents, Title represents the Title of the Original document, content represents the question-relevant content extracted from that document chunk.</p>
<p>Table 2
2Single-HopMulti-HopPOPQA TQA ARC-C HOTPOT 2WIKILM(acc)(acc)(acc)(acc)(acc)Proprietary LLMGPT-3.5-Turbo <em>29.374.375.357.354.3Advanced RAGSelf-RAG7B </em>54.966.447.341.639.1Self-RAG13B <em>55.869.353.147.545.7RQ-RAG </em>57.1-57.462.644.8With Downstream LM: Llama2-Chat 7BNo Compressor55.467.550.560.757.9RECOMPabstractive52.665.018.747.358.5LongLLMLingua60.370.551.661.355.3Refiner59.368.951.865.161.0With Downstream LM: Llama2-Chat 13BNo Compressor54.871.857.363.858.4RECOMPabstractive53.465.849.249.258.7LongLLMLingua58.371.657.860.156.2Refiner59.470.655.167.161.6With Downstream LM: Meta-Llama3-Instruct 8BNo Compressor64.274.477.170.467.3RECOMPabstractive50.163.348.046.256.1LongLLMLingua59.371.272.064.954.2Refiner59.769.371.167.263.7RECOMPLongLLMLinguaRefinerTask(avg #token)(avg #token)(avg #token)POPQA21.0764.6143.1TQA144.11,049.0206.7ARC-C20.71,022.2280.2HOTPOT28.0757.9130.02WIKI64.6578.6131.2Compress Rate96.6%48.9%89.1%
).We use Llama2-7B Chat tokenizer throughout the experiment.The results reveal that the RECOMP abstractive compressor Table 1: Accuracy on single-hop and multi-hop tasks.We report results of proprietary LLM, advanced RAG that works without Refiner, compared to off-the-shelf Llama2-(7,13)B-Chat and Meta-Llama3-Instruct-8B augmented by Refiner.Bold numbers indicate the best performance among compressors.* indicates results reported by concurrent work.-indicates the corresponding task is not reported by the original paper.</p>
<p>Table 2 :
2
Average number of output tokens and compress rates among tasks and compressors.</p>
<p>Table 3
3Single-HopMulti-HopPOPQA TQA ARC-C Hotpot 2WikiLM(acc)(acc)(acc)(acc)(acc)Refiner of top 5 document chunksRefiner + Llama2-Chat7B59.368.951.147.446.0Refiner + Llama2-Chat13B59.070.554.249.745.7Refiner + Llama2-Chat70B58.372.255.952.350.1Refiner + Meta-Llama3-Instruct8B58.369.171.849.048.0Refiner + Meta-Llama3-Instruct70B57.970.984.149.246.7STD0.571.3614.061.771.80Refiner of top 10 document chunksRefiner + Llama2-Chat7B59.368.951.865.161.0Refiner + Llama2-Chat13B59.470.655.167.161.6Refiner + Llama2-Chat70B60.872.157.069.868.3Refiner + Meta-Llama3-Instruct8B59.769.371.167.263.7Refiner + Meta-Llama3-Instruct70B59.871.184.768.661.8STD0.611.3113.751.772.98
, the standard deviation of in-task accuracy remains relatively constant,</p>
<p>Table 3 :
3
Results of various downstream LMs in Meta Llama family augmented by Refiner with top 5 and top 10 retrieved document chunks.A low standard deviation of accuracy score across downstream LMs in-task, indicating that the Refiner is agnostic to both the downstream LM and the upstream retrieval content length.</p>
<p>Table 4
4,
kens.Although our objective is to train the model to produce output that selectively replicates the source content, there is always a possibility of non-conformity, wherein the model fails to reproduce the content verbatim from the given document</p>
<p>Table 4 :
4
Ablation analysis on various downstream LMs in Meta Llama family augmented by Refiner with top 5 and top 10 retrieved document chunks.Accuracy decay and task-level statistic are presented by removing section parts from Refiner's structured output.</p>
<p>Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.2 Teacher Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.3 Structure Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.4 Environment and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.5 Training Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Part IAppendixTable of ContentsA Model CardsB Training DetailsB.1 C Evaluation DetailsD Ablation Analysis DetailsE More Analysis on Output StructureF Case Studyby academia and, in our opinion, holds significant potential. Therefore, we hope that our work will shed light on this process and inspire further research in this area. The Refiner dataset, Departement Operationnelle, Y. Bengio, R Ducharme, Pas-cal Vincent, and Centre Mathematiques. 2001. A neural probabilistic language model. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-roll L. Wainwright, Pamela Mishkin, Chong Zhang, Sand-hini Agarwal, Katarina Slama, Alex Ray, John Schul-man, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language mod-els to follow instructions with human feedback. Preprint, arXiv:2203.02155.Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xu-fang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, YuqingYang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dong-mei Zhang. 2024. LLMLingua-2: Data distillation forefficient and faithful task-agnostic prompt compression.ArXiv preprint, abs/2403.12968.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020.Zero: memory optimizations toward training trillion parameter models.In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '20.IEEE Press.C.1 Hyperparameters for Adavanced RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.2 Hyperparameters for Concurrent Compressor . . . . . . . . . . . . . . . . . . . . . . . . . . .C.3 Environment and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.4 Prompt Template for Refiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.5 Prompt Template for Downstream LM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.6 Evaluation Metric for ARC Challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p>
<p>Table 5 :
5
Model Cards.</p>
<p>Table 6 :
6
Hyper-parameters of teacher models.
Instruct</p>
<p>Table 7 :
7
Training data statistics.
Initial CleanedArc Challenge 11191119PubHealth95139513TriviaQA61888 56655HotpotQA90447 79157</p>
<p>Table 8 :
8
Prompts used to generated structured outputs from teacher models {query} and {context} represent the actual input query and retrieved documents.</p>
<p>Table 9 :
9
Prompt template for refiner where {query} represents the actual user input query, {documents} represents document chunks retrieved for extraction, {content} represents content in document and {title} represents title of the document from which the content derives.</p>
<p>Table 11 :
11
The result of regular-expression-based accuracy compared to previous work.C.6.2CodeComparisonFor a concise convey of our evaluation metric, we disclose previous work and our pseudo-python code in the following list.</p>
<h1>Previous Methodsdef match(prediction: str, ground_truth: list[str]):# Argument 'prediction' represents generated output from downstream LM,# and 'ground_truth' represents a list of answer key strings.# For example, if the ground truth choice for a question is A,# then ['A'] will be passed to ground_truth.for gt in ground truth:if gt in prediction:return 1return 0# Our Methodsimport redef arc_c_acc(prediction: str, ground_truth: list[str]):# The presence of 'is' preceding the letter 'A' to 'E'# is a necessary condition for a match to occur.regex_answer_key = re.compile(r"is[^\w]+([A-E])[^\w]+", re.DOTALL)# re.search only looks for the first match occurence.if len(ground_truth) == 0:return re.search(regex_answer_key, prediction) is Nonelst_res = []for key in ground_truth:search = re.search(regex_answer_key, prediction)if search is None:lst_res.append(False)continueif key.lower() == search.groups()[0].lower():lst_res.append(True)else:lst_res.append(False)# only when all ground truths are matched should we return True.return all(lst_res)</h1>
<p>Table 12 :
12
Refiner-augmented model accuracy under top 5 retrieval setting.
TaskRefiner acc. w/o. Section acc. DecayLlama2-7B-ChatPopQA59.357.71.6TriviaQA68.966.52.4ARC Challenge51.151.7-0.6HotpotQA47.447.30.12WikiMultihop46.046.3-0.2Llama2-13B-ChatPopQA59.058.50.5TriviaQA70.569.41.1ARC Challenge54.254.4-0.2HotpotQA49.749.50.22WikiMultihop49.747.0-1.3Llama2-70B-ChatPopQA58.358.00.3TriviaQA72.271.40.8ARC Challenge55.958.7-2.8HotpotQA52.352.20.12WikiMultihop50.150.2-0.1Meta-Llama3-8B-InstructPopQA58.357.90.4TriviaQA69.166.82.3ARC Challenge71.874.4-2.6HotpotQA49.049.2-0.22WikiMultihop48.048.00.0Meta-Llama3-70B-InstructPopQA57.957.60.3TriviaQA70.968.92.0ARC Challenge84.185.5-1.4HotpotQA49.248.70.52WikiMultihop46.746.8-0.1</p>
<p>Table 13 :
13
Refiner-augmented model accuracy under top 10 retrieval setting.</p>
<p>Ground bees', 'bees', 'Solitary bee', 'Apiformes', 'Bee flight', 'ground bee', 'bee flight', 'anthophila', 'solitary bee', 'apiforme', 'Solitary bees', 'ground bees', 'Anthophila', 'Bee', 'Bees', 'apiformes', 'Apiforme', 'Ground bee', 'solitary bees', 'bee' Assembly of "pUN" took place in December 2015 at the 21st Century Museum of Contemporary Art in Kanazawa, Japan.Entomofagia (Spanish for entomophagy-the practice of eating insects) is a series of culinary experiments.The first happened in 2013 at Casa do Vidro in São Paulo, where a small snack cart offered visitors a Brazilian street food specialty: giant ants.The project focuses on the enormous ecological advantages of shifting protein consumption to insects.As an alternative to the ubiquitous fast-food staple, the beef hamburger, Reyes created the Grasswhopper, a burger with a patty made of crickets, a common snack in Title: Insects in literature Content: Grasshopper" by SamsonSamsonov (1955)and Jerry Paris
QuestionApimania is an obsession with which creatures?Ground Truths 'Retrieved Document Chunks Title: Pedro Reyes (artist)Content:</p>
<p>won the American Society of Journalists &amp; Authors, Inc. Outstanding Book Award, General Nonfiction.A second deep sea book, "Weird Title: Obsession (novel) Content: Obsession (novel) Obsession is a mystery novel by American author Jonathan Kellerman.It is the 21st novel in the Alex Delaware series.Tanya Bigelow, a former patient, comes to Alex requesting help.She wants him and his friend Milo to investigate something her mother said on her death bed.Her mother told her that she did something terrible.No one believes that Tanya's mother Patty, who worked with Milo's partner at the hospital as a nurse, could have done anything terrible, but as the reader learns her past, it contains dark secrets.Alex has a new dog, Blanche, that Robin bought him after Spike died.Robin is living with Alex again.</p>
<p>Table 18 :
18
Two cases from TriviaQA evaluation set illustrating how Refiner helps downstream LM to highlight the correct information.</p>
<p>AcknowledgementWe thank Kunpeng Zhang for fruitful discussions in the early stages of this work.We thank Yibo Yan for valuable feedback on the paper, and hanying Sun, Fan Shi, jinghuai Ou for moral supports.code and model files will be released under the CC BY-NC 4.0 license.PromptSystem: You are an AI assistant backboned by selective content from different documents, answer user's question helpfully and precisely, with the guidance of the following steps: * If there are no content provided: determine whether it is still possible to answer precisely to the question.<em> If is possible, offer a helpful answer.Otherwise, offer the reason of impossibility.</em> If there exists contents: determine whether the necessary information to answer the question is either directly mentioned or can be inferred from the documents.<em> When there exists different information that can answer to the question, determine whether it is the question too opaque that causes the problem.</em> If not, answer with a summarized information.Otherwise, also provide advice or ask question to disambiguate.<em> When summarizing, ensure to include contents that are relevant to the question.Here is the content: {refiner} User: {query}Output Structure HOTPOT 2WIKImean std mean stdOriginal ContentNumbered Section Markdown Title 0.5% 0.8% -0.5% 1.2% Numbered Section Original Title 0.5% 0.8% -0.3% 1.2% Numbered Section Quote Title 0.7% 1.1% -0.1% 1.5% Hierarchy Section Markdown Title 0.5% 0.9% -0.4% 1.6% Hierarchy Section Quote Title 0.7% 0.9% -0.3% 1.5% Star Section Markdown Title 0.6% 0.7% -0.2% 1.4% Star Section Original Title 0.7% 0.8% -0.1% 1.4% Star Section Quote Title 0.7% 1.0% 0.2% 1.5%Quote ContentNumbered Section Markdown Title 0.9% 1.2% 0.0% 1.5% Numbered Section Original Title 1.0% 1.2% 0.1% 1.5% Numbered Section Quote Title 0.9% 1.3% 0.5% 2.0% Hierarchy Section Markdown Title 0.7% 1.2% -0.2% 1.8% Hierarchy Section Quote Title 1.0% 1.4% 0.2% 1.7% Star Section Markdown Title 0.8% 1.2% 0.2% 1.8% Star Section Original Title 1.1% 1.2% 0.2% 1.7% Star Section Quote Title 1.1% 1.5% 0.7% 2.1%F Case StudyBelow, we will provide several exemplary cases illustrating how our model assists downstream models in further extracting information to enhance accuracy.We select one case from PopQA and two cases from TriviaQA subset under top 5 retrieval setting.For case under PopQA, Refiner successfully disseminates similar yet different information using section, improves the quality of answer of downstream LM.While in both of the cases under TriviaQA, the Refiner-augmented models successfully answer with ground truths which are only mentioned indirectly.Llama2-7B-Chat George Lewis's occupation is footballer.He played as a centre-forward for Watford in the late 1930s and joined Southampton for a brief period after World War II.Refiner + Llama2-7B-Chat Based on the content provided, there are three individuals with the name George Lewis:1. GeorgeLewis (footballer, born 1913) -The content mentions that he was a Welsh footballer.Therefore, his occupation was a footballer.2. GeorgeLewis (footballer, born 2000)-The content mentions that he is a professional footballer.Therefore, his occupation is also a footballer.3. George E. Lewis -The content does not mention his occupation directly, but based on the fact that he is a composer, performer, and scholar of experimental music, it can be inferred that his occupation is related to music, possibly as a composer or musician.Therefore, the answer to the question "What is George Lewis's occupation?" would be:</em> For GeorgeLewis (footballer, born 1913)and GeorgeLewis (footballer, born 2000): footballer.* For George E. Lewis: composer, performer, and scholar of experimental music."Table17: Case study of how Refiner helps downstream LM to disseminate ambiguous information from PopQA evaluation set.QuestionFor which team did Babe Ruth blast his last Major League home run?Ground Truths 'boston braves', 'boston braves disambiguation', 'Boston Braves (disambiguation)', 'Boston Braves'Retrieved Document Chunks Title: Lou Chiozza Content: Braves, had been through for years but was still being exploited for his name.Just a few days before the Philadelphia series, the Babe hit three home runs in Pittsburgh.Chiozza, who had just been brought up from the Memphis Chicks, hit a short fly ball down the left field line that would have ordinarily at best been a double.Due to his advanced age and decreased mobility, Ruth stumbled after the ball in the outfield.The shortstop ran out, retrieved the ball and threw the ball home to barely stop Chiozza from an inside-the-park home run.Many in the park believed that the umpire felt sorry for the aging star and called an obviously safe Chiozza out at home plate.After Ruth realized that he was so slow that Lou almost made a home run on an ordinary base hit, he stood for a minute, folded his glove and walked off the field into the clubhouse.The Babe knew he was done and he officially retired a few days later on June 2, 1935.Chiozza recalled in his later life that he had wished Ruth had retired on a high note after hitting the three home runs in Pittsburgh In 1976, the team was purchased by media magnate Ted Turner, owner of superstation WTBS, as a means to keep the team (and one of his main programming staples) in Atlanta.The financially strapped Turner used money already paid to the team for their broadcast rights as a down-payment.Title: 1935 Boston Braves seasonContent: the team due to the attention he commanded.He also discovered that rather than give him a share of the Braves' profits, Fuchs expected him to invest some of "his" money in the team.Seeing a team in utter collapse and realizing he was finished even as a part-time player, Ruth retired on June 1, six days after he had what remains one of the most memorable afternoons in baseball history.He clouted what turned out to be the last three home runs of his career in a game at Forbes Field while playing the Pittsburgh Pirates.He'd wanted to Refiner 1.1.Babe Ruth Ruth played in the third game of the Pittsburgh series on May 25, 1935, and added one more tale to his playing legend.Ruth went 4-for-4, including three home runs, though the Braves lost the game 11-7.The last two were off Ruth's old Cubs nemesis, Guy Bush.1.2.Atlanta Braves Brave before asking to be traded to the Milwaukee Brewers, while Ruth finished his career as a Boston Brave.Llama2-13B-Chat According to the text, Babe Ruth's last Major League home run was hit off Guy Bush of the Pittsburgh Pirates at Forbes Field on May 25, 1935.Refiner + Llama2-13B-Chat Sure, I'd be happy to help!Based on the content provided, Babe Ruth played for the Boston Braves when he hit his last Major League home run.The content states that Ruth finished his career as a Boston Brave.Therefore, the answer to your question is the Boston Braves.
Lingua: Addressing scenarios for live interpretation and automatic dubbing. Nathan Anderson, Caleb Wilson, Stephen D Richardson, Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas. the 15th Biennial Conference of the Association for Machine Translation in the AmericasOrlando, USA20222Users and Providers Track and Government Track). Association for Machine Translation in the Americas</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, arXiv:2310.115112023Preprint</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2005.14165Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Preprint</p>
<p>Rq-rag: Learning to refine queries for retrieval augmented generation. Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, Jie Fu, arXiv:2404.006102024Preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, ArXiv, abs/1803.054572018</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, arXiv:2312.109972024Preprint</p>
<p>Realm: Retrieval-augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, arXiv:2002.089092020Preprint</p>
<p>Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021Preprint</p>
<p>Finetuning large language models with sequential instructions. Hanxu Hu, Pinzhen Chen, Edoardo M Ponti, arXiv:2403.077942024Preprint</p>
<p>Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, 10.48550/ARXIV.2112.091182021</p>
<p>Atlas: Few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, arXiv:2208.032992022Preprint</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Computing Surveys. 55122023</p>
<p>LongLLMLingua: Accelerating and enhancing llms in long context scenarios via prompt compression. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu, ArXiv preprint, abs/2310.068392023a</p>
<p>LLMLingua: Compressing prompts for accelerated inference of large language models. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu, 10.18653/v1/2023.emnlp-main.825Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023b</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017Preprint</p>
<p>Impossible distillation: from low-quality model to high-quality dataset &amp; model for summarization and paraphrasing. Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin Choi, arXiv:2305.166352024Preprint</p>
<p>Evaluating open-domain question answering in the era of large language models. Ehsan Kamalloo, Nouha Dziri, L A Charles, Davood Clarke, Rafiei, arXiv:2305.069842023Preprint</p>
<p>Large language models struggle to learn long-tail knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, arXiv:2211.084112023Preprint</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen Tau, Yih , arXiv:2004.049062020Preprint</p>
<p>Generalization through memorization: Nearest neighbor language models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, arXiv:1911.001722020Preprint</p>
<p>Explainable automated fact-checking for public health claims. Neema Kotonya, Francesca Toni, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, arXiv:2309.061802023Preprint</p>
<p>Retrieve-cluster-summarize: An alternative to end-to-end training for query-specific article generation. Connor Lennox, Sumanta Kashyapi, Laura Dietz, arXiv:2310.123612023Preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.114012021Preprint</p>
<p>Guozheng Li, Peng Wang, Wenjun Ke, Yikai Guo, Ke Ji, Ziyu Shang, Jiajun Liu, Zijie Xu, arXiv:2404.17809Recall, retrieve and reason: Towards better in-context relation extraction. 2024Preprint</p>
<p>Pritam Damania, and Soumith Chintala. Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, arXiv:2006.15704Pytorch distributed: Experiences on accelerating data parallel training. 2020Preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2019Preprint</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.546Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>A discrete hard EM approach for weakly supervised question answering. Sewon Min, Danqi Chen, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/D19-1284Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Generating benchmarks for factuality evaluation of language models. Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, arXiv:2307.069082024Kevin Leyton-Brown, Amnon Shashua, and Yoav ShohamPreprint</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, M Pandey ; Alexander, Rush, arXiv:2110.08207Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers2022Thomas Wang; Stella Biderman, Leo Gao, Thomas WolfPreprint</p>
<p>Supervised fine-tuning as inverse reinforcement learning. Hao Sun, arXiv:2403.120172024Preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instructionfollowing llama model. 2023</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey EdunovPreprintand Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Learning to filter context for retrieval-augmented generation. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig, arXiv:2311.083772023Preprint</p>
<p>Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, Yitao Liang, arXiv:2403.053132024Preprint</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, arXiv:2109.016522022Preprint</p>
<p>Continual learning for large language models: A survey. Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, Gholamreza Haffari, arXiv:2402.013642024Preprint</p>
<p>Recomp: Improving retrieval-augmented lms with compression and selective augmentation. Fangyuan Xu, Weijia Shi, Eunsol Choi, arXiv:2310.044082023Preprint</p>
<p>Corrective retrieval augmented generation. Jia-Chen Shi-Qi Yan, Yun Gu, Zhen-Hua Zhu, Ling, arXiv:2401.158842024Preprint</p>
<p>PRCA: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter. Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, Jing Xiao, 10.18653/v1/2023.emnlp-main.326Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023Preprint</p>
<p>Making retrieval-augmented language models robust to irrelevant context. Tomer Ori Yoran, Ori Wolfson, Jonathan Ram, Berant, arXiv:2310.015582024Preprint</p>
<p>Raft: Adapting language model to domain specific rag. Tianjun Zhang, G Shishir, Naman Patil, Sheng Jain, Matei Shen, Ion Zaharia, Joseph E Stoica, Gonzalez, arXiv:2403.101312024Preprint</p>
<p>Siren's song in the ai ocean: A survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi, arXiv:2309.012192023Preprint</p>
<p>Grasping the essentials: Tailoring large language models for zero-shot relation extraction. Sizhe Zhou, Yu Meng, Jin Bowen, Jiawei Han, arXiv:2402.111422024Preprint</p>
<p>Multi-teacher knowledge distillation as an effective method for compressing ensembles of neural networks. Konrad Zuchniak, arXiv:2302.072152023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1008 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1008</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1008</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-49429853</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1806.09614v2.pdf" target="_blank">Accuracy-based Curriculum Learning in Deep Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we investigate a new form of automated curriculum learning based on adaptive selection of accuracy requirements, called accuracy-based curriculum learning. Using a reinforcement learning agent based on the Deep Deterministic Policy Gradient algorithm and addressing the Reacher environment, we first show that an agent trained with various accuracy requirements sampled randomly learns more efficiently than when asked to be very accurate at all times. Then we show that adaptive selection of accuracy requirements, based on a local measure of competence progress, automatically generates a curriculum where difficulty progressively increases, resulting in a better learning efficiency than sampling randomly.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1008.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1008.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DDPG+UVFA (Reacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Deterministic Policy Gradient agent with Universal Value Function Approximator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous-control deep RL agent (DDPG) augmented with a UVFA input that includes the target accuracy parameter; trained in the 2-DOF Reacher environment using sparse success rewards and curriculum strategies over required accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accuracy-based Curriculum Learning in Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DDPG + UVFA agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep reinforcement learning agent using the DDPG algorithm (actor-critic deterministic policy gradients) with a Universal Value Function Approximator that takes state and an accuracy requirement ε as input; trained with sparse binary success rewards (success if end-effector within ε of goal). Competence-progress based sampling and random sampling over accuracy levels were evaluated as curriculum strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Reacher (OpenAI Gym)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2-degree-of-freedom planar robotic arm must move its end-effector to randomly sampled target positions; state includes arm angles, angular velocities, end-effector position and target position. Difficulty is varied by an accuracy threshold ε used in the sparse reward: reward = 0 if |p_finger − p_target| ≤ ε else −1. New target is sampled each trial, providing intrinsic exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task difficulty is defined by required accuracy ε (smaller ε = higher difficulty). Additional operational measures reported: number of training steps, evaluations every 1000 steps (10 episodes × 50 steps per evaluation), and competence progress computed over sliding windows of 2N evaluations (N=3, windows correspond to 3000 environment steps).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies across ε ∈ {0.02, 0.03, 0.04, 0.05}; highest complexity = ε=0.02 (most demanding), lowest complexity = ε=0.05 (easiest). Overall environment complexity is low-to-moderate (simple 2-DOF dynamics) but task difficulty is modulated by ε.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is the set and sampling distribution of accuracy requirements E = {0.02,0.03,0.04,0.05}; strategies: single fixed ε (baseline), RANDOM- (uniform sampling from E each episode), and ACTIVE- (sampling proportional to measured competence progress per ε). Sampling proportions over training are tracked and reported.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Medium (four discrete accuracy variants sampled during training); variation temporally nonstationary under ACTIVE- (sampling distribution evolves from favoring easy ε to favoring hard ε as competence progresses).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate on evaluation episodes (proportion of successful episodes) using ε=0.02; additionally competence score per ε measured as success rate over 10 test targets during periodic evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: RANDOM- (uniform sampling over ε) achieves higher success rate and lower run-to-run variance on ε=0.02 evaluation than the baseline trained only on ε=0.02; ACTIVE- (competence-progress sampling, β=4) achieves faster early learning, higher final success, and lower variability than RANDOM-. Exact numeric success rates are reported in figures but not tabulated in text; evaluations: 10 episodes × 50 steps every 1000 training steps, averaged over 10 independent runs.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly studies and documents the relationship: easier (larger ε) tasks yield faster early competence progress but plateau once mastered, while harder (smaller ε) tasks remain challenging and show sustained competence progress later. ACTIVE- sampling exploits this by prioritizing accuracies with high competence progress, producing an emergent curriculum that moves from low complexity (easy, large ε) to high complexity (hard, small ε). The authors also note a structural property: trajectories that satisfy a tight accuracy also satisfy looser accuracies, enabling transfer from easy to hard via generalization when ε is included in the state.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Baseline: agent trained only on the high-complexity condition (ε=0.02) — performance is worse (slower learning, higher variance) than RANDOM- and ACTIVE- on the ε=0.02 evaluation; no exact numeric value provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Not separately reported as an isolated test condition; training that includes many easy tasks (large ε) within RANDOM- or ACTIVE- led to faster early competence on those easy ε values, but specific success rates for a high-variation low-complexity-only regime are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>ACTIVE- (curriculum adapting sampling across ε including the high-complexity ε=0.02) yields the best observed performance on the evaluation ε=0.02 (faster initial learning, higher final accuracy, lower variance) compared to RANDOM- and baseline; numeric values are plotted but not explicitly listed in text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Not explicitly tested as a single condition in reported experiments (the baseline is low-variation but high-complexity; RANDOM- is higher-variation), so not available.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning: three strategies compared — (1) baseline single-task training on hardest ε (no curriculum), (2) RANDOM- uniform sampling across accuracy variants (multi-condition training / variation), (3) ACTIVE- competence-progress driven sampling (automated curriculum that prioritizes accuracies yielding maximal competence progress).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — agents trained with mixed easier accuracies (RANDOM- and especially ACTIVE-) generalize to better performance on the hard evaluation (ε=0.02) than an agent trained only on that hard accuracy, despite receiving less experience at ε=0.02 during training. The paper attributes this to transfer/generalization enabled by including ε in the UVFA input and to the nested property that meeting a tight accuracy implies meeting looser accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved sample efficiency observed: RANDOM- and ACTIVE- show faster early improvement than baseline; notable temporal detail: easier accuracies produce quick early progress and plateau by ≈150k training steps, after which sampling shifts toward harder ε; competence progress windows correspond to 3000 steps (N=3) and evaluations every 1000 steps (10 episodes of 50 steps). Exact counts of total training steps per run are not explicitly tabulated in text, but figure timelines show key transitions around 150,000 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Training on a mixture of accuracy requirements improves learning efficiency and reduces variance on the hardest evaluation accuracy compared to training only on the hardest requirement. 2) Competence-progress driven sampling (ACTIVE-) automatically generates a curriculum that orders accuracies from easy to hard, prioritizing easy tasks early (large ε) and shifting to hard tasks (small ε) later; this yields better performance than uniform random sampling across accuracies. 3) Easier tasks produce faster early competence progress but plateau, whereas harder tasks yield sustained progress later — leveraging this temporal structure via competence-progress measures improves overall learning. 4) The nested property of accuracy-based difficulty (success at small ε implies success at larger ε) facilitates transfer; this property may not hold for other notions of difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Competence progress cp per ε is computed as the absolute difference of average competence over two consecutive sliding windows of 3 evaluations (N=3), encouraging sampling when progress (including negative progress due to forgetting) is large; ACTIVE- uses a softmax-like prioritization P(i) ∝ cp_i^β with β=4 found effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accuracy-based Curriculum Learning in Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1008.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1008.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reacher-baseline (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline DDPG agent trained only on ε = 0.02</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline condition in this work: the DDPG+UVFA agent is always trained with the hardest accuracy requirement (ε=0.02) and evaluated on that same accuracy; used to compare curricula and variation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accuracy-based Curriculum Learning in Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Baseline DDPG (ε=0.02)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same DDPG+UVFA architecture but trained exclusively on the most demanding accuracy requirement (ε=0.02); serves as a no-curriculum control.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Reacher (OpenAI Gym), ε=0.02</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same 2-DOF Reacher environment with sparse binary success reward using the tightest accuracy threshold ε=0.02, making reward rarer and task harder.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity set by ε=0.02 (highest in the experimental set).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>High (ε=0.02)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Low variation (single fixed accuracy during training).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Evaluation success rate on ε=0.02 (proportion of successful episodes during periodic evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: baseline learns more slowly and shows higher variance on the ε=0.02 evaluation than RANDOM- and ACTIVE-; exact numeric values not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicit: low variation with high complexity yields worse sample-efficiency and higher variance than variable-accuracy training strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Baseline (see performance_value) — qualitatively worse than curricula conditions; no precise numeric value in text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training (no curriculum) on the hardest task</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not applicable (trained and evaluated on same ε).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower than RANDOM- and ACTIVE- (qualitatively), slower early improvement per plotted learning curves; exact numbers not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as control demonstrating that training only on hardest accuracy gives poorer learning speed and higher variance than training with varied accuracies or competence-progress curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accuracy-based Curriculum Learning in Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hindsight experience replay <em>(Rating: 2)</em></li>
                <li>R-IAC: Robust intrinsically motivated exploration and active learning <em>(Rating: 2)</em></li>
                <li>Reverse curriculum generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Intrinsically motivated goal exploration processes with automatic curriculum learning <em>(Rating: 2)</em></li>
                <li>Automated curriculum learning for neural networks <em>(Rating: 2)</em></li>
                <li>Active learning of inverse models with intrinsically motivated goal exploration in robots <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1008",
    "paper_id": "paper-49429853",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "DDPG+UVFA (Reacher)",
            "name_full": "Deep Deterministic Policy Gradient agent with Universal Value Function Approximator",
            "brief_description": "A continuous-control deep RL agent (DDPG) augmented with a UVFA input that includes the target accuracy parameter; trained in the 2-DOF Reacher environment using sparse success rewards and curriculum strategies over required accuracy.",
            "citation_title": "Accuracy-based Curriculum Learning in Deep Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "DDPG + UVFA agent",
            "agent_description": "Deep reinforcement learning agent using the DDPG algorithm (actor-critic deterministic policy gradients) with a Universal Value Function Approximator that takes state and an accuracy requirement ε as input; trained with sparse binary success rewards (success if end-effector within ε of goal). Competence-progress based sampling and random sampling over accuracy levels were evaluated as curriculum strategies.",
            "agent_type": "simulated agent",
            "environment_name": "Reacher (OpenAI Gym)",
            "environment_description": "A 2-degree-of-freedom planar robotic arm must move its end-effector to randomly sampled target positions; state includes arm angles, angular velocities, end-effector position and target position. Difficulty is varied by an accuracy threshold ε used in the sparse reward: reward = 0 if |p_finger − p_target| ≤ ε else −1. New target is sampled each trial, providing intrinsic exploration.",
            "complexity_measure": "Task difficulty is defined by required accuracy ε (smaller ε = higher difficulty). Additional operational measures reported: number of training steps, evaluations every 1000 steps (10 episodes × 50 steps per evaluation), and competence progress computed over sliding windows of 2N evaluations (N=3, windows correspond to 3000 environment steps).",
            "complexity_level": "Varies across ε ∈ {0.02, 0.03, 0.04, 0.05}; highest complexity = ε=0.02 (most demanding), lowest complexity = ε=0.05 (easiest). Overall environment complexity is low-to-moderate (simple 2-DOF dynamics) but task difficulty is modulated by ε.",
            "variation_measure": "Variation is the set and sampling distribution of accuracy requirements E = {0.02,0.03,0.04,0.05}; strategies: single fixed ε (baseline), RANDOM- (uniform sampling from E each episode), and ACTIVE- (sampling proportional to measured competence progress per ε). Sampling proportions over training are tracked and reported.",
            "variation_level": "Medium (four discrete accuracy variants sampled during training); variation temporally nonstationary under ACTIVE- (sampling distribution evolves from favoring easy ε to favoring hard ε as competence progresses).",
            "performance_metric": "Success rate on evaluation episodes (proportion of successful episodes) using ε=0.02; additionally competence score per ε measured as success rate over 10 test targets during periodic evaluations.",
            "performance_value": "Qualitative: RANDOM- (uniform sampling over ε) achieves higher success rate and lower run-to-run variance on ε=0.02 evaluation than the baseline trained only on ε=0.02; ACTIVE- (competence-progress sampling, β=4) achieves faster early learning, higher final success, and lower variability than RANDOM-. Exact numeric success rates are reported in figures but not tabulated in text; evaluations: 10 episodes × 50 steps every 1000 training steps, averaged over 10 independent runs.",
            "complexity_variation_relationship": "Yes — the paper explicitly studies and documents the relationship: easier (larger ε) tasks yield faster early competence progress but plateau once mastered, while harder (smaller ε) tasks remain challenging and show sustained competence progress later. ACTIVE- sampling exploits this by prioritizing accuracies with high competence progress, producing an emergent curriculum that moves from low complexity (easy, large ε) to high complexity (hard, small ε). The authors also note a structural property: trajectories that satisfy a tight accuracy also satisfy looser accuracies, enabling transfer from easy to hard via generalization when ε is included in the state.",
            "high_complexity_low_variation_performance": "Baseline: agent trained only on the high-complexity condition (ε=0.02) — performance is worse (slower learning, higher variance) than RANDOM- and ACTIVE- on the ε=0.02 evaluation; no exact numeric value provided in text.",
            "low_complexity_high_variation_performance": "Not separately reported as an isolated test condition; training that includes many easy tasks (large ε) within RANDOM- or ACTIVE- led to faster early competence on those easy ε values, but specific success rates for a high-variation low-complexity-only regime are not provided.",
            "high_complexity_high_variation_performance": "ACTIVE- (curriculum adapting sampling across ε including the high-complexity ε=0.02) yields the best observed performance on the evaluation ε=0.02 (faster initial learning, higher final accuracy, lower variance) compared to RANDOM- and baseline; numeric values are plotted but not explicitly listed in text.",
            "low_complexity_low_variation_performance": "Not explicitly tested as a single condition in reported experiments (the baseline is low-variation but high-complexity; RANDOM- is higher-variation), so not available.",
            "training_strategy": "Curriculum learning: three strategies compared — (1) baseline single-task training on hardest ε (no curriculum), (2) RANDOM- uniform sampling across accuracy variants (multi-condition training / variation), (3) ACTIVE- competence-progress driven sampling (automated curriculum that prioritizes accuracies yielding maximal competence progress).",
            "generalization_tested": true,
            "generalization_results": "Yes — agents trained with mixed easier accuracies (RANDOM- and especially ACTIVE-) generalize to better performance on the hard evaluation (ε=0.02) than an agent trained only on that hard accuracy, despite receiving less experience at ε=0.02 during training. The paper attributes this to transfer/generalization enabled by including ε in the UVFA input and to the nested property that meeting a tight accuracy implies meeting looser accuracies.",
            "sample_efficiency": "Improved sample efficiency observed: RANDOM- and ACTIVE- show faster early improvement than baseline; notable temporal detail: easier accuracies produce quick early progress and plateau by ≈150k training steps, after which sampling shifts toward harder ε; competence progress windows correspond to 3000 steps (N=3) and evaluations every 1000 steps (10 episodes of 50 steps). Exact counts of total training steps per run are not explicitly tabulated in text, but figure timelines show key transitions around 150,000 steps.",
            "key_findings": "1) Training on a mixture of accuracy requirements improves learning efficiency and reduces variance on the hardest evaluation accuracy compared to training only on the hardest requirement. 2) Competence-progress driven sampling (ACTIVE-) automatically generates a curriculum that orders accuracies from easy to hard, prioritizing easy tasks early (large ε) and shifting to hard tasks (small ε) later; this yields better performance than uniform random sampling across accuracies. 3) Easier tasks produce faster early competence progress but plateau, whereas harder tasks yield sustained progress later — leveraging this temporal structure via competence-progress measures improves overall learning. 4) The nested property of accuracy-based difficulty (success at small ε implies success at larger ε) facilitates transfer; this property may not hold for other notions of difficulty.",
            "additional_notes": "Competence progress cp per ε is computed as the absolute difference of average competence over two consecutive sliding windows of 3 evaluations (N=3), encouraging sampling when progress (including negative progress due to forgetting) is large; ACTIVE- uses a softmax-like prioritization P(i) ∝ cp_i^β with β=4 found effective.",
            "uuid": "e1008.0",
            "source_info": {
                "paper_title": "Accuracy-based Curriculum Learning in Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Reacher-baseline (mention)",
            "name_full": "Baseline DDPG agent trained only on ε = 0.02",
            "brief_description": "Baseline condition in this work: the DDPG+UVFA agent is always trained with the hardest accuracy requirement (ε=0.02) and evaluated on that same accuracy; used to compare curricula and variation strategies.",
            "citation_title": "Accuracy-based Curriculum Learning in Deep Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "Baseline DDPG (ε=0.02)",
            "agent_description": "Same DDPG+UVFA architecture but trained exclusively on the most demanding accuracy requirement (ε=0.02); serves as a no-curriculum control.",
            "agent_type": "simulated agent",
            "environment_name": "Reacher (OpenAI Gym), ε=0.02",
            "environment_description": "Same 2-DOF Reacher environment with sparse binary success reward using the tightest accuracy threshold ε=0.02, making reward rarer and task harder.",
            "complexity_measure": "Complexity set by ε=0.02 (highest in the experimental set).",
            "complexity_level": "High (ε=0.02)",
            "variation_measure": "Low variation (single fixed accuracy during training).",
            "variation_level": "Low",
            "performance_metric": "Evaluation success rate on ε=0.02 (proportion of successful episodes during periodic evaluations).",
            "performance_value": "Qualitative: baseline learns more slowly and shows higher variance on the ε=0.02 evaluation than RANDOM- and ACTIVE-; exact numeric values not enumerated in text.",
            "complexity_variation_relationship": "Implicit: low variation with high complexity yields worse sample-efficiency and higher variance than variable-accuracy training strategies.",
            "high_complexity_low_variation_performance": "Baseline (see performance_value) — qualitatively worse than curricula conditions; no precise numeric value in text.",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment training (no curriculum) on the hardest task",
            "generalization_tested": false,
            "generalization_results": "Not applicable (trained and evaluated on same ε).",
            "sample_efficiency": "Lower than RANDOM- and ACTIVE- (qualitatively), slower early improvement per plotted learning curves; exact numbers not provided in text.",
            "key_findings": "Serves as control demonstrating that training only on hardest accuracy gives poorer learning speed and higher variance than training with varied accuracies or competence-progress curricula.",
            "uuid": "e1008.1",
            "source_info": {
                "paper_title": "Accuracy-based Curriculum Learning in Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hindsight experience replay",
            "rating": 2,
            "sanitized_title": "hindsight_experience_replay"
        },
        {
            "paper_title": "R-IAC: Robust intrinsically motivated exploration and active learning",
            "rating": 2,
            "sanitized_title": "riac_robust_intrinsically_motivated_exploration_and_active_learning"
        },
        {
            "paper_title": "Reverse curriculum generation for reinforcement learning",
            "rating": 2,
            "sanitized_title": "reverse_curriculum_generation_for_reinforcement_learning"
        },
        {
            "paper_title": "Intrinsically motivated goal exploration processes with automatic curriculum learning",
            "rating": 2,
            "sanitized_title": "intrinsically_motivated_goal_exploration_processes_with_automatic_curriculum_learning"
        },
        {
            "paper_title": "Automated curriculum learning for neural networks",
            "rating": 2,
            "sanitized_title": "automated_curriculum_learning_for_neural_networks"
        },
        {
            "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "rating": 1,
            "sanitized_title": "active_learning_of_inverse_models_with_intrinsically_motivated_goal_exploration_in_robots"
        }
    ],
    "cost": 0.0100605,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Accuracy-based Curriculum Learning in Deep Reinforcement Learning</p>
<p>Pierre Fournier 
Mohamed Chetouani 
Pierre-Yves Oudeyer 
Olivier Sigaud 
Accuracy-based Curriculum Learning in Deep Reinforcement Learning</p>
<p>In this paper, we investigate a new form of automated curriculum learning based on adaptive selection of accuracy requirements, called accuracybased curriculum learning. Using a reinforcement learning agent based on the Deep Deterministic Policy Gradient algorithm and addressing the Reacher environment, we first show that an agent trained with various accuracy requirements sampled randomly learns more efficiently than when asked to be very accurate at all times. Then we show that adaptive selection of accuracy requirements, based on a local measure of competence progress, automatically generates a curriculum where difficulty progressively increases, resulting in a better learning efficiency than sampling randomly.</p>
<p>Introduction</p>
<p>When an agent has to learn to achieve a set of tasks, the curriculum learning problem can be defined as the problem of finding the most efficient sequence of learning situations in these various tasks so as to maximize its learning speed, either over the whole set of tasks or with respect to one of these tasks.</p>
<p>A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through transfer learning. However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum. Also, various task sets, as well as various learners, may differ in terms of which are the best learning curriculum. An important scientific challenge is thus how to design algorithms that can incrementally and online generate a curriculum that is efficient for a specific set of tasks and a specific learner.</p>
<p>An idea that has been explored in various strands of the 1 Sorbonne Université, ISIR, Paris, France 2 INRIA, Flowers Team, Bordeaux, France. Correspondence to: Pierre Fournier <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#105;&#101;&#114;&#114;&#101;&#46;&#102;&#111;&#117;&#114;&#110;&#105;&#101;&#114;&#64;&#105;&#115;&#105;&#114;&#46;&#117;&#112;&#109;&#99;&#46;&#102;&#114;">&#112;&#105;&#101;&#114;&#114;&#101;&#46;&#102;&#111;&#117;&#114;&#110;&#105;&#101;&#114;&#64;&#105;&#115;&#105;&#114;&#46;&#117;&#112;&#109;&#99;&#46;&#102;&#114;</a>.</p>
<p>literature (Schmidhuber, 1991;Oudeyer et al., 2007) has been to generate a learning curriculum by dynamically selecting learning situations which provide maximal learning progress at a given point in time. This idea has been used to automate the generation of learning curriculum for training robots , deep neural networks (Graves et al., 2017), as well as human learners in educational settings (Clement et al., 2013). It entails several challenges, including how to efficiently estimate expected learning progress, and how to optimize exploration and exploitation to maximize learning progress with bandit-like algorithms.</p>
<p>Another important challenge entailed by this approach is how to parameterize learning situations, i.e. to design the parameters over which learning progress is estimated and compared. In the case of an agent learning to reach various goals in the environment, the general notion of learning progress which might for instance denote progress in predicting some signal is replaced by a narrower notion of competence progress, which is more specific to learning to act. In the case of reaching, a natural parameterization consists in defining learning situations as particular regions of goal states (e.g. ), or particular regions of starting states (e.g. (Florensa et al., 2017)), or particular combinations of starting and end states. As some target or starting states might be easier to learn than others, thus producing higher competence progress in the beginning, a strategy based on competence progress will first focus on them and then move towards more complicated ones. If this parameterization is continuous, architectures like SAGG-RIAC  can be used to dynamically and incrementally learn these regions, and concurrently use them to select and order learning situations.</p>
<p>In this paper, we focus on another way to parameterize learning situations based on the notion of accuracy requirement of the tasks/goals. Many robotics tasks can be made more or less difficult by requiring different degrees of accuracy from the robot: learning to bring its end-effector within 10cm of a point may be easier than within 10mm. A task which was easy with loose accuracy requirements can become difficult if the accuracy constraint becomes tighter. The impact of accuracy requirement on learning efficiency in the context of curriculum learning can be particularly powerful, since in reinforcement learning (RL) progress is made by finding arXiv:1806.09614v2 [cs.</p>
<p>LG] 21 Sep 2018 rewarding experiences, and it is harder to find a source of reward when the accuracy requirement is stronger.</p>
<p>To capture this idea, we consider the required accuracy as a parameter of learning situations, and we define accuracybased curriculum learning as a specific form of curriculum learning which acts on the value of to improve learning efficiency. Then we study the impact of accuracy-based curriculum learning on the learning efficiency of a multigoal deep RL agent trying to learn various reaching tasks.</p>
<p>Our contributions are the following. First, we show that being trained with various accuracy requirements is beneficial to learning efficiency even when the required accuracies are sampled in a random order. Then we show that using accuracy-based curriculum learning based on the competence progress as defined in  enables to automate the sampling of accuracy constraints in order of increasing difficulty, and that such ordering results in a better learning efficiency than random sampling.</p>
<p>The paper is organized as follows. In Section 2, we present some works that are related to our main concerns. In Section 3, we quickly describe the Reacher environment used as experimental setup, the deep RL algorithm, Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015) and its Universal Value Function Approximation (UVFA) extension, as well as the accuracy-based curriculum learning algorithm we are using. We describe our results in Section 4. Finally, we discuss these results, conclude and describe potential avenues for future work in Section 5.</p>
<p>Related work</p>
<p>Our work contributes to the domain of curriculum learning, leveraging contributions made in different fields.</p>
<p>One line of research in machine learning (Schmidhuber, 1991) has proposed that measures of improvement of prediction errors could be used as intrinsic rewards to be maximized, generating a learning curriculum in an intrinsically motivated RL framework (Chentanez et al., 2005). It was later on extended to drive the selection of problems in the Powerplay framework (Schmidhuber, 2013).</p>
<p>Another independent line of research, in the domain of developmental robotics, has focused on modeling children's curiosity-driven exploration as a mechanism that enables them to learn world models through the adaptive selection of learning situations that maximize learning progress (Oudeyer et al., 2007). This line of research has developed algorithms enabling learning progress to be efficiently estimated in large continuous high-dimensional spaces (Baranes and Oudeyer, 2009), enabling robots to self-organize a learning curriculum and learn incrementally and online complex skills such as locomotion , tool use (Forestier and Oudeyer, 2016) or soft object manipulation (Nguyen and Oudeyer, 2014). This line of research also explored a variety of ways in which learning progress could be used to select learning situations, including the automated selection of policy parameters (Oudeyer et al., 2007), goals , models (Forestier and Oudeyer, 2016), and learning strategies (Nguyen and Oudeyer, 2012). While some of these lines of research have also considered the possibility to progressively increase forms of perceptual accuracy based on learning progress (e.g. the McSAGG-RIAC architecture, ), they have not considered the possibility to dynamically switch between different accuracy requirements levels as we study in this article.</p>
<p>These ideas were then introduced together in the context of deep neural network research in (Bengio et al., 2009), which popularized the term "curriculum learning". They are now the focus of intensive research efforts both in developmental robotics (Forestier and Oudeyer, 2016;Pere et al., 2018) and deep learning research (Graves et al., 2017;Matiisen et al., 2017;Florensa et al., 2017), and are key elements of recent curiosity-driven deep reinforcement learning techniques enabling to solve RL problems with rare or deceptive rewards, e.g. (Bellemare et al., 2016;Machado et al., 2017). They are also the focus of some theoretical work (Weinshall and Cohen, 2018), for example to characterize the families of task sets for which the learning progress mechanism generates an optimal curriculum (Lopes and Oudeyer, 2012).</p>
<p>In this context, our work follows closely the multi-goal architecture from the Hindsight Experience Replay (HER) algorithm (Andrychowicz et al., 2017). The key innovation in HER consists in letting the agent learn again from previous experience by replaying real transitions of an episode with different goals reached during that episode. The algorithm is built on a combination of Universal Value Function Approximators (UVFA) (Schaul et al., 2015) and the DDPG (Lillicrap et al., 2015) deep RL algorithm. It is shown to provide the agent with a form of implicit curriculum learning based on target goal positions. The present work is also built on UVFAs and DDPG but unlike HER, it relies on a principled idea of competence progress to provide an explicit curriculum for the agent. To our knowledge, our work is the first to combine UVFAs, DDPG and an explicit measure of competence progress.</p>
<p>Methods</p>
<p>The idea of building a curriculum of tasks on the base of various accuracy requirements can be applied to many environments and learning algorithms, provided that the latter can incorporate such requirements. In this paper we apply it to solve the simple Reacher environment from OpenAI Gym (Brockman et al., 2016) with the DDPG algorithm.</p>
<p>Reacher</p>
<p>The reacher environment is a two degree of freedom arm that must reach a target in its plane. In this environment, the agent state contains the arm angles and angular velocities, its end-effector position p f inger and the target position p target . We use a sparse reward function that happens to naturally incorporate a notion of accuracy:
r = 0, if |p f inger − p target | ≤ −1, otherwise(1)
with the parameter determining the accuracy required from the agent for being successful.</p>
<p>Importantly, a new target position p target is randomly sampled after each trial, providing a natural form of exploration.</p>
<p>DDPG</p>
<p>The DDPG algorithm (Lillicrap et al., 2015) is a deep RL algorithm based on the Deterministic Policy Gradient (Silver et al., 2014). It borrows the use of a replay buffer and a target network from DQN (Mnih et al., 2015).</p>
<p>The implementation of DDPG used in this work differs from the original one by several aspects. First, we use the gradient inversion technique from (Hausknecht and Stone, 2015) to better deal with the bounded action space and to avoid saturating the final tanh layer of the actor. Second, we use target clipping from (Andrychowicz et al., 2017) to mitigate target critic overestimation.</p>
<p>All the other parameters of DDPG used in this paper are taken from baselines (Dhariwal et al., 2017) used in systematic studies Henderson et al., 2017): we use a minibatch of size 64, a replay buffer of size 1e6, the critic and actor learning rates are respectively 0.001 and 0.0001, τ = 0.001 and γ = 0.99. Both critic and actor networks have two hidden layers of 400 and 300 neurons with ReLU activations and are densely connected. They are trained with the Adam optimizer.</p>
<p>In the experiments below, we do not use any exploration noise, as we have determined that such exploration noise is detrimental to performance in the Reacher environment, due to the intrinsic exploration provided by the environment itself.</p>
<p>Universal Value Function Approximators</p>
<p>In the Reacher environment, the state contains both the robot position p f inger and the target position p target and the target position is sampled randomly, making it natural to consider DDPG as implementing a form of Universal Value Function Approximator (UVFA) (Schaul et al., 2015).</p>
<p>However, we use a slightly more complex representation than just this state. Indeed, in the standard setting for RL with sparse rewards, the parameter is set at the beginning of training to a user-defined value and left untouched afterwards, whereas here changes from one training episode to the next. But changing during the reward/termination calculation without adding its value to the state would remove the MDP structure of the problem: a given state could be either terminal or not depending on , leading to incoherent updates for DDPG. Thus we extend the UVFA framework to contain both states and accuracies using a V (s, , θ) representation. The MDP structure of the environment is not changed by this addition: simply keeps constant whatever the transition, and is only used to compute the termination condition.</p>
<p>Epsilon-based curriculum</p>
<p>Two training strategies are studied in this work. The first, called RANDOM-, consists in sampling a training accuracy uniformly from E = {0.02, 0.03, 0.04, 0.05} at the beginning of each episode, and adding it to the input state of the UVFA for all the duration of the episode.</p>
<p>The second, called ACTIVE-, consists in measuring the agent competence progress for each ∈ E all along training, and sampling an accuracy at the beginning of each episode proportionally to its current competence progress measurement. Specifically, if cp i is the competence progress for i , then we define the probability of sampling i as
P ( i ) = cp β i k cp β k .
(2)</p>
<p>The exponent β determines how much prioritization is used, with β = 0 corresponding to the uniform sampling case (RANDOM-), and β = ∞ to only sampling the accuracy value resulting in the maximum competence progress. In the experiments below, we determined by sampling β in the range [0, 8] that using a value of β = 4 was providing the best results.</p>
<p>Both strategies are compared to a baseline that corresponds to the DDPG algorithm being trained to reach target positions with rewards obtained from the hard accuracy = 0.02. Thus the baseline is always trained with accuracy constraints that correspond to that used for evaluation. By contrast, both ACTIVE-and RANDOM-are designed to sometimes train on easier requirements, and thus train less with the evaluation accuracy constraint.</p>
<p>Competence progress measurement</p>
<p>The way we measure learning progress is directly taken from the SAGG-RIAC algorithm .</p>
<p>For each possible value of ∈ E, the agent is tested every 1000 steps on its ability to reach 10 randomly sampled target positions with this accuracy, providing a competence score between 0 and 1. From the ordered list of scores obtained this way during training, we extract a measure of competence progress following : if c i k is the score measured for i at measurement time t k , then the competence progress cp i at time T is the discrete absolute derivative of competence scores for i over a sliding window of the 2N more recent evaluations:
cp i =   T j=T −N c i j   −   T −N j=T −2N c i j   2N
.</p>
<p>(</p>
<p>The use of an absolute value guarantees that if competence starts dropping for a given , then its associated progress will be negative with high absolute value and thus will be sampled in priority over other slowly progressing accuracies. This avoids catastrophic forgetting. For all experiments, N = 3 is used to compute progress on sliding windows containing 3000 steps.</p>
<p>Results</p>
<p>In this Section we validate the hypotheses that: (1) sampling randomly from E is more efficient for learning high accuracy behaviors than always using the strongest accuracy requirement ( = 0.02), and this despite seeing less experience with such a high accuracy; (2) sampling using competence progress on each accuracy level results in ordering accuracy requirements from the easiest to the most difficult;</p>
<p>(3) the resulting curriculum improves learning efficiency over random selection. Figure 1 shows the benefits from training with multiple accuracies as well as that of prioritizing values of depending on the level of competence progress of the agent. Every 1000 steps, the evaluation consists in running the agent for ten 50-step episodes on randomly sampled target positions, and recording the proportion of successful episodes for = 0.02.</p>
<p>The RANDOM-strategy already provides a clear gain over the baseline: one can observe a significant increase in accuracy as well as a reduction of the variance across different runs. The ACTIVE-strategy is shown with β = 4. The resulting curriculum provides even better learning performance than RANDOM-: progress is faster at the beginning of the learning curve, the agent is more accurate in the end and shows less variability. on Fig. 2 that lower precisions lead to quicker progress at the beginning of learning compared to the most demanding task with = 0.02. After about 150K steps, the opposite trend is observed: the agent competence on low precisions starts to reach a plateau as it masters the reaching task with these accuracies, leading to a decrease in progress; instead high accuracy remains challenging and the associated competence progress stays higher until later during training.</p>
<p>In parallel, Fig. 3 shows the proportions of all sampled represented by each specific value of , and reflects how their sampling frequency changes with training. During the first 150K steps priority is given to low accuracy objectives with high competence progresses, and the situation is reverted afterwards, with the agent almost only sampling the strongest accuracy requirement in the end, for which it is still making progress.</p>
<p>Discussion and conclusion</p>
<p>We have shown that simply training on multiple accuracies and thus seeing less experience with = 0.02 in favor of larger values provides a substantial gain over the baseline. This suggests that the agent is able to take advantage of rewarding experience acquired with large values and then properly generalize the policy learned from these values to smaller ones. This idea is independent of the Reacher environment and could be leveraged to many simulated environments such as the Fetch environment from OpenAI (Brockman et al., 2016) or even to real robots. However, this result may depend from a specific feature of accuracy-based difficulty, which is that a trajectory fulfilling some accuracy requirement also fulfills any easier accuracy requirement. Difficulties in terms of final points do not share a similar structure, for instance.</p>
<p>Also we demonstrated that competence progress is an effective metric to build a curriculum in the context of deep reinforcement learning, and could be used with groups of tasks that differ by other aspects than their accuracy requirements as it is the case here. Specifically, one could apply this methodology to extract a curriculum from tasks corresponding to reaching distinct regions for a robotic arm, or from tasks distinct in nature and difficulty inside a more complex environment.</p>
<p>The idea of incorporating the final accuracy of the arm movement in the reward scheme of the agent reminds of dense reward functions used in standard versions of Reacherlike environments. In these cases, the shaped reward is proportional to the Euclidean distance between the extremity of the arm and the target position. Yet, we have no guarantee that two target positions close in the Euclidean space can be reached with close policies for complex robotic agents, and thus such shaping could be misleading. On the contrary, in our case, there is no assumption that the image of the control space in the goal space is Euclidean as the generalization from one accuracy to another is learned by the agent through the addition of to the input state of UVFAs. In the future, it would be useful to compare the efficiency of the curriculum obtained from our method with that of a method where a fixed increasing diffulty curriculum is used (see e.g. (Clement et al., 2013;Forestier et al., 2017)). Besides, we intend to compare the explicit form of curriculum based on competence progress used here and the implicit form of curriculum resulting from the HER mechanism.</p>
<p>Figures 2 and 3 Figure 1 .
31focus on the curriculum obtained with β = 4 and parallels the evolution of competence progresses for each value of with their sampling frequencies. We observe Average observed accuracy of the agent on 10 randomly sampled target positions across the environment with a required accuracy of 0.02 versus number of steps taken, averaged over 10 independent runs. The RANDOM-and ACTIVE-(with β = 4) strategies are compared to the baseline algorithm alone. Colored areas cover one standard deviation around the mean of the 10 runs.</p>
<p>Figure 2 .
2Evolution of competence progress for each ∈ E = {0.02, 0.03, 0.04, 0.05} with training steps, averaged over 10 runs, for the ACTIVE-strategy with β = 4.</p>
<p>Figure 3 .
3Proportions of all sampled during training represented by each specific value in E, versus number of steps taken, averaged over 10 runs, for the ACTIVE-strategy with β = 4.
AcknowledgmentsThis work was supported by the European Commission, within the DREAM project, and has received funding from the European Unions Horizon 2020 research and innovation program under grant agreement N o 640891.
M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, arXiv:1707.01495Hindsight experience replay. arXiv preprintAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, P. and Zaremba, W. (2017), 'Hindsight experience replay', arXiv preprint arXiv:1707.01495 .</p>
<p>R-IAC: Robust intrinsically motivated exploration and active learning. A Baranes, P.-Y Oudeyer, IEEE Transactions on Autonomous Mental Development. 13Baranes, A. and Oudeyer, P.-Y. (2009), 'R-IAC: Robust intrinsically motivated exploration and active learning', IEEE Transactions on Autonomous Mental Development 1(3), 155-169.</p>
<p>Active learning of inverse models with intrinsically motivated goal exploration in robots. A Baranes, P.-Y Oudeyer, Robotics and Autonomous Systems. 611Baranes, A. and Oudeyer, P.-Y. (2013), 'Active learning of inverse models with intrinsically motivated goal ex- ploration in robots', Robotics and Autonomous Systems 61(1), 49-73.</p>
<p>Accuracy-based Curriculum Learning in deep RL. Accuracy-based Curriculum Learning in deep RL</p>
<p>Unifying count-based exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems. Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D. and Munos, R. (2016), Unifying count-based exploration and intrinsic motivation, in 'Advances in Neu- ral Information Processing Systems', pp. 1471-1479.</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningACMBengio, Y., Louradour, J., Collobert, R. and Weston, J. (2009), Curriculum learning, in 'Proceedings of the 26th annual international conference on machine learning', ACM, pp. 41-48.</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540Openai gym. arXiv preprintBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J. and Zaremba, W. (2016), 'Openai gym', arXiv preprint arXiv:1606.01540 .</p>
<p>Intrinsically motivated reinforcement learning, in 'Advances in neural information processing systems. N Chentanez, A G Barto, S P Singh, Chentanez, N., Barto, A. G. and Singh, S. P. (2005), Intrin- sically motivated reinforcement learning, in 'Advances in neural information processing systems', pp. 1281-1288.</p>
<p>Multi-armed bandits for intelligent tutoring systems. B Clement, D Roy, P.-Y Oudeyer, M Lopes, Journal of Educational Data Mining. 72Clement, B., Roy, D., Oudeyer, P.-Y. and Lopes, M. (2013), 'Multi-armed bandits for intelligent tutoring systems', Journal of Educational Data Mining 7(2).</p>
<p>. P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, OpenAI baselinesDhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor, S. and Wu, Y. (2017), 'OpenAI baselines', https://github.com/openai/baselines.</p>
<p>Reverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, P Abbeel, arXiv:1707.05300arXiv preprintFlorensa, C., Held, D., Wulfmeier, M. and Abbeel, P. (2017), 'Reverse curriculum generation for reinforcement learn- ing', arXiv preprint arXiv:1707.05300 .</p>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. S Forestier, Y Mollard, P.-Y Oudeyer, arXiv:1708.02190arXiv preprintForestier, S., Mollard, Y. and Oudeyer, P.-Y. (2017), 'Intrinsically motivated goal exploration processes with automatic curriculum learning', arXiv preprint arXiv:1708.02190 .</p>
<p>Curiosity-driven development of tool use precursors: a computational model. S Forestier, P.-Y Oudeyer, 38th Annual Conference of the Cognitive Science Society. Forestier, S. and Oudeyer, P.-Y. (2016), Curiosity-driven de- velopment of tool use precursors: a computational model, in '38th Annual Conference of the Cognitive Science Society (CogSci 2016)', pp. 1859-1864.</p>
<p>Automated curriculum learning for neural networks. A Graves, M G Bellemare, J Menick, R Munos, K Kavukcuoglu, arXiv:1704.03003arXiv preprintGraves, A., Bellemare, M. G., Menick, J., Munos, R. and Kavukcuoglu, K. (2017), 'Automated curriculum learning for neural networks', arXiv preprint arXiv:1704.03003 .</p>
<p>Deep reinforcement learning in parameterized action space. M Hausknecht, P Stone, arXiv:1511.04143arXiv preprintHausknecht, M. and Stone, P. (2015), 'Deep reinforcement learning in parameterized action space', arXiv preprint arXiv:1511.04143 .</p>
<p>Deep reinforcement learning that matters. P Henderson, R Islam, P Bachman, J Pineau, D Precup, D Meger, arXiv:1709.06560arXiv preprintHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D. and Meger, D. (2017), 'Deep reinforcement learning that matters', arXiv preprint arXiv:1709.06560 .</p>
<p>Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. R Islam, P Henderson, M Gomrokchi, D Precup, Proceedings of the ICML 2017 workshop on Reproducibility in Machine Learning (RML). the ICML 2017 workshop on Reproducibility in Machine Learning (RML)Islam, R., Henderson, P., Gomrokchi, M. and Precup, D. (2017), Reproducibility of benchmarked deep reinforce- ment learning tasks for continuous control, in 'Proceed- ings of the ICML 2017 workshop on Reproducibility in Machine Learning (RML)'.</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D. (2015), 'Continuous control with deep reinforcement learning', arXiv preprint arXiv:1509.02971 .</p>
<p>The strategic student approach for life-long exploration and learning. M Lopes, P.-Y Oudeyer, 'IEEE International Conference on Development and Learning and Epigenetic Robotics. IEEELopes, M. and Oudeyer, P.-Y. (2012), The strategic student approach for life-long exploration and learning, in 'IEEE International Conference on Development and Learning and Epigenetic Robotics', IEEE, pp. 1-8.</p>
<p>A Laplacian framework for option discovery in reinforcement learning. M C Machado, M G Bellemare, M Bowling, arXiv:1703.00956arXiv preprintMachado, M. C., Bellemare, M. G. and Bowling, M. (2017), 'A Laplacian framework for option discovery in reinforce- ment learning', arXiv preprint arXiv:1703.00956 .</p>
<p>Teacher-student curriculum learning. T Matiisen, A Oliver, T Cohen, J Schulman, arXiv:1707.00183arXiv preprintMatiisen, T., Oliver, A., Cohen, T. and Schulman, J. (2017), 'Teacher-student curriculum learning', arXiv preprint arXiv:1707.00183 .</p>
<p>Humanlevel control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve- ness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G. et al. (2015), 'Human- level control through deep reinforcement learning', Na- ture 518(7540), 529-533.</p>
<p>Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. S M Nguyen, P.-Y Oudeyer, Paladyn. 33Nguyen, S. M. and Oudeyer, P.-Y. (2012), 'Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner', Paladyn 3(3), 136- 146.</p>
<p>Socially guided intrinsic motivation for robot learning of motor skills. S M Nguyen, P.-Y Oudeyer, Autonomous Robots. 363Nguyen, S. M. and Oudeyer, P.-Y. (2014), 'Socially guided intrinsic motivation for robot learning of motor skills', Autonomous Robots 36(3), 273-294.</p>
<p>Intrinsically motivated learning of real-world sensorimotor skills with developmental constraints, in 'Intrinsically motivated learning in natural and artificial systems. P.-Y Oudeyer, A Baranes, F Kaplan, SpringerOudeyer, P.-Y., Baranes, A. and Kaplan, F. (2013), Intrinsi- cally motivated learning of real-world sensorimotor skills with developmental constraints, in 'Intrinsically moti- vated learning in natural and artificial systems', Springer, pp. 303-365.</p>
<p>Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE Transactions on Evolutionary Computation. 112Oudeyer, P.-Y., Kaplan, F. and Hafner, V. V. (2007), 'Intrin- sic motivation systems for autonomous mental develop- ment', IEEE Transactions on Evolutionary Computation 11(2), 265-286.</p>
<p>Unsupervised learning of goal spaces for intrinsically motivated goal exploration. A Pere, S Forestier, O Sigaud, P.-Y Oudeyer, 'International Conference on Learning Representations (ICLR. Pere, A., Forestier, S., Sigaud, O. and Oudeyer, P.-Y. (2018), Unsupervised learning of goal spaces for intrinsically motivated goal exploration, in 'International Conference on Learning Representations (ICLR)'.</p>
<p>Universal value function approximators. T Schaul, D Horgan, K Gregor, D Silver, 'International Conference on Machine Learning. Schaul, T., Horgan, D., Gregor, K. and Silver, D. (2015), Universal value function approximators, in 'International Conference on Machine Learning', pp. 1312-1320.</p>
<p>Curious model-building control systems. J Schmidhuber, IEEE International Joint Conference on Neural Networks. IEEESchmidhuber, J. (1991), Curious model-building control sys- tems, in 'IEEE International Joint Conference on Neural Networks', IEEE, pp. 1458-1463.</p>
<p>Accuracy-based Curriculum Learning in deep RL. Accuracy-based Curriculum Learning in deep RL</p>
<p>Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. J Schmidhuber, 4313Schmidhuber, J. (2013), 'Powerplay: Training an increas- ingly general problem solver by continually searching for the simplest still unsolvable problem', Frontiers in psychology 4, 313.</p>
<p>Deterministic policy gradient algorithms. D Silver, G Lever, N Heess, T Degris, D Wierstra, M Riedmiller, Proceedings of the 30th International Conference in Machine Learning. the 30th International Conference in Machine LearningSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D. and Riedmiller, M. (2014), Deterministic policy gradi- ent algorithms, in 'Proceedings of the 30th International Conference in Machine Learning'.</p>
<p>Curriculum learning by transfer learning: Theory and experiments with deep networks. D Weinshall, G Cohen, arXiv:1802.03796arXiv preprintWeinshall, D. and Cohen, G. (2018), 'Curriculum learning by transfer learning: Theory and experiments with deep networks', arXiv preprint arXiv:1802.03796 .</p>            </div>
        </div>

    </div>
</body>
</html>
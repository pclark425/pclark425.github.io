<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8997 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8997</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8997</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-270737791</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.17789v1.pdf" target="_blank">Spanish and LLM Benchmarks: Is MMLU Lost in Translation?</a></p>
                <p><strong>Paper Abstract:</strong> The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics. As LLMs become adopted worldwide, evaluating them in languages other than English is increasingly important. However, most LLM benchmarks are simply translated using an automated tool and then run in the target language. This means that the results depend not only on the LLM performance in that language but also on the quality of the translation. In this paper, we consider the case of the well-known Massive Multitask Language Understanding (MMLU) benchmark. Selected categories of the benchmark are translated into Spanish using Azure Translator and ChatGPT4 and run on ChatGPT4. Next, the results are processed to identify the test items that produce different answers in Spanish and English. Those are then analyzed manually to understand if the automatic translation caused the change. The results show that a significant fraction of the failing items can be attributed to mistakes in the translation of the benchmark. These results make a strong case for improving benchmarks in languages other than English by at least revising the translations of the items and preferably by adapting the tests to the target language by experts.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8997.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8997.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 on MMLU (Spanish translations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 evaluated on selected MMLU benchmark categories (Miscellaneous, Philosophy, US foreign policy) with English and automatically translated Spanish items</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses GPT-4 (ChatGPT4) to answer MMLU multiple-choice questions in English and after automatic translation into Spanish (using Azure Translator and ChatGPT), analyzes items where answers change, and quantifies the performance drop attributable to translation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SPANISH AND LLM BENCHMARKS: IS MMLU LOST IN TRANSLATION? A PREPRINT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (ChatGPT4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art large language model from OpenAI; described in the paper as among the best-performing models on MMLU and used here to answer MMLU multiple-choice items in English and in Spanish after automatic translation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding) — selected categories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MMLU is a large multiple-choice benchmark covering many academic and professional topics. This study uses three MMLU categories: Miscellaneous (783 questions), Philosophy (311 questions), and US foreign policy (100 questions); it assesses factual knowledge, domain knowledge and reasoning across topics rather than classic laboratory cognitive-psychology tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported error rates (from Table 1) and implied accuracies for ChatGPT4: English — Miscellaneous: 47 incorrect (6%) → 94.0% accuracy; Philosophy: 43 incorrect (13.9%) → 86.1% accuracy; US foreign policy: 3 incorrect (3%) → 97.0% accuracy. Spanish (Azure Translator) — Miscellaneous: 65 incorrect (8.3%) → 91.7% accuracy; Philosophy: 66 incorrect (21.29%) → 78.71% accuracy; US foreign policy: 10 incorrect (10%) → 90.0% accuracy. Spanish (ChatGPT translation) — Miscellaneous: 66 incorrect (8.56%) → 91.44% accuracy; Philosophy: 57 incorrect (18.4%) → 81.6% accuracy; US foreign policy: 11 incorrect (11%) → 89.0% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human-baseline data reported in this paper. Within-model comparison shows that GPT-4 performs well on the original English MMLU items but exhibits a measurable performance decline when items are automatically translated to Spanish; the decline is larger in some categories (Philosophy, US foreign policy) and depends on the translation tool used (Azure translations induced more errors than ChatGPT translations in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Procedure: selected three MMLU categories (sizes above), translated items into Spanish with two translators (Azure Translator and ChatGPT4), ran the same MMLU items on GPT-4 in English and on both Spanish translations, logged answers and log probabilities, identified items correct in English but wrong in Spanish, had two experts independently review those items to classify translation errors and manually correct translations, then reran corrected Spanish items on GPT-4 to measure recovery. Dataset size ~1,200 items; translations and results available in public repository referenced in the paper. Prompts and model configuration details are not specified beyond using ChatGPT4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human baseline performance is provided, so direct comparison to human cognition is not possible here. The methodology only inspects items where GPT-4 was correct in English but wrong in Spanish, so translation errors that do not change the LLM's answer are not detected. Some corrected recoveries could be due to randomness in LLM outputs. Study limited to ~1,200 questions and two translation tools (Azure, ChatGPT); results depend on prompts and model configuration which are not fully specified. The MMLU categories are domain/knowledge tests rather than classic cognitive-psychology tests (e.g., Stroop, Raven), so findings pertain to benchmark translation impacts rather than cognitive-process replication. The paper documents categories of translation errors (proper-name mistranslation, technical-term errors, untranslated terms, cultural adaptation issues, meaning changes, grammatical errors) which can distort LLM evaluation in other languages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spanish and LLM Benchmarks: Is MMLU Lost in Translation?', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding. <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report. <em>(Rating: 2)</em></li>
                <li>Evaluating large language models: A comprehensive survey. <em>(Rating: 2)</em></li>
                <li>Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. <em>(Rating: 1)</em></li>
                <li>Hellaswag: Can a machine really finish your sentence?. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8997",
    "paper_id": "paper-270737791",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4 on MMLU (Spanish translations)",
            "name_full": "Generative Pre-trained Transformer 4 evaluated on selected MMLU benchmark categories (Miscellaneous, Philosophy, US foreign policy) with English and automatically translated Spanish items",
            "brief_description": "This paper uses GPT-4 (ChatGPT4) to answer MMLU multiple-choice questions in English and after automatic translation into Spanish (using Azure Translator and ChatGPT), analyzes items where answers change, and quantifies the performance drop attributable to translation errors.",
            "citation_title": "SPANISH AND LLM BENCHMARKS: IS MMLU LOST IN TRANSLATION? A PREPRINT",
            "mention_or_use": "use",
            "model_name": "GPT-4 (ChatGPT4)",
            "model_description": "A state-of-the-art large language model from OpenAI; described in the paper as among the best-performing models on MMLU and used here to answer MMLU multiple-choice items in English and in Spanish after automatic translation.",
            "model_size": null,
            "test_battery_name": "MMLU (Massive Multitask Language Understanding) — selected categories",
            "test_description": "MMLU is a large multiple-choice benchmark covering many academic and professional topics. This study uses three MMLU categories: Miscellaneous (783 questions), Philosophy (311 questions), and US foreign policy (100 questions); it assesses factual knowledge, domain knowledge and reasoning across topics rather than classic laboratory cognitive-psychology tasks.",
            "llm_performance": "Reported error rates (from Table 1) and implied accuracies for ChatGPT4: English — Miscellaneous: 47 incorrect (6%) → 94.0% accuracy; Philosophy: 43 incorrect (13.9%) → 86.1% accuracy; US foreign policy: 3 incorrect (3%) → 97.0% accuracy. Spanish (Azure Translator) — Miscellaneous: 65 incorrect (8.3%) → 91.7% accuracy; Philosophy: 66 incorrect (21.29%) → 78.71% accuracy; US foreign policy: 10 incorrect (10%) → 90.0% accuracy. Spanish (ChatGPT translation) — Miscellaneous: 66 incorrect (8.56%) → 91.44% accuracy; Philosophy: 57 incorrect (18.4%) → 81.6% accuracy; US foreign policy: 11 incorrect (11%) → 89.0% accuracy.",
            "human_baseline_performance": null,
            "performance_comparison": "No human-baseline data reported in this paper. Within-model comparison shows that GPT-4 performs well on the original English MMLU items but exhibits a measurable performance decline when items are automatically translated to Spanish; the decline is larger in some categories (Philosophy, US foreign policy) and depends on the translation tool used (Azure translations induced more errors than ChatGPT translations in this study).",
            "experimental_details": "Procedure: selected three MMLU categories (sizes above), translated items into Spanish with two translators (Azure Translator and ChatGPT4), ran the same MMLU items on GPT-4 in English and on both Spanish translations, logged answers and log probabilities, identified items correct in English but wrong in Spanish, had two experts independently review those items to classify translation errors and manually correct translations, then reran corrected Spanish items on GPT-4 to measure recovery. Dataset size ~1,200 items; translations and results available in public repository referenced in the paper. Prompts and model configuration details are not specified beyond using ChatGPT4.",
            "limitations_or_caveats": "No human baseline performance is provided, so direct comparison to human cognition is not possible here. The methodology only inspects items where GPT-4 was correct in English but wrong in Spanish, so translation errors that do not change the LLM's answer are not detected. Some corrected recoveries could be due to randomness in LLM outputs. Study limited to ~1,200 questions and two translation tools (Azure, ChatGPT); results depend on prompts and model configuration which are not fully specified. The MMLU categories are domain/knowledge tests rather than classic cognitive-psychology tests (e.g., Stroop, Raven), so findings pertain to benchmark translation impacts rather than cognitive-process replication. The paper documents categories of translation errors (proper-name mistranslation, technical-term errors, untranslated terms, cultural adaptation issues, meaning changes, grammatical errors) which can distort LLM evaluation in other languages.",
            "uuid": "e8997.0",
            "source_info": {
                "paper_title": "Spanish and LLM Benchmarks: Is MMLU Lost in Translation?",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding.",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Gpt-4 technical report.",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Evaluating large language models: A comprehensive survey.",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_a_comprehensive_survey"
        },
        {
            "paper_title": "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback.",
            "rating": 1,
            "sanitized_title": "okapi_instructiontuned_large_language_models_in_multiple_languages_with_reinforcement_learning_from_human_feedback"
        },
        {
            "paper_title": "Hellaswag: Can a machine really finish your sentence?.",
            "rating": 1,
            "sanitized_title": "hellaswag_can_a_machine_really_finish_your_sentence"
        }
    ],
    "cost": 0.0082785,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SPANISH AND LLM BENCHMARKS: IS MMLU LOST IN TRANSLATION? A PREPRINT
June 27, 2024</p>
<p>Irene Plaza 
Nina Melero 
Cristina Del Pozo 
Javier Conde 
Pedro Reviriego 
Marina Mayor-Rocher </p>
<p>ETSI de Telecomunicación Universidad Politécnica de Madrid
28040MadridSpain</p>
<p>Facultad de Filosofía y Letras
Universidad Autónoma de Madrid
28049MadridSpain</p>
<p>María Grandury SomosNLP 24402
PonferradaSpain</p>
<p>SPANISH AND LLM BENCHMARKS: IS MMLU LOST IN TRANSLATION? A PREPRINT
June 27, 2024F3E53882B3A67C7D454EEEA86C4F4B82EvaluationBenchmarksSpanish
The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics.As LLMs become adopted worldwide, evaluating them in languages other than English is increasingly important.However, most LLM benchmarks are simply translated using an automated tool and then run in the target language.This means that the results depend not only on the LLM performance in that language but also on the quality of the translation.In this paper, we consider the case of the well-known Massive Multitask Language Understanding (MMLU) benchmark.Selected categories of the benchmark are translated into Spanish using Azure Translator and ChatGPT4 and run on ChatGPT4.Next, the results are processed to identify the test items that produce different answers in Spanish and English.Those are then analyzed manually to understand if the automatic translation caused the change.The results show that a significant fraction of the failing items can be attributed to mistakes in the translation of the benchmark.These results make a strong case for improving benchmarks in languages other than English by at least revising the translations of the items and preferably by adapting the tests to the target language by experts.</p>
<p>Introduction</p>
<p>Large Language Models are becoming a fundamental block in modern computing systems enabling new applications and facilitating the interaction with users Minaee et al. (2024).However, LLMs have limitations and their performance has to be well understood before using them on a given application Zhao et al. (2023).This has motivated the development of a large number of LLM evaluation benchmarks that test the knowledge that models have of many different topics and how well they can perform tasks such as logic reasoning or problem-solving Guo et al. (2023).Most of these benchmarks are designed so that the LLM responses can be processed automatically thus enabling testing at scale with thousands of questions.This is commonly achieved by using multiple-choice tests.</p>
<p>There are LLM benchmarks to evaluate a wide range of tasks and topics.For example, there are tests to evaluate the capabilities of LLMs to solve common sense reasoning problems Zellers et al. (2019) or to answer mathematical questions Hendrycks et al. (2021b).To provide a more comprehensive evaluation, some benchmarks evaluate several tasks, for example, the Multitask Language Understanding (MMLU) test Hendrycks et al. (2021a) evaluates 57 different topics and other benchmarks increase the number of tasks and topics to more than 200 Srivastava et al. (2022).The speed and energy dissipation of LLMs are also important factors that are commonly evaluated in terms of the number of tokens generated per second, the memory used or the energy per token1 , or with more user-centric metrics like the time and energy needed to complete a given task Conde et al. (2024).As LLMs become pervasive and used in almost any domain and application, more benchmarks will be developed each having possibly thousands or even hundreds of thousands of questions.</p>
<p>Another dimension of LLM evaluation is their performance in languages other than English as in fact most users are native speakers of other languages.Most benchmarks are written in English with questions taken in many cases from different exams, such as university, high school or professional tests.The simplest approach is to translate these same tests into other languages and use them for multilingual evaluation.This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results.</p>
<p>In this work we perform an initial analysis of the impact of automatic translation on one of the most widely used LLM benchmarks, the Multitask Language Understanding (MMLU) test Hendrycks et al. (2021a) for one of the most commonly used and chosen as a second language to learn, Spanish.The analysis shows that automatic translation induces errors in the LLM answers and thus distorts the benchmark's results.Based on these findings, potential solutions to this problem are also briefly discussed.</p>
<p>The rest of the work is organized as follows, in section 2 the methodology used in our analysis is presented, followed by the results in section 3 and a discussion of their implications and potential solutions in section 4. The paper ends with the conclusion in section 5.</p>
<p>Methodology</p>
<p>This section discusses the methodology used in our analysis, first, the tests used in the evaluation and tools selected are discussed to then describe the evaluation procedure.</p>
<p>Tests and Tools</p>
<p>To evaluate the impact of automatic translation on the benchmarks, we have selected three categories from the MMLU test Hendrycks et al. (2021a): Miscellaneous, Philosophy, and US foreign policy with 783, 311, and 100 questions respectively.The first one covers a wide range of topics that can be affected by translation, while the second one focuses on well-known content that is universal.Finally, the last category focuses on US-related questions that may also be prone to translation errors.Therefore, the three categories can provide insights into the potential limitations of automatic translation.</p>
<p>In terms of translation tools, we consider two: Azure Translator and ChatGPT.The reasoning behind our choices is that Azure Translator was the tool used to evaluate MMLU for languages other than English in GPT4 Achiam et al. (2023).Therefore, our findings would be applicable to GPT4 evaluation results.On the other hand, it is interesting to check how well an LLM performs when translating tests that will be used to evaluate the same LLM.Thus we translate the questions with ChatGPT4 and then use them to evaluate it.This enables us to check whether using the same tool for translation and testing introduces any bias in the evaluation, for example, better performance may be achieved when the same tool is used for both.</p>
<p>Finally, the LLM used to answer the questions, both in English and Spanish, is GPT4, which is among the bestperforming LLMs on the MMLU benchmark to date.The rationale is that this model will produce the largest number of correct answers in the absence of translation errors and thus will be the most effective in detecting discrepancies due to translation errors.For example, a model that already produces wrong answers for 50% of the questions in English will not detect translation errors on that 50%.In the same way, it is possible that right answers in English are incorrectly translated into Spanish but answered correctly by the LLM.</p>
<p>Evaluation Procedure</p>
<p>The overall procedure is illustrated in Figure 1 and has the following steps:</p>
<ol>
<li>
<p>Translate the questions from the selected categories into Spanish using the chosen tools: Azure Translator and ChatGPT4.</p>
</li>
<li>
<p>Run the same MMLU tests on both the original and the Spanish translated versions and log the answers and their log probabilities.</p>
</li>
<li>
<p>Identify the questions in which the answers are correct in English and wrong in Spanish.</p>
</li>
<li>
<p>Analyze them manually.Two experts revise each question independently to avoid mistakes or biases in the evaluation and the questions are divided into two groups: correct and having translation errors.Revise and correct the translation errors.</p>
</li>
<li>
<p>Run the corrected questions again on the LLM.The number of correct answers is logged and reported to estimate the impact of translation errors on the benchmark results.</p>
</li>
</ol>
<p>The procedure is designed to evaluate the impact of the errors in the automatic translation on the test results.This enables us to focus only on the questions that are answered correctly in English and wrongly in any of the two Spanish translations, significantly reducing the effort needed for the manual check of the translation.This is relevant as it enables the methodology to scale to larger datasets with a reasonable effort and cost as each question is checked and corrected by two experts.</p>
<p>Limitations</p>
<p>The proposed methodology has a number of limitations.Firstly, it cannot detect errors in translation that do not affect the answer, i.e., if there is a mistake in a translation but the LLM answer is correct it will not be detected.This can be due to the translation error not affecting the LLM's ability to answer but also due to randomness in the responses.</p>
<p>Secondly, the fact that the LLM answers correctly to the revised translation may also be due to randomness in some cases.</p>
<p>Finally, there are also limitations related to the number of questions tested and the tools used.The evaluation was limited to approximately 1200 questions and only two automatic translation tools to keep the effort of manual revisions acceptable.To get a better estimate of the impact of automatic translations, more questions taken from different benchmarks and using several translation tools should be used.The results also depend on the prompts used to ask the LLM, the LLM used, and its configuration parameters.3 Results and Analysis</p>
<p>As per the proposed methodology, first, the questions are translated into Spanish with Azure Translator and ChatGPT.The translations as well as the rest of the results presented in the following, are available in a public repository2 .Next, the three versions of the selected MMLU questions are run on ChatGPT4 and the results in terms of the number and percentage of incorrect answers are summarized in Table 1.It can be observed that there is a performance loss when the tests are run in Spanish in all three categories and for both Azure and ChatGPT translations.The relative loss is significantly larger in Philosophy and US foreign policy.</p>
<p>The third step is to identify the questions that were correctly answered in English but failed in Spanish.The number of such questions is given in Table 2.For US foreign policy, adding those to the failures in English gives the number of failures in Spanish.This means that there are no correct answers in Spanish for items that failed in English.Instead, for the other two categories, the addition is larger than the number of failures in Spanish, which means that there are a few correct answers in Spanish for items that failed in English or that failed in both languages.Comparing the translations, the one done with Azure Translator has more questions correctly answered in English but failed in Spanish in total, and in two of the three categories.This may be due to ChatGPT understanding better its own translations.</p>
<p>In the fourth step, the questions with correct answers in English and wrong in Spanish were analyzed manually to check the translations done by Azure Translator and ChatGPT.There are clear examples of errors which can be attributed to the translation.In the Miscellaneous category in the question below, the correct answer is John Constable.For some reason, Azure Translator translates the name to "Juan Alguacil" (in boldface) but not the others and ChatGPT4 selects an incorrect answer.Instead, the answer is correct for the ChatGPT translation as it does not translate John Constable.</p>
<p>Questions
□ Juan Alguacil □ William Morris □ William Hogarth ❒ ✓ Joshua Reynolds
Similarly in the US foreign policy category, we have the following question that is wrongly answered in Spanish for both Azure Translator and ChatGPT translations.In this case, the issue is that the expression "American multiplication table" is translated literally into Spanish and loses its meaning of the large population growth in the US Sexton (2018).</p>
<p>Actually, there is no translation for this expression, which points to a fundamental limitation of translating benchmarks: there may be questions that cannot be translated.In addition to checking the translations to identify and correct mistakes, an analysis of those mistakes was made to try to find patterns in the errors.The conclusion was that the translation errors observed can be grouped into the following main categories:</p>
<p>Questions</p>
<p>Translation of a proper name: "John Constable" translated into "Juan Alguacil", or "Stephen King" as "Esteban Rey" which seems to demonstrate that the model does not identify proper names as entities.</p>
<ol>
<li>
<p>Incorrect translation of a technical term: "Furrow opener" translated into "abre surcos" instead of "arado", "lieutenant general" as "general de teniente" instead of "teniente general".</p>
</li>
<li>
<p>A term is not translated: "Wednesday child" translated into "El hijo de Wednesday", "Cicero" not translated into "Cicerón".</p>
</li>
<li>
<p>Cultural adaptation was needed in the translation: "The paper chase" translated into "La persecución del papel", which is not consistent with the name used in Spain; and measures (length, currencies) are not adapted.</p>
</li>
<li>
<p>Change of meaning: "grand" is incorrectly translated into Spanish as "big", "older" as "old" instead of as "great", "rules make take into account..." as "rules may take into account...".</p>
</li>
<li>
<p>Grammatical errors: the words in the question are feminine and the ones used in the options appear as masculine or vice versa; and the text sometimes contains ungrammatical sequences caused by literal translations ("how many pence make a pound?" translated as "¿cuántos peniques hacen una libra?".</p>
</li>
</ol>
<p>Table 3 shows the results of the analysis done on the questions that had correct answers in English and wrong in Spanish with the initial translation.First, the number of questions in which the manual checking found issues in the automatic translation is given.It can be observed that for Azure translator the majority of the questions have errors in the translation while for ChatGPT the numbers are lower but still significant.</p>
<p>As per the proposed methodology, in step 5, the incorrectly translated questions were translated manually and rerun on ChatGPT4.The results in terms of questions that are incorrectly answered after fixing the translation are also shown in 3. It can be seen that a significant fraction of the questions have the right answer after manually fixing the translation.In the case of the Azure translations, the percentage is above 34% for all three categories and exceeds 63% for Miscellaneous.In the case of ChatGPT, the number of questions corrected is again lower in total and lower in two of the three categories with values ranging from 9% to 37%.These results clearly illustrate how the use of automatic translations can impact the results of benchmarks ran in languages other than English.</p>
<p>Discussion</p>
<p>The findings of our initial evaluation suggest that the use of automatic translation of the questions in LLM benchmarks causes deviations in the results due to errors in the translation.To eliminate those deviations and ensure that the results are not contaminated by translation errors, the translations should be revised by experts and, ideally, the tests should be adapted to the language being evaluated.However, given the number of questions on the LLM benchmarks and the number of languages evaluated, this requires a large effort that calls for coordinated action from the community.</p>
<p>In fact, there are efforts in this direction such as the validation of the Spanish Okapi benchmarks as part of the #Somos600M Project Grandury (2024).This community annotation effort uses open-source frameworks and is performed by native Spanish-speaking volunteers.During the first two months, more than 60 persons participated and together covered one-third of the total number of dataset items, which shows how time-consuming it is to manually validate and correct these translations.</p>
<p>In the absence of manually adapted or checked benchmarks, our methodology enables a fast evaluation of the impact of translation errors with limited manual checking.Further refinements can be introduced to detect translation errors.For example, after translating into Spanish we could translate back into English the questions with different answers in both languages and run the questions again.When the answer to this English translation is different from the one of the original English question, the translation is likely to be the culprit of the error.In general, developing strategies to identify these issues automatically would be very helpful to understand the impact of automatic translation and also to correct the translation errors.</p>
<p>In this work, we have focused on Spanish which is one of the most widely used languages in the world and also is typically in the top five languages with more data on the LLM training datasets.It would be interesting to study the impact of translation errors on other languages.For languages with fewer data and speakers, we would expect a larger number of translation errors but also a larger number of genuine errors in the LLM answers.Therefore, the relative impact of translation errors on the benchmark results compared to Spanish can be either larger or smaller.</p>
<p>Conclusion and future work</p>
<p>In this work, we have analyzed the limitations of using automatic translation of English benchmarks to evaluate LLMs in other languages.In more detail, three categories from the MMLU benchmark have been translated into Spanish and run on ChatGPT4.As previously mentioned, it would be interesting to test other categories from the MMLU benchmark and other models.Then, the test items for which the LLM answers are different in English and Spanish have been identified and analyzed manually to understand if the differences can be attributed to the translation.The results show that a significant fraction of the differences are due to errors in the translation of the questions.These findings highlight the need to improve non-English LLM benchmarks by at least ensuring that the translations are correct and ideally by adapting the questions to the target language and culture.</p>
<p>The development of language-specific or at least language-adapted benchmarks should be a priority to provide better evaluation tools for multilingual LLMs.To achieve that goal, open initiatives are needed to coordinate the efforts of the community to develop for example language specific LLM leaderboards.</p>
<p>Figure 1 :
1
Figure 1: Diagram of the evaluation methodology used.</p>
<p>Table 1 :
1
Number (percentage) of incorrect answers for ChatGPT4.
CategoryEnglishSpanish (Azure) Spanish (ChatGPT)Miscellaneous47 (6%)65 (8.3%)66 (8.56%)Philosophy43 (13.9%) 66 (21.29%)57 (18.4%)US foreign policy 3 (3%)10 (10%)11 (11%)</p>
<p>Table 2 :
2
Number of correct answers in English and wrong in Spanish for ChatGPT4.
CategorySpanish (Azure) Spanish (ChatGPT)Miscellaneous3328Philosophy2921US foreign policy 78</p>
<p>Table 3 :
3
Analysis of the questions that had correct answers in English and wrong in Spanish for ChatGPT4.
Answers
https://huggingface.co/spaces/optimum/llm-perf-leaderboard
https://zenodo.org/records/11314109</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>E Beeching, C Fourrier, N Habib, S Han, N Lambert, N Rajani, O Sanseviero, L Tunstall, T Wolf, Open llm leaderboard. 2023</p>
<p>Speed and conversational large language models (llms): Not all is about tokens per second. J Conde, M González, P Reviriego, Z Gao, S Liu, F Lombardi, Computer. 5782024in press</p>
<p>The #somos600m project: Generating nlp resources that represent the diversity of the languages from latam, the caribbean, and spain. M Grandury, North American Chapter of the Association for Computational Linguistics Conference: LatinX in AI (LXAI) Research Workshop. 2024</p>
<p>Z Guo, R Jin, C Liu, Y Huang, D Shi, L Yu, Y Liu, J Li, B Xiong, D Xiong, arXiv:2310.19736Evaluating large language models: A comprehensive survey. 2023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2021a</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021barXiv preprint</p>
<p>Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. V D Lai, C Van Nguyen, N T Ngo, T Nguyen, F Dernoncourt, R A Rossi, T H Nguyen, arXiv:2307.160392023arXiv preprint</p>
<p>S Minaee, T Mikolov, N Nikzad, M Chenaghlu, R Socher, X Amatriain, J Gao, arXiv:2402.06196Large language models: A survey. 2024arXiv preprint</p>
<p>A Nation Forged by Crisis: A New American History. J Sexton, 2018Hachette UK</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, arXiv:1905.078302019arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
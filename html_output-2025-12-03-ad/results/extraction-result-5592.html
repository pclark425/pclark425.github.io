<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5592 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5592</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5592</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-269758155</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.07626v2.pdf" target="_blank">AnomalyLLM: Few-Shot Anomaly Edge Detection for Dynamic Graphs Using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps. With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type. Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications. In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM. To align the dynamic graph with LLMs, AnomalyLLM pretrains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the prototypes of word embeddings. Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection. Experiments on four datasets reveal that AnomalyLlmcan not only significantly improve the performance of few-shot anomaly detection, but also achieve superior results on new anomalies without any update of model parameters.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5592.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5592.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnomalyLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that leverages frozen large language model (LLM) backbones together with a dynamic-aware graph encoder, reprogramming-based modality alignment (text prototypes + cross-attention) and prompt-based in-context learning to perform few‚Äëshot anomaly edge detection on dynamic graphs (sequences of graph snapshots).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>vicuna-7B-v1.5 (primary backbone), vicuna-7B-v1.3, vicuna-7B-v1.1, Llama-2-7B (evaluated backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal transformer LLM backbones (7B-class models in the vicuna/Llama-2 family) used as frozen language models: the paper keeps LLM parameters intact and feeds reprogrammed edge embeddings via a prompt template; the LLM produces a contextual hidden-state vector for each edge which is fused with encoder outputs and scored by an MLP anomaly detector.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Few-shot, prompt-based in-context learning (ICL) with reprogramming-based modality alignment. Pipeline: (1) dynamic-aware contrastive pretraining of a graph encoder to produce edge embeddings; (2) reprogram edge embeddings into the LLM token embedding space using a set of clustered text prototypes and multi-head cross-attention; (3) feed the reprogrammed embedding into a frozen LLM inside a prompt (role, task description, examples, question) and take the LLM hidden state corresponding to an <Edge> token as an edge representation; (4) an MLP (trained on pseudo-labels) scores anomaly probability. LLM parameters remain frozen; encoder and MLP can be fine-tuned with pseudo-labels (randomly sampled edges as negatives) for alignment only.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured, temporal graph data: sequences of graph snapshots (dynamic graphs), represented as edge-centric structural-temporal subgraphs (i.e., structured/sequential graph data rather than plain text or tabular rows).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Edge anomalies in dynamic graphs: Contextual Dissimilarity Anomaly (CDA), Long-Path Links (LPL), Hub-Hub Links (HHL), randomly-inserted edges (pseudo anomalies used during alignment), and real-world labeled anomalies (in T-Finance and T-Social).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>UCI Messages (synthetic anomalies CDA/LPL/HHL), BlogCatalog (synthetic anomalies CDA/LPL/HHL), T-Finance (real labeled anomalies), T-Social (real labeled anomalies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Area Under ROC Curve (AUC). Representative reported results: AnomalyLLM achieves strong AUCs across datasets and shot settings ‚Äî e.g., UCI Message AUCs reported ~0.84 (various shot settings), BlogCatalog AUCs reported ~0.84‚Äì0.85 (various shot settings), real datasets: T-Finance 1-shot 0.8018, 5-shot 0.8056, 10-shot 0.8087; T-Social 1-shot 0.8101, 5-shot 0.8187, 10-shot 0.8206. Ablation results show drops when removing encoder, alignment or ICL (example: BlogCatalog w/o encoder AUCs ~0.78 vs full AnomalyLLM ~0.8488). Unsupervised comparisons (Appendix) also report consistent AUC gains over TADDY across anomaly ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>AnomalyLLM consistently outperforms traditional and recent baselines evaluated in the paper: DeepWalk, StrGNN, AddGraph, TADDY (Transformer-GNN), TGN, GDN and SAD. Examples: relative AUC improvements on UCI Message 5-shot setting vs SAD reported as approximately 19%, 18.5% and 20.3% for the three synthetic anomaly types; on real large-scale datasets AnomalyLLM beats TGN by ~20.6% AUC in one comparison. The paper attributes improvements to (i) dynamic-aware encoder capturing temporal structure, (ii) modality alignment to exploit LLM generalization, and (iii) ICL making effective use of few labeled examples without fine-tuning the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Documented or implied limitations include: (1) method is demonstrated on dynamic graph (edge) anomalies ‚Äî not on generic tabular lists or arbitrary unstructured lists; applicability to plain lists/tables is not evaluated in this paper; (2) modality alignment requires selecting/creating text prototypes (via prompting the LLM to generate graph-related words) and uses pseudo-labels (randomly sampled edges) for alignment ‚Äî these heuristics could misalign for anomaly types not well represented by pseudo-labels; (3) subgraph size selection is sensitive (too many nodes can introduce noise; paper reports performance drop beyond ~14 nodes); (4) the LLMs are kept frozen ‚Äî while efficient, this may limit ability to learn highly domain-specific anomaly semantics that require parameter updates; (5) alignment training (encoder + detector) and pretraining still incur compute (alignment on 30k pseudo edges reported ~250s/epoch and convergence reported around 5 epochs); (6) no experiments on classical tabular anomaly detection or list-based syntactic anomalies are reported, so generalization to those modalities is unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Key novel technical pieces: dynamic-aware contrastive pretraining of subgraph-based edge encoder; reprogramming edge embeddings with clustered text prototypes via cross-attention into the LLM embedding space; prompt template design for ICL (role, task description, examples, question); LLM kept intact (no LLM fine-tuning). The paper reports alignment times (Table 2), ablations (Table 3), and per-backbone trade-offs (vicuna-7B-v1.5 chosen as best balance of AUC and speed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AnomalyLLM: Few-Shot Anomaly Edge Detection for Dynamic Graphs Using Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Time-llm: Time series forecasting by reprogramming large language models <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5592",
    "paper_id": "paper-269758155",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "AnomalyLLM",
            "name_full": "AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models",
            "brief_description": "A method that leverages frozen large language model (LLM) backbones together with a dynamic-aware graph encoder, reprogramming-based modality alignment (text prototypes + cross-attention) and prompt-based in-context learning to perform few‚Äëshot anomaly edge detection on dynamic graphs (sequences of graph snapshots).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "vicuna-7B-v1.5 (primary backbone), vicuna-7B-v1.3, vicuna-7B-v1.1, Llama-2-7B (evaluated backbones)",
            "model_description": "Causal transformer LLM backbones (7B-class models in the vicuna/Llama-2 family) used as frozen language models: the paper keeps LLM parameters intact and feeds reprogrammed edge embeddings via a prompt template; the LLM produces a contextual hidden-state vector for each edge which is fused with encoder outputs and scored by an MLP anomaly detector.",
            "model_size": "7B",
            "anomaly_detection_method": "Few-shot, prompt-based in-context learning (ICL) with reprogramming-based modality alignment. Pipeline: (1) dynamic-aware contrastive pretraining of a graph encoder to produce edge embeddings; (2) reprogram edge embeddings into the LLM token embedding space using a set of clustered text prototypes and multi-head cross-attention; (3) feed the reprogrammed embedding into a frozen LLM inside a prompt (role, task description, examples, question) and take the LLM hidden state corresponding to an &lt;Edge&gt; token as an edge representation; (4) an MLP (trained on pseudo-labels) scores anomaly probability. LLM parameters remain frozen; encoder and MLP can be fine-tuned with pseudo-labels (randomly sampled edges as negatives) for alignment only.",
            "data_type": "Structured, temporal graph data: sequences of graph snapshots (dynamic graphs), represented as edge-centric structural-temporal subgraphs (i.e., structured/sequential graph data rather than plain text or tabular rows).",
            "anomaly_type": "Edge anomalies in dynamic graphs: Contextual Dissimilarity Anomaly (CDA), Long-Path Links (LPL), Hub-Hub Links (HHL), randomly-inserted edges (pseudo anomalies used during alignment), and real-world labeled anomalies (in T-Finance and T-Social).",
            "dataset_name": "UCI Messages (synthetic anomalies CDA/LPL/HHL), BlogCatalog (synthetic anomalies CDA/LPL/HHL), T-Finance (real labeled anomalies), T-Social (real labeled anomalies)",
            "performance_metrics": "Area Under ROC Curve (AUC). Representative reported results: AnomalyLLM achieves strong AUCs across datasets and shot settings ‚Äî e.g., UCI Message AUCs reported ~0.84 (various shot settings), BlogCatalog AUCs reported ~0.84‚Äì0.85 (various shot settings), real datasets: T-Finance 1-shot 0.8018, 5-shot 0.8056, 10-shot 0.8087; T-Social 1-shot 0.8101, 5-shot 0.8187, 10-shot 0.8206. Ablation results show drops when removing encoder, alignment or ICL (example: BlogCatalog w/o encoder AUCs ~0.78 vs full AnomalyLLM ~0.8488). Unsupervised comparisons (Appendix) also report consistent AUC gains over TADDY across anomaly ratios.",
            "baseline_comparison": "AnomalyLLM consistently outperforms traditional and recent baselines evaluated in the paper: DeepWalk, StrGNN, AddGraph, TADDY (Transformer-GNN), TGN, GDN and SAD. Examples: relative AUC improvements on UCI Message 5-shot setting vs SAD reported as approximately 19%, 18.5% and 20.3% for the three synthetic anomaly types; on real large-scale datasets AnomalyLLM beats TGN by ~20.6% AUC in one comparison. The paper attributes improvements to (i) dynamic-aware encoder capturing temporal structure, (ii) modality alignment to exploit LLM generalization, and (iii) ICL making effective use of few labeled examples without fine-tuning the LLM.",
            "limitations_or_failure_cases": "Documented or implied limitations include: (1) method is demonstrated on dynamic graph (edge) anomalies ‚Äî not on generic tabular lists or arbitrary unstructured lists; applicability to plain lists/tables is not evaluated in this paper; (2) modality alignment requires selecting/creating text prototypes (via prompting the LLM to generate graph-related words) and uses pseudo-labels (randomly sampled edges) for alignment ‚Äî these heuristics could misalign for anomaly types not well represented by pseudo-labels; (3) subgraph size selection is sensitive (too many nodes can introduce noise; paper reports performance drop beyond ~14 nodes); (4) the LLMs are kept frozen ‚Äî while efficient, this may limit ability to learn highly domain-specific anomaly semantics that require parameter updates; (5) alignment training (encoder + detector) and pretraining still incur compute (alignment on 30k pseudo edges reported ~250s/epoch and convergence reported around 5 epochs); (6) no experiments on classical tabular anomaly detection or list-based syntactic anomalies are reported, so generalization to those modalities is unknown.",
            "additional_notes": "Key novel technical pieces: dynamic-aware contrastive pretraining of subgraph-based edge encoder; reprogramming edge embeddings with clustered text prototypes via cross-attention into the LLM embedding space; prompt template design for ICL (role, task description, examples, question); LLM kept intact (no LLM fine-tuning). The paper reports alignment times (Table 2), ablations (Table 3), and per-backbone trade-offs (vicuna-7B-v1.5 chosen as best balance of AUC and speed).",
            "uuid": "e5592.0",
            "source_info": {
                "paper_title": "AnomalyLLM: Few-Shot Anomaly Edge Detection for Dynamic Graphs Using Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Time-llm: Time series forecasting by reprogramming large language models",
            "rating": 2,
            "sanitized_title": "timellm_time_series_forecasting_by_reprogramming_large_language_models"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 1,
            "sanitized_title": "reasoning_with_language_model_is_planning_with_world_model"
        }
    ],
    "cost": 0.011269,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models
28 Aug 2024</p>
<p>Shuo Liu 
Di Yao yaodi@ict.ac.cn 
Lanting Fang 
Wenbin Li 
Xiaowen Ji 
Jingping Bi 
Zhetao Li 
Kaiyu Feng </p>
<p>Institute of Computing Technology
Chinese Academy of Sciences</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences</p>
<p>Beijing Institute of Technology Zhetao Li Jinan University</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences Kaiyu Feng Beijing Institute of Technology</p>
<p>Southeast University</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences</p>
<p>WashingtonDCUSA</p>
<p>AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models
28 Aug 2024B33ACF2F9EAF81D3AFED12AD73AD9185arXiv:2405.07626v2[cs.LG]Dynamic GraphsAnomaly DetectionFew-Shot LearningLarge Language Models
Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps.With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type.Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications.In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM.To align the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the prototypes of word embeddings.Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection.Experiments on four datasets reveal that AnomalyLLM can not only significantly improve the performance of few-shot anomaly detection, but also achieve superior results on new anomalies without any update of model parameters.</p>
<p>INTRODUCTION</p>
<p>The dynamic graph is a powerful data structure for modeling the evolving relationships among entities over time in many domains of applications, including recommender systems [40], social networks [3], and data center DevOps [15].Anomaly edges in dynamic graphs, which refer to the unexpected or unusual relationships between entities [23], are valuable traces of almost all web applications, such as abnormal interactions between fraudsters and benign users or suspicious interactions between attacker nodes and user machines in computer networks.Due to the temporary nature of dynamics, the types of anomaly edges vary greatly, leading to the difficulty of acquiring sufficient labeled samples of new types.Therefore, detecting anomaly edges with few labeled samples plays a vital role in dynamic graph analysis and is of great importance for various applications, including network intrusions [1,39], financial fraud detection [13,22], and etc.</p>
<p>Recently, various techniques have been proposed to detect anomalies in dynamic graphs.Based on the usage of labeled information, existing solutions can be categorized into three groups: supervised methods, unsupervised methods, and semi-supervised methods.Supervised methods [6,24,25,37]utilize labeled training samples to build detectors that can identify anomalies from normal edges.Although they have demonstrated promising results, obtaining an adequate number of labeled anomaly edges for model training is challenging for dynamic graphs, which limits their scalability.Unsupervised methods [2,4,7,18,19,30,43,47] aim to identify anomalies in dynamic graphs without the use of label information.These approaches typically rely on statistical measures [7,18], graph topology [2,30], or graph embedding techniques [19,43,47] to capture deviations from normal patterns.Without label information, they are mainly designed to detect randomly inserted edges as anomalies and are hard to extend for other anomaly types.Only one work, namely SAD [35], tries to address the problem using semi-supervised learning.However, the training data used in SAD contains hundreds of labeled samples, which is also impractical in most cases.As shown in Figure 1, with the evolution of time, the anomaly edges may change and new types of anomaly edges would emerge.For these new types, only a few (less than 10) labeled samples are available for model training.Thus, the problem we aim to solve is to identify various types of anomaly edges in the dynamic graph with few labeled samples for each type.To the best of our knowledge, there is no existing work that can be directly used for this problem.</p>
<p>With the rapid progress of foundation models, large language models (LLMs) show a remarkable capability of understanding graph data [33,44] and generalizability on new tasks [31], which offers a promising path to achieve few shot anomaly edges detection for dynamic graphs.However, this task is also challenging in three The edge representations should not only encode the information of adjacent topology but also be aware of the temporal dynamics.</p>
<p>(2) Alignment between graph and neural language.LLMs operate on discrete tokens, whereas dynamic graphs change in continuous time.It remains an open challenge to align the semantics between dynamic graphs and word embeddings of LLMs.(3) Adaptation with few anomaly samples.To achieve few-shot detection, both LLMs and the anomaly detector should make full use of the label information of limited anomaly samples to identify different anomalies.</p>
<p>To solve the challenges, we proposed a novel method, namely AnomalyLLM, to integrate the power of LLMs and detect anomaly edges with few labeled samples.It is composed of three key modules, i.e., dynamic-aware contrastive pretraining, reprogrammingbased modality alignment, and in-context learning for few-shot detection.Without using the label information, AnomalyLLM first employs a novel structural-temporal sampler to organize triplewise subgraphs and pre-trains a dynamic-aware encoder of edges with contrastive loss.To align the graph encoder to LLMs, we keep the LLMs intact and reprogram the edge embeddings by text prototypes before feeding them into the frozen LLMs.Along with the reprogrammed edges, a prompt strategy is proposed to enrich the input context and direct the ability of LLMs.Both the edge embeddings and the output of LLMs are fused to identify the normal/random sampled edges.Moreover, to achieve few-shot, we employ in-context learning framework and design a prompt template that is flexible enough to encode a few labeled samples of various anomaly types.In this way, AnomalyLLM is able to detect different types of anomalies without modifying the model parameters.</p>
<p>Compared to existing solutions, AnomalyLLM has the following attractive advantages: (1) Anomaly type-agnostic.AnomalyLLM conducts the dynamic graph encoding and the modality alignment in an unsupervised manner.The information of anomaly type is only used to construct the prompt of in-context learning.For detecting different anomaly types, all we need is a new prompt, i.e. the model parameters are anomaly type-agnostic.(2) Fine-tuning free.AnomalyLLM directly uses the pre-trained LLMs as the backbone and keeps it intact during the reprogramming-based modality alignment.The parameters in LLMs do not require expensive fine-tuning computations.(3) Simple to upgrade.In AnomalyLLM, LLMs are only related to modality alignment parameters, and the training time for these parameters is not lengthy.If there is an alternative more powerful LLM, AnomalyLLM is simple to be upgraded by retraining the related parameters.The main contributions of this paper can be summarized as follows:</p>
<p>‚Ä¢ We propose a novel method AnomalyLLM leveraging the advanced capabilities of LLMs for few-shot anomaly edge detection.</p>
<p>To the best of our knowledge, this is the first work that integrates LLMs for anomaly detection of dynamic graphs.‚Ä¢ We introduce a reprogramming-based modality alignment technique, which represents the graph edge embeddings with some text prototypes, to bridge the gap between the dynamic-aware encoder and the LLMs.‚Ä¢ An in-context learning strategy is designed to integrate the information of a few labeled samples, making AnomalyLLM adaptable to various anomaly types with minimal computational overhead.</p>
<p>‚Ä¢ Extensive experiments on four datasets show that AnomalyLLM can not only consistently outperform all baselines in few-shot detection settings but also achieve high efficiency in both alignment tuning and inference.</p>
<p>RELATED WORK</p>
<p>In this section, we provide an overview of existing studies related to AnomalyLLM from three perspectives: (1) graph anomaly detection (2) Large Language Models (3) few-shot learning.Graph Anomaly Detection.Existing graph anomaly detection methods can be broadly divided into three categories, supervised method, unsupervised method, and semi-supervised method.Most supervised methods [6,24,25,37] rely on labeled data to train anomaly detectors, which may result in poor performance due to the limited number of samples in real-world scenarios.Unsupervised methods [2,4,7,18,19,30,43,47] primarily identify anomalies based on statistical measures or graph topology.These techniques mainly rely on randomly-inserted edges [45] during training, which differs from actual anomalies.Recently, with the advancement of semi-supervised techniques, a hybrid methods like SAD [35] have been proposed to incorporate both labeled and unlabeled data.However, these methods rely on a considerable amount of labeled samples.Nevertheless, all of these methods need the node attributes, which is not easy to obtained in dynamic graph data.</p>
<p>Large Language Models.The emergence of large language models [9,28] has ushered in a new era of few-shot learning capabilities, exemplified by their application in In-Context reasoning with minimal examples.Many LLM-based methods [33,44] are proposed to graph analysis, primarily focusing on leveraging the rich textual attributes inherent in graphs.These techniques mainly rely on modality alignment between graph representations and textual properties.However, this reliance significantly limits their applicability in scenarios where textual attributes are absent.While some efforts [14] have been made to enhance LLMs' understanding of non-textual data like time-series, through reprogramming techniques, the application of these methodologies to graph data, especially dynamic graphs, remains largely unexplored.</p>
<p>Few-shot Learning in Dynamic Graphs.The challenge of limited labeled data is pervasive in real-world applications.Many studies have explored for few-shot learning, using techniques like meta-learning or contrastive learning [5,20,36,38,42,46,48].However, these methods are generally tailored to static graphs or specific tasks [11,17], leaving a gap in anomaly edge detection for dynamic graphs.Our study addresses this gap by leveraging the potential of LLMs in a few-shot learning context for anomaly detection in dynamic graphs.</p>
<p>PRELIMINARY 3.1 Problem Definition</p>
<p>Let G = [G 1 , ..., G  , ..., G  ] denote a sequence of graph snapshots spanning timestamps 1 to T, where each snapshot G  = (V  , E  ) represents the state of the graph at time  with V  being the set of nodes and E  the set of edges.An edge   , = (   ,    ) ‚àà E  signifies an interaction between nodes    and    at time .The structure of each snapshot is encoded in a binary adjacency matrix A  ‚àà R √ó , where A  , = 1 if there is an edge between   and   at timestamp , and A  , = 0 otherwise.Considering the high cost of acquiring large-scale labeled anomaly samples in real-world scenarios, we focus on detecting anomaly edges leveraging only a minimal amount of labeled data.Note that we assume the nodes in G are relatively stable.Given a specified anomaly type T and related set of few anomaly edges E T = {T 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , T  }, where  is the number of anomaly edges, our objective is to detect whether edge   , in G  is an anomaly edge of type T or not.</p>
<p>Overview of AnomalyLLM</p>
<p>As shown in Figure 2, AnomalyLLM is a LLM enhanced few-shot anomaly detection framework.It consists of three key modules: dynamic-aware encoder, modality alignment and in-context learning for detection.</p>
<p>‚Ä¢ Given an edge   , , the dynamic-aware encoder captures the related temporal and structure information from the dynamic graph, and encodes it into the edge representation.We construct a series of structural-temporal subgraphs S  , of edge   , .Based on these subgraphs, AnomalyLLM generates the edge embedding r by fusing all the related subgraphs in S  , .‚Ä¢ Taking r as the input, we first select some dynamic graph-related words and cluster them into V ‚Ä≤ prototypes.AnomalyLLM adopt self-attention to reprogram the edge embedding r with the textual prototype and obtain h.Both existing edges and randomly selected edges are employed to construct pseudo labels for alignment fine-tuning.‚Ä¢ For few-shot detection, we utilize in-context learning to encode the label information from a few anomaly samples.A prompt template consisting of role definition, task description, examples and questions is designed to embed the edge representations h and detect various types of anomalies without any update of model parameters.</p>
<p>METHODOLOGY</p>
<p>As shown in Figure 2, AnomalyLLM consists of three key modules, i.e., dynamic-aware contrastive pretraining, reprogramming-based modality alignment, and in-context learning for few-shot detection.Next, we specify the details of each module respectively.</p>
<p>Dynamic-aware Contrastive Pretraining</p>
<p>Dynamic graphs are changing over time, leading to the difficulty in representing the structure and temporal information of the edges.</p>
<p>Existing solutions either focus on the structure information by averaging the context of adjacent nodes [47][2] or directly use sequential models to capture the temporal dynamics [19][45], which are not sufficient for the anomaly detection.In this section, we propose the dynamic-aware contrastive pretraining to systematically model both aspects and represent the edges with their adjacent subgraphs.The whole module consists of two subparts, i.e. dynamic-aware encoder and contrastive learning-based optimization. .To obtain the temporal context of   , , AnomalyLLM utilizes a sliding window Œì to filter a sequence of graph slices G Œì  = {G  ‚àíŒì+1 , . . ., G  }.For each graph slice, we use the described method to construct subgraphs.Therefore, a sequence of subgraph for   , can be constructed as follows:
S ùë° ùëñ,ùëó = {g ùúè ùëñ,ùëó } for ùúè = ùë° ‚àí Œì + 1, . . . , ùë°
S  , contains not only the structure but also the temporal context of   , .The representation of S  , can be used to detect the anomaly in G.</p>
<p>Subgraph-based Edge Encoder.Given the subgraph sequence S  , of edge   , , we feed them into the subgraph-based edge encoder which synergizes the Transformer and Graph Neural Network (GNN) models to obtain edge representation r  , ‚àà R   , where   represents the embedding dimension.Following the same setting as Taddy [19], we assume the nodes in G are stable and conduct the following four steps on the input S  , : ‚Ä¢ Node Encoding.For each node    in every g   within S  , , we construct the node encoding using three aspects, i.e.,
z ùëô = z diff (ùë£ ùúè ùëô ) + z dist (ùë£ ùúè ùëô ) + z temp (ùë£ ùúè ùëô ) ‚àà R ùëë ùëíùëõùëê .
Here, z diff (   ) represents the diffusion-based spatial encoding capturing the global structural role of node    , z dist (   ) denotes the distance-based spatial encoding, reflecting the local structural context; and z temp (   ) provides the relative temporal information of node    which is the same for all nodes at the time slice .‚Ä¢ Temporal Encoding.We model the temporal information of nodes in S  , by reorganizing the node encoding into an encoding sequence
Z ùëí = [[z ùëô ] ùë£ ùëô ‚ààùëî ùúè ùëñ,ùëó ] ùëî ùúè ùëñ,ùëó ‚ààS ùë° ùëñ,ùëó
, with the dimension of Z  being R (2(+1) ‚Ä¢Œì) √ó  .We feed Z  into a vanilla Transformer block to obtain the node embeddings N  = Transformer(Z e ).</p>
<p>The dimension of node embedding, , is specified here.‚Ä¢ Subgraph Encoding.Additionally, we employ GNN to generate the graph representations of all related subgraphs in S  , .For each subgraph    , we extract the related node embeddings (2) the embeddings between existing edges and randomly sampled edges should be distinguishable.</p>
<p>For edge   , , we check its adjacent graphs, G  ‚àí1 and G  +1 .The sampling should include two levels, i.e. edge level and subgraph level.As shown in Figure 3, we randomly sample a node v , where  =  ¬± 1 not directly connected to    and generate the edge embedding r , for the edge &lt;    , v &gt;.At the edge level, we employ a Multilayer Perceptron (MLP) layer as the anomaly detector to identify whether the input edge is randomly sampled.Here, we feed the embeddings of r  , and r , into the detector and employ binary cross-entropy loss to make them distinguishable:
L ùêµùê∂ùê∏ = ‚àí log(1 ‚àí MLP(r t i,j )) + log(MLP(r t i,j ))
At the subgraph level, we consider the subgraph of v as the negative sample of subgraph of    and utilize the subgraph of    in different timestamps as the positive sample to construct a triplet.</p>
<p>As shown in the right part of Figure 3, we sample negative subgraph and contrastive training triplet for node    .Since the edge embeddings are concatenations of subgraph embeddings, Anoma-lyLLM employ contrastive loss to enlarge the dissimilarity between subgraph embeddings, and the pretraining loss is the combination of both edge level loss and subgraph level loss.
L ùëêùëúùëõ = ‚àí log exp(cos(s ùëé , s ùëù )/ùõø) exp(cos(s ùëé , s ùëù )/ùõø) + exp(cos(s ùëé , s ùëõ )/ùõø)(1)L = L ùêµùê∂ùê∏ + L ùëêùëúùëõ(2)
where s  , s  , s  ‚àà R   represent the subgraph embeddings for the anchor, positive, and negative samples in the triplet.cos() denotes the cosine similarity between two sample embeddings, and  is a temperature parameter that controls the scale of the similarity scores.</p>
<p>Reprogramming-based Modality Alignment</p>
<p>For few-shot detection, the representations of edges should be general enough to be adapted to various anomaly types with few labeled samples.AnomalyLLM employs LLMs as the backbone to enhance the generalization ability of edge embeddings output by the dynamic-aware encoder.This is rather challenging because of the modality difference between dynamic graphs and neural languages.Thus, we propose reprogramming-based modality alignment techniques to bridge the gap.For simplicity, we omit the subscript and note the edge embedding with r.Taking the r as input, AnomalyLLM first reprograms it with the prototype of the word embeddings and feeds the reprogramed vector into LLMs to generate h ‚àà R  .Both r and h are fused as the final edge embedding to input to the LLM for anomaly detection.</p>
<p>Text Prototype</p>
<p>Reprogramming.Although LLMs are trained with neural languages, the learned parameters contain the knowledge of almost all domains and can be viewed as a world model [12].</p>
<p>To leverage the capability of LLM for dynamic graph analysis, we first select a subset of word embeddings and cluster them as text prototypes for reprogramming edge embeddings.Specifically, given the pre-trained word embeddings of LLMs, we refine a subset of words W ‚àà R  √ó related to dynamic graphs to generate text prototypes.In practice, we prompt the LLM with a question, i.e.Please generate a list of words related to dynamic graphs to align dynamic graph data with natural language vocabulary.The full version of this question can be found in the Appendix A.2.The output words in different rounds are combined to obtain  related words.Based on these words, we construct the text prototype with liner transformation:
W ‚Ä≤ = M ‚Ä¢ W
where M ‚àà R  ‚Ä≤ √ó and  ‚Ä≤ is the number of prototypes.Given an edge embedding r, AnomalyLLM utilize multi-head cross-attention to conduct reprogramming.We use r as the query vector and employ W ‚Ä≤ as the key and value matrices.For each attention head  in {1, . . ., }, we compute the related query, key and value matrices, i.e., Q  , K  V  .The attention operation for each head is formalized as:
z ùëê = ATTENTION(Q ùëê , K ùëê , V ùëê )
The outputs from all heads are aggregated to obtain z ‚àà R  .We then add z to the edge embedding r to obtain the reprogramed representation m ‚àà R  of the given edge   , .</p>
<p>Pseudo</p>
<p>Label for Anomaly Fine-tuning.In AnomalyLLM, the backbone LLM takes the reprogrammed input m as input to generate the final representation vector for anomaly detection.Since the parameters of LLMs are intact, the representation of LLM may not contain the information on edge anomalies and may not suit for few-shot detection.Therefore, we utilize the randomly sampled edges (detailed in Section 4.1.2) as pseudo anomaly labels to finetune the parameters of the dynamic-aware encoder and anomaly detector.</p>
<p>As shown in Figure 4, we design a template of prompt for both alignment fine-tuning and in-context learning detection.The template consists of four aspects: role definition, task description, examples and questions, where <Edge> is a mask token for the input edge embedding.We detail the prompt in Section 4.3.1.The instruction is fed into the LLM and the hidden state of the <Edge> token is selected as the final representation vector of edge .For conciseness, we use  to represent   , .This procedure can be formalized as follows:
H = LLM([u, m]
) where u ‚àà R √ó is the related embeddings of instruction templates and H ‚àà R (+1) √ó is the last hidden layer output of the LLM.We utilize the last position of H, i.e. h for detection.Note that our backbone LLM employs causal attention to compute h.Thus, for different edges, the front parts of h are the same.We can use this character to further reduce the computation workload in the pretraining procedure.</p>
<p>As described in Section 4.1.2,an MLP layer is employed to detect the randomly selected anomalies and output an anomaly score for input edge embedding.In this module, we reuse the MLP detector and replace the input edge embedding r with the reprogramed edge embedding r.The anomaly score for an edge  is computed with  () = MLP(h).We also used the randomly selected edges as negative samples and the existing edges as positive samples to construct pseudo labels.A binary cross-entropy (BCE) loss of pseudo labels is employed to optimize the parameters of the dynamic-aware encoder and the detector.L  = ‚àí log(1 ‚àí  ()) + log( ()) Note that the MLP detector is optimized in both pre-training and alignment fine-tuning.In few-shot anomaly detection, the MLP detector cooperates with the in-context learning strategy to detect various types of anomalies.During the whole procedure, the parameters of LLM are intact.</p>
<p>In-Context Learning for Few-Shot Detection</p>
<p>Given a set of anomaly edges E T = {T 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , T  } of anomaly type T, AnomalyLLM aim to detect whether the new edge  is an anomaly edge of T or not.Considering that the pretraining procedure of AnomalyLLM has no information about the anomaly type, we need to make full use of the labeled information of E T .In this paper, we proposed to use in-context learning that encodes edges in E T .Next, we introduce the construction of the prompt template and few-shot anomaly detection respectively.</p>
<p>Prompt Template Construction.</p>
<p>The ability of LLMs on downstream tasks can be unleashed by in-context learning which learns from the context provided by a prompt without any additional external data or explicit retraining.Thus, how to construct the prompt template is a critical problem.In AnomalyLLM, we argue As shown in Figure 4, the prompt first defines the role of LLM as a few-shot anomaly detector followed by the description of anomaly type T. For the example part, we select the same number of edges E ‚Ä≤ /E ‚Ä≤ T from E/E T as the normal and anomaly samples and generate the embedding of edges in E ‚Ä≤ ‚à™ E ‚Ä≤ T with dynamic-aware encoder denoted by M  .These edges are then processed through the reprogramming module for modality alignment and to build the prompt examples.
M ùêºùê∂ùêø = {m ùëùùëúùë† 1 , . . . , m ùëùùëúùë† ùëõ , m ùëõùëíùëî 1 , . . . , m ùëõùëíùëî ùëÉ , m ùëõùëíùë§ } h ùêºùê∂ùêø = LLM( [u ùêºùê∂ùêø , M ùêºùê∂ùêø ]) [: ‚àí1]; ùëì (ùëí ùëõùëíùë§ ) = MLP(h ICL )
where m   , m   ‚àà R   are the reprogrammed embeddings of the -th positive and negative edge examples, respectively, and m  ‚àà R   is the reprogrammed embedding of the edge under investigation.In the prompt template, we employ mask token <Edge> to represent the location of edge embeddings and each example has a related label tag to make use of the given few labeled data.Given a new edge   needed to be detected, we conduct the same operations of examples to obtain the edge embedding.4.3.2Few-shot anomaly detection.Using AnomalyLLM, we can conduct few-shot anomaly edge detection for various anomaly types without any update of parameters.For a specific anomaly type T, the ICL template can be constructed in advance.Assuming   is a new edge to be detected, AnomalyLLM utilize the dynamicaware encoder to obtain an intermediate vector and reprogram it with text prototypes.By embedding the reprogrammed vector into the ICL template, we obtain the input of LLM to generate the edge embedding h  .Then, the edge embedding is fed into the pre-trained anomaly decoder, i.e. the MLP layer, to calculate the probability of   to be an anomaly of T:
ùëì (ùëí ùëõùëíùë§ ) = MLP(h ICL )
For different anomaly types, we can build multiple ICL templates by using a few labeled samples for each type.The reprogrammed vector of   is embedded in these templates to generate the edge embedding and the anomaly probability of various anomaly types.Due to the causal attention mechanism of our backbone LLM, both the embedding of a few labeled edges and the intermediate embedding of ICL templates can be precomputed in advance.Once the reprogramed vector is generated, AnomalyLLM conducts constant operations to obtain the anomaly probability, leading to high efficiency.Next, we further analyze the complexity of AnomalyLLM to illustrate this character.According to the analysis, the complexity of detecting   , is the summarization of the four parts.Note that Œì, , ,  ‚Ä≤ and  are constant for AnomalyLLM, the inference complexity to detect   , is also a constant.</p>
<p>Complexity Analysis of AnomalyLLM</p>
<p>EXPERIMENTS</p>
<p>In this section, we conducted extensive experiments on Anoma-lyLLM to answer the following research questions: ‚Ä¢ Q1: What is the performance of AnomalyLLM in detecting different types of anomaly with few labeled anomaly edges for each type?‚Ä¢ Q2: How efficient of AnomalyLLM in model alignment and anomaly detection?‚Ä¢ Q3: What are the influences of the proposed modules and different backbone LLMs? ‚Ä¢ Q4: What is the performance of AnomalyLLM on real-world anomaly edge detection task?</p>
<p>Besides, we also studied the sensitivity of key parameters and the performance comparison on unsupervised anomaly edge detection.Due to the space limit, the results of these experiments are illustrated in Appendix A.5 and A.6.All the code and data are available at https://github.com/AnomalyLLM/AnomalyLLM.</p>
<p>Experimental Settings</p>
<p>We briefly introduce the experimental settings below.The detailed experimental settings can be found in the Appendix A.4.</p>
<p>Data Descriptions.</p>
<p>We use four public dynamic graph datasets to evaluate the performance of AnomalyLLM.The main experiments are conducted on two widely-used benchmark datasets, i.e., UCI Messages [26] and Blogcatalog [34].To evaluate the performance of AnomalyLLM on real-world anomaly detection task and test the capability of AnomalyLLM, we also employ two datasets with real anomaly, i.e.T-Finance [32] and T-Social [32], which have over 21 million and 73 million edges respectively.</p>
<p>Experimental Protocol.</p>
<p>In this paper, we utilize both synthetic anomaly and real anomaly to evaluate the performance of Anoma-lyLLM.Existing dynamic graphs either have no labeled anomaly edges or only have one anomaly type.To verify the ability of Anoma-lyLLM on various anomaly types, we follow the experiments of [21] and generate three kinds of systematic anomaly types, i.e., Contextual Dissimilarity Anomaly(CDA), Long-Path Links (LPL) and Hub-Hub Links(HHL) for UCI Messages and Blogcatalog datasets.The details of anomaly generation are described in Appendix A.4.For dynamic graphs having labeled anomaly, such as T-Finance and T-Social, we directly used the real anomaly label to conduct the experiments.In our experiments, we employ all nodes and edges to pretrain the dynamic-aware encoders and align them to the backbone LLMs.For anomaly detection, only a few labeled edges are available.We build 1-shot, 5-shot and 10-shot labeled edges for each anomaly type to obtain the AUC results on other edges.</p>
<p>Baselines.</p>
<p>We compare AnomalyLLM with seven baselines which can be categorized into three groups, i.e., general graph representation method, unsupervised anomaly detection methods, and semi-supervised anomaly detection methods.For the first group, we select DeepWalk [29] to generate the representations of edges.For unsupervised method, we employ the recent three works, i.e.StrGNN [2], AddGraph [47], and TADDY [19], as our baselines.For semi-supervised methods, we use GDN [6], TGN [41]and SAD [35] for performance comparison.The details of how to use these methods on our tasks are specified in the Appendix A.4.3.</p>
<p>Hyperparameters setting.</p>
<p>For subgraph construction, we set the number  to be 14 and Œì is 4.For edge encoder, the embedding dimension  is 512.AnomalyLLM employs 3-layers stack of Transformer.We train UCI Messages, BlogCatalog, T-Finance and T-Social datasets with 150 epochs.During the modality alignment, we fine-tune the encoder and anomaly detector for 20 epoch.All the experiments are conducted on the 2√óNvidia 3090Ti.</p>
<p>Performance Comparison</p>
<p>To answer Q1, we compare AnomalyLLM against seven baselines and summarize the results in Table 1.Overall, AnomalyLLM outperforms all baselines on all datasets.Compared with the general representation learning method, i.e., DeepWalk, AnomalyLLM achieve over 20% AUC improvement proving that the constructed structural-temporal subgraphs capture the dynamics of graph.For unsupervised anomaly detection methods, TADDY is the strongest baseline due to the Transformer-GNN encoder.However, it is also inferior to AnomalyLLM which can be attributed to the generalization power of LLMs.As to the semi-supervised methods, such as GDN and SAD, AnomalyLLM demonstrates notable improvements.For example, the relative AUC value improvements on the UCI Message dataset for different anomaly type in the 5-shot setting are 19%, 18.5% and 20.3%, respectively.This is because AnomalyLLM employ ICL to excel the useful information of few labeled data.</p>
<p>For different anomaly types, AnomalyLLM achieves stable improvements on CDA, LPL and HHL.We pretrain the dynamic-aware graph encoder for each dataset and detect different types of anomaly by only replacing the embedding of few labeled anomaly edges of the ICL template.As shown in Table 1, the AUC of different anomaly types are over 80% indicating that AnomalyLLM is anomaly type-agnostic.Moreover, with the increase of labeled samples, the performances of both AnomalyLLM and baseline methods are improved steadily.For example, compared to the 10-shot setting, the performance of SAD in the 1-shot setting significantly decreased, with their AUC dropping by approximately 14%.This is because SAD is designed to detect anomaly with hundreds of labeled data.AnomalyLLM still achieves over 0.82 AUC on both two datasets.We attribute this to the effectiveness of ICL module which excites the advanced capabilities of LLMs.</p>
<p>Efficiency Experiments</p>
<p>We study the efficiency of alignment and inference time to answer Q2 and prove that AnomalyLLM is flexible for different LLM backbones.</p>
<p>For the compared baselines, the fine-tuning procedure need be conducted in few-shot anomaly detection.As shown in the left part of Figure 5, the fine-tuning time increases linearly according to the number of edge sizes.For example, in 10-shot anomaly detection of 60, 000 edges, the fine-tuning time of Taddy is over 10,000 seconds.As to AnomalyLLM, there has no fine-tuning procedure in fewshot anomaly detection.We can obtain the detection results of new anomaly types by only replacing the embedding of labeled edges in ICL template.The inference time of ICL detection is shown in the right part of Figure 5.We can observe that the inference time of AnomalyLLM is comparable with other baselines under different batch sizes.This is because of the causal attention mechanism of LLMs.In model inference, the embeddings of the front part of ICL template stay unchanged for different input edges.Thus, AnomalyLLM is efficient for model inference and fine-tuning free for few-shot anomaly detection.</p>
<p>Furthermore, we study the alignment time that utilizes the pseudo label on BlogCatalog dataset to align the semantics of the neural language to dynamic graphs.As shown in Table 2, we count the alignment time of each epoch training by 30000 pseudo label edges.In our experience, the alignment procedure would be convergence in 5 epoch for different LLM backbones.As illustrated, the total alignment time of 30, 000 edges is about 1200 seconds, which is acceptable for replacing the LLM backbone.Therefore, AnomalyLLM is simple and efficient to be updated with more powerful LLMs.</p>
<p>Ablation Results:</p>
<p>To address Q3, we compare AnomalyLLM with three ablations to analyze the effectiveness of the proposed components.We remove the proposed dynamic-aware encoder, the alignment training module and the ICL detection respectively, and obtain w/o encoder, w/o ICL and w/o ICL.The experiment is conducted on BlogCatalog dataset and the results are shown in Table 3.We observe: (1) Comparing the results of AnomalyLLM with w/o encoder, we observe the edge construction by focusing on subgraph embeddings from both sides can extract useful information and capture the evolving properties of edges in dynamic graphs.For example, the AUC improves from 0.7726 to 0.8546 on UCI Message dataset.(2) From the results of w/o ICL and AnomalyLLM, we can conclude that the ICL's capacity to efficiently utilize minimal labeled data is more effective than fine-tuning.(3) AnomalyLLM achieves the best performance compared to all ablations, which proves the effectiveness of the proposed techniques.</p>
<p>Moreover, we also explore the performance of AnomalyLLM under different LLM backbones on BlogCatalog and UCI Message datasets.As illustrated in Figure 6, we assess the inference speed and AUC of various LLMs, including Llama-2-7B, vicuna-7B-v1.1,vicuna-7B-v1.3and vicuna-7B-v1.5.We can observe that vicuna-7B-v1.5 achieves the best performance and has the fastest inference time.To balance the performance and efficiency, we choose vicuna-7B-v1.5 as the LLM backbone.</p>
<p>Performance on Real-World Labeled Dataset</p>
<p>To answer Q4, we verify the performance of AnomalyLLM on two real-world datasets, i.e., T-Finance and T-Social, which have over 100 million edges.The results are summarized in Table 4. Overall, AnomalyLLM outperforms all baselines on all datasets.Compared with the state-of-the-art supervised learning method, i.e., TGN [41], AnomalyLLM achieve over 20.6% AUC improvement.For semi-supervised methods, i.e., GDN and SAD, AnomalyLLM demonstrates notable improvements.For example, compared to SAD, the relative AUC value improvements on the T-Social dataset for different shot settings are 21%, 20.7% and 18.8%, respectively.These results indicate AnomalyLLM is potential to be used in largescale dynamic graphs.</p>
<p>CONCLUSION</p>
<p>In this paper, we are the first to integrate LLMs with dynamic graph anomaly detection, addressing the challenge of few-shot anomaly edge detection.AnomalyLLM leverages LLMs to effectively understand and represent the evolving relationships in dynamic graphs.We introduce a novel approach that reprograms the edge embedding to align the semantics between dynamic graph and LLMs.Moreover, an ICL strategy is designed to enable efficient and accurate detection of various anomaly types with a few labeled samples.Extensive experiments across multiple datasets demonstrate that AnomalyLLM not only significantly outperforms existing methods in few-shot settings but also sets a new benchmark in the field.</p>
<p>A APPENDIX A.1 Detail of Dynamic Encoder</p>
<p>A.1.1 Calculation of Diffusion Matrix.Given the adjacency matrix A  ‚àà R √ó at timestamp , we calculate the diffusion matrix D  ‚àà R  √ó to select related nodes for the target edge.For brevity, we ignore the superscript , and the diffusion matrix D can be calculated according to the adjacency matrix A:
D = ‚àû ‚àëÔ∏Å ùëö=0 ùúÉ ùëö T ùëö ,
where T ‚àà R √ó is the generalized transition matrix and   is the weighting coefficient indicating the ratio of global-local information.It requires that ‚àû =0   = 1,   ‚àà [0, 1] and the eigenvalues   of T are bounded by   ‚àà [0, 1] to guarantee convergence.Different instantiations of diffusion matrix can be computed by applying specific definitions of T and  .For instance, Personalized PageRank (PPR) [27] chooses T = AS ‚àí1 and   =  (1 ‚àí )  , where S ‚àà R √ó is the diagonal degree matrix and  ‚àà (0, 1) is the teleport probability.Another popular example of diffusion matrix is the heat kernal [16], which chooses T = AS ‚àí1 and   =  ‚àí   /!, where  is the diffusion time.The solutions to PPR and heat kernel can be formulated as: Relative Temporal Encoding.This term aims to encode the temporal information of each node in the subgraph node set.Specifically, for each node    in the node set of    , the relative temporal encoding is defined as the difference between the occurring time  of target edge and the current time of timestamp .Therefore, relative temporal encoding is calculated as:
D PPR = ùõº (I ùëõ ‚àí (1 ‚àí ùõº)S ‚àí1/2 AS ‚àí1/2 ) ‚àí1 , D heat = exp(ùõΩAS ‚àí1 ‚àí ùõΩ). A.z temp (ùë£ ùúè ùëñ ) = ùëôùëñùëõùëíùëéùëü (||ùë° ‚àí ùúè ||) ‚àà R ùëë ùëíùëõùëê ,
where  (‚Ä¢) denotes the learnable linear mapping.</p>
<p>A.2 Detail of Prompt</p>
<p>In this section, we provide the detail of our prompt, including the prompt to generate words related to dynamic graphs and the prompt of In-Context Learning.</p>
<p>Prompt to generate words related to dynamic graphs.Please generate a list of words related to dynamic graphs.Dynamic graph data consists of nodes and edges, often representing networks that change over time.To align dynamic graph data with natural language vocabulary, it is essential to select words that can describe both the graph structure and its dynamic changes to form text prototypes.Please include words related to network topology, data fluidity, and time dependency.</p>
<p>Prompt of In-Context Learning.As an AI trained in the fewshot learning approach, I have been provided with examples of both normal and anomaly edges.The anomalies are identified as Contextual Dissimilarity Anomalies, where we first utilize node2vec to obtain the representation of each node in the graph, and connect the pairs of nodes with the maximum Euclidean distance as anomaly edges.These examples serve as a reference for detecting similar patterns in new edges.Please note the following examples and their labels, indicating whether they are normal or anomaly:</p>
<p>A.3 Complexity Analysis of Training</p>
<p>For each edge   , , the complexity of training consists of four parts, i.e., subgraph construction, dynamic-aware embedding computation, reprogramming and anomaly fine-tuning.</p>
<p>‚Ä¢ For the subgraph construction, based on the precomputed diffusion matrix,  related nodes should be selected for nodes   and   .Therefore, the complexity is  (Œì √ó ) where Œì is the temporal window size.A.4.3 Compared baselines.We compare AnomalyLLM with five state-of-the-art dynamic baselines representative works.The main ideas of these methods are listed as follows:</p>
<p>‚Ä¢ StrGNN [2] extracts the h-hop enclosing sub-graph of edges and leverages stacked GCN [19] and GRU to capture the spatial and temporal information.The learning model is trained in an endto-end way with negative sampling from "context-dependent" noise distribution.‚Ä¢ AddGraph [47] further constructs an end-to-end neural network model to capture dynamic graphs' spatial and temporal patterns.‚Ä¢ Deep Walk [29] utilizes a method based on random walks for embedding graphs.Starting from a specified node, it creates random walks of a predetermined length and employs a technique similar to Skip-gram to acquire embeddings for graphs without attributes.</p>
<p>‚Ä¢ TADDY [19] is a Transformer-based module that uses a transformer network to model spatial and temporal information simultaneously.</p>
<p>‚Ä¢ TGN [41] is a semi-supervised learning method that integrates memory modules with graph neural networks to capture dynamic behaviors in evolving graphs, enabling the learning of temporal interactions effectively.‚Ä¢ GDN [6] adopts a deviation loss to train GNN and uses a crossnetwork meta-learning algorithm for few-shot node anomaly detection.</p>
<p>‚Ä¢ SAD [35] is a semi-supervised module, which uses a combination of a time-equipped memory bank and a pseudo-label contrastive learning module to fully exploit the potential of large unlabeled samples and uncover underlying anomalies on evolving graph streams.</p>
<p>A.5 Sensitivity Analysis</p>
<p>To analyze the impact of selecting different numbers of nodes in the Structural-Temporal Subgraph Sampler, we introduced varying numbers of nodes in the contrastive learning module to assess the sensitivity of AnomalyLLM.We ranged the number of nodes from 2 to 20 and then presented the average performance of these configurations on the BlogCatalog dataset in Figure 6.As the number of nodes increased, AnomalyLLM demonstrated a substantial performance enhancement.A similar observation was made on the UCI dataset.Notably, there was a significant performance boost when the node count reached 10, but performance exhibited a slight decline after reaching 14 nodes.The rationale behind these results lies in the potential introduction of noise when selecting an excessive number of nodes to form a subgraph.Too many nodes can lead to subgraphs that are overly complex and include unnecessary information, thus interfering with the model's ability to learn and generalize key information.Additionally, as the number of nodes increases, the computational time required by the model also increases.Therefore, the selection of the number of nodes needs to strike a balance between subgraph complexity and information quality to achieve optimal performance.</p>
<p>A.6 Unsupervised Anomaly Detection</p>
<p>In addressing Q1, we benchmark AnomalyLLM against leading selfsupervised anomaly detection algorithms on the UCI and BlogCatalog datasets, with findings summarized in Table 7. Self-supervised methods for dynamic graphs, which operate without externally labeled data, hinge on capturing the temporal dynamics and nodal attribute changes to discern anomalies.These approaches conventionally employ synthetically generated anomalies for both training and evaluation phases.Empirical insights reveal: (1) AnomalyLLM, post self-supervised training on unlabeled data, delineates an appreciable performance uplift.Specifically, in identifying contextual anomalies within the UCI dataset, AnomalyLLM exhibits a superior average AUC margin over the top-performing baseline by 4.05% at 1% anomaly ratio, with this margin adjusting to 2.78% and 1.70% for 5% and 10% anomaly ratios, respectively.Such advancements underscore the efficacy of pre-training across a heterogeneous anomaly landscape in fostering adaptable representation skills, thus bolstering generalization across varied anomaly contexts.Remarkably, these gains accrue under uniform unsupervised conditions.(2) In scenarios featuring randomly typed anomalies, AnomalyLLM consistently outperforms, a testament to its adeptness at leveraging contextual cues.This proficiency in assimilating temporal and structural nuances endows AnomalyLLM with heightened sensitivity to anomalies, underscoring its robustness and adaptability in anomaly detection tasks.</p>
<p>Figure 1 :
1
Figure 1: The motivation of AnomalyLLM.In the real world, edge anomaly types are diverse, evolving over time, and typically associated with limited labeled data.</p>
<p>Figure 2 :
2
Figure 2: Overview of AnomalyLLM.AnomalyLLM comprises three modules: Dynamic-aware Contrastive Pretraining, Reprogramming-based Modality Alignment, and In-Context Learning for Few-Shot Detection.</p>
<p>Figure 3 :
3
Figure 3: Sample process of contrastive training triplet</p>
<p>Figure 4 :
4
Figure 4: The prompt of In-Context Learning that the prompt should contain the information of four aspects: role definition, task description, examples and question.As shown in Figure4, the prompt first defines the role of LLM as a few-shot anomaly detector followed by the description of anomaly type T. For the example part, we select the same number of edges E ‚Ä≤ /E</p>
<p>Due to the limitations of space, we only analyze the inference complexity here.The complexity of model training is detailed in the Appendix A.2.Given the well-optimized model, AnomalyLLM involve four parts to detect an edge   , , i.e., subgraph construction, dynamic-aware embedding computation, reprogramming and ICL inference of LLM.‚Ä¢ For subgraph construction, AnomalyLLM select  related nodes for nodes   and   .Cause the diffusion matrix of G at all timestamps can be precomputed, the complexity of this part is  (Œì √ó ) where Œì is the temporal window size.‚Ä¢ For dynamic-aware embedding, AnomalyLLM takes the nodes in the subgraphs as input and compute the z diff (  ), z dist (  ) and z temp (  ) for each node   as the node features.The complexity of this part is  (3).Then, the sequence of node features is fed to the Transformer block to obtain node embeddings, with the complexity of  ((2( + 1)Œì) 2  + 2( + 1)Œì 2 ).A GNN layer and average pooling layer of subgraphs is conducted on these embeddings to generate the dynamic-aware embedding r , , and the complexity is  (( + 1) 2 Œì).Therefore, the complexity of this part is  (( + 1) 2 Œì 2  + ( + 1)Œì 2 ).‚Ä¢ AnomalyLLM utilizes self attention to reprogram r , and generate m.The complexity is  ( ‚Ä≤  +  ‚Ä≤  2 ) =  ( ‚Ä≤  2 ).‚Ä¢ Due to the causal attention of LLM, the hidden states of the ICL template are the same except for the last <Edge> embedding h.Thus, for the inference of LLM, AnomalyLLM precomputes and stores the intermediate hidden state of ICL template, and directly conducts  ( ) feed-forward operations to obtain h, where  is the number of Transformer layers in LLM.</p>
<p>Figure 5 :
5
Figure 5: Inference time of AnomalyLLM</p>
<p>Figure 6 :
6
Figure 6: Performance of different LLM backbones</p>
<p>Example 1 :
1
<Edge> Label: Normal Example 2: <Edge> Label: Anomaly Example 3: <Edge> Label: Normal Example 4: <Edge> Label: Normal Example 5: <Edge> Label: Anomaly Example 6: <Edge> Label:Anomaly Example 7: <Edge> Label: Anomaly Example 8: <Edge> Label: Anomaly Example 9: <Edge> Label: Normal Example 10: <Edge> Label: Anomaly (Note: All the above examples are anomaly and represent the same type of anomaly.)Based on the pattern in the examples and samples provided, classify the sentiment of the following new edge.If the new edge is similar to the example edges, it should be considered anomaly.If it is dissimilar, it should be considered normal.New Example: <vector> Label:</p>
<p>4.1.1Dynamic-awareEncoder.Given an edge   , , we first construct structrual-temporal subgraphs S  , , then fed it into the subgarphbased edge encoder to obtain the edge representation r  , .Structural-Temporal Subgraph Construction.For an edge   , , we design to construct structural-temporal subgraphs for both source and target nodes.Given an edge   , = (   ,    ) ‚àà E  , we first construct a diffusion matrix[19] D  ‚àà R  √ó of E  to select the structure context, where  represents the number of nodes in E  .Each row    of D  indicates the connectivity strength of the  ‚àí ‚Ñé node with all other nodes in the graph G  .For   , = (   ,   ), we utilize    and    to select the most significant top- adjacent nodes of V  to form V   and V   as the subgraph nodes of the source node    and target node    respectively.Then, we link the nodes in V   to its related node    to generate E   and obtain the subgraphs g   = {V   , E   }.Similar operations are conducted for the target node    to obtain g   = {V </p>
<p>, E   }.In this way, both the source and the target in   , can be represented by the relevant surrounding subgraphs g  , = [g   , g   ]</p>
<p>Table 1 :
1
Performance comparison results of few-shot anomaly detection on multiple anomaly types.
DatasetModelCDA1-shot LPLHHLCDA5-shot LPLHHLCDA10-shot LPLHHLStrGNN0.5891 0.5756 0.5974 0.6018 0.6041 0.6122 0.6222 0.6329 0.6402AddGraph0.5994 0.6023 0.5988 0.6097 0.6033 0.6104 0.6216 0.6238 0.6172Deep Walk0.6102 0.6073 0.6202 0.6113 0.6122 0.6196 0.6155 0.6176 0.6154BlogCataLogTGN GDN0.6732 0.6699 0.6919 0.7112 0.7023 0.7118 0.7263 0.7387 0.7311 0.6733 0.6795 0.6609 0.6997 0.7051 0.7121 0.7321 0.7311 0.7319SAD0.6841 0.6792 0.6411 0.7002 0.7018 0.6988 0.7342 0.7216 0.7265TADDY0.6892 0.6983 0.6891 0.7148 0.7186 0.7177 0.7258 0.7326 0.7334AnomalyLLM 0.8288 0.8334 0.8255 0.8331 0.8319 0.8407 0.8402 0.8456 0.8447StrGNN0.6143 0.5956 0.5722 0.6113 0.7132 0.6512 0.6442 0.6724 0.6249AddGraph0.5842 0.5466 0.5647 0.6018 0.6667 0.6321 0.4642 0.5728 0.7001UCI MessageDeep Walk TGN GDN0.6198 0.6187 0.6142 0.6256 0.6263 0.6176 0.6255 0.6209 0.6197 0.6521 0.6535 0.6643 0.7098 0.7193 0.7155 0.7335 0.7365 0.7324 0.6577 0.6818 0.6611 0.7201 0.7289 0.7255 0.7493 0.7511 0.7546SAD0.6703 0.6587 0.6693 0.7102 0.7146 0.7194 0.7416 0.7453 0.7406TADDY0.6992 0.7078 0.6132 0.7204 0.7237 0.7218 0.7255 0.7278 0.7243AnomalyLLM 0.8414 0.8358 0.8368 0.8446 0.8459 0.8424 0.8488 0.8546 0.8442</p>
<p>Table 2 :
2
Alignment Fine-tuning Time of AnomalyLLM.
Pseudo Label Edges Alignment Time per Epoch (Seconds)10,00076.230,000250.7100,000801.2150,0001203.2</p>
<p>Table 3
3: Ablation ResultsDatasetMethodAnomaly Types CDA LPL HHLw/o ICL0.7406 0.7465 0.7328UCIw/o alignment 0.7849 0.7892 0.7994Messagew/o encoder0.7727 0.7883 0.7822AnomalyLLM 0.8402 0.8456 0.8447w/o ICL0.7398 0.7421 0.7396BlogCatalogw/o alignment 0.7767 0.7812 0.7726 w/o encoder 0.7821 0.7726 0.7732AnomalyLLM 0.8488 0.8546 0.8442</p>
<p>Table 4 :
4
Performance on Real-World Labeled Dataset
DatasetMethod1-shot 5-shot 10-shotAddGraph0.6126 0.6149 0.6277T-FinanceTGN GDN0.6646 0.6701 0.6865 0.6672 0.6689 0.6898SAD0.6724 0.6754 0.6876AnomalyLLM 0.8018 0.8056 0.8087AddGraph0.6116 0.6245 0.6221T-SocialTGN GDN0.6706 0.6754 0.6887 0.6694 0.6782 0.6908SAD0.6779 0.6746 0.6805AnomalyLLM 0.8101 0.8187 0.8206</p>
<p>1.2 Node Encoding.For each node    in every    within S  , , the node encoding is calculated by z = z diff (   ) + z dist (   ) + z temp (   ), where z diff (   ), z dist (   ) and z temp (   ) denotes the diffusion-based spatial encoding, the distance-based spatial encoding, and the relative temporal information, respectively.Here we introduce the calculation of the three encoding terms in detail.Diffusion-based Spatial Encoding.To encode the global information of each node, diffusion-based spatial encoding is designed based on the diffusion matrix.Specifically, we first calculate the edge connectivity vector d     , where  (‚Ä¢),  (‚Ä¢) and  (‚Ä¢) denote the index enquiring function, ranking function and learnable linear mapping, respectively.Distance-based Spatial Encoding.The distance-based spatial encoding captures the local information of each node.For each node    in the node set of a subgraph    , the distance to the target edge is encoded, which is further decomposed into the minimum value of the relative distances to    and    .Specifically, the distance-based spatial encoding is calculated as follows:
= d ùëñ + d ùëó . Then, for each node ùë£ ùúè ùëö inùëñ,ùëóùëî ùúè ùëñ , we sort all nodes of ùëî ùúè ùëñ accroding to their corresponding valueùëñ,ùëó in d ùëí ùë°:z diff (ùëß ùëö ) = ùëôùëñùëõùëíùëéùëü (ùëüùëéùëõùëò (d ùëí ùë° ùëñ,ùëó ùëö )])) ‚àà R z dist = ùëôùëñùëõùëíùëéùëü (ùëöùëñùëõ(ùëëùëñùë†ùë° (ùë£ ùúè [ùëñùëëùë• (ùë£ ùúè ùëö , ùë£ ùë° ùëñ ), ùëëùëñùë†ùë° (ùë£ ùúè ùëö , ùë£ ùë° ùëó ))) ‚àà R ùëë ùëíùëõùëê ,where ùëôùëñùëõùëíùëéùëü (‚Ä¢) is the learnable linear mapping and ùëëùëñùë†ùë° (‚Ä¢) is therelative distance computing function.</p>
<p>As for the anomaly fine-tuning, the instruction templates as well as the edge representation vector are feed to the large language model, with the complexity of  (  2  +  2 ), where  denotes the number of layers in the large language model.Four datasets are used for the evaluation, including two widely used benchmarks, i.e., UCI Message and Blog-Catalog, as well as two datasets with real anomalies, i.e., T-Finance and T-Social.The detailed statistics of these datasets are shown in Table 5.The UCI message and BlogCatalog datasets are relatively small in scale.Specifically, UCI message contains only 1,899 nodes and 59,835 edges, and BlogCatalog has 5,196 nodes and 171,743 nodes.The T-Finance and T-Social datasets are larger in scale.T-Finance has 39,357 nodes and 21,222,543 edges.The largest dataset, T-Social, has 5,781,065 nodes and 73,105,508 edges.While the UCI Message and BlogCatalog datasets lack anomaly labels, the proportion of anomaly edges in T-Finance and T-Social is 4.58% and 3.01%, respectively.These datasets provide a diverse range of graph sizes, enabling comprehensive evaluation of the proposed method.
‚Ä¢ For dynamic-aware embedding, the complexity mainly comes from calculating node features, obtaining node embeddings via Transformer block and generating subgraph encoding via GNN, whose complexity is ùëÇ (3√óùëë), ùëÇ ((2(ùêæ +1)Œì) 2 ùëë+2(ùêæ +1)Œìùëë 2 ), and ùëÇ (2(ùêæ + 1) A.4 Experiment Setting A.4.1 Dataset Statistics.
2Œì), respectively.Therefore, the overall complexity of dynamic-aware embedding is  (( + 1) 2 Œì 2  + ( + 1)Œì 2 ).‚Ä¢The reprogramming is implemented by a Transformer, whose complexity is  ( ‚Ä≤ ) +  ( ‚Ä≤  2 ) =  ( ‚Ä≤  2 ).‚Ä¢</p>
<p>Table 5 :
5
Statistics of datasets
DatasetNode Number Edge Number Anomaly (%)UCI Message189959835-BlogCatalog5196171743-T-Finance39357212225434.58T-Social5781065731055083.01A.4.2 Protocol. Due to the lack of anomaly labels in UCI Messageand BlogCatalog, three strategies are introduced to generate anom-aly edges for evaluation, i.e., Contextual Dissimilarity Anomalies(CDA), Long-Path Links (LPL) and Hub-Hub Links (HHL). The firststrategy, CDA, utilizes node2vec [10] to obtain the representationof each node in the graph, and connects the pairs of nodes with themaximum Euclidean distance as anomaly edges. Instead of consid-ering Euclidean distance in the representation space, LPL calculatesthe topological distance [8] between nodes and connects the pairs ofnodes with the farthest topological distance as the anomaly edges.The third strategy, HHL, connected pairs of hub nodes (i.e., nodeswith large degrees) with few shared neighbors as anomaly edges.</p>
<p>Table 6 :
6
Sensitivity analysis of AnomalyLLM w.r.t.different numbers of nodes in each subgraph    on the BlogCatalog.
Number2610141820AUC0.7624 0.7896 0.8389 0.8456 0.8412 0.8442</p>
<p>Table 7 :
7
Performance comparison reported in AUC measure without relying on external labeled data annomaly ratios
DatasetMethod1%5%10%TADDY BlogCatalog AnomalyLLM 0.8612 0.8651 0.9146 0.8388 0.8421 0.8844uciTADDY AnomalyLLM 0.8512 0.8633 0.9273 0.8370 0.8398 0.8912</p>
<p>Saner: Composing static and dynamic analysis to validate sanitization in web applications. Davide Balzarotti, Marco Cova, Vika Felmetsger, Nenad Jovanovic, Engin Kirda, Christopher Kruegel, Giovanni Vigna, 2008 IEEE Symposium on Security and Privacy. IEEE2008. 2008</p>
<p>Structural temporal graph neural networks for anomaly detection in dynamic graphs. Lei Cai, Zhengzhang Chen, Chen Luo, Jiaping Gui, Jingchao Ni, Ding Li, Haifeng Chen, Proceedings of the 30th ACM international conference on Information &amp; Knowledge Management. the 30th ACM international conference on Information &amp; Knowledge Management2021</p>
<p>Learning dynamic context graphs for predicting social events. Songgaojun Deng, Huzefa Rangwala, Yue Ning, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>Deep anomaly detection on attributed networks. Kaize Ding, Jundong Li, Rohit Bhanushali, Huan Liu, Proceedings of the 2019 SIAM International Conference on Data Mining. SIAM. the 2019 SIAM International Conference on Data Mining. SIAM2019</p>
<p>Graph prototypical networks for few-shot learning on attributed networks. Kaize Ding, Jianling Wang, Jundong Li, Kai Shu, Chenghao Liu, Huan Liu, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>Few-shot network anomaly detection via cross-network meta-learning. Kaize Ding, Qinghai Zhou, Hanghang Tong, Huan Liu, Proceedings of the Web Conference 2021. the Web Conference 20212021</p>
<p>Aane: Anomaly aware network embedding for anomalous link detection. Dongsheng Duan, Lingling Tong, Yangxi Li, Jie Lu, Lei Shi, Cheng Zhang, 2020 IEEE International Conference on Data Mining (ICDM). IEEE2020</p>
<p>AANE: Anomaly Aware Network Embedding For Anomalous Link Detection. Dongsheng Duan, Lingling Tong, Yangxi Li, Jie Lu, Lei Shi, Cheng Zhang, 20th IEEE International Conference on Data Mining, ICDM. 2020</p>
<p>Christopher Fifty, Jure Leskovec, Sebastian Thrun, arXiv:2310.08863Context Learning for Few-Shot Molecular Property Prediction. 2023. 2023arXiv preprint</p>
<p>node2vec: Scalable Feature Learning for Networks. Aditya Grover, Jure Leskovec, Proceedings of the 22nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining2016</p>
<p>Few-shot graph learning for molecular property prediction. Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, Nitesh V Chawla, Proceedings of the web conference 2021. the web conference 20212021</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023. 2023arXiv preprint</p>
<p>Dgraph: A large-scale financial dataset for graph anomaly detection. Xuanwen Huang, Yang Yang, Yang Wang, Chunping Wang, Zhisheng Zhang, Jiarong Xu, Lei Chen, Michalis Vazirgiannis, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, arXiv:2310.01728Time-llm: Time series forecasting by reprogramming large language models. 2023. 2023arXiv preprint</p>
<p>Service provider devops. Wolfgang John, Guido Marchetto, Felici√°n N√©meth, Pontus Skoldstrom, Rebecca Steinert, Catalin Meirosu, Ioanna Papafili, Kostas Pentikousis, IEEE Communications Magazine. 552017. 2017</p>
<p>Diffusion Kernels on Graphs and Other Discrete Input Spaces. Risi Kondor, John D Lafferty, Machine Learning, Proceedings of the Nineteenth International Conference. 2002. 2002</p>
<p>Few-shot learning for new user recommendation in location-based social networks. Ruirui Li, Xian Wu, Xian Wu, Wei Wang, Proceedings of The Web Conference. The Web Conference2020. 2020</p>
<p>Deep graph learning for anomalous citation detection. Jiaying Liu, Feng Xia, Xu Feng, Jing Ren, Huan Liu, IEEE Transactions on Neural Networks and Learning Systems. 332022. 2022</p>
<p>Anomaly detection in dynamic graphs via transformer. Yixin Liu, Shirui Pan, Yu Guang Wang, Fei Xiong, Liang Wang, Qingfeng Chen, Vincent, Lee, IEEE Transactions on Knowledge and Data Engineering. 2021. 2021</p>
<p>Relative and absolute location embedding for few-shot node classification on graph. Zemin Liu, Yuan Fang, Chenghao Liu, Steven Ch Hoi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202135</p>
<p>RGSE: Robust Graph Structure Embedding for Anomalous Link Detection. Zhen Liu, Wenbo Zuo, Dongning Zhang, Xiaodong Feng, IEEE Transactions on Big Data. 2023. 2023</p>
<p>BRIGHT-Graph Neural Networks in Real-Time Fraud Detection. Mingxuan Lu, Zhichao Han, Susie Xi Rao, Zitao Zhang, Yang Zhao, Yinan Shan, Ramesh Raghunathan, Ce Zhang, Jiawei Jiang, Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. the 31st ACM International Conference on Information &amp; Knowledge Management2022</p>
<p>A comprehensive survey on graph anomaly detection with deep learning. Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Hui Quan Z Sheng, Leman Xiong, Akoglu, IEEE Transactions on Knowledge and Data Engineering. 2021. 2021</p>
<p>Semi-supervised anomaly detection in dynamic communication networks. Xuying Meng, Suhang Wang, Zhimin Liang, Di Yao, Jihua Zhou, Yujun Zhang, Information Sciences. 5712021. 2021</p>
<p>Anomaly detection in the dynamics of web and social networks using associative memory. Volodymyr Miz, Benjamin Ricaud, Kirell Benzi, Pierre Vandergheynst, The World Wide Web Conference. 2019</p>
<p>Clustering in weighted networks. Tore Opsahl, Pietro Panzarasa, Social networks. 312009. 2009</p>
<p>The pagerank citation ranking: Bring order to the web. Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, 1998stanford UniversityTechnical report</p>
<p>In-context unlearning: Language models as few shot unlearners. Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju, arXiv:2310.075792023. 2023arXiv preprint</p>
<p>Deepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014</p>
<p>A scalable approach for outlier detection in edge streams using sketchbased approximations. Stephen Ranshous, Steve Harenberg, Kshitij Sharma, Nagiza F Samatova, Proceedings of the 2016 SIAM international conference on data mining. SIAM. the 2016 SIAM international conference on data mining. SIAM2016</p>
<p>All in One: Multi-Task Prompting for Graph Neural Networks. Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan, 2023. 2023</p>
<p>Rethinking graph neural networks for anomaly detection. Jianheng Tang, Jiajin Li, Ziqi Gao, Jia Li, International Conference on Machine Learning. PMLR2022</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023. 2023arXiv preprint</p>
<p>Relational learning via latent social dimensions. Lei Tang, Huan Liu, Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. the 15th ACM SIGKDD international conference on Knowledge discovery and data mining2009</p>
<p>Sheng Tian, Jihai Dong, Jintang Li, Wenlong Zhao, Xiaolong Xu, Bowen Song, Changhua Meng, Tianyi Zhang, Liang Chen, arXiv:2305.13573SAD: Semi-Supervised Anomaly Detection on Dynamic Graphs. 2023. 2023arXiv preprint</p>
<p>A semi-supervised graph attentive network for financial fraud detection. Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun Zhou, Shuang Yang, Yuan Qi, 2019 IEEE International Conference on Data Mining (ICDM). IEEE2019</p>
<p>A nodes' evolution diversity inspired method to detect anomalies in dynamic social networks. Huan Wang, Chunming Qiao, IEEE Transactions on Knowledge and Data Engineering. 322019. 2019</p>
<p>Contrastive meta learning with behavior multiplicity for recommendation. Wei Wei, Chao Huang, Lianghao Xia, Yong Xu, Jiashu Zhao, Dawei Yin, Proceedings of the fifteenth ACM international conference on web search and data mining. the fifteenth ACM international conference on web search and data mining2022</p>
<p>HiddenCPG: large-scale vulnerable clone detection using subgraph isomorphism of code property graphs. Seongil Wi, Sijae Woo, Joyce Jiyoung Whang, Sooel Son, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022</p>
<p>Graph neural networks in recommender systems: a survey. Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, Bin Cui, Comput. Surveys. 552022. 2022</p>
<p>Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan, arXiv:2002.07962Inductive representation learning on temporal graphs. 2020. 2020arXiv preprint</p>
<p>Xiongxiao Xu, Kaize Ding, Canyu Chen, Kai Shu, arXiv:2305.10668MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection. 2023. 2023arXiv preprint</p>
<p>Hvgrae: A hierarchical stochastic spatial-temporal embedding method for robust anomaly detection in dynamic networks. Chenming Yang, Liang Zhou, Hui Wen, Zhiheng Zhou, Yue Wu, arXiv:2007.069032020. 2020arXiv preprint</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023. 2023arXiv preprint</p>
<p>Netwalk: A flexible deep embedding approach for anomaly detection in dynamic networks. Wenchao Yu, Wei Cheng, C Charu, Kai Aggarwal, Haifeng Zhang, Wei Chen, Wang, Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018</p>
<p>DoubleAdapt: A Metalearning Approach to Incremental Learning for Stock Trend Forecasting. Lifan Zhao, Shuming Kong, Yanyan Shen, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>AddGraph: Anomaly Detection in Dynamic Graph Using Attention-based Temporal GCN. Li Zheng, Zhenpeng Li, Jian Li, Zhao Li, Jun Gao, IJCAI. 20193</p>
<p>WinGNN: Dynamic Graph Neural Networks with Random Gradient Aggregation Window. Yifan Zhu, Fangpeng Cong, Dan Zhang, Wenwen Gong, Qika Lin, Wenzheng Feng, Yuxiao Dong, Jie Tang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>            </div>
        </div>

    </div>
</body>
</html>
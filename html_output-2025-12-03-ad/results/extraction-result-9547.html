<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9547 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9547</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9547</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273374947</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.12130v1.pdf" target="_blank">Iter-AHMCL: Alleviate Hallucination for Large Language Model via Iterative Model-level Contrastive Learning</a></p>
                <p><strong>Paper Abstract:</strong> The development of Large Language Models (LLMs) has significantly advanced various AI applications in commercial and scientific research fields, such as scientific literature summarization, writing assistance, and knowledge graph construction. However, a significant challenge is the high risk of hallucination during LLM inference, which can lead to security concerns like factual inaccuracies, inconsistent information, and fabricated content. To tackle this issue, it is essential to develop effective methods for reducing hallucination while maintaining the original capabilities of the LLM. This paper introduces a novel approach called Iterative Model-level Contrastive Learning (Iter-AHMCL) to address hallucination. This method modifies the representation layers of pre-trained LLMs by using contrastive `positive' and `negative' models, trained on data with and without hallucinations. By leveraging the differences between these two models, we create a more straightforward pathway to eliminate hallucinations, and the iterative nature of contrastive learning further enhances performance. Experimental validation on four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen) finetuning with a specially designed dataset shows that our approach achieves an average improvement of 10.1 points on the TruthfulQA benchmark. Comprehensive experiments demonstrate the effectiveness of Iter-AHMCL in reducing hallucination while maintaining the general capabilities of LLMs.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9547",
    "paper_id": "paper-273374947",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00371775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ITER-AHMCL: ALLEVIATE HALLUCINATION FOR LARGE LANGUAGE MODEL VIA ITERATIVE MODEL-LEVEL CONTRASTIVE LEARNING A PREPRINT
October 17, 2024</p>
<p>Huiwen Wu huiwen0820@outlook.com 
Xiaohan Li xiaohan@zhejianglab.com 
Xiaogang Xu xiaogangxu00@gmail.com 
Jiafei Wu wujiafei@zhejianglab.com 
Deyi Zhang 
Zhe Liu zhe.liu@zhejianglab.com </p>
<p>Zhejiang Laboratory Hangzhou
ZhejiangChina</p>
<p>Zhejiang Laboratory Hangzhou
ZhejiangChina</p>
<p>The Chinese University of Hong Kong Zhejiang University Hangzhou
ZhejiangChina</p>
<p>Zhejiang Laboratory Hangzhou
ZhejiangChina</p>
<p>Zhejiang Laboratory Hangzhou
ZhejiangChina</p>
<p>Zhejiang Laboratory Hangzhou
ZhejiangChina</p>
<p>ITER-AHMCL: ALLEVIATE HALLUCINATION FOR LARGE LANGUAGE MODEL VIA ITERATIVE MODEL-LEVEL CONTRASTIVE LEARNING A PREPRINT
October 17, 20249CC98F221B82568181C01A314F4BBFE2arXiv:2410.12130v1[cs.CL]
The development of Large Language Models (LLMs) has significantly advanced various AI applications in commercial and scientific research fields, such as scientific literature summarization, writing assistance, and knowledge graph construction.However, a significant challenge is the high risk of hallucination during LLM inference, which can lead to security concerns like factual inaccuracies, inconsistent information, and fabricated content.To tackle this issue, it is essential to develop effective methods for reducing hallucination while maintaining the original capabilities of the LLM.This paper introduces a novel approach called Iterative Model-level Contrastive Learning (Iter-AHMCL) to address hallucination.This method modifies the representation layers of pre-trained LLMs by using contrastive 'positive' and 'negative' models, trained on data with and without hallucinations.By leveraging the differences between these two models, we create a more straightforward pathway to eliminate hallucinations, and the iterative nature of contrastive learning further enhances performance.Experimental validation on four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen) finetuning with a specially designed dataset shows that our approach achieves an average improvement of 10.1 points on the TruthfulQA benchmark.Comprehensive experiments demonstrate the effectiveness of Iter-AHMCL in reducing hallucination while maintaining the general capabilities of LLMs.</p>
<p>Introduction</p>
<p>The development of large language models (LLMs) has led to impressive successes in a wide range of artificial intelligence (AI) applications, from commercial usage like GPT-4 [1] to scientific research fields [32].The adoption of AI technologies, especially LLMs, is leading us into a groundbreaking era for scientific research.LLMs technology has unlocked a wide range of possibilities in scientific fields, from summarizing scientific literature reviews [20,17,3] to assisting in scientific writing [23,2,21] and constructing knowledge graphs [26,9,39].</p>
<p>Especially, we designed a new approach called Iterative Model-level Contrast Learning (Iter-AHMCL), and the vital characteristic is the formulation of model guidance.First, we construct positive and negative data using corresponding templates.Next, we pre-train positive and negative guidance models based on the general vector-guidance-based representation editing method [46].The goal of the positive model is to exhibit a favorable bias in hallucination evaluation by achieving a high score, while the negative model is trained to show the opposite bias.We then use these pre-trained positive and negative models as guidance to edit the representation layer, effectively controlling the model's tendency toward hallucination.Furthermore, we recognize the importance of adaptively updating the models that provide guidance.Better performance is achieved by evolving the LLM in tandem with the guidance models.To this end, we design a model-level iterative strategy, where the positive model is updated with one that performs better in hallucination evaluation, and the negative model is updated with one that performs worse.By leveraging the differences between these two models, we create a more direct pathway to reduce hallucinations.This iterative approach combined with contrastive learning further enhances overall performance.</p>
<p>In addition to improving performance in addressing hallucinations, our proposed Iter-AHMCL utilizes representation editing to adjust only the model's preferences related to hallucination problems, with minimal impact on its original capabilities.This is because the guidance in Iter-AHMCL is highly aligned with the direction relevant to hallucinations, while remaining orthogonal to other knowledge areas.We validate this claim through comprehensive evaluations from multiple perspectives.The overall procedure of Iter-AHMCL is illustrated in Figure 1.</p>
<p>To summarize, our contributions are listed as follows.</p>
<p>• We introduce a novel approach, called Iter-AHMCL, to eliminate hallucination in LLMs while preserving their general capabilities.In Iter-AHMCL, we offer a new perspective by adaptive developing models with positive and negative feature representations, implementing model-level contrastive learning guided by these models.• We implement an iterative approach to update the guidance model and establish model-level guidance.This iterative strategy is broadly applicable to various LLMs.The code and all models will be released to publication.• We conduct comprehensive experiments with various LLM models, and the evaluation results demonstrate that our method effectively reduces hallucinations while preserving general capabilities.</p>
<p>2 Related Work</p>
<p>Hallucination Reduction in LLMs</p>
<p>Hallucinations in LLMs occur when the model generates inaccurate or fictitious information, diverging from factual knowledge and occasionally producing responses that are not grounded in its training data [27].Several studies have sought to mitigate the hallucination phenomenon in the training and inference of LLMs [16,34,31].For instance, [16] analyzes hallucinations in medical generative QA systems using widely adopted LLMs and datasets.This work focuses on identifying and understanding problematic answers, presenting an interactive self-reflection methodology that enhances the factuality, consistency, and entailment of generated responses through a feedback process.[27] investigates the text generation mechanisms of LLMs to understand and mitigate hallucinations, while also offering insights into the training algorithms and effective utilization of these models.[25] employs a knowledge distillation methodology to reduce hallucinations in LLMs by transferring knowledge from a high-capacity teacher model to a compact student model.[34] introduces an innovative hallucination metric that assesses factuality by weighting answers from off-the-shelf LLMs as proxies for gold-standard answers, with the primary challenge being the quantification of the reference LLMs' expertise.[13] proposes a framework that utilizes a small language model for initial hallucination detection and a LLM for detailed explanations, optimized with prompting techniques to align their outputs.Although these methods reduce hallucinations in LLMs to a certain extent, existing methods focus more on detecting and measuring hallucinations.</p>
<p>Representation Editing</p>
<p>Representation editing is a technique for modifying a model's preferences or performance by altering its trained representations.It is widely used in both traditional machine learning (ML) [33] and the rapidly advancing LLMs [35,45,44].For example, in a machine learning scenario, [33] proposes a flexible unsupervised text attribute transfer framework that utilizes a Transformer-based autoencoder to learn latent representations and employs the Fast Gradient Iterative Modification algorithm to edit these representations until they align with the target attribute.In the context of LLMs, [45] introduces a method for controlling the model's preferences through dedicated alignment of the selected layer representations.[35] extends the representation editing method to jailbreak attacks and hallucination control scenarios.[44] accomplishes concept editing through adversarial representation engineering.[36] proposes Representation Editing (RED), a novel fine-tuning approach that modifies neural model representations, significantly reducing the number of trainable parameters while achieving results comparable to or exceeding those of full fine-tuning and other parameter-efficient fine-tuning (PEFT) methods.[19] pre-trains a multimodal encoder to produce text-aligned visual representations and designs a subject representation learning task that enables a diffusion model to generate new subject renditions using these representations.Although [35,45,44] present insightful approaches to concept editing through representation alignment, the performance in reducing hallucinations can be further enhanced.</p>
<p>Contrastive Learning</p>
<p>Contrastive learning is a self-supervised learning technique designed to learn useful representations of data by contrasting positive and negative samples.The core idea is to bring the representations of similar positive pairs closer together while pushing apart the representations of dissimilar pairs.This technique is widely applied to tasks such as image classification, especially when labels are scarce or costly to obtain.In [18], the author leverages the power of contrastive learning in supervised settings by bringing together points belonging to the same class in the embedding space while separating clusters of samples from different classes.In [10], the author introduces a momentum contrast method for unsupervised visual representation learning, viewing contrastive learning as a dictionary lookup.This approach involves building a dynamic dictionary using a queue and a moving average encoder to create an extensive and consistent dictionary.[42] proposes a graph contrastive learning framework for learning unsupervised representations of graph data across four settings: semi-supervised, unsupervised, transfer learning, and adversarial attacks.[37] extends graph contrastive learning to heterophilic graphs, where connected nodes have different class labels and features, by employing an asymmetric view of neighboring nodes.[5] presents a straightforward framework for contrastive learning of visual representations by introducing a learnable non-linear transformation between the representation and the contrastive loss.</p>
<p>[41] introduces a decoupled contrastive learning loss that removes the positive term from the denominator, significantly enhancing learning efficiency.Recent research has applied contrastive learning to enhance LLMs for tasks such as few-shot text classification [43], unified representation extraction [24], and machine translation [38].However, despite the widespread use of contrastive learning in traditional machine learning tasks, the primary challenge in adapting contrastive learning to LLMs lies in selecting appropriate positive and negative pairs.Inappropriate choices may result in suboptimal representations.</p>
<p>Methodology</p>
<p>In this section, we describe the main procedure of Iter-AHMCL.Throughout the paper, we use the following notations.{T, T + , T − } denotes the triplet consisting of the neural data, positive data, and negative data.T represents the set of them.M means the model to be fine-tuned.M + is the positive guidance model, while M − is the negative guidance model.{R, R + , R − } denotes the triplet consisting of the neural, positive, and negative feature representations.i is employed to indicate the iteration step in CL-IMG.M + i represents the updated positive guidance model at step i.R + i and R − i denote the representation of updated positive and negative guidance representation at step i.</p>
<p>Motivation</p>
<p>In recent research [45,44], the authors present a fine-tuning method to control model preferences through representation alignment, applying this technique to generate non-harmful and trustworthy responses.In the study by [45], the authors extract partial model layers to obtain representations and analyze the intermediate features of various concepts, such as honesty, fairness, and harmlessness.This enables them to edit and control the behavior of an LLM by directing a representation vector generated within the internal hidden layers.For example, when faced with a truthful question, the vector can be altered in two directions to influence the final answer: one direction to generate a more truthful response, and the other to develop a less truthful response.In this way, by editing the intermediate layer representations, one can alter the preference of the final output.The key is to find better guidance directions for the positive and negative components.In [45], the authors use a contrast vector-defined as the difference between representations generated by positive input data and those generated by negative data-to guide the direction.In [44], the authors train a discriminator to differentiate between positive and negative directions at the intermediate layers.However, these methods, which rely on sample-level guidance vectors, often depend heavily on carefully tuned hyperparameters and may fail to generalize to unseen data, leading to issues such as overfitting.To address this, we propose training two models as guidance, better learning the positive and negative directions in feature representations.The main objective is to enhance the separation between these directions, which can more effectively align with safety requirements while preserving the original capabilities of the model.</p>
<p>Contrast Triple Construction (CTC)</p>
<p>First, we define the contrast templates for fine-tuning the LLM.The fine-tuning data for Iter-AHMCL are constructed from the PKU-SafeRLHF dataset [7], available on Hugging Face.Each sample in the original training data consists of one prompt and two responses, along with labels.The label takes boolean values, indicating whether a response is safe or not.The existing sample T = {Ins, Res} contains two elements where Ins denotes the instruction and Res represents the response.We need to construct positive and negative instance pairs, which are crucial for the second phase of contrastive learning.Let Pos be the prompt of 'Please give a truthful answer' and Neg be the negative prompt of 'Please give an untruthful answer'.The positive instance is constructed by T + = {Ins, Pos, Res} while the negative instance is formulated as T − = {Ins, Neg, Res}.The input data of Iter-AHMCL is the triple {T, T − , T + }.We present an illustrative example in Figure 2.</p>
<p>Contrastive Learning with Contrasted Triple (CL-CT)</p>
<p>In this section, we elaborate on the strategy of CL-CT, the basic method for establishing guidance directions within the feature representation space.The input data is a triplet training sample, {T, T − , T + }, consisting of the original sample T, the sample with a positive template T + , and the sample with a negative template T − , as constructed in Sec.3.2.Using the provided frozen model M, we follow existing work [45] to select several layers for representation extraction, while utilizing other layers to perform the fine-tuning.In other words, we pass the data triplet through the frozen model M, specifying certain layers to obtain a triplet of representations {R, R + , R − }, where R = M(T) is the representation of the original data, R + = M(T + ) is the representation of the data with the positive template, and R − = M(T − ) is the representation of the data with the negative template.To enhance the LLM's ability to distinguish between positive and negative samples, we diverge from the approach in [45], which constructs the loss solely as the ℓ 2 distance between the positive and negative representations (Eq.( 1)).We further incorporate terms for the ℓ 2 distance between the positive and neutral representations, as well as between the negative and neutral ones.The goal is to enlarge the ℓ 2 distance between the neural representation R and the negative representation R − (Eq.( 3)) while eliminate the ℓ 2 distance between the original representation R and the positive representation R + (Eq.( 2)).
L LoRRA = ∥M(T + ) − M(T − )∥ 2 ;
(1)
L + = ∥M(T) − M(T + )∥ 2 ;
(2)
L − = ∥M(T) − M(T − )∥ 2 .
(
)3
Combine with the original LoRRA loss presented in [45], the loss function of CL-CT, denoted as L 1 , is
L 1 = L LoRRA + αL + − βL − ,(4)
where α and β are small non-negative constants.While this loss function has demonstrated improved effectiveness compared to the original LoRRA loss, we further amplify the influence of the guidance direction by proposing the development of a guidance model, which aids in extracting more accurate positive and negative directions.</p>
<p>Guidance Model Pre-training (GMP)</p>
<p>Before introducing the formulation of the new learning function, it is essential to elaborate on the training of the guidance model, which serves as a vital component.Therefore, in this section, we will discuss the pre-training procedure of the guidance model.To obtain better guidance, we train one positive guidance model and one negative guidance model, thereby enhancing the effectiveness of contrastive learning in CL-CT as presented in Sec.3.3.The pre-training data consists of two sub-datasets derived from the PKU-SafeRLHF datasets [7].The positive model M + is trained with the goal of reducing hallucinations.Thus, the training loss is defined the same way as the representation editing loss in Eq. ( 4).However, the goal of the negative model M − is to diminish its ability to generate responses to hallucination-related questions.Therefore, it has a negative objective compared to the editing loss and the positive guidance model training loss.The training loss for the negative guidance model is defined as in Eq. ( 5).The only difference lies in the coefficient terms for L + and L − .We set the coefficient for the positive ℓ 2 distance to be negative, while the coefficient for the negative ℓ 2 distance is set to positive, thereby increasing hallucination responses and creating a contrary model.
L 2 = L LoRRA − αL + + βL − ,(5)
where α and β are non-negative constants.</p>
<p>The formulation of M + and M − .The training strategy employs LoRA [12], which focuses on optimizing the low-rank components of each attention matrix.After completing the pre-training of the two models, we can obtain two adapters designed to provide positive and negative guidance.We then integrate these adapters with the frozen model M to create the positive guidance model M + and the negative guidance model M − .</p>
<p>Constrastive Learning with Model Guidance (CL-MG)</p>
<p>In this section, we discuss the application of the guidance model in contrastive learning.After obtaining the positive guidance model M + and the negative guidance model M − in Sec.3.4, we utilize them to generate representations for the guidance loss.Specifically, the positive representation is produced by the positive guidance model using the data sample with a positive template, expressed as R + = M + (T + ).In contrast, the negative representation is generated by the negative guidance model using the negative data sample, represented as R − = M − (T − ).Compared with the R + and R − generated by M(T + ) and M(T − ), the difference between M + (T + ) and M − (T − ) is more accurate to indicate the alignment direction of hallucination, since M + and M − are pre-trained to be more sensitive to the existence of hallucination.Thus, when computing the editing loss shown in Eq. ( 4), we modify the model used to generate the representation, and the loss function of CL-MG can be written as
L + M G (T, T + ) = ∥M(T) − M + (T + )∥ 2 ;(6)L − M G (T, T − ) = ∥M(T) − M − (T − )∥ 2 .(7)
Thus, the overall loss function for CL-MG is
L M G = L LoRRA + αL + M G − βL − M G ,(8)
where α and β are small non-negative loss weights.</p>
<p>Contrastive Learning with Iterative Model Guidance (CL-IMG)</p>
<p>After establishing CL-MG, we observe that the guidance model can be further improved.Therefore, in this section, we outline the iterative process for updating the pre-trained M + with more effective guidance models.This strategy is called CL-IMG.The long-term fine-tuning with CL-IMG is conducted using a continual learning strategy, incorporating feature editing training with an improved pre-trained guidance model.Since the positive training models and CL-MG share the same training loss and methodology, we iteratively update the positive models using the newly obtained best models from CL-MG.With this update, the loss function in the i-th round is defined as
L + Iter (i, T, T + ) = ∥M(T) − M + i (T + )∥ 2 ;(9)L − Iter (i, T, T − ) = ∥M(T) − M − (T − )∥ 2 ,(10)
where M + i is the positive model updated after i rounds.Meanwhile, the overall loss for contrastive learning with iterative model guidance is denoted as
L Iter = L LoRRA + αL + Iter − βL − Iter ,(11)
where α and β are small non-negative constants.</p>
<p>To summarize, we consolidate all the previously discussed components and describe the Iter-AHMCL in Algorithm 1.</p>
<p>Our method encompasses several key steps: 1.Data Preparation: We construct sets of contrasting data and use this data to pre-train the positive and negative guidance models.2. Guidance Model Utilization: We leverage the pre-trained guidance models to adjust the direction of the intermediate representations during the fine-tuning process.3. Iterative Improvement: To enhance fine-tuning performance, we iteratively update the guidance model, ensuring it continuously adapts and improves, thereby maintaining peak performance while demonstrating flexibility and resilience.</p>
<p>It is important to highlight that the objective of Algorithm 1 is to reduce hallucination.Our goal is to develop a more effective positive model throughout the training procedure of Iter-AHMCL.Consequently, we adopt an asymmetric approach to iteratively update the pre-trained models: we focus on updating the positive guidance model while keeping the negative guidance model unchanged.L LoRRA = ∥R + − R − ∥ 2 ; ▷ LoRRA loss 12:</p>
<p>L + Iter = ∥R + i − R∥ 2 ;</p>
<p>13:</p>
<p>L − Iter = ∥R − i − R∥ 2 ;</p>
<p>14:
L Iter = L LoRRA + αL + Iter −</p>
<p>Experimental Analysis</p>
<p>In this section, we present the main results from comprehensive experiments to demonstrate the efficiency and effectiveness of our methods Iter-AHMCL.Through the experimental analysis, we aim to answer the following research questions:</p>
<p>• RQ.  [7] and the Alpaca-instruction dataset [29].The PKU-SafeRLHF dataset contains 83,400 samples, while the Alpaca-instruction dataset comprises 52,000 samples.</p>
<p>II) Foundation Model Choice.Alpaca-native (Alpaca) [29] is an enhanced version of the LLaMa1-7B model, fine-tuned on synthetic data, developed by the Stanford team.LlaMA2-chat-hf (LLaMA2) [30] is an open-source collection of pre-trained and fine-tuned LLMs ranging in scale from 7B to 70B parameters, released in July 2023 by Meta.LlaMa3-chat-8b (LLaMA3) [8] is a suite of language models that natively support multilingual capabilities,  III) Compared Methods. 1) Foundation models refer to the original models downloaded from Hugging Face [15] without any further fine-tuning.2) LoRRA [45] provides a method for editing representations using contrast vectors to enhance the model's ability to distinguish between positive and negative directions. 3) Pure Model Guidance (Pure-MG) fine-tunes the models using a loss derived from pure positive and negative model guidance
L pure = αL + M G − βL − M G
. IV) Hyper-parameters.We present the hyper-parameters for GMP (Sec.3.4) and Iter-AHMCL (Sec.3.6) in Table 1.</p>
<p>V) Evaluation Methods. 1) TruthfulQA [22] is a benchmark designed to assess the accuracy and truthfulness of LLMs based on their generated responses to questions.The benchmark consists of 817 questions covering 38 diverse categories, including health, law, finance, and politics.We utilize the TruthfulQA [22] benchmark for evaluating hallucinations.According to the guidelines of TruthfulQA [22], we selected MC1 (Single-true) to evaluate the LLM model's capacity to identify factual statements.In MC1, the LLM is presented with a question and 4-5 answer choices.It undergoes a rigorous process to select the most probable correct answer, ensuring a thorough evaluation.The likelihood of each selection is computed independently, and the completion with the highest log probability is chosen.The reported score reflects the accuracy across all questions, with higher scores indicating better performance in reducing hallucinations.2) MMLU [11], which stands for Measuring Massive Multitask Language Understanding, serves as a benchmark for assessing the performance of language models.This benchmark comprises approximately 16,000 multiple-choice questions spanning 57 academic disciplines, including mathematics, philosophy, and medicine.3) C-Eval [14] is a comprehensive Chinese evaluation system designed to assess the advanced knowledge and reasoning skills of foundational models within a Chinese context.The system includes an extensive set of 13,948 multiple-choice questions across 52 distinct fields, covering various educational stages.We utilize six conventional subject categories: STEM, social sciences, humanities, other, average, and Avg (hard).The Avg (hard) category represents the mean score of the C-Eval hard benchmark, which includes subjects such as advanced mathematics, discrete mathematics, and college chemistry, all of which require significant reasoning skills for resolution.4) Qwen API [4] evaluates the model's performance in terms of accuracy, coherence, safety, and usability for real-world applications.In the evaluation results, 'Gold-Ref' refers to scores assigned to standard responses, 'Relevance' identifies significant content, 'Fluency' focuses on sentence quality, 'Coherence' assesses structure and logic, and 'Consistency' checks for factual agreement.</p>
<p>Pre-training Effect of Guidance (RQ.1)</p>
<p>In this section, we present the pre-training performance of both the positive and negative guidance models.The training data for the guidance models is derived from the PKU-SafeRLHF datasets [7].The loss function is constructed as shown in Eq. ( 8).Specifically, we set α = 10 and β = 1 for training the positive guidance model, and α = 1 and β = 10 for the negative one.We present the convergence behavior of the two models during pre-training in Figure 5 (Upper) and evaluate their performance using TruthfulQA [22] in Table 2.</p>
<p>Effect of Positive and Negative Representation (RQ.1)</p>
<p>To demonstrate the effect of the differences between positive and negative representations, we randomly sampled 500 triples from the contrastive dataset and recorded the ℓ 2 distance between the original representation and the positive representation, as well as the original representation and the negative one.We present the statistics of the recorded 500   3, with L + and L − defined in Eq. ( 2) and Eq. ( 3), respectively.The row labeled 'Mean' denotes the average ℓ 2 distance, while the row labeled 'Std.' indicates the standard deviation of the ℓ 2 distance values.Furthermore, we visualize the data distribution in Figure 5 (Lower).From the visualization, we observe that the ℓ 2 norms of positive and negative representations show significant differences in terms of their distributions.Furthermore, we compute the KL divergence KL(P, N ) = x∈X P (x) log (P (x)/N (x)) between the two ℓ 2 -norm vectors to verify this observation.</p>
<p>TruthfulQA Evaluation (RQ.2)</p>
<p>We present the evaluation results of the TruthfulQA Benchmark [22] for measuring the hallucination of trained models in Table 4. From Table 4, it is evident that our method, Iter-AHMCL, exhibits a significant improvement in the MC1 score based on the TruthfulQA evaluation compared to both the foundation models and the existing LoRRA [45] method.Specifically, for the LLaMA2 foundation model, we improve the MC1 score by up to 19.43 points compared to the foundation model and by 3.82 points compared to LoRRA.A consistent improvement can also be observed for the Alpaca and LLaMA3 models.Notably, for the Alpaca model, the LoRRA method decreases the MC1 score, while our method increases it by up to 3.38 points.In conclusion, Iter-AHMCL demonstrates efficient and consistent improvement in the hallucination reduction task across the three foundation models.</p>
<p>Knowledge Evaluation (RQ.3)</p>
<p>To answer RQ.3, we present the knowledge evaluation of the model fine-tuned with our method, Iter-AHMCL, in comparison to the foundation models and the LoRRA models based on the benchmarks of the MMLU [11] and C-Eval [14] datasets.Both MMLU and C-Eval are designed to evaluate the model's performance across a wide range of subjects.MMLU focuses on multiple-choice questions in various academic disciplines, while C-Eval targets a comprehensive set of tasks that include both multiple-choice and open-ended questions to assess the models' capabilities in different linguistic and knowledge domains.The results for MMLU and C-Eval are presented in Figure 4. We observe that Iter-AHMCL exerts no significant negative influence on the knowledge evaluation, demonstrating the model capability-preserving property.In this section, we discuss the use of the QWen LLM API for evaluation [4].The process involves utilizing foundational or fine-tuned models to generate responses for CNN-DailyMail News Text Summarization [6].Answer sheets are created from the model outputs and standardized answers, which are then input into the Qwen API.The API evaluates the answer sheets and provides results, which are subsequently analyzed statistically.The evaluation encompasses four aspects: Relevance, Fluency, Coherence, and Consistency.</p>
<p>We present the evaluation results in Table 5, based on the Qwen API.Our ongoing analysis of the Qwen scores, as shown in Table 5, begins by noting that the Gold-Ref achieves the highest scores across all four perspectives, reflecting the tailored evaluation methodology of Qwen.Furthermore, our method, Iter-AHMCL, consistently delivers stable results in all evaluation dimensions, slightly outperforming the Foundation models but not reaching the level of the Gold-Ref.This section demonstrates the benefits of iterative model guidance in addressing RQ.4.We take Iter-AHMCL applied to the foundation model Alpaca as an example.The training procedure is described in Algorithm 1.All training is conducted with a maximum of 1,250 iteration steps.The detailed iterative process of Iter-AHMCL and its improvements on the TruthfulQA Evaluation are shown in Table 6.By updating the positive guidance model, we iteratively enhance the MC1 score of the fine-tuned model.The long-term iterative process is illustrated in Figure 6, where the x-axis represents the training steps and the y-axis represents the MC1 score evaluated using TruthfulQA.From Figure 6, we observe that the score improves progressively, with oscillations occurring at the interchange points of the guidance model.The iterative updates of the positive guidance model may temporarily degrade the model's performance around the turning points, but they ultimately contribute to long-term improvements in the MC1 score.Additionally, we present the evaluation of checkpoints for three foundation models trained with Iter-AHMCL in Figure 7 (Lower), which demonstrates consistent improvements across different foundation models.</p>
<p>Transferability (RQ.5)</p>
<p>In this section, we address RQ.5: whether a pre-trained guidance model based on one foundation model can be transferred for the CL-MG/CL-IMG learning to another foundation model.To explore this, we conducted experiments using a positive guidance model trained on the foundation model LLaMA2 and Iter-AHMCL trained on Alpaca.The TruthfulQA Evaluation MC1 scores are presented in the last row of Table 6.In this experiment, the positive guidance model used is Iter-AHMCL-LLaMA2 at iteration 6, with the negative guidance model remaining the same as in other experiments.The transfer experiments yield performance comparable to the first iteration of Iter-AHMCL using a positive guidance model tuned from a homogeneous guidance model, demonstrating the transferability.</p>
<p>Conclusion</p>
<p>In our paper, we introduce a novel method called Iter-AHMCL, designed to reduce hallucination in LLM models while preserving their overall performance.This approach involves fine-tuning the representations at specific layers to enhance the model's capabilities through constructive learning.Unlike existing strategies that rely on sample-level contrasts, we propose formulating guidance at the feature representation level using specifically trained positive and negative model guidance.This allows us to establish contrasts at both the sample and model levels.Furthermore, this contrastive learning approach can be conducted iteratively by continuously updating the positive guidance model.Our comprehensive experiments, which include evaluations of hallucination and language ability, demonstrate the efficiency and effectiveness of our proposed methods.In the future, we plan to investigate the transferability of Iter-AHMCL in the cross-domain scenarios.</p>
<p>Figure 1 :
1
Figure 1: Overall Procedure of Iter-AHMCL.</p>
<p>Figure 2 :
2
Figure 2: The illustration of Contrast Triple Construction.</p>
<p>Figure 3 :
3
Figure 3: The illustration for computing the loss function of CL-MG.</p>
<p>Algorithm 1 4 :
14
Iter-AHMCL(M 0 , T , N, B, M + 0 , M − 0 , α, β) Require: M 0 : Original LLM model, T = {(T, T + , T − )}: constructed contrast triple set, N : maximum iteration step, B: batch size, M + 0 : pre-trained positive guidance model, M − 0 : pre-trained negative guidance model, α, β: hyper-parameters; 1: Initialize with pre-trained foundation model M = M 0 ; 2: Initialize the positve and negative guidance models M + = M + 0 and M − = M − 0 ; 3: loop N times Samples a batch B with a batch size of B from Triple Set T ; 5: for (T + , T, T − ) ∈ B do</p>
<p>Figure 4 :
4
Figure 4: Radar Plots of MMLU and C-Eval Evaluation Results.From the Left to Right are C-Eval-LLaMA2, C-Eval-Alpaca, C-Eval-LLaMA3, MMLU-LLaMA2, MMLU-Alpaca, and MMLU-LLaMA3, respectively.</p>
<p>Figure 5 :
5
Figure 5: Pre-training Loss of Positive and Negative Models (Left) and Violin Plots of ℓ 2 Distance for Positive and Negative Representation (Right).</p>
<p>Figure 6 :
6
Figure 6: Iterative Process of Model Guidance on Foundation Model LLaMA2.</p>
<p>Figure 7 :
7
Figure 7: Bar Compare of TruthfulQA [22] (Upper) and Checkpoints Evaluation during Iter-AHMCL (Lower).</p>
<p>Table 1 :
1
βL − Iter ; Hyperparameters Details of GMP and MGRE.
15:end for16: 17: 18:Update the positive model M + i+1 = M best ; Update iteration step i+ = 1;19: end loop N timesEnsure: Loss to be optimized.PhaseαβTarget LayersT 20} 12501010 −381632MGRE10.01.0{10, 12, 14, 16, 18, 20} 12501010 −381632
[22]uate the fine-tuned model M with TruthfulQA[22]and record the best one as M best ; max # Evaluation Step Learning Rate γ LoRA rank r Train Batch Size Eval Batch Size GMP {10.0, 1.0} {1.0, 10.0} {10,12, 14, 16, 18,</p>
<p>1.GMP Training Procedure.How does the training of GMP perform, and what is the divergence between positive and negative representations?• RQ.2.Hallucination Reduction Effect.How does Iter-AHMCL reduce the hallucination of an LLM model?• RQ.3.General Capability Preservation.How does Iter-AHMCL preserve the model's knowledge and general language ability?• RQ.4.Iterative Process Benefits.How does the iterative procedure help LLMs reduce hallucination through representation editing?• RQ.5.Transferability of Guidance Model.Does the guidance model have transferability from one LLM foundation model to another?We prepare two datasets for the overall training of Iter-AHMCL with the PKU-SafeRLHF dataset
4.1 Experimental SettingsI) Data Preparation.</p>
<p>Table 2 :
2
Performance of the Positive and Negative Model Trained in GMP.
model MC1 ↑ MC2 ↑ Mean(MC1, MC2) ↑M +0.2448 0.37770.3113M −0.2583 0.39020.3242</p>
<p>Table 3 :
3
Statistics of ℓ 2 Distance and its KL Divergence.
L +L −Mean30.938832.4959Std.21.184016.3295KL(P, N ) KL(N, P )KL Divergence0.01630.0162</p>
<p>Table 4 :
4
[45]Score with TruthfulQA Evaluation[22]of Iter-AHMCL Compared to Foundation Models and LoRRA[45].
MethodsLlaMA2 [30] Alpaca [29] LLaMA3 [8]Foundation0.31850.28070.2166LoRRA [45]0.47360.23370.2680Iter-AHMCL0.51280.31450.2705pairs of ℓ 2 distances in Table</p>
<p>Table 5 :
5
[4]n API[4]Evaluation Results.
MethodsRelevance ↑Fluency ↑Coherence ↑ Consistency ↑LLaMA2 [30]Gold-Ref3.84 ± 0.44 4.97 ± 0.17 3.96 ± 0.344.69 ± 0.48Foundation2.51 ± 0.73 3.38 ± 0.90 2.34 ± 0.762.94 ± 0.98LoRRA2.55 ± 0.74 3.50 ± 0.95 2.41 ± 0.832.99 ± 1.02Iter-AHMCL 2.60 ± 0.73 3.50 ± 0.92 2.44 ± 0.803.10 ± 0.98Alpaca [29]Gold-Ref3.91 ± 0.49 4.97 ± 0.17 4.04 ± 0.344.75 ± 0.46Foundation2.47 ± 0.84 3.10 ± 1.04 2.20 ± 0.772.96 ± 1.06LoRRA2.41 ± 0.78 3.08 ± 1.05 2.17 ± 0.752.92 ± 1.06Iter-AHMCL 2.58 ± 0.80 3.29 ± 1.07 2.34 ± 0.843.12 ± 1.06LLaMA3 [8]Gold-Ref3.78 ± 0.52 4.98 ± 0.14 3.99 ± 0.304.68 ± 0.49Foundation2.57 ± 0.74 3.60 ± 0.69 2.33 ± 0.713.21 ± 0.90LoRRA2.55 ± 0.77 3.59 ± 0.74 2.31 ± 0.733.18 ± 0.94Iter-AHMCL 2.53 ± 0.75 3.58 ± 0.74 2.30 ± 0.713.17 ± 0.91</p>
<p>Table 6 :
6
Benefits of Iterative Model Guidance with Alpaca as Foundation Model.
iM + iM + i−1M −LLaMA2 [30] Alpaca [29] LLaMA3 [8]0Foundation∅∅0.31450.20070.21661Iter-AHMCL-0GMP-PositiveGMP-Negative0.47360.24470.25822Iter-AHMCL-1Iter-AHMCL-0GMP-Negative0.48100.25090.26803Iter-AHMCL-2Iter-AHMCL-1GMP-Negative0.49080.25820.27054Iter-AHMCL-3Iter-AHMCL-2GMP-Negative0.51280.31450.27051 Iter-AHMCL-Transfer Iter-AHMCL-4-LLaMA2 GMP-Negative-0.2472-4.7 Iterative Process of Model Guidance (RQ.4)</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Chatgpt or gemini: Who makes the better scientific writing assistant. Faiza Hatoon S Alsagri, Shahab Farhat, Abdul Saquib Sohail, Jilani Khader, Saudagar, Journal of Academic Ethics. 2024</p>
<p>Using llm (large language model) to improve efficiency in literature review for undergraduate research. Ahmed Shouvik, Haiyan Antu, Cindy K Chen, Richards, LLM@ AIED. 2023</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, International conference on machine learning. PMLR2020</p>
<p>An examination of the cnn/dailymail neural summarization task. Vincent Chen, Eduardo Torres Montaño, Liezl Puzon, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics2017</p>
<p>Safe rlhf: Safe reinforcement learning from human feedback. Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Ontology-grounded automatic knowledge graph construction by llm under wikidata schema. Xiaohan Feng, Xixin Wu, Helen Meng, 2024</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, ICLR2020</p>
<p>Lora: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR2021</p>
<p>Mengya Hu, Rui Xu, Deren Lei, Yaxi Li, Mingyu Wang, Emily Ching, Eslam Kamal, Alex Deng, arXiv:2408.12748Slm meets llm: Balancing latency, interpretability and consistency in hallucination detection. 2024arXiv preprint</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, 2024NeurIPS</p>
<p>Hugging face. In Introduction to transformers for NLP: With the hugging face library and models to solve problems. Mohan Shashank, Jain, 2022Springer</p>
<p>Towards mitigating llm hallucination via self reflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods. Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, Jinghua Tan, arXiv:2403.029012024arXiv preprint</p>
<p>Supervised contrastive learning. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan, NeurIPS. 332020</p>
<p>Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Dongxu Li, Junnan Li, Steven Hoi, NeurIPS. 362024</p>
<p>Chatcite: Llm agent with human workflow guidance for comparative literature summary. Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen, arXiv:2403.025742024arXiv preprint</p>
<p>Mapping the increasing use of llms in scientific papers. Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, arXiv:2404.012682024arXiv preprint</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Corporate communication companion (ccc): An llm-empowered writing assistant for workplace social media. Zhuoran Lu, Sheshera Mysore, Tara Safavi, Jennifer Neville, Longqi Yang, Mengting Wan, arXiv:2405.046562024arXiv preprint</p>
<p>Unibind: Llm-augmented unified and balanced representation space to bind them all. Yuanhuiyi Lyu, Xu Zheng, Jiazhou Zhou, Lin Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Reducing llm hallucination using knowledge distillation: A case study with mistral large and mmlu benchmark. Daniel Mcdonald, Rachael Papadopoulos, Leslie Benningfield, Authorea Preprints. 2024</p>
<p>Llm-assisted knowledge graph engineering: Experiments with chatgpt. Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt Junghanns, Roy Meissner, Gordian Dziwis, Kirill Bulert, Michael Martin, Working conference on Artificial Intelligence Development for a Resilient and Sustainable Tomorrow. Fachmedien Wiesbaden WiesbadenSpringer2023</p>
<p>Hallucinations in llms: Understanding and addressing challenges. Gabrijela Perković, Antun Drobnjak, Ivica Botički, 2024 47th MIPRO ICT and Electronics Convention (MIPRO). IEEE2024</p>
<p>Analyzing and reducing catastrophic forgetting in parameter efficient tuning. Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, Wei Qin, arXiv:2402.188652024arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Karin Verspoor. 'fighting fire with fire'-using llms to combat llm hallucinations. 2024</p>
<p>Gpt-4: a new era of artificial intelligence in medicine. Ethan Waisberg, Joshua Ong, Mouayad Masalkhi, Sharif Amit Kamran, Nasif Zaman, Prithul Sarker, Andrew G Lee, Alireza Tavakkoli, Irish Journal of Medical Science. 19261971. 2023</p>
<p>Controllable unsupervised text attribute transfer via editing entangled latent representation. Ke Wang, Hang Hua, Xiaojun Wan, NeurIPS. 322019</p>
<p>Measuring and reducing llm hallucination without gold-standard answers via expertise-weighting. Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew Estornell, Yang Liu, arXiv:2402.104122024arXiv preprint</p>
<p>Jailbreak and guard aligned language models with only few in-context demonstrations. Zeming Wei, Yifei Wang, Yisen Wang, arXiv:2310.063872023arXiv preprint</p>
<p>Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, arXiv:2402.15179Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine-tuning via representation editing. 2024arXiv preprint</p>
<p>Simple and asymmetric graph contrastive learning without augmentations. Teng Xiao, Huaisheng Zhu, Zhengyu Chen, Suhang Wang, NeurIPS. 362024</p>
<p>Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Young Jin Kim, arXiv:2401.084172024arXiv preprint</p>
<p>Integrated application of llm model and knowledge graph in medical text mining and knowledge extraction. Jinzhu Yang, Social Medicine and Health Management. 522024</p>
<p>Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Li Yuan, arXiv:2310.01469Llm lies: Hallucinations are not bugs, but features as adversarial examples. 2023arXiv preprint</p>
<p>Decoupled contrastive learning. Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, Yubei Chen, Yann Lecun, European conference on computer vision. Springer2022</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, NeurIPS. 332020</p>
<p>La-ucl: Llm-augmented unsupervised contrastive learning framework for few-shot text classification. Jing Zhang, Hui Gao, Peng Zhang, Boda Feng, Wenmin Deng, Yuexian Hou, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024</p>
<p>Towards general conceptual model editing via adversarial representation engineering. Yihao Zhang, Zeming Wei, Jun Sun, Meng Sun, arXiv:2404.137522024arXiv preprint</p>
<p>Representation engineering: A top-down approach to ai transparency. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, arXiv:2310.014052023arXiv preprint</p>
<p>Daniel J James Y Zou, David C Hsu, Ryan P Parkes, Adams, Contrastive learning using spectral methods. 201326</p>            </div>
        </div>

    </div>
</body>
</html>
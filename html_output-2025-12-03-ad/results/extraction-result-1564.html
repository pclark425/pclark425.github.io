<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1564 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1564</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1564</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-98783</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/cs/0502029v1.pdf" target="_blank">Scalability of Genetic Programming and Probabilistic Incremental Program Evolution</a></p>
                <p><strong>Paper Abstract:</strong> This paper discusses scalability of standard genetic programming (GP) and the probabilistic incremental program evolution (PIPE). To investigate the need for both effective mixing and linkage learning, two test problems are considered: ORDER problem, which is rather easy for any recombination-based GP, and TRAP or the deceptive trap problem, which requires the algorithm to learn interactions among subsets of terminals. The scalability results show that both GP and PIPE scale up polynomially with problem size on the simple ORDER problem, but they both scale up exponentially on the deceptive problem. This indicates that while standard recombination is sufficient when no interactions need to be considered, for some problems linkage learning is necessary. These results are in agreement with the lessons learned in the domain of binary-string genetic algorithms (GAs). Furthermore, the paper investigates the effects of introducing utnnecessary and irrelevant primitives on the performance of GP and PIPE.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1564.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1564.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tree-based genetic algorithm that evolves programs using population-based selection and variation operators (subtree crossover and subtree mutation) applied to program trees; evaluated on program-expression fitness functions (ORDER and TRAP) to study scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Standard Genetic Programming (GP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GP represents candidate solutions as labeled trees (internal nodes = functions, leaves = terminals). Evolution proceeds by generating an initial random population, evaluating each program on a task-specific fitness function, selecting promising programs (binary tournament selection in this paper), and creating the next generation by applying variation operators (typically subtree crossover and subtree mutation) and elitist copying until termination. Implemented using the lilgp library for experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Subtree crossover: two parent program trees are selected and a randomly chosen subtree from each parent is swapped to create two offspring. Crossover probability in experiments was set to 1.0 (always apply crossover).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Standard subtree mutation (described in background): replace a randomly selected subtree of a program by a randomly generated subtree. NOTE: to focus on recombination effects, experiments in this paper used no mutation (mutation disabled).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Problem-specific fitness functions that measure functional correctness of programs: for ORDER, F(x)=sum_{i=1..l} f1(x_i) where f1(x_i)=1 if terminal Xi is the expressed terminal from pair (Xi, Xi) after inorder traversal (max fitness = l). For TRAP, expressed terminals are mapped to an l-bit string then partitioned into groups of k bits; each group contributes f_k(u) (a deceptive trap function) with parameters k and δ (experiments used k=3, δ=1); total fitness is sum over groups.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Qualitative algorithmic performance on functionality: On ORDER both GP and PIPE reach global optima with average-function-evaluations scaling as a low-order polynomial in l (PIPE slightly more efficient). On TRAP both GP and PIPE scale poorly (exponential growth in evaluations); GP performed slightly better than PIPE on TRAP (attributed to weaker recombination causing less disruption of partial solutions). No explicit numeric novelty/executability scores reported other than these scaling characterizations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Program-evolution benchmarks: ORDER (one-max-like) and TRAP (deceptive-trap-like), plus variants with NEG_JOIN and JUNK terminals to test irrelevant primitives and interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to PIPE (probabilistic recombination / EDA-style sampling) in experiments; conceptual baselines discussed include binary-string GAs, UMDA/EDAs, and references to hBOA software used in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Subtree crossover (recombination) is sufficient to scalably solve ORDER (decomposable, low-interaction) with low-order polynomial effort; both GP and PIPE tolerate irrelevant terminals and unnecessary functions reasonably well. However, when strong interactions exist (TRAP deceptive partitions), standard GP recombination without linkage learning fails to scale (exponential effort). Mutation was disabled in the experiments, so observed effects are attributable to recombination behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalability of Genetic Programming and Probabilistic Incremental Program Evolution', 'publication_date_yy_mm': '2005-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1564.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1564.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PIPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Incremental Program Evolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An EDA-style program-evolution algorithm that replaces pairwise crossover/mutation by building a probabilistic tree model of selected programs (node-wise independent probability tables) and sampling that model to generate new programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probabilistic Incremental Program Evolution (PIPE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PIPE represents programs as labeled trees and, after selection, builds a probabilistic model whose structure is the minimal tree covering selected programs. Each model node stores a probability distribution over functions/terminals observed at that node; child arities are handled by using the maximum arity across selected programs. New programs are generated by sampling the model recursively from the root, choosing a function/terminal per-node according to the stored probabilities. PIPE thus performs probabilistic recombination (univariate per-node model) instead of explicit crossover/mutation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Same problem-specific fitness functions as GP (ORDER and TRAP): ORDER counts expressed Xi terminals; TRAP maps expressed terminals to bitstrings and applies the deceptive trap f_k(u) per group (experiments used k=3, δ=1). Thus executability/functionality is measured via these fitness definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Qualitative algorithmic performance on functionality: PIPE was slightly more efficient than GP on ORDER (low-order polynomial scaling), attributed to PIPE's effective recombination for independent components; on TRAP both PIPE and GP scaled poorly (exponential growth in evaluations) and PIPE performed slightly worse than GP, attributed to PIPE's univariate model ignoring important interactions. No explicit numeric novelty/executability scores reported beyond scaling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Program-evolution benchmarks: ORDER and TRAP (and ORDER variants with NEG_JOIN and JUNK terminals).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to standard GP in experiments; conceptually related to univariate EDAs (UMDA) and referenced EDAs/PMBAs (e.g., UMDA, hBOA) as conceptual baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Replacing crossover/mutation with a univariate per-node probabilistic model (PIPE) yields slight efficiency gains on problems where program components are independent (ORDER). However, PIPE's univariate model fails to capture interactions required by deceptive TRAP problems, leading to exponential scaling similar to GP; this highlights the need for linkage learning / multivariate modeling for interacting program components. PIPE and GP both handle irrelevant JUNK terminals comparably well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalability of Genetic Programming and Probabilistic Incremental Program Evolution', 'publication_date_yy_mm': '2005-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Probabilistic incremental program evolution <em>(Rating: 2)</em></li>
                <li>Probabilistic model building and competent genetic programming <em>(Rating: 2)</em></li>
                <li>Learning probabilistic tree grammars for genetic programming <em>(Rating: 2)</em></li>
                <li>H-PIPE: Facilitating hierarchical program evolution through skip nodes <em>(Rating: 2)</em></li>
                <li>Genetic programming: On the programming of computers by means of natural selection <em>(Rating: 2)</em></li>
                <li>From recombination of genes to the estimation of distributions I. Binary parameters <em>(Rating: 1)</em></li>
                <li>A survey of optimization by building and using probabilistic models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1564",
    "paper_id": "paper-98783",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "GP",
            "name_full": "Standard Genetic Programming",
            "brief_description": "Tree-based genetic algorithm that evolves programs using population-based selection and variation operators (subtree crossover and subtree mutation) applied to program trees; evaluated on program-expression fitness functions (ORDER and TRAP) to study scalability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Standard Genetic Programming (GP)",
            "system_description": "GP represents candidate solutions as labeled trees (internal nodes = functions, leaves = terminals). Evolution proceeds by generating an initial random population, evaluating each program on a task-specific fitness function, selecting promising programs (binary tournament selection in this paper), and creating the next generation by applying variation operators (typically subtree crossover and subtree mutation) and elitist copying until termination. Implemented using the lilgp library for experiments in this paper.",
            "input_type": "programs",
            "crossover_operation": "Subtree crossover: two parent program trees are selected and a randomly chosen subtree from each parent is swapped to create two offspring. Crossover probability in experiments was set to 1.0 (always apply crossover).",
            "mutation_operation": "Standard subtree mutation (described in background): replace a randomly selected subtree of a program by a randomly generated subtree. NOTE: to focus on recombination effects, experiments in this paper used no mutation (mutation disabled).",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Problem-specific fitness functions that measure functional correctness of programs: for ORDER, F(x)=sum_{i=1..l} f1(x_i) where f1(x_i)=1 if terminal Xi is the expressed terminal from pair (Xi, Xi) after inorder traversal (max fitness = l). For TRAP, expressed terminals are mapped to an l-bit string then partitioned into groups of k bits; each group contributes f_k(u) (a deceptive trap function) with parameters k and δ (experiments used k=3, δ=1); total fitness is sum over groups.",
            "executability_results": "Qualitative algorithmic performance on functionality: On ORDER both GP and PIPE reach global optima with average-function-evaluations scaling as a low-order polynomial in l (PIPE slightly more efficient). On TRAP both GP and PIPE scale poorly (exponential growth in evaluations); GP performed slightly better than PIPE on TRAP (attributed to weaker recombination causing less disruption of partial solutions). No explicit numeric novelty/executability scores reported other than these scaling characterizations.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Program-evolution benchmarks: ORDER (one-max-like) and TRAP (deceptive-trap-like), plus variants with NEG_JOIN and JUNK terminals to test irrelevant primitives and interactions.",
            "comparison_baseline": "Compared directly to PIPE (probabilistic recombination / EDA-style sampling) in experiments; conceptual baselines discussed include binary-string GAs, UMDA/EDAs, and references to hBOA software used in some experiments.",
            "key_findings": "Subtree crossover (recombination) is sufficient to scalably solve ORDER (decomposable, low-interaction) with low-order polynomial effort; both GP and PIPE tolerate irrelevant terminals and unnecessary functions reasonably well. However, when strong interactions exist (TRAP deceptive partitions), standard GP recombination without linkage learning fails to scale (exponential effort). Mutation was disabled in the experiments, so observed effects are attributable to recombination behavior.",
            "uuid": "e1564.0",
            "source_info": {
                "paper_title": "Scalability of Genetic Programming and Probabilistic Incremental Program Evolution",
                "publication_date_yy_mm": "2005-02"
            }
        },
        {
            "name_short": "PIPE",
            "name_full": "Probabilistic Incremental Program Evolution",
            "brief_description": "An EDA-style program-evolution algorithm that replaces pairwise crossover/mutation by building a probabilistic tree model of selected programs (node-wise independent probability tables) and sampling that model to generate new programs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Probabilistic Incremental Program Evolution (PIPE)",
            "system_description": "PIPE represents programs as labeled trees and, after selection, builds a probabilistic model whose structure is the minimal tree covering selected programs. Each model node stores a probability distribution over functions/terminals observed at that node; child arities are handled by using the maximum arity across selected programs. New programs are generated by sampling the model recursively from the root, choosing a function/terminal per-node according to the stored probabilities. PIPE thus performs probabilistic recombination (univariate per-node model) instead of explicit crossover/mutation.",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Same problem-specific fitness functions as GP (ORDER and TRAP): ORDER counts expressed Xi terminals; TRAP maps expressed terminals to bitstrings and applies the deceptive trap f_k(u) per group (experiments used k=3, δ=1). Thus executability/functionality is measured via these fitness definitions.",
            "executability_results": "Qualitative algorithmic performance on functionality: PIPE was slightly more efficient than GP on ORDER (low-order polynomial scaling), attributed to PIPE's effective recombination for independent components; on TRAP both PIPE and GP scaled poorly (exponential growth in evaluations) and PIPE performed slightly worse than GP, attributed to PIPE's univariate model ignoring important interactions. No explicit numeric novelty/executability scores reported beyond scaling behavior.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Program-evolution benchmarks: ORDER and TRAP (and ORDER variants with NEG_JOIN and JUNK terminals).",
            "comparison_baseline": "Compared to standard GP in experiments; conceptually related to univariate EDAs (UMDA) and referenced EDAs/PMBAs (e.g., UMDA, hBOA) as conceptual baselines.",
            "key_findings": "Replacing crossover/mutation with a univariate per-node probabilistic model (PIPE) yields slight efficiency gains on problems where program components are independent (ORDER). However, PIPE's univariate model fails to capture interactions required by deceptive TRAP problems, leading to exponential scaling similar to GP; this highlights the need for linkage learning / multivariate modeling for interacting program components. PIPE and GP both handle irrelevant JUNK terminals comparably well.",
            "uuid": "e1564.1",
            "source_info": {
                "paper_title": "Scalability of Genetic Programming and Probabilistic Incremental Program Evolution",
                "publication_date_yy_mm": "2005-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Probabilistic incremental program evolution",
            "rating": 2,
            "sanitized_title": "probabilistic_incremental_program_evolution"
        },
        {
            "paper_title": "Probabilistic model building and competent genetic programming",
            "rating": 2,
            "sanitized_title": "probabilistic_model_building_and_competent_genetic_programming"
        },
        {
            "paper_title": "Learning probabilistic tree grammars for genetic programming",
            "rating": 2,
            "sanitized_title": "learning_probabilistic_tree_grammars_for_genetic_programming"
        },
        {
            "paper_title": "H-PIPE: Facilitating hierarchical program evolution through skip nodes",
            "rating": 2,
            "sanitized_title": "hpipe_facilitating_hierarchical_program_evolution_through_skip_nodes"
        },
        {
            "paper_title": "Genetic programming: On the programming of computers by means of natural selection",
            "rating": 2,
            "sanitized_title": "genetic_programming_on_the_programming_of_computers_by_means_of_natural_selection"
        },
        {
            "paper_title": "From recombination of genes to the estimation of distributions I. Binary parameters",
            "rating": 1,
            "sanitized_title": "from_recombination_of_genes_to_the_estimation_of_distributions_i_binary_parameters"
        },
        {
            "paper_title": "A survey of optimization by building and using probabilistic models",
            "rating": 1,
            "sanitized_title": "a_survey_of_optimization_by_building_and_using_probabilistic_models"
        }
    ],
    "cost": 0.008449249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scalability of Genetic Programming and Probabilistic Incremental Program Evolution
7 Feb 2005</p>
<p>Radovan Ondas ondasr@umsl.edu 
Dept. of Math and Computer Science
University of Missouri -St. Louis
Natural Bridge Rd. St. Louis
CCB 3318001, 63121MOUSA</p>
<p>Martin Pelikan pelikan@cs.umsl.edu 
Dept. of Math and Computer Science
University of Missouri -St. Louis
CCB 3208001, 63121Natural Bridge Rd. St. LouisMOUSA</p>
<p>Kumara Sastry ksastry@uiuc.edu 
University of Illinois at Urbana-Champaign Dept. of General
Engineering 117 Transportation Bldg. 104 S. Mathews Ave. Urbana61801ILUSA</p>
<p>Scalability of Genetic Programming and Probabilistic Incremental Program Evolution
7 Feb 2005I28 [Artificial Intelligence]: Problem Solving, Control Methods, and SearchI26 [Artificial Intelligence]: LearningG16 [Numerical Analysis]: Optimization Keywords Genetic programming, PIPE, scalability, order problem, trap problem
This paper discusses scalability of standard genetic programming (GP) and the probabilistic incremental program evolution (PIPE). To investigate the need for both effective mixing and linkage learning, two test problems are considered: ORDER problem, which is rather easy for any recombinationbased GP, and TRAP or the deceptive trap problem, which requires the algorithm to learn interactions among subsets of terminals. The scalability results show that both GP and PIPE scale up polynomially with problem size on the simple ORDER problem, but they both scale up exponentially on the deceptive problem. This indicates that while standard recombination is sufficient when no interactions need to be considered, for some problems linkage learning is necessary. These results are in agreement with the lessons learned in the domain of binary-string genetic algorithms (GAs). Furthermore, the paper investigates the effects of introducing unnecessary and irrelevant primitives on the performance of GP and PIPE.</p>
<p>INTRODUCTION</p>
<p>To solve large and complex problems, scalability is among the primary concerns of an optimization practitioner. However, only few studies [18,19] exist that study scalabil-ity in genetic programming (GP) [8]. The same holds for simple approaches to using probabilistic recombination in GP within the estimation of distribution algorithm (EDA) framework [12,9,14], such as the probabilistic incremental program evolution (PIPE) [17].</p>
<p>The purpose of this paper is to study the scalability of standard GP and PIPE on two decomposable GP problems: ORDER and TRAP. The two algorithms perform as expected and they solve ORDER scalably while failing to scale up on TRAP. Additionally, the paper studies the effects of introducing unnecessary and irrelevant primitives. Both GP and PIPE are shown to deal with these two sources of difficulty well. The results presented in this paper confirm that binary-string GAs have a lot in common with GP and PIPE, and thus the lessons learned in the design, study, and application of standard GAs and their extensions should carry over to GP as argued for example in [6,18,19].</p>
<p>The paper starts by describing the algorithms investigated in this paper: GP and PIPE. Section 3 explains test problems. Section 4 provides and discusses experimental results. Section 5 presents important topics for future work in this line of research. Section 6 summarizes the paper. Finally, Section 7 concludes the paper.</p>
<p>METHODS</p>
<p>Both GP and PIPE work with programs encoded as labeledtree structures and both can be applied to the same class of problems. While GP generates new candidate programs using standard variation operators, such as crossover and mutation, PIPE builds and samples a probabilistic model in the form of a tree of mutually independent nodes. Therefore, the difference between GP and PIPE is in their variation operator (see Figure 1). This section describes GP and PIPE. The section starts by discussing standard GP and closes by describing the probabilistic algorithm PIPE.</p>
<p>Genetic Programming</p>
<p>Genetic programming (GP) [8] is a genetic algorithm (GA) [4] that evolves programs instead of fixed-length strings. Programs are represented by trees where nodes represent functions and leaves represent variables and constants.</p>
<p>GP starts with a population of random candidate programs. Each program is evaluated on a given task and its fitness value is assigned. A population of promising programs is then selected using one of the standard GA selection operators, such as tournament or truncation selection. Some of the selected programs can be directly copied into the new population, the remaining ones are copied after applying variation operators, such as crossover and mutation. Crossover usually proceeds by exchanging randomly selected subtrees between two programs, whereas mutation usually replaces a randomly selected subtree of a program by a randomly generated one. This process is repeated until termination criteria are met.</p>
<p>Since standard GP variation operators proceed without considering interactions between different components of selected programs, they are likely to experience difficulties with solving problems where different program components interact strongly. However, problems that can be decomposed into subproblems of order one should be easy for any standard GP based on recombination. This intuition is verified with experiments in Section 4. Similar behavior can be observed in GAs; GAs with standard variation operators work great on problems with no interactions between decision variables [13,7,5], but they often fail for problems with highly interacting decision variables [20,5].</p>
<p>We implemented GP using the lilgp GP library developed by the Genetic Algorithms Research and Applications Group (GARAGe) at the Michigan State University.</p>
<p>PIPE</p>
<p>In the probabilistic incremental program evolution (PIPE) algorithm [16,17] computer programs or mathematical expressions are evolved like in GP [8]. However, pairwise crossover and mutation are replaced by building a proba-bilistic model of promising programs and sampling the model.</p>
<p>Like GP, PIPE represents programs by labeled trees where each internal node represents a function and each leaf represents a variable or a constant. The initial population is also generated at random. All programs in the population are then evaluated and selection is applied to select the population of promising programs. Instead of applying crossover and mutation to a part of the selected population to generate new programs, PIPE now builds a probabilistic model of the selected programs in the form of a tree. This probabilistic model is then sampled to generate new candidate programs that form the new population. The process is repeated until the termination criteria are met.</p>
<p>Next, the methods for learning and sampling the probabilistic model in PIPE are described.</p>
<p>Learning the Probabilistic Model</p>
<p>The probabilistic model in PIPE is a tree with the structure corresponding to the structure of candidate programs. Since different programs may be of different structure and size, the population is first parsed to find the smallest tree that contains every structure in the selected population. Each node of a program in the selected population then directly corresponds to one node in the model, whereas the children of each internal node represent arguments of the function in this node. Figure 2 illustrates probabilistic models used in PIPE.</p>
<p>If there are functions of different arities, the number of children of each node in the probabilistic model is equal to the maximum arity of a function in this node in the selected population. For a function of smaller arity, the first children are interpreted as arguments of this function (in an arbitrary fixed ordering). </p>
<p>PIPE then parses the selected population and computes</p>
<p>Sampling the Probabilistic Model</p>
<p>Sampling of the probabilistic model starts in the root of the probabilistic model. The same recursive procedure is used to generate each node. First, a function or terminal is generated in the current node based on the distribution encoded by the table of probabilities in this node. If the function requires several arguments, a necessary number of children are generated recursively. The recursive generation terminates in a node whenever a terminal is generated in this node and thus no children have to be generated. Since the probabilistic model is built from an actual population of programs, the sampling will never cross the boundaries of the model. Using the probabilistic model of PIPE to model and sample candidate programs resembles the univariate marginal distribution algorithm (UMDA) [12,1], which models each string position independently of the values in other positions. Interactions between each node and its context are ignored. That is why it can be expected that using this model will lead to inferior results on problems where program components interact strongly, similarly as the univariate model generally fails if string positions interact [20]. On the other hand, if different program components are mutually independent, PIPE should work great. This intuition is verified with experiments in Section 4.</p>
<p>We implemented PIPE by incorporating probabilistic recombination into the lilgp library developed by GARAGe at the Michigan State University.</p>
<p>TEST PROBLEMS</p>
<p>In order to test scalability, we need a class of problems where size can be modified while the inherent problem difficulty does not grow prohibitively fast. In fixed-length string GAs, decomposable problems of bounded difficulty [5] can be used as a challenging but solvable class of problems. Two types of decomposable problems for fixed-length string GAs are common: Onemax and concatenated traps. In onemax, the contribution of each bit is independent of its context. On the other hand, in concatenated traps, bits in each trap partition interact and cannot be effectively processed without considering other bits in the same trap partition.</p>
<p>Similar problems to onemax and concatenated traps were also created for GP where candidate solutions are represented by program trees [6,18]. Two classes of problems from [18] are considered:</p>
<ol>
<li>
<p>ORDER: OneMax-like, GP-easy problem.</p>
</li>
<li>
<p>TRAP: Deceptive-trap-like, GP-difficult problem ORDER should be easy for any recombination-based GP. However, since standard variation operators do not consider interactions between different program components, TRAP can be expected to lead to exponential scalability of both standard GP and PIPE. The problems are described next.</p>
</li>
</ol>
<p>Problem 1: Order</p>
<p>The primitive set of an l-primitive ORDER problem consist of a binary function JOIN and complimentary terminals Xi and Xi for i ∈ {1, 2, . . . , l}. A candidate solution of the ORDER problem is a binary tree with JOIN in all internal nodes and either Xi's or Xi's at its leaves. The candidate solution's output is determined by parsing the program tree inorder (from left to right). The program expresses Xi if, during the inorder parse, Xi is encountered before its complement Xi and neither Xi nor its complement are encountered earlier. For all i ∈ {1, 2, . . . , l}, if Xi is unexpressed, Xi is expressed instead. One terminal is thus expressed from each pair Xi and Xi.</p>
<p>For all i ∈ {1, 2, . . . , l}, an equal unit of fitness value is accredited if Xi is expressed:
f1(xi) = { 1 if xi ∈ {X1, X2, . . . , X l } 0 otherwise (1)
The fitness function for ORDER is defined as
F (x) = l i=1 f1(xi),(2)
where x is the set of primitives expressed by the program. Given that trees can be sufficiently large, the expression for a globally optimal solution of an l−primitive ORDER problem is {X1, X2, . . . , X l } and thus its fitness value is l.</p>
<p>For example, consider a candidate solution for a 4-primitive ORDER problem shown in Figure 3. The sequence of leaves visited during the inorder parse is {X3, X1, X1, X2, X4, X3}, the expression of this sequence is {X1, X2, X3, X4}, and the fitness of this solution is thus 2.</p>
<p>Problem 2: Deceptive Trap</p>
<p>In standard GAs, deceptive functions [3,5] are designed to thwart the very mechanism of selectorecombinative search by punishing any localized hillclimbing and requiring mixing of whole building blocks at or above the order of deception. Using such adversarially designed functions is a stiff testin some sense the stiffest test-of algorithm performance. The idea is that if an algorithm can beat an adversarially designed test functions, it can solve other problems that are equally hard or easier than the adversary. Furthermore, if the building blocks of such deceptive functions are not identified and respected by selectorecombinative GAs, then they almost always converge to the local minimum.</p>
<p>TRAP is designed to test the same mechanisms in GP. Fitness is computed so that if interactions between different components of the program are not considered, optimization may be mislead away from the global optimum. Similarly as with standard GAs on deceptive functions, standard GP is expected to fail in solving TRAP scalably, indicating the need for linkage learning in GP.</p>
<p>Programs in TRAP also consist of one binary function JOIN and l pairs of complementary primitives Xi and Xi. The expression mechanism of the program for TRAP is identical to that to that of ORDER. The difference is in the fitness evaluation procedure.</p>
<p>In TRAP, the expressed set of primitives is first mapped to an l-bit binary string. The ith bit of the string is 1 if and only if Xi was expressed; otherwise, the ith bit of the string is 0. The resulting binary string is then partitioned into groups of k bits each (the partitioning is fixed during the entire run) and a trap function is applied to each group:
f k (u) = { 1.0 u = k (1.0 − δ)(1 − u k−1 ) u &lt; k(3)
where u is the number of ones in the input string of k bits.</p>
<p>The fitness function of the trap function is then computed by adding the contributions of all groups of k bits together.</p>
<p>The difficulty of trap can be adjusted by modifying the values k, and δ. The problem becomes more difficult as the value of k is increased and that of δ is decreased. A 4-bit deceptive trap function is illustrated in Figure 4. In this paper we use traps with k = 3 and δ = 1. The important feature of additively separable trap functions is that if looking at the performance of any subset of k bits corresponding to one trap, it seems to be better to propagate 0s (here we need to eliminate Xis and substitute Xi or nothing). As shown in [18], if interactions between different components of the program are not considered, it can be expected that GP will scale up poorly on this problem.</p>
<p>Other primitives</p>
<p>In addition to ORDER and TRAP with JOIN and l terminal pairs, we tested GP and PIPE on ORDER with additional two primitives: A primitive negative join and junk or unexpressed terminals. The purpose of additional tests was to determine how GP and PIPE respond to more complex interactions and unnecessary program primitives.</p>
<p>Primitive negative-join</p>
<p>NEG JOIN affects all its descendant terminals by expressing each primitive Xi as its negation Xi; analogically, all descendants Xi are expressed as Xi. If a terminal has more NEG JOIN ancestors, only one of them is considered and the terminal is negated only once.</p>
<p>NEG JOIN is unnecessary for solving ORDER and it does not introduce a less complex or easier to find global optimum. Furthermore, NEG JOIN introduces interactions into ORDER because the best value in each leaf depends on its ancestors. Nonetheless, these interactions are relatively simple as many leaves are expected to contain NEG JOIN on the path to the root.</p>
<p>For example, for the program shown in Figure 5, the inorder pass through the program results in the following sequence of leaves: {X3, X1, X1, X2, X4, X3}. The expression gives us {X1, X2, X3, X4}, and thus the fitness is 3.</p>
<p>Junk-code terminals</p>
<p>Junk-code or JUNK terminals represent unnecessary primitives that are irrelevant for the particular problem. In biological terms, JUNK terminals correspond to junk code in DNA. During the expression phase, JUNK terminals are sim-  ply ignored and they thus do not influence the overall fitness at all.</p>
<p>Adding JUNK terminals makes the optimization problem more difficult, because additional primitives enlarge the search space without simplifying the problem. The influence of JUNK terminals can be tuned by changing the number of unique JUNK terminals. </p>
<p>EXPERIMENTS</p>
<p>This section compares the performance of GP and PIPE on three variants of ORDER and one variant of TRAP.</p>
<p>Description of experiments</p>
<p>The scalability of GP and PIPE was tested on four classes of problems:  (iv) ORDER with JUNK terminals, where the number of unique JUNK terminals is set to l/5.</p>
<p>The scalability experiments were performed by testing both algorithms on problem instances with an increasing number of primitives.</p>
<p>Additionally, the effects of increasing the number of unnecessary primitives on the performance of GP and PIPE were studied by testing GP and PIPE on a 20-primitive ORDER with an increasing number of JUNK terminals (from 5 to 40).</p>
<p>Binary tournament selection was used in both GP and PIPE. The probability of crossover in GP is set to 1.0. To focus on the effects of recombination, no mutation is used. The initial population in both methods was generated using the standard half-and-half method. Maximum tree depth was set to be one more than the depth of the minimum tree to store the global optimum. The population size that is within 10% of the minimum population size required to solve 30 independent runs is used. The population size is determined using a bisection method. The runs are terminated when the algorithms find the global optimum or when the number of generations is too large for the particular problem. Figure 7 shows the scalability of GP and PIPE on ORDER without NEG JOIN or JUNK terminals. Problem instances of different size were examined; more specifically, l = 5, 10, 20, 40, 60, 80, and 100. The figure shows the average number of function evaluations of 30 successful runs with respect to the problem size (number of positive literals). The results indicate that PIPE is slightly more efficient than GP but both GP and PIPE scale up with a low-order polynomial. These results are in agreement with the behavior observed in binary-string GAs on the simple onemax problem. On onemax, both simple GA and UMDA find the optimum in low-order polynomial time [13,7,5,15]; however, UMDA performs slightly better [15] because it uses a more effective recombination for this type of problems.   TRAP without NEG JOIN or JUNK terminals. The size of one trap is k = 3 and the signal difference is d = 1. Problem instances of different size were examined; more specifically, l = 6, 12, 18, 21, 24, and 33. On TRAP, GP performs slightly better than PIPE. This can be explained by its weaker recombination operator because here recombination causes disruption of important partial solutions [20] as can be hypothesized based on the performance of standard GAs on similar problems. Nonetheless, both GP and PIPE scale up poorly and they indicate an exponential growth of the number of function evaluations with problem size.  stance with l = 20 positive terminals contains 4 unique JUNK terminals. Both GP and PIPE seem to be capable of dealing with these irrelevant terminals and achieve performance comparable to that on basic ORDER.</p>
<p>Results</p>
<p>The last two sets of experiments are similar in that they show how the performance of GP and PIPE changes when adding irrelevant terminals into the representation. ORDER with l = 20 terminals is used with the number of JUNK terminals ranging from 5 to 40 (5, 10, 15, 20, and 40). The experiments differ in the bound on the maximum tree depth. Figure 11 shows the results with the depth limited to at most 6 (so there are at most 7 levels including the root). Figure 12 shows the results with the depth limited to at most 7 (so there are at most 8 levels including the root). The problem with the smaller maximum depth is more difficult for both GP and PIPE because JUNK terminals obstruct the creation of an optimal solution that is only slightly larger than the maximum allowed tree. PIPE deals better with this "lack of space" than GP does. However, in both cases, the number of evaluations still appears to grow with a low-order polynomial or slower as irrelevant terminals are added.</p>
<p>FUTURE WORK</p>
<p>Future work should study the scalability of GP, PIPE, and other similar approaches on the problems presented in this paper and other problems where problem size can be modified without affecting the inherent problem difficulty. The efforts to introducing linkage learning into GP (for example [18,2]) should continue to succeed in the design of robust GP methods that provide a scalable solution to broad classes of GP problems. Finally, more theory should be designed to match the achievements in this area in the domain of GAs [13,20,5,15,11,10].</p>
<p>SUMMARY</p>
<p>This paper focused on the scalability of two GP algorithms: standard GP and PIPE.</p>
<p>Two basic test functions were used: ORDER and TRAP. Both functions were defined using one binary function JOIN and l Figure 11: Scalability of GP and PIPE on ORDER with an increasing number of JUNK terminals (from 5 to 40). The maximum depth of candidate programs is 6.</p>
<p>complementary terminal pairs Xi and Xi for i ∈ {1, 2, . . . , l}.</p>
<p>ORDER can be solved without considering interactions between different program components, whereas TRAP introduces strong interactions, which make this function difficult for both standard crossover and mutation of GP, as well as the probabilistic recombination of PIPE.</p>
<p>The scalability of GP and PIPE was tested on basic ORDER and TRAP. Additionally, ORDER was extended by adding either of the following two primitives: (1) a binary function NEG JOIN and (2) JUNK (or irrelevant) terminals. Thus, there were 4 problem types examined.</p>
<p>On all four problem types, the scalability of GP and PIPE was first tested by applying these algorithms to problem instances of different size (number l of positive terminals). Then, the sensitivity of GP and PIPE to the proportion of irrelevant terminals to the relevant ones was examined.</p>
<p>CONCLUSIONS</p>
<p>The results presented in this paper indicate that the behavior of different variants of GP can be expected to be similar to that of standard binary-string GAs. There are two important consequences of this fact. First, as it was indicated in [18], to solve some classes of problems scalably, linkage learning may have to be incorporated into GP in order to identify and exploit interactions between different program components. Second, the lessons learned in the design and application of binary-string GAs should carry over to GP as argued for example in [6,19]; the first steps along this direction are represented by the decision-making model of the population sizing in GP [19], which was based on the decision-making population-sizing model for standard GAs [7,5].</p>
<p>The results also indicate that if the recombination operator captures interactions in the problem properly, increasing the mixing effects of recombination leads to better performance. That is why PIPE outperformed standard GP on problems where program components could be treated independently. This fact together with the need for linkage learning should encourage the application of probabilistic recombination operators of estimation of distribution algorithms (EDAs) [12,9,14] to the domain of GP. Some representatives of EDAs applied to the GP domain are [16,17,18,2].</p>
<p>Finally, the results show that both GP and PIPE can deal with irrelevant terminals and unnecessary functions relatively well and their performance gets only slightly worse when adding these primitives.</p>
<p>Figure 1 :
1Standard genetic programming (GP) and the probabilistic incremental program evolution (PIPE).</p>
<p>Figure 2 :
2A probabilistic model of a population of programs in the form of a tree with nodes representing the probabilities of functions and terminals. All nodes are modeled independently.the probabilities of different functions and terminals in each node of the probabilistic model. The nodes of the probabilistic model thus consist of tables of probabilities, and there is one probability for each function or terminal in each node.</p>
<p>Figure 3 :
3A candidate solution for a 4-primitive ORDER problem.The output of the program is {X1, X2, X3, X4} and the fitness of this solution is thus 2.</p>
<p>Figure 4 :
4A fully deceptive trap function with k = 4, and δ = 0.25.</p>
<p>Figure 5 :
5A candidate solution for 4-primitive ORDER problem with NEG JOIN. The output of the program is {X1, X2, X3, X4}, and the fitness of this solution is thus 3.</p>
<p>Figure 6 :
6A candidate solution for 4-primitive ORDER problem with JUNK terminals. The output of the program is {X1, X2, X3, X4}, and the fitness of this solution is thus 2.</p>
<p>Figure 6
6shows a tree with two JUNK terminals. The inorder parse results in the following sequence of leaves (ignoring JUNK): {X3, X1, X2, X4}. The expression gives us {X1, X2, X3, X4}, and thus the fitness of this solution is 2.</p>
<p>(i) Basic ORDER (no JUNK or NEG JOIN),(ii) basic TRAP (no JUNK or NEG JOIN), (iii) ORDER with NEG JOIN, and</p>
<p>Figure 7 :
7Scalability of GP and PIPE on ORDER.</p>
<p>Figure 8
8compares the scalability of GP and the PIPE on</p>
<p>Figure 8 :
8Scalability of GP and PIPE on TRAP.</p>
<p>Figure 9 :
9Scalability of GP and PIPE on ORDER with NEG JOIN.</p>
<p>Figure 9
9compares the scalability of GP and PIPE on ORDER with NEG JOIN. Problem instances of different size were examined; more specifically, l = 5, 10, 20, 40, 60, 80, and 100. Both GP and PIPE perform similarly as on basic ORDER without NEG JOIN, but there is a slight decrease in their performance because of the interactions introduced by NEG JOIN.</p>
<p>Figure 10 Figure 10 :
1010compares the scalability of GP and PIPE on ORDER with l/5 unique JUNK terminals. For example, a problem in-Scalability of GP and PIPE on ORDER with l/5 copies of JUNK terminals.</p>
<p>Figure 12 :
12Scalability of GP and PIPE on ORDER with an increasing number of JUNK terminals (from 5 to 40). The maximum depth of candidate programs is 7.
AcknowledgmentsThis work was partially supported by the Research Award and the Research Board at the University of Missouri. Some experiments were done using the hBOA software developed by Martin Pelikan and David E. Goldberg at the University of Illinois at Urbana-Champaign. This work was also sponsored by the Air Force Office of Scientific Research, Air Force Materiel Command, USAF, under grant F49620-03-1-0129, the National Science Foundation under ITR grant DMR-99-76550 (at Materials Computation Center), and ITR grant DMR-0121695 (at CPSD), and the Dept. of Energy under grant DEFG02-91ER45439 (at Fredrick Seitz MRL). The U.S. Government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation thereon.
Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning. S Baluja, No. CMU-CS-94-163Pittsburgh, PACarnegie Mellon UniversityTech. Rep.S. Baluja. Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning. Tech. Rep. No. CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA, 1994.</p>
<p>Learning probabilistic tree grammars for genetic programming. P A N Bosman, E D De, Jong , P. A. N. Bosman and E. D. de Jong. Learning probabilistic tree grammars for genetic programming. pages 190-199, 2004.</p>
<p>Analyzing deception in trap functions. K Deb, D E Goldberg, Foundations of Genetic Algorithms. 2K. Deb and D. E. Goldberg. Analyzing deception in trap functions. Foundations of Genetic Algorithms, 2:9-108, 1993.</p>
<p>Genetic algorithms in search, optimization, and machine learning. D E Goldberg, Addison-WesleyReading, MAD. E. Goldberg. Genetic algorithms in search, optimization, and machine learning. Addison-Wesley, Reading, MA, 1989.</p>
<p>The design of innovation: Lessons from and for competent genetic algorithms. D E Goldberg, Kluwer Academic Publishers7of Genetic Algorithms and Evolutionary ComputationD. E. Goldberg. The design of innovation: Lessons from and for competent genetic algorithms, volume 7 of Genetic Algorithms and Evolutionary Computation. Kluwer Academic Publishers, 2002.</p>
<p>Where does the good stuff go, and why? how contextual semantics influence program structure in simple genetic programming. D E Goldberg, U.-M O&apos;reilly, Proceedings of the First European Workshop on Genetic Programming. the First European Workshop on Genetic Programming1391D. E. Goldberg and U.-M. O'Reilly. Where does the good stuff go, and why? how contextual semantics influence program structure in simple genetic programming. Proceedings of the First European Workshop on Genetic Programming, 1391:16-36, 14-15</p>
<p>. Apr, Apr. 1998.</p>
<p>The gambler's ruin problem, genetic algorithms, and the sizing of populations. G R Harik, E Cantú-Paz, D E Goldberg, B L Miller, No. 96004Proceedings of the International Conference on Evolutionary Computation (ICEC-97). the International Conference on Evolutionary Computation (ICEC-97)Also IlliGAL ReportG. R. Harik, E. Cantú-Paz, D. E. Goldberg, and B. L. Miller. The gambler's ruin problem, genetic algorithms, and the sizing of populations. Proceedings of the International Conference on Evolutionary Computation (ICEC-97), pages 7-12, 1997. Also IlliGAL Report No. 96004.</p>
<p>Genetic programming: On the programming of computers by means of natural selection. J R Koza, MA: The MIT PressCambridgeJ. R. Koza. Genetic programming: On the programming of computers by means of natural selection. MA: The MIT Press, Cambridge, 1992.</p>
<p>P Larrañaga, J A Lozano, Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation. Kluwer. Boston, MAP. Larrañaga and J. A. Lozano, editors. Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation. Kluwer, Boston, MA, 2002.</p>
<p>Time complexity of genetic algorithms on exponentially scaled problems. F G Lobo, D E Goldberg, M Pelikan, Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2000). the Genetic and Evolutionary Computation Conference (GECCO-2000)F. G. Lobo, D. E. Goldberg, and M. Pelikan. Time complexity of genetic algorithms on exponentially scaled problems. Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2000), pages 151-158, 2000. Also IlliGAL Report No. 2000016.</p>
<p>Optimal sampling for genetic algorithms. Intelligent Engineering Systems through Artificial Neural Networks. B L Miller, D E Goldberg, 6B. L. Miller and D. E. Goldberg. Optimal sampling for genetic algorithms. Intelligent Engineering Systems through Artificial Neural Networks, 6:291-297, 1996.</p>
<p>From recombination of genes to the estimation of distributions I. Binary parameters. Parallel Problem Solving from Nature. H Mühlenbein, G Paaß, H. Mühlenbein and G. Paaß. From recombination of genes to the estimation of distributions I. Binary parameters. Parallel Problem Solving from Nature, pages 178-187, 1996.</p>
<p>Predictive models for the breeder genetic algorithm: I. Continuous parameter optimization. H Mühlenbein, D Schlierkamp-Voosen, Evolutionary Computation. 11H. Mühlenbein and D. Schlierkamp-Voosen. Predictive models for the breeder genetic algorithm: I. Continuous parameter optimization. Evolutionary Computation, 1(1):25-49, 1993.</p>
<p>A survey of optimization by building and using probabilistic models. M Pelikan, D E Goldberg, F Lobo, No. 99018Computational Optimization and Applications. 211Also IlliGAL ReportM. Pelikan, D. E. Goldberg, and F. Lobo. A survey of optimization by building and using probabilistic models. Computational Optimization and Applications, 21(1):5-20, 2002. Also IlliGAL Report No. 99018.</p>
<p>Scalability of the Bayesian optimization algorithm. M Pelikan, K Sastry, D E Goldberg, International Journal of Approximate Reasoning. 313Also IlliGAL ReportM. Pelikan, K. Sastry, and D. E. Goldberg. Scalability of the Bayesian optimization algorithm. International Journal of Approximate Reasoning, 31(3):221-258, 2002. Also IlliGAL Report No. 2001029.</p>
<p>Probabilistic incremental program evolution. R Salustowicz, J Schmidhuber, Evolutionary Computation. 52R. Salustowicz and J. Schmidhuber. Probabilistic incremental program evolution. Evolutionary Computation, 5(2):123-141, 1997.</p>
<p>H-PIPE: Facilitating hierarchical program evolution through skip nodes. R Salustowicz, J Schmidhuber, IDSIA-08-98Instituto Dalle Molle di Studi sull' Intelligenza Artificiale (IDSIA). Lugano, SwitzerlandTechnical ReportR. Salustowicz and J. Schmidhuber. H-PIPE: Facilitating hierarchical program evolution through skip nodes. Technical Report IDSIA-08-98, Instituto Dalle Molle di Studi sull' Intelligenza Artificiale (IDSIA), Lugano, Switzerland, 1998.</p>
<p>Probabilistic model building and competent genetic programming. K Sastry, D E Goldberg, K. Sastry and D. E. Goldberg. Probabilistic model building and competent genetic programming. April 2003.</p>
<p>Convergence-time models for the simple genetic algorithm with finite population. IlliGAL Report No. K Sastry, U.-M O&apos;reilly, D E Goldberg, Urbana, ILUniversity of Illinois at Urbana-Champaign, Illinois Genetic Algorithms LaboratoryK. Sastry, U.-M. O'Reilly, and D. E. Goldberg. Convergence-time models for the simple genetic algorithm with finite population. IlliGAL Report No. 2001028, University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL, 2001.</p>
<p>Analysis and design of genetic algorithms. D Thierens, Leuven, BelgiumKatholieke Universiteit LeuvenPhD thesisD. Thierens. Analysis and design of genetic algorithms. PhD thesis, Katholieke Universiteit Leuven, Leuven, Belgium, 1995.</p>            </div>
        </div>

    </div>
</body>
</html>
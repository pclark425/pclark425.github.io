<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7111 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7111</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7111</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-3744c10a10cb93979fab9e97b2db33db0e839ef1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3744c10a10cb93979fab9e97b2db33db0e839ef1" target="_blank">A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space</a></p>
                <p><strong>Paper Venue:</strong> Chemical Science</p>
                <p><strong>Paper TL;DR:</strong> A comparison of a graph-based genetic algorithm (GB-GA) and machine learning (ML) results for the optimization of log P values with a constraint for synthetic accessibility shows that the GA is as good as or better than the ML approaches for this particular property.</p>
                <p><strong>Paper Abstract:</strong> This paper presents a comparison of a graph-based genetic algorithm (GB-GA) and machine learning (ML) results for the optimization of log P values with a constraint for synthetic accessibility and shows that the GA is as good as or better than the ML approaches for this particular property.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7111.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7111.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GB-GA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Based Genetic Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based genetic algorithm implemented with RDKit that performs crossover and mutation directly on molecular graphs to optimize a penalized logP objective (J(m)) under synthetic-accessibility and size constraints; shown to be computationally efficient and competitive with ML methods for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GB-GA</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph-based genetic algorithm (evolutionary algorithm operating on molecular graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Initial mating pool sampled from the first 1000 molecules of the ZINC dataset (used only to seed population; algorithm is not a learned model)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Graph-level crossover and mutation operators applied to RDKit molecular-graph representations (reaction SMILES for defining crossovers and multiple mutation types e.g. append/insert/delete/change bond/add ring/change atom). Population sampling based on normalized J(m) scores; 20 population, 50 generations typical.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Molecular graph (nodes = atoms, edges = bonds) manipulated via RDKit; reaction SMILES used to specify operations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecular generation to maximize a penalized octanol–water partition coefficient objective (penalized logP J(m) / f(m)) subject to synthetic accessibility and ring-size penalties (i.e., property optimization for drug-like/material-like molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Synthetic accessibility score (SA) penalty; RingPenalty that subtracts (RingSize - 6) for rings larger than 6; explicit target molecular size sampled from distribution with mean 39.15 ± 3.50 non-H atoms; discard molecules with macrocycles ≥7 atoms, allene-in-ring, <5 heavy atoms, incorrect valences, or exceeding target size.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for molecular manipulation, property prediction (logP), valence checking, and synthetic accessibility scoring; algorithmic infrastructure implemented in Python using RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Subset of ZINC (first 1000 molecules used to seed mating pool and determine size statistics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Penalized logP score J(m) (normalized logP − SA − RingPenalty); reported mean/max J(m) over runs; number of molecule evaluations per run; CPU wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average maximum J(m) over 10 runs: 6.8 ± 0.7 (50% mutation rate, 1000 evaluations, ~30 s/run) and 7.4 ± 0.9 (1% mutation rate, 1000 evaluations, ~30 s/run). Best single-molecule J(m) values observed: 8.8 and 8.5. Initial mating pool mean J(m)=0.2, max=3.6.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not a learned model so depends on operator design; requires choice of mutation/crossover operators and filters; potential to produce chemically unusual structures that must be filtered (macrocycles, invalid valences). Authors note need to limit molecular size to make fair comparisons with ML methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7111.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7111.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GB-GM-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Based Generative Model with Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-ML probabilistic graph-growth generative model parameterized from small data (ZINC subset) that grows molecules atom-by-atom according to empirical bonding probabilities and is combined with Monte Carlo Tree Search (MCTS) to optimize properties (penalized logP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GB-GM-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic graph-growth generative model combined with Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Probabilities for bond orders, atom types, and specific 3-atom ring patterns estimated from the first 1000 molecules of the ZINC dataset (reference set).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Stochastic atom-by-atom growth using 'append' and 'insert' atom mutations and a tricyclic ring creation rule; operations chosen by probabilities inferred from the ZINC reference set; MCTS (leaf-parallelized, exploration factor 1/√2) used to guide growth to maximize reward (1 if current predicted J(m) exceeds best so far, else 0).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Molecular graph grown incrementally (graph tokens/operations; rules encoded for specific X~Y~Z patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Property optimization (maximize penalized logP J(m) / f(m)) to generate drug-like/material-like molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same penalized logP objective using RDKit logP prediction, SA penalty, ring penalty; stop rollouts if molecule exceeds target size; insertion/exclusion rules avoid aromatic and six-membered rings insertion sites in some operations.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for property prediction and valence checks; MCTS implementation modified from mcts.py (leaf parallelization); uses ZINC-derived empirical probabilities for growth.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC (first 1000 molecules used to compute bonding probabilities and ring statistics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Penalized logP J(m)/f(m) (max per run averaged across runs), number of tree traversals, CPU time.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GB-GM-MCTS (using 62% C=C-C ring probability): average maximum f(m) = 2.6 ± 0.6 (1000 evaluations, 90 s/run). With C=C-C probability increased to 80%: 3.4 ± 0.6 (1000 evals, 90 s). With 5000 traversals and 80% probability: 4.3 ± 0.6 (5000 evals, ~9 min/run).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Model biases reflect the (small) training-set-derived probabilities — e.g., under-represents benzene-like rings and certain larger-scale structural motifs because it grows atoms stochastically rather than assembling pre-existing functional groups; performance (higher scores) is sensitive to composition/probabilities derived from training set; simpler rules miss higher-level assembly patterns present in real molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7111.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7111.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN (SMILES generator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network SMILES Generative Model (Segler-style / Yang et al. usages)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based recurrent neural network that generates SMILES character-by-character to propose novel molecules; used in prior studies for de novo molecular generation and property optimization, sometimes combined with Bayesian optimization or MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Neural Network (RNN, SMILES generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence model / recurrent neural network (character-level SMILES generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on SMILES corpora (in referenced studies typically ZINC or similar drug-like SMILES datasets); this paper references Segler et al. and Yang et al. implementations that used ZINC-derived training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation character-by-character; in prior works combined with Bayesian optimization (BO) or Monte Carlo Tree Search (ChemTS-style) to bias generation toward higher property scores.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (text-based).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecular generation to optimize penalized logP (J(m)/f(m)) and other goal-directed molecular-design objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>In referenced comparisons, penalized logP objective (logP − SA − RingPenalty) used as optimization target; size limits applied indirectly via SMILES length limits in some prior works.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>When used for optimization, combined with Bayesian optimization or MCTS; RDKit used (in comparisons) for property prediction (logP) and synthetic-accessibility evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC-derived SMILES (referenced studies); exact dataset split not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Penalized logP J(m)/f(m) (average maximum per run), number of molecules generated/evaluated, CPU time.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>From referenced study (Yang et al.) reported in Table 2: 'Only RNN' average maximum J(m) = 4.8 ± 0.2 (~20000 molecule evaluations, ~8 h CPU); 'RNN + BO' = 4.5 ± 0.2 (~4000 evals, ~8 h). (These are comparative numbers compiled in this paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Relatively high computational cost reported (hours of CPU) compared to graph-based GA; depends on SMILES-based representation which can produce invalid SMILES; optimization can be data/motif-biased by the training corpus; requires substantial numbers of property evaluations to reach competitive scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7111.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7111.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN + BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network with Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combination where an RNN SMILES generator is coupled with Bayesian optimization to direct generation toward molecules with improved target properties (as reported in prior studies and compared here).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN + Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence model (RNN) augmented with Bayesian optimization loop</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on SMILES corpora (e.g., ZINC) in referenced works; this paper does not give exact training-set details for the BO variants.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>SMILES generation from RNN with Bayesian optimization used to search latent/control parameters or to select promising candidates for property evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Optimize penalized logP (J(m)/f(m)) for de novo molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Penalized logP objective (SA and ring penalties applied); other implicit constraints from SMILES model.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Bayesian optimization framework combined with RDKit property scoring per referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC-derived data in referenced studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Penalized logP J(m)/f(m), number of evaluations, CPU time.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported in Table 2 of this paper (from Yang et al.): RNN + BO average maximum J(m) = 4.5 ± 0.2 (~4000 evaluations, ~8 h CPU).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Computationally expensive; still under-performs graph-based GA on this penalized logP task given comparable evaluation budgets; depends on training data and BO tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7111.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7111.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous variational autoencoder (CVAE) that learns a continuous latent representation of molecules for generation and optimization, previously proposed for molecular design and compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CVAE (continuous variational autoencoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder latent-variable generative model (variational autoencoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on molecular datasets (e.g., SMILES from ZINC) in referenced studies; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Learn continuous latent space of molecules; sample/optimize in latent space and decode to SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Typically SMILES encoded into model; decoding yields SMILES (paper does not specify details beyond reference).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecular generation to optimize penalized logP objective.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Penalized logP (SA and ring penalty) used during evaluation in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Property predictions via RDKit used for evaluation in comparative benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Referenced works used ZINC-like datasets; this paper cites comparative results rather than retraining CVAE.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Penalized logP J(m)/f(m), reporting average maxima over runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported (from referenced study) in Table 2: CVAE + BO average maximum J(m) = 0.0 ± 0.9 (~100 evaluations, ~8 h CPU) as compiled here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Poor performance on this penalized logP benchmark compared to other methods (as reported); high CPU time for limited improvements; dependent on latent-space quality and training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7111.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7111.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational autoencoder that incorporates syntactic grammar constraints for improved generation of valid SMILES (Grammar VAE), previously used for molecular generation and compared in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GVAE (Grammar VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder generative model (variational autoencoder with grammar constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on molecular datasets (e.g., SMILES from ZINC) in referenced works; not explicitly detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Learn grammar-constrained latent representation and decode to valid SMILES strings; combined with BO in referenced comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES with grammar encoding</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Optimize penalized logP (J(m)/f(m)) as part of benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Penalized logP objective (SA and ring penalties applied during evaluation in comparative study).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for property evaluation in comparative benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC-derived datasets in referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Penalized logP J(m)/f(m), average maximum per run.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported in Table 2 (from referenced study): GVAE + BO average maximum J(m) = 0.2 ± 1.3 (~1000 evaluations, ~8 h CPU).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported to perform poorly on this penalized logP benchmark relative to GB-GA and some RNN/graph methods; high computational cost documented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7111.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7111.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemTS / Tsuda MCTS approach</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemTS (text-based generative model combined with Monte Carlo Tree Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based generative model (SMILES RNN) combined with MCTS (as developed/used by Tsuda and coworkers) to sequentially build SMILES via tree traversal and optimize molecular properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemTS (RNN + MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence-model-based generator (RNN/SMILES) combined with Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on SMILES datasets (e.g., ZINC) in referenced implementations; exact training set details not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Each SMILES character corresponds to a node in an MCTS tree; nodes are selected using MCTS to construct molecules sequentially and optimize a reward (e.g., penalized logP).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (character-level nodes in tree)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecule generation optimizing penalized logP (and other property objectives in related studies).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Penalized logP objective with SA and ring penalties, SMILES validity enforced via model; tree traversal budget limits (number of traversals).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>MCTS implementations; RDKit used in comparative evaluations for property scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC-derived SMILES in referenced studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Penalized logP J(m)/f(m), number of molecule evaluations, CPU time.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported in Table 2 (from Yang et al. and related works): ChemTS average maximum J(m) = 4.9 ± 0.5 (~5000 evals, ~2 h) and 5.6 ± 0.5 (~20000 evals, ~8 h) depending on evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Computational cost higher than GB-GM-MCTS/GB-GA; performance depends on number of tree traversals and training-set motifs; can extract predominant hydrophobic motifs (e.g., benzene rings) from training data leading to dataset bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7111.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7111.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Convolutional Policy Network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Policy Network (GCPN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning method that generates molecular graphs using graph convolutional networks and policy gradients to optimize properties; reported by You et al. and referenced in this paper for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Convolutional Policy Network (GCPN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph-based reinforcement learning (policy network with graph convolutions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on graph-structured molecular datasets (e.g., ZINC) in the referenced study; specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequential graph construction using a policy network (graph convolutional) trained with reinforcement learning to favor molecules with high target-property scores.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Molecular graph (nodes/edges manipulated directly).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Goal-directed molecular graph generation to maximize properties including penalized logP and other objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Penalized logP objective used for comparison in this paper; other chemical validity constraints enforced by graph approach.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for property evaluation in comparative benchmarks (as reported elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Typically ZINC or similar molecular graph datasets in the referenced study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Penalized logP J(m)/f(m) and other property-specific metrics; top scores compared to GB-GA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>You et al. reported top J(m) values in the range 7.8–8.0 using a graph-convolutional policy network; the paper notes GB-GA found slightly larger scores (8.8, 8.5).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>RL training can be complex and sensitive to reward shaping; method may emphasize motifs present in training data; computational cost not directly compared here but RL methods can be resource intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules <em>(Rating: 2)</em></li>
                <li>Grammar Variational Autoencoder <em>(Rating: 2)</em></li>
                <li>Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation <em>(Rating: 2)</em></li>
                <li>GuacaMol: Benchmarking Models for de Novo Molecular Design <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7111",
    "paper_id": "paper-3744c10a10cb93979fab9e97b2db33db0e839ef1",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "GB-GA",
            "name_full": "Graph-Based Genetic Algorithm",
            "brief_description": "A graph-based genetic algorithm implemented with RDKit that performs crossover and mutation directly on molecular graphs to optimize a penalized logP objective (J(m)) under synthetic-accessibility and size constraints; shown to be computationally efficient and competitive with ML methods for this task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GB-GA",
            "model_type": "graph-based genetic algorithm (evolutionary algorithm operating on molecular graphs)",
            "model_size": null,
            "training_data_description": "Initial mating pool sampled from the first 1000 molecules of the ZINC dataset (used only to seed population; algorithm is not a learned model)",
            "generation_method": "Graph-level crossover and mutation operators applied to RDKit molecular-graph representations (reaction SMILES for defining crossovers and multiple mutation types e.g. append/insert/delete/change bond/add ring/change atom). Population sampling based on normalized J(m) scores; 20 population, 50 generations typical.",
            "chemical_representation": "Molecular graph (nodes = atoms, edges = bonds) manipulated via RDKit; reaction SMILES used to specify operations.",
            "target_application": "De novo molecular generation to maximize a penalized octanol–water partition coefficient objective (penalized logP J(m) / f(m)) subject to synthetic accessibility and ring-size penalties (i.e., property optimization for drug-like/material-like molecules).",
            "constraints_used": "Synthetic accessibility score (SA) penalty; RingPenalty that subtracts (RingSize - 6) for rings larger than 6; explicit target molecular size sampled from distribution with mean 39.15 ± 3.50 non-H atoms; discard molecules with macrocycles ≥7 atoms, allene-in-ring, &lt;5 heavy atoms, incorrect valences, or exceeding target size.",
            "integration_with_external_tools": "RDKit used for molecular manipulation, property prediction (logP), valence checking, and synthetic accessibility scoring; algorithmic infrastructure implemented in Python using RDKit.",
            "dataset_used": "Subset of ZINC (first 1000 molecules used to seed mating pool and determine size statistics).",
            "evaluation_metrics": "Penalized logP score J(m) (normalized logP − SA − RingPenalty); reported mean/max J(m) over runs; number of molecule evaluations per run; CPU wall-clock time.",
            "reported_results": "Average maximum J(m) over 10 runs: 6.8 ± 0.7 (50% mutation rate, 1000 evaluations, ~30 s/run) and 7.4 ± 0.9 (1% mutation rate, 1000 evaluations, ~30 s/run). Best single-molecule J(m) values observed: 8.8 and 8.5. Initial mating pool mean J(m)=0.2, max=3.6.",
            "experimental_validation": false,
            "challenges_or_limitations": "Not a learned model so depends on operator design; requires choice of mutation/crossover operators and filters; potential to produce chemically unusual structures that must be filtered (macrocycles, invalid valences). Authors note need to limit molecular size to make fair comparisons with ML methods.",
            "uuid": "e7111.0",
            "source_info": {
                "paper_title": "A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "GB-GM-MCTS",
            "name_full": "Graph-Based Generative Model with Monte Carlo Tree Search",
            "brief_description": "A non-ML probabilistic graph-growth generative model parameterized from small data (ZINC subset) that grows molecules atom-by-atom according to empirical bonding probabilities and is combined with Monte Carlo Tree Search (MCTS) to optimize properties (penalized logP).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GB-GM-MCTS",
            "model_type": "probabilistic graph-growth generative model combined with Monte Carlo Tree Search",
            "model_size": null,
            "training_data_description": "Probabilities for bond orders, atom types, and specific 3-atom ring patterns estimated from the first 1000 molecules of the ZINC dataset (reference set).",
            "generation_method": "Stochastic atom-by-atom growth using 'append' and 'insert' atom mutations and a tricyclic ring creation rule; operations chosen by probabilities inferred from the ZINC reference set; MCTS (leaf-parallelized, exploration factor 1/√2) used to guide growth to maximize reward (1 if current predicted J(m) exceeds best so far, else 0).",
            "chemical_representation": "Molecular graph grown incrementally (graph tokens/operations; rules encoded for specific X~Y~Z patterns).",
            "target_application": "Property optimization (maximize penalized logP J(m) / f(m)) to generate drug-like/material-like molecules.",
            "constraints_used": "Same penalized logP objective using RDKit logP prediction, SA penalty, ring penalty; stop rollouts if molecule exceeds target size; insertion/exclusion rules avoid aromatic and six-membered rings insertion sites in some operations.",
            "integration_with_external_tools": "RDKit for property prediction and valence checks; MCTS implementation modified from mcts.py (leaf parallelization); uses ZINC-derived empirical probabilities for growth.",
            "dataset_used": "ZINC (first 1000 molecules used to compute bonding probabilities and ring statistics).",
            "evaluation_metrics": "Penalized logP J(m)/f(m) (max per run averaged across runs), number of tree traversals, CPU time.",
            "reported_results": "GB-GM-MCTS (using 62% C=C-C ring probability): average maximum f(m) = 2.6 ± 0.6 (1000 evaluations, 90 s/run). With C=C-C probability increased to 80%: 3.4 ± 0.6 (1000 evals, 90 s). With 5000 traversals and 80% probability: 4.3 ± 0.6 (5000 evals, ~9 min/run).",
            "experimental_validation": false,
            "challenges_or_limitations": "Model biases reflect the (small) training-set-derived probabilities — e.g., under-represents benzene-like rings and certain larger-scale structural motifs because it grows atoms stochastically rather than assembling pre-existing functional groups; performance (higher scores) is sensitive to composition/probabilities derived from training set; simpler rules miss higher-level assembly patterns present in real molecules.",
            "uuid": "e7111.1",
            "source_info": {
                "paper_title": "A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "RNN (SMILES generator)",
            "name_full": "Recurrent Neural Network SMILES Generative Model (Segler-style / Yang et al. usages)",
            "brief_description": "A text-based recurrent neural network that generates SMILES character-by-character to propose novel molecules; used in prior studies for de novo molecular generation and property optimization, sometimes combined with Bayesian optimization or MCTS.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Recurrent Neural Network (RNN, SMILES generator)",
            "model_type": "sequence model / recurrent neural network (character-level SMILES generator)",
            "model_size": null,
            "training_data_description": "Trained on SMILES corpora (in referenced studies typically ZINC or similar drug-like SMILES datasets); this paper references Segler et al. and Yang et al. implementations that used ZINC-derived training sets.",
            "generation_method": "Direct SMILES generation character-by-character; in prior works combined with Bayesian optimization (BO) or Monte Carlo Tree Search (ChemTS-style) to bias generation toward higher property scores.",
            "chemical_representation": "SMILES strings (text-based).",
            "target_application": "De novo molecular generation to optimize penalized logP (J(m)/f(m)) and other goal-directed molecular-design objectives.",
            "constraints_used": "In referenced comparisons, penalized logP objective (logP − SA − RingPenalty) used as optimization target; size limits applied indirectly via SMILES length limits in some prior works.",
            "integration_with_external_tools": "When used for optimization, combined with Bayesian optimization or MCTS; RDKit used (in comparisons) for property prediction (logP) and synthetic-accessibility evaluation.",
            "dataset_used": "ZINC-derived SMILES (referenced studies); exact dataset split not specified in this paper.",
            "evaluation_metrics": "Penalized logP J(m)/f(m) (average maximum per run), number of molecules generated/evaluated, CPU time.",
            "reported_results": "From referenced study (Yang et al.) reported in Table 2: 'Only RNN' average maximum J(m) = 4.8 ± 0.2 (~20000 molecule evaluations, ~8 h CPU); 'RNN + BO' = 4.5 ± 0.2 (~4000 evals, ~8 h). (These are comparative numbers compiled in this paper.)",
            "experimental_validation": false,
            "challenges_or_limitations": "Relatively high computational cost reported (hours of CPU) compared to graph-based GA; depends on SMILES-based representation which can produce invalid SMILES; optimization can be data/motif-biased by the training corpus; requires substantial numbers of property evaluations to reach competitive scores.",
            "uuid": "e7111.2",
            "source_info": {
                "paper_title": "A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "RNN + BO",
            "name_full": "Recurrent Neural Network with Bayesian Optimization",
            "brief_description": "A combination where an RNN SMILES generator is coupled with Bayesian optimization to direct generation toward molecules with improved target properties (as reported in prior studies and compared here).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RNN + Bayesian Optimization",
            "model_type": "sequence model (RNN) augmented with Bayesian optimization loop",
            "model_size": null,
            "training_data_description": "Trained on SMILES corpora (e.g., ZINC) in referenced works; this paper does not give exact training-set details for the BO variants.",
            "generation_method": "SMILES generation from RNN with Bayesian optimization used to search latent/control parameters or to select promising candidates for property evaluation.",
            "chemical_representation": "SMILES",
            "target_application": "Optimize penalized logP (J(m)/f(m)) for de novo molecule generation.",
            "constraints_used": "Penalized logP objective (SA and ring penalties applied); other implicit constraints from SMILES model.",
            "integration_with_external_tools": "Bayesian optimization framework combined with RDKit property scoring per referenced works.",
            "dataset_used": "ZINC-derived data in referenced studies.",
            "evaluation_metrics": "Penalized logP J(m)/f(m), number of evaluations, CPU time.",
            "reported_results": "Reported in Table 2 of this paper (from Yang et al.): RNN + BO average maximum J(m) = 4.5 ± 0.2 (~4000 evaluations, ~8 h CPU).",
            "experimental_validation": false,
            "challenges_or_limitations": "Computationally expensive; still under-performs graph-based GA on this penalized logP task given comparable evaluation budgets; depends on training data and BO tuning.",
            "uuid": "e7111.3",
            "source_info": {
                "paper_title": "A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "CVAE",
            "name_full": "Continuous Variational Autoencoder",
            "brief_description": "A continuous variational autoencoder (CVAE) that learns a continuous latent representation of molecules for generation and optimization, previously proposed for molecular design and compared here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CVAE (continuous variational autoencoder)",
            "model_type": "encoder-decoder latent-variable generative model (variational autoencoder)",
            "model_size": null,
            "training_data_description": "Trained on molecular datasets (e.g., SMILES from ZINC) in referenced studies; not detailed in this paper.",
            "generation_method": "Learn continuous latent space of molecules; sample/optimize in latent space and decode to SMILES.",
            "chemical_representation": "Typically SMILES encoded into model; decoding yields SMILES (paper does not specify details beyond reference).",
            "target_application": "De novo molecular generation to optimize penalized logP objective.",
            "constraints_used": "Penalized logP (SA and ring penalty) used during evaluation in comparisons.",
            "integration_with_external_tools": "Property predictions via RDKit used for evaluation in comparative benchmarks.",
            "dataset_used": "Referenced works used ZINC-like datasets; this paper cites comparative results rather than retraining CVAE.",
            "evaluation_metrics": "Penalized logP J(m)/f(m), reporting average maxima over runs.",
            "reported_results": "Reported (from referenced study) in Table 2: CVAE + BO average maximum J(m) = 0.0 ± 0.9 (~100 evaluations, ~8 h CPU) as compiled here.",
            "experimental_validation": false,
            "challenges_or_limitations": "Poor performance on this penalized logP benchmark compared to other methods (as reported); high CPU time for limited improvements; dependent on latent-space quality and training data.",
            "uuid": "e7111.4",
            "source_info": {
                "paper_title": "A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "GVAE",
            "name_full": "Grammar Variational Autoencoder",
            "brief_description": "A variational autoencoder that incorporates syntactic grammar constraints for improved generation of valid SMILES (Grammar VAE), previously used for molecular generation and compared in this study.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GVAE (Grammar VAE)",
            "model_type": "encoder-decoder generative model (variational autoencoder with grammar constraints)",
            "model_size": null,
            "training_data_description": "Trained on molecular datasets (e.g., SMILES from ZINC) in referenced works; not explicitly detailed in this paper.",
            "generation_method": "Learn grammar-constrained latent representation and decode to valid SMILES strings; combined with BO in referenced comparisons.",
            "chemical_representation": "SMILES with grammar encoding",
            "target_application": "Optimize penalized logP (J(m)/f(m)) as part of benchmark comparisons.",
            "constraints_used": "Penalized logP objective (SA and ring penalties applied during evaluation in comparative study).",
            "integration_with_external_tools": "RDKit for property evaluation in comparative benchmarks.",
            "dataset_used": "ZINC-derived datasets in referenced works.",
            "evaluation_metrics": "Penalized logP J(m)/f(m), average maximum per run.",
            "reported_results": "Reported in Table 2 (from referenced study): GVAE + BO average maximum J(m) = 0.2 ± 1.3 (~1000 evaluations, ~8 h CPU).",
            "experimental_validation": false,
            "challenges_or_limitations": "Reported to perform poorly on this penalized logP benchmark relative to GB-GA and some RNN/graph methods; high computational cost documented.",
            "uuid": "e7111.5",
            "source_info": {
                "paper_title": "A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "ChemTS / Tsuda MCTS approach",
            "name_full": "ChemTS (text-based generative model combined with Monte Carlo Tree Search)",
            "brief_description": "A text-based generative model (SMILES RNN) combined with MCTS (as developed/used by Tsuda and coworkers) to sequentially build SMILES via tree traversal and optimize molecular properties.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChemTS (RNN + MCTS)",
            "model_type": "sequence-model-based generator (RNN/SMILES) combined with Monte Carlo Tree Search",
            "model_size": null,
            "training_data_description": "Trained on SMILES datasets (e.g., ZINC) in referenced implementations; exact training set details not given in this paper.",
            "generation_method": "Each SMILES character corresponds to a node in an MCTS tree; nodes are selected using MCTS to construct molecules sequentially and optimize a reward (e.g., penalized logP).",
            "chemical_representation": "SMILES (character-level nodes in tree)",
            "target_application": "De novo molecule generation optimizing penalized logP (and other property objectives in related studies).",
            "constraints_used": "Penalized logP objective with SA and ring penalties, SMILES validity enforced via model; tree traversal budget limits (number of traversals).",
            "integration_with_external_tools": "MCTS implementations; RDKit used in comparative evaluations for property scoring.",
            "dataset_used": "ZINC-derived SMILES in referenced studies.",
            "evaluation_metrics": "Penalized logP J(m)/f(m), number of molecule evaluations, CPU time.",
            "reported_results": "Reported in Table 2 (from Yang et al. and related works): ChemTS average maximum J(m) = 4.9 ± 0.5 (~5000 evals, ~2 h) and 5.6 ± 0.5 (~20000 evals, ~8 h) depending on evaluation budget.",
            "experimental_validation": false,
            "challenges_or_limitations": "Computational cost higher than GB-GM-MCTS/GB-GA; performance depends on number of tree traversals and training-set motifs; can extract predominant hydrophobic motifs (e.g., benzene rings) from training data leading to dataset bias.",
            "uuid": "e7111.6",
            "source_info": {
                "paper_title": "A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Graph Convolutional Policy Network",
            "name_full": "Graph Convolutional Policy Network (GCPN)",
            "brief_description": "A reinforcement-learning method that generates molecular graphs using graph convolutional networks and policy gradients to optimize properties; reported by You et al. and referenced in this paper for comparison.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Graph Convolutional Policy Network (GCPN)",
            "model_type": "graph-based reinforcement learning (policy network with graph convolutions)",
            "model_size": null,
            "training_data_description": "Trained on graph-structured molecular datasets (e.g., ZINC) in the referenced study; specifics not provided in this paper.",
            "generation_method": "Sequential graph construction using a policy network (graph convolutional) trained with reinforcement learning to favor molecules with high target-property scores.",
            "chemical_representation": "Molecular graph (nodes/edges manipulated directly).",
            "target_application": "Goal-directed molecular graph generation to maximize properties including penalized logP and other objectives.",
            "constraints_used": "Penalized logP objective used for comparison in this paper; other chemical validity constraints enforced by graph approach.",
            "integration_with_external_tools": "RDKit for property evaluation in comparative benchmarks (as reported elsewhere).",
            "dataset_used": "Typically ZINC or similar molecular graph datasets in the referenced study.",
            "evaluation_metrics": "Penalized logP J(m)/f(m) and other property-specific metrics; top scores compared to GB-GA in this paper.",
            "reported_results": "You et al. reported top J(m) values in the range 7.8–8.0 using a graph-convolutional policy network; the paper notes GB-GA found slightly larger scores (8.8, 8.5).",
            "experimental_validation": false,
            "challenges_or_limitations": "RL training can be complex and sensitive to reward shaping; method may emphasize motifs present in training data; computational cost not directly compared here but RL methods can be resource intensive.",
            "uuid": "e7111.7",
            "source_info": {
                "paper_title": "A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
            "rating": 2
        },
        {
            "paper_title": "Grammar Variational Autoencoder",
            "rating": 2
        },
        {
            "paper_title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation",
            "rating": 2
        },
        {
            "paper_title": "GuacaMol: Benchmarking Models for de Novo Molecular Design",
            "rating": 1
        }
    ],
    "cost": 0.0171725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space $\dagger$</h1>
<p>Jan H. Jensen (D)</p>
<p>This paper presents a comparison of a graph-based genetic algorithm (GB-GA) and machine learning (ML) results for the optimization of $\log P$ values with a constraint for synthetic accessibility and shows that the GA is as good as or better than the ML approaches for this particular property. The molecules found by the GB-GA bear little resemblance to the molecules used to construct the initial mating pool, indicating that the GB-GA approach can traverse a relatively large distance in chemical space using relatively few (50) generations. The paper also introduces a new non-ML graph-based generative model (GB-GM) that can be parameterized using very small data sets and combined with a Monte Carlo tree search (MCTS) algorithm. The results are comparable to previously published results (Sci. Technol. Adv. Mater., 2017, 18, 972-976) using a recurrent neural network (RNN) generative model, and the GB-GM-based method is several orders of magnitude faster. The MCTS results seem more dependent on the composition of the training set than the GA approach for this particular property. Our results suggest that the performance of new ML-based generative models should be compared to that of more traditional, and often simpler, approaches such a GA.</p>
<p>Received 1st December 2018
Accepted 8th February 2019
DOI: 10.1039/c8sc05372c
rsc.li/chemical-science</p>
<h2>Introduction</h2>
<p>Within the past few years several papers have been published on using machine learning (ML) to generate molecules with the aim of optimizing their properties. ${ }^{1-8}$ While the performances of these generative models have been impressive, they are usually not compared to those of more traditional approaches such as genetic algorithms (GAs), ${ }^{9-12}$ and recent work by Tsuda and coworkers ${ }^{13}$ suggests that non-ML approaches may be competitive. The lack of comparison is perhaps in part due to the fact that there are no free or open source versions of implementation of these GAs available (ACSESS ${ }^{11}$ uses the proprietary OpenEye cheminformatics toolkits).</p>
<p>In this paper I present a comparison of graph-based GA and ML results for the optimization of $\log P$ values with a constraint for synthetic accessibility and show that the GA is as good as or better than the ML approaches for this particular property. I also introduce a new non-ML graph-based generative model that can be parameterized using very small data sets and combined with a Monte Carlo tree search algorithm. The implementation of both methods relies on the open source</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>RDKit cheminformatics package and the codes are made freely available.</p>
<h2>Computational methodology</h2>
<h2>Graph-based genetic algorithm (GB-GA)</h2>
<p>The graph-based genetic algorithm (GB-GA) combines the ideas from the algorithm developed by Brown et al. ${ }^{9}$ and the ACSESS algorithm developed by Virshup et al. ${ }^{11}$ and is implemented using the open source RDKit package. In this context "graph-based" means that crossovers and mutations are performed by altering a graph representation of the molecules as opposed to, say, a string based representation such as SMILES. The crossovers and mutations are defined using RDKit's reaction SMILES. Following Brown et al. ${ }^{9}$ a crossover can occur both at non-ring (Fig. 1a-c) and ring bonds (Fig. 1d-f), with equal probability and the positions of the cuts are chosen randomly. Molecules with macrocycles (seven atoms or more - Fig. 1f), allene centers in rings, fewer than five heavy atoms (Fig. 1c), incorrect valences as determined by RDKit, and more non-H atoms than the target size are discarded. The target size is a random number sampled from a distribution with a mean of 39.15 non-H atoms and a standard deviation of 3.50 non-H atoms as discussed later in the paper. The mutation operations are similar to those used by Virshup et al. ${ }^{11}$ (Fig. 2). The initial mating pool is drawn from a subset of the ZINC dataset used in previous studies, ${ }^{2,4,6}$ as described below, and molecules are sampled from the mating pool based on their normalized $\log P$ scores.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1 Two equally likely kinds of crossovers are considered: at nonring (a-c) and at ring positions (d-f). Two equally likely kinds of ring cuts are considered: adjacent bonds and bonds separated by one bond. For ring crossovers fragments can be mated using both single and double bonds. (c) and (e) each shows two examples of children made by the mating process. Methylflouride is discarded because it is too small and the cycloheptene ring is discarded because the ring is too large.</p>
<h2>A graph-based generative model with Monte Carlo tree search (GB-GM-MCTS)</h2>
<p>Tsuda and coworkers ${ }^{2,5}$ have combined the text-based generative model developed by Segler et al. ${ }^{1}$ with a Monte Carlo tree search (MCTS) algorithm to optimize molecular properties. In this approach each character in a SMILES string corresponds to a node in a tree network and each node is selected sequentially
using the MCTS approach. Inspired by Virshup et al. ${ }^{11}$ we developed a graph-based generative model (GB-GM) that grows the molecule one atom at a time and that can be combined with a MCTS. The GB-GM uses the "append atom" and "insert atom" mutations (Fig. 2) for atom addition to non-ring and ring atoms, respectively, but with different probabilities as described below. In addition to these two mutation-types, a new tricyclic ringcreation mutation is used: $X \sim Y \rightarrow X 1 \sim Z \sim Y 1$ where " 1 " indicates that $X$ and $Y$ are bonded and " $\sim$ " indicates an arbitrary bond order.</p>
<p>In order to create realistic looking molecules, such as those in the ZINC data set, the mutations and choice of element are chosen with probabilities obtained by an analysis of a subset of the molecules in the ZINC dataset. An analysis of first 1000 molecules in the ZINC dataset shows that $63 \%$ of the atoms are ring atoms, so the ring-creation or ring-insertion mutation is chosen $63 \%$ of the time. The most common 3 -atom combination in rings is $\mathrm{C}=\mathrm{C}-\mathrm{C}$, which accounts for $45 \%$ of all 3 -atom combinations in rings (Table 1), so a $\mathrm{C} \sim \mathrm{C} \rightarrow \mathrm{C}=\mathrm{C}-\mathrm{C}$ mutation is chosen $45 \%$ of the time, and similarly for the 34 other $X \sim Y \sim$ $Z$ combinations found in the data set. The site of insertion/ creation is chosen randomly and excludes aromatic and sixmembered rings. Similarly, for addition the most common bond involving at least one non-ring atom is $\mathrm{C}-\mathrm{C}$, so a $\mathrm{C} \rightarrow \mathrm{C}-\mathrm{C}$ mutation is chosen more often (see Table S1 ${ }^{1}$ for more details). A more specific bonding analysis, e.g. addition of C to $\mathrm{C}=\mathrm{C}$ vs. $\mathrm{C}-\mathrm{C}$, was considered but then the most probable bonding patterns are often not found in the early stages of molecule growth and the growth process effectively stops.</p>
<p>The GB-GM-MCTS code is a modified version of the mcts.py code ${ }^{14}$ modified for leaf parallelization with a maximum of 25 leaf nodes. The exploration factor is $1 / \sqrt{2}$ and rollout is terminated if the molecule exceeds the target size as described</p>
<h2>Append atom: 15\%</h2>
<p>$60 \%: X \rightarrow X-Z \quad Z=C, N, O, F, S, C l, B r$
$35 \%: X \rightarrow X=Z \quad Z=C, N, O$
$5 \%: X \rightarrow X \equiv Z \quad Z=C, N$
Insert atom: 15\%
$60 \%: X \sim Y \rightarrow X-Z-Y \quad Z=C, N, O, S$
$35 \%: X \sim Y \rightarrow X=Z-Y \quad Z=C, N$
$5 \%: X \sim Y \rightarrow X \equiv Z-Y \quad Z=C$</p>
<p>Delete atom: 14\%
$25 \%: X \sim Y \rightarrow X$
$25 \%: X \sim Y \sim Z \rightarrow X-Z$
$25 \%: X \sim Y\left(\sim Z_{1}\right) \sim Z_{2} \rightarrow X \sim Z_{1} \sim Z_{2}$
$19 \%: X \sim Y\left(\sim Z_{1}\right)\left(\sim Z_{2}\right) \sim Z_{3} \rightarrow X \sim Z_{1} \sim Z_{2} \sim Z_{3}$
$6 \%: X \sim Y\left(\sim Z_{1}\right)\left(\sim Z_{2}\right) \sim Z_{3} \rightarrow X \sim Z_{1}\left(\sim Z_{2}\right) \sim Z_{3}$</p>
<p>Change bond order: 14\%
$45 \%: X=Y$ or $X \equiv Y \quad X \rightarrow Y$
$45 \%: X-Y \rightarrow X=Y$
$5 \%: X \equiv Y \rightarrow X=Y$
$5 \%: X \sim Y^{<em>} \rightarrow X \equiv Y$
${ }^{</em>}\left(X\right.$ and $Y$ not in ring)
Delete ring bond: $14 \%$
$X \sim Y \rightarrow X \longrightarrow Y$</p>
<p>Add ring bond: 14\%
$X \sim(Y)_{n} \sim Z \rightarrow X-Z$
$5 \%: n=1 \quad 5 \%: n=2$
$45 \%: n=345 \%: n=4$</p>
<h1>Change atom type: $14 \%$</h1>
<p>$\mathrm{C}, \mathrm{N}, \mathrm{O}, \mathrm{F}, \mathrm{S}, \mathrm{Cl}, \mathrm{Br} \rightarrow \mathrm{C}, \mathrm{N}, \mathrm{O}, \mathrm{F}, \mathrm{S}, \mathrm{Cl}, \mathrm{Br}$</p>
<p>Fig. 2 Overview of mutation operations and their associated probabilities, e.g. if an "append atom" mutation is chosen then a single bond is added $60 \%$ of the time." $\sim$ " indicates an arbitrary bond order.</p>
<p>Table 1 Probability of the 15 most common 3-atom combinations in rings in the first 1000 structures of the ZINC data set ("ZINC"), and in the 1000 structures generated by the GB-GM method using the ZINC probabilities ("GB-GM (62\%)") and a probability set where the probability of [<em>] $=\left[{ }^{</em>}\right]-\left[{ }^{*}\right]$ type bonding is increased to $80 \%$ ("GB-GM (80\%)")</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Bonding</th>
<th style="text-align: left;">ZINC</th>
<th style="text-align: left;">GB-GM (62\%)</th>
<th style="text-align: left;">GB-GM (80\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{C}=\mathrm{C}-\mathrm{C}$</td>
<td style="text-align: left;">$45 \%$</td>
<td style="text-align: left;">$41 \%$</td>
<td style="text-align: left;">$53 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}-\mathrm{C}-\mathrm{C}$</td>
<td style="text-align: left;">$15 \%$</td>
<td style="text-align: left;">$23 \%$</td>
<td style="text-align: left;">$21 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}-\mathrm{C}-\mathrm{N}$</td>
<td style="text-align: left;">$9 \%$</td>
<td style="text-align: left;">$9 \%$</td>
<td style="text-align: left;">$6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}-\mathrm{N}-\mathrm{C}$</td>
<td style="text-align: left;">$6 \%$</td>
<td style="text-align: left;">$7 \%$</td>
<td style="text-align: left;">$5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}=\mathrm{C}-\mathrm{N}$</td>
<td style="text-align: left;">$4 \%$</td>
<td style="text-align: left;">$6 \%$</td>
<td style="text-align: left;">$4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{~N}=\mathrm{C}-\mathrm{C}$</td>
<td style="text-align: left;">$3 \%$</td>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: left;">$2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}=\mathrm{N}-\mathrm{C}$</td>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: left;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}-\mathrm{C}-\mathrm{O}$</td>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: left;">$2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{~N}=\mathrm{C}-\mathrm{N}$</td>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: left;">$0 \%$</td>
<td style="text-align: left;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}=\mathrm{N}-\mathrm{N}$</td>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: left;">$0 \%$</td>
<td style="text-align: left;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}-\mathrm{O}-\mathrm{C}$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}-\mathrm{N}-\mathrm{N}$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}=\mathrm{C}-\mathrm{S}$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}-\mathrm{S}-\mathrm{C}$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C}=\mathrm{C}-\mathrm{O}$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: left;">$1 \%$</td>
</tr>
</tbody>
</table>
<p>for the GB-GA. Any three- or four-membered alkene rings are subsequently expanded to five-membered rings by inserting C atoms. The reward function is 1 if the predicted $J(\mathrm{~m})$ value (see below) is larger than the largest value found so far and 0 otherwise. This reward function was found to work slightly better than the one used by Yang et al. ${ }^{2}$</p>
<h2>The penalized $\log P$ score</h2>
<p>Following Gómez-Bombarelli et al. ${ }^{4}$ and Yang et al. ${ }^{2}$ the goal is to maximize $J(\mathrm{~m})$ :</p>
<p>$$
J(\mathrm{~m})=\log P(\mathrm{~m})-\mathrm{SA}(\mathrm{~m})-\text { RingPenalty }(\mathrm{m})
$$</p>
<p>where $\log P(\mathrm{~m})$ is the octanol-water partition coefficient for a molecule (m) as predicted by RDKit, $\mathrm{SA}(\mathrm{m})$ is a synthetic accessibility score, ${ }^{15}$ and RingPenalty(m) penalizes unrealistically large rings by reducing the score by RingSize -6 where RingSize is the number of atoms in a ring. Following previous studies, each property is normalized to have zero mean and unit standard deviation across the ZINC dataset.
$J(\mathrm{~m})$ depends both on the number and types of atoms and can be made arbitrarily large by increasing the molecular size. Therefore it is important to limit the molecular size in order to make a fair comparison to previous studies. Yang et al. ${ }^{2}$ predict SMILES strings with a maximum length of 81 characters, but it is difficult to translate that restriction directly to molecular size since many of the characters do not refer to atoms. Instead, the 20 molecules found by Yang et al. ${ }^{2}$ are used to determine an average target size of $39.15 \pm 3.50$ non-H atoms.</p>
<h2>Results and discussion</h2>
<h2>GB-GA</h2>
<p>Ten GA simulations are performed and the maximum $J(\mathrm{~m})$ scores for each simulation are averaged. The population size is</p>
<p>20 and 50 generations are used (i.e. $1000 J(\mathrm{~m})$ evaluations per run). The initial mating pool is 20 random molecules sampled from the first 1000 molecules in the ZINC data set. The mean $J(\mathrm{~m})$-score for this set is 0.2 and the maximum value is 3.6 .</p>
<p>The average maximum $J(\mathrm{~m})$-score for the GA is $6.8 \pm 0.7$ and $7.4 \pm 0.9$ using a $50 \%$ and $1 \%$ mutation rate, respectively (Table 2). For comparison the best average maximum value found by Yang et al. ${ }^{2}$ is $5.6 \pm 0.5$, which required close to 20000 $J(\mathrm{~m})$-score evaluations. The latter took 8 hours each, while the GB-GA calculations took 30 seconds each on a laptop. These $J(\mathrm{~m})$-scores are also significantly larger than those of the other ML-based methods tried by Yang et al. ${ }^{2}$ (Table 2): a recurrent neutral network (RNN) with and without Bayesian optimization (BO), the continuous variational autoencoder ${ }^{4}$ (CVAE), and the grammar variational autoencoder ${ }^{5}$ (GVAE).</p>
<p>Fig. 3a and b show the molecules with the two highest $J(\mathrm{~m})$ scores found by the GB-GA. These scores, 8.8 and 8.5 , are slightly larger than the three largest values (7.8-8.0) obtained by You et al. ${ }^{6}$ using a graph convolutional policy network approach. The two molecules bear little resemblance to the molecules used to construct the initial mating pool. The molecules in the ZINC data set that are most similar to these two molecules have respective Tanimoto similarity scores of just 0.27 and 0.12 and corresponding $J(\mathrm{~m})$ values of 0.9 and -2.4 (Fig. S15). This indicates that the GB-GA approach can traverse a relatively large distance in chemical space using relatively few (50) generations.</p>
<p>Fig. 4 shows an example of how the maximum $J(\mathrm{~m})$ value evolves with each generation for 10 different GB-GA runs. While most of the runs have mostly peaked after about 20 generations the three best performing runs (run 1, 6, and 10) show significant improvements between 30 and 40 generations, so running fewer than 50 generations cannot be recommended for $J(\mathrm{~m})$ maximisation. None of the runs increased $J(\mathrm{~m})$ significantly after periods of 20 generations with no or little change in $J(\mathrm{~m})$ (with the possible exception of run 7). So a good strategy may be to terminate the GB-GA run if the $J(\mathrm{~m})$ value has not changed for more than 20 generations (at least for $J(\mathrm{~m})$ maximisation).</p>
<p>Table 2 Maximum $J(\mathrm{~m})$ scores averaged over 10 runs, the number of molecules evaluated per run, and the required CPU time per run. See the text for an explanation of the methods. Results for the non-GB methods are taken from the study of Yang et al. ${ }^{2}$ where the number of molecules evaluated per run is estimated based on the average number of molecules generated per minute and the CPU time</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Average $J(\mathrm{~m})$</th>
<th style="text-align: left;">No. molecules</th>
<th style="text-align: left;">CPU time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GB-GA $(50 \%)$</td>
<td style="text-align: left;">$6.8 \pm 0.7$</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">30 seconds</td>
</tr>
<tr>
<td style="text-align: left;">GB-GA $(1 \%)$</td>
<td style="text-align: left;">$7.4 \pm 0.9$</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">30 seconds</td>
</tr>
<tr>
<td style="text-align: left;">GB-GM-MCTS $(62 \%)$</td>
<td style="text-align: left;">$2.6 \pm 0.6$</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">90 seconds</td>
</tr>
<tr>
<td style="text-align: left;">GB-GM-MCTS $(80 \%)$</td>
<td style="text-align: left;">$3.4 \pm 0.6$</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">90 seconds</td>
</tr>
<tr>
<td style="text-align: left;">GB-GM-MCTS $(80 \%)$</td>
<td style="text-align: left;">$4.3 \pm 0.6$</td>
<td style="text-align: left;">5000</td>
<td style="text-align: left;">9 minutes</td>
</tr>
<tr>
<td style="text-align: left;">ChemTS</td>
<td style="text-align: left;">$4.9 \pm 0.5$</td>
<td style="text-align: left;">$\sim 5000$</td>
<td style="text-align: left;">2 hours</td>
</tr>
<tr>
<td style="text-align: left;">ChemTS</td>
<td style="text-align: left;">$5.6 \pm 0.5$</td>
<td style="text-align: left;">$\sim 20000$</td>
<td style="text-align: left;">8 hours</td>
</tr>
<tr>
<td style="text-align: left;">RNN + BO</td>
<td style="text-align: left;">$4.5 \pm 0.2$</td>
<td style="text-align: left;">$\sim 4000$</td>
<td style="text-align: left;">8 hours</td>
</tr>
<tr>
<td style="text-align: left;">Only RNN</td>
<td style="text-align: left;">$4.8 \pm 0.2$</td>
<td style="text-align: left;">$\sim 20000$</td>
<td style="text-align: left;">8 hours</td>
</tr>
<tr>
<td style="text-align: left;">CVAE + BO</td>
<td style="text-align: left;">$0.0 \pm 0.9$</td>
<td style="text-align: left;">$\sim 100$</td>
<td style="text-align: left;">8 hours</td>
</tr>
<tr>
<td style="text-align: left;">GVAE + BO</td>
<td style="text-align: left;">$0.2 \pm 1.3$</td>
<td style="text-align: left;">$\sim 1000$</td>
<td style="text-align: left;">8 hours</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3 Highest scoring molecules from the GB-GA (a and b) and GB-GM-MCTS (c and d) searches.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4 Plot of the highest $J(m)$ value found as a function of generations for 10 different GB-GA runs with a mutation rate of $1 \%$.</p>
<h2>GB-GM</h2>
<p>As described in the Computational methodology section, the GB-GM method grows a molecule one atom at a time where the choice of bond order and atom type is chosen probabilistically based on a bonding analysis of the first 1000 molecules in the ZINC dataset (referred to hereafter as the reference set). The GBGM model is tested by generating 1000 molecules using ethane as the "seed" molecule (which takes about 30 seconds on a laptop) and repeating the statistical bonding analysis. The average molecular size in the new data set is $23.2 \pm 4.1$ atoms,
which is nearly identical to that of the training set: $23.2 \pm 4.4$ atoms. There are 2498 rings compared to 2768 in the reference set and $59 \%$ of the atoms are in rings, which also is close to the $63 \%$ in the reference set. $41 \%$ of the 3 -atom combinations in rings is $\mathrm{C}=\mathrm{C}-\mathrm{C}$, which is reasonably close to the $45 \%$ in the reference set (Table 1). This difference is probably due to the fact that at least one of the two carbons must be secondary to "accept" the double bond, and if such a bonding pattern is not present then no " $=\mathrm{C}-$ " atom will be added. It is thus a little bit</p>
<p>Table 3 Number of occurrences of different ring-types in the first 1000 structures of the ZINC data set ("ZINC"), and in the 1000 structures generated by the GB-GM method using the ZINC probabilities ("GB-GM (62\%)") and a probability set where the probability of [<em>] $=$ $[</em>]-{*}$ type bonding is increased to $80 \%$ ("GB-GM (80\%)")</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Ring-type</th>
<th style="text-align: left;">ZINC</th>
<th style="text-align: left;">GM (62\%)</th>
<th style="text-align: left;">GM (80\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$[<em>] 1-{</em>}-{*} 1$</td>
<td style="text-align: left;">57</td>
<td style="text-align: left;">104</td>
<td style="text-align: left;">57</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1-{</em>}-{<em>}-{</em>} 1$</td>
<td style="text-align: left;">17</td>
<td style="text-align: left;">33</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1-{</em>}-{<em>}-{</em>}-{*} 1$</td>
<td style="text-align: left;">280</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1={</em>}-{<em>}-{</em>}-{*}$</td>
<td style="text-align: left;">120</td>
<td style="text-align: left;">396</td>
<td style="text-align: left;">408</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1={</em>}-{<em>}={</em>}-{*} 1$</td>
<td style="text-align: left;">470</td>
<td style="text-align: left;">132</td>
<td style="text-align: left;">221</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1-{</em>}-{<em>}-{</em>}-{<em>}-{</em>} 1$</td>
<td style="text-align: left;">409</td>
<td style="text-align: left;">64</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1={</em>}-{<em>}-{</em>}-{<em>}-{</em>} 1$</td>
<td style="text-align: left;">77</td>
<td style="text-align: left;">363</td>
<td style="text-align: left;">104</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1={</em>}-{<em>}={</em>}-{<em>}-{</em>} 1$</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">591</td>
<td style="text-align: left;">405</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1={</em>}-{<em>}-{</em>}={<em>}-{</em>} 1$</td>
<td style="text-align: left;">7</td>
<td style="text-align: left;">321</td>
<td style="text-align: left;">202</td>
</tr>
<tr>
<td style="text-align: left;">$[<em>] 1={</em>}-{<em>}={</em>}-{<em>}={</em>} 1$</td>
<td style="text-align: left;">1206</td>
<td style="text-align: left;">479</td>
<td style="text-align: left;">850</td>
</tr>
<tr>
<td style="text-align: left;">7-Membered ring</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">8-Membered ring</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">2768</td>
<td style="text-align: left;">2498</td>
<td style="text-align: left;">2263</td>
</tr>
</tbody>
</table>
<p>more likely that a " C " atom will be "accepted", compared to a " $=\mathrm{C}-$ " atom.</p>
<p>Not surprisingly, there are bigger differences for the larger scale features not specifically encoded in the rules such as the type of ring (Table 3). For example, there are many fewer benzene-type rings $\left([\left.{ }^{<em>}\right] 1=\left[{ }^{</em>}\right]-\left[{ }^{<em>}\right]=\left[{ }^{</em>}\right]-\left[{ }^{<em>}\right]=\left[{ }^{</em>}\right] 1\right)$ as well as cyclopentane $\left([\left.{ }^{<em>}\right] 1-\left[{ }^{</em>}\right]-\left[{ }^{<em>}\right]-\left[{ }^{</em>}\right]-\left[{ }^{<em>}\right] 1\right)$ and cyclohexane $\left([\left.{ }^{</em>}\right] 1-\right.$ $\left.\left[{ }^{<em>}\right]-\left[{ }^{</em>}\right]-\left[{ }^{<em>}\right]-\left[{ }^{</em>}\right] 1\right)$ type rings, while there are more of most of the other types compared to the reference set. The reason is that the molecules in the ZINC data set are not made by randomly adding atoms, but by assembling larger functional groups such as aliphatic and aromatic rings. As a result the average ring composition does not reflect the most likely ring compositions. It is possible to scale the probabilities to skew the results towards one or the other ring-type. For example in the last column the probabilities are scaled such that the probability of $X=Z-Y$ is $80 \%$ rather than the $62 \%$ in the reference set, which increases the number of benzene-like rings from 479 to 850 at the expense of the aliphatic rings.</p>
<h2>GB-GM-MCTS</h2>
<p>Ten GB-GM-MCTS simulations are performed using ethane as a seed molecule and the maximum $f(\mathrm{~m})$-scores for each simulation are averaged. The tree is traversed 1000 times, i.e. there are $1000 f(\mathrm{~m})$ evaluations per run. For GB-GM-MCTS the average maximum $f(\mathrm{~m})$-score value is $2.6 \pm 0.6$, which is significantly lower than the lowest value in the study of Yang et al. ${ }^{3}, 4.9 \pm 0.4$ (Table 2). The most likely explanation is that the GB-GM makes relatively few benzene rings (as discussed above), which, together with Cl atoms, is the defining structural feature of the high scoring molecules found by Yang et al. ${ }^{3}$ Indeed, if the probability of generating $\mathrm{C}=\mathrm{C}-\mathrm{C}$ containing rings is increased from $62 \%$ to $80 \%$ then the average maximum $f(\mathrm{~m})$-score increases to $3.4 \pm 0.6$. Increasing the number of tree traversals to 5000 increases the value to $4.3 \pm 0.6$, which is similar to the $4.9 \pm 0.4$ obtained by Yang et al. ${ }^{3}$ using as similar number of tree traversals. The latter run takes about 9 minutes, compared to 2 h in the study of Yang et al. ${ }^{3}$.</p>
<p>Fig. 3c and d show the highest scoring molecule found using 1000 and 5000 tree traversals. The molecules are similar to those found by Yang et al. ${ }^{3}$ in that they consist mostly of benzene rings, and the benzene ring is the most common hydrophobic structural motif in the ZINC data set. The GB-GA results show that higher $f(\mathrm{~m})$-scores can be obtained using long aliphatic chains, but this structural motif is relatively rare in the ZINC data set and therefore rarely suggested by the generative models.</p>
<h2>Conclusion and outlook</h2>
<p>This paper presents a comparison of a graph-based genetic algorithm (GB-GA) and machine learning (ML) results, compiled by Yang et al., ${ }^{3}$ for the optimization of $\log P$ values with a constraint for synthetic accessibility $(f(\mathrm{~m}))$ and shows that the GA is as good as or better than the ML-based approaches for this particular property. The GB-GA predicts
maximum $f(\mathrm{~m})$-values that, on average, are $1.3-1.8$ units higher than the best ML-based results reported by Yang et al., ${ }^{3}$ while also being several orders of magnitude computationally more efficient. Similarly, the GB-GA method finds molecules with maximum $f(\mathrm{~m})$-scores that, depending on the method, often are several units larger than those found with ML-based approaches. These molecules bear little resemblance to the molecules used to construct the initial mating pool, indicating that the GB-GA approach can traverse a relatively large distance in chemical space using relatively few (50) generations.</p>
<p>The paper also introduces a new non-ML graph-based generative model (GB-GM) that can be parameterized using very small data sets and combined with a Monte Carlo tree search (MCTS) algorithm such as the one used by Yang et al. ${ }^{3}$ The results are comparable to the results obtained by Yang et al. ${ }^{3}$ using a recurrent neural network (RNN) generative model, with maximum $f(\mathrm{~m})$-values of $4.3 \pm 0.6$ compared to $4.9 \pm 0.6$ found using 5000 property evaluations. While the results are slightly worse than the RNN results, the GB-GM-based method is several orders of magnitude faster. In both cases the MCTS approach essentially extracts the main hydrophobic structural motif (a benzene ring) found in the training set. The MCTS results thus seem more dependent on the composition of the training set than the GA approach for this particular property.</p>
<p>While the results are quite encouraging, it is important to perform similar comparisons for other properties before drawing general conclusions. In a very recent study Brown et al. ${ }^{16}$ have compared GB-GA and GB-GM-MCTS methods to SMILES-based ML and GA methods on 20 different optimization problems and conclude that "In terms of optimization performance, the best model is a genetic algorithm based on a graph representation of molecules". Our and their results strongly suggest that the performance of new ML-based generative models should be compared to that of more traditional, and often simpler, approaches such as GAs. Both the GB-GA and GB-GM-MCTS codes used in this study are freely available with the open source RDKit toolkit as the only dependence.</p>
<h2>Conflicts of interest</h2>
<p>There are no conflicts to declare.</p>
<h2>Acknowledgements</h2>
<p>I thank Bowen Liu for helpful discussions.</p>
<h2>References</h2>
<p>1 M. H. S. Segler, T. Kogej, C. Tyrchan and M. P. Waller, ACS Cent. Sci., 2017, 4, 120-131.
2 X. Yang, J. Zhang, K. Yoshizoe, K. Terayama and K. Tsuda, Sci. Technol. Adv. Mater., 2017, 18, 972-976.
3 M. J. Kusner, B. Paige and J. M. Hernandez-Lobato, Proceedings of 34th International Conference on Machine Learning, ICML, 2017, pp. 1945-1954.
4 R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla,</p>
<p>J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams and A. Aspuru-Guzik, ACS Cent. Sci., 2018, 4, 268-276.</p>
<p>5 M. Sumita, X. Yang, S. Ishihara, R. Tamura and K. Tsuda, ACS Cent. Sci., 2018, 4, 1126-1133.
6 J. You, B. Liu, R. Ying, V. Pande and J. Leskovec, arXiv:1806.02473, 2018.
7 B. Sanchez-Lengeling and A. Aspuru-Guzik, Science, 2018, 361, 360-365.
8 D. Neil, M. Segler, L. Guasch, M. Ahmed, D. Plumbley, M. Sellwood and N. Brown, Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design, 2018.</p>
<p>9 N. Brown, B. McKay, F. Gilardoni and J. Gasteiger, J. Chem. Inf. Comput. Sci., 2004, 44, 1079-1087.</p>
<p>10 N. M. O'Boyle, C. M. Campbell and G. R. Hutchison, J. Phys. Chem. C, 2011, 115, 16200-16210.
11 A. M. Virshup, J. Contreras-García, P. Wipf, W. Yang and D. N. Beratan, J. Am. Chem. Soc., 2013, 135, 7296-7303.</p>
<p>12 Y. Kanal and G. R. Hutchison, arXiv:1707.02949, 2017.
13 N. Yoshikawa, K. Terayama, T. Honma, K. Oono and K. Tsuda, arXiv:1804.02134, 2018.</p>
<p>14 Python Implementations of Monte Carlo Tree Search, https:// github.com/haroldsultan/MCTS, accessed, 2018-10-23.
15 P. Ertl and A. Schuffenhauer, J. Cheminf., 2009, 1, 8.
16 N. Brown, M. Fiscato, M. H. S. Segler and A. C. Vaucher, arXiv:1811.09621, 2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Department of Chemistry, University of Copenhagen, Copenhagen, Denmark. E-mail: jbjensen@chem.ku.dk; Web: http://www.twitter.com/janhjensen
$\dagger$ Electronic supplementary information (ESI) available: The codes used in this study can be found on GitHub: github.com/jensengroup/GB-GA/tree/v0.0 and github.com/jensengroup/GB-GM/tree/v0.0. See DOI: 10.1039/c8sc05372c&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7787 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7787</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7787</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-261214653</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/29872/31521" target="_blank">SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</a></p>
                <p><strong>Paper Abstract:</strong> Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a"dynamic"subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7787.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7787.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-level, multi-disciplinary evaluation benchmark for LLM scientific research ability covering basic knowledge, knowledge application, scientific calculation, and research ability with Static, Dynamic, and Experimental subsets to mitigate data leakage and assess subjective and objective capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs evaluated in this work (e.g., GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica-30B, Vicuna-13B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (undisclosed for some models; reported sizes for others such as Galactica-30B, Vicuna-13B, LLaMa-13B/7B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology, Chemistry, Physics (three core disciplines covered by the benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework for hypotheses/answers/experimental designs generated by LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SciEval multi-level evaluation (Static / Dynamic / Experimental)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Benchmarks LLMs across four knowledge dimensions mapped to Bloom's taxonomy using: Static Data (fixed objective questions), Dynamic Data (procedurally generated to avoid data leakage; chemistry & physics), and Experimental Data (subjective open-ended experiment questions assessed manually).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (Static & physics multiple-choice), BLEU (string answers), MSE (numerical answers), extract-match scores, manual human assessment (Experimental Data)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: % correct (A/B/C/D); BLEU: standard BLEU score (n-gram overlap); MSE: mean squared error for numerical predictions (numeric units as appropriate); extract-match: whether extracted answer matches ground truth; manual assessment: qualitative scoring of open-ended responses (details not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciEval (this work) â€” Static Data, Dynamic Data, Experimental Data</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Experimental Data answers are assessed manually by the authors; no detailed number of annotators or rating scales are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Only GPT-4, GPT-3.5-turbo and Claude-v1.3 surpass 60% average accuracy on Static Data; GPT-4 achieves best average accuracy and BLEU on Dynamic Data while Galactica-30B performs best on counting/calculation items; most models perform poorly on scientific calculation and physics dynamic questions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Risk of data leakage from pretraining corpora (addressed via Dynamic Data), limited CoT capabilities for many models, poor performance on calculation-heavy tasks and experimental result analysis, and manual assessment details for Experimental Data are not fully specified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7787.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BloomTax</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bloom's taxonomy (cognitive domain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical framework of cognitive skills (Remember, Understand, Apply, Analyze, Evaluate, Create) used to structure the SciEval evaluation into four mapped knowledge dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Benchmark-level mapping applied to models evaluated (GPT-4, GPT-3.5-turbo, Claude-v1.3, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / Education-aligned evaluation for scientific tasks (applies across biology, chemistry, physics)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation/assessment taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bloom's taxonomy-based multi-dimensional evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Map cognitive levels of Bloom's taxonomy to four SciEval knowledge dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) to ensure multi-level assessment of scientific capability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used as structural framework rather than direct numeric metric</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Not a numeric metric; provides hierarchical categories for designing items and interpreting capabilities (Remember->Create).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used to design SciEval item taxonomy (Static/Dynamic/Experimental subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to categorize items and analyze model strengths/weaknesses across cognitive levels; models underperform on higher-order calculation/analysis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Taxonomy guides coverage but does not itself provide quantitative evaluation; mapping to model outputs depends on item design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7787.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static Data subset (SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fixed collection of objective questions (multiple-choice, fill-in-the-blank, judgment) assembled from community Q&A and public datasets to allow standardized, quick model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated models (GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica, Vicuna, LLaMa, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see Table 3 in paper for model sizes where provided)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology, Chemistry, Physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>objective question answering (used to evaluate factual recall and application)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Static Data accuracy evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models answer fixed objective questions; results compared using accuracy across knowledge dimensions and disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = (number correct) / (total questions) * 100%; used per-discipline and averaged across Static Data.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Static Data (subset of SciEval); sources include Socratic Q&A and integrated datasets (MedQA test/USMLE subset, PubMedQA samples, Reagent Selection subset of ChemLLMBench)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4, GPT-3.5-turbo and Claude-v1.3 achieved >60% average accuracy on Static Data; other models performed substantially worse, especially on scientific calculation domain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Static Data is vulnerable to pretraining data leakage; SciEval complements with Dynamic Data to mitigate this.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7787.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Data subset (SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedurally-generated subset designed to prevent data leakage by producing new chemistry and physics questions from scientific principles and scripts that are updated regularly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated models (GPT-4, GPT-3.5-turbo, Galactica variants, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (KA) and Physics (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>procedural question generation to evaluate applied/calculation ability</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dynamic Data generation + evaluation (accuracy, BLEU, MSE, extract-match)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Chemistry dynamic items use molecular properties to generate questions; physics items are generated by Python scripts implementing physics formulas; items are refreshed regularly and a stable snapshot is maintained for fair comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (physics multiple-choice), BLEU (string answers in chemistry), MSE (numerical chemistry answers), extract-match</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: % correct for multiple-choice; BLEU: n-gram overlap score for string responses; MSE: mean squared error for numerical answers (large penalty: if prediction contains no number, MSE set to 1e10); extract-match: boolean match of extracted substrings.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Dynamic Data (SciEval dynamic subset; chemistry ~2000 items, physics ~890 items as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4 attains best average accuracy and BLEU on Dynamic Data; Galactica-30B performs best on counting/calculation chemistry items; many LLMs retain near-random accuracy on physics dynamic items and show very high MSE on chemistry calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dynamic generation reduces leakage but requires careful maintenance; chemistry answers can be complex (strings & numbers) needing multiple metrics; some models lack numeric precision and fail to produce numbers causing extreme MSE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7787.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experimental Data subset (SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of subjective open-ended questions derived from 12 basic scientific experiments intended to evaluate LLM research ability (principle, experiment design, data analysis, summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Subset evaluated for models with appropriate context length and web-access models (GPT-4, GPT-3.5-turbo, Claude-series, ERNIE Bot, SparkDesk where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General experimental sciences (basic experiments across biology/chemistry/physics)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>experimental design and analysis produced by LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Manual human assessment of open-ended experimental responses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each experiment comprises multiple open-ended questions; model outputs are evaluated manually by authors/reviewers for aspects like principle understanding, design quality, and result analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>qualitative manual scores (not numerically specified in the paper), per-experiment scores are reported in appendix but assessment process details are limited</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Open-ended qualitative assessment; no standardized numeric scale or inter-rater agreement reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Experimental Data (12 basic experiments collected from university lab courses within SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Responses assessed manually; paper states manual assessment but does not report number of raters, scoring rubric, or inter-rater reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Top-tier models (GPT-series, Claude-series) perform satisfactorily on experimental principle and design but struggle to analyze experimental results; other models perform poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Manual evaluation lacks reported annotator details and rubric; subjective evaluation is labor-intensive and hard to replicate without more details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7787.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary quantitative metric for objective questions (Static Data and physics multiple-choice in Dynamic Data), measuring the fraction of correct answers produced by an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to all evaluated models (GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology, Chemistry, Physics (applies to objective question subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric for answers/hypotheses phrased as multiple-choice or judgment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Accuracy on objective items</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute the percentage of multiple-choice or judgment items for which the model's selected answer matches the ground-truth label.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (reported as percent or proportion)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = (# correct answers / total # questions) * 100%; for 4-choice physics questions baseline random accuracy is 25%.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used on SciEval Static Data and physics Dynamic Data; also standard in other benchmarks compared (MMLU, AGIEval, C-EVAL).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper reports averages per model (e.g., GPT-4 highest; only GPT-4, GPT-3.5-turbo and Claude-v1.3 exceed ~60% on Static Data).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Accuracy is unsuitable for open-ended subjective items and vulnerable to pretraining data leakage; does not measure explanation quality or reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7787.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU score (Bilingual Evaluation Understudy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic n-gram overlap metric used to evaluate string-valued answers (e.g., SMILES or other string representations) in the chemistry Dynamic Data subset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bleu: a method for automatic evaluation of machine translation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to models producing string outputs on chemistry Dynamic Data (e.g., GPT-4, Galactica, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (string-encoded outputs such as SMILES or textual descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>string-answer quality metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU score for string answers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram overlap between model-generated string and reference string using standard BLEU computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU (score typically 0-100 or 0-1 depending on scaling; paper reports BLEU numeric values)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>BLEU computed per Papineni et al. (2002); higher BLEU indicates greater n-gram overlap with reference.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to chemistry Dynamic Data in SciEval (string-answer items like SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4 achieves highest average BLEU on Dynamic Data among evaluated models (specific BLEU numbers reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>BLEU measures surface overlap and may not capture chemical equivalence (e.g., different valid SMILES for same molecule) or semantic correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7787.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mean Squared Error (MSE) for numerical answers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A numerical loss metric used to evaluate numerical outputs in chemistry Dynamic Data (e.g., molecular weight calculations), with a large penalty applied when models produce no numeric output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to models producing numeric answers on chemistry Dynamic Data (GPT-4, Galactica, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (numerical calculation items) and other calculation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>numerical error metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mean Squared Error (MSE) for numerical predictions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute mean squared error between model numeric predictions and ground truth numeric values; assign MSE = 1e10 when model outputs contain no numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MSE (units = squared units of the target quantity; large sentinel value 1e10 for non-numeric outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>MSE = mean((prediction - ground_truth)^2) across items; sentinel rule: if prediction contains no number, MSE set to 1e10 to penalize non-numeric outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Chemistry portion of SciEval Dynamic Data</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Most LLMs exhibit very high MSE on chemistry numerical items indicating poor calculation precision; authors report extremely high MSEs and near-random performance for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>MSE is sensitive to scale of the target variable and the sentinel rule heavily penalizes non-numeric responses; numeric formatting/units differences can affect results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7787.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnswerOnly/CoT/3-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting & evaluation settings: Answer-Only (AO), Chain-of-Thought (CoT), 3-shot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three evaluation protocols applied to LLMs: AO requires only the final answer; CoT asks models to produce reasoning chains before answers; 3-shot provides three exemplars from dev for few-shot learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied across models (GPT-4, GPT-3.5-turbo, Claude, Galactica, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (applied to all SciEval subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation procedure / prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>AO / CoT / 3-shot evaluation settings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>AO: model gives only final answer; CoT: model is prompted to produce internal reasoning steps (Chain-of-Thought) before final answer; 3-shot: model given 3 exemplar Q&A pairs from dev set to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Comparative accuracy/BLEU/MSE as reported per setting</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same metrics (accuracy, BLEU, MSE) are applied to outputs collected under each prompting setting to measure effect of reasoning traces or few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to SciEval subsets (AO and CoT prompts shown in paper figures; 3-shot uses dev exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CoT improves performance primarily for GPT-series models on Static Data; 3-shot improves roughly half of LLMs; CoT and 3-shot significantly help chemistry Dynamic Data but have limited effect on physics Dynamic Data for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Not all models support CoT or return reliable chains; model API limitations prevented CoT/3-shot evaluation for some models (e.g., Claude variants in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7787.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7787.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IntegratedDatasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socratic Q&A, MedQA (USMLE), PubMedQA, Reagent Selection (ChemLLMBench), PubChem dev</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Public and community data sources incorporated into SciEval Static Data: Socratic Q&A as primary source, supplemented by MedQA (USMLE), PubMedQA samples, Reagent Selection subset, and PubChem for molecular info/dev links.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used to construct Static Data and dev exemplars for model evaluation (applies to evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Medicine/biomedical (MedQA, PubMedQA), general science (Socratic), chemistry (Reagent Selection, PubChem)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>datasets / corpora used to evaluate knowledge and comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Integration of community Q&A and established datasets into Static Data</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Crawl and rule-based filtering of Socratic Q&A; use GPT-4 to convert open-ended Q&A into multiple-choice by simplifying answers and generating distractors; incorporate test subsets from MedQA/USMLE, selected PubMedQA items framed as judgment, and a sampled portion of Reagent Selection as multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy on resulting multiple-choice items; judgment accuracy for PubMedQA-style yes/no/maybe</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy computed per item type (multi-choice or judgment); dev set composed of 5 examples per source/domain for few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Socratic Q&A (primary), MedQA (USMLE test set), PubMedQA (1000 expert-annotated items), Reagent Selection subset (ChemLLMBench), PubChem links for dynamic generation/dev</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors manually checked GPT-4 generated distractors and classifications to ensure data quality; no annotator counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>These integrated sources form the majority of Static Data; models' performances on this combined Static Data are reported (e.g., GPT-4 highest, only three models >60% avg accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Conversion of open-ended community QA to multiple-choice relied on GPT-4 generation and manual checks, which may introduce bias; Static sources remain vulnerable to pretraining leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding. <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models. <em>(Rating: 2)</em></li>
                <li>C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models. <em>(Rating: 2)</em></li>
                <li>MultiMedQA: a benchmark for medical question answering. <em>(Rating: 2)</em></li>
                <li>MATH: Measuring mathematical problem solving with the math dataset. <em>(Rating: 2)</em></li>
                <li>Chem-LLMBench <em>(Rating: 2)</em></li>
                <li>Bleu: a method for automatic evaluation of machine translation. <em>(Rating: 2)</em></li>
                <li>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7787",
    "paper_id": "paper-261214653",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "SciEval",
            "name_full": "SciEval benchmark",
            "brief_description": "A multi-level, multi-disciplinary evaluation benchmark for LLM scientific research ability covering basic knowledge, knowledge application, scientific calculation, and research ability with Static, Dynamic, and Experimental subsets to mitigate data leakage and assess subjective and objective capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs evaluated in this work (e.g., GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica-30B, Vicuna-13B, etc.)",
            "model_size": "various (undisclosed for some models; reported sizes for others such as Galactica-30B, Vicuna-13B, LLaMa-13B/7B, etc.)",
            "scientific_domain": "Biology, Chemistry, Physics (three core disciplines covered by the benchmark)",
            "theory_type": "evaluation framework for hypotheses/answers/experimental designs generated by LLMs",
            "evaluation_method_name": "SciEval multi-level evaluation (Static / Dynamic / Experimental)",
            "evaluation_method_description": "Benchmarks LLMs across four knowledge dimensions mapped to Bloom's taxonomy using: Static Data (fixed objective questions), Dynamic Data (procedurally generated to avoid data leakage; chemistry & physics), and Experimental Data (subjective open-ended experiment questions assessed manually).",
            "evaluation_metric": "accuracy (Static & physics multiple-choice), BLEU (string answers), MSE (numerical answers), extract-match scores, manual human assessment (Experimental Data)",
            "metric_definition": "Accuracy: % correct (A/B/C/D); BLEU: standard BLEU score (n-gram overlap); MSE: mean squared error for numerical predictions (numeric units as appropriate); extract-match: whether extracted answer matches ground truth; manual assessment: qualitative scoring of open-ended responses (details not specified).",
            "dataset_or_benchmark": "SciEval (this work) â€” Static Data, Dynamic Data, Experimental Data",
            "human_evaluation_details": "Experimental Data answers are assessed manually by the authors; no detailed number of annotators or rating scales are reported in the paper.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Only GPT-4, GPT-3.5-turbo and Claude-v1.3 surpass 60% average accuracy on Static Data; GPT-4 achieves best average accuracy and BLEU on Dynamic Data while Galactica-30B performs best on counting/calculation items; most models perform poorly on scientific calculation and physics dynamic questions.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Risk of data leakage from pretraining corpora (addressed via Dynamic Data), limited CoT capabilities for many models, poor performance on calculation-heavy tasks and experimental result analysis, and manual assessment details for Experimental Data are not fully specified.",
            "uuid": "e7787.0",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "BloomTax",
            "name_full": "Bloom's taxonomy (cognitive domain)",
            "brief_description": "A hierarchical framework of cognitive skills (Remember, Understand, Apply, Analyze, Evaluate, Create) used to structure the SciEval evaluation into four mapped knowledge dimensions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Benchmark-level mapping applied to models evaluated (GPT-4, GPT-3.5-turbo, Claude-v1.3, etc.)",
            "model_size": "n/a",
            "scientific_domain": "General / Education-aligned evaluation for scientific tasks (applies across biology, chemistry, physics)",
            "theory_type": "evaluation/assessment taxonomy",
            "evaluation_method_name": "Bloom's taxonomy-based multi-dimensional evaluation",
            "evaluation_method_description": "Map cognitive levels of Bloom's taxonomy to four SciEval knowledge dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) to ensure multi-level assessment of scientific capability.",
            "evaluation_metric": "Used as structural framework rather than direct numeric metric",
            "metric_definition": "Not a numeric metric; provides hierarchical categories for designing items and interpreting capabilities (Remember-&gt;Create).",
            "dataset_or_benchmark": "Used to design SciEval item taxonomy (Static/Dynamic/Experimental subsets)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used to categorize items and analyze model strengths/weaknesses across cognitive levels; models underperform on higher-order calculation/analysis tasks.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Taxonomy guides coverage but does not itself provide quantitative evaluation; mapping to model outputs depends on item design.",
            "uuid": "e7787.1",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Static Data",
            "name_full": "Static Data subset (SciEval)",
            "brief_description": "A fixed collection of objective questions (multiple-choice, fill-in-the-blank, judgment) assembled from community Q&A and public datasets to allow standardized, quick model comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluated models (GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica, Vicuna, LLaMa, etc.)",
            "model_size": "various (see Table 3 in paper for model sizes where provided)",
            "scientific_domain": "Biology, Chemistry, Physics",
            "theory_type": "objective question answering (used to evaluate factual recall and application)",
            "evaluation_method_name": "Static Data accuracy evaluation",
            "evaluation_method_description": "Models answer fixed objective questions; results compared using accuracy across knowledge dimensions and disciplines.",
            "evaluation_metric": "accuracy (percentage correct)",
            "metric_definition": "Accuracy = (number correct) / (total questions) * 100%; used per-discipline and averaged across Static Data.",
            "dataset_or_benchmark": "Static Data (subset of SciEval); sources include Socratic Q&A and integrated datasets (MedQA test/USMLE subset, PubMedQA samples, Reagent Selection subset of ChemLLMBench)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-4, GPT-3.5-turbo and Claude-v1.3 achieved &gt;60% average accuracy on Static Data; other models performed substantially worse, especially on scientific calculation domain.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Static Data is vulnerable to pretraining data leakage; SciEval complements with Dynamic Data to mitigate this.",
            "uuid": "e7787.2",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Dynamic Data",
            "name_full": "Dynamic Data subset (SciEval)",
            "brief_description": "A procedurally-generated subset designed to prevent data leakage by producing new chemistry and physics questions from scientific principles and scripts that are updated regularly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluated models (GPT-4, GPT-3.5-turbo, Galactica variants, etc.)",
            "model_size": "various",
            "scientific_domain": "Chemistry (KA) and Physics (SC)",
            "theory_type": "procedural question generation to evaluate applied/calculation ability",
            "evaluation_method_name": "Dynamic Data generation + evaluation (accuracy, BLEU, MSE, extract-match)",
            "evaluation_method_description": "Chemistry dynamic items use molecular properties to generate questions; physics items are generated by Python scripts implementing physics formulas; items are refreshed regularly and a stable snapshot is maintained for fair comparisons.",
            "evaluation_metric": "accuracy (physics multiple-choice), BLEU (string answers in chemistry), MSE (numerical chemistry answers), extract-match",
            "metric_definition": "Accuracy: % correct for multiple-choice; BLEU: n-gram overlap score for string responses; MSE: mean squared error for numerical answers (large penalty: if prediction contains no number, MSE set to 1e10); extract-match: boolean match of extracted substrings.",
            "dataset_or_benchmark": "Dynamic Data (SciEval dynamic subset; chemistry ~2000 items, physics ~890 items as reported)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-4 attains best average accuracy and BLEU on Dynamic Data; Galactica-30B performs best on counting/calculation chemistry items; many LLMs retain near-random accuracy on physics dynamic items and show very high MSE on chemistry calculations.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Dynamic generation reduces leakage but requires careful maintenance; chemistry answers can be complex (strings & numbers) needing multiple metrics; some models lack numeric precision and fail to produce numbers causing extreme MSE.",
            "uuid": "e7787.3",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Experimental Data",
            "name_full": "Experimental Data subset (SciEval)",
            "brief_description": "A set of subjective open-ended questions derived from 12 basic scientific experiments intended to evaluate LLM research ability (principle, experiment design, data analysis, summarization).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Subset evaluated for models with appropriate context length and web-access models (GPT-4, GPT-3.5-turbo, Claude-series, ERNIE Bot, SparkDesk where applicable)",
            "model_size": "various",
            "scientific_domain": "General experimental sciences (basic experiments across biology/chemistry/physics)",
            "theory_type": "experimental design and analysis produced by LLMs",
            "evaluation_method_name": "Manual human assessment of open-ended experimental responses",
            "evaluation_method_description": "Each experiment comprises multiple open-ended questions; model outputs are evaluated manually by authors/reviewers for aspects like principle understanding, design quality, and result analysis.",
            "evaluation_metric": "qualitative manual scores (not numerically specified in the paper), per-experiment scores are reported in appendix but assessment process details are limited",
            "metric_definition": "Open-ended qualitative assessment; no standardized numeric scale or inter-rater agreement reported.",
            "dataset_or_benchmark": "Experimental Data (12 basic experiments collected from university lab courses within SciEval)",
            "human_evaluation_details": "Responses assessed manually; paper states manual assessment but does not report number of raters, scoring rubric, or inter-rater reliability.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Top-tier models (GPT-series, Claude-series) perform satisfactorily on experimental principle and design but struggle to analyze experimental results; other models perform poorly.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Manual evaluation lacks reported annotator details and rubric; subjective evaluation is labor-intensive and hard to replicate without more details.",
            "uuid": "e7787.4",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Accuracy",
            "name_full": "Accuracy (percentage correct)",
            "brief_description": "Primary quantitative metric for objective questions (Static Data and physics multiple-choice in Dynamic Data), measuring the fraction of correct answers produced by an LLM.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to all evaluated models (GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica, etc.)",
            "model_size": "n/a",
            "scientific_domain": "Biology, Chemistry, Physics (applies to objective question subsets)",
            "theory_type": "evaluation metric for answers/hypotheses phrased as multiple-choice or judgment",
            "evaluation_method_name": "Accuracy on objective items",
            "evaluation_method_description": "Compute the percentage of multiple-choice or judgment items for which the model's selected answer matches the ground-truth label.",
            "evaluation_metric": "accuracy (reported as percent or proportion)",
            "metric_definition": "Accuracy = (# correct answers / total # questions) * 100%; for 4-choice physics questions baseline random accuracy is 25%.",
            "dataset_or_benchmark": "Used on SciEval Static Data and physics Dynamic Data; also standard in other benchmarks compared (MMLU, AGIEval, C-EVAL).",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Paper reports averages per model (e.g., GPT-4 highest; only GPT-4, GPT-3.5-turbo and Claude-v1.3 exceed ~60% on Static Data).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Accuracy is unsuitable for open-ended subjective items and vulnerable to pretraining data leakage; does not measure explanation quality or reproducibility.",
            "uuid": "e7787.5",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "BLEU",
            "name_full": "BLEU score (Bilingual Evaluation Understudy)",
            "brief_description": "An automatic n-gram overlap metric used to evaluate string-valued answers (e.g., SMILES or other string representations) in the chemistry Dynamic Data subset.",
            "citation_title": "Bleu: a method for automatic evaluation of machine translation.",
            "mention_or_use": "use",
            "model_name": "Applied to models producing string outputs on chemistry Dynamic Data (e.g., GPT-4, Galactica, etc.)",
            "model_size": "n/a",
            "scientific_domain": "Chemistry (string-encoded outputs such as SMILES or textual descriptions)",
            "theory_type": "string-answer quality metric",
            "evaluation_method_name": "BLEU score for string answers",
            "evaluation_method_description": "Compute n-gram overlap between model-generated string and reference string using standard BLEU computation.",
            "evaluation_metric": "BLEU (score typically 0-100 or 0-1 depending on scaling; paper reports BLEU numeric values)",
            "metric_definition": "BLEU computed per Papineni et al. (2002); higher BLEU indicates greater n-gram overlap with reference.",
            "dataset_or_benchmark": "Applied to chemistry Dynamic Data in SciEval (string-answer items like SMILES)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "GPT-4 achieves highest average BLEU on Dynamic Data among evaluated models (specific BLEU numbers reported in tables).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "BLEU measures surface overlap and may not capture chemical equivalence (e.g., different valid SMILES for same molecule) or semantic correctness.",
            "uuid": "e7787.6",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MSE",
            "name_full": "Mean Squared Error (MSE) for numerical answers",
            "brief_description": "A numerical loss metric used to evaluate numerical outputs in chemistry Dynamic Data (e.g., molecular weight calculations), with a large penalty applied when models produce no numeric output.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to models producing numeric answers on chemistry Dynamic Data (GPT-4, Galactica, etc.)",
            "model_size": "n/a",
            "scientific_domain": "Chemistry (numerical calculation items) and other calculation tasks",
            "theory_type": "numerical error metric",
            "evaluation_method_name": "Mean Squared Error (MSE) for numerical predictions",
            "evaluation_method_description": "Compute mean squared error between model numeric predictions and ground truth numeric values; assign MSE = 1e10 when model outputs contain no numbers.",
            "evaluation_metric": "MSE (units = squared units of the target quantity; large sentinel value 1e10 for non-numeric outputs)",
            "metric_definition": "MSE = mean((prediction - ground_truth)^2) across items; sentinel rule: if prediction contains no number, MSE set to 1e10 to penalize non-numeric outputs.",
            "dataset_or_benchmark": "Chemistry portion of SciEval Dynamic Data",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Most LLMs exhibit very high MSE on chemistry numerical items indicating poor calculation precision; authors report extremely high MSEs and near-random performance for many models.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "MSE is sensitive to scale of the target variable and the sentinel rule heavily penalizes non-numeric responses; numeric formatting/units differences can affect results.",
            "uuid": "e7787.7",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "AnswerOnly/CoT/3-shot",
            "name_full": "Prompting & evaluation settings: Answer-Only (AO), Chain-of-Thought (CoT), 3-shot",
            "brief_description": "Three evaluation protocols applied to LLMs: AO requires only the final answer; CoT asks models to produce reasoning chains before answers; 3-shot provides three exemplars from dev for few-shot learning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied across models (GPT-4, GPT-3.5-turbo, Claude, Galactica, etc.)",
            "model_size": "various",
            "scientific_domain": "Cross-domain (applied to all SciEval subsets)",
            "theory_type": "evaluation procedure / prompting strategy",
            "evaluation_method_name": "AO / CoT / 3-shot evaluation settings",
            "evaluation_method_description": "AO: model gives only final answer; CoT: model is prompted to produce internal reasoning steps (Chain-of-Thought) before final answer; 3-shot: model given 3 exemplar Q&A pairs from dev set to improve performance.",
            "evaluation_metric": "Comparative accuracy/BLEU/MSE as reported per setting",
            "metric_definition": "Same metrics (accuracy, BLEU, MSE) are applied to outputs collected under each prompting setting to measure effect of reasoning traces or few-shot exemplars.",
            "dataset_or_benchmark": "Applied to SciEval subsets (AO and CoT prompts shown in paper figures; 3-shot uses dev exemplars)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "CoT improves performance primarily for GPT-series models on Static Data; 3-shot improves roughly half of LLMs; CoT and 3-shot significantly help chemistry Dynamic Data but have limited effect on physics Dynamic Data for many models.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Not all models support CoT or return reliable chains; model API limitations prevented CoT/3-shot evaluation for some models (e.g., Claude variants in some settings).",
            "uuid": "e7787.8",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "IntegratedDatasets",
            "name_full": "Socratic Q&A, MedQA (USMLE), PubMedQA, Reagent Selection (ChemLLMBench), PubChem dev",
            "brief_description": "Public and community data sources incorporated into SciEval Static Data: Socratic Q&A as primary source, supplemented by MedQA (USMLE), PubMedQA samples, Reagent Selection subset, and PubChem for molecular info/dev links.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Used to construct Static Data and dev exemplars for model evaluation (applies to evaluated models)",
            "model_size": "n/a",
            "scientific_domain": "Medicine/biomedical (MedQA, PubMedQA), general science (Socratic), chemistry (Reagent Selection, PubChem)",
            "theory_type": "datasets / corpora used to evaluate knowledge and comprehension",
            "evaluation_method_name": "Integration of community Q&A and established datasets into Static Data",
            "evaluation_method_description": "Crawl and rule-based filtering of Socratic Q&A; use GPT-4 to convert open-ended Q&A into multiple-choice by simplifying answers and generating distractors; incorporate test subsets from MedQA/USMLE, selected PubMedQA items framed as judgment, and a sampled portion of Reagent Selection as multiple-choice.",
            "evaluation_metric": "accuracy on resulting multiple-choice items; judgment accuracy for PubMedQA-style yes/no/maybe",
            "metric_definition": "Accuracy computed per item type (multi-choice or judgment); dev set composed of 5 examples per source/domain for few-shot.",
            "dataset_or_benchmark": "Socratic Q&A (primary), MedQA (USMLE test set), PubMedQA (1000 expert-annotated items), Reagent Selection subset (ChemLLMBench), PubChem links for dynamic generation/dev",
            "human_evaluation_details": "Authors manually checked GPT-4 generated distractors and classifications to ensure data quality; no annotator counts provided.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "These integrated sources form the majority of Static Data; models' performances on this combined Static Data are reported (e.g., GPT-4 highest, only three models &gt;60% avg accuracy).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Conversion of open-ended community QA to multiple-choice relied on GPT-4 generation and manual checks, which may introduce bias; Static sources remain vulnerable to pretraining leakage.",
            "uuid": "e7787.9",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding.",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Holistic evaluation of language models.",
            "rating": 2,
            "sanitized_title": "holistic_evaluation_of_language_models"
        },
        {
            "paper_title": "C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models.",
            "rating": 2,
            "sanitized_title": "ceval_a_multilevel_multidiscipline_chinese_evaluation_suite_for_foundation_models"
        },
        {
            "paper_title": "MultiMedQA: a benchmark for medical question answering.",
            "rating": 2,
            "sanitized_title": "multimedqa_a_benchmark_for_medical_question_answering"
        },
        {
            "paper_title": "MATH: Measuring mathematical problem solving with the math dataset.",
            "rating": 2,
            "sanitized_title": "math_measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Chem-LLMBench",
            "rating": 2,
            "sanitized_title": "chemllmbench"
        },
        {
            "paper_title": "Bleu: a method for automatic evaluation of machine translation.",
            "rating": 2,
            "sanitized_title": "bleu_a_method_for_automatic_evaluation_of_machine_translation"
        },
        {
            "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models.",
            "rating": 2,
            "sanitized_title": "scibench_evaluating_collegelevel_scientific_problemsolving_abilities_of_large_language_models"
        }
    ],
    "cost": 0.01587275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</p>
<p>Liangtai Sun 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Yang Han csyanghan@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zihan Zhao 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Da Ma 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zhennan Shen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Baocai Chen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Kai Yu kai.yu@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research
8DE5F8747D39882235665D42A59CA88C
Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research.Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research.However, current benchmarks are mostly based on pre-collected objective questions.This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability.In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues.Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability.In particular, we design a "dynamic" subset based on scientific principles to prevent evaluation from potential data leakage.Both objective and subjective questions are included in SciEval.These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs.Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions.The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as ChatGPT (Schulman et al. 2022), have attracted widespread attention in general scenarios, including information search, code generation, and more.In the field of science, LLMs have also shown preliminary potential in improving scientific research efficiency and transforming scientific research paradigms (Blanco-Gonzalez et al. 2023;WANG and MIAO 2023).In the meanwhile, several scientific LLMs have been proposed by researchers (Taylor et al. 2022;Luo et al. 2022;Frey et al. 2022).In the general field, there are already numerous evaluation benchmarks to evaluate the language understanding, language generation and reasoning capabilities of LLMs, such as MMLU (Hendrycks et al. 2020), AGIEval (Zhong et al. 2023), and C-EVAL (Huang et al. 2023), shown in Table 1.Although these benchmarks cover data of science domain, the data sources are usually confined to educational materials, which can not adequately as-sess the research ability of LLMs and not align with real-life scientific research scenarios.In addition, some benchmarks have been proposed to evaluate the scientific capability of LLMs, such as MultiMedQA (Singhal et al. 2023), Chem-LLMBench (Guo et al. 2023), and MATH (Hendrycks et al. 2021), while these benchmarks are restricted to a specific scientific discipline, leaving a lack of a more general scientific evaluation benchmark. 1In addition, these benchmarks (1) lack evaluation systems for scientific capabilities, (2) are all based on objective questions, which are insufficient to assess scientific abilities, and (3) face the risk of data leakage.</p>
<p>In response to this gap, we present SciEval, an English benchmark designed to evaluate advanced abilities of LLMs in the scientific domain.SciEval consists of a total of about 18000 challenging scientific questions, spanning three important basic science fields: chemistry, physics and biology, each of which is further divided into multiple sub-topics.SciEval mainly has the following three characteristics:</p>
<p>â€¢ Multi-level and comprehensive evaluation of the ability of LLMs in the scientific field.Scientific ability of LLMs needs to be evaluated from multiple aspects.Leveraging cognitive domains of Bloom's taxonomy (Krathwohl 2002;Forehand 2010), which covers six levels, SciEval evaluates the scientific capabilities of large language models across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability, where each capability aligns with one or more cognitive levels.â€¢ Combination of objective and subjective questions.</p>
<p>SciEval is mainly based on objective questions, which allow for quick and standard model evaluations, involving multiple-choice, fill-in-the-blank, and judgment questions.These questions can help us understand whether the model can correctly understand and memorize scientific knowledge.However, objective questions are insufficient to assess scientific capability holistically.To better assess scientific reasoning and application ability, SciEval introduces a small number of subjective questions, involving a total of twelve basic science experiments, which is named Experimental Data.We conduct experiments to evaluate LLMs on SciEval in answer-only, chain-of-thought and few-shot settings.Results indicate that GPT-4 is the strongest model, with only GPT-4, GPT-3.5-turbo and Claude-v1.3surpassing 60% average accuracy on Static Data, signifying considerable opportunities for improvement.With the results of Dynamic Data, we find that these LLMs have little knowledge about molecules, and most models could only retain near-random accuracy in the physics subset.As for Experimental Data, some top-tier models could perform satisfactorily in experimental principle and design, while almost all models struggle to analyze the experimental results.With the analysis of experiment results, we claim that training on large-scale scientific corpus is helpful for the scientific ability of LLMs, and most LLMs perform bad on calculation problems, especially in physics domain.We hope SciEval can provide an excellent benchmark for the assessment of scientific capability of LLMs, and promote wide application in science.</p>
<p>Related Work</p>
<p>General Benchmarks for LLMs</p>
<p>To evaluate the performance of LLMs across different tasks, several benchmarks have been proposed.MMLU (Hendrycks et al. 2020) aims to develop a comprehensive test for evaluating text models in multi-task contexts.HELM (Liang et al. 2022) offers a comprehensive assessment, evaluating LLMs across various aspects, such as language understanding and common-sense reasoning.Big-Bench (Srivastava et al. 2022) introduces 204 challenging tasks covering various domains, aiming to evaluate tasks beyond the capabilities of existing language models.AGIEval (Zhong et al. 2023) serves as an evaluation framework for assessing the performance of foundation models in human-centric standardized exams.C-Eval (Huang et al. 2023) assesses the advanced knowledge and reasoning capabilities of foundation models in Chinese.</p>
<p>Specific Benchmarks for LLMs</p>
<p>Apart from general tasks, specific benchmarks are designed for certain downstream tasks.MultiMedQA (Singhal et al. 2023) focuses on medical question-answering, evaluating LLMs in terms of clinical knowledge and QA abilities.MATH (Hendrycks et al. 2021) assesses reasoning and problem-solving proficiencies of LLMs in mathematics.Sci-enceQA (Lu et al. 2022) proposes a multi-modal benchmark with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations, collected from elementary and high school science curricula.SCIBENCH (Wang et al. 2023) examines the reasoning capabilities required for complex scientific problem-solving and proposes two datasets of college-level scientific problems.Compared to these benchmarks, SciEval (1) evaluates scientific capabilities from multiple aspects, having a broader coverage, (2) uses data of community Q&amp;A, which is more flexible and diverse, (3) designs a subset of dynamic data, making an effort to mitigate data leakage.</p>
<p>3 The SciEval Dataset cognitive domain is frequently used to structure curriculum learning objectives, assessments and activities, and is broken into six levels: Remember, Understand, Apply, Analyze, Evaluate and Create, as is shown in Figure 1, which are suitable for the evaluation of scientific capability.</p>
<p>Based on the cognitive domain of Bloom's taxonomy, the evaluation system of SciEval consists of four knowledge dimensions: Basic Knowledge (BK), Knowledge Application (KA), Scientific Calculation (SC), and Research Ability (RA).As is shown in Figure 1, BK primarily assesses the fundamental scientific knowledge of LLMs.KA focuses on how to apply basic knowledge to solve scientific problems, requiring models to have comprehension, application, and analysis abilities.SC is a specialized application of knowledge that further examines complex reasoning capabilities of LLMs based on their general knowledge application abilities.RA assesses evaluation capabilities at a higher cognitive level, requiring models to participate in various aspects of scientific research, including problem formulation, experimental design, data analysis, and summarization.</p>
<p>Based on the evaluation system, we design three different types of data: Static Data, Dynamic Data, and Experimental Data.The Static Data covers all these four knowledge dimensions and will remain constant throughout, while the Dynamic Data examines from the aspects of Knowledge Application and Scientific Calculation and will be regularly updated to prevent any data leakage.The Experimental Data comprises a set of questions for twelve scientific experiments and can be used to evaluate the Research Ability.</p>
<p>Data Collection</p>
<p>Static Data The collection steps of Static Data are shown in Figure 2. The primary source of Static Data is Socratic Q&amp;A 2 , a community-driven website that covers a wide range of subjects such as science and literature.Specifically, we collect data from the fields of biology, chemistry, and physics.To ensure quality, we employ rule-based methods 2 https://socratic.org to preprocess the crawled data.While gathering the questions, we found that not all of them are suitable as titles.To address this, we utilize GPT-4 with the "Task 1" prompt, as depicted in Figure 2, to process these questions.Since most of the collected questions are open-ended and challenging to evaluate, we employ GPT-4 to simplify ground-truth answers and generate three wrong answers to formulate them as multiple-choice questions.Additionally, we classify the questions into their respective knowledge domains.And during this process, we manually check the generated content of GPT-4 to ensure data quality.</p>
<p>To make the dataset more diverse and comprehensive, we further integrate data from some publicly available datasets:</p>
<p>â€¢ MedQA (Jin et al. 2021) is a free-form multiple-choice OpenQA dataset for solving medical problems, collected from professional medical board exams.We use the test set of USMLE, which is the English subset of MedQA.</p>
<p>â€¢ PubMedQA (Jin et al. 2019) is a biomedical questionanswering dataset collected from PubMed abstracts.The task of PubMedQA is to answer research questions with yes/no/maybe using the corresponding abstracts, which is fit for evaluating the literature comprehension ability.We incorporate 1000 expert-annotated data from it and frame them as judgment questions.</p>
<p>â€¢ Reagent Selection (Guo et al. 2023) involves the identification and proposal of the most fitting reagents for a specific chemical reaction or process, which is a subset of ChemLLMBench.We randomly select 40% data and formulate them as multiple-choice questions.</p>
<p>Dynamic Data</p>
<p>The current training of LLMs often uses a large amount of data, resulting in a risk of data leakage for evaluation.In order to solve this problem, we design a "dynamic" subset, which can generate data dynamically according to scientific principles.The dynamic subset covers two disciplines, chemistry and physics.For chemistry data, we use the basic information and properties of molecules</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)</p>
<p>Socratic Q&amp;A</p>
<p>Crawl &amp; Filter</p>
<p>Raw Data</p>
<p>GPT-4 Filtered Data</p>
<p>Instruction: Given a question and its ground-truth answer, judge whether it is suitable to be used as the title of a multiple-choice question.Your answer should be "YES" or "NO".And please directly give the results without any explanation.</p>
<p>Task 1</p>
<p>Instruction: Given a question and a ground-truth answer, please simplify the answer as concise as possible.And I want to generate a 4-choice question using it, please generate 3 fake answers for me.Note that the length of the simplified answer and these 3 fake answers should be about the same and these 3 fake answers should be as confusing as possible.Furthermore, please help me to classify the domain of the question.There are three domains in total: Base Knowledge, Scientific Calculation, Knowledge Application.For physics data, we manually write some Python scripts according to the physics formulas.When obtaining the evaluation dataset, we will provide a regenerated version to users and we will update it regularly, while at the same time, we will maintain a stable version of the dynamic data to make a fair comparison.</p>
<p>Experimental Data To better evaluate the scientific thoughts and abilities of LLMs, SciEval introduces a subset of experimental data, involving 12 different basic scientific experiments.These experiments are collected from basic science experiment courses at university, and each experiment conducts a comprehensive investigation of the ability of LLMs in scientific research and experimentation from the perspectives of experimental principle, process, and analysis and summarization of experimental results.</p>
<p>Data Statistics</p>
<p>Summarized statistics are shown in For Static Data, we further split the data into dev, valid, and test set.For each data source, each knowledge domain, and each discipline, we randomly select 5 data to form the 3 https://pubchem.ncbi.nlm.nih.gov/dev set, which can be used for few-shot learning, and we split the remaining data with a ratio of 1:9 to construct the valid set and test set respectively.</p>
<p>Experiment</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".Please directly give the answer without any explanation.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".How many atoms are in 3.5 moles of arsenic atoms?</p>
<p>Experiment Setup</p>
<p>Prompts We evaluate LLMs in both Answer-Only (AO) and Chain-Of-Thought (CoT) (Kojima et al. 2022) settings.The prompts we used are shown in Figures 3 and 4 respectively.Furthermore, we also evaluate using 3-shot setting, where the three exemplars are selected from the dev set.Models In order to comprehensively assess the scientific capabilities of Large Language Models (LLMs), we evaluate 15 high-performing LLMs that are widely accessible.These models are selected to represent a diverse range of organizations and vary in size.The details of these models are summarized in Table 3.</p>
<p>Model</p>
<p>â€¢ GPT-3.5-turbo and GPT-4 (Schulman et al. 2022;Ope-nAI 2023) are the strongest GPT model variants from OpenAI that have undergone pretraining, instruction tuning, and reinforcement learning from human feedback (RLHF, (Ouyang et al. 2022)).â€¢ Claude 4 , developed by Anthropic, is often considered 4 https://www.anthropic.com/index/introducing-claude.comparable to GPT-3.5-turbo.We evaluate both the Claude-v1.We evaluate GPT-3.5-turbo,GPT4 and Claude on all three subsets, including Static Data, Dynamic Data, and Experimental Data.Since we can only assess ERNIE Bot and SparkDesk through web interface, we evaluate these two models on the Experimental Data.And for the rest LLMs with billions or tens of billions of parameters, since the length of the Experimental Data exceeds the length limit of these models7 , we evaluate them on Static Data and Dynamic Data, as is shown in Table 3.</p>
<p>Evaluation Metrics In the case of Static Data, all questions are objective, making accuracy the appropriate evaluation metric.For Dynamic Data, the physics questions are presented as multiple-choice questions, which can also be evaluated using accuracy.Conversely, the chemistry questions involve complex components, such as "What is the</p>
<p>Experiment Results</p>
<p>Answer-Only Setting Answer-only results of all the models on the test set are shown in Table 4 and detailed results of Static Data across different knowledge domains are provided in Appendix B. Analyzing the results of Static Data, GPT-4 demonstrates significantly superior performance compared to other models.And only GPT-4, GPT-3.5-turbo, and Claude-v1.3achieve an average accuracy exceeding 60%, which highlights the challenge posed by SciEval.</p>
<p>For the results of Dynamic Data, GPT-4 performs the best in terms of average accuracy and BLEU score.However, for counting and calculation questions, Galactica-30B yields the best results, indicating its strong aptitude in the field of science.Conversely, models with billions or tens of billions of parameters perform poorly on the chemistry subset, suggesting their limited knowledge about molecules.Regarding the performance of models on the physics subset, since all questions are 4-choices questions, the accuracy should be at least 25%.However, none of these models achieve satisfactory results in this subset.</p>
<p>As for Experimental Data, GPT-series models and Claude-series models achieve good results, while the other two models are not.The detailed scores models reached in each experiment are shown in Appendix C.However, although some models could get a great performance, during experiments, we find that these models are good at experimental principles and designing, while when it comes to analyzing the experiment results, the performances are not satisfying.</p>
<p>CoT Setting and 3-Shot setting Comparison of experiment results among Answer-Only, Chain-of-Thought and 3-Shot settings are shown in Figure 5 and Table 5. 9 And we refer detailed results to Appendix A and B.</p>
<p>The experimental results from Static Data reveal that solely the GPT-series LLMs get performance enhancement within the CoT setting due to the limited CoT capabilities of other LLMs.As for the 3-Shot setting, roughly half of the LLMs analyzed demonstrate superior performances relative to the Answer-Only setting.The performances of the remaining LLMs are closely similar to those observed within the Answer-Only setting.</p>
<p>From the experimental results of Dynamic Data, it is observed that both CoT and 3-Shot significantly enhance the performance of most Language Learning Models (LLMs) in the chemistry subset.However, the performances achieved are still not up to the mark.In the physics subset, the impact of CoT and 3-Shot on most LLMs is less pronounced, resulting in nearly random performances.Under the CoT setting, GPT-3.5-turboachieves an accuracy of 47.19, suggesting a robust understanding of physical principles.Conversely, the performance of GPT-4 is markedly poor, from which we find that despite its extensive knowledge of physical principles, it frequently employs incorrect formulas to solve problems.Nevertheless, GPT-4 attains an accuracy of 51.01 under 3-Shot setting, the highest among all models, demonstrating its ability to learn from a mere three examples.</p>
<p>Discussion</p>
<p>Training on large-scale scientific corpus is helpful.Based on experimental results (Table 4), Galactica (Taylor et al. 2022), which has been trained on an extensive scientific corpus, significantly outperforms other LLMs with a comparable number of parameters, although Galactica is trained with a much smaller amount of data.Remarkably, when tested on Dynamic Data, Galactica surpasses the GPTseries and Claude-series LLMs in computational problems.</p>
<p>Most LLMs perform bad on calculation problems, especially in physics domain.Detailed results across various knowledge domains on Static Data (refer to Appendix B) reveal that most LLMs underperform in the Scientific Calculation domain, while demonstrate relatively superior performance in other domains, which is particularly acute in the field of physics.Similar issues are also observed in Dynamic Data and Experimental Data.In the context of Dynamic Data, the mean square error, employed to evaluate calculation abilities within the chemistry subset, is exceedingly high for most LLMs, and almost all LLMs can only achieve nearly random performance within the physics subset.Regarding Experimental Data, our findings indicate that these LLMs struggle with the analysis of experimental results.</p>
<p>Conclusion</p>
<p>In this paper, we introduce SciEval, a benchmark designed to evaluate scientific capabilities of LLMs.SciEval comprises about 18,000 challenging scientific questions, covering three fundamental fields of science.SciEval assesses the scientific ability of LLMs across four dimensions.It incorporates both objective and subjective questions, and employs dynamic data generation to mitigate potential data leakage.We conduct comprehensive experiments on various advanced LLMs using SciEval and perform thorough analyses.Our experimental results reveal that most LLMs do not perform well on our benchmark, with the exception of the GPT-series and Claude-series LLMs.We hope that SciEval can serve as a robust benchmark for assessing scientific capabilities of LLMs.</p>
<p>Figure 1 :
1
Figure 1: The illustration of the evaluation system.SciEval covers three disciplines with amounts of sub-topics, and investigates four abilities, corresponding to six cognitive levels.</p>
<p>Figure 2 :
2
Figure 2: Data Collection steps of Static Data</p>
<p>Figure 3 :
3
Figure 3: An example of the prompt we used for AO setting.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>AFigure 4 :
4
Figure 4: An example of the prompt we used for CoT setting.The red text is the response from the model, while the blue text and black text are the inputted prompt.</p>
<p>Figure 5: Accuracy on Answer Only, Chain-of-Thought and 3-Shot settings of each LLMs for Static Data.</p>
<p>Table 1 :
1
Dataset comparison of SciEval and some other datasets covering science domain."BK"stands for Basic Knowledge, "KA" stands for Knowledge Application, "SC" stands for Scientific Calculation, and "RA" stands for Research Ability.
NameCategoryAbilitySourceData Type Dynamic #DataMMLUhumanities, social science, STEM, otherBK, KA, SCexam, book, course objective14079AGIEvalsocial science, STEM BK, KA, SCexamobjective8062C-EVALhumanities, social science, STEM, otherBK, KA, SCexamobjective12342MultiMedQAmedicalBK, KA, RAexam, researchobjective13115ChemLLMBench chemistryBK,KAknowledge baseobjective800MATHmathematicsSCexamobjective5000SciEvalscienceBK, KA,SC, RAcommunity QA, knowledge baseobjective + subjective15901model perfor-mance. And the objective questions other than DynamicData are referred to as Static Data.
â€¢ Dynamic data generation based on basic scientific principles.The huge amount of training data used for pre-training LLMs may cause the risk of data leakage for evaluation.In order to solve this problem, one of the main features of SciEval is the use of Dynamic Data, which can prevent potential data leakage and ensure the fairness and credibility of the evaluation results.The Dynamic Data will be updated regularly, and we will maintain a stable version to make a fair comparison of</p>
<p>Table 2
2, where we onlycount Static Data. For Dynamic Data, the chemistry part ex-amines the KA ability and contains 2000 data, while thephysics part evaluates the SC ability and involves 890 data.All these questions are in English and we show some dataexamples in Appendix D.AbilityBioChem PhyBasic Knowledge2147 2914456Knowledge Application 1379 372036Scientific Calculation3013401 1165Research Ability100000Total4830 10035 1657</p>
<p>Table 2 :
2
Statistics of Static Data</p>
<p>Table 3 :
3
Models evaluated in this paper.The "access" columns show whether we have full access to the model weights or we can only access through API or web.SD stands for Static Data, DD stands for Dynamic Data, and ED stands for Experimental Data.Marking " " means we evaluate the corresponding model on this subset.
Creator#Parameters Access SD DD EDGPT-4OpenAIundisclosedAPIGPT-3.5-turboOpenAIundisclosedAPIClaude-v1.3AnthropicundisclosedAPIClaude-instant-v1.1 AnthropicundisclosedAPIERNIE BotBaiduundisclosedWebSparkDeskiFLYTEKundisclosedWebVicunaLMSYS13BWeightsGalacticaMeta30B, 6.7BWeightsChatGLM2Tsinghua6BWeightsChatGLMTsinghua6BWeightsAlpacaStanford7BWeightsMOSSFudan16BWeightsLLaMaMeta7B, 13BWeightsModelStatic Data Biology Chemistry Physics Avg.Chemistry(DD) Acc. BLEU MSEPhysics(DD) Exp Acc. ScoreGPT-484.4969.3865.2273.93 11.05 23.78891.0925.8493.31GPT-3.5-turbo76.4264.3052.3066.97 7.6518.862008.7221.8088.27Claude-v1.372.5859.7254.9463.45 5.7521.981489.8726.1485.73Claude-instant-v1.170.4353.3652.3058.92 0.4516.078258.4621.4687.50Galactica-30B66.4850.1644.6554.960.94.14485.9922.47-Vicuna-13B58.3953.0645.1353.93 0.956.50766.6421.24-Galactica-6.7B57.8450.7730.9950.87 1.556.475519.8220.79-ChatGLM2-6B58.6244.0040.2648.440.21.863449.4424.83-ChatGLM-6B52.5445.3640.8047.23 0.752.4410303.9021.01-Alpaca-7B56.6642.4337.0146.540.22.92428419.2726.74-MOSS-16B47.7133.8731.7338.230.17.3730505.1724.27-LLaMa-13B48.5933.5619.4836.960.35.213707.017.08-LLaMa-7B36.2426.3815.0228.370.51.2611305.6514.38-ERNIE Bot--------61.12SparkDesk--------33.69</p>
<p>Table 4 :
4
Model performances of Answer-Only setting.The leaderboard is sorted by the average accuracy of Static Data.</p>
<p>Table 5 :
5
Results on Answer-Only, Chain-of-Thought and 3-Shot settings of each LLM for Dynamic Data.â†‘ means the performance is slightly better than that under Answer-Only setting, â†“ means worse, and âˆ¼ means the performance is nearly the same.
ModelAOChemistry CoT3-ShotAOPhysics CoT3-ShotGPT-411.05 11.65 â†‘ 12.42â†‘ 25.84 17.98 â†“ 51.01 â†‘GPT-3.5-turbo7.65 10.20 â†‘ 8.85 â†‘ 21.80 47.19 â†‘ 25.39 âˆ¼Galactica-6.7B1.551.75 â†‘3.05 â†‘ 20.79 23.37 âˆ¼ 21.12 âˆ¼Vicuna-13B0.951.95 â†‘1.80 â†‘ 21.24 18.65 âˆ¼ 23.37âˆ¼Galactica-30B0.902.60 â†‘3.30 â†‘ 22.47 14.72 â†“ 22.58 âˆ¼ChatGLM-6B0.750.80 â†‘1.15 â†‘ 21.01 25.39 âˆ¼ 23.37 âˆ¼LLaMa-7B0.500.10 â†“1.55 â†‘ 18.659.66 â†“27.53 â†‘LLaMa-13B0.300.25 âˆ¼ 2.11 â†‘7.085.84 âˆ¼22.70 â†‘ChatGLM2-6B 0.202.65 â†‘1.60 â†‘ 24.83 25.39 âˆ¼ 26.74 âˆ¼Alpaca-7B0.200.65 â†‘2.10 â†‘ 26.71 28.43 âˆ¼ 25.62 âˆ¼MOSS-16B0.100.85 â†‘0.65 â†‘ 24.27 25.06 âˆ¼ 26.40 âˆ¼
Due to the page limitation, we only compare some widely used benchmarks. For more information, we refer to(Chang et al.<br />
).The Thirty-Eighth AAAI Conference on Artificial Intelligence 
Scientific research requires different dimensions of knowledge, such as understanding and calculation, thence evaluation of scientific ability should be conducted at multiple levels. Bloom's taxonomy is a set of three hierarchical methods used for classification of educational learning objectives covering cognitive, affective and psychomotor domains. TheThe Thirty-Eighth AAAI Conference on Artificial Intelligence 
The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
https://yiyan.baidu.com/
https://xinghuo.xfyun.cn/ The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
The maximum context length of ChatGLM2 is extended to 32k, while it has limited ability to understand long texts. molecular weight of A?" and "What is the SMILES expression of B?". Hence, for questions with numerical answers, we employ
MSE 8 as the evaluation metric, while for questions with string answers, we utilize the BELU score(Papineni et al. 2002). Additionally, we also calculate the extract match scores. As for Experimental Data, each experiment consists of multiple open-ended questions. As a result, we assess the model-generated responses manually.
If the predictions do not contain any number, we will regard the MSE as 1 Ã— 10 10The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
When evaluating on CoT and 3-Shot settings, Claude-Instant and Claude are not available for us, due to the limitation of API.
AcknowledgementsThis work is funded by the China NSFC Projects (92370206, U23B2057, 62106142 and 62120106006) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).
The role of ai in drug discovery: challenges, opportunities, and strategies. A Blanco-Gonzalez, A Cabezon, A Seco-Gonzalez, D Conde-Torres, P Antelo-Riveiro, A Pineiro, R Garcia-Fandino, Pharmaceuticals. 1668912023</p>
<p>Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.03109A survey on evaluation of large language models. 2023arXiv preprint</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>M Forehand, Blooms taxonomy. Emerging perspectives on learning, teaching, and technology. 201041</p>
<p>N Frey, R Soklaski, S Axelrod, S Samsi, R Gomez-Bombarelli, C Coley, V Gadepally, Neural scaling of deep chemical models. 2022</p>
<p>What indeed can GPT models do in chemistry?. T Guo, K Guo, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, arXiv:2305.18365A comprehensive benchmark on eight tasks. 2023arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, J Lei, arXiv:2305.083222023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>A revision of Bloom's taxonomy: An overview. Theory into practice. D R Krathwohl, 200241</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2303.08774Advances in Neural Information Processing Systems. 202235Technical Report</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>ChatGPT: Optimizing language models for dialogue. J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, J F C Uribe, L Fedus, L Metz, M Pokorny, Nature. 2022. 2023Large language models encode clinical knowledge</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>MOSS: Training Conversational Language Models from Synthetic Data. T Sun, X Zhang, Z He, P Li, Q Cheng, H Yan, X Liu, Y Shao, Q Tang, X Zhao, K Chen, Y Zheng, Z Zhou, R Li, J Zhan, Y Zhou, L Li, X Yang, L Wu, Z Yin, X Huang, X Qiu, R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, N Hambro, E Azhar, F , arXiv:2302.13971Novel Paradigm for AIdriven Scientific Research: From AI4S to Intelligent Science. G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B RoziÃ¨re, Goyal, 2023. 2023. 2022. 2023. 202338arXiv preprintLlama: Open and efficient foundation language models</p>
<p>X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.10635SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. 2023arXiv preprint</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, arXiv:2306.05685Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023arXiv preprint</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, arXiv:2304.063642023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
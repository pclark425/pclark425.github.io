<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9690 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9690</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9690</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-261214653</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/29872/31521" target="_blank">SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</a></p>
                <p><strong>Paper Abstract:</strong> Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a"dynamic"subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9690.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9690.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-disciplinary benchmark (≈18,000 questions) that evaluates LLM scientific ability across four dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) using static, dynamic and experimental subsets guided by Bloom's taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple LLMs (evaluated: GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica-30B, Vicuna-13B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>N/A — SciEval is a benchmark (not an LLM); evaluated models include API-access models (GPT-4, GPT-3.5-turbo, Claude-v1.3) and open-weight models (Galactica, Vicuna, LLaMa variants).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-disciplinary: biology, chemistry, physics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated evaluation on objective questions (accuracy) and dynamic generated items; BLEU and MSE for chemistry dynamic items; manual scoring for open-ended Experimental Data; experimental conditions include Answer-Only (AO), Chain-of-Thought (CoT) and 3-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for objective questions; BLEU and extract-match for string outputs; Mean Squared Error (MSE) for numeric chemistry answers; manual qualitative scoring for Experimental Data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SciEval — static objective dataset (derived from community Q&A and public datasets), a dynamic subset generated from scientific principles to avoid data leakage, and an Experimental Data subset of 12 basic experiments (subjective/open-ended).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Authors report that GPT-4 is the strongest model on SciEval (Static Data average ≈73.9%), with only GPT-4, GPT-3.5-turbo and Claude-v1.3 exceeding 60% average accuracy on Static Data. Dynamic Data and Experimental Data expose major weaknesses (low chemistry dynamic accuracy and nearly random physics dynamic performance; experimental analysis is generally poor).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of data leakage from pretraining addressed via dynamic data generation; subjective Experimental Data requires manual scoring; dynamic items require special metrics (BLEU/MSE); benchmark highlights LLM calculation and experimental-result-analysis weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No direct head-to-head comparison to human expert performance reported; Experimental Data responses are manually evaluated (human-in-the-loop) but no formal human baseline is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multi-level evaluation based on Bloom's taxonomy, combine objective and subjective tasks, include a dynamic subset to mitigate data leakage, evaluate under AO/CoT/3-shot conditions, and use mixed automated (accuracy, BLEU, MSE) and manual evaluation for open-ended experimental outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9690.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval Static Data (objective subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fixed set of objective questions (multiple-choice, fill-in-the-blank, judgement) drawn from Socratic Q&A and integrated public datasets to measure recall and application across BK/KA/SC/RA dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple LLMs (evaluated: GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica-30B, and others)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models evaluated via API or weights; some models only web/API accessible per Table 3 in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology, Chemistry, Physics (multi-topic coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated accuracy on objective items; split into dev/valid/test; small dev set used for few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy (A/B/C/D choice correctness) is the main metric for Static Data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Static Data — curated objective questions (~15,901 overall in SciEval; Static portion drawn from Socratic Q&A, MedQA, PubMedQA, Reagent Selection subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>On Static Data only GPT-4, GPT-3.5-turbo and Claude-v1.3 exceed 60% average accuracy; GPT-4 achieves the highest static average (reported ≈73.93%). Many smaller models perform substantially worse (some ≈30–60% or lower depending on size).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Static items risk being present in LLM pretraining corpora; hence the paper includes dynamic data to reduce leakage; objective-only evaluation misses open-ended research skills.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Static Data uses standard objective-scoring analogous to exam-style evaluation; no explicit human baseline reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use objective questions for quick, reproducible scoring but supplement with dynamic and subjective tasks to fairly evaluate research-level capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9690.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval Dynamic Data (generated-by-principles subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A regularly updated subset that generates chemistry and physics items algorithmically from scientific principles to avoid data leakage from model pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple LLMs (evaluated: GPT-4, GPT-3.5-turbo, Galactica variants, Vicuna, LLaMa, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Evaluated models vary in access mode and size; dynamic subset intended to be unseen training data for all models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (KA/calculation) and Physics (SC/calculation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For chemistry dynamic items: string/numeric answers evaluated with BLEU (string), MSE (numeric) and extract-match; for physics dynamic items (multiple-choice) evaluated with accuracy. Dataset is regenerated periodically and a stable version is maintained for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for physics multiple-choice; BLEU and extract-match for string responses; MSE for numerical chemistry answers (if no numeric output, MSE treated as large penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Dynamic Data — chemistry subset (≈2000 items; KA) and physics subset (≈890 items; SC) generated by scripts or using molecule properties and physics formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Overall poor performance: GPT-4 attains the best average accuracy and BLEU among tested models but chemistry dynamic accuracy values are low (e.g., GPT-4 chemistry (DD) Acc reported ≈11.05 with BLEU ≈23.78) and many models perform near random on physics dynamic items (≈25% baseline). Galactica yields comparatively strong results on counting and calculation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Chemistry dynamic MSE extremely high for most models; physics dynamic often near-random indicating weak calculation/formula application; dynamic generation requires careful metric selection and regular updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Dynamic items are designed to be novel and thus more akin to testing reasoning than recall; no explicit human performance baselines reported.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use dynamic generation to mitigate pretraining leakage; evaluate numeric outputs with MSE and string outputs with BLEU/extract-match; maintain both regenerated and stable versions for fair longitudinal comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9690.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval Experimental Data (12 basic experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A subjective subset composed of 12 basic science experiments (collected from university experiment courses) to probe Research Ability: experimental principle, design, analysis and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple LLMs evaluated (GPT-4, GPT-3.5-turbo, Claude-v1.3; ERNIE Bot and SparkDesk evaluated via web where feasible)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Experimental Data evaluated primarily for models with sufficient context window or web/API access (some large open-weight models excluded due to length limits).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Basic experimental science across biology, chemistry, physics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual human evaluation of open-ended answers for experimental principle, design, and analysis; some models evaluated via web interfaces when API/weight access lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Manual qualitative scoring across criteria: principle understanding, experimental design quality, and experimental result analysis/summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Experimental Data — twelve full experiments with open-ended questions requiring written responses; constructed to evaluate higher-order research skills (RA).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Top-tier models (GPT-series, Claude-series) perform acceptably on experimental principles and design, but almost all models struggle with experimental-result analysis and summarization; detailed experiment-level scores are provided in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subjectivity requires manual scoring; long prompts exceed context limits for many models; comparisons are less automatable and reproducibility depends on human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Evaluation of Experimental Data uses human raters (manual assessment) similar to human peer review of experimental descriptions, but no formal human baseline is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include open-ended experimental tasks when evaluating research abilities; use manual scoring rubrics; ensure models evaluated have sufficient input-length capacity or provide truncated/structured inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9690.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answer-Only / CoT / 3-Shot Settings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Prompting Settings: Answer-Only (AO), Chain-of-Thought (CoT), and 3-shot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three prompting/evaluation regimes used for SciEval: AO (direct answer), CoT (encouraging chain-of-thought reasoning), and 3-shot (few-shot exemplars from dev set).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple LLMs (GPT-4, GPT-3.5-turbo, Claude-v1.3 and others; availability of CoT varies between models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Different LLMs show different responsiveness to CoT or few-shot prompting; some models lack robust CoT capability.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applies to all SciEval domains (biology, chemistry, physics)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare model performance across AO, CoT and 3-shot prompts to assess the impact of reasoning prompting and few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Change in accuracy (or other task-specific metrics) between AO, CoT and 3-shot settings; report increases/decreases or near-equal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to Static Data and Dynamic Data subsets to measure sensitivity to prompting strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CoT improved GPT-series performance on Static Data; about half of models improved under 3-shot. On Dynamic Data CoT and 3-shot substantially help chemistry for many LLMs but have limited effect on physics (most models remain near-random); GPT-3.5-turbo got a large CoT lift on physics (≈47.19% accuracy) while GPT-4 showed mixed behavior (poor under CoT for some physics items but best under 3-shot at ≈51.01%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not all models support CoT equally (limited CoT capability); API / availability restrictions prevented CoT/3-shot evaluation for some models (Claude variants unavailable for some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Prompting regimes are compared to human-like stepwise reasoning (CoT) vs. direct answers; no direct human baseline for prompting gains is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report results across AO/CoT/3-shot to capture prompting sensitivity; select few-shot exemplars from dev splits; be cautious interpreting CoT effects when models vary in CoT capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9690.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Metrics used in SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of automated and manual metrics used to evaluate LLM outputs: accuracy for objective tasks, BLEU and extract-match for string outputs, Mean Squared Error (MSE) for numeric chemistry answers, and manual grading for Experimental Data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple LLMs (evaluated across SciEval; metrics applied to all applicable models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Metrics are model-agnostic; chosen to match output type (categorical, string, numeric, open-ended).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied across biology, chemistry and physics subsets where appropriate</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated scoring for objective/dynamic items and manual scoring for experimental subjective items; special handling of missing numeric predictions (MSE set to large penalty if no number predicted).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for multiple-choice/judgement, BLEU for string similarity (chemical SMILES, names), MSE for numeric values (molecular weights etc.), extract match for exact string matches, manual rubric for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used across SciEval's Static, Dynamic and Experimental Data subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Accuracy revealed GPT-series lead on Static Data; BLEU/MSE revealed chemistry dynamic outputs are low-quality (low BLEU, very high MSE for many models); manual scoring showed competence on principles/design but failure on experimental analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>BLEU may inadequately capture chemical string correctness; MSE penalizes missing numeric outputs heavily (paper sets MSE to 1e10 when models output no number); manual evaluations are time-consuming and subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Manual scoring for Experimental Data approximates human evaluation but no standardized human baseline; metrics are analogous to common automated benchmarks rather than scientific peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Match metric to output type (use MSE for numbers, BLEU/extract-match for strings, accuracy for MCQs); define fallback penalties for missing output; complement automated metrics with manual evaluation for open-ended scientific reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9690.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 evaluated via API; undisclosed parameter count; instruction tuned and RLHF-trained variant of the GPT family that achieved state-of-the-art on SciEval among tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>OpenAI model (undisclosed params), API access, instruction tuned with RLHF as noted by the paper's reference to OpenAI releases.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluated across biology, chemistry and physics (multi-disciplinary SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated on Static Data (accuracy), Dynamic Data (accuracy/BLEU/MSE per subset), Experimental Data (manual scoring) under AO, CoT, and 3-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Static accuracy (main metric), BLEU and MSE for dynamic chemistry, accuracy for dynamic physics, manual scoring for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated across SciEval Static, Dynamic, and Experimental subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Ranked best overall on SciEval's Static Data (reported average ≈73.93%); best average on Dynamic Data by combined accuracy/BLEU but chemistry dynamic numeric performance remains weak (chemistry DD Acc ≈11.05, BLEU ≈23.78 reported); Experimental Data: strong on principles/design but poor at result analysis; CoT and 3-shot effects varied (CoT helped GPT-series on Static Data; GPT-4 performed best in physics dynamic under 3-shot with ≈51.01%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Despite broad knowledge, GPT-4 often applies incorrect formulas on physics dynamic items (CoT sometimes hurt physics answers); long-context limits and API constraints may shape evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No explicit human baselines provided; GPT-4 approximates high-performing model but still lags in calculation and experimental result analysis compared to expected scientific reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use CoT and few-shot prompting where beneficial, but validate that chain-of-thought yields correct formula application; combine automated and manual evaluation for complete assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9690.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI conversational model (API-access, undisclosed parameter count) evaluated as one of the stronger models on SciEval, showing pronounced gains under CoT in some physics dynamic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>OpenAI's GPT-3.5 series model (API), instruction tuned and RLHF-influenced as per references in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluated across biology, chemistry and physics on SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated on Static, Dynamic and Experimental subsets under AO/CoT/3-shot prompting; metrics per subset (accuracy, BLEU, MSE, manual).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for static/dynamic MCQs; BLEU/MSE for chemistry dynamic; manual for Experimental Data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated across SciEval dataset (Static, Dynamic, Experimental where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Achieved >60% average accuracy on Static Data (reported average ≈66.97%). Under CoT, GPT-3.5-turbo showed a strong lift on physics dynamic (CoT accuracy reported ≈47.19%), indicating good physics reasoning when CoT is available; performance still limited on chemistry dynamic numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CoT capability varies by model; chemistry numeric outputs still exhibit high MSE; lacks perfect formula application though CoT helps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No explicit human baseline; CoT performance suggests prompting can elicit stepwise reasoning comparable to human-style solution traces but not full human-level accuracy in calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include CoT evaluation as it can substantially improve reasoning for certain models and tasks (notably physics).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9690.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-v1.3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-v1.3 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude model (API-access); evaluated among the top-performing models on Static Data and Experimental Data in SciEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Claude-v1.3</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Anthropic's Claude series model, API-access, instruction-tuned; specific parameter count undisclosed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluated across biology, chemistry and physics (SciEval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>AO, CoT and 3-shot evaluation on Static and Dynamic Data; manual scoring on Experimental Data where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Static accuracy, BLEU/MSE for dynamic chemistry, accuracy for dynamic physics, manual grading for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on SciEval's Static, Dynamic and Experimental subsets (where accessible).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Claude-v1.3 achieved >60% average on Static Data (reported ≈63.45% average), performing comparably to GPT-3.5-turbo but below GPT-4; performed well on experimental principle/design but struggled on experimental-analysis tasks similar to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Availability of CoT/3-shot settings limited by API for some Claude variants; dynamic calculation weaknesses persist.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No formal human baseline; manual experiment scoring used to assess Research Ability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Evaluate Claude under multiple prompting regimes and supplement automated scores with manual inspection for experimental outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9690.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9690.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (30B) (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A science-focused model trained on a large scientific corpus; despite smaller overall data, it shows comparatively strong performance on calculation/counting tasks in SciEval Dynamic Data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Galactica-30B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Open-weight or published model family (30B parameters reported in paper) trained on extensive scientific texts; weight access available per authors' use.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Science-focused (chemistry and physics dynamic calculation tasks highlighted)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated on SciEval Static and Dynamic Data; metrics include accuracy, BLEU, MSE for dynamic chemistry, and accuracy for physics multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for MCQs and counting; BLEU/MSE for chemistry; manual evaluation for experiments when applicable (though some large models not tested on Experimental Data due to context limits).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated primarily on SciEval Static and Dynamic subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Galactica outperforms many similarly-sized models on scientific tasks and achieved the best results on counting and calculation items in Dynamic Data (better numerical/calculation aptitude than many non-science-specialized LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Galactica's advantage suggests domain-specific pretraining helps but it's not uniformly superior across all tasks; still not perfect on chemistry dynamic MSE or experimental analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No human baselines; authors infer training on scientific corpora improves performance relative to general models.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Training on large scientific corpora is beneficial for scientific calculation tasks; include domain-specialized models in benchmarks to probe domain advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring Massive Multitask Language Understanding <em>(Rating: 2)</em></li>
                <li>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models <em>(Rating: 2)</em></li>
                <li>C-EVAL: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models <em>(Rating: 1)</em></li>
                <li>MultiMedQA: Combining and Benchmarking Medical Question Answering Datasets <em>(Rating: 2)</em></li>
                <li>Chem-LLMBench <em>(Rating: 2)</em></li>
                <li>MATH: Measuring Mathematical Problem Solving with the MATH Dataset <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9690",
    "paper_id": "paper-261214653",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "SciEval",
            "name_full": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
            "brief_description": "A multi-disciplinary benchmark (≈18,000 questions) that evaluates LLM scientific ability across four dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) using static, dynamic and experimental subsets guided by Bloom's taxonomy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple LLMs (evaluated: GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica-30B, Vicuna-13B, etc.)",
            "llm_description": "N/A — SciEval is a benchmark (not an LLM); evaluated models include API-access models (GPT-4, GPT-3.5-turbo, Claude-v1.3) and open-weight models (Galactica, Vicuna, LLaMa variants).",
            "scientific_domain": "Multi-disciplinary: biology, chemistry, physics",
            "evaluation_method": "Automated evaluation on objective questions (accuracy) and dynamic generated items; BLEU and MSE for chemistry dynamic items; manual scoring for open-ended Experimental Data; experimental conditions include Answer-Only (AO), Chain-of-Thought (CoT) and 3-shot.",
            "evaluation_criteria": "Accuracy for objective questions; BLEU and extract-match for string outputs; Mean Squared Error (MSE) for numeric chemistry answers; manual qualitative scoring for Experimental Data.",
            "benchmark_or_dataset": "SciEval — static objective dataset (derived from community Q&A and public datasets), a dynamic subset generated from scientific principles to avoid data leakage, and an Experimental Data subset of 12 basic experiments (subjective/open-ended).",
            "results_summary": "Authors report that GPT-4 is the strongest model on SciEval (Static Data average ≈73.9%), with only GPT-4, GPT-3.5-turbo and Claude-v1.3 exceeding 60% average accuracy on Static Data. Dynamic Data and Experimental Data expose major weaknesses (low chemistry dynamic accuracy and nearly random physics dynamic performance; experimental analysis is generally poor).",
            "limitations_or_challenges": "Risk of data leakage from pretraining addressed via dynamic data generation; subjective Experimental Data requires manual scoring; dynamic items require special metrics (BLEU/MSE); benchmark highlights LLM calculation and experimental-result-analysis weaknesses.",
            "comparison_to_human_or_traditional": "No direct head-to-head comparison to human expert performance reported; Experimental Data responses are manually evaluated (human-in-the-loop) but no formal human baseline is provided.",
            "recommendations_or_best_practices": "Use multi-level evaluation based on Bloom's taxonomy, combine objective and subjective tasks, include a dynamic subset to mitigate data leakage, evaluate under AO/CoT/3-shot conditions, and use mixed automated (accuracy, BLEU, MSE) and manual evaluation for open-ended experimental outputs.",
            "uuid": "e9690.0",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Static Data",
            "name_full": "SciEval Static Data (objective subset)",
            "brief_description": "A fixed set of objective questions (multiple-choice, fill-in-the-blank, judgement) drawn from Socratic Q&A and integrated public datasets to measure recall and application across BK/KA/SC/RA dimensions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple LLMs (evaluated: GPT-4, GPT-3.5-turbo, Claude-v1.3, Galactica-30B, and others)",
            "llm_description": "Models evaluated via API or weights; some models only web/API accessible per Table 3 in the paper.",
            "scientific_domain": "Biology, Chemistry, Physics (multi-topic coverage)",
            "evaluation_method": "Automated accuracy on objective items; split into dev/valid/test; small dev set used for few-shot exemplars.",
            "evaluation_criteria": "Accuracy (A/B/C/D choice correctness) is the main metric for Static Data.",
            "benchmark_or_dataset": "Static Data — curated objective questions (~15,901 overall in SciEval; Static portion drawn from Socratic Q&A, MedQA, PubMedQA, Reagent Selection subsets).",
            "results_summary": "On Static Data only GPT-4, GPT-3.5-turbo and Claude-v1.3 exceed 60% average accuracy; GPT-4 achieves the highest static average (reported ≈73.93%). Many smaller models perform substantially worse (some ≈30–60% or lower depending on size).",
            "limitations_or_challenges": "Static items risk being present in LLM pretraining corpora; hence the paper includes dynamic data to reduce leakage; objective-only evaluation misses open-ended research skills.",
            "comparison_to_human_or_traditional": "Static Data uses standard objective-scoring analogous to exam-style evaluation; no explicit human baseline reported in the paper.",
            "recommendations_or_best_practices": "Use objective questions for quick, reproducible scoring but supplement with dynamic and subjective tasks to fairly evaluate research-level capabilities.",
            "uuid": "e9690.1",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Dynamic Data",
            "name_full": "SciEval Dynamic Data (generated-by-principles subset)",
            "brief_description": "A regularly updated subset that generates chemistry and physics items algorithmically from scientific principles to avoid data leakage from model pretraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple LLMs (evaluated: GPT-4, GPT-3.5-turbo, Galactica variants, Vicuna, LLaMa, etc.)",
            "llm_description": "Evaluated models vary in access mode and size; dynamic subset intended to be unseen training data for all models.",
            "scientific_domain": "Chemistry (KA/calculation) and Physics (SC/calculation)",
            "evaluation_method": "For chemistry dynamic items: string/numeric answers evaluated with BLEU (string), MSE (numeric) and extract-match; for physics dynamic items (multiple-choice) evaluated with accuracy. Dataset is regenerated periodically and a stable version is maintained for fairness.",
            "evaluation_criteria": "Accuracy for physics multiple-choice; BLEU and extract-match for string responses; MSE for numerical chemistry answers (if no numeric output, MSE treated as large penalty).",
            "benchmark_or_dataset": "Dynamic Data — chemistry subset (≈2000 items; KA) and physics subset (≈890 items; SC) generated by scripts or using molecule properties and physics formulas.",
            "results_summary": "Overall poor performance: GPT-4 attains the best average accuracy and BLEU among tested models but chemistry dynamic accuracy values are low (e.g., GPT-4 chemistry (DD) Acc reported ≈11.05 with BLEU ≈23.78) and many models perform near random on physics dynamic items (≈25% baseline). Galactica yields comparatively strong results on counting and calculation questions.",
            "limitations_or_challenges": "Chemistry dynamic MSE extremely high for most models; physics dynamic often near-random indicating weak calculation/formula application; dynamic generation requires careful metric selection and regular updates.",
            "comparison_to_human_or_traditional": "Dynamic items are designed to be novel and thus more akin to testing reasoning than recall; no explicit human performance baselines reported.",
            "recommendations_or_best_practices": "Use dynamic generation to mitigate pretraining leakage; evaluate numeric outputs with MSE and string outputs with BLEU/extract-match; maintain both regenerated and stable versions for fair longitudinal comparison.",
            "uuid": "e9690.2",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Experimental Data",
            "name_full": "SciEval Experimental Data (12 basic experiments)",
            "brief_description": "A subjective subset composed of 12 basic science experiments (collected from university experiment courses) to probe Research Ability: experimental principle, design, analysis and summarization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple LLMs evaluated (GPT-4, GPT-3.5-turbo, Claude-v1.3; ERNIE Bot and SparkDesk evaluated via web where feasible)",
            "llm_description": "Experimental Data evaluated primarily for models with sufficient context window or web/API access (some large open-weight models excluded due to length limits).",
            "scientific_domain": "Basic experimental science across biology, chemistry, physics",
            "evaluation_method": "Manual human evaluation of open-ended answers for experimental principle, design, and analysis; some models evaluated via web interfaces when API/weight access lacking.",
            "evaluation_criteria": "Manual qualitative scoring across criteria: principle understanding, experimental design quality, and experimental result analysis/summarization.",
            "benchmark_or_dataset": "Experimental Data — twelve full experiments with open-ended questions requiring written responses; constructed to evaluate higher-order research skills (RA).",
            "results_summary": "Top-tier models (GPT-series, Claude-series) perform acceptably on experimental principles and design, but almost all models struggle with experimental-result analysis and summarization; detailed experiment-level scores are provided in appendices.",
            "limitations_or_challenges": "Subjectivity requires manual scoring; long prompts exceed context limits for many models; comparisons are less automatable and reproducibility depends on human raters.",
            "comparison_to_human_or_traditional": "Evaluation of Experimental Data uses human raters (manual assessment) similar to human peer review of experimental descriptions, but no formal human baseline is presented.",
            "recommendations_or_best_practices": "Include open-ended experimental tasks when evaluating research abilities; use manual scoring rubrics; ensure models evaluated have sufficient input-length capacity or provide truncated/structured inputs.",
            "uuid": "e9690.3",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Answer-Only / CoT / 3-Shot Settings",
            "name_full": "Evaluation Prompting Settings: Answer-Only (AO), Chain-of-Thought (CoT), and 3-shot",
            "brief_description": "Three prompting/evaluation regimes used for SciEval: AO (direct answer), CoT (encouraging chain-of-thought reasoning), and 3-shot (few-shot exemplars from dev set).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple LLMs (GPT-4, GPT-3.5-turbo, Claude-v1.3 and others; availability of CoT varies between models)",
            "llm_description": "Different LLMs show different responsiveness to CoT or few-shot prompting; some models lack robust CoT capability.",
            "scientific_domain": "Applies to all SciEval domains (biology, chemistry, physics)",
            "evaluation_method": "Compare model performance across AO, CoT and 3-shot prompts to assess the impact of reasoning prompting and few-shot examples.",
            "evaluation_criteria": "Change in accuracy (or other task-specific metrics) between AO, CoT and 3-shot settings; report increases/decreases or near-equal performance.",
            "benchmark_or_dataset": "Applied to Static Data and Dynamic Data subsets to measure sensitivity to prompting strategy.",
            "results_summary": "CoT improved GPT-series performance on Static Data; about half of models improved under 3-shot. On Dynamic Data CoT and 3-shot substantially help chemistry for many LLMs but have limited effect on physics (most models remain near-random); GPT-3.5-turbo got a large CoT lift on physics (≈47.19% accuracy) while GPT-4 showed mixed behavior (poor under CoT for some physics items but best under 3-shot at ≈51.01%).",
            "limitations_or_challenges": "Not all models support CoT equally (limited CoT capability); API / availability restrictions prevented CoT/3-shot evaluation for some models (Claude variants unavailable for some settings).",
            "comparison_to_human_or_traditional": "Prompting regimes are compared to human-like stepwise reasoning (CoT) vs. direct answers; no direct human baseline for prompting gains is provided.",
            "recommendations_or_best_practices": "Report results across AO/CoT/3-shot to capture prompting sensitivity; select few-shot exemplars from dev splits; be cautious interpreting CoT effects when models vary in CoT capability.",
            "uuid": "e9690.4",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Evaluation Metrics",
            "name_full": "Evaluation Metrics used in SciEval",
            "brief_description": "Set of automated and manual metrics used to evaluate LLM outputs: accuracy for objective tasks, BLEU and extract-match for string outputs, Mean Squared Error (MSE) for numeric chemistry answers, and manual grading for Experimental Data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple LLMs (evaluated across SciEval; metrics applied to all applicable models)",
            "llm_description": "Metrics are model-agnostic; chosen to match output type (categorical, string, numeric, open-ended).",
            "scientific_domain": "Applied across biology, chemistry and physics subsets where appropriate",
            "evaluation_method": "Automated scoring for objective/dynamic items and manual scoring for experimental subjective items; special handling of missing numeric predictions (MSE set to large penalty if no number predicted).",
            "evaluation_criteria": "Accuracy for multiple-choice/judgement, BLEU for string similarity (chemical SMILES, names), MSE for numeric values (molecular weights etc.), extract match for exact string matches, manual rubric for experiments.",
            "benchmark_or_dataset": "Used across SciEval's Static, Dynamic and Experimental Data subsets.",
            "results_summary": "Accuracy revealed GPT-series lead on Static Data; BLEU/MSE revealed chemistry dynamic outputs are low-quality (low BLEU, very high MSE for many models); manual scoring showed competence on principles/design but failure on experimental analysis.",
            "limitations_or_challenges": "BLEU may inadequately capture chemical string correctness; MSE penalizes missing numeric outputs heavily (paper sets MSE to 1e10 when models output no number); manual evaluations are time-consuming and subjective.",
            "comparison_to_human_or_traditional": "Manual scoring for Experimental Data approximates human evaluation but no standardized human baseline; metrics are analogous to common automated benchmarks rather than scientific peer review.",
            "recommendations_or_best_practices": "Match metric to output type (use MSE for numbers, BLEU/extract-match for strings, accuracy for MCQs); define fallback penalties for missing output; complement automated metrics with manual evaluation for open-ended scientific reasoning.",
            "uuid": "e9690.5",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "OpenAI's GPT-4 evaluated via API; undisclosed parameter count; instruction tuned and RLHF-trained variant of the GPT family that achieved state-of-the-art on SciEval among tested models.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "GPT-4",
            "llm_description": "OpenAI model (undisclosed params), API access, instruction tuned with RLHF as noted by the paper's reference to OpenAI releases.",
            "scientific_domain": "Evaluated across biology, chemistry and physics (multi-disciplinary SciEval)",
            "evaluation_method": "Evaluated on Static Data (accuracy), Dynamic Data (accuracy/BLEU/MSE per subset), Experimental Data (manual scoring) under AO, CoT, and 3-shot prompting.",
            "evaluation_criteria": "Static accuracy (main metric), BLEU and MSE for dynamic chemistry, accuracy for dynamic physics, manual scoring for experiments.",
            "benchmark_or_dataset": "Evaluated across SciEval Static, Dynamic, and Experimental subsets.",
            "results_summary": "Ranked best overall on SciEval's Static Data (reported average ≈73.93%); best average on Dynamic Data by combined accuracy/BLEU but chemistry dynamic numeric performance remains weak (chemistry DD Acc ≈11.05, BLEU ≈23.78 reported); Experimental Data: strong on principles/design but poor at result analysis; CoT and 3-shot effects varied (CoT helped GPT-series on Static Data; GPT-4 performed best in physics dynamic under 3-shot with ≈51.01%).",
            "limitations_or_challenges": "Despite broad knowledge, GPT-4 often applies incorrect formulas on physics dynamic items (CoT sometimes hurt physics answers); long-context limits and API constraints may shape evaluation.",
            "comparison_to_human_or_traditional": "No explicit human baselines provided; GPT-4 approximates high-performing model but still lags in calculation and experimental result analysis compared to expected scientific reasoning.",
            "recommendations_or_best_practices": "Use CoT and few-shot prompting where beneficial, but validate that chain-of-thought yields correct formula application; combine automated and manual evaluation for complete assessment.",
            "uuid": "e9690.6",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "OpenAI conversational model (API-access, undisclosed parameter count) evaluated as one of the stronger models on SciEval, showing pronounced gains under CoT in some physics dynamic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "GPT-3.5-turbo",
            "llm_description": "OpenAI's GPT-3.5 series model (API), instruction tuned and RLHF-influenced as per references in the paper.",
            "scientific_domain": "Evaluated across biology, chemistry and physics on SciEval",
            "evaluation_method": "Evaluated on Static, Dynamic and Experimental subsets under AO/CoT/3-shot prompting; metrics per subset (accuracy, BLEU, MSE, manual).",
            "evaluation_criteria": "Accuracy for static/dynamic MCQs; BLEU/MSE for chemistry dynamic; manual for Experimental Data.",
            "benchmark_or_dataset": "Evaluated across SciEval dataset (Static, Dynamic, Experimental where applicable).",
            "results_summary": "Achieved &gt;60% average accuracy on Static Data (reported average ≈66.97%). Under CoT, GPT-3.5-turbo showed a strong lift on physics dynamic (CoT accuracy reported ≈47.19%), indicating good physics reasoning when CoT is available; performance still limited on chemistry dynamic numeric tasks.",
            "limitations_or_challenges": "CoT capability varies by model; chemistry numeric outputs still exhibit high MSE; lacks perfect formula application though CoT helps.",
            "comparison_to_human_or_traditional": "No explicit human baseline; CoT performance suggests prompting can elicit stepwise reasoning comparable to human-style solution traces but not full human-level accuracy in calculations.",
            "recommendations_or_best_practices": "Include CoT evaluation as it can substantially improve reasoning for certain models and tasks (notably physics).",
            "uuid": "e9690.7",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Claude-v1.3",
            "name_full": "Claude-v1.3 (Anthropic)",
            "brief_description": "Anthropic's Claude model (API-access); evaluated among the top-performing models on Static Data and Experimental Data in SciEval.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Claude-v1.3",
            "llm_description": "Anthropic's Claude series model, API-access, instruction-tuned; specific parameter count undisclosed in the paper.",
            "scientific_domain": "Evaluated across biology, chemistry and physics (SciEval)",
            "evaluation_method": "AO, CoT and 3-shot evaluation on Static and Dynamic Data; manual scoring on Experimental Data where applicable.",
            "evaluation_criteria": "Static accuracy, BLEU/MSE for dynamic chemistry, accuracy for dynamic physics, manual grading for experiments.",
            "benchmark_or_dataset": "Evaluated on SciEval's Static, Dynamic and Experimental subsets (where accessible).",
            "results_summary": "Claude-v1.3 achieved &gt;60% average on Static Data (reported ≈63.45% average), performing comparably to GPT-3.5-turbo but below GPT-4; performed well on experimental principle/design but struggled on experimental-analysis tasks similar to other models.",
            "limitations_or_challenges": "Availability of CoT/3-shot settings limited by API for some Claude variants; dynamic calculation weaknesses persist.",
            "comparison_to_human_or_traditional": "No formal human baseline; manual experiment scoring used to assess Research Ability.",
            "recommendations_or_best_practices": "Evaluate Claude under multiple prompting regimes and supplement automated scores with manual inspection for experimental outputs.",
            "uuid": "e9690.8",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Galactica-30B",
            "name_full": "Galactica (30B) (Meta)",
            "brief_description": "A science-focused model trained on a large scientific corpus; despite smaller overall data, it shows comparatively strong performance on calculation/counting tasks in SciEval Dynamic Data.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Galactica-30B",
            "llm_description": "Open-weight or published model family (30B parameters reported in paper) trained on extensive scientific texts; weight access available per authors' use.",
            "scientific_domain": "Science-focused (chemistry and physics dynamic calculation tasks highlighted)",
            "evaluation_method": "Evaluated on SciEval Static and Dynamic Data; metrics include accuracy, BLEU, MSE for dynamic chemistry, and accuracy for physics multiple-choice.",
            "evaluation_criteria": "Accuracy for MCQs and counting; BLEU/MSE for chemistry; manual evaluation for experiments when applicable (though some large models not tested on Experimental Data due to context limits).",
            "benchmark_or_dataset": "Evaluated primarily on SciEval Static and Dynamic subsets.",
            "results_summary": "Galactica outperforms many similarly-sized models on scientific tasks and achieved the best results on counting and calculation items in Dynamic Data (better numerical/calculation aptitude than many non-science-specialized LLMs).",
            "limitations_or_challenges": "Galactica's advantage suggests domain-specific pretraining helps but it's not uniformly superior across all tasks; still not perfect on chemistry dynamic MSE or experimental analysis.",
            "comparison_to_human_or_traditional": "No human baselines; authors infer training on scientific corpora improves performance relative to general models.",
            "recommendations_or_best_practices": "Training on large scientific corpora is beneficial for scientific calculation tasks; include domain-specialized models in benchmarks to probe domain advantages.",
            "uuid": "e9690.9",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring Massive Multitask Language Understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
            "rating": 2,
            "sanitized_title": "agieval_a_humancentric_benchmark_for_evaluating_foundation_models"
        },
        {
            "paper_title": "C-EVAL: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
            "rating": 1,
            "sanitized_title": "ceval_a_multilevel_multidiscipline_chinese_evaluation_suite_for_foundation_models"
        },
        {
            "paper_title": "MultiMedQA: Combining and Benchmarking Medical Question Answering Datasets",
            "rating": 2,
            "sanitized_title": "multimedqa_combining_and_benchmarking_medical_question_answering_datasets"
        },
        {
            "paper_title": "Chem-LLMBench",
            "rating": 2,
            "sanitized_title": "chemllmbench"
        },
        {
            "paper_title": "MATH: Measuring Mathematical Problem Solving with the MATH Dataset",
            "rating": 1,
            "sanitized_title": "math_measuring_mathematical_problem_solving_with_the_math_dataset"
        }
    ],
    "cost": 0.0164975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</p>
<p>Liangtai Sun 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Yang Han csyanghan@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zihan Zhao 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Da Ma 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zhennan Shen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Baocai Chen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Kai Yu kai.yu@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research
8DE5F8747D39882235665D42A59CA88C
Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research.Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research.However, current benchmarks are mostly based on pre-collected objective questions.This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability.In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues.Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability.In particular, we design a "dynamic" subset based on scientific principles to prevent evaluation from potential data leakage.Both objective and subjective questions are included in SciEval.These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs.Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions.The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as ChatGPT (Schulman et al. 2022), have attracted widespread attention in general scenarios, including information search, code generation, and more.In the field of science, LLMs have also shown preliminary potential in improving scientific research efficiency and transforming scientific research paradigms (Blanco-Gonzalez et al. 2023;WANG and MIAO 2023).In the meanwhile, several scientific LLMs have been proposed by researchers (Taylor et al. 2022;Luo et al. 2022;Frey et al. 2022).In the general field, there are already numerous evaluation benchmarks to evaluate the language understanding, language generation and reasoning capabilities of LLMs, such as MMLU (Hendrycks et al. 2020), AGIEval (Zhong et al. 2023), and C-EVAL (Huang et al. 2023), shown in Table 1.Although these benchmarks cover data of science domain, the data sources are usually confined to educational materials, which can not adequately as-sess the research ability of LLMs and not align with real-life scientific research scenarios.In addition, some benchmarks have been proposed to evaluate the scientific capability of LLMs, such as MultiMedQA (Singhal et al. 2023), Chem-LLMBench (Guo et al. 2023), and MATH (Hendrycks et al. 2021), while these benchmarks are restricted to a specific scientific discipline, leaving a lack of a more general scientific evaluation benchmark. 1In addition, these benchmarks (1) lack evaluation systems for scientific capabilities, (2) are all based on objective questions, which are insufficient to assess scientific abilities, and (3) face the risk of data leakage.</p>
<p>In response to this gap, we present SciEval, an English benchmark designed to evaluate advanced abilities of LLMs in the scientific domain.SciEval consists of a total of about 18000 challenging scientific questions, spanning three important basic science fields: chemistry, physics and biology, each of which is further divided into multiple sub-topics.SciEval mainly has the following three characteristics:</p>
<p>• Multi-level and comprehensive evaluation of the ability of LLMs in the scientific field.Scientific ability of LLMs needs to be evaluated from multiple aspects.Leveraging cognitive domains of Bloom's taxonomy (Krathwohl 2002;Forehand 2010), which covers six levels, SciEval evaluates the scientific capabilities of large language models across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability, where each capability aligns with one or more cognitive levels.• Combination of objective and subjective questions.</p>
<p>SciEval is mainly based on objective questions, which allow for quick and standard model evaluations, involving multiple-choice, fill-in-the-blank, and judgment questions.These questions can help us understand whether the model can correctly understand and memorize scientific knowledge.However, objective questions are insufficient to assess scientific capability holistically.To better assess scientific reasoning and application ability, SciEval introduces a small number of subjective questions, involving a total of twelve basic science experiments, which is named Experimental Data.We conduct experiments to evaluate LLMs on SciEval in answer-only, chain-of-thought and few-shot settings.Results indicate that GPT-4 is the strongest model, with only GPT-4, GPT-3.5-turbo and Claude-v1.3surpassing 60% average accuracy on Static Data, signifying considerable opportunities for improvement.With the results of Dynamic Data, we find that these LLMs have little knowledge about molecules, and most models could only retain near-random accuracy in the physics subset.As for Experimental Data, some top-tier models could perform satisfactorily in experimental principle and design, while almost all models struggle to analyze the experimental results.With the analysis of experiment results, we claim that training on large-scale scientific corpus is helpful for the scientific ability of LLMs, and most LLMs perform bad on calculation problems, especially in physics domain.We hope SciEval can provide an excellent benchmark for the assessment of scientific capability of LLMs, and promote wide application in science.</p>
<p>Related Work</p>
<p>General Benchmarks for LLMs</p>
<p>To evaluate the performance of LLMs across different tasks, several benchmarks have been proposed.MMLU (Hendrycks et al. 2020) aims to develop a comprehensive test for evaluating text models in multi-task contexts.HELM (Liang et al. 2022) offers a comprehensive assessment, evaluating LLMs across various aspects, such as language understanding and common-sense reasoning.Big-Bench (Srivastava et al. 2022) introduces 204 challenging tasks covering various domains, aiming to evaluate tasks beyond the capabilities of existing language models.AGIEval (Zhong et al. 2023) serves as an evaluation framework for assessing the performance of foundation models in human-centric standardized exams.C-Eval (Huang et al. 2023) assesses the advanced knowledge and reasoning capabilities of foundation models in Chinese.</p>
<p>Specific Benchmarks for LLMs</p>
<p>Apart from general tasks, specific benchmarks are designed for certain downstream tasks.MultiMedQA (Singhal et al. 2023) focuses on medical question-answering, evaluating LLMs in terms of clinical knowledge and QA abilities.MATH (Hendrycks et al. 2021) assesses reasoning and problem-solving proficiencies of LLMs in mathematics.Sci-enceQA (Lu et al. 2022) proposes a multi-modal benchmark with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations, collected from elementary and high school science curricula.SCIBENCH (Wang et al. 2023) examines the reasoning capabilities required for complex scientific problem-solving and proposes two datasets of college-level scientific problems.Compared to these benchmarks, SciEval (1) evaluates scientific capabilities from multiple aspects, having a broader coverage, (2) uses data of community Q&amp;A, which is more flexible and diverse, (3) designs a subset of dynamic data, making an effort to mitigate data leakage.</p>
<p>3 The SciEval Dataset cognitive domain is frequently used to structure curriculum learning objectives, assessments and activities, and is broken into six levels: Remember, Understand, Apply, Analyze, Evaluate and Create, as is shown in Figure 1, which are suitable for the evaluation of scientific capability.</p>
<p>Based on the cognitive domain of Bloom's taxonomy, the evaluation system of SciEval consists of four knowledge dimensions: Basic Knowledge (BK), Knowledge Application (KA), Scientific Calculation (SC), and Research Ability (RA).As is shown in Figure 1, BK primarily assesses the fundamental scientific knowledge of LLMs.KA focuses on how to apply basic knowledge to solve scientific problems, requiring models to have comprehension, application, and analysis abilities.SC is a specialized application of knowledge that further examines complex reasoning capabilities of LLMs based on their general knowledge application abilities.RA assesses evaluation capabilities at a higher cognitive level, requiring models to participate in various aspects of scientific research, including problem formulation, experimental design, data analysis, and summarization.</p>
<p>Based on the evaluation system, we design three different types of data: Static Data, Dynamic Data, and Experimental Data.The Static Data covers all these four knowledge dimensions and will remain constant throughout, while the Dynamic Data examines from the aspects of Knowledge Application and Scientific Calculation and will be regularly updated to prevent any data leakage.The Experimental Data comprises a set of questions for twelve scientific experiments and can be used to evaluate the Research Ability.</p>
<p>Data Collection</p>
<p>Static Data The collection steps of Static Data are shown in Figure 2. The primary source of Static Data is Socratic Q&amp;A 2 , a community-driven website that covers a wide range of subjects such as science and literature.Specifically, we collect data from the fields of biology, chemistry, and physics.To ensure quality, we employ rule-based methods 2 https://socratic.org to preprocess the crawled data.While gathering the questions, we found that not all of them are suitable as titles.To address this, we utilize GPT-4 with the "Task 1" prompt, as depicted in Figure 2, to process these questions.Since most of the collected questions are open-ended and challenging to evaluate, we employ GPT-4 to simplify ground-truth answers and generate three wrong answers to formulate them as multiple-choice questions.Additionally, we classify the questions into their respective knowledge domains.And during this process, we manually check the generated content of GPT-4 to ensure data quality.</p>
<p>To make the dataset more diverse and comprehensive, we further integrate data from some publicly available datasets:</p>
<p>• MedQA (Jin et al. 2021) is a free-form multiple-choice OpenQA dataset for solving medical problems, collected from professional medical board exams.We use the test set of USMLE, which is the English subset of MedQA.</p>
<p>• PubMedQA (Jin et al. 2019) is a biomedical questionanswering dataset collected from PubMed abstracts.The task of PubMedQA is to answer research questions with yes/no/maybe using the corresponding abstracts, which is fit for evaluating the literature comprehension ability.We incorporate 1000 expert-annotated data from it and frame them as judgment questions.</p>
<p>• Reagent Selection (Guo et al. 2023) involves the identification and proposal of the most fitting reagents for a specific chemical reaction or process, which is a subset of ChemLLMBench.We randomly select 40% data and formulate them as multiple-choice questions.</p>
<p>Dynamic Data</p>
<p>The current training of LLMs often uses a large amount of data, resulting in a risk of data leakage for evaluation.In order to solve this problem, we design a "dynamic" subset, which can generate data dynamically according to scientific principles.The dynamic subset covers two disciplines, chemistry and physics.For chemistry data, we use the basic information and properties of molecules</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)</p>
<p>Socratic Q&amp;A</p>
<p>Crawl &amp; Filter</p>
<p>Raw Data</p>
<p>GPT-4 Filtered Data</p>
<p>Instruction: Given a question and its ground-truth answer, judge whether it is suitable to be used as the title of a multiple-choice question.Your answer should be "YES" or "NO".And please directly give the results without any explanation.</p>
<p>Task 1</p>
<p>Instruction: Given a question and a ground-truth answer, please simplify the answer as concise as possible.And I want to generate a 4-choice question using it, please generate 3 fake answers for me.Note that the length of the simplified answer and these 3 fake answers should be about the same and these 3 fake answers should be as confusing as possible.Furthermore, please help me to classify the domain of the question.There are three domains in total: Base Knowledge, Scientific Calculation, Knowledge Application.For physics data, we manually write some Python scripts according to the physics formulas.When obtaining the evaluation dataset, we will provide a regenerated version to users and we will update it regularly, while at the same time, we will maintain a stable version of the dynamic data to make a fair comparison.</p>
<p>Experimental Data To better evaluate the scientific thoughts and abilities of LLMs, SciEval introduces a subset of experimental data, involving 12 different basic scientific experiments.These experiments are collected from basic science experiment courses at university, and each experiment conducts a comprehensive investigation of the ability of LLMs in scientific research and experimentation from the perspectives of experimental principle, process, and analysis and summarization of experimental results.</p>
<p>Data Statistics</p>
<p>Summarized statistics are shown in For Static Data, we further split the data into dev, valid, and test set.For each data source, each knowledge domain, and each discipline, we randomly select 5 data to form the 3 https://pubchem.ncbi.nlm.nih.gov/dev set, which can be used for few-shot learning, and we split the remaining data with a ratio of 1:9 to construct the valid set and test set respectively.</p>
<p>Experiment</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".Please directly give the answer without any explanation.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".How many atoms are in 3.5 moles of arsenic atoms?</p>
<p>Experiment Setup</p>
<p>Prompts We evaluate LLMs in both Answer-Only (AO) and Chain-Of-Thought (CoT) (Kojima et al. 2022) settings.The prompts we used are shown in Figures 3 and 4 respectively.Furthermore, we also evaluate using 3-shot setting, where the three exemplars are selected from the dev set.Models In order to comprehensively assess the scientific capabilities of Large Language Models (LLMs), we evaluate 15 high-performing LLMs that are widely accessible.These models are selected to represent a diverse range of organizations and vary in size.The details of these models are summarized in Table 3.</p>
<p>Model</p>
<p>• GPT-3.5-turbo and GPT-4 (Schulman et al. 2022;Ope-nAI 2023) are the strongest GPT model variants from OpenAI that have undergone pretraining, instruction tuning, and reinforcement learning from human feedback (RLHF, (Ouyang et al. 2022)).• Claude 4 , developed by Anthropic, is often considered 4 https://www.anthropic.com/index/introducing-claude.comparable to GPT-3.5-turbo.We evaluate both the Claude-v1.We evaluate GPT-3.5-turbo,GPT4 and Claude on all three subsets, including Static Data, Dynamic Data, and Experimental Data.Since we can only assess ERNIE Bot and SparkDesk through web interface, we evaluate these two models on the Experimental Data.And for the rest LLMs with billions or tens of billions of parameters, since the length of the Experimental Data exceeds the length limit of these models7 , we evaluate them on Static Data and Dynamic Data, as is shown in Table 3.</p>
<p>Evaluation Metrics In the case of Static Data, all questions are objective, making accuracy the appropriate evaluation metric.For Dynamic Data, the physics questions are presented as multiple-choice questions, which can also be evaluated using accuracy.Conversely, the chemistry questions involve complex components, such as "What is the</p>
<p>Experiment Results</p>
<p>Answer-Only Setting Answer-only results of all the models on the test set are shown in Table 4 and detailed results of Static Data across different knowledge domains are provided in Appendix B. Analyzing the results of Static Data, GPT-4 demonstrates significantly superior performance compared to other models.And only GPT-4, GPT-3.5-turbo, and Claude-v1.3achieve an average accuracy exceeding 60%, which highlights the challenge posed by SciEval.</p>
<p>For the results of Dynamic Data, GPT-4 performs the best in terms of average accuracy and BLEU score.However, for counting and calculation questions, Galactica-30B yields the best results, indicating its strong aptitude in the field of science.Conversely, models with billions or tens of billions of parameters perform poorly on the chemistry subset, suggesting their limited knowledge about molecules.Regarding the performance of models on the physics subset, since all questions are 4-choices questions, the accuracy should be at least 25%.However, none of these models achieve satisfactory results in this subset.</p>
<p>As for Experimental Data, GPT-series models and Claude-series models achieve good results, while the other two models are not.The detailed scores models reached in each experiment are shown in Appendix C.However, although some models could get a great performance, during experiments, we find that these models are good at experimental principles and designing, while when it comes to analyzing the experiment results, the performances are not satisfying.</p>
<p>CoT Setting and 3-Shot setting Comparison of experiment results among Answer-Only, Chain-of-Thought and 3-Shot settings are shown in Figure 5 and Table 5. 9 And we refer detailed results to Appendix A and B.</p>
<p>The experimental results from Static Data reveal that solely the GPT-series LLMs get performance enhancement within the CoT setting due to the limited CoT capabilities of other LLMs.As for the 3-Shot setting, roughly half of the LLMs analyzed demonstrate superior performances relative to the Answer-Only setting.The performances of the remaining LLMs are closely similar to those observed within the Answer-Only setting.</p>
<p>From the experimental results of Dynamic Data, it is observed that both CoT and 3-Shot significantly enhance the performance of most Language Learning Models (LLMs) in the chemistry subset.However, the performances achieved are still not up to the mark.In the physics subset, the impact of CoT and 3-Shot on most LLMs is less pronounced, resulting in nearly random performances.Under the CoT setting, GPT-3.5-turboachieves an accuracy of 47.19, suggesting a robust understanding of physical principles.Conversely, the performance of GPT-4 is markedly poor, from which we find that despite its extensive knowledge of physical principles, it frequently employs incorrect formulas to solve problems.Nevertheless, GPT-4 attains an accuracy of 51.01 under 3-Shot setting, the highest among all models, demonstrating its ability to learn from a mere three examples.</p>
<p>Discussion</p>
<p>Training on large-scale scientific corpus is helpful.Based on experimental results (Table 4), Galactica (Taylor et al. 2022), which has been trained on an extensive scientific corpus, significantly outperforms other LLMs with a comparable number of parameters, although Galactica is trained with a much smaller amount of data.Remarkably, when tested on Dynamic Data, Galactica surpasses the GPTseries and Claude-series LLMs in computational problems.</p>
<p>Most LLMs perform bad on calculation problems, especially in physics domain.Detailed results across various knowledge domains on Static Data (refer to Appendix B) reveal that most LLMs underperform in the Scientific Calculation domain, while demonstrate relatively superior performance in other domains, which is particularly acute in the field of physics.Similar issues are also observed in Dynamic Data and Experimental Data.In the context of Dynamic Data, the mean square error, employed to evaluate calculation abilities within the chemistry subset, is exceedingly high for most LLMs, and almost all LLMs can only achieve nearly random performance within the physics subset.Regarding Experimental Data, our findings indicate that these LLMs struggle with the analysis of experimental results.</p>
<p>Conclusion</p>
<p>In this paper, we introduce SciEval, a benchmark designed to evaluate scientific capabilities of LLMs.SciEval comprises about 18,000 challenging scientific questions, covering three fundamental fields of science.SciEval assesses the scientific ability of LLMs across four dimensions.It incorporates both objective and subjective questions, and employs dynamic data generation to mitigate potential data leakage.We conduct comprehensive experiments on various advanced LLMs using SciEval and perform thorough analyses.Our experimental results reveal that most LLMs do not perform well on our benchmark, with the exception of the GPT-series and Claude-series LLMs.We hope that SciEval can serve as a robust benchmark for assessing scientific capabilities of LLMs.</p>
<p>Figure 1 :
1
Figure 1: The illustration of the evaluation system.SciEval covers three disciplines with amounts of sub-topics, and investigates four abilities, corresponding to six cognitive levels.</p>
<p>Figure 2 :
2
Figure 2: Data Collection steps of Static Data</p>
<p>Figure 3 :
3
Figure 3: An example of the prompt we used for AO setting.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>AFigure 4 :
4
Figure 4: An example of the prompt we used for CoT setting.The red text is the response from the model, while the blue text and black text are the inputted prompt.</p>
<p>Figure 5: Accuracy on Answer Only, Chain-of-Thought and 3-Shot settings of each LLMs for Static Data.</p>
<p>Table 1 :
1
Dataset comparison of SciEval and some other datasets covering science domain."BK"stands for Basic Knowledge, "KA" stands for Knowledge Application, "SC" stands for Scientific Calculation, and "RA" stands for Research Ability.
NameCategoryAbilitySourceData Type Dynamic #DataMMLUhumanities, social science, STEM, otherBK, KA, SCexam, book, course objective14079AGIEvalsocial science, STEM BK, KA, SCexamobjective8062C-EVALhumanities, social science, STEM, otherBK, KA, SCexamobjective12342MultiMedQAmedicalBK, KA, RAexam, researchobjective13115ChemLLMBench chemistryBK,KAknowledge baseobjective800MATHmathematicsSCexamobjective5000SciEvalscienceBK, KA,SC, RAcommunity QA, knowledge baseobjective + subjective15901model perfor-mance. And the objective questions other than DynamicData are referred to as Static Data.
• Dynamic data generation based on basic scientific principles.The huge amount of training data used for pre-training LLMs may cause the risk of data leakage for evaluation.In order to solve this problem, one of the main features of SciEval is the use of Dynamic Data, which can prevent potential data leakage and ensure the fairness and credibility of the evaluation results.The Dynamic Data will be updated regularly, and we will maintain a stable version to make a fair comparison of</p>
<p>Table 2
2, where we onlycount Static Data. For Dynamic Data, the chemistry part ex-amines the KA ability and contains 2000 data, while thephysics part evaluates the SC ability and involves 890 data.All these questions are in English and we show some dataexamples in Appendix D.AbilityBioChem PhyBasic Knowledge2147 2914456Knowledge Application 1379 372036Scientific Calculation3013401 1165Research Ability100000Total4830 10035 1657</p>
<p>Table 2 :
2
Statistics of Static Data</p>
<p>Table 3 :
3
Models evaluated in this paper.The "access" columns show whether we have full access to the model weights or we can only access through API or web.SD stands for Static Data, DD stands for Dynamic Data, and ED stands for Experimental Data.Marking " " means we evaluate the corresponding model on this subset.
Creator#Parameters Access SD DD EDGPT-4OpenAIundisclosedAPIGPT-3.5-turboOpenAIundisclosedAPIClaude-v1.3AnthropicundisclosedAPIClaude-instant-v1.1 AnthropicundisclosedAPIERNIE BotBaiduundisclosedWebSparkDeskiFLYTEKundisclosedWebVicunaLMSYS13BWeightsGalacticaMeta30B, 6.7BWeightsChatGLM2Tsinghua6BWeightsChatGLMTsinghua6BWeightsAlpacaStanford7BWeightsMOSSFudan16BWeightsLLaMaMeta7B, 13BWeightsModelStatic Data Biology Chemistry Physics Avg.Chemistry(DD) Acc. BLEU MSEPhysics(DD) Exp Acc. ScoreGPT-484.4969.3865.2273.93 11.05 23.78891.0925.8493.31GPT-3.5-turbo76.4264.3052.3066.97 7.6518.862008.7221.8088.27Claude-v1.372.5859.7254.9463.45 5.7521.981489.8726.1485.73Claude-instant-v1.170.4353.3652.3058.92 0.4516.078258.4621.4687.50Galactica-30B66.4850.1644.6554.960.94.14485.9922.47-Vicuna-13B58.3953.0645.1353.93 0.956.50766.6421.24-Galactica-6.7B57.8450.7730.9950.87 1.556.475519.8220.79-ChatGLM2-6B58.6244.0040.2648.440.21.863449.4424.83-ChatGLM-6B52.5445.3640.8047.23 0.752.4410303.9021.01-Alpaca-7B56.6642.4337.0146.540.22.92428419.2726.74-MOSS-16B47.7133.8731.7338.230.17.3730505.1724.27-LLaMa-13B48.5933.5619.4836.960.35.213707.017.08-LLaMa-7B36.2426.3815.0228.370.51.2611305.6514.38-ERNIE Bot--------61.12SparkDesk--------33.69</p>
<p>Table 4 :
4
Model performances of Answer-Only setting.The leaderboard is sorted by the average accuracy of Static Data.</p>
<p>Table 5 :
5
Results on Answer-Only, Chain-of-Thought and 3-Shot settings of each LLM for Dynamic Data.↑ means the performance is slightly better than that under Answer-Only setting, ↓ means worse, and ∼ means the performance is nearly the same.
ModelAOChemistry CoT3-ShotAOPhysics CoT3-ShotGPT-411.05 11.65 ↑ 12.42↑ 25.84 17.98 ↓ 51.01 ↑GPT-3.5-turbo7.65 10.20 ↑ 8.85 ↑ 21.80 47.19 ↑ 25.39 ∼Galactica-6.7B1.551.75 ↑3.05 ↑ 20.79 23.37 ∼ 21.12 ∼Vicuna-13B0.951.95 ↑1.80 ↑ 21.24 18.65 ∼ 23.37∼Galactica-30B0.902.60 ↑3.30 ↑ 22.47 14.72 ↓ 22.58 ∼ChatGLM-6B0.750.80 ↑1.15 ↑ 21.01 25.39 ∼ 23.37 ∼LLaMa-7B0.500.10 ↓1.55 ↑ 18.659.66 ↓27.53 ↑LLaMa-13B0.300.25 ∼ 2.11 ↑7.085.84 ∼22.70 ↑ChatGLM2-6B 0.202.65 ↑1.60 ↑ 24.83 25.39 ∼ 26.74 ∼Alpaca-7B0.200.65 ↑2.10 ↑ 26.71 28.43 ∼ 25.62 ∼MOSS-16B0.100.85 ↑0.65 ↑ 24.27 25.06 ∼ 26.40 ∼
Due to the page limitation, we only compare some widely used benchmarks. For more information, we refer to(Chang et al.<br />
).The Thirty-Eighth AAAI Conference on Artificial Intelligence 
Scientific research requires different dimensions of knowledge, such as understanding and calculation, thence evaluation of scientific ability should be conducted at multiple levels. Bloom's taxonomy is a set of three hierarchical methods used for classification of educational learning objectives covering cognitive, affective and psychomotor domains. TheThe Thirty-Eighth AAAI Conference on Artificial Intelligence 
The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
https://yiyan.baidu.com/
https://xinghuo.xfyun.cn/ The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
The maximum context length of ChatGLM2 is extended to 32k, while it has limited ability to understand long texts. molecular weight of A?" and "What is the SMILES expression of B?". Hence, for questions with numerical answers, we employ
MSE 8 as the evaluation metric, while for questions with string answers, we utilize the BELU score(Papineni et al. 2002). Additionally, we also calculate the extract match scores. As for Experimental Data, each experiment consists of multiple open-ended questions. As a result, we assess the model-generated responses manually.
If the predictions do not contain any number, we will regard the MSE as 1 × 10 10The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
When evaluating on CoT and 3-Shot settings, Claude-Instant and Claude are not available for us, due to the limitation of API.
AcknowledgementsThis work is funded by the China NSFC Projects (92370206, U23B2057, 62106142 and 62120106006) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).
The role of ai in drug discovery: challenges, opportunities, and strategies. A Blanco-Gonzalez, A Cabezon, A Seco-Gonzalez, D Conde-Torres, P Antelo-Riveiro, A Pineiro, R Garcia-Fandino, Pharmaceuticals. 1668912023</p>
<p>Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.03109A survey on evaluation of large language models. 2023arXiv preprint</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>M Forehand, Blooms taxonomy. Emerging perspectives on learning, teaching, and technology. 201041</p>
<p>N Frey, R Soklaski, S Axelrod, S Samsi, R Gomez-Bombarelli, C Coley, V Gadepally, Neural scaling of deep chemical models. 2022</p>
<p>What indeed can GPT models do in chemistry?. T Guo, K Guo, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, arXiv:2305.18365A comprehensive benchmark on eight tasks. 2023arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, J Lei, arXiv:2305.083222023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>A revision of Bloom's taxonomy: An overview. Theory into practice. D R Krathwohl, 200241</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2303.08774Advances in Neural Information Processing Systems. 202235Technical Report</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>ChatGPT: Optimizing language models for dialogue. J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, J F C Uribe, L Fedus, L Metz, M Pokorny, Nature. 2022. 2023Large language models encode clinical knowledge</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>MOSS: Training Conversational Language Models from Synthetic Data. T Sun, X Zhang, Z He, P Li, Q Cheng, H Yan, X Liu, Y Shao, Q Tang, X Zhao, K Chen, Y Zheng, Z Zhou, R Li, J Zhan, Y Zhou, L Li, X Yang, L Wu, Z Yin, X Huang, X Qiu, R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, N Hambro, E Azhar, F , arXiv:2302.13971Novel Paradigm for AIdriven Scientific Research: From AI4S to Intelligent Science. G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, Goyal, 2023. 2023. 2022. 2023. 202338arXiv preprintLlama: Open and efficient foundation language models</p>
<p>X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.10635SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. 2023arXiv preprint</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, arXiv:2306.05685Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023arXiv preprint</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, arXiv:2304.063642023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
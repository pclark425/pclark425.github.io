<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-386 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-386</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-386</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-252683758</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.01081v3.pdf" target="_blank">On The Effects Of Data Normalisation For Domain Adaptation On EEG Data</a></p>
                <p><strong>Paper Abstract:</strong> In the Machine Learning (ML) literature, a well-known problem is the Dataset Shift problem where, differently from the ML standard hypothesis, the data in the training and test sets can follow different probability distributions, leading ML systems toward poor generalisation performances. This problem is intensely felt in the Brain-Computer Interface (BCI) context, where bio-signals as Electroencephalographic (EEG) are often used. In fact, EEG signals are highly non-stationary both over time and between different subjects. To overcome this problem, several proposed solutions are based on recent transfer learning approaches such as Domain Adaption (DA). In several cases, however, the actual causes of the improvements remain ambiguous. This paper focuses on the impact of data normalisation, or standardisation strategies applied together with DA methods. In particular, using \textit{SEED}, \textit{DEAP}, and \textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated the impact of different normalization strategies applied with and without several well-known DA methods, comparing the obtained performances. It results that the choice of the normalisation strategy plays a key role on the classifier performances in DA scenarios, and interestingly, in several cases, the use of only an appropriate normalisation schema outperforms the DA technique.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e386.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e386.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DA->EEG (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptation of Domain Adaptation methods from image/general ML to EEG emotion/BCI tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents and discusses the trend of taking domain-adaptation (DA) methods developed in general machine learning and image-classification contexts and applying or adapting them to EEG-based tasks (emotion recognition, motor imagery), noting both methodological adaptations and empirical outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Domain adaptation methods (general transfer from image/ML contexts to EEG)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Domain adaptation (DA) methods aim to reduce distribution mismatch between a labelled source domain and an unlabelled (or sparsely labelled) target domain by transforming features or learning representations that make source and target distributions more similar (examples: projection-based methods using distribution discrepancy metrics, and adversarial approaches that learn domain-invariant representations). In image and other ML contexts these include methods based on Maximum Mean Discrepancy (MMD), Transfer Component Analysis (TCA), Domain Adversarial Neural Networks (DANN), Adversarial Discriminative Domain Adaptation (ADDA), etc. The paper describes these methods conceptually and how they are being ported to EEG classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transfer learning technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer vision / general machine learning (image classification and ML benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>neuroscience / EEG-based emotion recognition and motor-imagery BCI</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>The paper reports that when DA methods are applied to EEG tasks researchers often: (i) replace convolutional/image backbones with fully-connected or EEG-appropriate feature extractors, (ii) compute domain statistics on EEG-derived features (e.g., Differential Entropy bands, CSP outputs), (iii) alter training/validation protocols to accommodate LOSO-CV or session-based splits, and (iv) adapt the DA objective to EEG-specific pre-processing steps (filtering, CSP, DE features). The paper cites literature (e.g., [25]) and discusses that direct transfers require these contextual adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — the paper reports mixed outcomes in literature and its experiments: DA methods adapted from ML can improve EEG cross-subject/session performance in some cases, but the paper's experiments show that a correct normalization strategy alone often matches or outperforms the adapted DA methods; thus success is conditional on preprocessing and model choices.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>EEG-specific issues: strong non-stationarity across subjects/sessions, different SNR and channel characteristics than images, small sample sizes per subject, confounding effects from preprocessing (e.g., normalization) that can mask or mimic DA gains; architecture mismatch (image CNNs vs EEG feature extractors) and differing assumptions about spatial/temporal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Conceptual commonality of representation learning; availability of unlabeled target EEG data (required by unsupervised DA); precomputed EEG feature representations (DE, CSP) that fit into existing DA pipelines; open datasets (SEED, DEAP, BCI IV 2a).</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Need to adapt feature extractors to EEG (e.g., MLPs or EEG-specific networks), compute domain statistics per-subject/session, use proper cross-validation schemes (LOSO-CV, HLSO), tune hyperparameters for EEG data, and ensure access to unlabelled target-domain EEG samples.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Partially generalizable within EEG and other biosignal domains (other physiological signals) but not trivially plug-and-play; success depends on tailored preprocessing (filtering, feature extraction) and careful normalization; cross-dataset generalization requires further adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (transfer learning/DA frameworks), plus some tacit practical know-how about EEG preprocessing and experimental splits.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e386.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e386.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DANN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-Adversarial Neural Network (DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adversarial domain-adaptation method that trains a feature extractor to produce representations predictive of labels but indistinguishable across source and target domains by a domain classifier trained adversarially.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain-adversarial training of neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Domain-Adversarial Neural Network (DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>DANN uses a feature extractor whose outputs feed both a label predictor (trained to minimize classification loss on source-labelled data) and a domain discriminator (trained to distinguish source vs target). The feature extractor is trained to minimize label loss while maximizing domain classification loss (adversarial objective), thereby encouraging domain-invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / deep domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general machine learning / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>EEG classification (emotion recognition in SEED/DEAP, motor-imagery in BCI IV 2a)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>In experiments the paper: (i) implemented DANN with fully-connected multilayer perceptrons as feature extractors and classifiers suited to EEG features (DE, CSP, etc.), (ii) constrained network depth/nodes (<=3 layers, nodes 1–1000) and searched activations among ReLU/Sigmoid/LeakyReLU, (iii) tuned hyperparameters with Bayesian optimization, (iv) used early stopping and 10% stratified validation, and (v) used the best DANN architecture also as baseline ANN and for ADDA for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>mixed/partially successful — empirical results: on SEED subject-independent folds DANN sometimes improved over no-normalization but was outperformed by a no-DA ANN combined with appropriate per-subject normalization (e.g., Z2 gave NoDA-ANN 81.52% vs DANN 79.03%); on BCI IV 2a DANN with Z3 achieved best DANN mean 68.13% (improvement over some baselines), while normalization alone improved performance by ~6% in some cases. On DEAP DANN performed best for some normalizations (Z3 41.27%) but overall improvements were modest and highly dependent on normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Normalization confounds (simple z-score variants often explain much of the DA gains), EEG non-stationarity, model architecture selection mismatch, small per-subject sample sizes, and instability of adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of unlabelled target data for adversarial training, standardized EEG features (DE, CSP) that feed into MLPs, and automated hyperparameter search to adapt architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires unlabelled target samples during training (unsupervised DA), careful hyperparameter search, early stopping/validation, and appropriate normalization strategy choices to avoid confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable to other EEG classification tasks but results are sensitive to preprocessing; not guaranteed to outperform simpler pipelines unless properly tuned and combined with appropriate normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>computational/explicit procedural (algorithmic objective, network architectures) and tacit practical know-how (stabilizing adversarial training for EEG).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e386.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e386.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Discriminative Domain Adaptation (ADDA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step adversarial DA method that trains separate encoders for source and target and adversarially aligns the target encoder outputs to the source encoder's feature distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial discriminative domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Adversarial Discriminative Domain Adaptation (ADDA)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>ADDA first trains a source encoder and classifier on labelled source data, then trains a target encoder adversarially (with a domain discriminator) to map target inputs into the source encoder's feature space so that the source classifier can be applied to target data.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / deep domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general machine learning / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>EEG classification (SEED/DEAP/BCI IV 2a)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Implemented with fully-connected MLP encoders for EEG feature inputs, used the same architecture search and hyperparameter tuning as for DANN; ADDA's two-encoder procedure was applied to EEG-derived features (DE and CSP) with adversarial training adapted to per-dataset cross-validation regimes (LOSO/HLSO). The best DANN architecture was reused for ADDA for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>limited/variable — in several reported experiments ADDA underperformed compared to NoDA-ANN when an appropriate normalization was applied (e.g., SEED Z2 NoDA-ANN 81.52% vs ADDA 70.43%). In other cases ADDA gave comparable or lower performance; overall improvements were dataset- and normalization-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Same as DANN: adversarial instability, sensitivity to normalization strategies, domain shifts in EEG that are sometimes reducible by simpler normalization, and limited per-domain data for stable adversarial alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Separable source/target encoder design allowed reuse of source classifier, and standardized EEG features enabled plugging into the ADDA pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to labelled source and unlabelled target EEG samples, careful training schedule for two-phase adversarial alignment, computational resources for adversarial training and hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable to EEG tasks but success is conditional; ADDA may require more careful adaptation for EEG than for images.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit algorithmic procedure combined with practical/adaptive know-how for adversarial training on EEG data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e386.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e386.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer Component Analysis (TCA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A shallow domain adaptation method that finds a feature transformation minimizing MMD between source and target while preserving data variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain adaptation via transfer component analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Transfer Component Analysis (TCA)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>TCA projects source and target data into a latent subspace (via kernel methods) chosen to minimize Maximum Mean Discrepancy (MMD) between domains and preserve intrinsic data variance, producing features that can be used by a downstream classifier (here, SVM).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / projection-based domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general machine learning (statistical domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>EEG classification (SEED/DEAP/BCI IV 2a)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with parameter tuning</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied TCA to EEG feature matrices (DE features, CSP outputs) selecting kernels among Linear/RBF/Gaussian as in prior EEG TCA work; used TCA as a preprocessing projector before SVM classification; hyperparameters (kernel, projection dimensionality) selected by grid/search as in prior shallow-DA practice.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — in SEED and BCI experiments TCA combined with SVM sometimes improved over no-DA SVM depending on normalization (e.g., SEED Z0: TCA-SVM 71.58% vs noDA-SVM 52.47% in that particular row), but overall the best outcomes on SEED were often achieved by KPCA-SVM or by no-DA models with Z2 normalization. Success depended on normalization type.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Choice of kernel and projection dimensionality, sensitivity to EEG normalization choices (Z0–Z3), and limited interpretability of projection for EEG domain-specific structure.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of precomputed EEG features and established MMD-based objectives matched TCA's assumptions; simple pipeline (project then SVM) made implementation straightforward.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Careful kernel selection and hyperparameter tuning, and consistent normalization strategy across source/target inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>TCA is broadly applicable to EEG and other structured feature datasets but results depend on appropriate kernel choice and preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural/algorithmic knowledge and parameter-selection know-how.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e386.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e386.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KPCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kernel Principal Component Analysis (KPCA) used as shallow DA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Kernel PCA is used here as a projection-based preprocessing (viewed as shallow DA) to transform EEG features before SVM classification to help cross-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kernel principal component analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Kernel PCA (used as a projection/preranker for DA)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>KPCA performs nonlinear dimensionality reduction by applying PCA in a kernel-induced feature space; when used prior to classification it can produce representations that reduce domain discrepancies and improve classifier robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data analysis technique / projection-based preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general machine learning / nonlinear dimensionality reduction</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>EEG classification (SEED/DEAP/BCI IV 2a)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with tuning</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied KPCA to EEG DE features or CSP features and then trained SVM on projected features; kernel choice and number of components were selected from candidate values to fit EEG data characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in several shallow-DA experiments — e.g., SEED subject-independent best reported shallow result was KPCA-SVM on Z2 with mean accuracy 80.74% (±6.11), nearly matching the best ANN result; KPCA improved SVM performance especially when combined with appropriate normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Selecting kernel and projection dimensionality; sensitivity to normalization; potential overfitting with small per-subject data.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>KPCA's flexible nonlinear projection matched the nonlinear nature of EEG features; straightforward integration into SVM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Hyperparameter selection, computational resources for kernel methods on moderate-sized EEG feature sets, and consistent normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable to other EEG tasks and other biosignal domains provided kernels and component counts are tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit methodological knowledge and parameter tuning expertise.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e386.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e386.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Z-score variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Z-score normalization strategies adapted to multi-subject/session EEG DA (Z0, Z1, Z2, Z3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four explicit z-score normalization protocols were defined and tested to control how mean and standard deviation are estimated across training subjects and test subject/session, showing major impact on DA outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Z-score normalization (four adapted variants Z0, Z1, Z2, Z3)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Standard z-score scales features by subtracting a mean and dividing by a standard deviation. The paper defines four operational variants for EEG DA: Z0 — compute µ,σ on training set and apply to test subject; Z1 — normalize each training subject with their own µ_s,σ_s while test subject is normalized with global training µ,σ; Z2 — normalize every subject/session using that subject/session's own µ_s,σ_s (regardless of train/test split); Z3 — normalize training set with global training µ,σ and normalize test subject with its own µ_s,σ_s. These variants control whether normalization is per-subject or pooled and whether test statistics are used.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data preprocessing technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>classical ML / statistics (standard normalization practice)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>EEG domain adaptation / BCI and emotion recognition</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Defined four specific estimation/application rules for µ and σ tailored to multi-subject EEG datasets to test how per-subject vs pooled normalization affects domain shift and DA effectiveness; implemented and compared across datasets (SEED, DEAP, BCI IV 2a) and CV strategies (LOSO, HLSO).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>highly successful and central to findings — empirical results show that normalization choice often explains most performance gains: e.g., SEED subject-independent Z2 + NoDA-ANN achieved 81.52% (±7.26) outperforming DANN/ADDA; on BCI IV 2a normalization alone produced ~6% improvement in some settings. The paper's main claim is that appropriate z-score variant can outperform or obviate complex DA methods.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Potential leakage if test statistics are (improperly) used in training; need to decide whether using test µ_s,σ_s in unsupervised DA is permissible (paper distinguishes the two µ/σ estimation philosophies), and practical constraints when target subject statistics are unavailable at deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simplicity of z-score, easy computation per-subject/session, and strong effect on centering/scaling EEG feature distributions which directly reduce inter-subject conditional shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Availability of per-subject/session data to compute µ_s,σ_s when using Z2/Z3; clear experimental protocol about whether unlabelled target data (for µ/σ) is permissible during training (unsupervised DA assumption), and careful CV design to avoid information leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Highly generalizable within multi-subject EEG and likely to other physiological biosignals where per-subject baselines dominate; the exact best variant is dataset- and task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural knowledge (precise normalization rules) and practical know-how about choosing the proper normalization variant to mitigate domain shift.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain-adversarial training of neural networks. <em>(Rating: 2)</em></li>
                <li>Adversarial discriminative domain adaptation. <em>(Rating: 2)</em></li>
                <li>Domain adaptation via transfer component analysis. <em>(Rating: 2)</em></li>
                <li>Personal-zscore: Eliminating individual difference for eeg-based cross-subject emotion recognition. <em>(Rating: 2)</em></li>
                <li>Investigating the impact of data normalization on classification performance. <em>(Rating: 1)</em></li>
                <li>Transfer components between subjects for eeg-based emotion recognition. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-386",
    "paper_id": "paper-252683758",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "DA-&gt;EEG (general)",
            "name_full": "Adaptation of Domain Adaptation methods from image/general ML to EEG emotion/BCI tasks",
            "brief_description": "The paper documents and discusses the trend of taking domain-adaptation (DA) methods developed in general machine learning and image-classification contexts and applying or adapting them to EEG-based tasks (emotion recognition, motor imagery), noting both methodological adaptations and empirical outcomes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "procedure_name": "Domain adaptation methods (general transfer from image/ML contexts to EEG)",
            "procedure_description": "Domain adaptation (DA) methods aim to reduce distribution mismatch between a labelled source domain and an unlabelled (or sparsely labelled) target domain by transforming features or learning representations that make source and target distributions more similar (examples: projection-based methods using distribution discrepancy metrics, and adversarial approaches that learn domain-invariant representations). In image and other ML contexts these include methods based on Maximum Mean Discrepancy (MMD), Transfer Component Analysis (TCA), Domain Adversarial Neural Networks (DANN), Adversarial Discriminative Domain Adaptation (ADDA), etc. The paper describes these methods conceptually and how they are being ported to EEG classification tasks.",
            "procedure_type": "computational method / transfer learning technique",
            "source_domain": "computer vision / general machine learning (image classification and ML benchmarks)",
            "target_domain": "neuroscience / EEG-based emotion recognition and motor-imagery BCI",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "The paper reports that when DA methods are applied to EEG tasks researchers often: (i) replace convolutional/image backbones with fully-connected or EEG-appropriate feature extractors, (ii) compute domain statistics on EEG-derived features (e.g., Differential Entropy bands, CSP outputs), (iii) alter training/validation protocols to accommodate LOSO-CV or session-based splits, and (iv) adapt the DA objective to EEG-specific pre-processing steps (filtering, CSP, DE features). The paper cites literature (e.g., [25]) and discusses that direct transfers require these contextual adjustments.",
            "transfer_success": "partially successful — the paper reports mixed outcomes in literature and its experiments: DA methods adapted from ML can improve EEG cross-subject/session performance in some cases, but the paper's experiments show that a correct normalization strategy alone often matches or outperforms the adapted DA methods; thus success is conditional on preprocessing and model choices.",
            "barriers_encountered": "EEG-specific issues: strong non-stationarity across subjects/sessions, different SNR and channel characteristics than images, small sample sizes per subject, confounding effects from preprocessing (e.g., normalization) that can mask or mimic DA gains; architecture mismatch (image CNNs vs EEG feature extractors) and differing assumptions about spatial/temporal structure.",
            "facilitating_factors": "Conceptual commonality of representation learning; availability of unlabeled target EEG data (required by unsupervised DA); precomputed EEG feature representations (DE, CSP) that fit into existing DA pipelines; open datasets (SEED, DEAP, BCI IV 2a).",
            "contextual_requirements": "Need to adapt feature extractors to EEG (e.g., MLPs or EEG-specific networks), compute domain statistics per-subject/session, use proper cross-validation schemes (LOSO-CV, HLSO), tune hyperparameters for EEG data, and ensure access to unlabelled target-domain EEG samples.",
            "generalizability": "Partially generalizable within EEG and other biosignal domains (other physiological signals) but not trivially plug-and-play; success depends on tailored preprocessing (filtering, feature extraction) and careful normalization; cross-dataset generalization requires further adaptation.",
            "knowledge_type": "explicit procedural steps and theoretical principles (transfer learning/DA frameworks), plus some tacit practical know-how about EEG preprocessing and experimental splits.",
            "uuid": "e386.0"
        },
        {
            "name_short": "DANN",
            "name_full": "Domain-Adversarial Neural Network (DANN)",
            "brief_description": "Adversarial domain-adaptation method that trains a feature extractor to produce representations predictive of labels but indistinguishable across source and target domains by a domain classifier trained adversarially.",
            "citation_title": "Domain-adversarial training of neural networks.",
            "mention_or_use": "use",
            "procedure_name": "Domain-Adversarial Neural Network (DANN)",
            "procedure_description": "DANN uses a feature extractor whose outputs feed both a label predictor (trained to minimize classification loss on source-labelled data) and a domain discriminator (trained to distinguish source vs target). The feature extractor is trained to minimize label loss while maximizing domain classification loss (adversarial objective), thereby encouraging domain-invariant features.",
            "procedure_type": "computational method / deep domain adaptation",
            "source_domain": "general machine learning / computer vision",
            "target_domain": "EEG classification (emotion recognition in SEED/DEAP, motor-imagery in BCI IV 2a)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "In experiments the paper: (i) implemented DANN with fully-connected multilayer perceptrons as feature extractors and classifiers suited to EEG features (DE, CSP, etc.), (ii) constrained network depth/nodes (&lt;=3 layers, nodes 1–1000) and searched activations among ReLU/Sigmoid/LeakyReLU, (iii) tuned hyperparameters with Bayesian optimization, (iv) used early stopping and 10% stratified validation, and (v) used the best DANN architecture also as baseline ANN and for ADDA for fairness.",
            "transfer_success": "mixed/partially successful — empirical results: on SEED subject-independent folds DANN sometimes improved over no-normalization but was outperformed by a no-DA ANN combined with appropriate per-subject normalization (e.g., Z2 gave NoDA-ANN 81.52% vs DANN 79.03%); on BCI IV 2a DANN with Z3 achieved best DANN mean 68.13% (improvement over some baselines), while normalization alone improved performance by ~6% in some cases. On DEAP DANN performed best for some normalizations (Z3 41.27%) but overall improvements were modest and highly dependent on normalization.",
            "barriers_encountered": "Normalization confounds (simple z-score variants often explain much of the DA gains), EEG non-stationarity, model architecture selection mismatch, small per-subject sample sizes, and instability of adversarial training.",
            "facilitating_factors": "Availability of unlabelled target data for adversarial training, standardized EEG features (DE, CSP) that feed into MLPs, and automated hyperparameter search to adapt architecture.",
            "contextual_requirements": "Requires unlabelled target samples during training (unsupervised DA), careful hyperparameter search, early stopping/validation, and appropriate normalization strategy choices to avoid confounding.",
            "generalizability": "Applicable to other EEG classification tasks but results are sensitive to preprocessing; not guaranteed to outperform simpler pipelines unless properly tuned and combined with appropriate normalization.",
            "knowledge_type": "computational/explicit procedural (algorithmic objective, network architectures) and tacit practical know-how (stabilizing adversarial training for EEG).",
            "uuid": "e386.1"
        },
        {
            "name_short": "ADDA",
            "name_full": "Adversarial Discriminative Domain Adaptation (ADDA)",
            "brief_description": "A two-step adversarial DA method that trains separate encoders for source and target and adversarially aligns the target encoder outputs to the source encoder's feature distribution.",
            "citation_title": "Adversarial discriminative domain adaptation.",
            "mention_or_use": "use",
            "procedure_name": "Adversarial Discriminative Domain Adaptation (ADDA)",
            "procedure_description": "ADDA first trains a source encoder and classifier on labelled source data, then trains a target encoder adversarially (with a domain discriminator) to map target inputs into the source encoder's feature space so that the source classifier can be applied to target data.",
            "procedure_type": "computational method / deep domain adaptation",
            "source_domain": "general machine learning / computer vision",
            "target_domain": "EEG classification (SEED/DEAP/BCI IV 2a)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Implemented with fully-connected MLP encoders for EEG feature inputs, used the same architecture search and hyperparameter tuning as for DANN; ADDA's two-encoder procedure was applied to EEG-derived features (DE and CSP) with adversarial training adapted to per-dataset cross-validation regimes (LOSO/HLSO). The best DANN architecture was reused for ADDA for fairness.",
            "transfer_success": "limited/variable — in several reported experiments ADDA underperformed compared to NoDA-ANN when an appropriate normalization was applied (e.g., SEED Z2 NoDA-ANN 81.52% vs ADDA 70.43%). In other cases ADDA gave comparable or lower performance; overall improvements were dataset- and normalization-dependent.",
            "barriers_encountered": "Same as DANN: adversarial instability, sensitivity to normalization strategies, domain shifts in EEG that are sometimes reducible by simpler normalization, and limited per-domain data for stable adversarial alignment.",
            "facilitating_factors": "Separable source/target encoder design allowed reuse of source classifier, and standardized EEG features enabled plugging into the ADDA pipeline.",
            "contextual_requirements": "Access to labelled source and unlabelled target EEG samples, careful training schedule for two-phase adversarial alignment, computational resources for adversarial training and hyperparameter tuning.",
            "generalizability": "Applicable to EEG tasks but success is conditional; ADDA may require more careful adaptation for EEG than for images.",
            "knowledge_type": "explicit algorithmic procedure combined with practical/adaptive know-how for adversarial training on EEG data.",
            "uuid": "e386.2"
        },
        {
            "name_short": "TCA",
            "name_full": "Transfer Component Analysis (TCA)",
            "brief_description": "A shallow domain adaptation method that finds a feature transformation minimizing MMD between source and target while preserving data variance.",
            "citation_title": "Domain adaptation via transfer component analysis.",
            "mention_or_use": "use",
            "procedure_name": "Transfer Component Analysis (TCA)",
            "procedure_description": "TCA projects source and target data into a latent subspace (via kernel methods) chosen to minimize Maximum Mean Discrepancy (MMD) between domains and preserve intrinsic data variance, producing features that can be used by a downstream classifier (here, SVM).",
            "procedure_type": "computational method / projection-based domain adaptation",
            "source_domain": "general machine learning (statistical domain adaptation)",
            "target_domain": "EEG classification (SEED/DEAP/BCI IV 2a)",
            "transfer_type": "direct application with parameter tuning",
            "modifications_made": "Applied TCA to EEG feature matrices (DE features, CSP outputs) selecting kernels among Linear/RBF/Gaussian as in prior EEG TCA work; used TCA as a preprocessing projector before SVM classification; hyperparameters (kernel, projection dimensionality) selected by grid/search as in prior shallow-DA practice.",
            "transfer_success": "partially successful — in SEED and BCI experiments TCA combined with SVM sometimes improved over no-DA SVM depending on normalization (e.g., SEED Z0: TCA-SVM 71.58% vs noDA-SVM 52.47% in that particular row), but overall the best outcomes on SEED were often achieved by KPCA-SVM or by no-DA models with Z2 normalization. Success depended on normalization type.",
            "barriers_encountered": "Choice of kernel and projection dimensionality, sensitivity to EEG normalization choices (Z0–Z3), and limited interpretability of projection for EEG domain-specific structure.",
            "facilitating_factors": "Availability of precomputed EEG features and established MMD-based objectives matched TCA's assumptions; simple pipeline (project then SVM) made implementation straightforward.",
            "contextual_requirements": "Careful kernel selection and hyperparameter tuning, and consistent normalization strategy across source/target inputs.",
            "generalizability": "TCA is broadly applicable to EEG and other structured feature datasets but results depend on appropriate kernel choice and preprocessing.",
            "knowledge_type": "explicit procedural/algorithmic knowledge and parameter-selection know-how.",
            "uuid": "e386.3"
        },
        {
            "name_short": "KPCA",
            "name_full": "Kernel Principal Component Analysis (KPCA) used as shallow DA",
            "brief_description": "Kernel PCA is used here as a projection-based preprocessing (viewed as shallow DA) to transform EEG features before SVM classification to help cross-domain generalization.",
            "citation_title": "Kernel principal component analysis.",
            "mention_or_use": "use",
            "procedure_name": "Kernel PCA (used as a projection/preranker for DA)",
            "procedure_description": "KPCA performs nonlinear dimensionality reduction by applying PCA in a kernel-induced feature space; when used prior to classification it can produce representations that reduce domain discrepancies and improve classifier robustness.",
            "procedure_type": "data analysis technique / projection-based preprocessing",
            "source_domain": "general machine learning / nonlinear dimensionality reduction",
            "target_domain": "EEG classification (SEED/DEAP/BCI IV 2a)",
            "transfer_type": "direct application with tuning",
            "modifications_made": "Applied KPCA to EEG DE features or CSP features and then trained SVM on projected features; kernel choice and number of components were selected from candidate values to fit EEG data characteristics.",
            "transfer_success": "successful in several shallow-DA experiments — e.g., SEED subject-independent best reported shallow result was KPCA-SVM on Z2 with mean accuracy 80.74% (±6.11), nearly matching the best ANN result; KPCA improved SVM performance especially when combined with appropriate normalization.",
            "barriers_encountered": "Selecting kernel and projection dimensionality; sensitivity to normalization; potential overfitting with small per-subject data.",
            "facilitating_factors": "KPCA's flexible nonlinear projection matched the nonlinear nature of EEG features; straightforward integration into SVM pipeline.",
            "contextual_requirements": "Hyperparameter selection, computational resources for kernel methods on moderate-sized EEG feature sets, and consistent normalization.",
            "generalizability": "Likely generalizable to other EEG tasks and other biosignal domains provided kernels and component counts are tuned.",
            "knowledge_type": "explicit methodological knowledge and parameter tuning expertise.",
            "uuid": "e386.4"
        },
        {
            "name_short": "Z-score variants",
            "name_full": "Z-score normalization strategies adapted to multi-subject/session EEG DA (Z0, Z1, Z2, Z3)",
            "brief_description": "Four explicit z-score normalization protocols were defined and tested to control how mean and standard deviation are estimated across training subjects and test subject/session, showing major impact on DA outcomes.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Z-score normalization (four adapted variants Z0, Z1, Z2, Z3)",
            "procedure_description": "Standard z-score scales features by subtracting a mean and dividing by a standard deviation. The paper defines four operational variants for EEG DA: Z0 — compute µ,σ on training set and apply to test subject; Z1 — normalize each training subject with their own µ_s,σ_s while test subject is normalized with global training µ,σ; Z2 — normalize every subject/session using that subject/session's own µ_s,σ_s (regardless of train/test split); Z3 — normalize training set with global training µ,σ and normalize test subject with its own µ_s,σ_s. These variants control whether normalization is per-subject or pooled and whether test statistics are used.",
            "procedure_type": "data preprocessing technique",
            "source_domain": "classical ML / statistics (standard normalization practice)",
            "target_domain": "EEG domain adaptation / BCI and emotion recognition",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Defined four specific estimation/application rules for µ and σ tailored to multi-subject EEG datasets to test how per-subject vs pooled normalization affects domain shift and DA effectiveness; implemented and compared across datasets (SEED, DEAP, BCI IV 2a) and CV strategies (LOSO, HLSO).",
            "transfer_success": "highly successful and central to findings — empirical results show that normalization choice often explains most performance gains: e.g., SEED subject-independent Z2 + NoDA-ANN achieved 81.52% (±7.26) outperforming DANN/ADDA; on BCI IV 2a normalization alone produced ~6% improvement in some settings. The paper's main claim is that appropriate z-score variant can outperform or obviate complex DA methods.",
            "barriers_encountered": "Potential leakage if test statistics are (improperly) used in training; need to decide whether using test µ_s,σ_s in unsupervised DA is permissible (paper distinguishes the two µ/σ estimation philosophies), and practical constraints when target subject statistics are unavailable at deployment.",
            "facilitating_factors": "Simplicity of z-score, easy computation per-subject/session, and strong effect on centering/scaling EEG feature distributions which directly reduce inter-subject conditional shifts.",
            "contextual_requirements": "Availability of per-subject/session data to compute µ_s,σ_s when using Z2/Z3; clear experimental protocol about whether unlabelled target data (for µ/σ) is permissible during training (unsupervised DA assumption), and careful CV design to avoid information leakage.",
            "generalizability": "Highly generalizable within multi-subject EEG and likely to other physiological biosignals where per-subject baselines dominate; the exact best variant is dataset- and task-dependent.",
            "knowledge_type": "explicit procedural knowledge (precise normalization rules) and practical know-how about choosing the proper normalization variant to mitigate domain shift.",
            "uuid": "e386.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain-adversarial training of neural networks.",
            "rating": 2,
            "sanitized_title": "domainadversarial_training_of_neural_networks"
        },
        {
            "paper_title": "Adversarial discriminative domain adaptation.",
            "rating": 2,
            "sanitized_title": "adversarial_discriminative_domain_adaptation"
        },
        {
            "paper_title": "Domain adaptation via transfer component analysis.",
            "rating": 2,
            "sanitized_title": "domain_adaptation_via_transfer_component_analysis"
        },
        {
            "paper_title": "Personal-zscore: Eliminating individual difference for eeg-based cross-subject emotion recognition.",
            "rating": 2,
            "sanitized_title": "personalzscore_eliminating_individual_difference_for_eegbased_crosssubject_emotion_recognition"
        },
        {
            "paper_title": "Investigating the impact of data normalization on classification performance.",
            "rating": 1,
            "sanitized_title": "investigating_the_impact_of_data_normalization_on_classification_performance"
        },
        {
            "paper_title": "Transfer components between subjects for eeg-based emotion recognition.",
            "rating": 2,
            "sanitized_title": "transfer_components_between_subjects_for_eegbased_emotion_recognition"
        }
    ],
    "cost": 0.017004,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On The Effects Of Data Normalisation For Domain Adaptation On EEG Data</p>
<p>Andrea Apicella 
Department of Electrical Engineering and Information Technology
University of Naples Federico II
NaplesItaly</p>
<p>Laboratory of Augmented Reality for Health Monitoring (ARHeMLab)</p>
<p>Francesco Isgrò 
Department of Electrical Engineering and Information Technology
University of Naples Federico II
NaplesItaly</p>
<p>Laboratory of Augmented Reality for Health Monitoring (ARHeMLab)</p>
<p>Andrea Pollastro 
Department of Electrical Engineering and Information Technology
University of Naples Federico II
NaplesItaly</p>
<p>Laboratory of Augmented Reality for Health Monitoring (ARHeMLab)</p>
<p>Roberto Prevete 
Department of Electrical Engineering and Information Technology
University of Naples Federico II
NaplesItaly</p>
<p>Laboratory of Augmented Reality for Health Monitoring (ARHeMLab)</p>
<p>On The Effects Of Data Normalisation For Domain Adaptation On EEG Data
10.1016/j.engappai.2023BCI · EEG · domain shift · normalization · scaling · pre- processing
In the Machine Learning (ML) literature, a well-known problem is the Dataset Shift problem where, differently from the ML standard hypothesis, the data in the training and test sets can follow different probability distributions, leading ML systems toward poor generalisation performances. This problem is intensely felt in the Brain-Computer Interface (BCI) context, where bio-signals as Electroencephalographic (EEG) are often used. In fact, EEG signals are highly non-stationary both over time and between different subjects. To overcome this problem, several proposed solutions are based on recent transfer learning approaches such as Domain Adaption (DA). In several cases, however, the actual causes of the improvements remain ambiguous. This paper focuses on the impact of data normalisation, or standardisation strategies applied together with DA methods. In particular, using SEED, DEAP, and BCI Competition IV 2a EEG datasets, we experimentally evaluated the impact of different normalization strategies applied with and without several well-known DA methods, comparing the obtained performances. It results that the choice of the normalisation strategy plays a key role on the classifier performances in DA scenarios, and interestingly, in several cases, the use of only an appropriate normalisation schema outperforms the DA technique.</p>
<p>Introduction</p>
<p>In recent years, Brain-Computer Interfaces (BCIs) have been emerging as technology allowing the human brain to communicate with external devices without the use of peripheral nerves and muscles, enhancing the interaction capability of the user with the environment. BCI applications go from severely disabled persons for rehabilitation purposes to healthy subjects for devising new types This paper has been published in its final version on Engineering Applications of Artificial Intelligence journal with DOI https://doi.org/10.1016/j.engappai.2023. 106205 of applications [1]. In particular, BCI has a growing interest in the scientific community thanks to its implication in several medical fields, such as assisting [2], monitoring [3], enhancing [4], or diagnosing patients' emotional or physical states [5,6]. Current literature reports that patients subjected to BCI-based Rehabilitation methods show benefit and improvement in their injured capacities [7]. Currently, several methods exist to allow the interaction between humans and machines. In particular, several proposals for BCI methods based on Electroencephalographic (EEG) signals are made. This is because measuring and monitoring the brain's electrical activity can provide important information related to the brain's physiological, functional, and pathological status. EEG signals are particularly suitable for this aim thanks to their essential qualities, such as non-invasiveness and high temporal resolution.</p>
<p>Modern Machine Learning (ML) methods such as Deep Neural Networks (DNNs) are mainly used to process acquired EEG signals for several tasks, such as emotion classification, engagement and attention detection. In general, a supervised ML model learns from human classified data to generalise to new unknown data. The standard pipeline to develop an ML system consists in i) data acquisition, ii) data preprocessing, iii) feature extraction, iv) model learning v) model validation. However, the performance obtained using classical ML methods in EEG-related tasks is often poor [8]. This is mainly because the EEG signal is highly non-stationary [9], substantial differences across the EEG acquired at different times or from different subjects exist, even with the same affect felt. More in detail, the starting hypothesis of the traditional ML methods states that all the used data, whether used in the training process or not, come from the same probability distribution. This assumption results are not always verified in the case of EEG signals. In the ML literature, this is an instance of the Dataset Shift problem [10]. In a nutshell, a Dataset Shift arises when the starting ML assumption is not valid, so the distribution of the training data differs from the data distribution used outside of the training stage. In other words, a model trained on a set of EEG data acquired from a given subject at a specific time (or during a specific session) should not work as expected in classifying EEG signals acquired from a different subject at different times. In other words, the model has poor generalisation performance. A first attempt to mitigate this problem is training specific models for each subject (Subject-Dependent models) to reduce the performance gap due to using the same ML system on different users. However, non-stationary signal problems related to the different user's physical and psychological conditions at different times remain. Furthermore, a Subject-Dependent model is valid only for the subject providing training data acquisition, making these models expensive and not very versatile and uncomfortable to the user, who will be tied to initial acquisition sessions before it can actually use the system for real classifications.</p>
<p>For these reasons, newer studies [11,12] tried to overcome these limits given by Dataset Shift, taking into account the difference between the data distribution probabilities (domains) acquired in different times and for different subjects. Several proposed solutions are based on Transfer Learning (TL) [13], a set of approaches aiming to transfer the knowledge learned from a system to improve another. TL approaches can be categorised into several subfamilies. One of the most famous is the Domain Adaptation (DA) [12] approaches family. DA approaches start from the hypothesis that unlabeled data from the target domain are also available during the training stage. For example, in the case of EEG-based emotion recognition, class-labelled data can be acquired in an initial session and classified using a standardised labelling protocol (e.g., questionnaires administered during the task). In contrast, class-unlabeled data can be acquired in a later session. DA provides several methods exploiting both labeled and unlabeled data to build an ML model able to minimise the discrepancy between the two data distributions, leading to better classification performances on unlabeled data. Thus, performance improvements are often reported using DA methods in several EEG-based classification studies. However, from a methodological point of view, it is essential to note that the pipeline to develop and evaluate an ML model consists of several steps which can influence each other [14]. Consequently, in several cases [15] the causes of the improvements can remain ambiguous. This paper focuses on the impact of data normalisation, or standardisation strategies applied together with DA methods.</p>
<p>However, DA methods assume that all the class-labelled data used during the training comes from the same source probability distribution (source domain), i.e. all the labelled data belong to the same unique domain. This assumption is often neglected in several EEG-based works [16,17], considering all the labeled data together during the training stage. Indeed, in several cross-subject/crosssession studies adopting DA strategies, it is not hard to see attempts to generalise toward an unseen domain (a subject or a session) using learning/source data acquisitions from several other and different sessions/subjects without considering their different probability distributions, so treated as belonging to the same domain. Despite this, performance improvements are often reported using DA methods in several EEG-based classification studies. We hypothesise that this improvement may not be caused by the DA method but by some data normalisation or standardisation strategies applied a priori.</p>
<p>More in detail, in ML applications, normalisation functions [18] are often applied to pre-process the input features before to be fed to the ML system. Normalisation functions are often adopted to scale or transform the features such that each feature has a uniform contribution to the ML pipeline. In [18] is shown that using some normalisation function can impact or not on the final classification performance, depending on the different features and properties that data may have. However, several tasks involving EEG and ML methods applying well-known normalisation functions (such as Z-score normalisation [18]) on the input features have been proposed over the years (for example, [19]). In many of these studies, the normalisation function is often a de-facto standard in an EEG ML pipeline. In particular, one of the most used normalisation strategies is the Z-score normalisation, consisting of a translation and a scaling of the data with respect to its mean and variance. For instance, in [20,21,22,23,24] is shown that using a normalisation function can affect the cross-subject performances.</p>
<p>In particular, the translation with respect to the mean can already be seen as a simple form of domain adaptation.</p>
<p>This study aims to investigate if and how some normalisation strategies affect the performance of some of the DA methods applied to EEG signal classification. The main contribution of this research work is that in several EEG classification problems, the higher impact in reducing the domain shift seems to be due mainly to the data normalisation stage rather than the application of several DA methods commonly used in the literature.</p>
<p>The paper is organised as follows: in Section 2 some of the most known DA methods are reported, in Section 4 the DA framework is described, and our hypothesis is expressed, in Section 5 the experimental assessment, and the obtained results are reported, in Section 7 the obtained results are discussed. Finally, Section 8 is left to the final remarks.</p>
<p>Related works</p>
<p>As in this work, we want to investigate the impact of input normalization strategies on DA methods. We first discuss DA approaches. Then, we present the main standard data normalization techniques in this context. Finally, we highlight differences and similarities with related research studies.</p>
<p>More recently, Transfer Learning (TL) methods are receiving strong attention from the scientific community. TL methods are based on the concept of Domain. Following the survey of Pan et al. [13], a Domain can be defined as a set D = {F, P (X)} where F is a feature space and P (X) is the marginal probability distribution of a specific dataset X = {x 1 , x 2 , . . . , x n } ∈ F . Domain Adaptation methods start from the hypothesis that data sampled from two different Domains are available, called Source Domain and Target Domain, respectively. The main difference between Source and Target is that, while both data and labels S Source = {(x i , y i )} n i=1 can be sampled from the Source domain, only feature data points X T arget = {x j } m j=1 ∈ F T arget sampled from the Target Domain are available during the training stage, without any knowledge (unsupervised DA) or minimal knowledge (semi-supervised DA) of their real labels. DA methods are getting a great deal of attention in the scientific community in different contexts, such as image classification, voice recognition, etc., and several proposals have been made over the years. One trend of the literature is to adapt DA methods originally proposed in a context (e.g., image classification) to another one (e.g., EEG emotion recognition). For example, in [25] methods to adapt DA strategies from the image classification context to EEG emotion classification are proposed. However, each context has its characteristics and peculiarities, making it not trivial to adapt a DA method from one task to another. The scientific community attempted to adapt well-established DA methods to tasks involving EEG signal processing in the emotion recognition field.</p>
<p>In [15], DA methods are divided into two main categories: i) shallow DA methods, where a representation function projecting the source and the target data is given a-priori, and deep DA methods, where the data representation is learned as part of the DA strategy.</p>
<p>For instance, one of the most known shallow DA methods is Transfer Component Analysis (TCA, [26]). TCA searches for a data transformation based on the Maximum Mean Discrepancy (MMD, [27]). MMD was proposed to test the similarity between two probability distributions. An empirical estimation of MMD is given by
M M D(X S , X T ) = || 1 |X S | |X S | i=1 φ(x (i) S ) − 1 |X T | |X T | i=1 φ(x (i) T )|| 2 H where X S = {x (i) S } M i=1 and X T = {x (i) T } N i=1
are data sampled from the source and the target domain respectively, while φ(·) is an appropriate feature mapping.</p>
<p>Starting from the hypothesis that the data are sampled from two different domains, TCA searches for a transformation of the data such that the data variance is maximally preserved reducing, at the same time, the MMD discrepancy between the domains distributions.</p>
<p>An evaluation of the TCA on EEG data for emotion recognition was made in [16]. While it is not specifically proposed for Domain Adaptation, Kernel-PCA (KPCA, [28]) can be viewed as another shallow-DA strategy. In a nutshell, KPCA uses the kernel trick to project the data into proper kernel space and then apply the PCA to the projected data.</p>
<p>On another side, many modern deep DA strategies rely on Domain Adversarial Learning approaches, proposed in [15,29,30]. In a nutshell, these proposals learn a DNN feature representation considering both the desired task and the discrepancy between the Source and the Target domain. The goal is to make the data distributions indistinguishable for an ad-hoc domain discriminator. The final model is a deep neural network model (Domain Adversarial Neural Network, DANN) predicting, for each input, both the corresponding class and the belonging domain. Therefore, learning a feature mapping that maximises the class prediction performances and the domain classification loss to make the feature distributions as similar as possible is made. Adversarial Discriminative Domain Adaptation (ADDA) is another Domain adversarial learning strategy proposed in [31]. Differently from DANN, ADDA learns two autoencoders E S and E T , to represent the Source and the Target domains, respectively. Furthermore, E S is trained together with a classifier C, exploiting the available Source domain labelled data. Then, through an adversarial learning procedure, E T is trained to map the Target domain data to the space of the E S outputs. Finally, target data in E S can be classified by C.</p>
<p>Domain adversarial learning methods are widely used in several studies for EEG data recognition, for example, in [31,32,33].</p>
<p>All the methods mentioned above only consider two domains: the Source and the Target one.</p>
<p>However, simple methods used to reduce gaps between different data relied on data normalisation schemes, such as min-max or z-score normalisation, where data are transformed using simple functions that leverage statistics on the data itself. For instance, in [20,21,22,23,24] is shown that just a proper normalisation schemes to preprocess the EEG data can affect the cross-subject performances.</p>
<p>In [20] several normalisation schemes were applied following two different schemas: i) All-subjects, where the whole dataset was normalised, ii) Singlesubject, where the normalisation is made individually for each subject. The All-subject schema is the most common method used to mitigate the impact of each data value on the entire dataset. Single-subject, instead, consider each subject individually, applying normalisation to each subject. The authors empirically showed that Single-subject Z-score performs better in an EEG emotion recognition problem with respect to other normalisation schemes as min-max normalisation.</p>
<p>In [21] single-subject Z-score normalisation is effectively used to improve the performance in the cross-subject case of a student engagement detection problem. [23] scaled the range of each subject's features using the means of the subject features across the classes.</p>
<p>[22] applies single-subject Z − score normalisation after each neural network layer (Stratified Normalisation).</p>
<p>In [24] a simple transformation of the original data for better classification performance is proposed. It uses binary indicator features composed of 0s and 1s, depending on whether the original feature is lower or higher than the median feature value. This leads to a more effective reduction of the subject-dependent part of the EEG signal.</p>
<p>[34] the effect of different normalisation strategies is evaluated on DAN and a proposed domain adaptation method (MS-MDA) in an emotion recognition context. The reported results showed that the normalisation scheme could significantly impact the final classification performance.</p>
<p>Notation</p>
<p>In the remainder of this work we adopt the following notation: let X ∈ R n×m an EEG dataset having n samples and m features per sample acquired from N subjects, and X s ∈ R ns×m a subset of X containing only the n s samples related to a subject s. We denote with µ and σ respectively the mean and the standard deviation estimates computed on X. We denote with µ s and σ s respectively the mean and the standard deviation estimates computed on X s .</p>
<p>Problem description</p>
<p>Dealing with the non-stationarity of EEG signals is among the major challenges for the BCI research [35,36,37]. Non-stationarity of EEG signals over time can be observed in conjunction with changes in behaviours and mental states of the observed subjects. From a statistical point of view, it refers to a continuous change in a class definition causing a change in data distributions [38], thus implying a high variability of the signals among different experimental sessions for each subject. Moreover, high signal variability can also be seen among different subjects due to individual differences expressed through EEGs [39].</p>
<p>In the context of a classification task for a set of subjects, usually two scenarios are mainly explored: the building of a subject-independent model shared by all the subjects, where the goal is to realise a unique model able to be used on any subject with high performance, or the fitting of a subject-dependent model, where specific models are built for each subject. Due to the consequences of the high variability of the EEG signals, in these scenarios the hypothesis that the training set comes from the same probability distribution of the test set can be violated.</p>
<p>In the context of ML, due to the problems related to the high variability of the EEG signals, subjects and/or sessions can be considered as belonging to different domains affected by a distribution shift [40]. For this reason, in several works regarding EEG signals classification, Domain Adaptation techniques improved classification performances (e.g., [40,41,42]).</p>
<p>In this paper, we aim to investigate the hypothesis that the improvements in classifier performance reported by several affirmed DA methods may be strongly conditioned by data normalisation strategies rather than the DA techniques. For instance, Chen et al. in [20] have already highlighted the impact of representing each domain via z -score on the classification of the signals, but without analysing its impact on classical DA methods. We remember that the z-score Z of a set of data X can be computed as:
Z(X, µ, σ) = X − µ σ
, where µ and σ are usually the mean and the standard deviation computed respectively over the features of X. In fact, the authors emphasised how the application of the z-score to highly clustered domain subjects could help mitigate individual differences of the signals in the feature space.</p>
<p>In the presence of data coming from several subject domains affected by domain shifts and processed through DA techniques, we wonder whether the data normalization stage might be a critical step when one applies a DA method. Our idea is intuitively represented in the simple example shown in Figure 1, where we can see two different domains, D 0 and D 1 , having the same feature and label space but affected by a domain shift. Assuming that the two domains have the same conditional probabilities P (Y D0 |X D0 ) = P (Y D1 |X D1 ) as in Figure 1, a proper domain z-score data normalization stage, a scenario in which conditional distributions mostly overlap could be verified, thus mitigating the domain shift problem without the using of any DA method.</p>
<p>In the remainder of this paper, we investigate the impact of the normalization stage on DA methods through experiments on different EEG datasets. In particular, for each dataset, we compare the impact of different normalization strategies applied with and without several DA methods and the performance obtained by the DA strategies as usually described in the literature.</p>
<p>Experimental assessment</p>
<p>In this section, we investigate the impact of the normalization stage on DA methods considering the z-score as normalization strategy.</p>
<p>In classical ML problems, two main assumptions are that i) we have no access to test data during the training stage, ii) training and test data belong to the same domain. In this context, to compute the z-score normalization, µ and σ are usually estimated only over the training data due to the assumption that both the training and the test data are samples drawn from the same distribution, therefore sharing the same estimated parameters. On the other hand, in an Unsupervised Domain Adaptation scenarios, the training and test set are not usually drawn from the same distributions, and a set of unlabelled test data is supposed to be available during the training. In this case, µ and σ can be estimated in two ways:</p>
<ol>
<li>µ and σ are estimated separately on training data and unlabelled test data; 2. µ and σ are estimated only on training data, as in the ML classical scenarios.</li>
</ol>
<p>In the context of EEG data, acquisitions are made across several subjects/sessions. Since each subject/session can be considered as a different domain due to nonstationarity of EEG signals, two different hypothesis about the belonging domains can be made: a. all the subjects/sessions are considered as belonging to the same domain; b. each subject/session is considered as a different domain.</p>
<p>Considering these different conditions, several modalities emerge to perform zscore normalization in the contexts of EEG-data and DA methods. The following z-score normalization strategies were examined in this paper: -Z 0 : the training set was transformed computing µ and σ on the only training data; the test subject/session was transformed using parameters µ and σ computed over the training set (i.e., it corresponds to the the classical zscore normalization applied on the training data); -Z 1 : each subject/session s belonging to the training set was transformed using its own parameters µ s and σ s ; the test subject/session was transformed using µ and σ computed on the whole training data; -Z 2 : each subject/session s, regardless the training/test set partitioning, was transformed using its own parameters µ s and σ s ; -Z 3 : the training set was transformed using parameters µ and σ computed on the whole training data; the test subject/session was transformed on its own parameters µ s and σ s .</p>
<p>Our hypothesis was explored in a series of experiments on three EEG datasets: SEED [43], BCI Competition IV 2a [44] and DEAP [45]. Further details regarding the mentioned datasets are provided in this section. We point out that our interest in these experiments is in investigating the normalisation strategies' impact on DA methods in terms of performance degradation/enhancement of classifiers and not in providing new state-of-the-art results on the involved datasets.</p>
<p>For each dataset, we conducted our experiments on the four normalisation strategies described above using different frameworks typically used in DA: i) a deepDA-based framework, where we analysed the performances of the two well-known deep DA methods DANN [30] and ADDA [31], applied on ANNs, and comparing their performance with the one achieved using the same ANN architectures without the DA components, ii) a shallow DA-based framework, where we compared the performances obtained using a typical projection-based method as TCA [26] and KPCA [46], followed by a Support Vector Machine (SVM) [47] classifier, with those achieved using the SVM classifier only. Figure  2 shows the general processing pipeline adopted in this work.</p>
<p>Model performances were obtained adopting i) the Leave-One-Subject-Out Cross-Validation (LOSO-CV) strategy for the subject-independent case where, for each iteration, the training set resulted to be a composition of multiple training subjects while the test set was composed by just one test subject, ii) the Hold-Last-Session-Out (HLSO) strategy for the subject-dependent case, where the last session from a chronological point of view was considered as test set while the others are considered as training set. These experiments were not performed on the DEAP dataset since just one session is provided.</p>
<p>For the shallow DA-based experiments, we followed the setup proposed in [16], searching for the best kernel methods among {Linear, RBF, Gaussian}, while for ANNs-based ones, according to the original architectures of the DANN and ADDA methods, full-connected multi-layered neural networks were chosen as models for each architectural component (feature extractor, label predictor, domain classifier). Hyperparameters were tuned using a bayesian optimisation method [48]. In particular, for each architectural component, the number of layers was constrained to a maximum of 3, the number of nodes per layer was searched in the set {1, 2, ..., 1000} and the activation function was searched among ReLU, Sigmoid and LeakyReLU. Moreover, for the ANNs-based experiments, each experiment was made considering early stopping as convergence criterion with 20 epochs as patience; the 10 % of the training set was extracted and considered as validation set using stratified sampling [49] on class labels; optimisation was performed using Adam optimiser [50] with a learning rate that was searched in the space {0.1, 0.01, ..., 0.0001}. In order to ensure fairness in experimental conditions, the best architecture found in the DANN method was also used in ADDA and in the pure ANN without DA components (i.e. domain classifier). The accuracy score was used for each experiment to evaluate the performance of the method.</p>
<p>SEED</p>
<p>The SEED dataset consists of EEG data from 15 subjects while watching 15 video clips of about 4 minutes. Each video clip was chosen to induce positive, neutral and negative emotions. For each subject, three data sessions were collected with an interval of about one week. EEG signals were recorded in 62 channels using the ESI Neuroscan System 3 according to the international 10-20 system, at a sampling rate of 1000 Hz and downsampled to 200 Hz. Following [51], we considered the pre-computed Differential Entropy (DE) features smoothed by Linear Dynamic Systems (LDS). DE features are pre-computed, for each second, in each channel, over the following five bands: Delta (1-3 Hz); Theta (4-7 Hz); Alpha (8-13 Hz); Beta (14-30 Hz); Gamma (31-50 Hz). As in [51], following a sampling stratified on class labels, 1000 samples for each subject were randomly selected as training set due to the limited available memory and computation time.</p>
<p>BCI Competition IV 2a</p>
<p>The BCI Competition IV 2a dataset consists of EEG data acquired from 9 subjects during motor imagery tasks. The dataset involves 4 EEG measurement classes: left hand, right hand, feet, and tongue. 22 Ag/AgCl electrodes recorded EEG signals at a sampling rate of 250 Hz. The EEG signals were filtered using the IIR Butterworth filter of order 5 with a bandpass cut-off frequency of 8 to 30 Hz. The four-class classification problem was reduced to a binary classification problem, thus considering left-hand and right-hand labels as in, for example, [52,53,54]. Finally, the Common Spatial Pattern (CSP) [55] was applied since it is a widely recognised feature extraction technique involved in classification tasks on the motor imagery studies [56].</p>
<p>DEAP</p>
<p>The DEAP dataset consists of EEG data acquired from 32 subjects while they were exposed to 40 of about 1 minute. EEG signals were recorded in 32 channels using the Biosemi ActiveTwo devices 4 at a sampling rate of 512 Hz and downsampled to 128 Hz. After watching each video, each subject was required to rate each video in terms of valence (pleasantness level), arousal (excitation level), dominance (control power), liking (preference) and familiarity (knowledge of the stimulus), where each rating ranged from one (weakest) to nine (strongest). Only the familiarity level ranged from one to five. The EEG signals were recorded by Biosemi ActiveTwo devices at a sampling rate of 512 Hz and downsampled to 128 Hz. Following [40], we labelled each trial discretizing and partitioning the dimensional emotion space as follows:</p>
<p>positive if valence rating is greater than 7; neutral if valence rating is smaller than 7 and greater than 3; negative if valence rating is smaller than 3.</p>
<p>Moreover, as in [40], since trials 18, 16 and 38 had the most participants reporting to have successfully induced positive, neutral and negative emotion, a subset of subjects that reported a successful emotion induction with these trials were was selected. In particular, data related to subjects 2, 5, 10,11,12,13,14,15,19,22,24,26,28, and 31 were involved in the experimental assessments. Finally, DE was applied to EEG data in the bands Delta, Theta, Alpha, Beta and Gamma, as for the SEED dataset.</p>
<p>Results</p>
<p>In this Section, we present the results collected in our series of experiments. For each experiment, the results obtained without normalization are reported under the heading of "noNorm". For the ANNs and SVMs based experiment, with "noDA-ANN" and "noDA-SVM" we refer to the pure architectures without the DA components (thus, only with their feature-extractor and label predictor). For the subject-independent experiments, we report the mean and the standard deviation over the folds for each type of normalisation. On the other hand, for the subject-dependent experiments, we report the mean and the standard deviation over the subjects for each kind of normalisation.</p>
<p>SEED</p>
<p>In Table 1 the results related to the subject-independent experiments on SEED are reported. Regarding the deep-DA experiments, for the Z 0 and Z 2 normalisations, the use of the ANN leads to better results than those obtained through the DANN and ADDA methods. For the Z 3 normalizaion the use of the ANN leads to results comparable with the ones reached by using DANN, but higher than those reached by using ADDA; for the Z 1 normalisation, NoDA-ANN performances are lower than those of DANN, but higher than the ones achieved using ADDA (the same situation is also encountered for the noN orm case). The best performances are attributed to the NoDA-ANN case on the Z 2 normalisation with a mean accuracy of 81.52 ± 7.26. Thus, the use of only Z 2 normalisation outperforms the other tested methods. For the shallow-DA based experiments, instead, we can observe that for the Z 0 normalisation, the use of SVM leads to lower results than using TCA-SVM, but higher than using KPCA-SVM; for the Z 1 and Z 2 normalisations, NoDA-SVM leads to lower results than both DA techniques; for the Z 3 normalisation, NoDA-SVM reaches lower results than TCA-SVM, but comparable with KPCA-SVM; for the noN orm case, NoDA-SVM leads results higher than TCA-SVM but lower than KPCA-SVM. The best performances are attributed to the KPCA-SVM case on the Z 2 normalisation with a mean accuracy of 80.74 ± 6.11. However, the most significant improvement seems to be obtained by the Z 2 normalisation in the NoDA-SVM, improving the performance to 74.71 ± 8.47 % from an initial 52.96±9.82 % accuracy without any normalisation, while the use of DA methods gives an improvement of about 6 %. In Table 2 the results related to the subject-dependent experiments on SEED are reported. For the deep-DA experiments, on the noN orm, Z 0 , Z 1 and Z 2 cases NoDA-ANN leads to higher results than DA method; for the Z 3 normalisation, the use of ANN leads to lower results than the DANN method, but higher than the ADDA method. The best performances are attributed to the NoDA-ANN case on the Z 2 normalisation with a mean accuracy of 83.93 ± 9.60. Regarding the shallow-DA based experiments, on the noN orm, Z 2 and Z 3 normalisations noDA-SVM achieves better results than SVM applied with DA methods; for the Z 0 normalisation, NoDA-SVM performances are lower than those of TCA-SVM but higher than the ones of KPCA-SVM; for the Z 0 normalisation, the use of NoDA-SVM leads to results lower than those of TCA-SVM but comparable with the ones obtained through KPCA-SVM. The best performances are attributed to the NoDA-SVM case on the Z 3 normalisation with a mean accuracy of 86.56±8.15. Therefore, also in this case the use of a simple normalisation method seems to be more effective than the selected DA-methods. </p>
<p>BCI Competition IV 2a</p>
<p>Differently from experiments on SEED, only results related to Z 1 , Z 2 and Z 3 normalizations are reported since the CSP implementation 5 we already performed a z -score normalization, thus making the Z 0 normalization unnecessary in our experiments. In Table 3 the results related to the subject-independent experiments on BCI Competition IV 2a are reported.  In Table 4 </p>
<p>DEAP</p>
<p>Differently from SEED and BCI Competition IV 2a, experiments on DEAP were performed only for the subject-independent case since the dataset provided a single session. The results are reported in Table 5. </p>
<p>Discussion</p>
<p>The experimental results suggest that the normalisation method one uses plays a crucial role in improving the classification performances in DA approaches on EEG data. We will focus our discussions on results related to the SEED dataset, but similar observations can also be made for BCI Competition IV 2a and DEAP.</p>
<p>Subject-independent experiments</p>
<p>In Table 6, data related to a fold of a subject-independent experiment are represented using t-SNE [58] before the application of any DA method for each normalization type. It is interesting to notice how for Z 1 and Z 2 normalisations, a similar scenario
Z0 Z1 Z2 Z3</p>
<p>Domains</p>
<p>Training/Test Labels Table 6: Graphical representation of SEED data on subject-independent experiments after each type of normalization. On the first row, all domains involved in the series of experimental are marked by different colors; on the second row, training and test data are marked with blue and red, respectively; on the third row, data are distinguished by their label. On each column, data transformed by the relative normalization type are shown.</p>
<p>to Figure 1 is verified on these data: after the normalisation stage, clusters of data having the same labels are observable, corroborating the hypothesis that conditional distributions over the subjects could be equal or similar, thus leading the normalisation to reduce the domain shifts without DA methods.</p>
<p>In the ANNs based experiments, we can notice that ANN model achieves the best performances on Z 2 normalisation without using any DA method. Moreover, this can also be observed from how performances are distributed on the ANN method as the type of normalisation changes: accuracy means are distributed from a minimum of ∼ 46 % to a maximum of ∼ 82 %.</p>
<p>On the other hand, for the Projection Matrix-based experiments, the highest performances are reached by the KPCA-SVM method on Z 2 normalisation. According to how accuracy means vary as the normalisation type changes (from a minimum of ∼ 59 % to a maximum of ∼ 81 %), we can hypothesise that also the right balance between DA methods and normalisation type has an impact on performances.</p>
<p>Comparing the ANNs based experiments with the Projection Matrix-based ones, we can conclude that the impact DA methods could be affected by the choice of the model: in the first case, using ANNs, the DA methods does not give any contribution; in the second case, the DA method contributes to improving the SVM performances.</p>
<p>Subject-dependent experiments</p>
<p>In Table 7, data related to a subject sampled during the subject-dependent experiments are represented before applying any DA method for each normalisation type.</p>
<p>Also in this case, a scenario similar to Figure 1 is verified, particularly on Z 2 normalisation where data having the same labels are clustered, thus leading to suppose that domains could have equal or similar conditional distributions.</p>
<p>In this case, on deep DA-based experiments, the best results are achieved by noDA-ANN on Z 2 normalisation without using DA methods. In contrast, in the shallow DA-based experiments, best results are achieved by noDA-SVM on Z 3 normalisation. Also, for subject-dependent experiments, for the best methods, performances change as the normalisation type changes: accuracy means vary from a minimum of ∼ 45 % to a maximum of ∼ 84 % (Artificial Neural Network based) and from a minimum of ∼ 64 % to a maximum of ∼ 87 % (shallow DA based). Thus, as in the subject-independent case, we can observe that the normalisation type significantly impacts the classifier performances in DA problems. Consequently, a careful choice of normalisation type, DA method and classification model should be made. To sum up, we can state that when one develops and tests a DA method to classify EEG data, the effect of the normalisation step on the classification performances should be carefully weighed, and a suitable choice of the normalisation method could drastically improve the effectiveness of the DA method or, even avoid the use of DA methods.</p>
<p>Conclusions</p>
<p>In this work, we examined the effect of data normalisation in several DA approaches. Starting from the hypothesis that the prior data normalisation could strongly condition the performances reported by several DA methods, considering the z-score as the base normalisation procedure, we firstly defined four z-score variations. Then we conducted several experiments on different EEG datasets Z0 Z1 Z2 Z3</p>
<p>Domains</p>
<p>Training/Test Labels Table 7: Graphical representation of SEED data on subject-dependent experiments after each type of normalization. On the first row, all domains involved in the series of experimental are marked by different colors; on the second row, training and test data are marked with blue and red, respectively; on the third row, data are distinguished by their label. On each column, data transformed by the relative normalization type are shown.</p>
<p>to analyse the effect of each normalisation strategy applied with and without DA methods. In particular, we dealt with two scenarios typically encountered in EEG classification problems, the subject-independent and subject-dependent cases, where each subject and session can be considered as a different domain due to the non-stationarity of EEG signals.</p>
<p>The results show that the normalisation stage highly impacts classifier performances in several DA scenarios. The best results are achieved by pure ANNs (deep DA) and SVMs (shallow DA) in several cases, combined with an appropriate normalisation schema, without the need for the investigated DA techniques. However, in other cases, the best results are achieved by DA methods combined with a particular type of normalisation, allowing us to consider that searching for the right balance between DA methods and normalisation type could improve classifier performances.</p>
<p>Understanding the impact that the normalisation strategies have on DA approaches could be helpful to improve the performances obtained through DA methods or, in some cases, to avoid DA methods that often turn out to be highly time and hardware-consuming and leading, moreover, to simpler models.</p>
<p>Data availability</p>
<p>The datasets used during the current study are available at:</p>
<p>-SEED: https://bcmi.sjtu.edu.cn/home/seed/ -BCI Competition IV 2a: https://www.bbci.de/competition/iv/ -DEAP: https://www.eecs.qmul.ac.uk/mmv/datasets/deap/</p>
<p>Fig. 1 :
1A graphical representation of the hypothesis explored in this work. Given two domains D 0 and D 1 sharing the same feature and label spaces and affected by a domain shift (left), the application of a z-score normalization could reduce the domain shift between the domains (right) regardless any DA technique.</p>
<p>Fig. 2 :
2Graphical representation of the processing pipeline adopted in this work. After the data partitioning and normalization stages, the impact of the data normalization is inspected on a given ML technique M with and without DA methods (respectively, M DA and M noDA ). Then, performances are evaluated on the testing set through the models m DA and m noDA fitted during the training stages and compared.</p>
<p>Table 1 :
1SEED -Leave-One-Subject-Out Cross-Validation Accuracy, Mean % (Std %)deep DA 
shallow DA 
noDA-ANN 
DANN 
ADDA 
noDA-SVM 
TCA-SVM 
KPCA-SVM 
noNorm 45.50 (13.18) 50.65 (12.19) 33.13 (0.22) 
52.96 (9.82) 
46.68 (13.34) 58.61 (7.50) 
Z0 
48.31 (14.09) 43.60 (11.86) 43.97 (12.86) 52.47 (11.33) 71.58 (7.16) 
48.16 (13.22) 
Z1 
50.54 (15.13) 60.35 (21.45) 46.53 (12.92) 52.32 (15.06) 79.70 (8.98) 
53.59 (16.91) 
Z2 
81.52 (7.26) 
79.03 (7.71) 
70.43 (14.17) 74.71 (8.47) 
80.09 (6.51) 
80.74 (6.11) 
Z3 
75.22 (7.85) 
75.79 (4.78) 
60.57 (13.92) 73.24 (8.37) 
76.37 (7.44) 
73.91 (7.31) </p>
<p>Table 2 :
2SEED -Cross-Session Accuracy, Mean % (Std %) </p>
<p>Deep DA 
Shallow DA 
noDA-ANN 
DANN 
ADDA 
SVM 
TCA-SVM 
KPCA-SVM 
noNorm 45.47 (20.43) 43.15 (17.43) 37.10 (10.83) 63.63 (18.92) 47.52 (12.45) 61.31 (14.33) 
Z0 
64.85 (17.38) 51.76 (17.71) 57.33 (19.52) 62.32 (17.33) 79.56 (10.21) 61.66 (16.88) 
Z1 
66.37 (16.16) 55.25 (19.95) 65.07 (17.50) 63.39 (20.02) 76.96 (12.99) 63.25 (15.92) 
Z2 
83.93 (9.60) 
83.84 (10.55) 76.80 (12.50) 85.59 (9.89) 
81.67 (11.83) 83.62 (9.45) 
Z3 
83.09 (9.98) 
84.43 (9.67) 
77.80 (12.82) 86.56 (8.15) 
83.15 (10.92) 84.09 (10.45) </p>
<p>Table 3 :
3BCI Comp. IV 2a -Leave-One-Subject-Out Cross-Validation Accuracy, Mean % (Std %) </p>
<p>Deep DA 
Shallow DA 
noDA-ANN 
DANN 
ADDA 
noDA-SVM 
TCA-SVM 
KPCA-SVM 
noNorm 61.42 (13.36) 62.19 (13.43) 63.22 (12.90) 61.03 (12.70) 56.10 (10.62) 61.11 (12.62) 
Z1 
62.11 (13.54) 61.73 (13.02) 62.84 (13.77) 61.72 (10.30) 61.03 (11.76) 61.73 (10.93) 
Z2 
67.98 (11.81) 67.52 (11.40) 68.58 (10.96) 68.52 (11.35) 67.90 (11.89) 68.44 (12.02) 
Z3 
63.36 (11.48) 68.13 (13.08) 59.77 (12.80) 67.90 (12.35) 67.44 (13.55) 67.82 (12.48) </p>
<p>For the deep-DA experiments, NoDA-ANN always leads to lower or comparable 
results with DA methods, except for the Z 3 normalization where its perfor-
mance are lower than DANN but higher than ADDA. The best performances 
are attributed to the DANN case on the Z 3 normalization with a mean accu-
racy of 68.13 ± 13.08. However, the use of the only DANN method without any 
normalisation gives an improvement less than 1 %, while the use of the only 
normalisation can lead an improvement of about 6 %, showing that the normal-
isation can have a significant effect on the final performance. 
For the shallow-DA based experiments instead, for each normalization type in-
cluding noN orm, NoDA-SVM always reaches results higher or comparable with 
DA methods. The best performances are attributed to the NoDA-SVM case on </p>
<p>Table 4 :
4BCI Comp. IV 2a -Cross-Session Accuracy, Mean % (Std %) </p>
<p>deep DA 
shallow DA 
noDA-ANN 
DANN 
ADDA 
noDA-SVM 
TCA-SVM 
KPCA-SVM 
noNorm 57.56 (10.47) 55.94 (8.87) 
54.79 (15.46) 55.79 (9.38) 
50.54 (1.52) 
60.88 (13.31) 
Z1 
54.63 (10.32) 56.94 (8.63) 
59.39 (8.87) 
55.48 (9.62) 
55.79 (9.63) 
61.50 (10.04) 
Z2 
64.97 (14.28) 64.12 (15.39) 58.24 (15.71) 63.43 (15.10) 63.66 (15.17) 68.36 (11.47) 
Z3 
63.27 (13.77) 62.27 (13.96) 61.30 (16.64) 67.82 (12.48) 67.43 (13.55) 68.13 (12.22) </p>
<p>Table 5 :
5For the deep-DA experiments, on the Z 0 normalization, NoDA-ANN shows lower results than the DA methods; for the Z 1 and Z 3 normalizations, ANN performances are lower than those of DANN but higher than the ones obtained through ADDA; for the Z 2 normalization, NoDA-ANN results are higher than those of DA methods; for the noN orm, NoDA-ANN results are equal to DANN ones and higher than ADDA ones. The best performances are attributed to the DANN case on the Z 3 normalization with a mean accuracy of 41.27 ± 14.91. Regarding the shallow-DA based experiments, noDA-SVM always leads to lower results than DA methods. The best performances are attributed to the KPCA-SVM case on the Z 3 normalization with a mean accuracy of 43.77±12.68. In this case the DA methods give an important improvement in the performance, further increased by the normalisation methods, showing the importance of using both of them.DEAP -Leave-One-Subject-Out Cross-Validation Accuracy, Mean % (Std %) </p>
<p>deep DA 
shallow DA 
noDA-ANN 
DANN 
ADDA 
noDA-SVM 
TCA-SVM 
KPCA-SVM 
noNorm 34.21 (4.11) 
34.21 (3.15) 
33.93 (2.15) 
31.31 (9.76) 
36.90 (12.34) 41.23 (10.02) 
Z0 
30.44 (10.13) 31.11 (13.38) 34.92 (13.64) 32.56 (1.94) 
41.23 (13.26) 38.33 (9.86) 
Z1 
35.60 (8.72) 
36.51 (7.90) 
34.92 (13.64) 32.55 (10.83) 42.66 (11.70) 34.52 (7.42) 
Z2 
36.67 (12.45) 35.52 (13.84) 32.94 (11.59) 32.13 (14.77) 42.46 (15.99) 40.67 (15.13) 
Z3 
39.33 (14.08) 41.27 (14.91) 38.89 (14.16) 33.73 (14.75) 42.62 (17.06) 43.77 (12.68) </p>
<p>https://compumedicsneuroscan.com/
https://www.biosemi.com
In this work we performed CSP on data using the implementation provided by the Python package MNE[57] </p>
<p>A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges. Christian Mühl, Brendan Allison, Anton Nijholt, Guillaume Chanel, Brain-Computer Interfaces. 12Christian Mühl, Brendan Allison, Anton Nijholt, and Guillaume Chanel. A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges. Brain-Computer Interfaces, 1(2):66-84, 2014.</p>
<p>Review on motor imagery based bci systems for upper limb post-stroke neurorehabilitation: From designing to application. Rig Muhammad Ahmed Khan, Das, K Helle, Sadasivan Iversen, Puthusserypady, Computers in Biology and Medicine. 123103843Muhammad Ahmed Khan, Rig Das, Helle K Iversen, and Sadasivan Puthussery- pady. Review on motor imagery based bci systems for upper limb post-stroke neurorehabilitation: From designing to application. Computers in Biology and Medicine, 123:103843, 2020.</p>
<p>Design, implementation, and metrological characterization of a wearable, integrated ar-bci hands-free system for health 4.0 monitoring. Pasquale Arpaia, Luigi Egidio De Benedetto, Duraccio, Measurement. 177109280Pasquale Arpaia, Egidio De Benedetto, and Luigi Duraccio. Design, implementa- tion, and metrological characterization of a wearable, integrated ar-bci hands-free system for health 4.0 monitoring. Measurement, 177:109280, 2021.</p>
<p>Focus: enhancing children's engagement in reading by using contextual bci training sessions. Jin Huang, Chun Yu, Yuntao Wang, Yuhang Zhao, Siqi Liu, Chou Mo, Jie Liu, Lie Zhang, Yuanchun Shi, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsJin Huang, Chun Yu, Yuntao Wang, Yuhang Zhao, Siqi Liu, Chou Mo, Jie Liu, Lie Zhang, and Yuanchun Shi. Focus: enhancing children's engagement in reading by using contextual bci training sessions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1905-1908, 2014.</p>
<p>How to successfully classify eeg in motor imagery bci: a metrological analysis of the state of the art. Pasquale Arpaia, Antonio Esposito, Angela Natalizio, Marco Parvis, Journal of Neural Engineering. Pasquale Arpaia, Antonio Esposito, Angela Natalizio, and Marco Parvis. How to successfully classify eeg in motor imagery bci: a metrological analysis of the state of the art. Journal of Neural Engineering, 2022.</p>
<p>Eeg-based detection of emotional valence towards a reproducible measurement of emotions. A Apicella, P Arpaia, G Mastrati, N Moccaldi, Scientific Reports. 1112021Apicella A., Arpaia P., Mastrati G., and Moccaldi N. Eeg-based detection of emotional valence towards a reproducible measurement of emotions. Scientific Reports, 11(1), 2021.</p>
<p>Emotion recognition using eeg and physiological data for robot-assisted rehabilitation systems. Elif Gümüslü, Duygun Erol Barkana, Hatice Köse, Companion publication of the 2020 international conference on multimodal interaction. Elif Gümüslü, Duygun Erol Barkana, and Hatice Köse. Emotion recognition using eeg and physiological data for robot-assisted rehabilitation systems. In Companion publication of the 2020 international conference on multimodal interaction, pages 379-387, 2020.</p>
<p>Hierarchical convolutional neural networks for eeg-based emotion recognition. Jinpeng Li, Zhaoxiang Zhang, Huiguang He, Cognitive Computation. 102Jinpeng Li, Zhaoxiang Zhang, and Huiguang He. Hierarchical convolutional neural networks for eeg-based emotion recognition. Cognitive Computation, 10(2):368- 380, 2018.</p>
<p>Generator-based domain adaptation method with knowledge free for cross-subject eeg emotion recognition. Dongmin Huang, Sijin Zhou, Dazhi Jiang, Cognitive Computation. Dongmin Huang, Sijin Zhou, and Dazhi Jiang. Generator-based domain adaptation method with knowledge free for cross-subject eeg emotion recognition. Cognitive Computation, pages 1-12, 2022.</p>
<p>Dataset shift in machine learning. Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, Neil D Lawrence, Mit PressJoaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008.</p>
<p>Domain generalization: A survey. Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, Chen Change Loy, IEEE Transactions on Pattern Analysis and Machine Intelligence. Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.</p>
<p>A review of domain adaptation without target labels. M Wouter, Marco Kouw, Loog, IEEE transactions on pattern analysis and machine intelligence. 43Wouter M Kouw and Marco Loog. A review of domain adaptation without target labels. IEEE transactions on pattern analysis and machine intelligence, 43(3):766- 785, 2019.</p>
<p>A survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Transactions on knowledge and data engineering. 2210Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transac- tions on knowledge and data engineering, 22(10):1345-1359, 2009.</p>
<p>Understanding development process of machine learning systems: Challenges and solutions. Iftekhar Elizamary De Souza Nascimento, Edson Ahmed, Oliveira, Igor Márcio Piedade Palheta, Tayana Steinmacher, Conte, ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM). IEEEElizamary de Souza Nascimento, Iftekhar Ahmed, Edson Oliveira, Márcio Piedade Palheta, Igor Steinmacher, and Tayana Conte. Understanding development pro- cess of machine learning systems: Challenges and solutions. In 2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), pages 1-6. IEEE, 2019.</p>
<p>Unsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor Lempitsky, International conference on machine learning. PMLRYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by back- propagation. In International conference on machine learning, pages 1180-1189. PMLR, 2015.</p>
<p>Transfer components between subjects for eeg-based emotion recognition. Wei-Long Zheng, Yong-Qi Zhang, Jia-Yi Zhu, Bao-Liang Lu, 2015 international conference on affective computing and intelligent interaction (ACII). IEEEWei-Long Zheng, Yong-Qi Zhang, Jia-Yi Zhu, and Bao-Liang Lu. Transfer compo- nents between subjects for eeg-based emotion recognition. In 2015 international conference on affective computing and intelligent interaction (ACII), pages 917- 922. IEEE, 2015.</p>
<p>A fast, efficient domain adaptation technique for cross-domain electroencephalography (eeg)-based emotion recognition. Xin Chai, Qisong Wang, Yongping Zhao, Yongqiang Li, Dan Liu, Xin Liu, Ou Bai, Sensors. 1751014Xin Chai, Qisong Wang, Yongping Zhao, Yongqiang Li, Dan Liu, Xin Liu, and Ou Bai. A fast, efficient domain adaptation technique for cross-domain electroen- cephalography (eeg)-based emotion recognition. Sensors, 17(5):1014, 2017.</p>
<p>Investigating the impact of data normalization on classification performance. Dalwinder Singh, Birmohan Singh, Applied Soft Computing. 97105524Dalwinder Singh and Birmohan Singh. Investigating the impact of data normal- ization on classification performance. Applied Soft Computing, 97:105524, 2020.</p>
<p>Decoding premovement patterns with task-related component analysis. Feng Duan, Hao Jia, Zhe Sun, Kai Zhang, Yangyang Dai, Yu Zhang, Cognitive Computation. 135Feng Duan, Hao Jia, Zhe Sun, Kai Zhang, Yangyang Dai, and Yu Zhang. Decoding premovement patterns with task-related component analysis. Cognitive Computa- tion, 13(5):1389-1405, 2021.</p>
<p>Personal-zscore: Eliminating individual difference for eeg-based cross-subject emotion recognition. Huayu Chen, Shuting Sun, Jianxiu Li, Ruilan Yu, Nan Li, Xiaowei Li, Bin Hu, IEEE Transactions on Affective Computing. Huayu Chen, Shuting Sun, Jianxiu Li, Ruilan Yu, Nan Li, Xiaowei Li, and Bin Hu. Personal-zscore: Eliminating individual difference for eeg-based cross-subject emotion recognition. IEEE Transactions on Affective Computing, 2021.</p>
<p>Eeg-based measurement system for monitoring student engagement in learning 4. Andrea Apicella, Pasquale Arpaia, Mirco Frosolone, Giovanni Improta, Nicola Moccaldi, Andrea Pollastro, 0. Scientific Reports. 121Andrea Apicella, Pasquale Arpaia, Mirco Frosolone, Giovanni Improta, Nicola Moccaldi, and Andrea Pollastro. Eeg-based measurement system for monitoring student engagement in learning 4.0. Scientific Reports, 12(1):1-13, 2022.</p>
<p>Cross-subject eeg-based emotion recognition through neural networks with stratified normalization. Javier Fernandez, Nicholas Guttenberg, Olaf Witkowski, Antoine Pasquali, Frontiers in neuroscience. 1511Javier Fernandez, Nicholas Guttenberg, Olaf Witkowski, and Antoine Pasquali. Cross-subject eeg-based emotion recognition through neural networks with strati- fied normalization. Frontiers in neuroscience, 15:11, 2021.</p>
<p>Eeg-based affect and workload recognition in a virtual driving environment for asd intervention. Jing Fan, Joshua W Wade, Alexandra P Key, Zachary E Warren, Nilanjan Sarkar, IEEE Transactions on Biomedical Engineering. 651Jing Fan, Joshua W Wade, Alexandra P Key, Zachary E Warren, and Nilanjan Sarkar. Eeg-based affect and workload recognition in a virtual driving environment for asd intervention. IEEE Transactions on Biomedical Engineering, 65(1):43-51, 2017.</p>
<p>Combining inter-subject modeling with a subject-based data transformation to improve affect recognition from eeg signals. Miguel Arevalillo-Herráez, Maximo Cobos, Sandra Roger, Miguel García-Pineda, Sensors. 19132999Miguel Arevalillo-Herráez, Maximo Cobos, Sandra Roger, and Miguel García- Pineda. Combining inter-subject modeling with a subject-based data transfor- mation to improve affect recognition from eeg signals. Sensors, 19(13):2999, 2019.</p>
<p>Learning subject-generalized topographical eeg embeddings using deep variational autoencoders and domain-adversarial regularization. Juan Lorenzo Hagad, Tsukasa Kimura, Ken-Ichi Fukui, Masayuki Numao, Sensors. 2151792Juan Lorenzo Hagad, Tsukasa Kimura, Ken-ichi Fukui, and Masayuki Numao. Learning subject-generalized topographical eeg embeddings using deep variational autoencoders and domain-adversarial regularization. Sensors, 21(5):1792, 2021.</p>
<p>Domain adaptation via transfer component analysis. Ivor W Sinno Jialin Pan, James T Tsang, Qiang Kwok, Yang, IEEE transactions on neural networks. 222Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adap- tation via transfer component analysis. IEEE transactions on neural networks, 22(2):199-210, 2010.</p>
<p>A kernel method for the two-sample-problem. Advances in neural information processing systems. Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, Alex Smola, 19Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alex Smola. A kernel method for the two-sample-problem. Advances in neural infor- mation processing systems, 19, 2006.</p>
<p>Kernel principal component analysis. Bernhard Schölkopf, Alexander Smola, Klaus-Robert Müller, International conference on artificial neural networks. SpringerBernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Kernel princi- pal component analysis. In International conference on artificial neural networks, pages 583-588. Springer, 1997.</p>
<p>Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, arXiv:1412.4446Domain-adversarial neural networks. arXiv preprintHana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014.</p>
<p>Domainadversarial training of neural networks. The journal of machine learning research. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor Lempitsky, 17Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain- adversarial training of neural networks. The journal of machine learning research, 17(1):2096-2030, 2016.</p>
<p>Adversarial discriminative domain adaptation. Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discrim- inative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167-7176, 2017.</p>
<p>Two-level domain adaptation neural network for eegbased emotion recognition. Guangcheng Bao, Ning Zhuang, Li Tong, Bin Yan, Jun Shu, Linyuan Wang, Ying Zeng, Zhichong Shen, Frontiers in Human Neuroscience. 14Guangcheng Bao, Ning Zhuang, Li Tong, Bin Yan, Jun Shu, Linyuan Wang, Ying Zeng, and Zhichong Shen. Two-level domain adaptation neural network for eeg- based emotion recognition. Frontiers in Human Neuroscience, 14, 2020.</p>
<p>From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition. Yang Li, Wenming Zheng, Lei Wang, Yuan Zong, Zhen Cui, IEEE Transactions on Affective Computing. Yang Li, Wenming Zheng, Lei Wang, Yuan Zong, and Zhen Cui. From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition. IEEE Transactions on Affective Computing, 2019.</p>
<p>Ms-mda: Multisource marginal distribution adaptation for cross-subject and crosssession eeg emotion recognition. Hao Chen, Ming Jin, Zhunan Li, Cunhang Fan, Jinpeng Li, Huiguang He, Frontiers in Neuroscience. 152021Hao Chen, Ming Jin, Zhunan Li, Cunhang Fan, Jinpeng Li, and Huiguang He. Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross- session eeg emotion recognition. Frontiers in Neuroscience, 15, 2021.</p>
<p>Stationarity of the eeg series. S Blanco, Garcia, L R Quian Quiroga, O A Romanelli, Rosso, IEEE Engineering in medicine and biology Magazine. 144S Blanco, H Garcia, R Quian Quiroga, L Romanelli, and OA Rosso. Stationarity of the eeg series. IEEE Engineering in medicine and biology Magazine, 14(4):395-399, 1995.</p>
<p>A review on transfer learning approaches in brain-computer interface. Signal Processing and Machine Learning for Brain-Machine Interfaces. Jake Ahmed M Azab, Toth, S Lyudmila, Mahnaz Mihaylova, Arvaneh, Ahmed M Azab, Jake Toth, Lyudmila S Mihaylova, and Mahnaz Arvaneh. A review on transfer learning approaches in brain-computer interface. Signal Processing and Machine Learning for Brain-Machine Interfaces, pages 81-98, 2018.</p>
<p>Riemannian approaches in braincomputer interfaces: a review. Florian Yger, Maxime Berar, Fabien Lotte, IEEE Transactions on Neural Systems and Rehabilitation Engineering. 25Florian Yger, Maxime Berar, and Fabien Lotte. Riemannian approaches in brain- computer interfaces: a review. IEEE Transactions on Neural Systems and Reha- bilitation Engineering, 25(10):1753-1762, 2016.</p>
<p>Dynamically weighted ensemble classification for nonstationary eeg processing. Cuntai Sidath Ravindra Liyanage, Haihong Guan, Kai Keng Zhang, Jianxin Ang, Tong Heng Xu, Lee, Journal of neural engineering. 10336007Sidath Ravindra Liyanage, Cuntai Guan, Haihong Zhang, Kai Keng Ang, JianXin Xu, and Tong Heng Lee. Dynamically weighted ensemble classification for non- stationary eeg processing. Journal of neural engineering, 10(3):036007, 2013.</p>
<p>Individual differences in eeg theta and alpha dynamics during working memory correlate with fmri responses across subjects. A Jed, Michiro Meltzer, Negishi, C Linda, R Todd Mayes, Constable, Clinical neurophysiology. 11811Jed A Meltzer, Michiro Negishi, Linda C Mayes, and R Todd Constable. Individual differences in eeg theta and alpha dynamics during working memory correlate with fmri responses across subjects. Clinical neurophysiology, 118(11):2419-2436, 2007.</p>
<p>Domain adaptation techniques for eeg-based emotion recognition: a comparative study on two public datasets. Zirui Lan, Olga Sourina, Lipo Wang, Reinhold Scherer, Gernot R Müller-Putz , IEEE Transactions on Cognitive and Developmental Systems. 111Zirui Lan, Olga Sourina, Lipo Wang, Reinhold Scherer, and Gernot R Müller-Putz. Domain adaptation techniques for eeg-based emotion recognition: a comparative study on two public datasets. IEEE Transactions on Cognitive and Developmental Systems, 11(1):85-94, 2018.</p>
<p>Domain adaptation for eeg emotion recognition based on latent representation similarity. Jinpeng Li, Shuang Qiu, Changde Du, Yixin Wang, Huiguang He, IEEE Transactions on Cognitive and Developmental Systems. 122Jinpeng Li, Shuang Qiu, Changde Du, Yixin Wang, and Huiguang He. Domain adaptation for eeg emotion recognition based on latent representation similarity. IEEE Transactions on Cognitive and Developmental Systems, 12(2):344-353, 2019.</p>
<p>Deep representation-based domain adaptation for nonstationary eeg classification. He Zhao, Qingqing Zheng, Kai Ma, Huiqi Li, Yefeng Zheng, IEEE Transactions on Neural Networks and Learning Systems. 322He Zhao, Qingqing Zheng, Kai Ma, Huiqi Li, and Yefeng Zheng. Deep representation-based domain adaptation for nonstationary eeg classification. IEEE Transactions on Neural Networks and Learning Systems, 32(2):535-545, 2020.</p>
<p>Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks. Wei-Long Zheng, Bao-Liang Lu, IEEE Transactions on Autonomous Mental Development. 73Wei-Long Zheng and Bao-Liang Lu. Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks. IEEE Transactions on Autonomous Mental Development, 7(3):162-175, 2015.</p>
<p>. IEEE Transactions on Neural Networks and Learning Systems. 322IEEE Transactions on Neural Networks and Learning Systems, 32(2):535-545, 2020.</p>
<p>Deap: A database for emotion analysis; using physiological signals. Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, IEEE transactions on affective computing. 31Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological signals. IEEE transactions on affective computing, 3(1):18-31, 2011.</p>
<p>Nonlinear component analysis as a kernel eigenvalue problem. Bernhard Schölkopf, Alexander Smola, Klaus-Robert Müller, Neural computation. 105Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Nonlinear com- ponent analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299- 1319, 1998.</p>
<p>What is a support vector machine?. S William, Noble, Nature biotechnology. 2412William S Noble. What is a support vector machine? Nature biotechnology, 24(12):1565-1567, 2006.</p>
<p>Practical bayesian optimization of machine learning algorithms. Jasper Snoek, Hugo Larochelle, Ryan P Adams, Advances in neural information processing systems. 25Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimiza- tion of machine learning algorithms. Advances in neural information processing systems, 25, 2012.</p>
<p>Stratified sampling. Van L Parsons, Wiley StatsRef: Statistics Reference Online. Van L Parsons. Stratified sampling. Wiley StatsRef: Statistics Reference Online, pages 1-11, 2014.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Reducing the subject variability of eeg signals with adversarial domain generalization. Bo-Qun Ma, He Li, Wei-Long Zheng, Bao-Liang Lu, International Conference on Neural Information Processing. SpringerBo-Qun Ma, He Li, Wei-Long Zheng, and Bao-Liang Lu. Reducing the subject variability of eeg signals with adversarial domain generalization. In International Conference on Neural Information Processing, pages 30-42. Springer, 2019.</p>
<p>Reducing execution time for real-time motor imagery based bci systems. Sahar Selim, Manal Tantawi, Howida Shedeed, Amr Badr, International Conference on Advanced Intelligent Systems and Informatics. SpringerSahar Selim, Manal Tantawi, Howida Shedeed, and Amr Badr. Reducing execution time for real-time motor imagery based bci systems. In International Conference on Advanced Intelligent Systems and Informatics, pages 555-565. Springer, 2016.</p>
<p>Time window and frequency band optimization using regularized neighbourhood component analysis for multi-view motor imagery eeg classification. Nitesh Singh Malan, Shiru Sharma, Biomedical Signal Processing and Control. 67102550Nitesh Singh Malan and Shiru Sharma. Time window and frequency band opti- mization using regularized neighbourhood component analysis for multi-view mo- tor imagery eeg classification. Biomedical Signal Processing and Control, 67:102550, 2021.</p>
<p>Complex common spatial patterns on time-frequency decomposed eeg for brain-computer interface. Vasilisa Mishuhina, Xudong Jiang, Pattern Recognition. 115107918Vasilisa Mishuhina and Xudong Jiang. Complex common spatial patterns on time-frequency decomposed eeg for brain-computer interface. Pattern Recognition, 115:107918, 2021.</p>
<p>Optimizing spatial filters for robust eeg single-trial analysis. Benjamin Blankertz, Ryota Tomioka, Steven Lemm, Motoaki Kawanabe, Klaus-Robert Muller, IEEE Signal processing magazine. 251Benjamin Blankertz, Ryota Tomioka, Steven Lemm, Motoaki Kawanabe, and Klaus-Robert Muller. Optimizing spatial filters for robust eeg single-trial anal- ysis. IEEE Signal processing magazine, 25(1):41-56, 2007.</p>
<p>Eeg-based brain-computer interfaces using motor-imagery: Techniques and challenges. Natasha Padfield, Jaime Zabalza, Huimin Zhao, Valentin Masero, Jinchang Ren, Sensors. 1961423Natasha Padfield, Jaime Zabalza, Huimin Zhao, Valentin Masero, and Jinchang Ren. Eeg-based brain-computer interfaces using motor-imagery: Techniques and challenges. Sensors, 19(6):1423, 2019.</p>
<p>MEG and EEG data analysis with MNE-Python. Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A Engemann, Daniel Strohmeier, Christian Brodbeck, Roman Goj, Mainak Jas, Teon Brooks, Lauri Parkkonen, Matti S Hämäläinen, Frontiers in Neuroscience. 7267Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A. Engemann, Daniel Strohmeier, Christian Brodbeck, Roman Goj, Mainak Jas, Teon Brooks, Lauri Parkkonen, and Matti S. Hämäläinen. MEG and EEG data analysis with MNE- Python. Frontiers in Neuroscience, 7(267):1-13, 2013.</p>
<p>Journal of machine learning research. Laurens Van Der Maaten, Geoffrey Hinton, 9Visualizing data using t-sneLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Jour- nal of machine learning research, 9(11), 2008.</p>            </div>
        </div>

    </div>
</body>
</html>
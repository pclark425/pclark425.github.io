<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6389 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6389</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6389</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-270094960</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.19327v4.pdf" target="_blank">MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6389.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6389.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAP-Neo-7B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAP-Neo 7-billion-parameter base model (pre-alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The MAP-Neo 7B decoder-only transformer trained from scratch on the Matrix Data Pile (≈4.5T tokens) with an increased share of code and high-quality academic/mathematical texts; evaluated on standard math benchmarks GSM8K and MATH using few-shot Chain-of-Thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAP-Neo-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Matrix Data Pile (bilingual English + Chinese) ≈4.5T tokens, mixture includes ~52.6% Common Crawl, ~22.3% programming code, plus academic papers, books, and curated high-quality math and reasoning subsets; dedicated 'decay' phase increased code and high-quality instruction-like data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems (GSM8K) and formal/math contest problems (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems / symbolic expressions (free-response), evaluated via MAmmoTH (parsing/generation) for MATH and GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school multi-step arithmetic; MATH: higher-difficulty contest-level math problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot (4-shot) evaluation; for GSM8K they used a simple Chain-of-Thought prompt: "Let's think step by step." (paper explicitly states CoT for GSM8K and 4-shot for both GSM8K and MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task accuracy / problem-solution correctness (percentage) as reported by benchmark frameworks (MAmmoTH / GSM8K/MATH evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 53.68% (reported in Table 9 for MAP-Neo-7B; evaluated with 4-shot + CoT prompting); MATH: 20.7% (reported in Table 9, 4-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper reports architecture & training decisions relevant to numeric reasoning (e.g., tokenizer design that slices numbers into individual digits), and analyzes effects of training data quality and scale (NEO scaling law). There is no dedicated mechanistic probe (e.g., activation/attention probing or logit‑lens) of arithmetic computation reported; the internal discussion links improved math performance primarily to higher-quality math/code corpora and additional decay-phase fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>The paper does not document detailed arithmetic-specific failure-mode probes (e.g., off-by-one) but notes general weaknesses remain on hard math (low MATH scores) and that pretraining/tokenizer bugs affected downstream capabilities (a tokenizer whitespace bug caused degraded code performance). The authors emphasize data quality and tokenization as contributors to failures.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance on math improves with more and higher-quality training data and targeted decay-phase training (increase in code/math/instructional data); the authors introduce the NEO scaling law (adds a -d·log(D) term) arguing diverse high-quality corpora produce faster loss reduction with very large D than predicted by Chinchilla; empirically MAP-Neo benefits from large D and curated math/code content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6389.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6389.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAP-Neo tokenizer numeric handling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAP-Neo SentencePiece BPE tokenizer with digit-level slicing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tokenizer used for MAP-Neo: SentencePiece BPE with vocab size 64k, constrained piece length, and a deliberate policy to slice all numbers into individual digits (and fall back unknown UTF-8 to bytes), intended to improve handling of code, math and Chinese.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAP-Neo tokenizer (used with MAP-Neo-2B and -7B models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>SentencePiece BPE tokenization (subword) with custom settings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>vocabulary size 64k (tokenizer parameter)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Tokenizer trained on 50B samples drawn from the pre-training corpus with higher sampling weights for code, math, and academic data (max sentence-piece length = 16), multilingual (English + Chinese) mixing.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Indirectly affects performance on GSM8K/MATH and code benchmarks (HumanEval etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>numeric tokenization for downstream arithmetic and code generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>applies to numeric tokens inside natural-language and code prompts (affects tokenization granularity of numbers and digits)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>affects multi-digit and symbolic numeric expressions and code numeric tokens</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>not a prompting method; tokenizer design interacts with prompting and downstream CoT prompts by determining token granularity for numbers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not reported as a standalone metric; impact measured indirectly via downstream tasks (code metrics and math benchmarks) and longitudinal checkpoint evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not reported as a direct numeric metric; authors report that fixing a whitespace-removal bug in tokenizer settings materially improved code metrics and stabilized training; digit-splitting was explicitly chosen (no numeric performance number attached).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors argue tokenization choices (digit-splitting, disabling extra-whitespace removal) materially affected downstream code performance and stability during pre-training; they observe different compression rates across data categories and languages and emphasize tokenizer settings influence math/code competence.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>A prior tokenizer bug (remove extra whitespaces enabled) caused unstable and poor code metrics during pre-training until corrected; digit-level tokenization can change sequence length and may affect positional encoding/attention costs but no direct numeric overflow or arithmetic-specific failure modes are analyzed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Tokenizer fixes and more targeted high-quality data in the decay phase improved code and math-related performance; authors attribute part of math/code gains to improved tokenization paired with additional high-quality training data and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6389.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6389.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting: Chain-of-Thought (CoT) usage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) few-shot prompting (explicit 'Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For GSM8K the authors used a simple Chain-of-Thought prompt template — the canonical 'Let's think step by step.' — in a 4-shot evaluation configuration to elicit intermediate reasoning and improve arithmetic problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAP-Neo-7B (evaluation context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (applies to the evaluated model)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Evaluation-time prompting applied to models pre-trained on Matrix Data Pile; no further training alterations tied to CoT at inference time beyond instruction/data in SFT phases.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (explicit)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems requiring chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems presented in few-shot exemplars followed by the test question</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step arithmetic (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot Chain-of-Thought (explicit trigger phrase 'Let's think step by step.'), 4-shot evaluation stated in paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on GSM8K (percentage) reported in Table 9</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 53.68% for MAP-Neo-7B with 4-shot + CoT prompting (Table 9)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper reports use of CoT prompt for GSM8K; no mechanistic analysis of how CoT changes internal activations or representations is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No CoT-specific failure analysis provided (e.g., hallucinated chains, mistaken arithmetic in intermediary steps) beyond the overall task error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>CoT is applied consistently across models; paper links overall better math performance to data quality and model/data scale rather than probing CoT-specific scaling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseekmath: Pushing the limits of mathematical reasoning in open language models <em>(Rating: 2)</em></li>
                <li>TheoremQA: A theorem-driven question answering dataset <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6389",
    "paper_id": "paper-270094960",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "MAP-Neo-7B (base)",
            "name_full": "MAP-Neo 7-billion-parameter base model (pre-alignment)",
            "brief_description": "The MAP-Neo 7B decoder-only transformer trained from scratch on the Matrix Data Pile (≈4.5T tokens) with an increased share of code and high-quality academic/mathematical texts; evaluated on standard math benchmarks GSM8K and MATH using few-shot Chain-of-Thought prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MAP-Neo-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Matrix Data Pile (bilingual English + Chinese) ≈4.5T tokens, mixture includes ~52.6% Common Crawl, ~22.3% programming code, plus academic papers, books, and curated high-quality math and reasoning subsets; dedicated 'decay' phase increased code and high-quality instruction-like data.",
            "benchmark_name": "GSM8K; MATH",
            "task_type": "multi-step arithmetic word problems (GSM8K) and formal/math contest problems (MATH)",
            "problem_format": "natural-language math word problems / symbolic expressions (free-response), evaluated via MAmmoTH (parsing/generation) for MATH and GSM8K",
            "difficulty_level": "GSM8K: grade-school multi-step arithmetic; MATH: higher-difficulty contest-level math problems",
            "prompting_method": "Few-shot (4-shot) evaluation; for GSM8K they used a simple Chain-of-Thought prompt: \"Let's think step by step.\" (paper explicitly states CoT for GSM8K and 4-shot for both GSM8K and MATH)",
            "performance_metric": "Task accuracy / problem-solution correctness (percentage) as reported by benchmark frameworks (MAmmoTH / GSM8K/MATH evaluation)",
            "performance_value": "GSM8K: 53.68% (reported in Table 9 for MAP-Neo-7B; evaluated with 4-shot + CoT prompting); MATH: 20.7% (reported in Table 9, 4-shot)",
            "internal_analysis": "Paper reports architecture & training decisions relevant to numeric reasoning (e.g., tokenizer design that slices numbers into individual digits), and analyzes effects of training data quality and scale (NEO scaling law). There is no dedicated mechanistic probe (e.g., activation/attention probing or logit‑lens) of arithmetic computation reported; the internal discussion links improved math performance primarily to higher-quality math/code corpora and additional decay-phase fine-tuning.",
            "failure_modes": "The paper does not document detailed arithmetic-specific failure-mode probes (e.g., off-by-one) but notes general weaknesses remain on hard math (low MATH scores) and that pretraining/tokenizer bugs affected downstream capabilities (a tokenizer whitespace bug caused degraded code performance). The authors emphasize data quality and tokenization as contributors to failures.",
            "scaling_trend": "Performance on math improves with more and higher-quality training data and targeted decay-phase training (increase in code/math/instructional data); the authors introduce the NEO scaling law (adds a -d·log(D) term) arguing diverse high-quality corpora produce faster loss reduction with very large D than predicted by Chinchilla; empirically MAP-Neo benefits from large D and curated math/code content.",
            "uuid": "e6389.0",
            "source_info": {
                "paper_title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MAP-Neo tokenizer numeric handling",
            "name_full": "MAP-Neo SentencePiece BPE tokenizer with digit-level slicing",
            "brief_description": "Tokenizer used for MAP-Neo: SentencePiece BPE with vocab size 64k, constrained piece length, and a deliberate policy to slice all numbers into individual digits (and fall back unknown UTF-8 to bytes), intended to improve handling of code, math and Chinese.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MAP-Neo tokenizer (used with MAP-Neo-2B and -7B models)",
            "model_family": "SentencePiece BPE tokenization (subword) with custom settings",
            "model_size": "vocabulary size 64k (tokenizer parameter)",
            "training_data_description": "Tokenizer trained on 50B samples drawn from the pre-training corpus with higher sampling weights for code, math, and academic data (max sentence-piece length = 16), multilingual (English + Chinese) mixing.",
            "benchmark_name": "Indirectly affects performance on GSM8K/MATH and code benchmarks (HumanEval etc.)",
            "task_type": "numeric tokenization for downstream arithmetic and code generation tasks",
            "problem_format": "applies to numeric tokens inside natural-language and code prompts (affects tokenization granularity of numbers and digits)",
            "difficulty_level": "affects multi-digit and symbolic numeric expressions and code numeric tokens",
            "prompting_method": "not a prompting method; tokenizer design interacts with prompting and downstream CoT prompts by determining token granularity for numbers",
            "performance_metric": "Not reported as a standalone metric; impact measured indirectly via downstream tasks (code metrics and math benchmarks) and longitudinal checkpoint evaluations",
            "performance_value": "Not reported as a direct numeric metric; authors report that fixing a whitespace-removal bug in tokenizer settings materially improved code metrics and stabilized training; digit-splitting was explicitly chosen (no numeric performance number attached).",
            "internal_analysis": "Authors argue tokenization choices (digit-splitting, disabling extra-whitespace removal) materially affected downstream code performance and stability during pre-training; they observe different compression rates across data categories and languages and emphasize tokenizer settings influence math/code competence.",
            "failure_modes": "A prior tokenizer bug (remove extra whitespaces enabled) caused unstable and poor code metrics during pre-training until corrected; digit-level tokenization can change sequence length and may affect positional encoding/attention costs but no direct numeric overflow or arithmetic-specific failure modes are analyzed in the paper.",
            "scaling_trend": "Tokenizer fixes and more targeted high-quality data in the decay phase improved code and math-related performance; authors attribute part of math/code gains to improved tokenization paired with additional high-quality training data and fine-tuning.",
            "uuid": "e6389.1",
            "source_info": {
                "paper_title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Prompting: Chain-of-Thought (CoT) usage",
            "name_full": "Chain-of-Thought (CoT) few-shot prompting (explicit 'Let's think step by step')",
            "brief_description": "For GSM8K the authors used a simple Chain-of-Thought prompt template — the canonical 'Let's think step by step.' — in a 4-shot evaluation configuration to elicit intermediate reasoning and improve arithmetic problem solving.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MAP-Neo-7B (evaluation context)",
            "model_family": "decoder-only transformer",
            "model_size": "7B (applies to the evaluated model)",
            "training_data_description": "Evaluation-time prompting applied to models pre-trained on Matrix Data Pile; no further training alterations tied to CoT at inference time beyond instruction/data in SFT phases.",
            "benchmark_name": "GSM8K (explicit)",
            "task_type": "multi-step arithmetic word problems requiring chain-of-thought reasoning",
            "problem_format": "natural-language word problems presented in few-shot exemplars followed by the test question",
            "difficulty_level": "grade-school multi-step arithmetic (GSM8K)",
            "prompting_method": "Few-shot Chain-of-Thought (explicit trigger phrase 'Let's think step by step.'), 4-shot evaluation stated in paper",
            "performance_metric": "accuracy on GSM8K (percentage) reported in Table 9",
            "performance_value": "GSM8K: 53.68% for MAP-Neo-7B with 4-shot + CoT prompting (Table 9)",
            "internal_analysis": "Paper reports use of CoT prompt for GSM8K; no mechanistic analysis of how CoT changes internal activations or representations is provided.",
            "failure_modes": "No CoT-specific failure analysis provided (e.g., hallucinated chains, mistaken arithmetic in intermediary steps) beyond the overall task error rates.",
            "scaling_trend": "CoT is applied consistently across models; paper links overall better math performance to data quality and model/data scale rather than probing CoT-specific scaling behavior.",
            "uuid": "e6389.2",
            "source_info": {
                "paper_title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "rating": 2,
            "sanitized_title": "deepseekmath_pushing_the_limits_of_mathematical_reasoning_in_open_language_models"
        },
        {
            "paper_title": "TheoremQA: A theorem-driven question answering dataset",
            "rating": 2,
            "sanitized_title": "theoremqa_a_theoremdriven_question_answering_dataset"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        }
    ],
    "cost": 0.01916875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</p>
<p>M-A-P
University of Waterloo
Wuhan AI Research
01AI</p>
<p>MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series
0727DD913466529A13F65C63BD199C6D
Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks.However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details.Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs.However, only the model's weights are provided with most details undisclosed (e.g., intermediate checkpoints, pre-training corpus, and training code, etc).To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided.These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks.However, we observe that the existing truly open LLMs are still inferior to existing state-of-the-art LLMs with similar model sizes on reasoning, knowledge, and coding tasks.To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens.Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs.Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework 1 are provided.Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.</p>
<p>Introduction</p>
<p>The advent of generalist large language models (LLMs) such as GPT-4 [1], Claude [4], and Gemini [80] has significantly expanded the boundaries of Natural Language Processing (NLP) and is paving the way towards Artificial General Intelligence (AGI).These models exhibit universal capabilities, including complex reasoning [116,89], role-playing [107], creative writing [105], psychological assessment [112], scientific education [18], and music generation [115,75,29], among others.However, the most advanced ones remain closed-source due to commercial interests [1,4,80].In this paper, we argue that open-source and transparent LLMs are essential for both the democratization of LLMs and further academic research, especially considering the substantial resources these models consume.</p>
<p>Previous works have released numerous open-source or even transparent LLMs.For example, the LLaMA series [101,102,3] released the weights, thereby significantly boosting the development of the open-source LLM community.However, they are not transparent because they do not disclose the details of their training data.BLOOM [86] trained a multilingual language model with 176 billion parameters and open-sourced its model weights, intermediate checkpoints, and training corpus.Models like LLM360 [66] and Pythia [9] further provided their training codes, optimizer state checkpoints, analysis codes, and data pipelines.</p>
<p>These models make significant contributions to building transparent ecosystems, yet generally lag behind industry-level LLMs such as LLaMA [3], Mistral [48] and Yi [113], etc. OLMo [36] has made a great stride in narrowing this gap by improving pre-training data and data processing pipelines, and introducing more open-source components, including training logs and ablations.Nonetheless, it remains less proficient, especially in areas like coding (HumanEval [15]), reasoning (MATH [41], GSM8K [23]), knowledge (MMLU [40]), and multilingualism (CMMLU [60]).</p>
<p>To remedy these issues, we introduce MAP-Neo, a fully open-source and transparent bilingual LLM suite that achieves superior performance to close the gap with closed-source models.Specifically, the entire workflow of building an LLM includes:</p>
<ol>
<li>
<p>Data Curation Pipeline: We provide the code for the curation and cleaning of training data (both English and Chinese), including a stable OCR system, the data recalling mechanism in DeepSeek-Math [89], the integration of previous open-source data processing pipelines, and support for distributed data processing based on Spark2 , among others.</p>
</li>
<li>
<p>Data: We release our pre-training corpus, namely Matrix Data Pile, along with the training data for supervised fine-tuning and alignment training.</p>
</li>
<li>
<p>Model Architecture: We provide the codes and details of our modeling architecture.</p>
</li>
</ol>
<p>Model Training:</p>
<p>We offer the training codes for our tokenizer, base models, instructiontuned models, and aligned models.Additionally, we address some issues of the Megatron-LM framework 3 , enhancing its support for more robust and efficient distributed training.Moreover, we introduce the NEO Scaling Law designed to optimize scaling up LLMs using a pre-training dataset sourced from diverse corpora.</p>
<p>Model Checkpoints:</p>
<p>We not only release the final models on HuggingFace but also make the intermediate checkpoints available for reproducibility.</p>
<ol>
<li>
<p>Infrastructure: This report details the infrastructure for stable training.</p>
</li>
<li>
<p>Evaluation: We also provide detailed evaluation codes and thorough evaluation settings for benchmarking the performance of LLMs.</p>
</li>
<li>
<p>Analysis and Lessons: This report elaborates on numerous techniques and recipes, such as optimization tricks at different phases of pre-training, and offers insights into building LLMs through rigorous analysis and ablations.</p>
</li>
</ol>
<p>Our work is a milestone towards fully transparent LLMs with advanced abilities, even competitive with the top closed-source LLMs.Notably, our contribution is not just a novel foundational model but also a comprehensive handbook for building LLMs from scratch, covering the entire workflow.We believe that our model provides a critical reference for the community, particularly for non-English regions of the world engaged in LLM research.</p>
<p>Related Works</p>
<p>Table 1: Compare with other open-source large language models (LLMs).All metrics are obtained using the same evaluation manner, and the details are shown in Table 9. Non-transparent models are listed above the dashed line, while the transparent LLMs are shown below.The development of open-source large language models (LLMs) is pivotal for advancing artificial intelligence research and applications.Recent efforts in this domain have been focused on not only enhancing model performance [48,3] but also ensuring transparency and reproducibility [9,66,36,128].Our model, MAP-Neo-7B, emerges as the new lead in this evolving landscape, as shown in Table 1, which balances performance and transparency.</p>
<p>The MAP-Neo model series represents a step forward in emphasizing full transparency, aligning it alongside other contemporary models such as Mistral [48], LLaMA3 [3], Pythia [9], Amber [66], and OLMo [36].Unlike these models, which often lack either intermediate checkpoints, comprehensive data cleaning processes, or accessible pre-training corpus and reproduction code, MAP-Neo excels by integrating all these elements.This commitment to the openness of MAP-Neo facilitates in-depth analysis and independent validation by the research community.</p>
<p>Performance-wise, MAP-Neo-7B demonstrates superior capabilities across a broad scope of benchmarks including Chinese and English understanding on C-EVAL [46] and MMLU [20], mathematical ability on GSM8K [23] and MATH [41], and code ability on HumanEval [15].Notably, MAP-Neo-7B is the only model in our comparative analysis to achieve all checks in transparency, as well as the highest scores across all tests compared with other transparent LLMs, underscoring the effectiveness of the training and the quality of the data.</p>
<p>The most similar work to MAP-Neo is OLMo [36], which is the pioneering work to fully open-source LLMs.However, their performance is compromised in several aspects like knowledge, coding, and mathematical reasoning.Moreover, OLMo cannot handle languages beyond English.MAP-Neo sets a new standard for transparency and performance in the field of open-source LLMs.By fostering a fully transparent development process, MAP-Neo not only enhances its utility and trustworthiness but also provides a valuable framework for future research, promoting further advancements and collaborative efforts in the community.</p>
<p>Tokenizer</p>
<p>We train our tokenizer using the byte-pair encoding (BPE) algorithm [88] via the implementation of SentencePiece [56].The training data consists of 50B samples from the pre-training corpus, and the maximum length is cut to 64K.We assign higher sampling weights to code, math, and high-quality academic data.To balance the computational efficiency and model performance, we propose to set the vocabulary size to 64000 and constrain the max sentence-piece length to 16 to improve the Chinese performance.</p>
<p>Notably, we slice all numbers into individual digits and fall back unknown UTF-8 characters to byte granularity.We do not use any normalization strategy on the training samples and do not add dummy prefixes.The character coverage rate is set to 0.9999.Particularly, the remove extra whitespaces parameter is set to False, which is turned on by default in the SentencePieceTrainer.This setting can severely impact code performance during pre-training, as normal code indentation is treated as a single space.We encountered a specific issue during the initial phase of our model's pre-training.Initially, we did not disable the 'remove extra whitespaces' parameter, which is enabled by default in the SentencePieceTrainer.In the training process, we observe steady improvements in the QA reasoning and mathematics benchmarks, but the code metrics exhibit fluctuations and do not show expected improvements.To address this issue, we fixed this bug in the second phase of our training ( §6.2), which stabilizes and significantly improves the code metrics.Furthermore, we observe that this issue is well addressed in the decay phase training stages under the new tokenizer settings, where rapid improvements are achieved.</p>
<p>Moreover, we also investigate the compression rates across various categories of data, categorized by both language (Chinese and English) and data source quality (high-quality and web-sourced) as shown in Table 2. Specifically, first, we observe that the high-quality data (HQ) including complex reasoning, mathematical, and general knowledge texts, showing different compression rates between Chinese (HQ cn) and English (HQ en).The HQ cn category has a compression rate of 1.577, while the HQ en category exhibited a higher rate of 3.311 characters per token.Second, data sourced from the web (Web) also comprise more characters than Chinese ones.This suggests a significant variation in tokenization efficiency or character usage between languages, possibly due to the linguistic structure and the tokenization methods.Third, it should be mentioned that even with similar compression rates, the settings of the tokenizer can cause significant fluctuations in the pre-training process.Therefore, it remains necessary to further investigate tokenization strategies for subsequent usage scenarios.To address the pressing need for more diverse and transparent datasets in language modeling, we introduce Matrix, a bilingual pre-training corpus of 4.5T tokens.Upon its release, Matrix could be the largest transparent LLM pre-training corpus to our best knowledge.Specifically, Matrix provides the details of the data collection and processing along with a high-performance toolkit.Additionally, we design Matrix based on the idea of retrieving, filtering, and cleaning high-quality data under various practical circumstances, which are discussed as follows:</p>
<p>• Given a set of existing (English) pre-training datasets, how do we re-process and improve the quality?§4.1 • How do we construct a large-scale, topic-comprehensive corpus from scratch, on the less explored Chinese content?§4.2 • If we have enormous printed documents, how do we build an efficient and effective system to extract viable textual contents?§4.3 • When specifying a domain of interest, how do we find relevant high-quality data from the wild of web content?§4.4</p>
<p>The final composition of the corpus is as follows: 52.55% from Common Crawl, 22.29% from programming code, and the rest from academic papers, books, and other printed materials, as illustrated in Figure 2. The detailed methodologies for processing these sources are described in the subsequent sections, and a comprehensive illustration of the sources is provided in Table 16.</p>
<p>Re-processing Pipeline for Open Datasets</p>
<p>Although several processed pre-trainig corpus (mostly in English) have been released by previous works [95,74], we argue that there is still room for a more meticulously designed pipeline to improve the existing data.Besides, it should be mentioned that existing LLMs can be easily improved by continuous pre-training with high-quality data.Therefore, we further re-process the selected web content-based corpora to produce the English subset of Matrix data mixture.The source comes from the Head and Middle parts of RedPajama-Data-V2 [25], CC part of Dolma [95], the EN part of Cultrax [72], the Refined-Web part of Amber [66], SlimPajama [94] and falcon [74].The precise distribution of our English dataset is listed in Table 3.The procedure involves filtering and multi-step deduplication.The diagram in Figure 3a shows the processing orders and the retention rates.</p>
<p>Filtering</p>
<p>To further filter out the relatively low-quality corpus from open-source datasets, we propose to use heuristic rules for text filtering.These rules are designed to identify and remove poor-quality data, thereby preventing potential model performance degradation caused by a flawed pre-training corpus.Since our composite dataset is made up of corpora from multiple sources, we adapt well-designed cleaning methods [74,14,76,78] and tailor our rules for each one to ensure quality consistency.</p>
<p>For the RedPajama-Data-v2 dataset [25], which provides quality annotations for each text, we integrate our heuristic rules with these annotations to refine data quality evaluation and further perform random sampling on the dataset to confirm the thresholds for every rule.For datasets lacking quality annotations, we apply the established rules and thresholds derived from RedPajama-V2, while customizing them to align with the unique characteristics of each dataset.For example, the Dolma dataset [95] comprises six subsets, namely Wikipedia, PeS2o, Stack Code, Gutenberg, C4, and CC, each with different data characteristics.Given the unique characteristics of each subset, we conduct individual sampling and evaluation to ensure that the modifications in rules and thresholds are aligned with our filtering requirements.Specifically, for the CC subset, we adjust the unique word and text length thresholds.For the Gutenberg subset, which predominantly contains book texts, we apply only a few rules to avoid the time-consuming process of executing extensive heuristic checks on long texts.</p>
<p>The filtering process involves: 1) Document-level and sentence-level filtering to ensure text length adequacy, character meaningfulness, and consistency; 2) Duplicate text removal, including n-grams and sentences; 3) Sensitive word check to eliminate texts containing any terms from a blacklist.</p>
<p>Deduplication</p>
<p>It has been reported that repetitive text can lead to a decline in model performance [58,51,42], which makes deduplication a crucial step in corpus processing.By eliminating duplicates, we can significantly reduce the rate of emitted memorization and make model training more efficient [58].</p>
<p>Repetitions can be categorized into exact duplicates and near duplicates.For exact duplicates, we employ exact document deduplication to remove them.For near duplicates, we utilize Minhash LSH deduplication to remove them as much as possible.In addition, there are instances where parts of the text are completely duplicated, and in these cases, the Minhash method struggles to remove them.To address this, we have adopted two methods for partially removing such content: paragraph deduplication and exact substring deduplication.</p>
<p>Exact Document Deduplication Exact document deduplication is a method used to evaluate an entire text to determine if it is identical to another.If it is found to be exactly the same, the duplicate will be removed.For processing data in English, Spark is employed to handle the dataset.Due to the vast volume of data, there may be issues with insufficient memory.The solution to this problem involves batching the text data into separate buckets for storage.Each bucket's data is then processed in turn to remove duplicates.</p>
<p>Minhash LSH Deduplication Minhash [13] is an excellent method for removing near duplicates, especially for web page data, and is widely used for similarity search and duplicate detection in large datasets [104,33,37].It can handle very common scenarios where the text content is essentially the same, but the scattered template blocks of the web pages are different.The principle of MinHash is to represent a set with smaller hash values, which can then be used to estimate the Jaccard similarity [47] between two sets: Jaccard(A, B) = (A ∩ B)/(A ∪ B).</p>
<p>MinHash involves using multiple distinct hash functions that map each element of a set to a larger numerical domain.For each set, these multiple hash functions are applied to all elements within the set, and the smallest hash value produced by each hash function is chosen as its minimum hash value.Thus, each set can be represented by a vector of these minimum hash values, forming the set's MinHash signature.For text data, an n-gram approach can be used to construct a set.</p>
<p>After obtaining the signature of the text, Locality-Sensitive Hashing (LSH) [35] is employed to rapidly identify candidate set pairs that exceed a certain threshold in Jaccard similarity.This accelerates the search process for similar items.The specific approach divides the signature into several bands, each containing several hash values.Another hash function is then used to map each band to a hash bucket.All sets with the same band hash are mapped to the same hash bucket.All set pairs in the same hash bucket are considered candidate similar pairs without further specificity regarding their similarity.</p>
<p>Here, we utilize 128 unique hash functions to form signatures, divided into 9 bands, with each band containing 13 hash values.Consequently, the Jaccard threshold is set at 0.8.</p>
<p>Upon identifying similar pairs, connected components are constructed.Within each component of the connected components, one text is retained while the others are eliminated.For processing vast amounts of data efficiently, a distributed implementation [53] based on map-reduce is adopted.</p>
<p>Paragraph Deduplication Paragraph deduplication involves removing all duplicate paragraphs within a text.A paragraph is defined as a section of text separated by the newline UTF-8 character "\n".Paragraph deduplication is an effective method for removing website navigation headers, advertisements, and similar elements.Since paragraph deduplication involves deleting parts of the text, it may cause some interference with content analysis.</p>
<p>Its concrete implementation first involves splitting the text into multiple paragraphs using newline utf-8 character "\n", with each paragraph being tagged with its corresponding document id and offset in the text.Then, each paragraph is hashed using SHA256.Next, the hash values are deduplicated.After deduplication, the deduplicated text is restored according to the document ID and offset.</p>
<p>Exact Substring Deduplication This method follows [58].Given the diversity of languages, when the length of repeated text is sufficiently long, it is highly likely that they are either derived from one another or sourced from the same reference.Therefore, when two texts, t i and t j share sufficiently a long substring, that is t a..a+k i = t b..b+k j , one of them is removed.For the selection of the length threshold, we adhere to the setting in [58], choosing k=50.Due to our distributed environment, the memory of a single node is insufficient to hold all the data.Therefore, we did not adopt the implementation in [58].In our work, we segment each text into sliding windows of 50 characters with a step size of 1.We then compute the SHA256 hash value for each window along with its corresponding document ID and offset.Subsequently, for windows with identical hash values, we mark them as duplicates except the first one.Finally, using the text ID and offset, we restore the original strings and decide whether to delete a segment based on the duplicate marker.</p>
<p>Corpora Crawl from Scratch Pipeline</p>
<p>We further provide a pipeline to crawl and process the web content from scratch and showcase it with the Chinese language data, which could be a step-by-step guide for follow-up research to build a new up-to-date corpus.We take the corpus produced in such a pipeline as the Chinese subset of Matrix, where 80.6% is derived from the Chinese web pages we crawled and others from several open datasets, as listed in Table 4.The pipeline overview and details are illustrated in Figure 3b.</p>
<p>Filtering</p>
<p>The filtering rules for Chinese datasets are specifically tailored to address their unique challenges, differing from those applied to relatively well-processed English datasets in §4.1.Considering the large proportion of HTML-converted data in Chinese datasets, we focus intensively on eliminating HTML-related artifacts and rectifying textual inconsistencies.Furthermore, given the significant linguistic differences between Chinese and English, we conduct targeted sampling of documents within Chinese datasets, which aims to reassess and adjust the thresholds and details of our filtering rules, ensuring their suitability for the unique language characteristics of Chinese text.For example, we refine the rules to distinguish between 'characters' and 'words' in Chinese texts, adapting the tokenization method accordingly.</p>
<p>Our Chinese filtering steps are similar to the rules adapted to filter Massive Appropriate Pre-train Chinese Corpus (MAP-CC) [30]: 1) Data format unification to boost processing efficiency.2) URL removal.This step is conducted in two stages: first, removing texts with URLs listed in Blacklist T1; followed by a comprehensive sweep to eliminate residual URLs.3) Sentence-level and document filtering to discard text that is excessively brief, substandard, or logically incoherent.4).Duplicates removal, including n-grams and sentences.</p>
<p>Deduplication</p>
<p>The deduplication of Chinese data includes Exact Document Deduplication, MinHash Deduplication, and Similar Line Deduplication.Due to difficulties in deploying Spark in the environment for processing Chinese, we have re-implemented the first two methods.For Exact Document Deduplication, there are slight differences from the implementation for English, mainly to save memory, where we have adopted a Bloom Filter approach and set the false positive rate of the Bloom Filter to 0.001.Discussions on Exact Document and MinHash LSH Deduplication can be found in §4.1.2.</p>
<p>We did not use Exact substring deduplication because when crawling web pages, it is common to repeatedly crawl the same content multiple times in a signal document.Additionally, when extracting the main text from HTML, there is often a loss of one or two words.The combination of these two situations violates the assumption in [58] that "it is rare for the same idea to be expressed identically in multiple documents unless one expression is derived from the other, or both are quoting from a shared source."Therefore, after Exact substring deduplication, there will be cases where extra words are retained, greatly reducing the readability of the text.Hence, we propose a Similar Line deduplication method to address this issue.</p>
<p>Similar Line Deduplication</p>
<p>To address the scenario where identical content appears multiple times within a text, a direct method is to divide the text into lines using specific delimiters and then compare the similarity between each line.If they are similar, the subsequent line is removed.The division of lines includes the use of the following delimiters: "[", ".", "!", "?", "\", ". . . . . .", "]".We use edit distance to judge whether two lines L 1 and L 2 are similar as follows:
isSimilar(L 1 , L 2 ) =    T rue min(|L 1 |, |L 2 |) ≥ 15 ∧ editDist(L 1 , L 2 ) &lt; 0.1 × min(|L 1 |, |L 2 |) T rue min(|L 1 |, |L 2 |) &lt; 15 ∧ L 1 = L 2 F alse otherwise,
where |L| is the length of line L and "editDist" is short for edit distance.</p>
<p>Due to the computational complexity of calculating edit distance being O(len(L 1 ) × len(L 2 )), to accelerate this process, we additionally propose two methods to judge dissimilarity:</p>
<ol>
<li>
<p>Is the length difference between the two lines greater than one-tenth of the length of the shorter line?</p>
</li>
<li>
<p>Is the ratio of the intersection of the sets of characters and the union of the sets of characters in L 1 and L 2 less than one-third?</p>
</li>
</ol>
<p>Note that the first method has a computational complexity of O(1), and the second method has a complexity of O(len(L 1 ) + len(L 2 )).Thus, these methods can significantly improve the speed of calculation.Clearly, if either of the above two questions is positive, the lines cannot be considered similar.Otherwise, we calculate isSimilar(L 1 , L 2 ) to obtain the similarity between L 1 and L 2 .</p>
<p>Document Conversion Pipeline</p>
<p>The documents are usually better formatted, in concentrated topics, and with more consistent expressions compared to noisy web content.However, it seems to be a gold mine of high-quality corpus except that the golds lie deeply under the digital dirt.Such digital documents are mostly  stored as standard PDFs with diverse layouts or scanned images with inconsistent quality, making it challenging to build datasets upon.We observe two core issues in designing an effective conversion pipeline to extract plain text from documents: i) analyzing layout information and identifying different layout elements including text, titles, captions, images, tables, and formulas, and ii) recognizing the relationships among these layout components.
Figure Table Equation Others Content Blocks Drop Image Blocks Table Blocks Latex Blocks Ordering Post Process MarkDown
We survey the existing open-source solutions for document conversion and find some distinguished projects with good performances: PP-StructureV2 [59], Marker4 , Vary [108], and Nougat [11].However, along with their merits, each of them exhibits limitations that could be addressed to further enhance performance: PP-StructureV2 cannot recognize LaTeX format content and necessary postprocessing stages; Marker and Texify5 support few languages and do not process figures effectively; Nougat has limited support for multi-column data and recognized languages; Vary and Vary-toy require considerable computational resources.Therefore, we propose a framework consisting of disentangled processing components, allowing us to leverage the strengths of these models together.For example, we utilize Marker for enhanced language support and PP-StructureV2 for efficient layout parsing.As illustrated in Fig. 4, our document conversion framework is comprised of four parts: Layout Detection, Element Recognition, Ordering, and Post Process.The decoupling between each module enhances interpretability and simplifies the upgrade, addition, and replacement of various components.</p>
<p>Layout Detection segments the document into multiple parts such as formulas, text, headers, and footers.The Pipeline employs a lightweight target detection model provided by PP-StructureV2, which is computationally efficient and performs exceptionally well.This model's performance is further enhanced by employing the FGD (Feature Gradient Descent) algorithm, which optimizes feature extraction for more accurate layout detection.</p>
<p>Element Recognition incorporates various models to identify different elements.For formula recognition, the TrOCR model trained through Pix2Text outperforms other formula recognition models such as Latex-OCR and Taxify, supporting recognition of formulas embedded within paragraphs and non-conventional formulas, thus effectively addressing most formula recognition scenarios.Text recognition employs PP-OCRv4, Text recognition employs PP-OCRv4, notable for its compatibility with multiple computing devices and boasts strong recognition capabilities; approximately one hundred language recognition models have been publicly released, applicable to a broader range of document recognition tasks.Figures are saved as images and inserted in the subsequent merging phase.Table reconstruction is achieved using SLANet, which represents tables in HTML format.Other regions, such as headers, footers, and page numbers, are discarded and do not proceed to the post-processing and reconstruction stages.</p>
<p>Ordering In document conversion tasks, correctly handling the relationships between blocks is of paramount importance.To acquire high-quality conversion data, we need to properly handle complex layout scenarios such as multi-column and cross-page conditions.In the ordering stage, we use LayoutLMv3 [45] for column detection and sorting different areas according to specific rules.This strategy not only enhances the accuracy of the task but also significantly optimizes the readability.</p>
<p>Post-processing.The texts extracted by OCR usually could not be directly used and require additional processing as follows:</p>
<ol>
<li>Broken-up sentences: In text extracted from images, sentences may be fragmented across different lines or pages, resulting in a single sentence being divided into multiple segments.</li>
</ol>
<p>Effective OCR text extraction necessitates the identification and rejoining of these fragmented sentences to reconstruct coherent, complete sentences.2. Hyphenated words: Certain words may be split into two parts within the text due to formatting constraints, connected by hyphens (e.g., network-ing).Text extraction must recognize these hyphenated words and merge them back into a single, complete word (e.g., networking).3. Broken math formulas: OCRed mathematical formulas in Markdown may experience issues such as missing elements, incorrect symbols, or fragmented expressions.To address this issue, we fine-tune a 7-billion parameter open-source pre-trained language model [7] on supervised learning data pairs (x i , y i ).Here, x i represents the instruction for detecting and correcting errors in the given texts, and y i represents the corrected output texts.We adopt vLLM to enable faster inference through quantization and efficient memory management of attention keys and values using PagedAttention, among other optimizations.The prompt templates used for processing both both languages are provided in Appendix A.10.</p>
<p>By incorporating these strategies, we can significantly improve the quality and coherence of OCR-ed texts, mitigating common errors and enhancing the overall readability and usability of extracted content.We use FastDeploy6 , a highly efficient AI inference deployment tool, as the codebase of our implementation, which can fully exploit the advantages of multithreading to optimize inference speed and computational overhead.Overall, while maintaining performance and deployment efficiency, we provide a framework for document conversion that covers comprehensive scenarios, including recognizing layout information, supporting table reconstruction, and formula recognition.</p>
<p>High-Quality Supplement Data Collection</p>
<p>In this section, we present our method for High-Quality Supplement Data Collection, which applies to a diverse range of topics and enhances the robustness of datasets.Inspired by [89], which adopts an iterative pipeline to facilitate the acquisition of large-scale, high-quality data from Common Crawl, we propose to select high-quality data for mathematics, scientific exam synthetic data, and wiki-based content in our Matrix.</p>
<p>The procedural phases of the iterative pipeline are enumerated as follows:</p>
<p>• Seed Dataset Collection: Collect a high-quality seed dataset for the field of interest, like mathematics, code, or wiki-based content.• Domain Definition and Sampling: Define a domain as data entries within the seed dataset sharing the same base URL and extract samples from each domain in the seed dataset as positive samples to enhance format diversity.Correspondingly, acquire an equal amount of data from Common Crawl as negative samples.• Model Training: Employ a FastText model [50] for binary classification to discern data relevance to the specified field.Training parameters are set as follows: three epochs, a learning rate of 0.1, an embedding dimension of 256, and an n-gram of 3. The model is quantized to augment operational efficiency within constrained memory capacities, reducing its size to approximately 10% of its original footprint.• Data Confidence Assessment: Utilize the trained FastText model to estimate the confidence of Common Crawl data qualifying as positive.Retain data sequenced from highest to lowest confidence.To streamline the confidence sorting process, initially sample a subset of data to establish a viable threshold that balances data exclusion with retention needs.• Data Evaluation: Assess the retained data via ChatGPT 3.5 [1], employing the URL to determine field specificity.This stage aims to mitigate the incidence of false positives while maintaining a requisite recall rate.• Data Recall and Annotation: Revisit domains where over 10% of the data was recognized as field-specific.Annotate this data subset using ChatGPT 3.5 [1] via URL.The data selection for Common Crawl focused on the English content of the RedPajama V2 dataset [25].The seed dataset for the mathematics segment is sourced from OpenWebMath [6], while the science synthetic dataset is from specific domains such as Chemrxiv, biorxiv, and proprietary crawled exercise data from open-source datasets, e.g.wanjuan-exam [38], WebInstruct [117], Web Of Science [55].Wiki data is procured directly from wiki websites.</p>
<p>Model</p>
<p>Model Architecture</p>
<p>The MAP-Neo model architecture is grounded on the transformer decoder as outlined by Vaswani et al. [103].The essential parameters defining this architecture are detailed in Table 5.The models are trained with a context length of 8192 tokens, incorporating several enhancements proposed after the original transformer concept.These enhancements are listed below:</p>
<p>Multi-Query Attention [92].The 7B model variant employs multi-head attention, whereas the 2B model checkpoints implement multi-query attention, using a single key-value head configuration (num kv heads = 1).This modification is based on ablation studies indicating that multi-query attention is particularly effective at more minor scales [92].</p>
<p>RoPE Embeddings [97].Instead of traditional absolute positional embeddings, we utilize rotary positional embeddings at each layer and share these embeddings between the inputs and outputs, minimizing the overall model size.</p>
<p>RMSNorm.To ensure stable training, each transformer sub-layer-including both the attention and feedforward layers-is normalized using RMSNorm [120].</p>
<p>Activation Function We use SwiGLU [93] as our activation function.</p>
<p>Model Scale Hyperparameters</p>
<p>In this work, we compare two different model scales: 2B and 7B parameters.Since these models are standard dense Transformers.These models are constructed using the hyperparameters in Table 5.</p>
<p>The two models are trained identically (except for training data) using the same vocabulary and batch size.Training details are shown in §3 and §5.1.texts to develop its general text generation capability.Subsequently, during the decay phase, we focus on enhancing the reliability of the model's generated content by incorporating high-quality data and mode code data.The distribution of data used across different phases is depicted in Figure 5.Note that we increase the volume of code data in the decay phase.Specifically, during the fundamental phase, since Stack V2 [68] was not yet available, we utilized Stack V1 [54] and repeated the dataset twice to achieve a balanced data ratio.In the decay phase, with the release of Stack V2 [68], we incorporated it as the code component for training.Moreover, we perform further data distribution tuning including duplicated high-quality data sources, such as books, judicial decisions, and government reports for training, to improve the model's performance.The open-source data used for pre-training is shown in Table 16, the data repetition details are shown in Table 17 and the training hyperparameters are shown in Table 6.</p>
<p>Figure 5: The data mixture ratios in MAP-Neo pre-training stage.The left is the fundamental phase and the right shows the decay phase.</p>
<p>Fundamental Phase: General Ability Acquisition</p>
<p>During the fundamental phase, we employ a two-stage learning rate scheduler (LRS) to equip the model with a robust capability for general text generation.The LRS is modeled as a piecewise function, consisting of an initial warmup phase where the learning rate linearly ascends from a base rate of η a = 2 × 10 −5 to peak learning rate η max = 2 × 10 −4 over t warmup = 2k steps.This is followed by a cosine decay phase, during which the rate gradually diminishes back to η b = 2 × 10 −5 over about 365k steps.The learning rate f (t) as a function of time t can be delineated as follows:
f (t) = η a + (η max − η a ) t twarmup if t ≤ t warmup η b + (η max − η b ) 1 2 1 + cos π t−twarmup ttotal−twarmup if t warmup &lt; t ≤ t total , (1)
where t is the current timestep, t warmup denotes the duration of the warmup phase, and t total represents the total number of training timesteps.This learning phase processes about 3, 726 billion tokens, ensuring the model's robust training on diverse textual data.This meticulous configuration of learning rates and extensive processing optimize training dynamics and efficiency, fostering a steady maturation of the model's capabilities.</p>
<p>Decay Phase: Improvement and Rectification</p>
<p>Owing to the issue in training tokenizer as claimed in §3, the model encounters test failures in code generation tasks, despite its strong language understanding capabilities acquired during the fundamental phase.To address this issue, we have introduced an additional decay phase specifically designed to utilize a tokenizer of the fixed version.The learning rate in this decay phase initiates at η c = 2 × 10 −4 and undergoes exponential decay over t decay = 148k steps, with a half-life T corresponding to half the t decay steps, similar to the decay phase employed by MiniCPM [44], which can be formulated as follows:
f (t) = η c × 0.5 t T if t ≤ t delay , (2)
where t is the current timestep of the decay phase.This strategic adjustment not only rectifies the initial tokenization flaws but also enhances the model's performance on code generation tasks.</p>
<p>During this phase, the model processes a total of about 778 billion tokens, which primarily consist of high-quality instruction data.We also simultaneously increased the proportion of code in the data from 14.77% to 17.04%.This adjustment significantly enhances the overall performance of the model.The deliberate enrichment of the dataset with a higher ratio of code, coupled with instructional inputs, ensures a more robust and versatile model, adept at tackling complex coding tasks as well as understanding and generating professional responses in different fields.</p>
<p>Alignment</p>
<p>Supervised Fine-tuning</p>
<p>To align with the human behavior of LLMs, the initial step is to perform Supervised Fine-Tuning (SFT).Our SFT also consists of two phases.In the first phase, we collect a large amount of instruction data to enhance the foundational abilities of LLMs.In the second phase, we build upon the capabilities established in the first phase and propose to improve the chat abilities of MAP-Neo.This process finetunes a pre-trained LLM on chat-style data, including both queries and responses.We illustrate the details of data construction and training strategies.</p>
<p>Data</p>
<p>Foundational Phase: Enhancing Instruction Following Abilities In the first phase, our focus is to significantly boost the model's foundational abilities (e.g., code and math skills), where we utilize over 2 million instructional data points during this phase.Specifically, the first phase includes the entire OpenHermes 2.5 [99], where we exclude segments related to the TheoremQA benchmark [16] to prevent benchmark data leakage.Additionally, we incorporate the complete Code-Feedback [125] dataset and a subset of WebInstructSub [117] data.</p>
<p>Chat Phase: Enhancing Chat Abilities In the second phase, we focus on improving the model's chat abilities while maintaining the foundational skills acquired in the first phase.For this purpose, we collect over 100k multi-turn dialogue data sourced from real user conversations.To ensure the model retains its foundational capabilities, we include 5k math and code-related data points extracted from the first phase.Our experiments have demonstrated that this additional phase of SFT significantly boosts the model's performance on chat benchmarks, such as MT-Bench [124] and AlpacaEval [62], without compromising its foundational abilities.</p>
<p>By following this two-phase approach, we ensure that our model can not only maintain a strong foundation in essential skills but also generate natural, helpful, and contextually accurate responses.</p>
<p>Training</p>
<p>Consistent with pre-training, we also apply the next-token prediction objective as the training task for SFT.Note that we apply the loss masks for the system and user inputs.The model's training process utilizes the AdamW optimizer with the hyperparameters in Table 6.</p>
<p>The sequence length is limited to 8192, and the batch size is 512.The training process consists of two phases using the same hyperparameters.In the first phase, the model is trained for 3 epochs using over 2 million instructional data points, focusing on enhancing foundational abilities.In the second phase, the model is trained for 1 epoch using over 100k multi-turn dialogue data to enhance its chat abilities while maintaining the foundational skills acquired in the first phase.</p>
<p>Iterative DPO</p>
<p>DPO Direct Preference Optimization (DPO) [77] is a straightforward and effective method for aligning language models with human feedback.It converts the preference loss [12] into a loss function over the language model, thereby bypassing the need for explicit reward modeling [12] and reinforcement learning [19,87].Starting with a supervised fine-tuned language model, denoted as π sft , DPO collects a dataset D = {(x, y w , y l ) i }, which consists of human preferences between two responses generated by π sft : y w (preferred) and y l (dispreferred) to the same prompt x.Using this dataset, DPO parameterizes a language model π θ and directly estimates its parameters through maximum likelihood estimation on the human preference dataset D as follows:
L DPO (π θ ; π sft , D) = −E (x,yw,y l )∼D log σ β log π θ (y w |x) π sft (y w |x) − β log π θ (y l |x) π sft (y l |x) . (3)
Iterative DPO.We follow Storm-7B [64] to use the Iterative DPO [111] pipeline to develop our chat model.Specifically, we employ three iterations, with each iteration consisting of three stages: 1) generating paired responses, 2) labeling responses using reward models, and 3) training the LLM with DPO loss as described in Eq. 3. We utilize Nectar7 as our prompt dataset and Starling-RM-34B8 [126] as our reward model.This model is finetuned from Yi-34B-Chat [113] and generates a scalar output for any given prompt and response.To preserve the multilingual capabilities of our model, we also adopt a preference dataset 9 in Chinese in the 3-rd iteration.</p>
<p>We report the length-controlled win rate of AlpacaEval2.0[32] to demonstrate the performance progress of our model in Table 7.The results show that performance improves with each iteration, indicating that our model becomes increasingly aligned with human values.The scaling laws are capable of predicting training configuration for the training of LLMs.This principle emphasizes the importance of the ratio between the amount of training data D (measured in tokens) and the size of the model N (in terms of parameters).In this section, we applied the Chinchilla Law in Eq. 4 [43], OpenAI Law in Eq. 5 [52], a derivation of Symbolic Music Scaling law in Eq. 6 [75] and our proposed method on our dataset to fit our models, where A, B, E, α, β, α c , D c , α N , N c and d are hyperparameters to be optimized.The original SMS scaling law introduces two modifications to the Chinchilla law.The first modification addresses the repetition of training data, which is not considered in our study.The second modification concerns the interaction between the number of model parameters, N , and the dataset size, D. Specifically, it posits that the loss curve as a function of D, represented as B D β , is influenced by N .This interaction between the number of model parameters and dataset size is also reflected in the OpenAI scaling law.However, our version of SMS law, as detailed in Eq. 6, is simpler and yields superior results compared to the corresponding model in the OpenAI framework.
L(N, D) = A N α + B D β + E (4) L(N, D) = N c N α N α D + D c D α D(5)L(N, D) = d N α • D β + A N α + B D β + E.(6
The motivation for fitting scaling laws is to optimize the loss under the bounds of computational resources.This process is formalized as minimizing the validation cross-entropy loss L, subject to constraints imposed by available computational resources (C), specifically floating-point operations per second (FLOPs), as denoted below:
arg min N,D L(N, D) s.t. FLOPs(N, D) = C(7)
Given that our model is trained on almost non-repetitive and high-quality data, we utilize the training loss instead of the validation loss for the scaling law application.</p>
<p>NEO Scaling Law</p>
<p>We train models with sizes of 250M, 460M, and 980M parameters using 1000B tokens of training data.These models are then used to predict the scaling law, which guides the training of a model with 7.8B parameters on 3.07T (3065B) tokens during phase 1.To evaluate the fit of the scaling law, we employ the Huber loss ( δ = 1e − 3) between the actual logloss and the predicted logloss, along with the R 2 value between the true loss and predicted loss.Optimization of the scaling law is performed using the LBFGS algorithm.This approach is applied consistently across the Chinchilla law and the symbolic music scaling law.By leveraging these methods, we aim to ensure the accuracy and reliability of our scaling law predictions, enabling efficient training of large-scale language models.</p>
<p>Figure 6 illustrates the training loss values alongside the Chinchilla law predictions.Although the Chinchilla law fits well, with the predicted loss curve falling within the fluctuations of the actual loss curve, its trend appears flatter compared to the actual loss curve.The actual loss decreases more rapidly than predicted by the Chinchilla formula (i.e.B D β ), suggesting our dataset with diverse high-quality corpora can further decrease the loss value when D is large.To address this discrepancy between Chinchilla prediction and observation, we introduce the following equation, denoted as NEO scaling law, which includes one additional regularization term log(D) for datasets containing several trillion tokens across various corpora:
L(N, D) = A N α + B D β + E − d • log(D)(8)
Note that although the regularization term −d • log(D) theoretically results in no lower bound on loss as D approaches negative infinity suggesting potential imperfection of the formula, the value of d typically ranges in our experiments between 1e-2 and 3e-2.Therefore, for a dataset size less than hundreds of trillion tokens, the loss remains within a reasonable range.</p>
<p>From the following Table 8, we observe that the NEO scaling law equation yields significantly better results on the training set and testing set.Under the prediction of the NEO scaling law and the computational resource constraint of 1.5 × 10 23 FLOPs, the optimal configuration is to train a 10B parameter model with 2.5T tokens, providing a predicted loss value of 0.6597.To ensure comparability with baseline models, we choose to keep our model size at 7.8B parameters, similar to the Llama-base model.This configuration with a 7.8B parameter model with 3.07T tokens requires slightly fewer computational resources and results in a similar prediction loss value (0.6618).Meanwhile, after training, We observe that the real training loss in this configuration is 0.6591, which is close to the prediction loss value and demonstrates the effectiveness of the NEO scaling law.</p>
<p>Generalization of NEO Scaling Law</p>
<p>The NEO scaling law can be applicable to a broader range of models beyond MAP-Neo.Specifically, in Figure 7, we illustrate the fit results of the Chinchilla scaling law (yellow dashed line) and the NEO scaling law (red solid line) to the DeepSeek LLM [28] with the 7B and 67B parameters, which also pre-trained on a dataset with multiple corpura including Chinese, English and codes.</p>
<p>We observe that for the largest model sizes (i.e.MAP-Neo-7B and DeepSeek-67B), the predictions of Chinchilla Law tend to underestimate the actual loss when the dataset size (D) is small and overestimate the actual loss as model parameters and training data scale up.In contrast, our predictions of our NEO Scaling Law produce better fitting results when compared with the results of Chinchilla Law for MAP-Neo-7B and DeepSeek-67B.We further suggest NEO Scaling law might be more suitable for the situation with a large diverse pre-training dataset with multiple high-quality dataset sources.For more discussion on NEO scaling law on other models, please refer to Appendix A.8.</p>
<p>Infrastructure</p>
<p>Our advanced infrastructure consists of two primary components: a data processing system and a training system.The training system is designed to support both pre-training and fine-tuning stages, enabling comprehensive model development.</p>
<p>Our infrastructure is designed to handle extensive data processing tasks for both English and Chinese datasets.We utilize robust systems to ensure efficient and scalable processing capabilities across different languages.Spark [118] is used for distributed computing, and object storage is used to save the data.Each machine is configured with a 64-core CPU, 256GB of memory, and 1TB of local disk.There are a total of 94 machines.For the Chinese data processing, there are a total of 14 machines.Among them, 6 machines have a 96-core CPU and 180GB of memory, while the other 8 machines have a 48-core CPU and 190GB of memory.Network File System (NFS) [84] is used as the distributed file storage system.</p>
<p>In the pre-training stage, the Megatron-Core toolkit is utilized for its capacity to train large-scale language models, featuring up to hundreds of billions of parameters.Compared to the tokens per second (TPS) metric, the usage of Megatron-core achieves a rate of 7200 TPS when training a 7B model, which surpasses the performance of 6400 TPS observed under the same settings without employing Megatron-core.This is accomplished using both model and data parallelism techniques.We implement several strategies to manage our large datasets and model complexities effectively.Firstly, we introduce programs to identify and temporarily remove tainted computing nodes from the resource pool due to software or hardware errors by automatic inspection, prediction, and labeling.Secondly, we make modifications to Megatron-LM to specifically prevent overflow issues detailed in A.3 when processing large data corpora.Lastly, we implement task recovery mechanisms that utilize strategically selected checkpoint iterations to safeguard against potential failures during training.These enhancements ensure optimal performance and reliability in our large-scale training operations.</p>
<p>To ensure optimal utilization of our computational resources, our infrastructure design incorporates a sophisticated network topology and hardware configuration, facilitating efficient workload distribution and data transfer for complex model training tasks.Our infrastructure utilizes distributed computing techniques to optimize the training of our models.Specifically, our 7B model is trained using an H800 configuration with 512 GPUs across 64 nodes and employs NCCL for backend distribution with ibp as the network interface and mlx5 of InfiniBand hardware to enhance inter-GPU communication.</p>
<p>Tensor model parallelism is configured to utilize 2 GPUs, distributing the execution of a single transformer module across these units to enhance efficiency.For our 2B models, we utilize all 256 GPUs with tensor model parallelism set to 1 to ensure effective data replication.We further amplify scalability and efficiency by employing techniques similar to ZeRO-1 for sharding the optimizer state.This approach enables the management of more extensive datasets and more complex model training with significantly reduced memory overhead.</p>
<p>Our cluster consists of machines with dual Intel Xeon CPUs and eight NVIDIA H800 GPUs.The architecture facilitates high-speed data transfer, with each CPU socket interfacing with two PCIe Gen4 x16 lanes connected to dedicated PCIe switches.These switches manage the connections to a local NVMe SSD, an RDMA-capable Network Interface Card (NIC), and two GPUs.Inter-CPU communication is facilitated by Intel's Ultra Path Interconnect (UPI), with both CPUs linked to a dual-port TCP NIC supporting 100 Gbps.Each machine's network configuration includes four RDMA NICs, each offering 200 Gbps of full duplex bandwidth and integrated GPU Direct RDMA capabilities.Notably, the GPU array is interconnected through four NVIDIA NVSwitches, enabling robust intra-GPU communication with a bandwidth of 400 Gbps.This advanced configuration underscores the cluster's capability to handle large-scale model training with exceptional efficiency and speed.</p>
<p>Regarding the inter-machine connections of our data center, we implement a dual-layer Clos network architecture wherein each minipod accommodates at least 512 H800 servers interconnected via a high-speed RDMA network.Within this architecture, each S0 switch is equipped with 64 ports, each supporting a bandwidth of 400 Gbps.This arrangement ensures a network convergence ratio of 1:1, a critical factor in maintaining optimal data flow and reducing bottlenecks.Connectivity within this structure is meticulously organized such that every two S0 switches serve 32 servers, with a total of 32 S0 switches networking within each minipod.This setup exemplifies an advanced implementation designed to maximize throughput and minimize latency in data center environments.</p>
<p>Evaluations</p>
<p>The thorough evaluation demonstrates that the MAP-Neo model family achieves inspiring performance both on automatic benchmarks of base models and chat models.Compared to the previous transparent LLM series, we underline MAP-Neo's distinctive performance on code, math, and instruction following abilities, which not only endows the MAP-Neo with academic and practical value.We present the results of our base models compared to several well-known LLMs, e.g.LLama3-8B and Mistral-7B, across standard academic benchmarks.All our evaluation metrics are derived from our assessments, ensuring consistency and transparency.We do not perform any post-processing on the evaluation content, maintaining the integrity of the raw outputs.</p>
<p>Our evaluation spans a comprehensive suite of public benchmarks in both English and Chinese, leveraging an internal evaluation framework designed for rigorous assessment.These benchmarks include a diverse range of datasets catering to multiple disciplines and aspects of language understanding and reasoning.Our evaluation strategy encompasses various metrics, including language modeling, specialized knowledge, and code generation.For datasets requiring multiple-choice selection, we employ a perplexity-based evaluation.For generation-based datasets, we generate free text and parse the results accordingly.The detailed results of our comparison with other base models are shown in Table 9.</p>
<p>Standard Benchmarks We include Boolean Questions(BoolQ) [21], Physical Interaction QA(PIQA) [10], Social Interaction QA(SIQA) [85], HellaSwag [119], WinoGrande [83], ARC-Challenge(ARC-c) [22], OpenBookQA-Fact [70], CommonsenseQA [98], and MMLU [40] to assess general reasoning capabilities.All these benchmarks are tested with a 0-shot configuration, except for MMLU, which is evaluated with a 5-shot setup.</p>
<p>Code Generation We report the pass@1 scores of the evaluated models on HumanEval [15], HumanEval-Plus, MBPP [5], and MBPP-Plus, all with a 0-shot configuration, following the EvalPlus framework [63].</p>
<p>World Knowledge We include NaturalQuestions(NQ) [57] and TriviaQA [49] to assess world knowledge.Both benchmarks are tested with a 0-shot configuration.</p>
<p>Reading Comprehension</p>
<p>We report the 0-shot average on SQuAD2.0 [79].</p>
<p>Exams We report the average scores for MATH [41] and GSM8K [23], both with a 4-shot configuration.For GSM8K, we employ a simple Chain-of-Thought prompting strategy: "Let's think step by step."For both datasets, we use the MAmmoTH evaluation framework [116].</p>
<p>Chinese We use CMMLU [60] and CEval [46] to assess performance on Chinese language tasks.Both benchmarks are tested with a 5-shot configuration.</p>
<p>Discussions</p>
<p>Data Quality MAP-Neo demonstrates significantly better performance on math, code, and complex reasoning by incorporating high-quality data, compared to previous transparent LLMs, e.g.Amber [66] and Pythia [9], adopting (presumably) lower quality data.</p>
<p>Gap between our MAP-Neo and other transparent LLMs</p>
<p>In Table 9, we note that transparent LLMs still significantly lag behind the performance of frontier industrial Open-weight LLMs with similar sizes (e.g.LLama3-8B, Mistral-7B).In contrast, our MAP-Neo can match or even surpass them on part of the automatic benchmarks about math, code, and Chinese knowledge.We call for increased participation in the development of transparent LLMs to further advance the LLM democratization.</p>
<p>Aligned Model Performance 10.2.1 Main Results</p>
<p>To accurately evaluate the realistic conversational performance of our aligned models, we selected several benchmarks that measure various aspects of model capabilities.These benchmarks were chosen for their ability to comprehensively assess key abilities such as alignment, instructionfollowing, real-world performance, and alignment with human preferences.Below are the specific benchmarks we used and the unique capabilities they evaluate:</p>
<p>AlignBench [65] AlignBench evaluates the alignment capabilities of Chinese LLMs, ensuring high reliability and interpretability through a comprehensive, multi-dimensional benchmark and human-inthe-loop data curation.</p>
<p>AlpacaEval [62,32,31] AlpacaEval measures instruction-following models' performance efficiently and reliably through an LLM-based automatic evaluation, validated against extensive human annotations.</p>
<p>Arena-Hard [61] Arena-Hard evaluates LLMs' real-world performance and ability to reflect human preferences by constructing benchmarks from live data and ensuring robust model capability separation.</p>
<p>CHC-Bench [30] CHC-Bench evaluates LLMs on their proficiency in Chinese culture, history, and language, with tasks like composing poetry, understanding ancient Chinese, and explaining Chinese terms, emphasizing the challenges for models trained mainly on English datasets.</p>
<p>MT-Bench [124] MT-Bench assesses LLM-based chat assistants' alignment with human preferences using strong LLMs as judges, achieving high agreement with human evaluations.</p>
<p>MMLU-Pro [106] For the aligned models, we further evaluate MMLU-Pro [106] with a 5-shot configuration to reflect the model's capabilities more comprehensively.</p>
<p>Discussions</p>
<p>The effectiveness of Iterative DPO In Table 10, when compared to Neo-7B-SFT, Neo-7B-Instruct shows significant improvement on the chat-related benchmark datasets (e.g., AlignBench, AlpacaEval, Arena-Hard, and CHC-Bench), which further demonstrates the effectiveness of our Iterative DPO.</p>
<p>The performance of the chat model Table 10 shows that Amber-7B-Chat and OLMo-7B-Instruct perform poorly on Chat Benchmarks.We assume that the limited capabilities of the base model may severely limit the performance of corresponding instruction-tuned models on chat benchmarks.</p>
<p>Societal Impact</p>
<p>Data Colonialism is a deep concern when firms decide to exploit an algorithm product.[27] conceptualize the data colonialism framework and argue that Big Tech Giants, particularly in the U.S., use their massive data power to manipulate human behaviors and judgments and track people's traces continuously, forming a new social order.This suggests that controlling and owning data benefits firms' market status and generates large returns.So, making LLMs as firms' proprietary models is a common practice in the industry.[2] discuss the barriers to AI democratization, such as the concentration of AI capabilities in large tech firms and elite universities.They underscore the importance of democratizing access to AI resources to mitigate the risks of data colonialism and promote equitable access to AI technologies across all institutions.[91] discuss the dominance of proprietary LLMs and the need for high-performing open-source alternatives.They propose methods to enhance open-source models to compete with proprietary models while addressing privacy and resource-constrained concerns.They also point out how important the open-source model is in the LLMs community and acknowledge that firms with fewer resources and sensitive information are hesitant to trust the proprietary models.However, most LLMs are the product of a massive English corpus and are trained from English scratch [122].How the open-source model can benefit the non-English language community and its data democratization remains unclear.</p>
<p>Additionally, most open-source models are not thoroughly transparent.Open-source large language models (LLMs) often claim to be transparent and accessible, but many critical aspects of their development, such as data cleaning processes and pre-training code, remain undisclosed.This lack of transparency hampers reproducibility and the ability to fully understand and trust these models [110].For firms with financial constraints and privacy concerns, it is not economical to train their LLMs.Even though most open-source models give open access to the final and some intermediate checkpoints, they keep data sources, data pre-training code, and data processing methods opaque, those of which are the most costly parts of setting up an LLM.That is the key issue we want to tackle and then hope to promote full transparency in our community.</p>
<p>In our report, the MAP-Neo model might complement the current scarcity of Chinese corpus in LLMs.Importantly, our bi-lingual language model is a "thorough" open-source model-disclosing all key processes from sources of searching original data, and data cleaning to pre-training code base.Those disclosures significantly reduce the cost of deploying and customizing a LLM, especially for a Chinese LLM.It might have potential societal impacts.Firms with the need for a Chinese version of LLM but face constraints can be more able to leverage benefits from LLMs by using or referencing our MAP-Neo Model.It might improve social welfare in total and make a more vivid and diversified Chinese LLMs community [24].Our advocates for thorough open-source action may attract more Chinese LLM researchers or relevant firms to fully disclose their models because thorough transparent open-source models can bring them sizable benefits from more constructive feedback and criticism.Those might make their models better and eventually accelerate the iterations of Chinese LLMs and empower the local community [81].Overall, open innovation practices like disclosing the MAP-Neo model might alleviate the dominance of English LLMs and improve the inclusivity of the international LLMs community.</p>
<p>Those open innovation practices may also benefit Small and Medium enterprises (SME) to introduce new products effectively [96] and efficiently with easier implementation of their own customized LLMs, which may partially mitigate the threats of data colonialism from Big Tech Giants.Our Map-Neo model's open and economical attributes give an optimistic outlook for researchers in academia.</p>
<p>Those attributes suggest that it is not hard and costly to set up the university's own AI without depending on specific Big Tech Giants' help.If universities have independent and decentralized control over their data and AI processes, it will prevent large companies from AI monopolization and promote data and AI democratization.</p>
<p>Conclusion</p>
<p>In this paper, we introduce MAP-Neo, which makes strides toward enhancing the transparency and accessibility of large language models (LLMs) by offering a fully open-source bilingual LLM suite.By sharing thoroughly detailed processes, from data curation, pre-training corpus (i.e., Matrix Data Pile), and model training to evaluation, we aim to support the academic and open-source communities in advancing transparent NLP research.Moreover, MAP-Neo narrows the gap with industry-level models (typically closed-source) with enhanced reasoning, instruction-following, and coding abilities.We hope that our work provides a valuable resource for researchers and developers, contributing to a broader effort to democratize access to advanced LLM technologies.</p>
<p>Contributions and Acknowledgments</p>
<p>Team Leaders:
• Ge Zhang, M-A-P,</p>
<p>A.3 Training Framework Overflow Details</p>
<p>In this section, we address an overflow problem within the Megatron-core.The issue arises when the variable num samples, defined as int64 t, exceeds the capacity of int32 t associated with the sample idx.This discrepancy can lead to memory leaks and undefined behavior, as illustrated in Fig. 8.</p>
<p>A.4 Detailed Prompts in Intermediate Checkpoints Evaluation</p>
<p>Here we present the prompts used for each dataset in the intermediate checkpoints evaluation.Since the prompts for datasets (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, CommonsenseQA, MMLU, CEval, CMMLU) are concatenated with the questions and the answers (options), we have not listed them separately.Below, we provide the prompts used for other datasets.→ sqrt {{ x -2}}}}{{\ sqrt {{5 -x }}}} $ .}}\nSolution:" BOT : The expressions inside each square root must be non -→ negative .Therefore , $x -2 \ ge 0$ , so $x \ ge2$ , and $5 -x → \ ge 0$ , so $x \ le 5 $ .Also , the denominator cannot be → equal to zero , so $5 -x &gt;0 $ , which gives $x &lt;5 $ .Therefore , → the domain of the expression is $ \ boxed {{ [2 ,5)</p>
<p>A.5 Detailed Results</p>
<p>The evaluation results of all intermediate checkpoints are obtained using the OpenCompass framework [26].From figure 10, We can observe that the Chinchilla law already provides a good fit for OLMo and does not underestimate the loss when the model parameters are large but the data volume is small like MAP-Neo 7B and DeepSeek 67B.Such a phenomenon might be due to the distribution of the pre-training dataset.Deepseek's pre-training data distribution closely resembles that of NEO, with a higher proportion of Chinese data, code data, and high-quality filtered data compared to OLMo whose pre-training data in English</p>
<p>The Chinchilla scaling law is originally formulated for scenarios where the training data is relatively homogeneous and primarily English-centric.It tends to perform well under these conditions.However, when the training dataset size is smaller (e.g., significantly less than 500 billion tokens) and the model parameter count is high (e.g., 7 billion or more), the diversity of the data leads to a slower reduction in loss than predicted by Chinchilla.Conversely, with larger datasets (e.g., greater than 1.5 trillion tokens), the diversity contributes to a continued decrease in loss, diverging from the flattening and lower-bounded trajectory suggested by the B D β term in the Chinchilla law.Current evidence is limited as few models are pre-trained across multiple large high-quality corpora.Yi and Qwen [7] undergo A.9 Compression Rate
请扮演一个AI校对员，我需要你的专业技能 来 帮 助 我 校 对 一 段 文 本 。 这 段 文 本 是 我 通 过OCR技术从一份原始文档中提取出来的， 我怀疑在字符识别的过程中可能发生了一些 错误。具体来说，可能存在拼写错误、语法 错误、标点用错或者格式排列问题。请特别 注意生成的内容中有很多识别错误的空格与 换行符。请将段落整理成正确的语义通顺的 格式。输出格式应为组织完善的Markdown 内 容。不能改变语言，即不能将中文内容改为 英 文 。 一 些 内 容 是 混 合 语 言 的 ， 即 中 文 主 要内容夹杂英文符号，请按照原段落位置的 语言输出。下面是我提供的文本内容，请你 帮我仔细检查并校对，请直接输出修订后文 本，并不要包含其他内容。{内容} 你应该输 出： ➤ Demonstrations
English Content Before Post-processing English Content After Post-processing</p>
<p>T h e D ev elo p i n g P a th o f C i v il S er v a n t S y stem i n C h i n a ：B ased on C o m p reh en siv e I n ter p reta ti on of C i vi l S er va n t L a w A bst r a c t：C iv i l S e r va nt L a W i S the f h-st com pre he nsi v e l a w of hum an m an a g em e nt of c iv i l se rv an t i n our country．T he civil serv an t system has undergone a great leap forward from Temporary Regulation of C i v i l S erv a nts to C }vi l S e rv a n t L a w ．C om par i ng to the T emp or a ry R eg ula tio n C }vi l S e rv a n ts ，C i l S e r va n t W ha s ma ny ne w c ontents i nc lud ing new co nnota tio n of the co nc epts an d som e new rul e s that are written into the law fo r the fi rst time．T here are alSO some adiustments to the former articles．K ey W or ds ：C i v i l serv ant；D e v el opi ng P a th ；C i vi l S e rv a nt L a w 1 1 2</p>
<p>The Developing Path of the Civil Servant System in China: Based on Comprehensive Interpretation of the Civil Servant Law Abstract: The Civil Servant Law is the first comprehensive law on human resource management for civil servants in our country.The civil servant system has made a great leap forward from the Temporary Regulation of Civil Servants to the Civil Servant Law.Compared to the Temporary Regulation of Civil Servants, the Civil Servant Law contains many new contents, including new connotations of concepts and some new rules that are written into the law for the first time.There are also some adjustments to the former articles.Keywords: Civil servant; Developing path; Civil Servant Law 112</p>
<p>Chinese Content Before Post-processing Chinese Content After Post-processing
路 上 只 我 一 个 人 ， 背 着 手 踱 着 。 这一 片天地 好像 是 我的 ；我 也像 超 出 了 平常的自己， 到 了 另一世 界里。 我 爱 热 闹 ， 也 爱 冷 静 ； 爱 群 居 ， 也 爱 独处。像 今 晚上， 一 个 人 在这苍茫的 月 下，什 么都 可以 想， 什么 都 可以 不 想 ，便 觉是 个自 由的 人。白 天里 一定 要 做 的 事 ， 一 定 要 说 的 话 ， 现 在 都 可 不 理 。 这 是 独 处 的 妙 处 ， 我 且 受 用 这 无 边 的荷香 月 色 好了。 路上只我一个人，背着手踱着。这一片天地 好像是我的；我也像超出了平常的自己，到 了另一世界里。我爱热闹，也爱冷静；爱群 居，也爱独处。像今晚上，一个人在这苍茫 的月下，什么都可以想，什么都可以不想， 便觉是个自由的人。白天里一定要做的事， 一定要说的话，现在都可不理。这是独处的 妙处，我且受用这无边的荷香月色好了。
Figure 1 :
1
Figure 1: MAP-Neo shows impressive performance on base (Left) and chat (Right) models compared to both popular open-weight and recent transparent large language models with similar sizes.</p>
<p>Figure 2 :
2
Figure 2: Statistics of the Matrix Pile Data Distribution: The inner pie chart represents the language distribution, while the outer loop indicates the proportion of meta-categories in the corpus.</p>
<p>(a) Re-processing retention rates for the corpora in §4.1.(b)Processing retention rates for the corpora crawled from scratch in §4.2.</p>
<p>Figure 3 :
3
Figure 3: Funnel Diagram for the two main data pipelines.The darker part of each row represents the retention proportion for each processing step and the lighter one for the filtered corpora.</p>
<p>Figure 4 :
4
Figure 4: The document conversion framework is composed of various sub-models for different parts.</p>
<p>Figure 6 :
6
Figure 6: The training loss value is represented by the blue line.The Chinchilla law prediction is shown in yellow, and the NEO scaling law prediction is depicted in green.We fit the Chinchilla law and NEO law on 250M, 460M, and 980M and predict the model behavior on both training samples and samples from the 7B model.</p>
<p>Figure 7 :
7
Figure 7: The loss curve of Chinchilla Law prediction and the NEO Scaling law prediction for the DeepSeek LLM.We use loss values from both 7B and 67B for fitting and prediction.</p>
<p>University of Waterloo, 01.AI, Data &amp; Pretrain &amp; Evaluation &amp; Model Architecture &amp; Codebase &amp; Alignment • Scott Qu, M-A-P, University of Manchester, 01.AI, Codebase &amp; Model Architecture &amp; Infra &amp; Pretrain • Jiaheng Liu, M-A-P, Scaling Law &amp; Alignment Core Contributors: (Alphabet Order) • Chenchen Zhang, Independent Researcher, Pretrain • Chenghua Lin.M-A-P, University of Manchester, Data • Chou Leuang Yu, CUHK-Shenzhen, Alignment &amp; Data • Danny Pan, Peking University, Data &amp; Codebase • Esther Cheng, Peking University, Data • Jie Liu, The Chinese University of Hong Kong, Alignment • Qunshu Lin, 2077AI, Data • Raven Yuan, M-A-P, Pretrain &amp; Infra • Tuney Zheng, M-A-P, 01.AI, University of Waterloo, Pretrain &amp; Evaluation &amp; Alignment • Wei Pang, University of Waterloo, Data • Xinrun Du, M-A-P, 01.AI, Codebase &amp; Pretrain &amp; Alignment &amp; Evaluation • Yiming Liang, Institute of Automation, Chinese Academy of Sciences, Alignment &amp; Evaluation • Yinghao Ma, M-A-P, Queen Mary University of London, Scaling Law • Yizhi Li, M-A-P, University of Manchester, Data • Ziyang Ma, M-A-P, Shanghai Jiao Tong University, Alignment Contributors: (Alphabet Order) • Bill Lin, University of Southern California, Alignment • Emmanouil Benetos, Queen Mary University of London, Scaling Law • Huan Yang, University of Warwick , Ethics &amp; Societal Impact • Junting Zhou, Peking University, Data &amp; Scaling Law • Kaijing Ma, Tongji University, Data • Minghao Liu, 2077AI, Data • Morry Niu, 01.AI, Codebase • Noah Wang, 01.AI, Alignment • Quehry Que, Independent Researcher, Data • Ruibo Liu, Dartmouth University, Pretrain &amp; Model Architecture • Sine Liu, Independent Researcher, Infra • Shawn Guo, 01.AI, Data • Soren Gao, Fudan University, Tokenization • Wangchunshu Zhou, M-A-P &amp; AIWaves Inc., Data • Xinyue Zhang, Unity, Ethics &amp; Data • Yizhi Zhou, Nanjing University, Data • Yubo Wang, University of Waterloo, Pretrain • Yuelin Bai, M-A-P, Shenzhen Institute of Advanced Technology, CAS, Data • Yuhan Zhang, M-A-P, Data • Yuxiang Zhang, M-A-P, Waseda University, Codebase &amp; Evaluation &amp; Data • Zenith Wang, Independent Researcher, Data • Zhenzhu Yang, China University of Geosciences Beijing, Ethics &amp; Data • Zijian Zhao, 2077AI, Data Advisors: • Jiajun Zhang, Wuhan AI Research, Institute of Automation, Chinese Academy of Sciences • Wanli Ouyang, The Chinese University of Hong Kong, Shanghai AI Lab • Wenhao Huang, 01.AI • Wenhu Chen, University of Waterloo 14 Multimodal Art Projection Multimodal Art Projection (M-A-P) is an open-source research community.The community members are working on Artificial Intelligence-Generated Content (AIGC) topics, including text, audio, and vision modalities.We aim to prompt open research on large language/music/multimodal models (LLMs/LMMs) training, data collection, and development of fun applications.</p>
<p>Figure 8 :
8
Figure 8: Code modification for our training framework modification.</p>
<p>BoolQ</p>
<p>HUMAN : Passage : { passage }\nQuestion : { question }? BOT : Answer : No / Yes HumanEval HUMAN : Complete the following python code :\n{ prompt } MBPP HUMAN : You are an expert Python programmer , and here is your → task : Write a function to find the similar elements from → the given two tuple lists .Your code should pass these → tests :\n \n assert similar_elements ((3 , 4 , 5 , 6) ,(5 , 7 , 4 , →</p>
<p>NaturalQuestionsHUMAN:</p>
<p>Answer these questions , your answer should be as simple → as possible , start your answer with the prompt \ ' The → answer is \ '.\nQ : { question }? BOT : A : Triviaqa HUMAN : Answer these questions , your answer should be as simple → as possible , start your answer with the prompt \ ' The → answer is \ '.\nQ : { question }? BOT : A : SQuAD 2.0 HUMAN : { context }\nAccording to the above passage , answer the → following question .If it is impossible to answer → according to the passage , answer ' impossible to answer ':\n → Question : { question } BOT : Answer : MATH HUMAN : Problem :\nFind the domain of the expression $ \ frac {{\</p>
<p>Figure 9 :
9
Figure 9: Performance of fundamental phase intermediate checkpoints on MMLU, CEvel, CMMLU, Hellaswag, GSM8K, and ARC-c.</p>
<p>Figure 10 :
10
Figure 10: The loss curve, Chinchilla Law prediction and the NEO Scaling law prediction of the OLMo LLM.We use loss values from both 1B and 7B for fitting and prediction.</p>
<p>Table 2 :
22.9511.5773.3111.4183.6992.5584 Matrix Data Pile
Average Compression Rates by Category.These subsets are not uniformly proportioned in the training set.A detailed distribution is shown in Appendix 18.Code HQ cn HQ en Web cn Web en Others</p>
<p>Table 3 :
3
The composition sources of re-processed English web subset.The proportion denotes dividing the size of the current dataset by the total size of the whole dataset.
DatasetPartsUTF-8 bytes (TB) Availability Proportion (%)RedPajama-Data-V2 [25] Head and Middle 200Public92.38Dolma [95]CC6.4Public2.96Cultrax [72]EN1.2Public0.55Amber [66]Refined-Web4.23Public1.95SlimPajama [94]Whole2.43Public1.12Falcon [74]Whole1.01Public0.47CultraY [100]EN1.24Public0.57</p>
<p>Table 4 :
4
The composition sources of the Chinese web subset.
DatasetPartsUTF-8 bytes (TB)AvailabilityProportion (%)Crawled Web DataWhole14.3Self-constructed80.60CCIWhole0.10Public0.59Chinesewebtext [14] Whole1.40Public7.89Wanjuan [38]Text0.57Public3.19Yayi2 [69]Whole0.49Public2.76Cultrax [72]ZH0.28Public1.56Skypile [109]Whole0.60Public3.41</p>
<p>• Model Refinement and Iteration: Integrate unconfirmed positive data from prior iterations into the positive samples to diversify the FastText model's training base.Subsequently, initiate a new iteration cycle beginning from the training stage.</p>
<p>Table 5 :
5
Model architecture details.We list the number of layers, d model , the number of attention heads, and attention head size.The feed-forward size d ff is always 8 × d model .
Model# Layers # Heads dmodel # Feedforward dims # KV headsMAP-Neo 2B1882048163841MAP-Neo 7B2816307224576166 Pre-training
In the pre-training process, we employ a two-stage pre-training strategy to train the MAP-Neo model.The first stage termed the fundamental phase, involves training the model on a vast corpus of generic</p>
<p>Table 6 :
6
Model training details.
PhasesLearning Rate Weight Decay Warmup Ratio BatchsizePre-training (Fundamental Phase)2e-40.10.00551024Pre-training (Decay Phase)2e-40.101024SFT (Fundamental Phase)2e-500.05512SFT (Chat Phase)2e-500.05512Iterative DPO5e-600.1256</p>
<p>Table 7 :
7
The length-controlled win rate of MAP-Neo at different iterations on the AlpacaEval2.0leaderboard.For "SFT", we report the performance of our model using two-phase SFT.
ModelSFT Iteration 1 Iteration 2 Iteration 3LC Win Rate (%) 9.7710.0215.5916.658 Scaling Law of MAP-Neo8.1 Problem Definition</p>
<p>)
1.3 1.4 250M params MAP-Neo model 1.5 Loss Value Chinchilla Law NEO Scaling Law460M params MAP-Neo model Loss Value Chinchilla Law NEO Scaling Law980M params MAP-Neo model Loss Value Chinchilla Law NEO Scaling Law7790M params MAP-Neo model Loss Value Chinchilla Law NEO Scaling Law1.2loss Values1.0 1.10.90.80.70.60200400 # Tokens (B) 600800 10000200400 # Tokens (B) 600800 10000200400 # Tokens (B) 600800 10000 500 1000 1500 2000 2500 3000 # Tokens (B)</p>
<p>Table 8 :
8
Comparison of parametric fitting on R 2 and Huber Loss of different scaling laws.
Paramatic fitR 2 Value (train) ↑ Huber Loss (train) ↓ R 2 Value (test) ↑ Huber Loss (test) ↓Chinchilla Law0.24830.16650.43080.3372OpenAI Law0.22681.0424-0.29160.6023SMS Law0.24840.16650.43060.3375NEO Scaling Law0.73610.29610.67200.2081</p>
<p>Table 9 :
9
Performance comparison of various base models on different benchmarks.The best results are in blue , the second-best results are underline, and the third-best results are in fbox .
DatasetLLama3-8B Mistral-7B LLama2-7B Amber-7B OLMo-7B OLMo-1.7-7B Pythia-6.9B Pythia-12B MAP-Neo-7BStandard BenchmarksBoolQ66.8264.170.6763.5268.4170.4962.4561.0781.07PIQA81.1281.1878.1876.827980.2575.5276.1776.55SIQA47.3447.1345.5042.8944.1154.7142.3244.3268.22HellaSwag74.5276.4971.2766.7670.3272.3759.663.0470.74WinoGrande72.3875.369.5364.6466.5469.2260.8563.6959.83ARC-c79.6671.5335.9324.4124.0749.8322.7125.0868.14OpenBookQA-Fact69.081.042.6026.624.664.42528.682.0CommonsensQA69.767.5766.505760.4469.0455.4554.7969.94MMLU-AVG66.5264.0446.8028.0728.5153.5226.3927.0658.14<em>-humanities70.4168.0451.4730.1725.5255.0326.8727.3960.7</em>-stem56.2253.2138.0227.6628.6844.1726.7728.1349.84<em>-social-science76.073.6552.2027.1830.0562.1924.3226.2666.78</em>-other68.9467.049.9927.3729.8657.6727.2525.9159.73Code GenerationHumaneval33.528.013.413.411.617.19.18.523.8Humaneval-Plus29.323.211.612.29.815.28.57.320.1MBPP61.446.829.122.82732.316.115.634.9MBPP-Plus51.638.922.818.521.225.713.211.129.9World KnowledgeNQ10.149.315.073.10.661.020.861.839.97TriviaQA51.9456.4752.4426.6531.9745.1616.9724.3142.36Reading ComprehensionSQuAD2.040.8812.5341.3231.1527.0530.4322.5423.1130.98ExamsMATH20.7615.746.143.881.64.863.824.5420.7GSM8K54.7447.4616.223.645.8428.433.413.9453.68ChineseC-EVAL-AVG49.8347.5432.3723.8227.3935.2124.6424.8257.68<em>-stem45.2644.7428.2822.3625.7532.3623.9427.2750.35</em>-social-science58.0954.839.2225.9531.8740.4326.3423.7870.23<em>-humanities50.651.5237.1121.1926.2935.521.720.0563.49</em>-other49.8442.0628.8427.1627.435.3627.2826.0853.78<em>-hard32.4133.9725.2119.6327.1229.1622.9927.0541.07CMMLU-AVG50.7244.6331.8525.7725.5336.7425.3424.8855.1</em>-humanities53.144.5932.5024.8626.6537.0425.8125.4162.24<em>-stem43.5937.8229.0525.6125.2431.9424.2923.745.62</em>-social-science52.5946.3732.6025.8325.1738.1425.7825.1759.39<em>-other53.9849.8333.3526.6525.4339.8825.4725.3353.39</em>-china-specific44.8140.8429.2724.9624.9734.9126.525.3455.84</p>
<p>Table 10 :
10
Performance comparison of various aligned models on different benchmarks.The best results are in blue , the second-best results are underline, and the third-best results are in fbox .
DatasetLLama-3-8B (Instruct)Mistral-7B (Instruct-v0.2)LLama-2-7B Amber-7B OLMo-7B MAP-Neo-7B MAP-Neo-7B (Chat) (Chat) (Instruct) (SFT) (Instruct)Chat BenchmarksAlignBench6.175.274.332.853.24.635.04AlpacaEval22.917.15.41.213.649.7716.65Arena-Hard20.612.64.61.21.71011.5CHC-Bench5.536.864.73.133.916.147.42MT-Bench8.17.56.65.25.37.17.1Standard BenchmarksBoolQ75.0582.8774.7766.5172.284.5981.28PIQA80.0982.4376.0177.4875.376.0675.24SIQA51.2350.4148.7244.8848.4151.6952.25HellaSwag71.3980.1171.3267.8475.1868.568.7WinoGrande71.973.468.3564.9666.6965.1966.06ARC-c81.3673.5655.5937.2957.638080OpenBookQA-Fact8785.474.436.67485.485.4CommonsenseQA73.5575.8470.1160.2863.4768.3970.35MMLU-Pro38.1230.8621.6114.6516.2728.0828.74MMLU67.160.8148.2238.847.4758.2858.28<em>-humanities70.6766.5852.7139.1948.3360.460.85</em>-stem56.9750.0137.9833.783851.8652.29<em>-social-science76.969.7555.8142.8556.5766.1965.6</em>-other69.362.5551.6942.0352.0658.2657.68Code GenerationHumanEval48.842.11417.714.6334.145.1HumanEval-Plus44.536.012.21412.831.737.8MBPP70.139.729.128.020.144.444.4MBPP-Plus59.333.322.823.516.738.136World KnowledgeNQ8.251.141.53.020.533.82.41Triviaqa56.3245.0646.7930.9527.9138.7727.09Reading ComprehensionSQuAD2.066.9915.0119.6113.1242.1344.5825.2ExamsMATH29.2813.146.94.21.835.3635.58GSM8K79.2349.2267.5913.572.0273.16ChineseC-Eval50.7643.7235.6726.2935.1855.4256.97<em>-stem47.4741.3532.5923.9931.4347.3749.08</em>-social-science57.0547.7540.0426.7742.1369.2170.75<em>-humanities48.3247.3336.9628.2634.0363.1763.14</em>-other53.4840.7436.0128.0636.8149.7852.63<em>-hard31.0427.3228.4522.7726.3338.4139.55CMMLU51.6842.6733.930.0935.5555.2755.01</em>-humanities52.5542.0135.4530.4834.7863.462.99<em>-stem44.0936.8229.3326.7630.3647.2946.69</em>-social-science53.0244.4134.5530.9738.0457.5557.79<em>-other57.5847.336.7732.2538.4553.9353.44</em>-china-specific45.8639.2232.6428.3833.9755.6955.9</p>
<p>DONE ] \n \n HUMAN : You are an expert Python programmer , and here is your → task : Write a python function to identify non -prime → numbers .Your code should pass these tests :\n \n assert DONE ] \n \n HUMAN : You are an expert Python programmer , and here is your → task : Write a function to find the largest integers from → a given list of numbers using heap queue algorithm .Your → code should pass these tests :\n \n assert he ap <em>q ue ue _l ar ge st → ( [25 , 35 , 22 , 85 , 14 , 65 , 75 , 22 , 58] ,3) ==[85 , 75 , 65] \n → assert hea p</em> qu eu e_ la rg es t ( [25 , 35 , 22 , 85 , 14 , 65 , 75 , → 22 , 58] ,2) ==[85 , 75] \n assert h eap _q ue ue _l ar ge st ( [25 , → 35 , 22 , 85 , 14 , 65 , 75 , 22 , 58] ,5) ==[85 , 75 , 65 , 58 , 35] → \n BOT : [ BEGIN ]\n ' import heapq as hq \ r\ndef he ap _q ue ue _l ar ge st ( nums → , n ) :\ r\n largest_nums = hq .nlargest (n , nums ) \ r\n return → largest_nums ' \n[ DONE ] \n \n HUMAN : You are an expert Python programmer , and here is your → task : { text } Your code should pass these tests :\n \n { → test_list } \n BOT : [ BEGIN ]\n
BOT : [ BEGIN ]\n ' def similar_elements ( test_tup1 , test_tup2 ) :\ r\n→ res = tuple ( set ( test_tup1 ) &amp; set ( test_tup2 ) ) \ r\n return (→ res ) ' \n[ → is_not_prime (2) == False \n assert is_not_prime (10) ==→ True \n assert is_not_prime (35) == True \nBOT : [ BEGIN ]\n ' import math \ r\ndef is_not_prime ( n ) :\ r\nresult→ = False \ r\nfor i in range (2 , int ( math . sqrt ( n ) ) + 1) :\ r\n→if n % i == 0:\ r\nresult = True \ r\n→ return result ' \n[) ) == (4 , 5)\n assert similar_elements ((1 , 2 , 3 , 4) ,(5 ,→ 4 , 3 , 7) ) == (3 , 4) \n assert similar_elements ((11 , 12 ,→ 14 , 13) ,(17 , 15 , 14 , 13) ) == (13 , 14) \n</p>
<p>}} $ .\nFinal→Answer: The final answer is $ [2 ,5) $ .I hope it is → correct .\nHUMAN:Problem :\nIf $ \ det \ mathbf {{ A }} = 2 $ and $ \ det \ mathbf {{ B → }} = 12 , $ then find $ \ det (\ mathbf {{ A }} \ mathbf {{ B }}) .$\n→Solution :BOT : We have that $ \ det (\ mathbf {{ A }} \ mathbf {{ B }}) = (\ det \ → mathbf {{ A }}) (\ det \ mathbf {{ B }}) = (2) (12) = \ boxed → {{24}}.$\nFinalAnswer : The final answer is $24$ .I hope → it is correct .\nHUMAN : Problem :\nTerrell usually lifts two 20 -pound weights 12 → times .If he uses two 15 -pound weights instead , how many → times must Terrell lift them in order to lift the same → total weight ?\nSolution : BOT : If Terrell lifts two 20 -pound weights 12 times , he lifts a → total of $2 \ cdot 12\ cdot20 =480 $ pounds of weight .If he → lifts two 15 -pound weights instead for $n$ times , he will → lift a total of $2 \ cdot15 \ cdot n =30 n$ pounds of weight .→ Equating this to 480 pounds , we can solve for $n$ : \ → begin {{ align <em>}} 30 n &amp;=480\\ \ Rightarrow \ qquad n → &amp;=480/30=\ boxed {{16}} \ end {{ align </em>}}\nFinal Answer : The → final answer is $16$ .I hope it is correct .\nHUMAN : Problem :\nIf the system of equations : \ begin {{ align <em>}} 6 → x -4 y &amp;= a ,\\ 6y -9 x &amp;= b .\ end {{ align </em>}} has a solution $ (x , → y ) $ where $x$ and $y$ are both nonzero , find $ \ frac {{ a → }}{{ b }} , $ assuming $b$ is nonzero .\nSolution: BOT : If we multiply the first equation by $ -\ frac {{3}}{{2}} $ , → we obtain $$6y -9 x = -\ frac {{3}}{{2}} a .$$Since we also know → that $6y -9 x = b$ , we have $$ -\ frac {{3}}{{2}} a = b \ → Rightarrow \ frac {{ a }}{{ b }}=\ boxed {{ -\ frac {{2}}{{3}}}}.→ $$\nFinal Answer : The final answer is $ -\ frac {{2}}{{3}} $ .→ 10 minutes = 30 minutes .\nAndthey want to include 30 → minutes for lunch each day , so 120 minutes for breaks + → 30 minutes for snack breaks + 30 minutes for lunch = 180 → minutes , or 180 / 60 minutes per hour = 3 extra hours .\nSo→ Angelo and Melanie want to plan 12 hours to study + 3 → hours of breaks = 15 hours total .\nTheywant to study no → more than 4 hours each day , 15 hours / 4 hours each day = → 3.75\nThey will need to plan to study 4 days to allow for → all the time they need .\nTheanswer is 4 , → in the first three baskets .\nThenumber of apples in the → fourth basket is 9 -2=7\nThere are also 15 -2=13 oranges in → the fourth basket\nThe combined number of oranges and → apples in the fourth basket is 13+7=20\nThe fourth basket → also contains 14 -2=12 bananas .\nIntotal , the fourth → basket has 20+12=32 fruits .\nThefour baskets together → have 32+114=146 fruits .\nTheanswer is 146\n,
HUMAN : Question : Mark ' s basketball team scores 25 2 pointers , 8→ 3 pointers and 10 free throws . Their opponents score→ double the 2 pointers but half the 3 pointers and free→ throws . What ' s the total number of points scored by both HUMAN : Question : { question }\nLet ' s think step by step\nAnswer :→ teams added together ?\nLet ' s think step by step\nAnswer : ,TheoremQABOT : Mark ' s team scores 25 2 pointers , meaning they scored HUAMN : You are a mathematician , you are supposed to answer the → 25<em>2= 50 points in 2 pointers .\nHis team also scores 6 3 → given question . You need to output the answer in your → pointers , meaning they scored 8</em>3= 24 points in 3 → final sentence like " Therefore , the answer is ...". The → pointers\nThey scored 10 free throws , and free throws → answer can only be one of the following forms :\n1. a → count as one point so they scored 10<em>1=10 points in free → numerical value like 0.1 , no symbol and no unit at all .\n → throws .\nAll together his team scored 50+24+10= 84 points\n → 2. a list of number like [2 , 3 , 4].\n3. True / False .\n4. an → Mark ' s opponents scored double his team ' s number of 2 → option like ( a ) , ( b ) , ( c ) , ( d )\nQuestion : { Question }\nLet \ ' → pointers , meaning they scored 50</em>2=100 points in 2 → pointers .\nHis opponents scored half his team ' s number of → s think step by step .→ 3 pointers , meaning they scored 24/2= 12 points in 3→ pointers .\nThey also scored half Mark ' s team ' s points in→ free throws , meaning they scored 10/2=5 points in free→ throws .\nAll together Mark ' s opponents scored 100+12+5=117→ points\nThe total score for the game is both team ' s→ scores added together , so it is 84+117=201 points\nThe→ answer is 201\n,→ I hope it is correct .\nHUMAN : Problem :\n{ problem }\nSolution :\n→ If she buys 2/5 times more of each item , what would beGSM8K → the total number of the items she will have if she→ currently has 60 marbles ?\nLet ' s think step by step\nAnswer HUMAN : Question : Angelo and Melanie want to plan how many hours → over the next week they should study together for their → : ,→ test next week . They have 2 chapters of their textbook to BOT : When Bella buys 2/5 times more marbles , she ' ll have → study and 4 worksheets to memorize . They figure out that → increased the number of marbles by 2/5<em>60 = 24\nThe total → they should dedicate 3 hours to each chapter of their → number of marbles she ' ll have is 60+24 = 84\nIf Bella → textbook and 1.5 hours for each worksheet . If they plan → currently has 60 marbles , and she has two times as many → to study no more than 4 hours each day , how many days → marbles as frisbees , she has 60/2 = 30 frisbees .\nIf Bella → should they plan to study total over the next week if → buys 2/5 times more frisbees , she ' ll have 2/5</em>30 = 12 → they take a 10 -minute break every hour , include 3 10 -→ more frisbees .\nThe total number of frisbees she ' ll have → minute snack breaks each day , and 30 minutes for lunch → will increase to 30+12 = 42\nBella also has 20 more → each day ?\nLet ' s think step by step\nAnswer : , → frisbees than deck cards , meaning she has 30 -20 = 10 deck→ cards\nIf she buys 2/5 times more deck cards , she ' ll have BOT : Angelo and Melanie think they should dedicate 3 hours to → 2/5<em>10 = 4 more deck cards .\nThe total number of deck → each of the 2 chapters , 3 hours x 2 chapters = 6 hours → cards she ' ll have is 10+4 = 14\nTogether , Bella will have → total .\nFor the worksheets they plan to dedicate 1.5 hours → for each worksheet , 1.5 hours x 4 worksheets = 6 hours → a total of 14+42+84 = 140 items\nThe answer is 140\n,→ total .\nAngelo and Melanie need to start with planning 12 HUMAN : Question : A group of 4 fruit baskets contains 9 apples , → hours to study , at 4 hours a day , 12 / 4 = 3 days .\n → 15 oranges , and 14 bananas in the first three baskets and → However , they need to include time for breaks and lunch . → 2 less of each fruit in the fourth basket . How many → Every hour they want to include a 10 -minute break , so 12 → total hours x 10 minutes = 120 extra minutes for breaks .\n → fruits are there ?\nLet ' s think step by step\nAnswer : ,→ They also want to include 3 10 -minute snack breaks , 3 x
HUMAN : Question : Bella has two times as many marbles as → frisbees .She also has 20 more frisbees than deck cards .BOT : For the first three baskets , the number of apples and → oranges in one basket is 9+15=24\nIn total , together with → bananas , the number of fruits in one basket is 24+14=38 → for the first three baskets .\nSincethere are three → baskets each having 38 fruits , there are 3</em>38=114 fruits</p>
<p>Table 13 :
13
This table showcases evaluation results across a variety of datasets for models trained with different token amounts, ranging from 20B to 1859.86B.Additionally, results for models trained with 2099.84B to 3726.33B tokens can be found in Table14."Avg" represents the average over the benchmark.The "<em>" symbol refers to subsets within the MMLU, CMMLU, and C-Eval.
Dataset20.00B 60.00B 99.99B 359.97B 599.95B 859.93B 1099.91B 1299.90B 1599.88B 1859.86BStandard BenchmarksBoolQ58.8160.2858.9661.961.6262.2962.3563.6759.0261.35PIQA67.2570.3573.5676.0676.1277.6477.7577.5877.5877.91SIQA38.3341.0440.4341.7141.442.4842.9942.9942.4344.06HellaSwag32.5347.0752.0361.3263.6164.8365.7566.1167.3567.69WinoGrande52.0953.1253.255.2555.4157.3857.9358.0958.0959.67ARC-e35.2743.3951.1557.557.3257.558.0258.9162.0860.14ARC-c23.392023.7329.4928.1431.8632.232.232.5433.22OpenBookQA-Fact26.223.823.828.843.648.651.859.66270CommonsenseQA34.3248.3251.4359.5461.4363.7266.0964.9565.1965.44MMLU-AVG24.824.3826.7236.0643.9247.3247.9650.6551.1851.95</em>-humanities24.525.2526.7137.544.5849.2650.5852.9253.6254.29<em>-stem24.423.2626.7630.8336.8239.9840.8942.742.7244.24</em>-social-science22.823.5826.939.749.0753.7753.7157.7558.9358.73*-other27.5225.8726.5238.8948.950.1550.3753.4253.9454.62Code GenerationHumanEval0.612.444.276.17.327.937.327.329.156.1MBPP00.403.46.66.49.29.48.86.6World KnowledgeNQ0.081.552.85.15.797.517.849.349.038.01TriviaQA1.26.99.5419.6425.9722.2428.628.2234.1931.31Reading ComprehensionSQuAD2.04.5415.9424.227.0631.0531.4830.6812.5631.3525.76ExamsMATH0.61.221.162.622.83.183.63.823.444.24GSM8k1.590.760.994.097.669.7812.0511.5215.2414.48TheoremQA000.50.751.381.51.380.750.620.38</p>
<p>Table 14 :
14
This table showcases evaluation results across a variety of datasets for models trained with different token amounts, ranging from 2099.84B to 3726.33B.Additionally, results for models trained with 20B to 1859.86B tokens can be found in Table13."Avg" represents the average over the benchmark.The "<em>" symbol refers to subsets within the MMLU, CMMLU, and C-Eval.
Dataset2099.84B 2359.82B 2599.80B 2859.78B 3099.76B 3299.74B 3599.72B 3726.33BStandard BenchmarksBoolQ60.8963.1259.3664.5663.6763.1865.3566.09PIQA77.9178.0278.3578.5678.9478.6778.1378.29SIQA44.0644.0643.343.7144.0143.4544.6343.19HellaSwag68.668.5269.0469.7870.0670.0270.4670.17WinoGrande58.9659.8359.9160.0661.2559.7559.6760.46ARC-e62.4362.6163.3261.7361.7362.2662.4364.02ARC-c23.392037.2935.9335.5936.9534.5834.58OpenBookQA-Fact63.861.6606659.659.46962.2CommonsenseQA66.4265.7767.3267.9867.5767.5767.8167.73MMLU-AVG52.7253.2553.9354.7155.3455.855.4255.91</em>-humanities54.1856.7557.257.2558.3458.1958.7159.22<em>-stem44.4844.0844.1945.5645.9246.5346.4246.37</em>-social-science60.7261.0261.8763.4563.8764.8662.6363.72<em>-other55.955.9657.5857.4958.2358.6158.6159.33Code GenerationHumanEval8.543.666.716.717.323.669.769.15MBPP8.49.48.88.68.88.89.29World KnowledgeNQ10.9710.1910.0311.7710.6612.6311.4411.27TriviaQA36.5331.0637.939.2940.8141.2741.0839.54Reading ComprehensionSQuAD2.025.2926.9811.355.136.1816.6815.558.72ExamsMATH4.844.344.945.365.65.725.95.76GSM8k14.9417.3617.2918.9519.1819.7919.1121.3TheoremQA1.380.5132.3821.52.5ChineseC-EVAL-AVG52.5255.6257.457.0356.0257.5758.157.13</em>-stem44.7749.5251.8449.0846.5250.2650.2649.47<em>-social-science66.7170.371.6271.4370.7473.3373.0572.61</em>-humanities58.0858.661.9662.0961.1460.9661.4662.06<em>-other48.1850.3950.0353.3554.7953.1255.4152.03</em>-hard34.839.8944.0839.8736.4241.2638.6440.47CMMLU-AVG52.4554.7956.1556.6357.3358.1157.758.32<em>-humanities56.4260.2360.9761.096363.6963.7365.04</em>-stem42.1744.3845.9546.1646.5447.8246.6347.91<em>-social-science57.3459.2660.4261.7962.2762.9362.8363.25</em>-other53.5155.3457.2757.0957.457.8457.557.05*-china-specific54.8256.8658.1558.0759.5160.4760.7460.29</p>
<p>Table 15 :
15
This table shows the evaluation results across a variety of datasets for models of different train tokens in the decay phase, from 62.91B to 723.52B."Avg" represents the average over the benchmark.The "<em>" symbol refers to subsets within the MMLU, CMMLU, and C-Eval.
Dataset62.91B 104.86B 199.23B 293.60B 419.43B 524.29B 639.63B 723.52BStandard BenchmarksBoolQ52.5150.734763.765.3878.3270.3481.07PIQA75.4175.4175.7376.7177.0475.975.376.55SIQA48.3649.1350.3150.3151.2869.4568.7368.22HellaSwag62.7963.9865.1966.1766.4369.577070.74WinoGrande62.0463.6964.0165.7566.0659.4359.5959.83ARC-c58.6460.3463.7361.3668.4745.4263.3968.14OpenBookQA-Fact75.676.277.274.27979.673.482CommonsenseQA60.3663.0663.7264.5463.1468.9669.769.94MMLU-AVG52.5353.3154.8355.5156.1157.1757.3658.14</em>-humanities54.5957.4457.858.1259.560.7659.7760.7<em>-stem45.6845.5848.3747.2948.4849.8249.3149.84</em>-social-science59.660.6961.1963.9564.4564.7865.2766.78*-other53.9453.6555.4357.1456.1757.3159.4259.73Code GenerationHumanEval6.17.323.0511.590.6121.9520.1224.39MBPP20.82524.828.228.42727.827World KnowledgeNQ4.046.45.435.043.219.948.239.97TriviaQA15.2727.3124.7634.3244.0337.532.6642.36Reading ComprehensionSQuAD2.033.7213.5727.130.8916.829.5619.3730.98ExamsMATH6.628.6210.0812.8812.2414.0614.1214.66</p>
<p>Table 18 :
18
Detailed Compression Rates by Category and Dataset.
Category DatasetCompression RateSampled Code(cpp)2.988Sampled Code(Java)3.301Sampled Code(All)3.355CodeSampled Github2.988Sampled Code(Other)3.426CodeGPT-CN2.458Sampled LeetCode2.050The Stack V13.041COIG-PC1.835Sampled Novel1.284HQ cnSampled Reference Book1.240Exams High Quality2.290Zhihu High Quality1.377Zhihu Instruction1.434Arxiv High Quality2.976Sampled News Paper3.613HQ enSampled English Books2.079flan v23.645Huggingface Wiki3.520UltraTextbooks4.030AutoMathText2.756BioInstructQA3.284OthersSynthetic science exam instruction1.508open-web-math3.263SMolInstruct1.978Web cnCommon Crawl1.418Web enCommon Crawl3.699A.10 OCR Post Processing</p>
<p>Table 19 :
19
The OCR prompt templates with Demonstrations in Chinese and English Prompt Template for OCR Post-processing ➤ Prompt Templates Prompt for English Contents Prompt for Chinese Contents From an original document using OCR technology, there may be errors in character recognition, potentially including spelling mistakes, grammatical errors, incorrect punctuation, or formatting issues.Pay special attention to misplaced spaces and line breaks that often occur in OCR-generated content.I need you to reorganize the paragraph into a properly formatted and semantically coherent form.Here's the text I've provided.Kindly check and correct it meticulously.Please output only the revised text without including any additional content i.e. any comments from you.The output format should be a well-organized markdown content.Do not change the language, i.e.Do not change Chinese content to English.Some contents are mixed language i.e.Chinese main content with English symbols.Also, do not change the original language.Please do not generate any unrelated additional comments!Here's one of the texts that needs to be processed: {content} You should output:</p>
<p>https://github.com/multimodal-art-projection/MAP-NEO
https://spark.apache.org/
https://github.com/NVIDIA/Megatron-LM
https://github.com/VikParuchuri/marker
https://github.com/VikParuchuri/texify
https://github.com/PaddlePaddle/FastDeploy
https://huggingface.co/datasets/berkeley-nest/Nectar
https://huggingface.co/Nexusflow/Starling-RM-34B
https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh
Fraction of full brackets 【】 ≤ 0.1 -Fraction of digital words over non-punctuation words ≤ 0.3 -Lines ending with "readmore" etc. ≤ 0.3 Endings include: "readmore", "展开", "更多", "。。。" Lines starting with bullet point ≤ 0.9Bullet points: "•", "•", "⃝", "□", "※", "•" etc. Fraction of punctuation in words &gt; 0 -Fraction of unique words &gt; 0.1 -Entropy of unigram distribution ≥ 3 -Text quality score &gt; 0.4 Evaluated by fasttext Duplicates Filtering Fraction of characters in duplicate word 10-grams ≤ 0.60 -Fraction of characters in duplicate word 9-grams ≤ 0.60 -Fraction of characters in duplicate word 8-grams ≤ 0.60 -Fraction of characters in duplicate word 7-grams ≤ 0.60 -Fraction of characters in duplicate word 6-grams ≤ 0.60 -Fraction of characters in duplicate word 5-grams ≤ 0.60 -Fraction of characters in top word 4-grams ≤ 0.16 -Fraction of characters in top word 3-grams ≤ 0.18 -Fraction of characters in top word 2-grams ≤ 0.20 -Fraction of duplicate sentences ≤ 0.30 -Fraction of characters in duplicate sentences ≤ 0.20 -A AppendixA.1 Details of Heuristic Rules for English Texts Mean word length[3,10]-Lines that end with an ellipsis ≤ 0.2 Defined as ellipsis: '...', '. . .', '. . . . . .' Lines starting with bullet point ≤ 0.9Bullet points: "•", "•", "⃝", "□", "※", "•" etc. Words that contain no alphabetical character ≤ 0.Prohibited Words FilteringText should not contain words in the bad words list Words related to pornography, politics, violence, etc.A.2 Details of Heuristic Rules for Chinese Texts
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>The de-democratization of ai: Deep learning and the compute divide in artificial intelligence research. Nur Ahmed, Muntasir Wahed, arXiv:2010.155812020arXiv preprint</p>
<p>Llama 3 model card. A I , Meta , 2024</p>
<p>Ai Anthropic, The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. 2024</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marcodos Santos, Stephen Mcaleer, Albertq Jiang, Jia Deng, Stella Biderman, Sean Welleck, Llemma: An open language model for mathematics. </p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, Leandro Von Werra, Cosmopedia, 2024</p>
<p>Pythia: A suite for analyzing large language models across training and scaling. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, O' Kyle, Eric Brien, Mohammad Hallahan, Shivanshu Aflah Khan, Purohit, Sai Usvsn, Edward Prashanth, Raff, International Conference on Machine Learning. PMLR2023</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Nougat: Neural optical understanding for academic documents. Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic, 2023</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. Allan Ralph, Milton E Bradley, Terry, Biometrika. 00063444393/41952</p>
<p>On the resemblance and containment of documents. Andrei Z Broder, Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171). Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)IEEE1997</p>
<p>Chinesewebtext: Large-scale high-quality chinese web text extracted with effective evaluation model. Jianghao Chen, Pu Jian, Tengxiao Xi, Dongyi Yi, Qianlong Du, Chenglin Ding, Guibo Zhu, Chengqing Zong, Jinqiao Wang, Jiajun Zhang, 2023</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Theoremqa: A theorem-driven question answering dataset. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Agent-flan: Designing data and methods of effective agent tuning for large language models. Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Dahua Lin, Kai Chen, Feng Zhao, arXiv:2403.128812024arXiv preprint</p>
<p>Alexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Jameson Aragon, Arturo Rodríguez Fanlo, Simon Frieder, Simon Machado, Akshara Prabhakar, Ellie Thieu, Jiachen T Wang, Zirui Wang, Xindi Wu, Mengzhou Xia, Wenhan Jia, Jiatong Yu, Jun-Jie Zhu, Zhiyong Jason Ren, Sanjeev Arora, Danqi Chen, arXiv:2402.11111Language models as science tutors. 2024arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730Advances in neural information processing systems</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Journal of Machine Learning Research. 25702024</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, arXiv:1905.100442019arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Open innovation and withinindustry diversification in small and medium enterprises: The case of open source software firms. Evila Massimo G Colombo, Cristina Piva, Rossi-Lamastra, Research policy. 4352014</p>
<p>Redpajama: an open dataset for training large language models. 2023Together Computer</p>
<p>Opencompass: A universal evaluation platform for foundation models. 2023</p>
<p>Data colonialism: Rethinking big data's relation to the contemporary subject. Nick Couldry, Ulises A Mejias, Television &amp; New Media. 2042019</p>
<p>Deepseek-Ai , arXiv:2401.02954Deepseek llm: Scaling open-source language models with longtermism. 2024arXiv preprint</p>
<p>Composerx: Multi-agent symbolic music composition with llms. Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, Jiahao Pan, Ge Zhang, Hanfeng Lin, arXiv:2404.180812024arXiv preprint</p>
<p>Chinese tiny llm: Pretraining a chinese-centric large language model. Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, Ge Zhang, 2024</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-controlled alpacaeval: A simple way to debias automatic evaluators. 2024arXiv preprint</p>
<p>Identifying and characterizing highly similar notes in big clinical note datasets. Rodney A Gabriel, Tsung-Ting Kuo, Julian Mcauley, Chun-Nan Hsu, Journal of biomedical informatics. 822018</p>
<p>Openllama: An open reproduction of llama. Xinyang Geng, Hao Liu, May 2023</p>
<p>Similarity search in high dimensions via hashing. Aristides Gionis, Piotr Indyk, Rajeev Motwani, Vldb. 199999</p>
<p>Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, arXiv:2402.00838Accelerating the science of language models. 2024arXiv preprint</p>
<p>Deduplication of scholarly documents using locality sensitive hashing and word embeddings. Bikash Gyawali, Lucas Anastasiou, Petr Knoth, 2020</p>
<p>Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, Dahua Lin, 2023</p>
<p>Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset. Peter Henderson, * , Mark S Krass, * , Lucia Zheng, Neel Guha, Christopher D Manning, Dan Jurafsky, Daniel E Ho, 2022</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Scaling laws and interpretability of learning from repeated data. Danny Hernandez, Tom Brown, Tom Conerly, Nova Dassarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, arXiv:2205.104872022arXiv preprint</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Unveiling the potential of small language models with scalable training strategies. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, arXiv:2404.063952024arXiv preprint</p>
<p>Layoutlmv3: Pre-training for document ai with unified text and image masking. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, Advances in Neural Information Processing Systems. 362024</p>
<p>The distribution of the flora in the alpine zone. Paul Jaccard, New phytologist. 121912</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017arXiv preprint</p>
<p>Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, Tomas Mikolov, Fasttext, zip: Compressing text classification models. arXiv: Computation and Language,arXiv: Computation and Language. Nov 2016</p>
<p>The minipile challenge for data-efficient language models. Jean Kaddour, arXiv:2304.084422023arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Connected components in mapreduce and beyond. Raimondas Kiveris, Silvio Lattanzi, Vahab Mirrokni, Vibhor Rastogi, Sergei Vassilvitskii, Proceedings of the ACM Symposium on Cloud Computing. the ACM Symposium on Cloud Computing2014</p>
<p>Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, 2022Preprint</p>
<p>Hdltex: Hierarchical deep learning for text classification. Kamran Kowsari, Donald E Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi, , , Matthew S Gerber, Laura E Barnes, Machine Learning and Applications (ICMLA), 2017 16th IEEE. 2017</p>
<p>Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, arXiv:1808.062262018arXiv preprint</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini, arXiv:2107.06499Deduplicating training data makes language models better. 2021arXiv preprint</p>
<p>Chenxia Li, Ruoyu Guo, Jun Zhou, Mengtao An, Yuning Du, Lingfeng Zhu, Yi Liu, Xiaoguang Hu, Dianhai Yu, arXiv:2210.05391Pp-structurev2: A stronger document analysis system. 2022arXiv preprint</p>
<p>Cmmlu: Measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, arXiv:2306.092122023arXiv preprint</p>
<p>From live data to high-quality benchmarks: The arena-hard pipeline. Tianle Li, * , Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, April 2024</p>
<p>Alpacaeval: An automatic evaluator of instructionfollowing models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Iterative length-regularized direct preference optimization: A case study on improving 7b language models to gpt-4 level. Jie Liu, Zhanhui Zhou, Jiaheng Liu, Xingyuan Bu, Chao Yang, Zhong Han-Sen, Wanli Ouyang, arXiv:2406.118172024arXiv preprint</p>
<p>Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench: Benchmarking chinese alignment of large language models. 2023</p>
<p>. Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, Eric P Xing, 2023Llm360: Towards fully transparent open-source llms</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.136882023arXiv preprint</p>
<p>. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian Mcauley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, 2024Arjun GuhaNima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas WolfLeandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation</p>
<p>Yayi 2: Multilingual open-source large language models. Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, arXiv:2312.148622023arXiv preprint</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, arXiv:1809.027892018arXiv preprint</p>
<p>tiny-strange-textbooks (revision 6f304f1). Nam Pham, 2024</p>
<p>Thuat Nguyen, Chien Van Nguyen, Dac Viet, Hieu Lai, Man, Trung Nghia, Franck Ngo, Ryan A Dernoncourt, Thien Huu Rossi, Nguyen, arXiv:2309.09400Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages. 2023arXiv preprint</p>
<p>Openwebmath: An open dataset of high-quality mathematical web text. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, Jimmy Ba, 2023</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. 2023arXiv preprint</p>
<p>Mupt: A generative symbolic music pretrained transformer. Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, arXiv:2404.063932024arXiv preprint</p>
<p>. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George Van Den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat Mcaleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang , Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Laura Weidinger, Iason Gabriel, William Isaac2022Oriol Vinyals, Kareem AyoubAurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman; Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer; Jeff Stanway, Lorrayne BennettCyprien de Masson d'AutumeDemis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training gopher</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, 2023</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 10.48550/arXiv.1910.10683arXiv:1910.10683October 2019arXiv e-prints, art</p>
<p>Know what you don't know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.038222018arXiv preprint</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, arXiv:2403.055302024arXiv preprint</p>
<p>Data epistemologies, the coloniality of power, and resistance. Paola Ricaurte, Television &amp; New Media. 2042019</p>
<p>Bringing language models up to speed. Ronsor, Bigknow2022, BigKnow2022, 2023</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 6492021</p>
<p>Design and implementation of the sun network filesystem. Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, Bob Lyon, Proceedings of the summer 1985 USENIX conference. the summer 1985 USENIX conference1985</p>
<p>Maarten Sap, Derek Hannah Rashkin, Ronan Chen, Yejin Lebras, Choi, Socialiqa, arXiv:1904.09728Commonsense reasoning about social interactions. 2019arXiv preprint</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, Matthias Franc ¸ois Yvon, Jonathan Gallé, Alexander M Tow, Stella Rush, Albert Biderman, Pawan Webson, Thomas Sasanka Ammanamanchi, Benoît Wang, Niklas Sagot, Albert Muennighoff, Olatunji Villanova Del Moral, Rachel Ruwase, Stas Bawden, Angelina Bekman, Iz Mcmillan-Major, Huu Beltagy, Lucile Nguyen, Samson Saulnier, Pedro Ortiz Tan, Victor Suarez, Hugo Sanh, Yacine Laurenc ¸on, Julien Jernite, Margaret Launay, Colin Mitchell, Aaron Raffel, Adi Gokaslan, Aitor Simhi, Alham Soroa, Amit Fikri Aji, Anna Alfassy, Ariel Kreisberg Rogers, Canwen Nitzav, Chenghao Xu, Chris Mou, Christopher Emezue, Colin Klamm, Leong, David Daniel Van Strien, Dragomir Ifeoluwa Adelani, Eduardo González Radev, Efrat Ponferrada, Ethan Levkovizh, Eyal Kim, Francesco Bar Natan, Gérard De Toni, Germán Dupont, Giada Kruszewski, Hady Pistilli, Hamza Elsahar, Hieu Benyamina, Ian Tran, Idris Yu, Isaac Abdulmumin, Itziar Johnson, Javier Gonzalez-Dios, Jenny De La Rosa, Jesse Chim, Jian Dodge, Jonathan Zhu, Jörg Chang, Joseph Frohberg, Joydeep Tobing, Khalid Bhattacharjee, Kimbo Almubarak, Kyle Chen, Leandro Lo, Leon Von Werra, Long Weber, Loubna Phan, Ludovic Ben Allal, Manan Tanguy, Manuel Dey, Maraim Romero Muñoz, María Masoud, Mario Grandury, Max Šaško, Maximin Huang, Mayank Coavoux, Mike Singh, Tian-Jian, Minh Chien Jiang, Mohammad A Vu, Mustafa Jauhar, Nishant Ghaleb, Nora Subramani, Nurulaqilla Kassner, Olivier Khamis, Omar Nguyen, Ona Espejel, Paulo De Gibert, Peter Villegas, Pierre Henderson, Priscilla Colombo, Quentin Amuok, Rheza Lhoest, Rishi Harliman, Roberto Bommasani, Rui Luis López, Salomey Ribeiro, Sampo Osei, Sebastian Pyysalo, Shamik Nagel, Shamsuddeen Bose, Shanya Hassan Muhammad, Shayne Sharma, Somaieh Longpre, Stanislav Nikpoor, Suhas Silberberg, Sydney Pai, Tiago Zink, Timo Timponi Torrent, Tristan Schick, Valentin Thrush, Vassilina Danchev, Veronika Nikoulina, Violette Laippala, Vrinda Lepercq, Zaid Prabhu, Zeerak Alyafeai, Arun Talat, Benjamin Raja, Chenglei Heinzerling, Elizabeth Si, Sabrina J Salesky, Wilson Y Mielke, Abheesht Lee, Andrea Sharma, Antoine Santilli, Arnaud Chaffin, Debajyoti Stiegler, Eliza Datta, Gunjan Szczechla, Han Chhablani, Harshit Wang, Hendrik Pandey, Jason Strobelt, Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, Saiful Bari, Maged S Al-Shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Pierre Patrick Von Platen, Pierre Cornette, Rémi Franc ¸ois Lavallée, Samyam Lacroix, Sanchit Rajbhandari, Shaden Gandhi, Stéphane Smith, Suraj Requena, Tim Patil, Ahmed Dettmers, Amanpreet Baruwa, Anastasia Singh, Anne-Laure Cheveleva, Arjun Ligozat, Aurélie Subramonian, Charles Névéol, Dan Lovering, Deepak Garrette, Ehud Tunuguntla, Ekaterina Reiter, Ekaterina Taktasheva, Eli Voloshina, Genta Bogdanov, Hailey Indra Winata, Jan-Christoph Schoelkopf, Jekaterina Kalo, Jessica Novikova, Jordan Zosa Forde, Jungo Clive, Ken Kasai, Liam Kawamura, Marine Hazan, Miruna Carpuat, Najoung Clinciu, Newton Kim, Oleg Cheng, Omer Serikov, Oskar Antverg, Rui Van Der Wal, Ruochen Zhang, Sebastian Zhang, Shani Gehrmann, Tatiana Pais, Thomas Shavrina, Tian Scialom, Tomasz Yun, Verena Limisiewicz, Vitaly Rieser, Vladislav Protasov, Yada Mikhailov, Yonatan Pruksachatkun, Zachary Belinkov, Zdeněk Bamberger, Alice Kasner, Amanda Rueda, Amir Pestana, Ammar Feizpour, Amy Khan, Ana Faranak, Anthony Santos, Antigona Hevia, Arash Unldreaj, Arezoo Aghagol, Aycha Abdollahi, Azadeh Tammour, Bahareh Hajihosseini, Benjamin Behroozi, Bharat Ajibade, Carlos Saxena, Danish Muñoz Ferrandis, David Contractor, Davis Lansky, Douwe David, Kiela ; Chenxi, Chirag Zhou, Chuxin Jain, Clémentine Xu, Fourrier, Daniel Daniel León Periñán, Dian Molano, Enrique Yu, Fabio Manjavacas, Florian Barth, Gabriel Fuhrimann, Giyaseddin Altay, Gully Bayrak, Helena U Burns, Imane Vrabec, Ishani Bello, Jihyun Dash, John Kang, Jonas Giorgi, Jose Golde, Karthik David Posada, Lokesh Rangasai Sivaraman, Lu Bulchandani, Luisa Liu, Samuele Shinzato, Garda, Shubhanshu Shlok S Deshmukh, Sid Mishra, Simon Kiblawi, Sinee Ott, Srishti Sang-Aroonsiri, Stefan Kumar, Schweter, Hyung Won Chung, Jaesung Tae, Jason Phang. Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts; Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz; Bo Wang, Caio Brito,; Maria A Castillo; Marianna Nezhurina; Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert MartinMaiko Takeuchi, Marc PàmiesNovember 2023Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel de Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam ; Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis LabrakAlfredo Palasciano. Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. working paper or preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal policy optimization algorithms. 2017</p>
<p>Rico Sennrich, Barry Haddow, Alexandra Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. 2015arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Li, Daya Wu, Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>BIGPATENT: A large-scale dataset for abstractive and coherent summarization. Eva Sharma, Chen Li, Lu Wang, CoRR, abs/1906.037412019</p>
<p>Democratizing llms: An exploration of cost-performance trade-offs in self-refined open-source models. Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Zhenhailong Wang, Heng Ji, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Fast transformer decoding: One write-head is all you need. Noam Shazeer, 2019</p>
<p>Glu variants improve transformer. Noam Shazeer, 2020</p>
<p>SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, Nolan Dey, 2023slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama</p>
<p>Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo, Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. 2024arXiv preprint</p>
<p>Open innovation practices in smes and large enterprises. André Spithoven, Wim Vanhaverbeke, Nadine Roijakkers, Small business economics. 412013</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, 2023</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937Commonsenseqa: A question answering challenge targeting commonsense knowledge. 2018arXiv preprint</p>
<p>Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants. Teknium, 2023</p>
<p>Culturay: A large cleaned multilingual dataset of 75 languages. Huu Nguyen, Thuat Nguyen, Thien Nguyen, 2024</p>
<p>Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, 2023ARXIV</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.092882023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey EdunovarXiv preprintand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 2023</p>
<p>Not just bigger: Towards better-quality web corpora. Yannick Versley, Yana Panchenko, Proceedings of the seventh Web as Corpus Workshop (WAC7). the seventh Web as Corpus Workshop (WAC7)2012</p>
<p>Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, arXiv:2401.17268Yuchen Eleanor Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing. 2024arXiv preprint</p>
<p>Mmlu-pro: Towards more robust and challenging multi-task language understanding evaluation. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Li Tianle, Shiguang Guo, Aaran Arulraj, Xuan He, Weiming Ren, Ziyan Jiang, Alex Zhuang, Kai Wang, Richard Fan, Max Ku, Xiang Yue, Wenhu Chen, 2024Manuscript in preparation</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, Junran Peng, arXiv:2310.00746Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023arXiv preprint</p>
<p>Vary: Scaling up the vision vocabulary for large vision-language models. Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, Xiangyu Zhang, arXiv:2312.061092023arXiv preprint</p>
<p>Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou, Skywork: A more open bilingual foundation model. 2023</p>
<p>A systematic evaluation of large language models of code. Uri Frank F Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine Programming2022</p>
<p>Some things are more cringe than others: Preference optimization with the pairwise cringe loss. Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, Jason Weston, arXiv:2312.166822023arXiv preprint</p>
<p>Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang, arXiv:2402.12326Llm agents for psychology: A study on gamified assessments. 2024arXiv preprint</p>
<p>Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, arXiv:2403.046522024ai. arXiv preprint</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, arXiv:2402.093912024arXiv preprint</p>
<p>Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang, Yuhang Wu, Cong Liu, Ziya Zhou, arXiv:2402.16153Understanding and generating music intrinsically with llm. 2024arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, arXiv:2405.03548Mammoth2: Scaling instructions from the web. 2024arXiv preprint</p>
<p>Resilient distributed datasets: A {Fault-Tolerant} abstraction for {In-Memory} cluster computing. Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy Mc-Cauly, Michael J Franklin, Scott Shenker, Ion Stoica, 9th USENIX symposium on networked systems design and implementation. 201212</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.078302019arXiv preprint</p>
<p>Root mean square layer normalization. Biao Zhang, Rico Sennrich, 2019</p>
<p>Chinese open instruction generalist: A preliminary release. Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, arXiv:2304.079872023arXiv preprint</p>
<p>Don't trust chatgpt when your question is not in english: A study of multilingual abilities and types of llms. Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Automathtext: Autonomous data selection with language models for mathematical texts. Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao, arXiv:2402.076252024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Opencodeinterpreter: Integrating code generation with execution and refinement. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, Xiang Yue, arXiv:2402.146582024arXiv preprint</p>
<p>Starling-7b: Improving llm helpfulness &amp; harmlessness with rlaif. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Jiantao Jiao, November 2023</p>
<p>Structlm: Towards building generalist models for structured knowledge grounding. Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W Huang, Jie Fu, Xiang Yue, Wenhu Chen, 2024</p>
<p>Xiaomin Zhuang, Yufan Jiang, Qiaozhi He, Zhihua Wu, arXiv:2405.04828Chuxin: 1.6 b technical report. 2024arXiv preprint</p>
<p>. C-Eval-Avg Chinese, </p>
<p>List of open-source datasets used during pretraining. Dataset URL Agent. 16</p>
<p>. Loc-Pd-Books , </p>
<p>. Automathtext, </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9701 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9701</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9701</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-277955946</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.15205v1.pdf" target="_blank">Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges</a></p>
                <p><strong>Paper Abstract:</strong> Retrieval-augmented generation (RAG) enables large language models (LLMs) to generate answers with citations from source documents containing"ground truth", thereby reducing system hallucinations. A crucial factor in RAG evaluation is"support", whether the information in the cited documents supports the answer. To this end, we conducted a large-scale comparative study of 45 participant submissions on 36 topics to the TREC 2024 RAG Track, comparing an automatic LLM judge (GPT-4o) against human judges for support assessment. We considered two conditions: (1) fully manual assessments from scratch and (2) manual assessments with post-editing of LLM predictions. Our results indicate that for 56% of the manual from-scratch assessments, human and GPT-4o predictions match perfectly (on a three-level scale), increasing to 72% in the manual with post-editing condition. Furthermore, by carefully analyzing the disagreements in an unbiased study, we found that an independent human judge correlates better with GPT-4o than a human judge, suggesting that LLM judges can be a reliable alternative for support assessment. To conclude, we provide a qualitative analysis of human and GPT-4o errors to help guide future iterations of support assessment.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9701.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9701.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TREC-RAG Judge Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-judge (GPT-4o) versus Human Judges for Support Evaluation in the TREC 2024 RAG Track</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale empirical comparison of an LLM judge (GPT-4o) and trained human annotators on sentence-level 'support' judgments (Full/Partial/No Support) for citations in retrieval-augmented generation (RAG) answers across 45 participant runs and 36 topics in TREC 2024.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Retrieval-augmented generation (RAG) — sentence-level support evaluation (verifiability of answer sentences against cited passages).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o (primary); LLAMA-3.1 405B used for disagreement re-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Each answer sentence paired with its first cited passage was sent (one passage at a time) to GPT-4o (Microsoft Azure API) with a prompt asking for one of three labels: Full Support / Partial Support / No Support; label-only response, no explanation. The same prompt was used for LLAMA-3.1 in the disagreement re-assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>NIST-provided human annotators trained on the task; two annotation conditions: (1) manual from-scratch — human reads sentence + cited passage and labels FS/PS/NS, and (2) manual with post-editing — human reads sentence + passage and is shown the GPT-4o label as a reference then provides a label. Sparse annotation protocol: only the first cited passage per sentence was judged. Annotation counts: manual from-scratch: 6,742 annotations on 22 topics; manual with post-editing: 4,165 annotations on 14 topics.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Multiple agreement measures reported: percent perfect match between GPT-4o and human judgments — 56% (manual from-scratch) and 72% (manual with post-editing); run-level correlation Kendall's τ > 0.79 between GPT-4o and human weighted precision/recall scores; confusion-matrix breakdowns (see paper) showing humans more often label 'No Support' while GPT-4o prefers 'Partial Support'; unbiased disagreement study Cohen's κ values: independent human vs GPT-4o = 0.29 and 0.27 (two conditions) versus independent human vs NIST human = -0.03 and 0.07; LLAMA-3.1 vs GPT-4o Cohen's κ = 0.60 and 0.46, while LLAMA vs human = -0.20 and -0.06. Independent-judge match rates: independent judge matched NIST human 31% and GPT-4o 51% (manual from-scratch); 37% and 52% respectively (post-editing).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When using LLMs as judges instead of humans the paper identifies systematic differences rather than wholesale superiority: (1) LLM judges tend to assign higher support labels (more 'Partial'/'Full') and are less conservative than humans, reducing 'No Support' labels; (2) LLMs can miss subtle, sentence-final or fine-grained information and can conflate similar terms (lexical confusions), producing false positives (labeling 'Partial' when actual support is absent) or false negatives (missing support present at sentence end); (3) disagreement is concentrated in the middle of the support spectrum ('Partial Support' cases), indicating loss of human nuance in borderline cases; (4) LLMs produce different label distributions (more 'partial') which changes precision/recall trade-offs and can degrade fidelity to human conservatism.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete divergences drawn from the paper (Taylor Swift topic examples): (a) An answer sentence 'As she matured, her understanding of relationships evolved...' — Human: Full Support; GPT-4o: Partial Support (GPT-4o missed full corroboration). (b) Cases where GPT-4o labeled 'Partial Support' while human labeled 'No Support' when theme overlaps exist but specific supporting text is absent (false positives). (c) GPT-4o confusion of semantically similar roles (e.g., police vs security specialists) causing mislabeling. (d) GPT-4o bias toward 'full support' by not evaluating the whole sentence (especially information at the end), leading to an incorrectly elevated label. These examples are reported in Table 2, Table 4 and the Qualitative analysis / disagreement sections.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Counter-evidence and caveats reported in the paper: (1) High overall agreement: 56% exact label match from-scratch, rising to 72% with post-editing; run-level weighted precision/recall correlations exceed 0.79 (Kendall's τ), suggesting LLM judges can reliably reproduce relative system rankings. (2) Unbiased re-evaluation found the independent human judge and another LLM (LLAMA-3.1) agreed more with GPT-4o than with the original human annotator in many disagreement cases, suggesting some human annotations are themselves noisy. (3) LLM-to-LLM agreement is high (LLAMA vs GPT-4o κ up to 0.60), indicating consistency among LLM judges. (4) Showing GPT-4o labels to human annotators (post-editing) substantially increases agreement, implying hybrid workflows (LLM suggestions + human oversight) mitigate losses.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Abstract; Sections: 'Experimental Results' (Weighted Precision and Recall, Confusion Matrices), 'Annotator Disagreements' (disagreement re-evaluation, Cohen's κ), 'Qualitative analysis' (error examples), and Conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating Verifiability in Generative Search Engines <em>(Rating: 2)</em></li>
                <li>How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior <em>(Rating: 2)</em></li>
                <li>FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows" <em>(Rating: 1)</em></li>
                <li>RAGAs: Automated Evaluation of Retrieval Augmented Generation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9701",
    "paper_id": "paper-277955946",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "TREC-RAG Judge Comparison",
            "name_full": "Comparison of LLM-as-a-judge (GPT-4o) versus Human Judges for Support Evaluation in the TREC 2024 RAG Track",
            "brief_description": "Large-scale empirical comparison of an LLM judge (GPT-4o) and trained human annotators on sentence-level 'support' judgments (Full/Partial/No Support) for citations in retrieval-augmented generation (RAG) answers across 45 participant runs and 36 topics in TREC 2024.",
            "citation_title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges",
            "mention_or_use": "use",
            "task_domain": "Retrieval-augmented generation (RAG) — sentence-level support evaluation (verifiability of answer sentences against cited passages).",
            "llm_judge_model": "GPT-4o (primary); LLAMA-3.1 405B used for disagreement re-evaluation.",
            "llm_judge_setup": "Each answer sentence paired with its first cited passage was sent (one passage at a time) to GPT-4o (Microsoft Azure API) with a prompt asking for one of three labels: Full Support / Partial Support / No Support; label-only response, no explanation. The same prompt was used for LLAMA-3.1 in the disagreement re-assessment.",
            "human_evaluation_setup": "NIST-provided human annotators trained on the task; two annotation conditions: (1) manual from-scratch — human reads sentence + cited passage and labels FS/PS/NS, and (2) manual with post-editing — human reads sentence + passage and is shown the GPT-4o label as a reference then provides a label. Sparse annotation protocol: only the first cited passage per sentence was judged. Annotation counts: manual from-scratch: 6,742 annotations on 22 topics; manual with post-editing: 4,165 annotations on 14 topics.",
            "agreement_metric": "Multiple agreement measures reported: percent perfect match between GPT-4o and human judgments — 56% (manual from-scratch) and 72% (manual with post-editing); run-level correlation Kendall's τ &gt; 0.79 between GPT-4o and human weighted precision/recall scores; confusion-matrix breakdowns (see paper) showing humans more often label 'No Support' while GPT-4o prefers 'Partial Support'; unbiased disagreement study Cohen's κ values: independent human vs GPT-4o = 0.29 and 0.27 (two conditions) versus independent human vs NIST human = -0.03 and 0.07; LLAMA-3.1 vs GPT-4o Cohen's κ = 0.60 and 0.46, while LLAMA vs human = -0.20 and -0.06. Independent-judge match rates: independent judge matched NIST human 31% and GPT-4o 51% (manual from-scratch); 37% and 52% respectively (post-editing).",
            "losses_identified": "When using LLMs as judges instead of humans the paper identifies systematic differences rather than wholesale superiority: (1) LLM judges tend to assign higher support labels (more 'Partial'/'Full') and are less conservative than humans, reducing 'No Support' labels; (2) LLMs can miss subtle, sentence-final or fine-grained information and can conflate similar terms (lexical confusions), producing false positives (labeling 'Partial' when actual support is absent) or false negatives (missing support present at sentence end); (3) disagreement is concentrated in the middle of the support spectrum ('Partial Support' cases), indicating loss of human nuance in borderline cases; (4) LLMs produce different label distributions (more 'partial') which changes precision/recall trade-offs and can degrade fidelity to human conservatism.",
            "examples_of_loss": "Concrete divergences drawn from the paper (Taylor Swift topic examples): (a) An answer sentence 'As she matured, her understanding of relationships evolved...' — Human: Full Support; GPT-4o: Partial Support (GPT-4o missed full corroboration). (b) Cases where GPT-4o labeled 'Partial Support' while human labeled 'No Support' when theme overlaps exist but specific supporting text is absent (false positives). (c) GPT-4o confusion of semantically similar roles (e.g., police vs security specialists) causing mislabeling. (d) GPT-4o bias toward 'full support' by not evaluating the whole sentence (especially information at the end), leading to an incorrectly elevated label. These examples are reported in Table 2, Table 4 and the Qualitative analysis / disagreement sections.",
            "counterexamples_or_caveats": "Counter-evidence and caveats reported in the paper: (1) High overall agreement: 56% exact label match from-scratch, rising to 72% with post-editing; run-level weighted precision/recall correlations exceed 0.79 (Kendall's τ), suggesting LLM judges can reliably reproduce relative system rankings. (2) Unbiased re-evaluation found the independent human judge and another LLM (LLAMA-3.1) agreed more with GPT-4o than with the original human annotator in many disagreement cases, suggesting some human annotations are themselves noisy. (3) LLM-to-LLM agreement is high (LLAMA vs GPT-4o κ up to 0.60), indicating consistency among LLM judges. (4) Showing GPT-4o labels to human annotators (post-editing) substantially increases agreement, implying hybrid workflows (LLM suggestions + human oversight) mitigate losses.",
            "paper_reference": "Abstract; Sections: 'Experimental Results' (Weighted Precision and Recall, Confusion Matrices), 'Annotator Disagreements' (disagreement re-evaluation, Cohen's κ), 'Qualitative analysis' (error examples), and Conclusion.",
            "uuid": "e9701.0",
            "source_info": {
                "paper_title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating Verifiability in Generative Search Engines",
            "rating": 2,
            "sanitized_title": "evaluating_verifiability_in_generative_search_engines"
        },
        {
            "paper_title": "How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior",
            "rating": 2,
            "sanitized_title": "how_faithful_are_rag_models_quantifying_the_tugofwar_between_rag_and_llms_internal_prior"
        },
        {
            "paper_title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\"",
            "rating": 1,
            "sanitized_title": "faitheval_can_your_language_model_stay_faithful_to_context_even_if_the_moon_is_made_of_marshmallows"
        },
        {
            "paper_title": "RAGAs: Automated Evaluation of Retrieval Augmented Generation",
            "rating": 2,
            "sanitized_title": "ragas_automated_evaluation_of_retrieval_augmented_generation"
        }
    ],
    "cost": 0.009596,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges
21 Apr 2025</p>
<p>Nandan Thakur 
University of Waterloo</p>
<p>Ronak Pradeep 
University of Waterloo</p>
<p>Shivani Upadhyay 
University of Waterloo</p>
<p>Daniel Campos 
University of Waterloo</p>
<p>Nick Craswell 
University of Waterloo</p>
<p>Jimmy Lin 
University of Waterloo</p>
<p>Snowflake 
University of Waterloo</p>
<p>Microsoft 
University of Waterloo</p>
<p>Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges
21 Apr 20253149940BC508E6578B19E0A6D63B45C5arXiv:2504.15205v1[cs.CL]
Retrieval-augmented generation (RAG) enables large language models (LLMs) to generate answers with citations from source documents containing "ground truth", thereby reducing system hallucinations.A crucial factor in RAG evaluation is "support"-whether the information in the cited documents supports the answer.To this end, we conducted a large-scale comparative study of 45 participant submissions on 36 topics to the TREC 2024 RAG Track, comparing an automatic LLM judge (GPT-4o) against human judges for support assessment.We considered two conditions: (1) fully manual assessments from scratch and (2) manual assessments with post-editing of LLM predictions.Our results indicate that for 56% of the manual from-scratch assessments, human and GPT-4o predictions match perfectly (on a three-level scale), increasing to 72% in the manual with post-editing condition.Furthermore, by carefully analyzing the disagreements in an unbiased study, we found that an independent human judge correlates better with GPT-4o than a human judge, suggesting that LLM judges can be a reliable alternative for support assessment.To conclude, we provide a qualitative analysis of human and GPT-4o errors to help guide future iterations of support assessment.</p>
<p>Introduction</p>
<p>Retrieval-Augmented Generation (RAG) has recently gained popularity in both academic and industrial settings (e.g., Bing Search [19] and popular frameworks like LangChain [4]).In RAG, large language models (LLMs) generate answers to user queries that include citations to source documents as necessary [11,16,12,3].RAG systems improve factuality and verifiability, reducing hallucinations observed in "closed-book" LLM generation [14,16,10,18].Document-level citations for supporting facts in LLM-generated answers are integral to any deployed RAG system.Therefore, support evaluation assesses whether a RAG answer factually supports the information present in the cited documents, which is crucial for evaluating the quality of a RAG system.Prior work on support evaluation in the RAG literature [17,5,26,9,29,31,25] relies on an automatic judge, i.e., an LLM as a proxy judge.However, it is unknown whether an LLM judge can potentially replace a human judge for support evaluation.This paper examines results from the TREC 2024 RAG Track, assessing 45 participant systems on 36 information-based queries.A sample query and answer is shown in Table 1.We conducted a largescale comparative study between human and LLM judges using resources provided by the National Institute of Standards and Technology (NIST) to better understand whether support assessment can be automated.Unique to the TREC setup, we contrast our automatic judgment process using a strong LLM judge (like GPT-4o) against a manual process under two conditions: (1) manual from scratch, Topic: how taylor swift's age affects her relationships Answer: Taylor Swift's age has significantly influenced her relationships, often contributing to both the dynamics and outcomes of her romances [ ]. Swift's relationships have frequently involved notable age gaps, which have sometimes led to complications [ ].For instance, her relationship with John Mayer, who was 11 years her senior, reportedly strained due to the age difference and Mayer's reputation as a playboy, leading to Swift feeling taken advantage of [0, 3,4].This relationship inspired her song "Dear John," reflecting her emotional turmoil [3,4].Similarly, Swift's relationship with Jake Gyllenhaal, who was nine years older, also highlighted the impact of age differences.Gyllenhaal reportedly felt the age gap, and Swift's inexperience made it difficult for her to navigate the complexities of the relationship, leading to heartbreak and inspiring several songs on her "Red" album [8,1,2].[...] In summary, Taylor Swift's age has played a crucial role in shaping her relationships, influencing both the dynamics and outcomes [8,1].Age differences have often led to complications, while relationships with peers have been relatively simpler.Over time, her maturity has allowed her to navigate romantic complexities with greater understanding and resilience.[ ] Passage [3]: A Timeline Of Taylor Swift's Age-Inappropriate Romances | Business Insider [...] Taylor Swift, 21, &amp; John Mayer, 32.The inappropriateness of Swift's dating habits peaked when she started seeing 11-years her senior and notorious playboy, John Mayer.Neither party confirmed the romance, but they reportedly dated through February 2010 [...]</p>
<p>Passage [8]: What Really Went Wrong With Jake Gyllenhaal And Taylor Swift?[...] Taylor Swift and Jake Gyllenhaal dated from October to December 2010.[...] What went wrong with these two "twin fire signs?"It turns out, a lot.Maybe that's why it's such a touchy subject for him these days!The age gap, when they dated, Gyllenhaal was 29 to Swift's 20, and by the time they split, he was 30 to her 21."He said he could feel the age difference," a source told Us Weekly [...] Table 1: A sample RAG answer segmented into multiple sentences with citations to passages retrieved from the MS MARCO V2.1 segment collection for a query about Taylor Swift.We highlight the cited passages, which are provided as a list (that can be empty, i.e., zero citations).</p>
<p>where human annotators perform assessments from scratch and (2) manual with post-editing, where human annotators are shown GPT-4o predictions during the evaluation process.</p>
<p>In this paper, we focus exclusively on support, i.e., whether the information in an answer sentence is supported by the cited documents, which we consider as the "ground truth".This is, of course, only one aspect of RAG evaluation.For in-depth analyses of other aspects of the TREC 2024 RAG Track, we refer readers to Upadhyay et al. [28] for relevance assessment and Pradeep et al. [23] for nugget evaluation.</p>
<p>Our experimental results indicate that GPT-4o and human judgments perfectly match 56% of the time in the manual from-scratch condition, increasing to 72% in the manual with post-editing condition.These results show promise in using LLM judges for support assessment in both conditions.We measured support of a system's overall answer in terms of two metrics: weighted precision and weighted recall, where precision penalizes overcitation, and recall penalizes undercitation.We observe a high correlation at the run level (above 0.79 Kendall τ ) between GPT-4o and human judges, providing evidence that LLMs can potentially replace human judges for support evaluation.</p>
<p>In addition, to better understand the discrepancies between GPT-4o and human judges, we conducted an unbiased disagreement study with an independent human judge who carefully re-assessed 537 randomly sampled pairs, including both assessment conditions.Our results surprisingly show that the independent judge agrees more with GPT-4o than the human judge (e.g., Cohen's κ of 0.27 vs. 0.07).Finally, we discuss annotation errors to help improve future iterations of support evaluation.</p>
<p>Background and Related Work</p>
<p>What we refer to as support evaluation represents an attempt to determine whether information in a RAG answer is factually supported by its cited documents.In our work, we evaluate support at the sentence level in the answer.We assume an answer r segmented into n sentences, r = {a 1 , • • • , a n }, where each answer sentence a i can contain a maximum of m document citations, Human Judge: Full Support GPT-4o Judge: Full Support Answer Sentence: The age difference was a significant factor in their breakup, with Gyllenhaal not ready to commit, further exacerbating Swift's emotional distress.Conversely, Swift's relationships with peers closer to her age, such as Joe Jonas, were less fraught with such issues.
a i = {d i , • • • , d m },
Passage ID [8]: doc_48_737500982#1_1325021022 Title: What Went Wrong With Jake Gyllenhaal And Taylor Swift?Text: Taylor Swift and Jake Gyllenhaal dated from October to December 2010.[...] "He said he could feel the age difference," a source told Us Weekly.[...] "When Jake broke her heart, she was so inexperienced she didn't know how to deal with it ...She wasn't used to all the head games and the lies but now she's way less naive."</p>
<p>Human Judge: Full Support GPT-4o Judge: Partial Support Answer Sentence: As she matured, her understanding of relationships evolved, making her less naive and more discerning in her romantic choices.</p>
<p>Passage ID [8]: doc_48_737500982#1_1325021022 Title: What Went Wrong With Jake Gyllenhaal And Taylor Swift?Text: Taylor Swift and Jake Gyllenhaal dated from October to December 2010.[...] "He said he could feel the age difference," a source told Us Weekly.[...] "When Jake broke her heart, she was so inexperienced she didn't know how to deal with it.She wasn't used to all the head games and the lies but now she's way less naive."</p>
<p>Human Judge: No Support GPT-4o Judge: Partial Support Table 2: Examples of support evaluation with GPT-4o and human judges for the Taylor Swift topic "how taylor swift's age affects her relationships".The fragment of the passage that supports the answer sentence is highlighted.each of which are documents drawn from a corpus. 1 Support is calculated as the function f (a i , d j ) = s i,j where f can be a human or LLM judge that generates a scalar value s i,j , indicating the extent that the cited document d j provides support to sentence a i .A few examples of support evaluation are shown in Table 2. Apart from RAG, support has been primarily explored in the literature for summarization [15,13] and natural language explanations [2,27].</p>
<p>Previous work on support evaluation in RAG used different automatic judges: examples include an natural language inference (NLI) model [10], LLM with prompting [9], or even fine-tuned custom LLMs [26] as the automatic judge.Wu et al. [29] evaluated the tug of war between an LLM's internal prior over supporting wrong context information.Similar to our formulation, Ming et al. [20] provided an evaluation benchmark consisting of academic question answering (QA) datasets with human validation and Liu et al. [17] evaluated the quality of proprietary search engine outputs with crowdsourced human judges.In contrast, our work is one of the first to conduct a large-scale human annotation study-encompassing 11K human assessments over multiple RAG systems on 36 topics containing non-factoid, decompositional, and multi-perspective queries.This study design provides a rich backdrop for comparing human and GPT-4o judges for support evaluation.</p>
<p>Track Description &amp; Assessment Methodology</p>
<p>TREC 2024 RAG Track</p>
<p>The Text Retrieval Conference (TREC) has led the way in many aspects of evaluation in information retrieval (IR), natural language processing (NLP), and beyond, for accelerating research within the community (both researchers and practitioners).Each year, TREC organizes several tracks, focused on topics ranging from text or multimodal retrieval [6,7,30] to conversational QA [1].</p>
<p>The context for this work is the TREC 2024 RAG Track, which was divided into three tasks: retrieval (R), augmented generation (AG), and retrieval-augmented generation (RAG).Here, we focus on the generation part, i.e., participant systems are presented with queries (called topics in TREC parlance) and candidate passages.These candidate passages are either generated by us (i.e., the track organizers) and shared with all the participants [22] (the AG task) or each participant can directly perform retrieval from the MS MARCO V2.1 segment collection (the end-to-end RAG task).The candidate passages provide the context or grounding of RAG for synthesizing the final free-form answer.We require that answers be segmented into sentences, and that each sentence is associated with citations to passages from the corpus, as shown in Table 1.Since many teams participated in the TREC 2024 RAG Track, our human and LLM judges were exposed to multiple answers and cited documents during support evaluation.</p>
<p>Passage collection.The MS MARCO V2.1 segment collection contains 113,520,750 text passages, derived from a deduplicated version of the MS MARCO V2 document collection [6] by removing near-duplicate documents using locality-sensitive hashing (LSH) with MinHash and 9-gram shingles.This reduced the original document count from 11,959,635 to 10,960,555 documents.Passages were derived from the corresponding document collection using a sliding-window chunking technique with overlap-specifically, using windows of 10 sentences with a stride of 5 sentences, producing passages typically between 500-1000 characters.Each passage comprises a title field containing the title of the passage and a text field containing the body of the passage.</p>
<p>Topic collection.For the TREC 2024 RAG Track topics (queries), we leveraged a fresh scrape of Bing Search logs containing non-factoid queries that are multifaceted and subjective, warranting RAG systems to provide long-form answers [22,24].We gathered topics close to the submission period of the evaluation (around July 2024) to avoid staleness and minimize potential data leakage.Here, we are not concerned about data contamination in support assessment: since we are retrieving from web corpora, our passages are likely present in LLM pre-training data.Due to budget constraints with human annotations, we only ran evaluations using a subset of 36 topics selected from the complete TREC 2024 RAG Track topic collection.</p>
<p>Support Assessment</p>
<p>Consistent with previous support evaluations in RAG [17,10], we used a three-level grade, with the following associated descriptions for each support level: FS Full Support: All of the information in the answer sentence is factually consistent with and supported by the cited passage.PS Partial Support: Some of the information in the answer sentence is factually consistent with and supported by the cited passage, but other parts of the sentence are not supported.NS No Support: The cited passage is completely irrelevant and does not support any part of the answer sentence.</p>
<p>An edge case is a sentence with zero citations: We automatically consider the support assessment to be "no support", as the sentence does not cite any retrieved passage.</p>
<p>Next, in order to evaluate the quality of LLM judges in contrast to human judges, we conducted our support assessment with human judges under two conditions: (1) manual from scratch and (2) manual with post-editing.We describe both conditions in detail below:</p>
<ol>
<li>
<p>Manual from scratch.In this condition, a human judge is provided with the answer sentence and the cited passage.The judge reads both carefully and evaluates whether the answer sentence is supported by the cited passage (according to one of the labels above).</p>
</li>
<li>
<p>Manual with post-editing.In this condition, a human judge is provided with the answer sentence, the cited passage, and the support judgment label given by the LLM judge.The human judge reads the sentence and passage carefully and provides an assessment using the LLM judgment label as a reference.</p>
</li>
</ol>
<p>In this task, you will evaluate whether each statement is supported by its corresponding citations.Note that the system responses may appear very fluent and well-formed, but contain slight inaccuracies that are not easy to discern at first glance.Pay close attention to the text.</p>
<p>You will be provided with a statement and its corresponding passage which the statement cites.It may be helpful to ask yourself whether it is accurate to say "according to the citation ..." with the statement following this phrase.Be sure to check all of the information in the statement.You will be given three options:</p>
<p>• Full Support: All of the information in the statement is supported in the citation.</p>
<p>• Partial Support: Some parts of the information are supported in the citation, but other parts are missing.</p>
<p>• No Support: The citation does not support any part of the statement.</p>
<p>Please provide your response based on the information in the citation.If you are unsure, use your best judgment.Respond as either "Full Support", "Partial Support", or "No Support" with no additional information.</p>
<p>Statement: {statement} Citation: {citation} For automatic labeling, we utilized GPT-4o as an automatic judge.We ran inference using the Microsoft Azure API [21], providing a single passage at a time in the prompt2 using the answer sentence and the cited passage.The GPT-4o judge is presented with each sentence and its cited passage and asked to determine the support label without any explanation (full support, partial support, or no support).The prompt used is given in Figure 1.</p>
<p>Computational Cost &amp; Evaluation Tradeoffs</p>
<p>In the TREC 2024 RAG Track, we allowed participants to provide citations for up to 20 passages per answer sentence.To judge each sentence and its cited passage, our protocol requires a human judge to read the answer sentence and a relatively long text passage (typically, 500-1000 characters).Thus, conducting an exhaustive evaluation of all cited passages for every answer sentence across multiple participants was not feasible given our budget constraints.</p>
<p>Therefore, we had to choose between sparse and dense annotations.Dense annotations would provide fewer judged topics, but each answer sentence would be evaluated against k cited passages.On the other hand, sparse annotations would provide higher diversity in judged topics, but at the cost of judging fewer cited passages for every answer sentence.</p>
<p>We opted for sparse annotations to achieve more judged topics.We fixed both the human and GPT-4o judge to evaluate only the first cited passage of every answer sentence for all participants.As with all TREC evaluations, NIST provided the resources to perform human evaluations based on the guidance of the track organizers (i.e., us).NIST first trained every human judge to understand the task, and then each human judge evaluated each topic sequentially.</p>
<p>Support Evaluation Metrics</p>
<p>Support can be evaluated across two dimensions, similar to Liu et al. [17]: (1) weighted precision, accounting for how many correct passage citations are present in the generated answer, and (2) weighted recall, accounting for how many sentences in the answer are supported by passage citations We define both metrics below:</p>
<p>Weighted precision.This metric measures the weighted proportion of citations that support each answer sentence.We assign a weight to s(a i , d j ) of 1.0 to Full Support (FS), 0.5 to Partial Support (PS), and 0 to No Support (NS) for the answer sentence and cited passage.To explain the metric clearly, let us assume a RAG answer with 3 sentences = {a 1 , a
Weighted Precision = s(a 1 , p 1 ) + s(a 2 , p 2 ) count({a 1 , p 1 }, {a 2 , p 2 }) = 0.5 + 1 2 = 0.75
Weighted recall.This metric measures the weighted proportion of answer sentences that are supported by their cited passages.We assign the same weights as defined above in weighted precision.</p>
<p>For the above example, we compute weighted recall as follows:
Weighted recall = s(a 1 , p 1 ) + s(a 2 , p 2 ) count({a 1 , a 2 , a 3 }) = 0.5 + 1 3 = 0.5
Precision penalizes overcitation in the answer text that is often distracting and unnecessary for the user.On the other hand, recall penalizes answers with undercitation, i.e., sentences with zero citations.Therefore, if a sentence has zero citations, it reduces the recall score but keeps precision unchanged.As described above, we evaluated only the first cited passage per answer sentence for support.Therefore, the weighted recall and precision scores are identical if all answer sentences have at least one citation.We save for future work how to best evaluate multiple passage citations for every sentence in the answer, as it would require a much larger annotation budget.</p>
<p>4</p>
<p>Experimental Results</p>
<p>For the TREC 2024 RAG Track, NIST received 93 runs from 20 groups for the end-to-end RAG task and 53 runs from 11 groups for the AG task.Given resource constraints, we evaluated only the two highest-priority submissions from each group across the RAG and AG tasks.As shown in Table 3, this translates into 31 runs from 18 groups for RAG and 14 runs from 9 groups for AG, totaling a maximum of 45 participant submissions for each topic.The human judges were able to complete judgments for 36 topics, sparsely annotated: 6,742 annotations on 22 topics in the manual from-scratch condition and 4,165 annotations on 14 topics in the manual with post-editing condition.</p>
<p>Weighted Precision and Recall</p>
<p>Table 5 and Table 6 show the average weighted precision and recall scores of all participant runs for both the AG and RAG tasks in the manual from-scratch condition on the 22 topics evaluated by human and GPT-4o judges, respectively.Table 7 and Table 8 show the average weighted precision and recall scores for all participants in the manual with post-editing condition on the 14 topics evaluated by human and GPT-4o judges, respectively.We sort the runs in terms of the average weighted precision score in descending order.</p>
<p>In Figure 2, we show scatter plots of weighted precision and recall scores obtained by all participant submissions.Run-level scores (denoted by ×) are strongly correlated (all above 0.79 Kendall's τ ) between GPT-4o and human annotations.Per-topic averages (denoted by △) vary on both axes, where certain topics achieve a higher weighted precision and recall score than humans over GPT-4o, and vice versa.Individual participant scores (denoted by or □) show a high variance in both weighted precision and recall scores.This is likely due to the mismatch of human annotators preferring "no support", whereas GPT-4o prefers "partial support".Overall, we observe the majority of scores in the  bottom right triangle, indicating that humans take a more conservative approach and provide lower levels of support overall than GPT-4o, leading to lower weighted precision and recall scores.</p>
<p>Confusion Matrices</p>
<p>Next, to better understand how often the GPT-4o judge agrees with the human judges, we plot the confusion matrices in Figure 3.We compare predictions by human judges with GPT-4o on two conditions: manual from scratch and manual with post-editing.</p>
<p>Manual from-scratch condition.For 56% (13.7% + 11.9% + 30.4%),GPT-4o and the human judge perfectly agreed on their support judgment on 22 topics.Both "full support" and "no support" categories have higher percentages (30.4% and 13.7%), showing that humans and GPT-4o as judges agreed more on both ends of the spectrum.For 15.1%, the GPT-4o judge considered an annotation as "partial support", which the human judge annotated as "no support".Finally, an important observation is that the GPT-4o judge is more likely to provide a higher support label than the human judge (the upper right triangle has a higher combined percentage over the lower left triangle).</p>
<p>Manual with post-editing condition.From the previous condition, we see the increase in perfect agreement rise to 72.1% (15.9% + 18.7% + 37.5%) on 14 topics that were annotated with post-editing GPT-4o labels.This shows that sentences and cited passages with "partial support" that led to disagreements in the manual from-scratch condition are reduced.In this condition, human judges are more likely to agree with the GPT-4o judge unless it is an obvious mistake, i.e., when the GPT-4o</p>
<p>No Support Partial Support Full Support  judge considers an annotation to be "full support" and the human judge considers it to be "no support" (increased now to 6.3% from 5.9% in the manual from-scratch condition).</p>
<p>GPT-4o Judge</p>
<p>Annotator Disagreements</p>
<p>In the experiments reported in Section 4, we observed frequent disagreements between the human and GPT-4o judge.To further study this, we performed unbiased annotations from scratch by carefully re-assessing the support judgment of randomly sampled disagreements between the human and GPT-4o judge, with an independent human judge and another LLM judge using LLAMA-3.1 405B [8] (with the same prompt as GPT-4o in Figure 1).We randomly sampled 15 disagreement pairs per topic, re-evaluating 537 sentences and their first cited passages, including both assessment conditions:</p>
<p>(1) manual from scratch and (2) manual with post-editing.</p>
<p>Results.As shown in Figure 4, we interestingly find the independent human judge to be better correlated with GPT-4o than the human judge provided by NIST (Cohen's κ of 0.29 and 0.27 versus −0.03 and 0.07) in the manual from-scratch condition.The independent judge fully matched 31% of the time with the human judge and 51% of the time with the GPT-4o judge.Similarly, in the manual with post-editing condition, the independent judge fully matched 37% of the time with the human judge and 52% of the time with the GPT-4o judge.LLAMA-3.1 405B had a stronger correlation with another LLM (GPT-4o) over human judges (Cohen's κ of 0.60 and 0.46 versus −0.20 and −0.06), demonstrating the high likelihood of different LLMs providing similar prediction labels.</p>
<p>From the label distributions in the disagreement analysis shown in Figure 5, we observed that both LLMs (LLAMA-3.1 405B and GPT-4o) labeled about 49-51% of sentences and their cited passage as "partial support", whereas the human judge labeled 47% of the sentences as "no support".The independent judge labeled 50% of the sentences as "full support".We keep as future work explorations of consistency: why LLM judges labeled only a few sentences as "no support", and similarly, why human judges labeled a majority of sentences as "no support".</p>
<p>Qualitative analysis.We further assessed examples qualitatively to understand failure cases, for example, when a human or GPT-4o judge makes mistakes during support evaluation.In Human Judge: Full Support ✓ GPT-4o Judge: Partial Support × Table 4: Examples of annotation mistakes by either GPT-4o or the human judge for the topic "how taylor swift's age affects her relationships" taken from the disagreement analysis.The fragment of the passage that supports the answer sentence is highlighted.</p>
<p>show a few examples of annotation mistakes on the Taylor Swift topic found in either the human or GPT-4o judgment.Overall, we summarize a few of the following errors made by GPT-4o:</p>
<p>• GPT-4o can confuse words or phrases with similar meanings; for example, it is unable to distinguish between police and security specialists.</p>
<p>• GPT-4o can miss out on evaluating the whole sentence (especially information present at the end of the sentence), biasing towards the "full support" label.</p>
<p>• GPT-4o can label "partial support" if the theme in the answer sentence is similar, but the passage does not support any text present in the answer sentence, i.e., "no support".</p>
<p>On the other hand, human judges make mistakes due to not reading the passages carefully.In some cases, answer sentences that were directly stated in the middle of or at the end of a passage, or mentioned in parts of the passage, were surprisingly unnoticed by a human judge.This causes the human judge to label such cases as "no support" instead of "full support".Lastly, we observe that a human judge occasionally labels an answer sentence as "full support" even though the passage doesn't provide any support information.We suspect that this could be due to an inherent bias relying on the human judge's memory or understanding of the topic, instead of strictly relying on the actual passage text.</p>
<p>Conclusion</p>
<p>In this work, we evaluated support in RAG answers by analyzing 45 submissions across 36 topics from the TREC 2024 RAG Track in a large-scale comparative study involving both humans and LLMs as judges.We critiqued and evaluated strong LLM judges, like GPT-4o, against human annotators for support assessment.</p>
<p>Our results show a high agreement between GPT-4o and human judgments, with a perfect match between judgments occurring 56% of the time in the manual from-scratch condition, increasing to 72% in the manual with post-editing condition.We observe that disagreements between humans and LLMs mainly occur for sentence-passage pairs indicating partial support, i.e., in the middle of the support evaluation spectrum.</p>
<p>To better understand these disagreements, we conducted an unbiased evaluation by carefully reassessing judgments with an independent human judge and a different LLM.Interestingly, in cases of disagreements, both the independent human judge and the LLAMA-3.1 judge agreed more with the GPT-4o judge than with the human judge, providing evidence for widely divergent opinions and perhaps the veracity of using LLMs for support evaluation.Further research could explore the nuances of disagreements between human and LLM judges and investigate limitations of both humans and LLMs to improve future iterations of support assessment.</p>
<p>Figure 1 :
1
Figure 1: Prompt used by the GPT-4o judge for support evaluation.</p>
<p>Figure 2 :
2
Figure 2: Correlations between scores from human and GPT-4o judges for the manual from-scratch condition (top) and the manual with post-editing condition (bottom), measuring weighted precision and recall.Red markers show run-level scores, yellow triangles show per-topic averages, and blue dots or green boxes show all individual topic/run combinations.Each plot is annotated with rank correlations showing Kendall's τ .</p>
<p>Figure 3 :Figure 4 :Full 1 Figure 5 :
3415
Figure 3: Confusion matrices comparing predictions from human and GPT-4o judges for the manual from-scratch condition (left) and the manual with post-editing condition (right).</p>
<p>Answer Sentence: For instance, her relationship with John Mayer, who was 11 years her senior, reportedly strained due to the age difference and Mayer's reputation as a playboy, leading to Swift feeling taken advantage of.Timeline Of Taylor Swift's Age-Inappropriate Romances | Business Insider Text: [...] 2010: Taylor Swift, 21, &amp; John Mayer, 321 / 12 And then the inappropriateness of Swift's dating habits peaked [...] and then the heartbroken young Swift penned the song 'Dear John' about the break up.Earlier this year, Mayer admitted that he felt 'humiliated' when he heard the song, but Swift refuses to admit it's about him, telling Glamour magazine it was 'presumptuous' of him to think the song was about him.
Passage ID [0]: doc_04_1081579649#7_2253255175Title: Timeline of Taylor Swift's RelationshipsText: 2009: Taylor Swift, 20, &amp; Taylor Lautner, [...] 2010:Taylor Swift, 21, &amp; John Mayer, 321 / 12 And then the in-appropriateness of Swift's dating habits peaked when shestarted seeing 11-years her senior and notorious playboy,John Mayer.Human Judge: Partial SupportGPT-4o Judge: Partial SupportAnswer Sentence: This relationship inspired her song"Dear John," reflecting her emotional turmoil.Passage ID [3]: doc_35_202251892#8_427548986Title:</p>
<p>Table 3 :
3
Descriptive statistics for support judgments for the (1) manual from-scratch condition and (2) manual with post-editing condition for 45 participant submissions on 36 topics.{p 1 , p 2 }.Now, let's assume that passage p 1 partially supports a 1 , passage p 2 fully supports a 2 , and a 3 has zero citations.We compute weighted precision as follows:
Condition#Topics #AnnotationsFSSupport level PS NS(1a) Manual from scratch (Human)226,7422,752 1,652 2,338(1b) Automatic (GPT-4o)226,7423,110 2,421 1,211(2a) Manual with post-editing (Human)144,1651,812 1,076 1,277(2b) Automatic (GPT-4o)144,1652,045 1,330790
2 , a 3 }, and a corpus C with 2 passages:</p>
<p>Human Judge 13.7% 15.1% 5.9% 2.8% 11.9% 9.8% 1.5% 8.9% 30.4% Manual from Scratch: 22 Topics
No Support0.25 0.30No SupportPartial Support Full Support0.05 0.15 0.20 0.10Partial Support Full SupportNo Support Partial Support Full Support GPT-4o Judge</p>
<p>Human Judge 15.9% 8.5% 6.3% 1.8% 18.7% 5.4% 1.3% 4.7% 37.5% Manual with Post-editing: 14 Topics
0.350.300.250.200.150.100.05</p>
<p>Table 4 ,
4
we Answer Sentence: Swift started dating 19-year-old One Direction frontman, Harry Styles, barely after her breakup from Conor Kennedy.Her first Hollywood romance with Joe Jonas was age-appropriate, as both were 19 at the time.Timeline of Taylor Swift's Relationships Text: [...] 2008: Taylor Swift, 19 &amp; Joe Jonas, 19.Swift's first Hollywood romance was perfectly age-appropriate.While Swift was just emerging onto the pop scene, Jonas, along with his singing and dancing brothers, were at the top of their game.[...] Answer Sentence: Swift has dated younger men, such as 17-year-old Taylor Lautner when she was 20, which was controversial due to the age difference.[...] John Mayer (December 2009-February 2010) The 11-year age gap between Taylor and John reportedly strained their relationship, with Taylor feeling that her naivety was taken advantage of.She references their ugly breakup in her song "Dear John".Songs: Dear John [...]
Passage ID [0]: doc_04_1081579649#7_2253255175 Title: A Timeline Of Taylor Swift's Age-Inappropriate Romances | Business Insider Text: Today, Taylor Swift turns 23, but you would never know how old the singer actually is based on her love life. From 18-year-old Conor Kennedy to 35-year-old John Mayer, Swift is no ageist when it comes to who she will date. Young or old, British or American, Swift really doesn't discriminate in her love life. But at the age of 23, Swift has already had at least 10 high-profile relationships (some more flings, really) that have all ended the samePassage ID [8]: doc_35_202251892#7_427547583 Title: A Timeline Of Taylor Swift's Age-Inappropriate Romances | Business Insider Text: 2009: Taylor Swift, 20, &amp; Taylor Lautner, 17. And here's where she started to slide. Technically, this relation-ship was possibly illegal in many states as 20-year-old Swift was dating a minor. [...] In October 2010, Swift revealed her crush: 'Taylor Lautner. It's always going to be Taylor.' It's believed that the song 'Back To December' is Swift's apology to Lautner for their breakup. [...]way -in song. In honour of her 23rd year, let's relive Swift's vast array of boyfriends, shall we? [...]Human Judge: Full Support ✓ GPT-4o Judge: Partial Support ×Human Judge: Full Support × GPT-4o Judge: No Support ✓Answer Sentence: Swift's youth and inexperience have been cited as factors in her relationships, with some part-Answer Sentence: Passage ID [3]: doc_04_1081579649#2_2253244363 Title: Human Judge: No Support ×ners reportedly taking advantage of her naivety. Passage ID [8]: doc_48_481103263#4_840591332 Title: Taylor Swift Boyfriends: Taylor Swift's Relation-ships | New Idea Magazine Text:GPT-4o Judge: Full Support ✓</p>
<p>Table 5 :
5
Weighted precision and recall scores for the top two runs from each group in TREC 2024 RAG Track under the manual from-scratch condition on 22 topics evaluated by human judges provided by NIST.#Sentences denotes the average number of sentences in the participant's submitted answer.
Run IDGroupTask Weighted Precision Weighted Recall #Sentencesag_rag_gpt35_expansion_rrf_20IITD-IRLRAG0.8680.8684.82Ranked_Iterative_Fact_Extraction_and_RefinementTREMA-UNHRAG0.8610.86113.68Enhanced_Iterative_Fact_Refinement_and_Prioritization TREMA-UNHRAG0.8460.84613.45UWCragWaterlooClarkeAG0.8410.8417.91zeph_test_rag_rrf_expand_queryIITD-IRLAG0.8070.8074.64ldilab_gpt_4oldisnuRAG0.7910.51412.86dilab_repllama_listt5_pass3_gpt4oldisnuAG0.7840.49113.45baseline_frag_rag24.test_command-r-plus_top20coordinatorsAG0.7780.71811.68iiia_dedup_p1_straight_agIIIA-UNIPDRAG0.7670.7153.77cir_gpt-4o-mini_no_reranking_50_0.5_100_301_p1CIRRAG0.7570.7577.05baseline_frag_rag24.test_gpt-4o_top20coordinatorsAG0.7410.46714.18neuragfixneuAG0.7370.45915.82neuragneuAG0.7350.45815.82ielab-b70bf-70bqp-70bafsielabAG0.7330.5913.18baseline_rag24.test_l31_70b_instruct_top20coordinatorsRAG0.7310.7197.50cohere+post_processingKMLRAG0.7120.67118.00listgalore_l31-70b_ragnarokv4_top20h2olooAG0.7110.6229.64iiia_standard_p1_straight_agIIIA-UNIPDRAG0.7020.6106.23webis-manualwebisAG0.7020.41514.14listgalore_gpt4o_ragnarokv4_top20h2olooAG0.6990.56312.14cir_gpt-4o-mini_Cosine_50_0.5_100_301_p1CIRRAG0.6770.6777.05UDInfolab.RAG.AnsAIInfoLabAG0.6590.6597.18FT-llama3uog-thtAG0.6400.6404.64baseline_top_5uis-iaiRAG0.6390.6393.23agtask-bm25-colbert_faiss-gpt4o-llama70bsoftbank-meisei RAG0.6330.6338.27UDInfolab.RAG.QueryInfoLabAG0.6230.6236.86webis-rag-run0-taskragwebisAG0.6160.6166.27buwbuwAG0.5970.5978.14oneshot_post_sentencedbuwAG0.5810.4459.09ruc001Ruc01AG0.5770.47914.77webis-rag-run1-taskragwebisAG0.5310.5316.18gpt_miniKMLRAG0.4940.4947.45rag_bm25-colbert_faiss-gpt4o-llama70bsoftbank-meiseiAG0.4750.4757.27ICL-mistraluog-thtAG0.4540.3455.82UWCgaragWaterlooClarkeAG0.4430.4259.95iiresearch-bm25-top10-llama3-8b-instructii_researchAG0.4330.3924.14ginger_top_5uis-iaiRAG0.4190.4194.32LAS-splade-mxbai-mmr8-RAGncsu-lasAG0.3760.27012.82BEST_cot_gpt3.5citiAG0.3290.2746.45SECOND_cot_gpt3.5citiAG0.3190.2295.50ISIR-IRIT-zephyr_query_genIRITAG0.2730.1326.95ielab-b70bf-70bqfs-ad_hocielabAG0.2530.2534.59LAS-splade-mxbai-rrf-mmr8ncsu-lasAG0.2370.17013.41ISIR-IRIT-zephyr_p2IRITAG0.1590.1126.73qrant_bge_geminiSGUAG0.1570.1576.45</p>
<p>Table 6 :
6
Weighted precision and recall scores for the top two runs from each group in TREC 2024 RAG Track under the manual from-scratch condition on 22 topics evaluated by the GPT-4o judge.#Sentences denotes the average number of sentences in the participant's submitted answer.
Run IDGroupTask Weighted Precision Weighted Recall #Sentencesag_rag_gpt35_expansion_rrf_20IITD-IRLRAG0.7930.7934.79Enhanced_Iterative_Fact_Refinement_and_Prioritization TREMA-UNHRAG0.7730.77314.5Ranked_Iterative_Fact_Extraction_and_RefinementTREMA-UNHRAG0.7720.77214.07baseline_frag_rag24.test_command-r-plus_top20coordinatorsAG0.7600.68312.29neuragneuAG0.7350.44215.71listgalore_gpt4o_ragnarokv4_top20h2olooAG0.7340.57512.21baseline_frag_rag24.test_gpt-4o_top20coordinatorsAG0.7260.40414.64UWCragWaterlooClarkeAG0.7240.7246.64zeph_test_rag_rrf_expand_queryIITD-IRLAG0.7090.7094.07dilab_repllama_listt5_pass3_gpt4oldisnuAG0.7090.41812.0listgalore_l31-70b_ragnarokv4_top20h2olooAG0.6790.5939.57baseline_rag24.test_l31_70b_instruct_top20coordinatorsRAG0.6680.6607.5neuragfixneuAG0.6660.39815.71iiia_standard_p1_straight_agIIIA-UNIPDRAG0.6620.6024.93ielab-b70bf-70bqp-70bafsielabAG0.6580.5712.86UDInfolab.RAG.QueryInfoLabAG0.6540.6547.71iiia_dedup_p1_straight_agIIIA-UNIPDRAG0.6500.6344.43ldilab_gpt_4oldisnuRAG0.6460.36312.79cir_gpt-4o-mini_no_reranking_50_0.5_100_301_p1CIRRAG0.6440.6446.86UDInfolab.RAG.AnsAIInfoLabAG0.6390.6397.43webis-rag-run0-taskragwebisAG0.6110.6115.21baseline_top_5uis-iaiRAG0.5710.5713.07FT-llama3uog-thtAG0.5570.5573.29cohere+post_processingKMLRAG0.5560.55618.79cir_gpt-4o-mini_Cosine_50_0.5_100_301_p1CIRRAG0.5380.5387.29agtask-bm25-colbert_faiss-gpt4o-llama70bsoftbank-meisei RAG0.5260.5267.43ruc001Ruc01AG0.5160.45811.21oneshot_post_sentencedbuwAG0.4600.35712.0buwbuwAG0.4590.4599.5gpt_miniKMLRAG0.4170.4178.36ginger_top_5uis-iaiRAG0.4130.4133.79rag_bm25-colbert_faiss-gpt4o-llama70bsoftbank-meiseiAG0.3980.3987.29webis-rag-run1-taskragwebisAG0.3560.3565.57ielab-b70bf-70bqfs-ad_hocielabAG0.3520.3504.71LAS-splade-mxbai-mmr8-RAGncsu-lasAG0.3220.24111.71iiresearch-bm25-top10-llama3-8b-instructii_researchAG0.3210.3211.86SECOND_cot_gpt3.5citiAG0.2870.2245.14ICL-mistraluog-thtAG0.2500.1964.29UWCgaragWaterlooClarkeAG0.2370.2019.29BEST_cot_gpt3.5citiAG0.2160.1703.71LAS-splade-mxbai-rrf-mmr8ncsu-lasAG0.1900.14412.21qrant_bge_geminiSGUAG0.1470.1475.79ISIR-IRIT-zephyr_p2IRITAG0.1430.0472.86ISIR-IRIT-zephyr_query_genIRITAG0.1430.0551.79webis-manualwebisAG0.1060.0751.71</p>
<p>Table 7 :
7
Weighted precision and recall scores for the top two runs from each group in TREC 2024 RAG Track under the manual with post-editing condition on 14 topics evaluated by human judges provided by NIST.#Sentences denotes the average number of sentences in the participant's submitted answer.</p>
<p>Consistent with parlance in the field, we refer to documents in the generic sense, even though in actuality a "document" may be a passage (as in our case), a PDF, or even an image.
Separately, we experimented with providing multiple cited passages at once, but we anecdotally observed that providing one passage at a time performs better with the GPT-4o judge.
AcknowledgmentsThis work would not have been possible without the annotator team at NIST.We are grateful for their contributions.This research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada.Additional funding is provided by Snowflake, Microsoft via the Accelerating Foundation Models Research program, and an Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korean Government (MSIT) (No.RS-2024-00457882, National AI Research Lab Project).Thanks to Corby Rosset for providing the test queries, based on the methodology developed in Researchy Questions [24].
TREC iKAT 2023: A Test Collection for Evaluating Conversational and Interactive Knowledge Assistants. Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffrey Dalton, Leif Azzopardi, 10.1145/3626772.3657860Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Faithfulness Tests for Natural Language Explanations. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, Isabelle Augenstein, 10.18653/V1/2023.ACL-SHORT.25Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20232Short Papers), ACL 2023</p>
<p>Improving Language Models by Retrieving from Trillions of Tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Oriol Irving, Simon Vinyals, Karen Osindero, Jack W Simonyan, Erich Rae, Laurent Elsen, Sifre, International Conference on Machine Learning, ICML 2022. Baltimore, Maryland, USAPMLR2022. 17-23 July 2022162of Proceedings of Machine Learning Research</p>
<p>. Harrison Chase, 2022LangChain</p>
<p>Benchmarking Large Language Models in Retrieval-Augmented Generation. Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun, 10.1609/aaai.v38i16.29728Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Overview of the TREC 2022 Deep Learning Track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M Voorhees, Ian Soboroff, Proceedings of the Thirty-First Text REtrieval Conference (TREC 2022). the Thirty-First Text REtrieval Conference (TREC 2022)Gaithersburg, Maryland2022</p>
<p>Overview of the TREC 2023 Deep Learning Track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Hossein A Rahmani, Daniel Campos, Jimmy Lin, Ellen M Voorhees, Ian Soboroff, Proceedings of the Thirty-Second Text REtrieval Conference (TREC 2023). the Thirty-Second Text REtrieval Conference (TREC 2023)Gaithersburg, Maryland2023</p>
<p>. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, 10.48550/ARXIV.2407.21783and 520 others. 2024. The Llama 3 Herd of Models. CoRR, abs/2407.21783</p>
<p>RAGAs: Automated Evaluation of Retrieval Augmented Generation. Shahul Es, Jithin James, Luis Espinosa Anke, Steven Schockaert, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. the 18th Conference of the European Chapter of the Association for Computational Linguistics: System DemonstrationsSt. Julians, Malta. Association for Computational Linguistics2024</p>
<p>Enabling Large Language Models to Generate Text with Citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Retrieval Augmented Language Model Pre-Training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Proceedings of the 37th International Conference on Machine Learning, ICML 2020. the 37th International Conference on Machine Learning, ICML 2020PMLR2020. 13-18 July 2020119of Proceedings of Machine Learning Research</p>
<p>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. Gautier Izacard, Edouard Grave, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021OnlineAssociation for Computational Linguistics2021. April 19 -23, 2021</p>
<p>Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model. Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q Zhu, 10.18653/V1/2023.EMNLP-MAIN.679Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Generalization through Memorization: Nearest Neighbor Language Models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization. Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander R Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu, 10.18653/V1/2023.EMNLP-MAIN.600Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. S H Patrick, Ethan Lewis, Aleksandra Perez, Fabio Piktus, Vladimir Petroni, Naman Karpukhin, Heinrich Goyal, Mike Küttler, Wen-Tau Lewis, Tim Yih, Sebastian Rocktäschel, Douwe Riedel, Kiela, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020. 2020. 2020. December 6-12, 2020</p>
<p>Evaluating Verifiability in Generative Search Engines. Nelson Liu, Tianyi Zhang, Percy Liang, 10.18653/v1/2023.findings-emnlp.467Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Lost in the Middle: How Language Models Use Long Contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638/2336043/tacl_a_00638.pdfTransactions of the Association for Computational Linguistics. 122024</p>
<p>Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web. 2023Microsoft</p>
<p>FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows. Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty, 10.48550/ARXIV.2410.03727CoRR, abs/2410.037272024</p>
<p>Ronak Pradeep, Nandan Thakur, Sahel Sharifymoghaddam, Eric Zhang, Ryan Nguyen, Daniel Campos, Nick Craswell, Jimmy Lin, arXiv:2406.16828Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track. 2024</p>
<p>Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the AutoNuggetizer Framework. Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick Craswell, Jimmy Lin, 10.48550/ARXIV.2411.09607CoRR, abs/2411.096072024</p>
<p>Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents. Corby Rosset, Ho-Lam Chung, Guanghui Qin, Ethan C Chau, Zhuo Feng, Ahmed Awadallah, Jennifer Neville, Nikhil Rao, 10.48550/ARXIV.2402.17896CoRR, abs/2402.178962024</p>
<p>RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation. Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, Zheng Zhang, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2024. 2024. 2024. December 10 -15, 2024</p>
<p>ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems. Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia, 10.18653/v1/2024.naacl-long.20Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models. Y Noah, Oana-Maria Siegel, Nicolas Camburu, María Heess, Pérez-Ortiz, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024 -Short Papers. the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024 -Short PapersBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 2024</p>
<p>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look. Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa , Trang Dang, Jimmy Lin, 10.48550/ARXIV.2411.08275CoRR, abs/2411.082752024</p>
<p>How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior. Kevin Wu, Eric Wu, James Zou, 10.48550/ARXIV.2404.10198CoRR, abs/2404.101982024</p>
<p>AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. Jheng-Hong Yang, Carlos Lassance, Rafael Sampaio De Rezende, Krishna Srinivasan, Miriam Redi, Stéphane Clinchant, Jimmy Lin, 10.1145/3539618.3591903Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Evaluation of Retrieval-Augmented Generation: A Survey. Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu, 10.48550/ARXIV.2405.07437CoRR, abs/2405.074372024</p>            </div>
        </div>

    </div>
</body>
</html>
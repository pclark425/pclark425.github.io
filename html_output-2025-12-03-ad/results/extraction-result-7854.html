<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7854 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7854</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7854</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-271709606</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.02666v1.pdf" target="_blank">Self-Taught Evaluators</a></p>
                <p><strong>Paper Abstract:</strong> Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7854.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7854.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT-Bench agreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agreement between LLM-as-a-Judge and human judgments on MT-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative agreement of the paper's Self-Taught Evaluator (iterative Llama-3-70B-Instruct fine-tuning) with human judgments on MT-Bench, reported as percent agreement on non-tie examples across training iterations and compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Self-Taught Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise judgement of instruction-following responses (general instruction following / dialogue evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MT-Bench (agreement with human judgments, ties excluded)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Self-Taught Evaluator (iteratively fine-tuned Llama-3-70B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Llama-3-70B-Instruct base, iteratively fine-tuned as an LLM-as-a-Judge using synthetic preference pairs and generated reasoning chains; multiple iterations reported (Iteration 1..5).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human judgments provided by MT-Bench (unnamed human annotators); ties excluded for agreement computation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>agreement rate (percentage of non-tie examples where judge agrees with human)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>78.9</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>order sensitivity (ties excluded); remaining gap/variance across tasks relative to best human agreement; occasional inconsistency across orders</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Agreement improved across iterations (seed 65.2% avg acc in some table representations for pairwise order metrics; in agreement-with-human table seed 77.8% -> iter1 79.0% -> iter2 78.7% -> iter3 78.9% -> iter4 77.5% -> iter5 78.9%); the Self-Taught Evaluator performs on par with or slightly outperforms GPT-4 (GPT4-0125 reported ~79.1% in the same table). Reporting excludes ties and averages across response-order permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Can reach human-level agreement (on-par with GPT-4) after synthetic iterative training; explainable judgments via chain-of-thought; scalable without additional human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pairwise evaluation; ties excluded; two response orders tested and averaged; judgments sampled (diverse generations) and consensus/majority procedures used during inference; reported per-iteration results for models fine-tuned iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Taught Evaluators', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7854.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7854.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RewardBench performance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation accuracy of LLM-as-a-Judge on RewardBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Performance (accuracy) of the Self-Taught Evaluator on RewardBench, showing large improvement over the seed Llama-3-70B-Instruct and competitive or superior performance compared to reward models trained with human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Self-Taught Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise preference evaluation of assistant responses (RewardBench leaderboard metric)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>RewardBench</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Self-Taught Evaluator (Llama-3-70B-Instruct fine-tuned iteratively)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Llama-3-70B-Instruct, iteratively fine-tuned using synthetic preference pairs and judged reasoning chains; majority-vote variants (e.g., 32-sample majority vote) also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>not directly human evaluators for final leaderboard scores; comparisons are to reward models trained on human-labeled datasets (e.g., HelpSteer2) and leaderboard entries</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>RewardBench overall accuracy / leaderboard score (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>88.3</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>worse on easier Chat category relative to seed (possible distribution focus on harder examples); higher inference cost due to generative chain-of-thought outputs</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-Taught Evaluator improved from seed 75.4% to 88.3% (iteration 5); with 32-sample majority voting reached 88.7%; this performance matches or outperforms reward models trained on human-labeled data (e.g., Llama-3-70B-Instruct w/ HelpSteer2 as LLM-as-a-Judge: 85.6%); classifier-based reward model (nvidia/Llama3 70B RM with HelpSteer2) reported 88.8% on the leaderboard (comparable).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Achieves top leaderboard performance without any human annotated preference labels; scalable synthetic-data pipeline; improves across iterations automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pairwise comparisons on RewardBench; models fine-tuned iteratively on synthetic preference/judgment data; majority-vote inference (e.g., 32 samples) evaluated; per-category breakdowns (Chat, Chat Hard, Safety, Reasoning) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Taught Evaluators', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7854.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7854.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic vs human-labeled training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of iterative training on synthetic preferences versus human-labeled preferences (HelpSteer2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct comparison in the paper between iterative self-training on synthetic preference data and iterative training using human-labeled HelpSteer2 preferences, reporting that synthetic-data iterative training yielded larger improvements on RewardBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Self-Taught Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Training reward/evaluator models for pairwise response preference evaluation (RewardBench evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HelpSteer2 (human-labeled preferences) and synthetic preferences derived from WildChat/GSM8K/etc.</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Self-Taught Evaluator (synthetic) vs Llama-3-70B-Instruct fine-tuned on HelpSteer2 (human-labeled)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Both use Llama-3-70B variants as base; one pipeline uses only synthetic preference pairs and synthetic judged chains, the other uses human-labeled HelpSteer2 preference labels to train an LLM-as-a-Judge.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>HelpSteer2 human annotated preference labels (dataset-provided human scores aggregated into a preference)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>RewardBench overall accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Human-labeled iterative training showed smaller per-iteration gains and did not outperform synthetic-only iterative training in this setup; some other supervised approaches (classifier RMs) trained on human labels can match or slightly exceed synthetic approach on RewardBench.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Iterative training with synthetic preferences yielded larger improvements on RewardBench (final 88.3) than iterative training using HelpSteer2 labels (final reported ~85.6 when training an LLM-as-a-Judge on HelpSteer2). However, other supervised reward-model training approaches using HelpSteer2 (classifier RM) have reported comparable top scores (e.g., 88.8) — indicating dataset and model-family interactions matter.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Synthetic-only pipeline avoids expensive human annotation and can adapt as models improve; combining synthetic and human data retained strong performance and in some mixes gave slight improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Iterative training where in one condition judgments and preferences are synthetic (constructed via instruction modification and generated bad responses) and in the other condition the HelpSteer2 human-labeled preferences are used; evaluations run on RewardBench for final comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Taught Evaluators', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7854.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7854.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Position bias / order sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Order sensitivity (position bias) of generative LLM-as-a-Judge evaluations on RewardBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed failure mode where LLM-as-a-Judge judgments change with the order in which responses (A/B) are presented; quantified via experiments that fix the winner's position or randomize order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Self-Taught Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise response preference evaluation (RewardBench)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>RewardBench (order-sensitivity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Self-Taught Evaluator (iteration 5 reported)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Llama-3-70B-Instruct iteratively fine-tuned; evaluation includes shuffling response order and extreme cases where the winning response is always first or always last.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>not applicable (analysis of judge model sensitivity), but human labels used elsewhere for ground truth comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (percentage) under different response-order conditions</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>88.3</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>position bias (judgment changes with response order); variance across different seeds for ordering can materially affect reported scores</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>On the 5th-iteration model evaluated on RewardBench: average (recommended average of y_w always first and y_l always first) = 88.3%; when y_w (winning response) always placed first accuracy = 85.5%; when y_l (losing) always placed first accuracy = 91.1%. Authors recommend reporting averaged performance across orders to mitigate bias.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Majority-vote / order-averaging procedures can reduce variance and improve reported performance; analysis reveals sensitivity so practitioners can correct evaluation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluated model under three ordering regimes (randomly shuffled seeds, winner always first, loser always first); reported per-regime accuracies and recommended reporting average of extremes to fairly consider order effect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Taught Evaluators', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7854.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7854.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HelpSteer2 validation comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agreement and position-consistent accuracy on HelpSteer2 validation when using LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of the Self-Taught Evaluator against human labels on the HelpSteer2 validation split shows improved average accuracy and improved position-consistent accuracy relative to the seed Llama-3-70B-Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Self-Taught Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise preference evaluation using human-labeled HelpSteer2 preferences</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HelpSteer2 (validation split)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Self-Taught Evaluator (iteratively fine-tuned Llama-3-70B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Llama-3-70B-Instruct base; evaluated by swapping response order and computing average accuracy and position-consistent accuracy using HelpSteer2 human aggregated scores as ground truth preference.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>HelpSteer2 human labels (aggregated multi-aspect scores used to derive ground-truth preference)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>average accuracy across two response orders; position-consistent accuracy (judgment must be same across both orders to be counted)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>position inconsistency (models penalized when predictions flip across orders); no explicit numeric agreement values provided in text for all reported fields</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-Taught Evaluator improved both average accuracy and position-consistent accuracy compared to the seed model, indicating synthetic training improved robustness to order and overall agreement with human-labeled preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Improved consistency and accuracy relative to seed without using human labels in training pipeline; can exploit human-labeled splits for selection and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluation on HelpSteer2 validation; two response orders averaged and position-consistent accuracy computed; human labels used to derive ground-truth preference by summing weighted aspect scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Taught Evaluators', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RewardBench: Evaluating reward models for language modeling <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>HelpSteer2: Open-source dataset for training top-performing reward models <em>(Rating: 2)</em></li>
                <li>LLMs instead of human judges? A large scale empirical study across 20 NLP evaluation tasks <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7854",
    "paper_id": "paper-271709606",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "MT-Bench agreement",
            "name_full": "Agreement between LLM-as-a-Judge and human judgments on MT-Bench",
            "brief_description": "Quantitative agreement of the paper's Self-Taught Evaluator (iterative Llama-3-70B-Instruct fine-tuning) with human judgments on MT-Bench, reported as percent agreement on non-tie examples across training iterations and compared to GPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Self-Taught Evaluators",
            "evaluation_task": "Pairwise judgement of instruction-following responses (general instruction following / dialogue evaluation)",
            "dataset_name": "MT-Bench (agreement with human judgments, ties excluded)",
            "judge_model_name": "Self-Taught Evaluator (iteratively fine-tuned Llama-3-70B-Instruct)",
            "judge_model_details": "Llama-3-70B-Instruct base, iteratively fine-tuned as an LLM-as-a-Judge using synthetic preference pairs and generated reasoning chains; multiple iterations reported (Iteration 1..5).",
            "human_evaluator_type": "human judgments provided by MT-Bench (unnamed human annotators); ties excluded for agreement computation",
            "agreement_metric": "agreement rate (percentage of non-tie examples where judge agrees with human)",
            "agreement_score": 78.9,
            "reported_loss_aspects": "order sensitivity (ties excluded); remaining gap/variance across tasks relative to best human agreement; occasional inconsistency across orders",
            "qualitative_findings": "Agreement improved across iterations (seed 65.2% avg acc in some table representations for pairwise order metrics; in agreement-with-human table seed 77.8% -&gt; iter1 79.0% -&gt; iter2 78.7% -&gt; iter3 78.9% -&gt; iter4 77.5% -&gt; iter5 78.9%); the Self-Taught Evaluator performs on par with or slightly outperforms GPT-4 (GPT4-0125 reported ~79.1% in the same table). Reporting excludes ties and averages across response-order permutations.",
            "advantages_of_llm_judge": "Can reach human-level agreement (on-par with GPT-4) after synthetic iterative training; explainable judgments via chain-of-thought; scalable without additional human annotation.",
            "experimental_setting": "Pairwise evaluation; ties excluded; two response orders tested and averaged; judgments sampled (diverse generations) and consensus/majority procedures used during inference; reported per-iteration results for models fine-tuned iteratively.",
            "uuid": "e7854.0",
            "source_info": {
                "paper_title": "Self-Taught Evaluators",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "RewardBench performance",
            "name_full": "Evaluation accuracy of LLM-as-a-Judge on RewardBench",
            "brief_description": "Performance (accuracy) of the Self-Taught Evaluator on RewardBench, showing large improvement over the seed Llama-3-70B-Instruct and competitive or superior performance compared to reward models trained with human labels.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Self-Taught Evaluators",
            "evaluation_task": "Pairwise preference evaluation of assistant responses (RewardBench leaderboard metric)",
            "dataset_name": "RewardBench",
            "judge_model_name": "Self-Taught Evaluator (Llama-3-70B-Instruct fine-tuned iteratively)",
            "judge_model_details": "Llama-3-70B-Instruct, iteratively fine-tuned using synthetic preference pairs and judged reasoning chains; majority-vote variants (e.g., 32-sample majority vote) also evaluated.",
            "human_evaluator_type": "not directly human evaluators for final leaderboard scores; comparisons are to reward models trained on human-labeled datasets (e.g., HelpSteer2) and leaderboard entries",
            "agreement_metric": "RewardBench overall accuracy / leaderboard score (percentage)",
            "agreement_score": 88.3,
            "reported_loss_aspects": "worse on easier Chat category relative to seed (possible distribution focus on harder examples); higher inference cost due to generative chain-of-thought outputs",
            "qualitative_findings": "Self-Taught Evaluator improved from seed 75.4% to 88.3% (iteration 5); with 32-sample majority voting reached 88.7%; this performance matches or outperforms reward models trained on human-labeled data (e.g., Llama-3-70B-Instruct w/ HelpSteer2 as LLM-as-a-Judge: 85.6%); classifier-based reward model (nvidia/Llama3 70B RM with HelpSteer2) reported 88.8% on the leaderboard (comparable).",
            "advantages_of_llm_judge": "Achieves top leaderboard performance without any human annotated preference labels; scalable synthetic-data pipeline; improves across iterations automatically.",
            "experimental_setting": "Pairwise comparisons on RewardBench; models fine-tuned iteratively on synthetic preference/judgment data; majority-vote inference (e.g., 32 samples) evaluated; per-category breakdowns (Chat, Chat Hard, Safety, Reasoning) reported.",
            "uuid": "e7854.1",
            "source_info": {
                "paper_title": "Self-Taught Evaluators",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Synthetic vs human-labeled training",
            "name_full": "Comparison of iterative training on synthetic preferences versus human-labeled preferences (HelpSteer2)",
            "brief_description": "Direct comparison in the paper between iterative self-training on synthetic preference data and iterative training using human-labeled HelpSteer2 preferences, reporting that synthetic-data iterative training yielded larger improvements on RewardBench.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Self-Taught Evaluators",
            "evaluation_task": "Training reward/evaluator models for pairwise response preference evaluation (RewardBench evaluation)",
            "dataset_name": "HelpSteer2 (human-labeled preferences) and synthetic preferences derived from WildChat/GSM8K/etc.",
            "judge_model_name": "Self-Taught Evaluator (synthetic) vs Llama-3-70B-Instruct fine-tuned on HelpSteer2 (human-labeled)",
            "judge_model_details": "Both use Llama-3-70B variants as base; one pipeline uses only synthetic preference pairs and synthetic judged chains, the other uses human-labeled HelpSteer2 preference labels to train an LLM-as-a-Judge.",
            "human_evaluator_type": "HelpSteer2 human annotated preference labels (dataset-provided human scores aggregated into a preference)",
            "agreement_metric": "RewardBench overall accuracy (percentage)",
            "agreement_score": null,
            "reported_loss_aspects": "Human-labeled iterative training showed smaller per-iteration gains and did not outperform synthetic-only iterative training in this setup; some other supervised approaches (classifier RMs) trained on human labels can match or slightly exceed synthetic approach on RewardBench.",
            "qualitative_findings": "Iterative training with synthetic preferences yielded larger improvements on RewardBench (final 88.3) than iterative training using HelpSteer2 labels (final reported ~85.6 when training an LLM-as-a-Judge on HelpSteer2). However, other supervised reward-model training approaches using HelpSteer2 (classifier RM) have reported comparable top scores (e.g., 88.8) — indicating dataset and model-family interactions matter.",
            "advantages_of_llm_judge": "Synthetic-only pipeline avoids expensive human annotation and can adapt as models improve; combining synthetic and human data retained strong performance and in some mixes gave slight improvements.",
            "experimental_setting": "Iterative training where in one condition judgments and preferences are synthetic (constructed via instruction modification and generated bad responses) and in the other condition the HelpSteer2 human-labeled preferences are used; evaluations run on RewardBench for final comparison.",
            "uuid": "e7854.2",
            "source_info": {
                "paper_title": "Self-Taught Evaluators",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Position bias / order sensitivity",
            "name_full": "Order sensitivity (position bias) of generative LLM-as-a-Judge evaluations on RewardBench",
            "brief_description": "Observed failure mode where LLM-as-a-Judge judgments change with the order in which responses (A/B) are presented; quantified via experiments that fix the winner's position or randomize order.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Self-Taught Evaluators",
            "evaluation_task": "Pairwise response preference evaluation (RewardBench)",
            "dataset_name": "RewardBench (order-sensitivity analysis)",
            "judge_model_name": "Self-Taught Evaluator (iteration 5 reported)",
            "judge_model_details": "Llama-3-70B-Instruct iteratively fine-tuned; evaluation includes shuffling response order and extreme cases where the winning response is always first or always last.",
            "human_evaluator_type": "not applicable (analysis of judge model sensitivity), but human labels used elsewhere for ground truth comparisons",
            "agreement_metric": "accuracy (percentage) under different response-order conditions",
            "agreement_score": 88.3,
            "reported_loss_aspects": "position bias (judgment changes with response order); variance across different seeds for ordering can materially affect reported scores",
            "qualitative_findings": "On the 5th-iteration model evaluated on RewardBench: average (recommended average of y_w always first and y_l always first) = 88.3%; when y_w (winning response) always placed first accuracy = 85.5%; when y_l (losing) always placed first accuracy = 91.1%. Authors recommend reporting averaged performance across orders to mitigate bias.",
            "advantages_of_llm_judge": "Majority-vote / order-averaging procedures can reduce variance and improve reported performance; analysis reveals sensitivity so practitioners can correct evaluation protocols.",
            "experimental_setting": "Evaluated model under three ordering regimes (randomly shuffled seeds, winner always first, loser always first); reported per-regime accuracies and recommended reporting average of extremes to fairly consider order effect.",
            "uuid": "e7854.3",
            "source_info": {
                "paper_title": "Self-Taught Evaluators",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "HelpSteer2 validation comparison",
            "name_full": "Agreement and position-consistent accuracy on HelpSteer2 validation when using LLM-as-a-Judge",
            "brief_description": "Evaluation of the Self-Taught Evaluator against human labels on the HelpSteer2 validation split shows improved average accuracy and improved position-consistent accuracy relative to the seed Llama-3-70B-Instruct.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Self-Taught Evaluators",
            "evaluation_task": "Pairwise preference evaluation using human-labeled HelpSteer2 preferences",
            "dataset_name": "HelpSteer2 (validation split)",
            "judge_model_name": "Self-Taught Evaluator (iteratively fine-tuned Llama-3-70B-Instruct)",
            "judge_model_details": "Llama-3-70B-Instruct base; evaluated by swapping response order and computing average accuracy and position-consistent accuracy using HelpSteer2 human aggregated scores as ground truth preference.",
            "human_evaluator_type": "HelpSteer2 human labels (aggregated multi-aspect scores used to derive ground-truth preference)",
            "agreement_metric": "average accuracy across two response orders; position-consistent accuracy (judgment must be same across both orders to be counted)",
            "agreement_score": null,
            "reported_loss_aspects": "position inconsistency (models penalized when predictions flip across orders); no explicit numeric agreement values provided in text for all reported fields",
            "qualitative_findings": "Self-Taught Evaluator improved both average accuracy and position-consistent accuracy compared to the seed model, indicating synthetic training improved robustness to order and overall agreement with human-labeled preferences.",
            "advantages_of_llm_judge": "Improved consistency and accuracy relative to seed without using human labels in training pipeline; can exploit human-labeled splits for selection and validation.",
            "experimental_setting": "Evaluation on HelpSteer2 validation; two response orders averaged and position-consistent accuracy computed; human labels used to derive ground-truth preference by summing weighted aspect scores.",
            "uuid": "e7854.4",
            "source_info": {
                "paper_title": "Self-Taught Evaluators",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RewardBench: Evaluating reward models for language modeling",
            "rating": 2,
            "sanitized_title": "rewardbench_evaluating_reward_models_for_language_modeling"
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "HelpSteer2: Open-source dataset for training top-performing reward models",
            "rating": 2,
            "sanitized_title": "helpsteer2_opensource_dataset_for_training_topperforming_reward_models"
        },
        {
            "paper_title": "LLMs instead of human judges? A large scale empirical study across 20 NLP evaluation tasks",
            "rating": 2,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        },
        {
            "paper_title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 1,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        }
    ],
    "cost": 0.01538525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Taught Evaluators
8 Aug 2024</p>
<p>Tianlu Wang 
equal contribution Meta FAIR</p>
<p>Ilia Kulikov 
equal contribution Meta FAIR</p>
<p>Olga Golovneva 
equal contribution Meta FAIR</p>
<p>Ping Yu 
equal contribution Meta FAIR</p>
<p>Weizhe Yuan 
equal contribution Meta FAIR</p>
<p>Jane Dwivedi 
equal contribution Meta FAIR</p>
<p>Yu Richard 
equal contribution Meta FAIR</p>
<p>Yuanzhe Pang 
equal contribution Meta FAIR</p>
<p>Maryam Fazel-Zarandi 
equal contribution Meta FAIR</p>
<p>Jason Weston 
equal contribution Meta FAIR</p>
<p>Xian Li 
equal contribution Meta FAIR</p>
<p>Self-Taught Evaluators
8 Aug 2024AC33AD5DFC4F1B07B31965F5C49DE809arXiv:2408.02666v2[cs.CL]
Model-based evaluation is at the heart of successful model development -as a reward model for training, and as a replacement for human evaluation.To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve.In this work, we present an approach that aims to improve evaluators without human annotations, using synthetic training data only.Starting from unlabeled instructions, our iterative selfimprovement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions.Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench.This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.</p>
<p>Introduction</p>
<p>Large language models (LLMs) rely on strong evaluators at every stage of the development lifecycle.They are used at training time as reward models to align with human preferences (Bai et al., 2022;Ouyang et al., 2022) or for iterative selfimprovement (Yuan et al., 2024), and at inference time as an alternative to human evaluation (Li et al., 2023;Chiang and Lee, 2023;Wang et al., 2023a;Liu et al., 2023).Improvements in evaluation capabilities will thus clearly benefit this entire workflow -including empowering the scientific research process itself as we aim to develop better overall techniques.</p>
<p>Building such strong evaluator models usually relies on large amounts of high-quality preference data from human annotation over model responses, which can be costly and time-consuming to collect, as it requires expert annotation for challenging tasks (e.g., coding and mathematics).This dependency on human-generated data poses significant challenges for scaling to new tasks or evaluation criteria.Furthermore, as new models inevitably improve over older ones, these existing annotations will typically become outdated, as the judgments are based on annotations of older, less performant, model responses.</p>
<p>In this work, we instead explore an iterative selftraining approach which uses no human annotated preferences in the training loop, relying purely on synthetically generated data.Given a seed model, our method first uses prompting to generate contrasting synthetic preference pairs for a given input, such that one response is designed to be inferior to the other.Next, using the model as an LLM-as-a-Judge, we generate reasoning traces and judgments for these pairs, which we can label as correct or not given our synthetic preference pair design.After training on this labeled data we obtain a superior LLM-as-a-Judge, from which we can then iterate the whole process in order for it to self-improve.</p>
<p>In our experiments, starting from Llama-3-70B-Instruct, the proposed method improves the accuracy on RewardBench (Lambert et al., 2024) from 75.4 to 88.7 (with majority vote, or 88.3 without).This matches or outperforms the performance of reward models derived from the same Llama-3-70B-Instruct model that uses human annotations, for example using the HelpSteer2 dataset (Wang et al., 2024b) of 10k annotations achieves a performance of 85.6 using the same LLM-as-a-Judge setup.</p>
<p>Related Work</p>
<p>LLM-based Evaluators While traditional evaluation benchmarks employ automated metrics that require a reference answer (Wang et al., 2019; Ra- Because human evaluation per response can be costly, many recent works have proposed LLMs as effective evaluators.These come in several flavors: as classifiers that output scores directly (Zhu et al., 2023;Wang et al., 2024a) or via LLM-as-a-Judge prompting that can first generate a chain-ofthought in natural language, which helps provide explanations for judgments (Zheng et al., 2023).</p>
<p>Responses can also be scored alone (Kim et al., 2023) or pairwise relative to each other (Dubois et al., 2023;Li et al., 2023;Bai et al., 2023;Saha et al., 2024).LLM evaluation shows great promise as a scalable proxy for human raters, and in the case of LLM-as-a-Judge as an explainable proxy as well (Ye et al., 2024;Zheng et al., 2023).However, many of these "off-the-shelf" evaluators demonstrate high variance across many tasks (Bavaresco et al., 2024), indicating the need for improved methods.</p>
<p>Synthetic Data Synthetic data has emerged as a promising solution for efficiently acquiring training examples and can be particularly valuable in settings where real-world data can be hard to access (e.g., weather data covering all conditions (Lam et al., 2023)) or where correct annotations can be challenging to acquire (e.g., coding tasks (Liu et al., 2024)).Additionally, synthetic data has the benefit of being easily customizable to specific requirements, such as different evaluation criteria or safety constraints (Kim et al., 2023;El Emam et al., 2020;Howe et al., 2017).The use of synthetic data has been beneficial in model alignment (Lee et al., 2023), improving the original model's capabilities (Yuan et al., 2024;Li et al., 2024a;Yu et al., 2024;Li et al., 2024b), and teaching the model new skills (Schick et al., 2023;Lanchantin et al., 2023).In the context of evaluation, synthetic data has been used to measure tasks such as factuality (Wei et al., 2024;Feng et al., 2023), safety (Perez et al., 2023;Hubinger et al., 2024), coding (Gu et al., 2024), and general instruction following (Zeng et al., 2024), showing strong correlation with real human judgments.The West-of-n approach (Pace et al., 2024) has been used to improve reward models by constructing preference pairs using the best and worst scoring pairs from an initial model.</p>
<p>For LLM-as-a-Judge models specifically, synthetic responses have been generated prompting the LLM to produce a given quality response (Kim et al., 2023).</p>
<p>Method</p>
<p>We consider the setting of pairwise evaluation using the LLM-as-a-Judge approach (Zheng et al., 2023) that takes:</p>
<p>• an input (user instruction) x; and</p>
<p>• two possible assistant responses y (A) and y (B)  to the user instruction x; and</p>
<p>• the evaluation prompt containing the rubric and asking to evaluate and choose the winning answer, see e.g., Figure 8.</p>
<p>The goal of the LLM-as-a-Judge model is to output a preference of which response y is better: A or B. In order to do this it is common to output, prior to the final judgment, a chain-of-thought (or "reasoning chain"), which is a set of steps generated in natural language that helps the model decide its final judgment.</p>
<p>Such models can be used as pairwise reward models to build training data for preference optimization, e.g., for training methods like DPO (Rafailov et al., 2023), Iterative DPO (Xu et al., 2023) and Self-Rewarding methods (Yuan et al., 2024).They can also be used for evaluation; e.g., many popular benchmark leaderboards are built by using a fixed LLM-as-a-Judge evaluation model (Li et al., 2023) such as GPT4 (Achiam et al., 2023).</p>
<p>We propose a novel recipe for training such an evaluator.Our overall method is an iterative training scheme that bootstraps improvements by annotating the current model's judgments using constructed synthetic data -so that the Self-Taught Evaluator is more performant on the next iteration.</p>
<p>Our overall pipeline is thus as follows:</p>
<p>• Initialization: We assume access to a large set of human-written user instructions, e.g., of the type that is commonly collected in production systems, and an initial seed LLM.</p>
<p>• Instruction Selection: We next select a challenging, balanced distribution of user instructions from the uncurated set by categorizing them via LLM.</p>
<p>• Response Pair Construction: For each user instruction (example) we create a preference pair of two model responses (chosen &amp; rejected), generating them via prompting such that the rejected response is likely of lower quality than the chosen response.</p>
<p>• Iterative Training: We then iterate the following two steps:</p>
<p>(i) Judgment Annotation: For each example, we sample from the current model up to N times LLM-as-a-Judge generated reasoning traces and judgments.If we find a correct judgment we add that example to our training set, otherwise we discard it.</p>
<p>(ii) Model Fine-tuning: We fine-tune the model on the newly constructed training set which yields an updated model for the next iteration.</p>
<p>Note that in each iteration of training the size of the training set depends on the quality of the current model.We expect that as the model improves, the size of the training set will increase as well, as the model will be able to find more correct judgments, giving the model a kind of automatic curriculum.</p>
<p>We next describe each of these steps in detail.</p>
<p>Initialization</p>
<p>We assume we have access to a pool of user instructions {x i }.Each sample x i can either be one single text instruction or a multi-turn dialog history of turns between the user and the assistant, with the last turn being an instruction or question from the user.Instructions typically involve different skills such as general knowledge and reasoning, coding, safety, and mathematical reasoning.</p>
<p>Instruction Selection</p>
<p>Given a pool of human-written user instructions, there may be a large degree of noise, as well as an imbalance in terms of topic, variety, difficulty, and ability of the model to answer.We therefore aim to select a subset of instructions to generate highquality synthetic responses and judgments that can be further used for training.We classify each input using an LLM into a given category, for example coding, reasoning, brainstorming, etc.The precise prompt we use is given in Figure 7.We are then free to select data from within those categories, and to discard certain categories not deemed to be useful for training.</p>
<p>Response Pair Construction</p>
<p>For each input x i in our curated training pool, we next generate preference data involving two responses y (w) i and y</p>
<p>(l)</p>
<p>i where w is expected to be preferable (winning) over l (losing).We achieve this by generating the data in a synthetic manner without using human annotation.</p>
<p>Given the instruction x i , we first prompt an instruction-following LLM to generate a baseline response y w i as usual.We then prompt the LLM to generate a "noisy" version of the original instruction x ′ i = ϕ(x i ).We do this using the prompt template given in Figure 2, where we ask to "generate a modified instruction that is highly relevant but not semantically identical to the instruction above from the user."We then prompt the LLM for a high-quality response y l i to x ′ i , which would not be a good response for x i .This yields a synthetic preference y w i ≻ y l i for the original input x i .This paired data is then used to construct training examples:
(x i , y (A) i , y (B) i )
where we randomize the order of whether the winner is w = A or w = B, which is important to deal with position bias for LLM-as-a-Judge inference.</p>
<p>Prompt Template for Generating Response Pairs with Synthetic Preference</p>
<p>Below is a conversation between an user and an AI Assistant.{Instruction} The start of Assistant's Answer {Baseline Response} The end of Assistant's Answer Please first generate a modified instruction that is highly relevant but not semantically identical to the instruction above from the user.Then write a high-quality answer which is a good response to the modified instruction but not a good response to the original user question.IMPORTANT: Please strictly follow the following format:</p>
<p>User Question Modified <provide a modified instruction here></p>
<p>The start of Assistant's answer to the modified instruction <provide a high-quality response to the modified instruction> The end of Assistant's answer to the modified instruction Figure 2: Generating Synthetic Response Pairs.We use the following prompt template which is used to generate a "worse response" y l .Given an instruction x and baseline response y w generated by an instruction-following LLM as usual, this prompt is used to first generate a "noisy" version x ′ of the original instruction x, and then a best-attempt y l at responding to x ′ .y l is then treated as a poor response to x, giving a preference pair y w i ≻ y l i .</p>
<p>Judgment Annotation</p>
<p>Our LLM-as-a-Judge model is used to generate evaluation judgments (reasoning chains and verdicts) {j i } for each training example e i := (x i , y
(A) i , y(B)
i ) in the following manner: for a given input e i , we collect N diverse evaluations J := {j 1 i , . . ., j N i } by sampling from the model.We then apply rejection sampling to filter J by removing j n i when the final verdict disagrees with the ground truth labeling, derived from Subsection 3.3.We then select a single correct reasoning chain and verdict at random from the pool of correct solutions.If no such judgment exists (J is empty) then we discard the example.</p>
<p>This now allows us to construct our final training examples of synthetic preferences for fine-tuning:
((x i , y (A) i , y (B) i ), j i ).</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Training.Our initial model M 0 is initialized from Llama3-70B-Instruct.In each iteration i = 1, . . .T , we use model M i−1 from the previous iteration to generate synthetic preferences followed by judgments on the training data, and then finetune Llama3-70B-Instruct again.We use fairseq2 library (Balioglu, 2023) for instruction finetuning and vLLM (Kwon et al., 2023) for inference.During training the negative log-likelihood loss is only applied to the evaluation part, i.e., j i of the training example.Training hyperparameters are provided in Table 7. Model selection is done using a combination of pairwise judgment accuracy and position bias computed over the held out set.Sampling parameters used for generations are provided in Table 8.</p>
<p>Instructions and Responses.We start with a large pool of human-written instructions {x i } from the WildChat dataset (Zhao et al., 2024).To perform prompt selection, we annotate the category of each instruction with the Mixtral 22Bx8 Instruct model, using the template in Figure 7 and select 20,582 examples in the reasoning category, as we expect these to be challenging inputs.For the selected inputs we generate synthetic responses y w i and y l i using Mixtral 22Bx8 Instruct following Subsection 3.3 and Figure 2. The training data is constructed as (<system prompt>, {(x i , y
(A) i , y (B) i , j i )}).
We use the standard system prompt from MT-Bench and Reward-Bench as shown in Figure 8.</p>
<p>Majority Vote Inference.As LLM-as-a-Judge uses chain-of-though reasoning chains generated by the LLM followed by a verdict, it is known that majority vote inference can yield improvements in these cases (Wang et al., 2023b).At inference time when evaluating final performance we sample generations N times, and take the final judgment to be the most common verdict.</p>
<p>Other Data Sources</p>
<p>To understand the effectiveness of the proposed method, we generate synthetic judgments using the same approach but based on the following data sources:</p>
<p>• HelpSteer2 (Wang et al., 2024b).We generate evaluations conditioned on the scores of helpfulness, correctness, coherence, complexity and verbosity provided the dataset.</p>
<p>We use the aggregated score to derive the ground truth preference for each example using the recommended weighting [0.65, 0.8, 0.45, 0.55, −0.4] 1 .</p>
<p>• GSM8K (Cobbe et al., 2021).We sample from an instruction-following model multiple times to get y w when the final solution agrees with the ground truth and y l vise versa.</p>
<p>• Coding instructions from WildChat.Similar to the "reasoning" prompts we selected from WildChat used in the main experiment, we also experimented with prompts annotated with the "Coding" category.</p>
<p>• hh_rlhf (Bai et al., 2022).We generate evaluations on the prompts and responses provided 1 Recommended weighting was taken from https://huggingface.co/nvidia/Llama3-70B-SteerLM-RM.</p>
<p>in the "harmless_base" training split.Then we take human preferences provided by the dataset as ground truth to perform rejection sampling to construct judgments.</p>
<p>Evaluation</p>
<p>We evaluate the accuracy of our Self-Taught Evaluator model on the following benchmarks:</p>
<p>• RewardBench (Lambert et al., 2024).We use the standard evaluation protocol provided by the leaderboard.</p>
<p>• MT-Bench (Zheng et al., 2023).We report agreement rate with human judgments when examples with ties are excluded.</p>
<p>• HelpSteer2 (Wang et al., 2024b).We evaluate on the validation split.</p>
<p>Results</p>
<p>RewardBench</p>
<p>Results on RewardBench are given in Table 1.We find that our Self-Taught Evaluator which is trained iteratively on synthetic data without any annotated preference labels significantly improves over the seed Llama3-70B-Instruct model, matching topperforming reward models trained with labeled data.Our approach improves its results across training iterations, and achieves an overall score of 88.3 on iteration 5, while the seed model it starts from obtains 75.4.Training an LLM-as-a-Judge in a similar manner starting from the same seed using the labeled HelpSteer2 data we only obtain 85.6, hence we obtain superior performance without using human labeled data.Compared to the seed model, we observe improvements using our approach in evaluating instructions in the Chat Hard, Safety and Reasoning categories, while being worse on the easier Chat category -perhaps because our unlabeled training data focused the model on harder examples.</p>
<p>Improving results further with majority voting</p>
<p>As also shown in Table 1, with 32-sample majority voting, our third iteration of Self-Taught Evaluator model reaches an overall performance of 88.7 on RewardBench, outperforming many other existing reward models.</p>
<p>MT-Bench</p>
<p>We report results on MT-Bench in votes (A and B are considered equally good).Since our models are trained to give binary decisions, we only report the agreement on non-tie examples.For each pair of responses A and B, we test two orders:</p>
<p>where response A appears first and response B appears first, and average the results.We find that our Self-Taught Evaluator again outperforms the Llama3-70B-Instruct seed model, and is on par or slightly outperforms GPT4-0125.</p>
<p>HelpSteer2</p>
<p>Results on the HelpSteer2 validation set are given in Table 3.We report the average accuracy of two orders and three seeds by swapping the response order in a similar manner, as well as reporting both orders separately (right answer first or second) to test for position bias.We further compute the position-consistent accuracy, treating a judgment as incorrect when a model has different predictions on the two orderings.We use the human labels from the Helpsteer2 dataset and treat the response with higher summed scores as the better response.We find that our Self-Taught Evaluator method improves both average accuracy and position-consistent accuracy compared to the seed Llama-3-70B-Instruct model.</p>
<p>6 Ablations and Analysis</p>
<p>Synthetic Data from Other Sources</p>
<p>In Table 4, we compare Self-Taught Evaluator models trained on synthetic preferences constructed from different sources.We found data sources focusing on different skills, such as coding, mathematical reasoning, etc. are all effective in turning a strong instruction-following LLM into a strong LLM-as-a-Judge.Intuitively, we find that data sources generally improve the categories in Re-wardBench that are related to their distribution.</p>
<p>Synthetic Bad Response Generation</p>
<p>In our experiments we generate synthetic data by first generating a similar instruction, and then a good response for the similar instruction -with the aim that this will be a bad response for the original instruction.An alternative is to just prompt an LLM to generate a bad response to the original instruction directly.We use the prompt template given in Figure 10 and otherwise conduct training as before on the same set of reasoning-based instructions.This approach obtains a RewardBench overall score of 80.7, which still works -but is     worse than using our proposed approach, which achieves 83.8.</p>
<p>Comparison of Synthetic Data with Human Annotated Data</p>
<p>We conducted the same iterative training using labeled preference data from HelpSteer2 (Wang et al., 2024b), rather than synthetic data.On Reward-Bench, as is shown in Table 5, the improvement from each iteration is smaller and the final model did not outperform iterative training on synthetic preferences.We note that these experiments use data to train an LLM-as-a-Judge.Other results in the literature have used the HelpSteer2 to train classifier-based reward models with slightly better results on RewardBench, e.g., obtaining 88.8 using Llama-3-70B, see Table 1.</p>
<p>Iterative Training by Initializing from Labeled Data</p>
<p>We further explore how to utilize labeled data in our pipeline.We first finetune a model on Help-steer2 (Wang et al., 2024b) and use this model to generate judgements.In this way, we obtain synthetic data by utilizing a model finetuned on labeled data.We conducted iterative training and present results in Table 12.We observed good performance compared to the seed model (Llama-3-70B-Instruct), however it does not clearly outperform conducting iterative training with unlabeled data alone.</p>
<p>Combining Synthetic and Human Labeled Preference Data</p>
<p>We compare how combining synthetic preference data with human labelled preference data affects model performance.In particular, we combine synthetic preferences generated from reasoning Wild-Chat prompts with the human labeled HelpSteer2 dataset (train split) and report performance in Table 6.We compare to first-iteration models trained on single data source, and select the best checkpoint for joint training using the validation split of HelpSteer2 and holdout set of synthetic preferences (in-distribution), as well as safety and code synthetic preferences (out-of-distribution).We then report evaluation results on RewardBench.The results show that overall the models retain strong performance across different data mixing weights, with slight improvements on overall accuracy.</p>
<p>Instruction complexity</p>
<p>We analyze the length distribution of the curated training set of selected instructions in Figure 3.The dataset has a long-tail distribution of input length, with most of the examples less than 500 tokens.In contrast, the full dataset (i.e., the full data before the instruction selection step of Subsection 3.2) has a cluster of very long instructions, containing content such as long-form coding instructions or transcripts.</p>
<p>We further instruct Llama-3-70B-Instruct to in-fer the complexity (using a score of 1-5) and category of each input instruction, as well as the length of the expected output, following the procedure in Yuan et al. (2024).From Figure 4 and Figure 6, we see that the curated dataset has more complex instructions involving logical reasoning/science whereas the full dataset has a greater proportion focused on relationships and entertainment.Finally, in Figure 5 we see that the anticipated length of the response is higher for the full dataset than the curated one, perhaps because of the greater frequency of lengthy, and sometimes repetitive instructions.</p>
<p>Conclusion</p>
<p>We present a scalable approach to build a strong generalist evaluator to perform model-based evaluation of LLM outputs.Our method constructs synthetic preferences over pairs of responses without using any human annotation.Our Self-Taught evaluator with iterative training over these synthetic preferences greatly boosts the accuracy of a strong seed LLM (Llama3-70B-Instruct) as an evaluator, from 75.4 to 88.7 on RewardBench, a new state-ofthe-art for generative LLM-as-a-Judge methods.</p>
<p>Limitations</p>
<p>Generative LLM-as-a-Judge models usually have longer outputs and thus higher inference cost than reward models that simply output a score, as LLMas-a-Judge typically first generates a reasoning chain.Further, we have used relatively large LLMs in this work (70B parameters) and made no study of whether our approach works on smaller models.Since we use a seed model to generate first synthetic preferences during our iterative training scheme, one of the assumptions is that the model is capable of generating reasonable evaluations.Thus, our approach is limited by having a capable instruction fine-tuned model which is already reasonably aligned to human (or legal/policy) preferences.Furthermore, we only investigated and reported metrics involving evaluation accuracy improvements, rather than computational requirement concerns.We also only investigated pairwise evaluation, i.e., comparing two responses, whereas it is also possible to use LLM-as-a-Judge models (or any other model) to evaluate the quality of single responses, e.g., giving them a score out of 5 or 10, rather than a pairwise A vs B judgment.We leave evaluating single responses to future work.</p>
<p>Seed</p>
<p>A.4 Using Different Models for Training Data Generation</p>
<p>In Table 10</p>
<p>Figure 1 :
1
Figure 1: Self-Taught Evaluator iterative training scheme.jpurkaret al., 2016), these types of benchmarks can pose severe limitations when evaluating openended or complex instructions where multiple valid answers are possible (e.g., creative writing and coding).Because human evaluation per response can be costly, many recent works have proposed LLMs as effective evaluators.These come in several flavors: as classifiers that output scores directly(Zhu et al., 2023;Wang et al., 2024a) or via LLM-as-a-Judge prompting that can first generate a chain-ofthought in natural language, which helps provide explanations for judgments(Zheng et al., 2023).Responses can also be scored alone(Kim et al., 2023) or pairwise relative to each other(Dubois et al., 2023;Li et al., 2023;Bai et al., 2023;Saha et al., 2024).LLM evaluation shows great promise as a scalable proxy for human raters, and in the case of LLM-as-a-Judge as an explainable proxy as well(Ye et al., 2024;Zheng et al., 2023).However, many of these "off-the-shelf" evaluators demonstrate high variance across many tasks(Bavaresco et al., 2024), indicating the need for improved methods.</p>
<ol>
<li>5
5
Model Fine-tuning (Iterative Training) Our Self-Taught Evaluator (LLM-as-a-Judge model) is first initialized with the seed LLM.The model is then trained in an iterative manner.At each iteration, we annotate the training examples with judgments as described in Subsection 3.4 using the current model, giving training examples {(x i , y (A) i , y (B) i , j i )}.These are used to train the next iteration's model by fine-tuning.Note that we initialize from the seed model at each iteration.</li>
</ol>
<p>Judge Annotation.For each training example, we sample N = 15 judgments from the model M i−1 and retain one positive sample j i per example.Then over the entire dataset we sample the same amount of examples from different labels ("A is better", "B is better") to ensure balanced training.Judgements for training M 0 were sampled from Mixtral 22Bx8 Instruct, and from the Llama model being trained in all subsequent iterations.</p>
<p>Figure 3 :
3
Figure 3: Distribution of curated training set of selected instructions compared to the full WildChat dataset.</p>
<p>Figure 4 :
4
Figure 4: Distribution of inferred complexities of curated training data versus all instructions in WildChat.</p>
<p>Figure 5 :
5
Figure 5: Distribution of estimated output lengths of curated training data versus all instructions in WildChat.</p>
<p>Figure 6 :
6
Figure 6: Distribution of inferred categories of curated training data versus all instructions in WildChat.</p>
<p>Figure 8 :
8
Figure 8: Prompt template for Judgment Annotation.This is the same prompt as used in MT-Bench and RewardBench.</p>
<p>Figure 9 :
9
Figure 9: An illustrative example of judgment generation given an instruction and two responses.</p>
<p>Table 2 .
2
UnlikeRewardBench, the MT-Bench dataset contains tie
ModelOverall Chat Chat Hard Safety ReasoningLlama-3-70B-Instruct (seed)75.497.658.969.278.5Self-Taught Evaluator, trained on synthetic data onlyIteration 183.998.369.085.782.6Iteration 286.097.575.489.581.7Iteration 387.597.279.189.783.9Iteration 487.798.080.390.582.2Iteration 588.396.684.291.581.0w/ majority voting using 32 samples88.796.984.091.582.5Baselines with Labeled DataLlama-3-70B-Instruct w/ HelpSteer2, LLM-as-a-Judge85.696.970.088.886.7nvidia/Llama3 70B RM with HelpSteer2, classifier <em>88.891.380.392.890.7Other SoTA LLM-as-a-Judge baseline modelsGPT4 0125 </em>84.395.374.387.286.9Gemini 1.5 Pro 0514 *88.192.380.687.592.0Llama3.1-405B-Instruct83.798.075.174.786.8Llama3.1-70B-Instruct82.297.869.776.385.2</p>
<p>Table 1 :
1
RewardBench Results.Our Self-Taught Evaluator trained on synthetic data without any human annotated preference labels matches top-performing reward models trained with labeled data.Models marked with (*) are taken from the RewardBench leaderboard.
ModelAgreement with HumanLlama-3-70B-Instruct (seed)77.8Self-Taught Evaluator, trained on synthetic data onlyIteration 179.0Iteration 278.7Iteration 378.9Iteration 477.5Iteration 578.9w/ majority voting using 32 samples79.5Other SoTA LLM-as-a-Judge baseline modelsGPT4-012579.1</p>
<p>Table 2 :
2
MT-Bench Results.Our Self-Taught Evaluator trained on synthetic data without any human annotated preference labels performs on par with GPT-4 judgments.
Model0-1 Acc 1-0 Acc Avg Acc Position-consistent AccLlama-3-70B-Instruct (seed)65.265.865.556.5Self-Taught Evaluator, trained on synthetic data onlyIteration 168.168.768.459.4Iteration 269.669.469.558.8Iteration 370.371.270.861.1Iteration 471.071.771.461.9Iteration 571.670.371.060.6</p>
<p>Table 3 :
3
(Wang et al., 2024b)terative training on synthetic preferences improves position-consistent accuracy compared to Llama3-70B-Instruct, measured on the HelpSteer2(Wang et al., 2024b)validation split.
Source forModelsynthetic preferences Overall Chat Chat Hard Safety ReasoningLlama-3-70B-Instruct75.497.658.969.278.5safety (hh_rlhf)79.697.255.487.078.8math (GSM8K)79.396.158.879.483.0coding (WildChat)79.496.655.985.379.7reasoning (WildChat)83.597.570.684.281.6</p>
<p>Table 4 :
4
Supervised fine-tuning with synthetic preferences from different sources improves Llama-3-70B-Instruct on various categories, as measured on RewardBench.Largest improvement in each category is highlighted in bold.</p>
<p>Table 5 :
5
Iterative training with labeled data also shows improvement on RewardBench.However, it does not outperform iterative training with synthetic preferences .
ModelOverall ChatChat HardSafety ReasoningLlama-3-70B-Instruct (seed)75.497.658.969.278.5Self-Taught Evaluator, trained on labeled HelpSteer2 preferencesIteration 185.696.970.088.886.7Iteration 286.396.172.491.185.7Iteration 387.095.074.291.287.8Iteration 487.094.177.291.685.1synthetic:HelpSteer2 ratio Overall Chat Chat Hard Safety Reasoning1 : 00.8350.9750.7060.8420.8160 : 10.8560.9690.7000.8880.8671 : 10.8420.9720.6810.8810.8361 : 20.8580.9720.7110.8910.8571 : 50.8470.9750.6810.8890.8442 : 10.8330.9720.6890.8470.8235 : 10.8580.9720.7260.8800.853</p>
<p>Table 6 :
6
Mixing data sources in different proportions can improve performance of the fine-tuned model.Synthetic preference data is generated with the Llama3-70B-Instruct model.</p>
<p>Table 9 :
9
Average accuracy on RewardBench when order of responses changes.
Average Accuracy188.91188.411188.6111188.71111188.3y w always first85.5y l always first91.1</p>
<p>we present evaluation on RewardBench of models finetuned on different training data.Note in our Self-Taught Evaluator approach we can use different LLMs to generate responses and judgements.Specifically, we try using Mixtral 22Bx8 Instruct or Llama-3-70B-Instruct in various combinations.We then finetune the Llama-3-70B-Instruct model and test on RewardBench.As shown in Table 10, the model finetuned on data generated by using the Mixtral 22Bx8 Instruct model to judge Mixtral 22Bx8 Instruct model generated responses achieves the best performance.</p>
<p>Table 11 :
11
MT-Bench Per-category Results.Our Self-Taught Evaluator trained on synthetic data without any human annotated preference labels performs on par with GPT-4 judgments.
ModelOverall ChatChat HardSafety ReasoningLlama-3-70B-Instruct (seed)75.497.658.969.278.5Self-Taught Evaluator, trained on synthetic data generated by a finetuned model (Helpsteer2)Iteration 187.095.875.890.785.8Iteration 286.692.277.491.285.8</p>
<p>Table 12 :
12
Iterative training on synthetic data generated by a model that is first fine-tuned on labeled data (Helpsteer2).</p>
<p>AcknowledgementsWe thank Jing Xu and Janice Lan for discussions and support in the project overall, and Can Balioglu for his feedback and support in LLM training using the fairseq2(Balioglu, 2023)library.We thank Nathan Lambert for the help with RewardBench, and Yuntian Deng and Yejin Choi for the help with WildChat.A AppendixA.1 Prompt TemplatesWe provide the prompt templates used for annotating and selecting instructions (Figure7), annotating judgments with synthetic preferences (Figure8), and generating ablation synthetic preference data with bad responses (Figure10). Figure9illustrates an training example based on synthetic preference data.A.2 More Training and Evaluation DetailsWe include training hyper-parameters in Table7and sampling parameters in Table8.A.3 Position Order Evaluation on RewardBenchWe notice that when we evaluate generative models on RewardBench, the order of two responses in Prompt Template for Selecting Instructions Secondly, score the instruction in terms of complexity: how complex you think it is to answer from 1-10 (where 10 is a complex question whereby first reasoning or breaking down the question into multiple subquestions for example might help improve the answer).Thirdly, indicate how long you think the response to the instruction should be, either (a) 1 sentence, (b) 1-3 sentences, (c) 1 paragraph, (d) 2 paragraphs, or (e) 3 or more paragraphs.Provide your final response in the following format: Category: <one of the categories above> Complexity: <score out of 10> Length: <choose from (a) to (e)>.DO NOT provide the actual response.each example is not fixed.More specifically, for each example, the winning response (y w ) can be randomly placed before or after the losing response (y l ).Generative models may output different judgements when the order of responses changes.Thus, we analyze how the performance varies when dif- ferent seeds are used to decide response order.In Table9, we test our model from the 5th iteration of training on RewardBench with the response order randomly shuffled, as well as two extreme cases where the winning answer always appear first or last.We recommend to report the average performance (88.3 for our 5th iteration) of "y w always first" and "y l always first" as it fairly considers both orders.Prompt Template for Judgment AnnotationPlease act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.Begin your evaluation by comparing the two responses and provide a short explanation.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.After providing your explanation, output your final verdict by strictly following this format: " Please rewrite the Assistant's Answer to make it worse.Specifically, the rewritten worse answer should closely resemble the original answer but is worse in terms of one or multiple of the following aspects: helpfulness, correctness, coherence, verbosity.IMPORTANT: Please strictly follow the following format: First, choose one or multiple aspects to generate a worse answer, such as rewrite the original answer to be unhelpful, incorrect, lack of coherence, more verbose, etc.[The start of a rewritten worse answer] <provide a worse answer here> [The end of a rewritten worse answer]Figure10: Generating a Bad Response for an Instruction.This approach is an ablation compared to our proposed approach described in the main paper.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Benchmarking foundation models with language-model-as-an-examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>. Can Balioglu, 2023. fairseq2</p>
<p>Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, arXiv:2406.18403LLMs instead of human judges? A large scale empirical study across 20 NLP evaluation tasks. 2024arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>AlpacaFarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Practical synthetic data generation: balancing privacy and the broad availability of data. Khaled El Emam, Lucy Mosquera, Richard Hoptroff, 2020O'Reilly Media</p>
<p>FactKB: Generalizable factuality evaluation using language models enhanced with factual knowledge. Shangbin Feng, Vidhisha Balachandran, Yuyang Bai, Yulia Tsvetkov, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, Sida I Wang, arXiv:2401.03065CRUXEval: A benchmark for code reasoning, understanding and execution. 2024arXiv preprint</p>
<p>Bill Howe, Julia Stoyanovich, Haoyue Ping, Bernease Herman, Matt Gee, arXiv:1710.08874Synthetic data for social good. 2017arXiv preprint</p>
<p>Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte Macdiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, arXiv:2401.05566Sleeper agents: Training deceptive llms that persist through safety training. 2024arXiv preprint</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Learning skillful mediumrange global weather forecasting. Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Science. 38266772023</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Choi, arXiv:2403.13787RewardBench: Evaluating reward models for language modeling. 2024arXiv preprint</p>
<p>Learning to reason and memorize with self-notes. Jack Lanchantin, Shubham Toshniwal, Jason E Weston, Arthur Szlam, Sainbayar Sukhbaatar, arXiv:2309.00267RLAIF: Scaling reinforcement learning from human feedback with ai feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi, 2023. 2023arXiv preprintThirty-seventh Conference on Neural Information Processing Systems</p>
<p>GSM-Plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi, arXiv:2402.192552024aarXiv preprint</p>
<p>Self-alignment with instruction backtranslation. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason E Weston, Mike Lewis, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>AlpacaEval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Changshu Liu, Shizhuo , Dylan Zhang, Reyhaneh Jabbarvand, arXiv:2402.09664CodeMind: A framework to challenge large language models for code reasoning. 2024arXiv preprint</p>
<p>G-eval: NLG evaluation using GPT-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>West-of-N: Synthetic preference generation for improved reward modeling. Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn, ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models. 2024</p>
<p>Discovering language model behaviors with model-written evaluations. Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>Branchsolve-merge improves large language model evaluation and generation. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, International Conference on Learning Representations. 2019</p>
<p>Interpretable preferences via multi-objective reward modeling and mixture. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang, arXiv:2406.128452024aofexperts. arXiv preprint</p>
<p>Is ChatGPT a good NLG evaluator? a preliminary study. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, 10.18653/v1/2023.newsum-1.1Proceedings of the 4th New Frontiers in Summarization Workshop. the 4th New Frontiers in Summarization WorkshopSingapore2023aAssociation for Computational Linguistics</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev, arXiv:2406.08673HelpSteer2: Open-source dataset for training top-performing reward models. 2024barXiv preprint</p>
<p>Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, arXiv:2403.18802Long-form factuality in large language models. 2024arXiv preprint</p>
<p>Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss. Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, Jason Weston ; Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, arXiv:2312.16682The Twelfth International Conference on Learning Representations. 2023. 2024arXiv preprintFLASK: Fine-grained language model evaluation based on alignment skill sets</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Y U Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>Evaluating large language models at evaluating instruction following. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>WildChat: 1M ChatGPT interaction logs in the wild. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, Yuntian Deng, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Jiantao Jiao, Starling-7B: Increasing LLM Helpfulness &amp; Harmlessness with RLAIF. 2023</p>
<p>Model Overall Chat Chat Hard Safety Reasoning Llama-3-70B-Instruct (seed). </p>
<p>Self-Taught Evaluator, trained on synthetic data only Mixtral judge Mixtral. 83</p>
<p>Performance on RewardBench of models finetuned on different training data. Model Overall writing stem coding math humanities reasoning roleplay extraction Llama-3-70B-Instruct (seed). Table. 10</p>
<p>Self-Taught Evaluator, trained on synthetic data only Iteration 1. </p>
<p>Other SoTA LLM-as-a-Judge baseline models. </p>            </div>
        </div>

    </div>
</body>
</html>
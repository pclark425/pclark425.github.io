<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-436 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-436</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-436</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-270738805</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.08820v3.pdf" target="_blank">How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots</a></p>
                <p><strong>Paper Abstract:</strong> Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities. However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond. We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches. We report preliminary results on the classical symbolic approach, deep-learned neural networks, and modern ideas using large language models as knowledge base. From analyzing their trade-offs, we conclude that a hybrid approach is necessary, and thereby present a new use case for the emerging field of neuro-symbolic artificial intelligence.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e436.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e436.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Primed-LLM Planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Primed Large Language Model used as Planner and Knowledge Base</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a large language model (e.g., ChatGPT-4) primed to emulate a symbolic planner while simultaneously providing common-sense knowledge (object privacy attribution), producing natural-language plans from natural-language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Primed-LLM Planner (constraint-observing)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multimodal LLM (experimentally ChatGPT-4) is primed with a prompt that instructs it to behave like a symbolic planner and to observe stated constraints; it consumes sensor/memory simulation and natural-language constraints and outputs a plan in natural language. The model also serves as an integrated knowledge base for social norms (e.g., which objects are private).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Implicit symbolic-style prompts and prompt-encoded constraints (natural-language representation of predicates/requirements); no explicit symbolic engine is embedded, but symbolic preconditions/effects are simulated via the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Large transformer-based neural model (LLM, e.g., ChatGPT-4) performing likelihood-based text generation as the procedural planning mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Prompt engineering / priming: encode declarative constraints and domain state in the input prompt so the LLM conditions its generation on them; constraints are not enforced by a separate symbolic engine but are implicitly integrated by the model's conditional generation.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Inferred common-sense object attributions (e.g., recognizing diaries as private without explicit rules), flexible multimodal prompt handling, and the ability to produce human-readable plans; but stochasticity in output and lack of formal guarantees are emergent trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Constrained task planning for assistive humanoid robot toy domain (table-cleaning example) and ad-hoc puzzles (river-crossing example used to probe correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>LLM shows generalization from broad textual training data allowing inference of social norms and transfer across domains (e.g., attributing privacy), but demonstrates failure modes on structured combinatorial problems (river-crossing) where symbolic correctness is required.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Outputs are human-readable natural-language plans (easy to inspect), but the internal reasoning is opaque; the prompt encodes declarative aspects which provides some inspectability of constraints, yet the model's internal decision process is not explainable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Probabilistic instabilities (non-deterministic outputs), no formal guarantees on correctness or stability, failure on structured symbolic problems (river-crossing counterexample), and inability to reliably enforce constraints beyond what conditioning via prompts affords.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Empirical priming of LLMs as planners; relies on LLMs' internalized corpus-level common-sense knowledge rather than formal symbolic reasoning; no formal guarantees or theoretical correctness proofs provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e436.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e436.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Level-2 Hybrid (Symbolic+LLM KB)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Loosely-coupled Symbolic Planner with LLM Knowledge Base (Kautz level 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic planner (e.g., PDDL-based) augmented by an LLM used as a knowledge-base subroutine to answer contextual queries such as object privacy attribution; training processes are decoupled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic Planner + LLM Knowledge Base (Level 2)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A classical symbolic planner (PDDL) performs core planning/search while invoking an LLM subroutine to resolve ambiguous or commonsense attributes (e.g., is this object private?), with both components trained/created independently.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Classical symbolic planning with PDDL: explicit predicates, actions with preconditions and effects, and symbolic plan search.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Large language model acting as an auxiliary neural subroutine to provide commonsense judgments or attribute classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Loose coupling: symbolic planner calls the LLM as a subroutine at runtime to query object properties or social-norm inferences; there is no joint training—each component is trained or specified separately.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Retains symbolic guarantees on planning structure while gaining flexible commonsense knowledge from the LLM, reducing manual encoding of many social-norm rules; improved domain coverage for attributes not easily formalized.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Constrained task planning (care-home table-cleaning) where object-level privacy attribution is required during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Generalization derives from the LLM's knowledge for object attribution; overall system generalization depends on symbolic model coverage plus LLM robustness—likely better coverage for commonsense cases than symbolic-only but brittle when LLM subroutine is probabilistic.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Core plan structure remains interpretable (symbolic), while LLM-provided attributions may be inspectable as text responses but are not formally verifiable; overall interpretability improved relative to pure neural systems because planning trace remains symbolic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Decoupled training means no joint optimization; LLM subroutine can be stochastic and may provide inconsistent attributions leading to unstable planning outcomes; no formal guarantees that LLM answers satisfy symbolic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division-of-labor: symbolic system maintains guarantees and structure while neural subroutine supplies hard-to-formalize knowledge; follows Kautz's level-2 neuro-symbolic integration taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e436.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e436.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Level-3 Hybrid (Pipeline + Feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jointly-trained Pipeline: Neural Planner ↔ Symbolic Constraint Enforcement (Kautz level 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pipeline architectures where outputs of one system feed the other (neural↔symbolic) with a feedback loop enabling joint training adjustments—e.g., neural planner outputs symbolic plans which are checked/filtered by a symbolic constraint system and feedback is used to refine the neural component.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Planner + Symbolic Constraint Enforcement (Level 3)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline in which a neural planner (or LLM-based planner) generates candidate (symbolic) plans and a symbolic constraint enforcement module (e.g., LTL checker or PDDL constraint filter) post-processes or vetoes invalid plans; training may include feedback from the symbolic module to the neural planner.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic constraint checker/enforcer (e.g., temporal-logic-based constraints or symbolic validator implemented outside the neural model).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural planner (deep network or LLM) producing plans or symbolic plans as output.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline with feedback: neural system produces plan; symbolic system verifies/enforces constraints and can provide supervisory feedback/loss signals used to fine-tune the neural system (joint training/feedback loop).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines speed and flexibility of neural plan generation with strong constraint enforcement via symbolic post-checking; allows fast candidate generation with safety/hard-rule guarantees via symbolic veto or repair.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Constrained robot task planning; paper cites Yang et al.'s approach coupling LLMs with linear temporal logic enforcement as an example.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Neural generator may generalize to diverse tasks while symbolic enforcer ensures constraint compliance; overall system can generalize broadly but depends on robustness of feedback and the ability to correct neural outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic enforcement provides clear rationale for rejected plans; generated plans remain inspectable; interpretability benefits from the symbolic verification phase.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires reliable translation between neural outputs and symbolic representations; feedback integration is non-trivial; symbolic veto may force frequent retraining or limit neural creativity; system-level guarantees depend on soundness of symbolic enforcer plus correctness of neural-to-symbol translation.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Pipeline with supervisory feedback; grounded in Kautz level-3 description emphasizing joint training loops between neural and symbolic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e436.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e436.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Level-4 Hybrid (Synthetic Data Injection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Systems Trained on Symbolically-Generated Synthetic Data (Kautz level 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Symbolic knowledge is compiled into synthetic datasets (valid/invalid examples) used to train neural models so that symbolic constraints are embedded implicitly in the neural weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Planner / Constraint Model trained on Synthetic Symbolic Data (Level 4)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Symbolic domain knowledge (planning rules, constraint logic) is used to generate synthetic training pairs (tasks ↔ valid plans or access requests ↔ decisions). Neural planners or neural constraint classifiers are trained on this dataset so that symbolic regularities are captured by the network during supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolically-generated datasets reflecting formal rules in PDDL or access-control logics (synthetic examples labeled by symbolic reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural planner or neural constraint classifier trained on the synthetic data (deep networks, possibly LLM fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Encode symbolic knowledge into the training set (data-level integration); no explicit symbolic module at runtime—neural model internalizes symbolic constraints through supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Neural models can learn to imitate symbolic constraint satisfaction while retaining distributed representations that support generalization; reduced run-time reliance on symbolic engines and improved speed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Training neural planners or neural access-control models on synthetic logs derived from symbolic constraints; proposed for constrained task planning and DLBAC-like access control.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Potentially improved coverage for varied instances due to large synthetic datasets; generalization quality depends on representativeness of synthetic generation process and can fail if real-world data deviates from synthetic assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Resulting neural model is less interpretable despite being trained on symbolic rules; explanations require extracting symbolic proxies or post-hoc methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>If synthetic data does not capture real-world perturbations, models may be brittle; internalized symbolic rules are not guaranteed (no formal enforcement) and catastrophic forgetting can occur during incremental updates.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Data-level injection of symbolic priors: symbol-to-data compilation enabling neural internalization of symbolic constraints (Kautz level 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e436.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e436.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Level-5 Hybrid (Symbolic Loss Embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Logic Embedded in Neural Loss Function (Kautz level 5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Symbolic constraints are encoded directly into the neural training objective as differentiable penalties or differentiable approximations, causing the neural model to prefer constraint-satisfying outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic Constraints in Neural Loss (Level 5)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Symbolic constraint formulas are transformed into differentiable loss components (or approximations thereof) and incorporated into the neural planner's training objective so that illegal plans are penalized and legal ones rewarded during gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Formal symbolic constraints (predicate logic, temporal logic) transformed into differentiable penalty terms or surrogate losses that reflect violation severity.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Gradient-based neural planners (deep networks) optimized with composite loss functions combining task reward and symbolic-constraint penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Loss-level integration: symbolic logic mapped to loss components (differentiable or relaxed approximations), enabling end-to-end training where symbolic constraints directly shape neural weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Neural planner internalizes constraints more robustly than pure data-level injection, potentially reducing invalid plan generation and producing behavior that more consistently respects symbolic rules while retaining neural generality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Proposed for constrained planning and access-control enforcement (e.g., penalizing constraint violations during planner training); no concrete numerical benchmarks provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Potential for better constraint compliance generalization compared to purely data-driven neural models; depends strongly on quality of differentiable encoding and optimization dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Some interpretability arises from explicit loss terms corresponding to symbolic rules, but internal neural representations remain distributed; explicit proofs of constraint satisfaction are not typically achievable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Technical difficulty: symbolic logic is inherently non-differentiable and mapping to loss functions is challenging; risk of approximate / soft enforcement rather than hard guarantees; optimization may produce local minima that trade off constraint satisfaction unpredictably.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Tight coupling via objective shaping: symbolic knowledge becomes part of the optimization objective, aligning neural learning with declarative rules (Kautz level 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e436.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e436.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Level-6 Vision (Deep Neuro-Symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integrated Neuro-Symbolic Engine with Internal Symbolic Reasoning (Kautz level 6)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hypothesized system where symbolic reasoning capabilities are implemented within a neural engine such that distinction between symbolic and neural components is blurred and the system can perform both fast intuitive and slow logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Integrated Neuro-Symbolic System (Level 6)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A speculative architecture where symbolic representations and reasoning are learned and executed wholly within a neural substrate, enabling the model to 'think fast and slow'—performing both pattern-based intuition and explicit logical inference within the same learned engine.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Internalized symbolic representations and reasoning mechanisms encoded within neural weights and activations (no separate symbolic module exposed).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>End-to-end neural engine capable of both distributed pattern processing and learned symbolic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Full internalization: symbolic knowledge and reasoning procedures are learned as part of the neural model's structure and weights such that symbolic processing emerges from neural computation.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>If realized, would combine the human-like duality of fast intuition and slow logical deliberation with symbolic guarantees and neural-scale generalization; could allow both explainability and universal task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visionary proposition for constrained universal task planning for humanoid robots; no empirical evaluation exists presently.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Expected to generalize strongly across tasks while also reliably satisfying symbolic constraints, but currently hypothetical and unproven.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Potential for strong interpretability if symbolic processes are recoverable; currently unknown and speculative.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Currently an open grand challenge — unclear feasibility, no concrete architectures or training regimes demonstrated; major research problems remain (signal-to-symbol gap, training regimes, verification).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Conceptual: unify Systems 1 and 2 (Kahneman) in a learned neural substrate; advocated as ultimate neuro-symbolic integration (Kautz level 6).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e436.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e436.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DLBAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Learning Based Access Control (DLBAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of models where neural networks perform access-control decisions, encoding constraints and authorizations in distributed representations learned from logs and metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toward Deep Learning Based Access Control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Learning Based Access Control (DLBAC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A neural access-control model (e.g., classifier) trained on access logs and metadata to predict allow/deny decisions, used as a learned constraint model that can be queried by planners or integrated into planner training.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>When used in hybrid contexts, symbolic policies can be the source for synthetic training data, but DLBAC itself is a learned distributed representation rather than explicit symbolic logic.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Deep neural classifier/regressor trained on historical access logs and metadata; used to make policy decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Can be queried by symbolic planners as an oracle (loose coupling) or used to shape neural planners' training (data-level or loss-level integration); paper discusses using DLBAC as a learned constraint model queried by planners.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Encodes complex policy patterns from historical data, enabling automated, scalable access decisions and incorporation of user-specific patterns not easily captured symbolically.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Access-control decision prediction; referenced precision about 0.9 in cited work indicating non-negligible error rate.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Precision ~0.9 reported (cited, from Nobi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Precision ~0.9 (as reported for neural access decision models in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Generalizes to patterns present in training logs but vulnerable to distributional shift; incremental updates are challenging due to catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Low interpretability compared with symbolic policies; decisions are data-driven and require post-hoc explanation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Non-zero error rates (≈0.1 error); requires retraining or careful incremental update schemes when policies change; lacks formal guarantees required for hard security/privacy constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Empirical learned policy induction; can be combined with symbolic systems to trade off enforcement guarantees vs. scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toward Deep Learning Based Access Control <em>(Rating: 2)</em></li>
                <li>Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents <em>(Rating: 2)</em></li>
                <li>Plug and Play Language Models: A Simple Approach to Controlled Text Generation <em>(Rating: 1)</em></li>
                <li>CTRL: A Conditional Transformer Language Model for Controllable Generation <em>(Rating: 1)</em></li>
                <li>Neurosymbolic AI: The 3rd wave. <em>(Rating: 2)</em></li>
                <li>Neurosymbolic AI - Why, What, and How. <em>(Rating: 2)</em></li>
                <li>The third AI summer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-436",
    "paper_id": "paper-270738805",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "Primed-LLM Planner",
            "name_full": "Primed Large Language Model used as Planner and Knowledge Base",
            "brief_description": "Using a large language model (e.g., ChatGPT-4) primed to emulate a symbolic planner while simultaneously providing common-sense knowledge (object privacy attribution), producing natural-language plans from natural-language prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Primed-LLM Planner (constraint-observing)",
            "system_description": "A multimodal LLM (experimentally ChatGPT-4) is primed with a prompt that instructs it to behave like a symbolic planner and to observe stated constraints; it consumes sensor/memory simulation and natural-language constraints and outputs a plan in natural language. The model also serves as an integrated knowledge base for social norms (e.g., which objects are private).",
            "declarative_component": "Implicit symbolic-style prompts and prompt-encoded constraints (natural-language representation of predicates/requirements); no explicit symbolic engine is embedded, but symbolic preconditions/effects are simulated via the prompt.",
            "imperative_component": "Large transformer-based neural model (LLM, e.g., ChatGPT-4) performing likelihood-based text generation as the procedural planning mechanism.",
            "integration_method": "Prompt engineering / priming: encode declarative constraints and domain state in the input prompt so the LLM conditions its generation on them; constraints are not enforced by a separate symbolic engine but are implicitly integrated by the model's conditional generation.",
            "emergent_properties": "Inferred common-sense object attributions (e.g., recognizing diaries as private without explicit rules), flexible multimodal prompt handling, and the ability to produce human-readable plans; but stochasticity in output and lack of formal guarantees are emergent trade-offs.",
            "task_or_benchmark": "Constrained task planning for assistive humanoid robot toy domain (table-cleaning example) and ad-hoc puzzles (river-crossing example used to probe correctness).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "LLM shows generalization from broad textual training data allowing inference of social norms and transfer across domains (e.g., attributing privacy), but demonstrates failure modes on structured combinatorial problems (river-crossing) where symbolic correctness is required.",
            "interpretability_properties": "Outputs are human-readable natural-language plans (easy to inspect), but the internal reasoning is opaque; the prompt encodes declarative aspects which provides some inspectability of constraints, yet the model's internal decision process is not explainable.",
            "limitations_or_failures": "Probabilistic instabilities (non-deterministic outputs), no formal guarantees on correctness or stability, failure on structured symbolic problems (river-crossing counterexample), and inability to reliably enforce constraints beyond what conditioning via prompts affords.",
            "theoretical_framework": "Empirical priming of LLMs as planners; relies on LLMs' internalized corpus-level common-sense knowledge rather than formal symbolic reasoning; no formal guarantees or theoretical correctness proofs provided.",
            "uuid": "e436.0",
            "source_info": {
                "paper_title": "How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Level-2 Hybrid (Symbolic+LLM KB)",
            "name_full": "Loosely-coupled Symbolic Planner with LLM Knowledge Base (Kautz level 2)",
            "brief_description": "A symbolic planner (e.g., PDDL-based) augmented by an LLM used as a knowledge-base subroutine to answer contextual queries such as object privacy attribution; training processes are decoupled.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Symbolic Planner + LLM Knowledge Base (Level 2)",
            "system_description": "A classical symbolic planner (PDDL) performs core planning/search while invoking an LLM subroutine to resolve ambiguous or commonsense attributes (e.g., is this object private?), with both components trained/created independently.",
            "declarative_component": "Classical symbolic planning with PDDL: explicit predicates, actions with preconditions and effects, and symbolic plan search.",
            "imperative_component": "Large language model acting as an auxiliary neural subroutine to provide commonsense judgments or attribute classifications.",
            "integration_method": "Loose coupling: symbolic planner calls the LLM as a subroutine at runtime to query object properties or social-norm inferences; there is no joint training—each component is trained or specified separately.",
            "emergent_properties": "Retains symbolic guarantees on planning structure while gaining flexible commonsense knowledge from the LLM, reducing manual encoding of many social-norm rules; improved domain coverage for attributes not easily formalized.",
            "task_or_benchmark": "Constrained task planning (care-home table-cleaning) where object-level privacy attribution is required during planning.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Generalization derives from the LLM's knowledge for object attribution; overall system generalization depends on symbolic model coverage plus LLM robustness—likely better coverage for commonsense cases than symbolic-only but brittle when LLM subroutine is probabilistic.",
            "interpretability_properties": "Core plan structure remains interpretable (symbolic), while LLM-provided attributions may be inspectable as text responses but are not formally verifiable; overall interpretability improved relative to pure neural systems because planning trace remains symbolic.",
            "limitations_or_failures": "Decoupled training means no joint optimization; LLM subroutine can be stochastic and may provide inconsistent attributions leading to unstable planning outcomes; no formal guarantees that LLM answers satisfy symbolic constraints.",
            "theoretical_framework": "Division-of-labor: symbolic system maintains guarantees and structure while neural subroutine supplies hard-to-formalize knowledge; follows Kautz's level-2 neuro-symbolic integration taxonomy.",
            "uuid": "e436.1",
            "source_info": {
                "paper_title": "How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Level-3 Hybrid (Pipeline + Feedback)",
            "name_full": "Jointly-trained Pipeline: Neural Planner ↔ Symbolic Constraint Enforcement (Kautz level 3)",
            "brief_description": "Pipeline architectures where outputs of one system feed the other (neural↔symbolic) with a feedback loop enabling joint training adjustments—e.g., neural planner outputs symbolic plans which are checked/filtered by a symbolic constraint system and feedback is used to refine the neural component.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Neural Planner + Symbolic Constraint Enforcement (Level 3)",
            "system_description": "A pipeline in which a neural planner (or LLM-based planner) generates candidate (symbolic) plans and a symbolic constraint enforcement module (e.g., LTL checker or PDDL constraint filter) post-processes or vetoes invalid plans; training may include feedback from the symbolic module to the neural planner.",
            "declarative_component": "Symbolic constraint checker/enforcer (e.g., temporal-logic-based constraints or symbolic validator implemented outside the neural model).",
            "imperative_component": "Neural planner (deep network or LLM) producing plans or symbolic plans as output.",
            "integration_method": "Pipeline with feedback: neural system produces plan; symbolic system verifies/enforces constraints and can provide supervisory feedback/loss signals used to fine-tune the neural system (joint training/feedback loop).",
            "emergent_properties": "Combines speed and flexibility of neural plan generation with strong constraint enforcement via symbolic post-checking; allows fast candidate generation with safety/hard-rule guarantees via symbolic veto or repair.",
            "task_or_benchmark": "Constrained robot task planning; paper cites Yang et al.'s approach coupling LLMs with linear temporal logic enforcement as an example.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Neural generator may generalize to diverse tasks while symbolic enforcer ensures constraint compliance; overall system can generalize broadly but depends on robustness of feedback and the ability to correct neural outputs.",
            "interpretability_properties": "Symbolic enforcement provides clear rationale for rejected plans; generated plans remain inspectable; interpretability benefits from the symbolic verification phase.",
            "limitations_or_failures": "Requires reliable translation between neural outputs and symbolic representations; feedback integration is non-trivial; symbolic veto may force frequent retraining or limit neural creativity; system-level guarantees depend on soundness of symbolic enforcer plus correctness of neural-to-symbol translation.",
            "theoretical_framework": "Pipeline with supervisory feedback; grounded in Kautz level-3 description emphasizing joint training loops between neural and symbolic modules.",
            "uuid": "e436.2",
            "source_info": {
                "paper_title": "How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Level-4 Hybrid (Synthetic Data Injection)",
            "name_full": "Neural Systems Trained on Symbolically-Generated Synthetic Data (Kautz level 4)",
            "brief_description": "Symbolic knowledge is compiled into synthetic datasets (valid/invalid examples) used to train neural models so that symbolic constraints are embedded implicitly in the neural weights.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Neural Planner / Constraint Model trained on Synthetic Symbolic Data (Level 4)",
            "system_description": "Symbolic domain knowledge (planning rules, constraint logic) is used to generate synthetic training pairs (tasks ↔ valid plans or access requests ↔ decisions). Neural planners or neural constraint classifiers are trained on this dataset so that symbolic regularities are captured by the network during supervised training.",
            "declarative_component": "Symbolically-generated datasets reflecting formal rules in PDDL or access-control logics (synthetic examples labeled by symbolic reasoning).",
            "imperative_component": "Neural planner or neural constraint classifier trained on the synthetic data (deep networks, possibly LLM fine-tuning).",
            "integration_method": "Encode symbolic knowledge into the training set (data-level integration); no explicit symbolic module at runtime—neural model internalizes symbolic constraints through supervised learning.",
            "emergent_properties": "Neural models can learn to imitate symbolic constraint satisfaction while retaining distributed representations that support generalization; reduced run-time reliance on symbolic engines and improved speed.",
            "task_or_benchmark": "Training neural planners or neural access-control models on synthetic logs derived from symbolic constraints; proposed for constrained task planning and DLBAC-like access control.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Potentially improved coverage for varied instances due to large synthetic datasets; generalization quality depends on representativeness of synthetic generation process and can fail if real-world data deviates from synthetic assumptions.",
            "interpretability_properties": "Resulting neural model is less interpretable despite being trained on symbolic rules; explanations require extracting symbolic proxies or post-hoc methods.",
            "limitations_or_failures": "If synthetic data does not capture real-world perturbations, models may be brittle; internalized symbolic rules are not guaranteed (no formal enforcement) and catastrophic forgetting can occur during incremental updates.",
            "theoretical_framework": "Data-level injection of symbolic priors: symbol-to-data compilation enabling neural internalization of symbolic constraints (Kautz level 4).",
            "uuid": "e436.3",
            "source_info": {
                "paper_title": "How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Level-5 Hybrid (Symbolic Loss Embedding)",
            "name_full": "Symbolic Logic Embedded in Neural Loss Function (Kautz level 5)",
            "brief_description": "Symbolic constraints are encoded directly into the neural training objective as differentiable penalties or differentiable approximations, causing the neural model to prefer constraint-satisfying outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Symbolic Constraints in Neural Loss (Level 5)",
            "system_description": "Symbolic constraint formulas are transformed into differentiable loss components (or approximations thereof) and incorporated into the neural planner's training objective so that illegal plans are penalized and legal ones rewarded during gradient-based optimization.",
            "declarative_component": "Formal symbolic constraints (predicate logic, temporal logic) transformed into differentiable penalty terms or surrogate losses that reflect violation severity.",
            "imperative_component": "Gradient-based neural planners (deep networks) optimized with composite loss functions combining task reward and symbolic-constraint penalties.",
            "integration_method": "Loss-level integration: symbolic logic mapped to loss components (differentiable or relaxed approximations), enabling end-to-end training where symbolic constraints directly shape neural weight updates.",
            "emergent_properties": "Neural planner internalizes constraints more robustly than pure data-level injection, potentially reducing invalid plan generation and producing behavior that more consistently respects symbolic rules while retaining neural generality.",
            "task_or_benchmark": "Proposed for constrained planning and access-control enforcement (e.g., penalizing constraint violations during planner training); no concrete numerical benchmarks provided in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Potential for better constraint compliance generalization compared to purely data-driven neural models; depends strongly on quality of differentiable encoding and optimization dynamics.",
            "interpretability_properties": "Some interpretability arises from explicit loss terms corresponding to symbolic rules, but internal neural representations remain distributed; explicit proofs of constraint satisfaction are not typically achievable.",
            "limitations_or_failures": "Technical difficulty: symbolic logic is inherently non-differentiable and mapping to loss functions is challenging; risk of approximate / soft enforcement rather than hard guarantees; optimization may produce local minima that trade off constraint satisfaction unpredictably.",
            "theoretical_framework": "Tight coupling via objective shaping: symbolic knowledge becomes part of the optimization objective, aligning neural learning with declarative rules (Kautz level 5).",
            "uuid": "e436.4",
            "source_info": {
                "paper_title": "How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Level-6 Vision (Deep Neuro-Symbolic)",
            "name_full": "Integrated Neuro-Symbolic Engine with Internal Symbolic Reasoning (Kautz level 6)",
            "brief_description": "A hypothesized system where symbolic reasoning capabilities are implemented within a neural engine such that distinction between symbolic and neural components is blurred and the system can perform both fast intuitive and slow logical reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Deep Integrated Neuro-Symbolic System (Level 6)",
            "system_description": "A speculative architecture where symbolic representations and reasoning are learned and executed wholly within a neural substrate, enabling the model to 'think fast and slow'—performing both pattern-based intuition and explicit logical inference within the same learned engine.",
            "declarative_component": "Internalized symbolic representations and reasoning mechanisms encoded within neural weights and activations (no separate symbolic module exposed).",
            "imperative_component": "End-to-end neural engine capable of both distributed pattern processing and learned symbolic manipulation.",
            "integration_method": "Full internalization: symbolic knowledge and reasoning procedures are learned as part of the neural model's structure and weights such that symbolic processing emerges from neural computation.",
            "emergent_properties": "If realized, would combine the human-like duality of fast intuition and slow logical deliberation with symbolic guarantees and neural-scale generalization; could allow both explainability and universal task performance.",
            "task_or_benchmark": "Visionary proposition for constrained universal task planning for humanoid robots; no empirical evaluation exists presently.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Expected to generalize strongly across tasks while also reliably satisfying symbolic constraints, but currently hypothetical and unproven.",
            "interpretability_properties": "Potential for strong interpretability if symbolic processes are recoverable; currently unknown and speculative.",
            "limitations_or_failures": "Currently an open grand challenge — unclear feasibility, no concrete architectures or training regimes demonstrated; major research problems remain (signal-to-symbol gap, training regimes, verification).",
            "theoretical_framework": "Conceptual: unify Systems 1 and 2 (Kahneman) in a learned neural substrate; advocated as ultimate neuro-symbolic integration (Kautz level 6).",
            "uuid": "e436.5",
            "source_info": {
                "paper_title": "How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DLBAC",
            "name_full": "Deep Learning Based Access Control (DLBAC)",
            "brief_description": "A class of models where neural networks perform access-control decisions, encoding constraints and authorizations in distributed representations learned from logs and metadata.",
            "citation_title": "Toward Deep Learning Based Access Control",
            "mention_or_use": "mention",
            "system_name": "Deep Learning Based Access Control (DLBAC)",
            "system_description": "A neural access-control model (e.g., classifier) trained on access logs and metadata to predict allow/deny decisions, used as a learned constraint model that can be queried by planners or integrated into planner training.",
            "declarative_component": "When used in hybrid contexts, symbolic policies can be the source for synthetic training data, but DLBAC itself is a learned distributed representation rather than explicit symbolic logic.",
            "imperative_component": "Deep neural classifier/regressor trained on historical access logs and metadata; used to make policy decisions.",
            "integration_method": "Can be queried by symbolic planners as an oracle (loose coupling) or used to shape neural planners' training (data-level or loss-level integration); paper discusses using DLBAC as a learned constraint model queried by planners.",
            "emergent_properties": "Encodes complex policy patterns from historical data, enabling automated, scalable access decisions and incorporation of user-specific patterns not easily captured symbolically.",
            "task_or_benchmark": "Access-control decision prediction; referenced precision about 0.9 in cited work indicating non-negligible error rate.",
            "hybrid_performance": "Precision ~0.9 reported (cited, from Nobi et al.)",
            "declarative_only_performance": null,
            "imperative_only_performance": "Precision ~0.9 (as reported for neural access decision models in cited work)",
            "has_comparative_results": false,
            "generalization_properties": "Generalizes to patterns present in training logs but vulnerable to distributional shift; incremental updates are challenging due to catastrophic forgetting.",
            "interpretability_properties": "Low interpretability compared with symbolic policies; decisions are data-driven and require post-hoc explanation methods.",
            "limitations_or_failures": "Non-zero error rates (≈0.1 error); requires retraining or careful incremental update schemes when policies change; lacks formal guarantees required for hard security/privacy constraints.",
            "theoretical_framework": "Empirical learned policy induction; can be combined with symbolic systems to trade off enforcement guarantees vs. scalability.",
            "uuid": "e436.6",
            "source_info": {
                "paper_title": "How to Raise a Robot — A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toward Deep Learning Based Access Control",
            "rating": 2,
            "sanitized_title": "toward_deep_learning_based_access_control"
        },
        {
            "paper_title": "Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents",
            "rating": 2,
            "sanitized_title": "plug_in_the_safety_chip_enforcing_constraints_for_llmdriven_robot_agents"
        },
        {
            "paper_title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
            "rating": 1,
            "sanitized_title": "plug_and_play_language_models_a_simple_approach_to_controlled_text_generation"
        },
        {
            "paper_title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
            "rating": 1,
            "sanitized_title": "ctrl_a_conditional_transformer_language_model_for_controllable_generation"
        },
        {
            "paper_title": "Neurosymbolic AI: The 3rd wave.",
            "rating": 2,
            "sanitized_title": "neurosymbolic_ai_the_3rd_wave"
        },
        {
            "paper_title": "Neurosymbolic AI - Why, What, and How.",
            "rating": 2,
            "sanitized_title": "neurosymbolic_ai_why_what_and_how"
        },
        {
            "paper_title": "The third AI summer",
            "rating": 1,
            "sanitized_title": "the_third_ai_summer"
        }
    ],
    "cost": 0.01590925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How to Raise a Robot -A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots
27 Dec 2023</p>
<p>Florian Jacob</p>
<p>How to Raise a Robot -A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots
27 Dec 20238C29190A3CCBB05AE726FA938D997BA9arXiv:2312.08820v3[cs.RO]Humanoid RobotsRobot Task PlanningActivity-Centric Access ControlDeep Learning based Access ControlLarge Language ModelsNeuro-Symbolic Access Control Personally ownedcan be moved Diary: Very personalshould not be moved
Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities.However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond.We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches.We report preliminary results on the classical symbolic approach, deep-learned neural networks, and modern ideas using large language models as knowledge base.From analyzing their trade-offs, we conclude that a hybrid approach is necessary, and thereby present a new use case for the emerging field of neurosymbolic artificial intelligence.CCS CONCEPTS• Security and privacy → Access control; • Computer systems organization → Robotics; • Computing methodologies → Neural networks; Robotic planning.</p>
<p>INTRODUCTION</p>
<p>With an intensifying labor shortage in the care sector, assistive humanoid robots will most likely become a necessity in the future of our aging society.These robots will need to invade the most private spaces of humans they care for, thus safety, security and privacy issues in this field are of utmost importance.The potential of humanoid robots to assist humans lies in the ability to learn whatever is needed for assistance.However, 'whatever is needed' has to be restricted to safeguard safety, security, and privacy policies and preferences.The challenge for classical approaches, i.e., based on symbols and logical formulas, to ensure constraints lies in the task universality of humanoid robots.Their wide range of tasks and deployment favors approaches in which the robot is granted increased sovereignty and should learn its authorizations in the field.Learning promises scalability of problem complexity up to task universality, while keeping manual specification complexity manageable for humans.However, learning changes the nature of Figure 1: An assistive robot is tasked to clean the table in the personal room of a care home resident.The challenge is to integrate an abstract "do not move private objects" policy as specific constraints into robot task planning.While privacy has a common understanding, it is a subjective preference that needs ad-hoc assessment by the robot.On the table are care home-owned dishes, thus not personal.The newspaper is personally-owned but contains no private information.The diary is personal and private.</p>
<p>access control from certain enforcement to probable observance.Future use cases of assistive robots in care home will require complex systems of requirements regarding safety, security and a lot of personalized requirements when directly interacting with patients.While some of these requirements are prescribed in policy documents like the official instructions for human care-home workers accessible to the robot via manual translation or machine learning, some requirements are purely based on "common sense" and a common understanding of social norms not compiled in a machinereadable way.An example of such norm is what objects a human considers as private belongings and what objects can inoffensively be interacted with.In this tech report, we investigate how symbolic and neural methods to robot task planning can ensure symbolic and neural constraints beyond access control, and how constraints from policy documents and social norms can be inferred and observed using large language models.We discuss the inherent trade-offs, and propose neuro-symbolic hybrid methods as vision to bring assisted living with humanoid robots into practice.</p>
<p>As toy example illustrated in Fig. 1, a robot is tasked to clean a table in a personal room, for which the robot now needs a plan.It considers whether it is allowed to move objects on the table, i.e., the authorization to move an object is a precondition of the action for moving that object.We assume constraints that formalize policies like "do not move personal items", which allow, e.g., the removal of care home owned dishes, but disallow moving a diary.To put user-instructed policies like that in concrete terms, the robot needs a kind of policy information database or knowledge base to decide whether an action with an object is actually constrained by a policy.</p>
<p>RELATED WORK</p>
<p>In a first attempt to tackle this problem of balancing sovereignty and 'obedience', we proposed a joint modeling framework for robot task planning and access control [1].By directly incorporating access control into task planning, robots are unable to even "think about" forbidden behavior.This tech report is an extended version of our poster abstract [6].While the fit between the notion of an activity in Activity-Centric Access Control (ACAC) [5] and task planning was shown in theory, we now report on preliminary practical insights.Recently, machine-learning-based approaches to assist or even completely perform access decisions have been proposed, as seen with Machine-Learning-and Deep-Learningbased Access Control (MLBAC/DLBAC) [11].</p>
<p>PROBLEM STATEMENT AND DEFINITIONS</p>
<p>We describe the problem as follows.A prompt is an instruction given by a user to perform a task to reach a certain goal.A signalbased prompt is, e.g., a user-written text or a microphone recording.A symbol-based prompt is formalized and refers to abstract symbols, like subjects, objects, and locations with attributes.A plan is a sequence of actions to be performed by the robot to fulfill the task and reach the goal based on its current initial state.A symbol-based plan is a sequence of abstract actions with their pre-and postconditions (effects) that is converted to a signal-based plan in the form of actuator commands needed for execution.Planning is the problem of finding such a sequence of actions.Symbolic planning does so by reshaping logical formulas, and needs a symbol-based prompt as input to output a symbolic plan.Symbolic planning can be implemented either with classical search algorithms or symbolic artificial intelligence, but always uses a localist data representation where one symbol is represented in one discrete variable in information processing.Neural planning is the forward pass through a deep neural network that learned to plan, and can either use a symbolor signal-based prompt, to either create a symbol-or signal-based plan.End-to-end planning infers a signal-based plan from a signalbased prompt.Neural information processing usually operates on a distributed data representation where one specific representation of a symbol in the respective feature space is mapped to not one single neuron, but a continuous activation pattern of many neurons.The distributed data representation is the key that allowed deep neural networks to advance past symbolic methods in terms of generalizing from their learning material and flexibility.We say that policies are abstract ideas of what is allowed and what is off limits, from individual user preferences over administrative instruction documents to non-written social norms.We define constraints as machine-executable representations of policies that have to be considered by the planner to produce plans that adhere to the constraints.A symbolic constraint is a constraint in form of, e.g., a predicate-logical formula, as in classical access control.A neural constraint is a constraint in form of a deep neural network.Solving planning problems under constraints represents a classical topic.But when scaling up the complexity of tasks, constraints, and environments to task universality in human-inhabited private spaces, classical methods might not deliver the performance required for real-world use like humanoid robots in assisted living scenarios.In particular, there might be an unacceptable overhead in manual specification of constraints as well as an equally unacceptable overhead in computing costs.As we will indicate in Section 4, however, neural planning struggles to actually ensure safety, security, and privacy constraints in its generated plans.Constrained task-universal planning requires a hybrid method, for which we see large language models, due to their proficiency in a broad range of tasks and internalized "common sense", and neuro-symbolic artificial intelligence, due to bridging the gap between localist and distributed data representation, as most promising research directions (c.f.Section 5).</p>
<p>ANALYSIS OF STANDARD METHODS 4.1 Constraint-Ensuring Symbolic Planning</p>
<p>In symbolic planning, a planner finds a symbolic plan by searching through possible robot actions, trying to find transitions that transform the current state of the robot and its environment to the goal state inferred from the user prompt.In practice, the defacto standard for this is the Planning Domain Definition Language (PDDL) [7], which we used in the following.In our case, the initial state is the current state of the robot and its environment, the goal state is the goal that the prompt described as target, and the transition rules are the actions the robot is capable of.The symbolic planner will then try to find a plan, i.e., a sequence of transitions that transforms the initial state into the goal state.Each action has preconditions that need to hold in order for it to be executed.Typical preconditions are that the robot has to be at a table and emptyhanded to grab an item, or that an item has to be unobstructed to be grabbed.Incorporating safety, security, and privacy constraints in form of preconditions is a natural approach to generate plans that are aware of such constraints, as shown in Listing 1. Listing 1: Excerpt from our PDDL domain description Since the abstraction of a PDDL transition corresponds to ACAC's main abstraction of an activity, these two concepts can be easily combined.This workflow is illustrated in Fig. 2. We identify three different phases when designing such planner.Only during runtime we automatically generate plans, while during development and training time, manual work needs to be done.We were able to map 1 the preconditions of ACAC directly into the preconditions of PDDL, as well as the resulting conditions of an activity, which could be represented as effect in PDDL.</p>
<p>However, ACAC's contextual conditions and current conditions, which are checked during the execution of an action, are not directly translatable.Contextual conditions require PDDL extensions to query external data.Current conditions require PDDL extensions that allow temporal planning and durative, interruptible actions.While it is possible to incorporate toy examples of ACAC into PDDL, we question the scalability up to the humanoid robots in a care home use case without requiring an equally complex manual specification.Every possible action, every possible object and every ACAC policy needs to be considered, inherently prohibiting such systems from being task-universal.While symbolic artificial intelligence can learn incrementally in the field and can thereby reduce manual specification complexity while still being understandable and tunable by humans, adapting symbolic artificial intelligence to a broad range of situations is still a 'very manual' process.</p>
<p>By relaxation from constraint-ensuring to constraint-observing planning, recent developments in the field of machine-learning based access control can be employed.Feasibility of access decisions performed by machine learning methods such as neural networks was, e.g., shown by Nobi et al. [12].However, current systems show quite large error margins, reaching a precision of about 0.9.Nobi et al. propose the Deep Learning Based Access Control model, where a neural network directly performs access decisions.Such systems are mostly trained on access logs and further user and resource metadata.To combine symbolic planning with a learned access control model, the planner queries the learned model in addition or instead of symbolic constraint formulas, and uses a learned heuristic function that classifies branches of the state space as dead ends if a constraint is violated.</p>
<p>Constraint-Observing Neural Planning</p>
<p>Deep-learned neural planners promise to be scalable to complex deployments, and are the natural counterpart to deep-learning based access control.When combining a neural constraint model with a neural planner, one gets a workflow as illustrated in Fig. 3. Again we identify three different phases.Compared to symbolic planning with symbolic constraints, i.e., classical access control, we can now perform the training phase automatically.Only the development needs manual work, which improves scalability.</p>
<p>However, incremental learning without catastrophic forgetting is still an unsolved problem in deep neural networks.Therefore, changes in the policy make retraining of the neural planner and neural constraints necessary.Still, ways to tackle policy changes in a manageable way have been shown regarding DLBAC administration [11].The main idea is to retrain the networks in small steps, only using training data that represents the update that should be introduced.This intends to only change the neural networks to incorporate the new data from the update, but this cannot be guaranteed.One could also consider incorporating neural constraints into the reward calculation of reinforcement-learning based planners.</p>
<p>We can also combine neural planning with symbolic activity control.Analogously to the previous paragraph, but unconventionally for Generative Adversarial Network (GANs), one might use symbolic constraints as discriminators to train the end-to-end learning-based neural planners.It is challenging to provide a distributed representation of constraints, since symbolic constraints are localist representations.As approximation, a first step can be realized by using the differentiable loss function provided by existing deep learning based constraint systems, which can be used to train the network.Although this approach might not be as scalable to complex deployments due to manual policy engineering as learning neural constraints, it is more efficient to administrate since the symbolic constraints do not need to be retrained after a change.For our running example, we now need to incorporate policies on what are personal items again, which makes the whole process inherently more complex.Even though the neural planner learns its behavior, extensive policy engineering needs to be done.Once the training of the end-to-end learning-based planner is finished, this approach behaves similarly to using a deep-learning based activity control system: Given a prompt, the robot performs a forward pass through the neural planning network and receives a plan that probably observes the desired constraints.</p>
<p>ANALYSIS OF UPCOMING METHODS</p>
<p>While symbolic and neural constraints are quite different compared to each other, the process of integrating any type of constraints in either symbolic or neural planning is similar.Therefore, one is not limited to choosing only one combination.We instead argue for a hybrid approach to combine the best of both worlds: one can use neural planning and neural constraints for versatility and universality, but ensure critical policies as symbolic constraints during runtime.For critical situations with limited universality, one can also fall back to symbolic planning to find an accurate solution slowly.Neural systems could also serve as recommendation engine for new policies that enhance the existing logical solution.</p>
<p>Recently, Large Language Models (LLMs) became a popular method for artificial intelligence.Especially the model ChatGPT by OpenAI [13] caught the attention of the general public, due to its seemingly close resemblance of human conversation while also being able to give symbolic answers, like solving a task in a programming language or even PDDL.Due to their proficiency in processing and generating both signal-and symbol-based input and output, they are a natural building block for hybrid systems.</p>
<p>Neuro-symbolic artificial intelligence stands for the combination of symbolic and neural approaches to solve problems, and promises the "best of both worlds", i.e., combining the advantages of both symbolic and neural methods.Thus, the popularity of neurosymbolic methods has risen in recent years [14], and those methods promise to bring together the flexibility of neural planning with guaranteeing constraint satisfaction of symbolic planning.We discuss LLMs using our toy example next, and then generalize to discuss neuro-symbolic methods to constraint-ensuring universal task planning in the following.</p>
<p>LLMs as Planner and Knowledge Base</p>
<p>While LLMs represent a break-through for dialogues with humans, LLMs are capable of solving many kinds of problems given the right prompt.Due to this versatility, LLMs are foundational models, from which a problem-specific model is derived through priming.The LLM can act on the priming because of the breadth and sheer size of its training data, which includes descriptions of planning problems, but also allows the models to generalize knowledge between  domains -for an LLM, solving a planning problem like the classical wolf, goat and cabbage river crossing problem in English is of no fundamental difference than solving it in another natural language, or in a formal planning language like PDDL.</p>
<p>We now describe an experiment with priming LLMs to behave like symbolic planers to solve constrained planing problems.We can see such a workflow illustrated in Fig. 4. With recent advances in multimodal large language model, we envision the model to operate not only on textual information, but also on audio-visual user prompts.We tested the primed LLM approach 1 by formulating our running example as natural-language prompt, and used ChatGPT 4 as constraint-observing neural planner to generate natural-language plans, shown in Listing 2. Specifically, lines 1 and 12 reduce ChatGPT to a constraint-observing neural planner by making it emulate a constraint-ensuring symbolic planner.Lines 2-8 simulate sensors and memory, lines 9-11 instruct ChatGPT with a goal and constraints.We obtain a plan that reaches the goal of cleaning the table, while not interacting with the most personal item on the table, the diary.This experiment shows that we can use a GPT-based LLM to do neural planning while observing neural constraints inferred from natural-language prompts.</p>
<p>One interesting effect of using LLMs for constraint-observing planning is their performance on problems that require knowledge of common sense.Our toy example delivered an insight into such behavior.We tasked the LLM-based planner with only handling objects that are not considered private.From experience and intuition, humans know that a diary is a very private object and dirty dishes are not, but machines need to encode such information somehow.When using LLMs as planner, such information does not need to be given explicitly to the planner, because the LLM can infer it from its training data.Since it is trained on extensive textual data that encodes a lot of human interaction, such social norms are known.When asked whether a diary is private, a yes has a higher probability than a no.The LLM therefore not only acts as a symbolic planner, but also as knowledge base for common sense and object attribution for the simulated planner.This integrated knowledge is of tremendous value for systems deployed in real world applications, because such social norms will no longer need to be encoded directly, if a system should obey to them.Especially humanoid assistive robots act in very sensitive environments and Given the constraints and the goal, the robot needs to first distinguish between objects that are personal and objects that belong to the care home.</p>
<p>Analyzing each object ensures the robot can determine its nature and importance.The newspaper, although bought by the resident, could be seen as potentially non−intrusive.However, it was paid for by the resident which could imply some personal value, so it's left untouched.The diary is most certainly private, so it's not touched.The dirty dishes belong to the care home and must be cleaned, so they are the only objects removed from the table, in line with the constraints set.</p>
<p>Listing 2: A prompt to simulate a symbolic planner for a basic domain description, and the ChatGPT 4 answer.Shared chat transcript at:</p>
<p>https://chat.openai.com/share/77d31c8a-239c-4d57-a96c-f70078f88503</p>
<p>will need to follow a very complex set of rules.While some of these rules will be available in the form of certain documents, some rules will only exist in the form of social norms, for which no single "ISO norm" document exists.LLMs provide a solution to encoding such norms, without extensive work on building a symbolic version of them.Thus, LLMs serve as knowledge bases for such systems.</p>
<p>In our experiments, the dishes were always moved to the kitchen, while the newspaper and diary were always considered too personal to interact with.However, with ChatGPT 3.5, the newspaper was sometimes interacted with and sometimes not, showing probabilistic instabilities not present with a (non-probabilistic) symbolic planner.These instabilities stem from the fact that the LLM only simulated a symbolic planner, using neural methods.The LLM chose among the predicted text continuations with the highest probability, and if two alternatives have no clear winner, the solution will vary.In contrast to actual symbolic planning, there are no guarantees on stability, correctness, or other properties of the solution.In contrast, systems built to perform robot task planning usually have guarantees regarding the validity and correctness Listing 3: A river crossing puzzle as stated in Wikipedia [15], but with the wolf replaced by a pig.The ChatGPT 4 solution leaves either of the herbivores alone with the cabbage.Shared chat transcript: https://chat.openai.com/share/b9eef797-a4ad-4b50-a9d1-3cc294e744f6 of produced plans, but LLMs instructed to behave like symbolic planners cannot provide guarantees on stability, correctness, or other properties of the solution.Planning-primed LLMs produce outputs that sound like a plan, but that are not necessarily a correct or even valid plan.For example, Listing 3 shows a prompt for a variation of the river-crossing problem usually formulated using a wolf, a goat, and a cabbage, which is a common way to test the limits of LLMs (c.f., e.g., Gerhard Paaß in [4]).Here, the wolf is replaced with a pig, and the constraints are stated accordingly that neither of the animals can be left unattended with the cabbage.While ChatGPT 4 confirms this fact, its plan does not ensure this constraint, as it leaves one of the animals alone with the cabbage, due to missing symbolic understanding of the problem.Instead, it provides a solution similar to the regular solution with the wolf, due to its prevalence in the training data.This example demonstrates that while likelihood-based text completion can simulate symbolic reasoning, it is currently no replacement for it.</p>
<p>While LLMs worked well for showcasing versatile planning behavior based on the knowledge of social norms in our toy example, the river-crossing problem example confirms the need for symbolic reasoning to get correct solutions.The emerging field of neurosymbolic artificial intelligence acts as a promising direction to combine versatility and correctness, which we look into next.</p>
<p>Integration of Neural and Symbolic AI</p>
<p>The term neuro-symbolic AI directly describes systems that combine symbolic with neural approaches.The above-mentioned hybrid systems would fall into this category.Neuro-symbolic AI systems are differentiated on the extent of the integration of symbolic data representation in the neural process.Kautz [9] differentiates between six different types of neuro-symbolic integration (c.f.Table 1).This classification ranges from standard deep learning or large language Lvl Description based on Garcez et al. [3] Example for Constrained Robot Task Planning  1: Levels of neuro-symbolic integration according to Kautz [9] and their respective realization for robot-task planning.</p>
<p>models, where the input and output can be symbolic, but the inner workings are not, to type six integration, where full symbolic reasoning should be possible within the neural engine.We especially want to emphasize how the integration of symbolic reasoning into the training process happens on each level.For our analysis, we consider each level and give an example mapping for our constrained task planning scenario.Our first experiment using a symbolic planner and PDDL from Section 4.1 has no neural reasoning at all, i.e., provides no neurosymbolic integration, and therefore one could say it is a "level zero" system.The same is true for our end-to-end learned neural planner architecture from Section 4.2, as no symbolic representation is involved.</p>
<p>Kautz' categorization starts with level one, which is classical deep learning trained on symbolic input to produce symbolic output.Since there is no symbolic reasoning inside the system, the data representation is purely distributed inside the neural engine.The integration of constraint satisfaction may be possible by only using valid plans as training data, hoping to not generate invalid ones.The LLM system from Section 5.1 would fall in this category, if we prompted it with symbolic inputs to produce symbolic outputs, e.g. in PDDL.</p>
<p>On level two, there are loosely coupled systems.This loose coupling is given if a symbolic problem solver uses a neural system as some kind of sub-routine, as stated by Kautz [9].In our constrained task planning case, this level could be reached with a symbolic planer like in our first experiment, enhanced to use a LLM as knowledge base.The sub-routine would allow the symbolic planner to check an item for its level of privacy and the societal knowledge encoded in the LLM.It is important to note that the training process of both parts is completely decoupled, i.e., the symbolic part has no influence on the training of the neural part and vice-versa.</p>
<p>The third level is, e.g., the "do not cross the red line" approach.We again have a distinct neural and symbolic system.On this level, the output of one is given as input to the other one.While on level two, the data flow was primarily handled by one of the systems, with the other one being just a sub-routine, it is now handled by both systems.A typical example is the pipeline of a neural computer vision system which produces the symbolic input for a symbolic planner, whose symbolic output is translated to motor signals by another neural system.In our scenario, an example would be a pipeline that feeds the symbolic plans of an LLM-based planner to a symbolic constraint enforcement system, as seen with Yang et al.'s LLM coupled with a linear temporal logic constraint enforcement [16].An important distinction of level three in contrast to level two is that now both systems take part in a joint training procedure.Kautz mentions a feedback loop that flows back from the symbolic system to optimize the training of the neural system.</p>
<p>Level four represents systems where symbolic knowledge is put into the neural system by encoding it into synthetic training data.This approach essentially means to use symbolic knowledge on the problem domain, but also general symbolic knowledge of physics or mathematics applied to the domain, to generate valid and invalid solutions to many problem instances.Note that finding a problem instance for a given, randomly-generated solution, i.e., to generate a domain in which a given plan does not violate any constraints, might be much faster than the other way around, i.e., actual symbolic planning.An exemplary system on this level would be a neural constraint enforcement machine trained on synthetic decisions logs.The training data would consist of access requests and corresponding access decisions, generated from a set of symbolic constraints that formalize the access policy.Analogously, a neural task planner could be trained with synthetic pairs of tasks and suitable plans.It is important to note the difference to a training on real world access logs here: All synthetic training samples have symbolic knowledge inherently encoded.This is not necessarily the case on real world data, where the inherent logic might be overlaid by pertubations, or not be present at all.Symbolically generated, synthetic training data further deepens the integration of the symbolic reasoning into the training process of the neural system.On level three, it was a feedback loop back into the training, now it is directly embedded.</p>
<p>On level five, one looks at systems where symbolic logic is directly mapped onto the loss function of the neural system.This level is especially challenging because the distributed data representation of a symbolic system is typically not differentiable.In our planning example, this approach would mean that for example a symbolic constraint enforcement is directly mapped into the loss function of the neural planner.Plans that are valid would be emphasized by it and plans that are not valid would be penalized.Since the symbolic logic is directly in the loss function of the neural system, we cannot distinguish the symbolic part in the training process anymore, it has become part of it.</p>
<p>The last step is further strengthening this integration of symbolic logic into the neural system.Such systems are described by level six neuro-symbolic systems, representing the deepest neurosymbolic integration.On this level, a real distinction of neural and symbolic parts is almost impossible: The symbolic knowledge is deeply encoded in the whole neural system.Systems on this level might be compared to human brains, bridging the gap between automatic and instinctive part of the brain and the slow and logical part, according to Kahnemanns notion of Systems 1 and 2 [8].State of the art neural systems correspond to the System 1 fast and instinctive part, and are not able to bridge the gap to also think slow and logical.This behavior is exemplarily demonstrated in Listing 3, where ChatGPT "reads over" the peculiarities of the specific problem instance, and instead provides a solution similar to the one for the common instance presumably more prevalent in its training data.Our example for this level is a neural planner that runs completely on a symbolic representation and its constraints.The vagueness of this example is because such system do not exist currently, and it is still unclear how they could look like in practice, or whether they are feasible at all.However, such level six systems would be learned symbolic reasoning machines.Due to their capability of "thinking, fast and slow" both in the neural and symbolic world, they would be a game-changer for constrained task planning.One would be able to completely explain the machines and guarantee their behavior, neither of which is currently possible using black-box deep-learned neural methods.</p>
<p>CHALLENGES AHEAD 6.1 Neuro-Symbolic Integration of Constraint Satisfaction and Task Planning</p>
<p>On current systems, we observe that task planning for real-world robots scales best to complex situations using neural planners, while in the access control community, symbolic approaches are most prevalent to strictly enforce security and privacy constraints.Therefore, we see the challenge of neuro-symbolic integration for constrained robot task planning in integrating a neural planner with a symbolic constraint system.While deepening the integration of the symbolic and neural spheres is still an open research problem, we present goals and challenges for deep neuro-symbolic integration for constrained robot task planning in the remainder of this section.Deep-learned neural planning requires a distributed representation of plans instead of a localist representation as a list of task actions.With end-to-end learning-based robot task planners, we get this distributed representation of plans for free.With the feature space being the sensor data, we can directly perform neural operations on plan-level, which is encoded by the internal weights of the neural network that represents the planner.To interact with a neural planner, constraints need a distributed representation as well.Recent developments in deep learning based access control promise comparable approaches to the distributed representation of end-to-end learning based robot planners.The deep neural networks that perform access decisions encode distributed constraints in their internal weights, and provide a feature space on which such constraints can be learned.However, satisfaction of constraints in distributed representation can still not be guaranteed, as seen in the sections above.</p>
<p>Thus the integration of constraint-ensuring neural task planning and comes down to the following challenges that can be considered variations of the well-known signal-to-symbol gap challenges:</p>
<p>• How can a distributed representation of planning work together with a distributed representation of constraints?• How can symbolic algorithms reason on a suitable distributed data representations?• How to learn symbolic knowledge on distributed feature spaces?</p>
<p>In this paper, we described approaches up to level two in the previous sections.As a research community, we can currently build neuro-symbolic systems up to level three and four.For level three, we can use existing systems and plug them together, level four can be realized using synthetic training data generated from symbolic knowledge on planning and constraints.Obviously, the leap to level five is the challenges, however, it would also bring humanoid assistive robots on a new level that might be necessary for practical task-universal usage in the real world.Existing neural constraint enforcement systems such as DLBAC [12] show that the integration of such systems into the loss function of planners appears on the horizon of current research.Controlled text generation of LLMs is also a current research topic, addressing the question of how to make a LLM produce text of a specific kind, as seen with Dathari et al. [2] or Keskar et al. [10].Such imposing of constraints are a promising fit for imposing constraints on planners to use LLMs in some form, as knowledge base or even directly as planner.While these controllers also are neural networks as of today and therefore are level four systems at most, an approach with symbolic controllers would be a level five neuro-symbolic AI.</p>
<p>A type six neuro-symbolic system that utilizes truly symbolic reasoning inside a neural engine would make many problems of constraining task-universal robots disappear, and thereby represent a major break-through for secure, private, but task-universal humanoid assistive robots in the real world.Such a highly-integrated neuro-symbolic system would be as task-universal, flexible and fast as today's neural planners, but still able to guarantee that learned constraints would be met as today's symbolic planners.A truly neuro-symbolic system would easily be able to scale in the dimension of task and constraint complexity, because those could be learned and would not need to be manually designed.However, while we seem to have major parts for functioning level five neurosymbolic systems, level six still presents a grand challenge that is possibly much farther off in the future, if achievable at all.</p>
<p>However, a deep level six neuro-symbolic integration might not be necessary for acceptable trade-offs regarding constrained task planning.Pure symbolic problem solving always comes with the advantage of guaranteeing certain behavior, which neural approaches cannot provide.For example, one could think of high-level symbolic planners that provide a general idea, and only fine-grained planning is done using neural systems.The same idea can be used for constraints that are imposed or observed by such planning systems.The integration of symbolic reasoning might also be done by imposing symbolic red lines to the neural planning system.The neural planner would then be constrained by a symbolic system that encodes rules of high importance.Aspects with a lower security level can however be constrained by a neural planning system, because errors do not have such an impact there.</p>
<p>Next Steps</p>
<p>In this paper, we argued that neuro-symbolic hybrid approaches are necessary to bring constrained task-universal planning to realworld problem complexity.We listed many variations of possible hybrid approaches, which still need to prove their feasibility in real robot experiments.Feasibility especially means how well the different approaches cope with practical task and policy complexity in e.g.our assistive humanoid robot in care homes scenario.A practical feasibility analysis could also determine necessary and sufficient levels of neuro-symbolic integration for the scenario, finding out what type of neuro-symbolic integration would fit best.While deploying neuro-symbolic systems on real robots might be an extensive challenge, simulating such situations with the necessary degree of realism using information gathered from care homes seems feasible.</p>
<p>In the space of possible neuro-symbolic approaches, we see LLMs as most promising building block due to their inherent knowledge on social norms and ability to work with multi-modal, layered instructions -we imagine a LLM that is primed to behave like a humanoid assistive robot in a care home using documents and norms for human trainees, that use input from the robot's camera, other sensors and memory to perform a task given as voice command in natural language.However, further research into guaranteeing valid plans or even correct plans when using LLMs is necessary -we see a promising direction in building upon advances in controlling LLM output as a way to make LLM-based planners adhere to constraints.In addition, performance of LLM-based planning in comparison to standard symbolic and neural planning systems in terms of speed and correctness is yet to be evaluated.Specific to LLMs used as knowledge base for planners, the general performance of inferring social norms also has to be evaluated.</p>
<p>Orthogonal to testing the feasibility of current ideas for hybrid approaches, neuro-symbolic AI advances as a field of itself.Any progress in this field, especially in the integration of a neural engine and a symbolic constraint system, would be directly applicable in constrained task planning for humanoid assistive robots.</p>
<p>CONCLUSION</p>
<p>We discussed combinations of symbolic and neural constrained task planning approaches.We highlighted their trade-offs and showed their benefits and shortcomings.While neural planners scale better to complex deployments in terms of required manual specification, they introduce an error margin and hinder administrability.The usage of neural systems such as LLMs as knowledge base for access decisions presents a direct way of incorporating social norms into such access decisions.Designing symbolic planning systems, however, requires more manual tasks than neural planning systems.While neural systems seem have in advantage when scaling problem complexity up to task universality, to actually teach them security, they do not only challenge us to quantify what we mean with security, but also to make that notion differentiable.In order to create deployable systems one should, therefore, strive for hybrid designs to combine best of both worlds: neural planning with neural constraints leads to probable observance of policies, critical policies will be safeguarded by symbolic constraints.Neuro-symbolic AI promises to advance this field of constrained task planning of assistive humanoid robots.</p>
<p>Figure 2 :
2
Figure 2: Workflow of designing a robot task planner using symbolic planning and symbolic constraints.We differentiate between three phases: In the development phase, the rules for the constraints are designed and the modeling for the symbolic planner is done.The training phase consists of the incorporation of such constraints into the planner.During runtime, the plan is generated and executed.</p>
<dl>
<dt>1 (</dt>
<dt>1</dt>
<dd>a c t i o n c l e a n _ f r o m _ t a b l e 2 : p a r a m e t e r s 3 ( ?r o b o t − r o b o t 4 ?t a b l e − l o c a t i o n 5 ?o b j − o n _ t a b l e 6 ?remove − l o c a t i o n ) 7 : p r e c o n d i t i o n ( and 8 ( n o n _ p e r s o n a l ?o b j ) 9 ( a t ?r o b o t ?t a b l e ) ( a t ?o b j ?t a b l e ) ( r e m o v e _ l o c ? remove ) ) : e f f e c t ( and ( not ( a t ?o b j ?t a b l e ) ) ( a t ?o b j ?remove ) ) ) . . .( : i n i t ( a t r o b o t s t a r t ) ( a t newspaper t a b l e ) ( a t d i a r y t a b l e ) ( a t d i s h e s t a b l e ) ( n o n _ p e r s o n a l d i s h e s ) ( n o n _ p e r s o n a l newspaper ) ( p e r s o n a l d i a r y ) ( r e m o v e _ l o c remove ) ) ( : g o a l ( f o r a l l ( ?o b j − o n _ t a b l e ) ( or ( and ( n o n _ p e r s o n a l ?o b j ) ( a t ?o b j remove ) ) ( and ( p e r s o n a l ?o b j ) ( a t ?o b j t a b l e ) ) ) ) ) )</dd>
</dl>
<p>Figure 3 :
3
Figure 3: Workflow of designing a neural planner that satisfies neural constraints.The development phase now consists only of data collection.The training phase includes the actual training of the deep neural network, while the neural constraints are being fed into the training of the planner.During runtime, a forward pass through the network is performed.</p>
<p>Figure 4 :
4
Figure4: Workflow of designing a robot task planner using a multimodal large language model as foundation model.In the development phase, the prompts for priming are manually engineered.</p>
<p>1</p>
<p>Suppose you are a symbolic planning system that generates plans for task− universal assistive and humanoid robots based on a domain description that is provided below. 2 Domain: 3 − Care home that uses robots to care for their residents.The care home has lots of personal rooms for the residents, common spaces like a dining area, and administrative spaces like a kitchen or offices 4 − Personal room of a resident, that has a table inside 5 − on the table are the following things: 6 − newspaper that was paid for by the resident 7 − dirty dishes that are owned by the care home and are cleaned by the care home 8 − a diary that most certainly contains very private information 9 Goal: Remove as many objects from the table as possible Constraints: Do not interact with objects that you would consider personal.Possible actions: Move, analyze an object, take an object, place an object Put out the plan in terms of a sequence of actions the robot performs.Put out the action you used and the object you interacted with or where the object was placed.You can only use the actions from the list above.Put out a list of actions and objects that the plan consists of.Summarize the reasoning for the overall plan as a single text paragraph.The plan must be conform with all given constraints.</p>
<h1>Exemplary output from ChatGPT 4Plan Sequence:1. Analyze object: newspaper2. Analyze object: dirty dishes3. Analyze object: diary4. Take object: dirty dishes5. Move to: kitchen or designated cleaning area6. Place object: dirty dishes in cleaning area or dishwashing stationReasoning:</h1>
<p>Table
1 Standard neural networksLearned neural network plan-deep-learned for usingner that produces symbolicsymbolic input and outputplans from symbolic prompts2 A neural network is looselyA LLM is used as knowledgecoupled with a symbolicbase for a symbolic constraintproblem solversystem3 A neural network focus-A neural planner producesing on one task is interact-symbolic plans, which are ver-ing with a symbolic systemified post-planning to ensureperforming a complemen-symbolic security and privacytary taskconstraints trough a symbolicconstraint system4 Symbolic knowledge isThe training data is generatedgiven into the training setby a simulator based on sym-of a neural networkbolic domain and constraintrepresentations5 Symbolic logic is mappedThe loss function of a neuralonto an embedding whichplanner has an embedded sym-acts as a soft-constraint onbolic constraint system, penal-the network's loss functionizing illegal plans and empha-sizing legal ones6 True symbolic reasoningA neural planner runs com-inside a neural enginepletely on a symbolic repre-sentation of the domain andits constraints
A farmer with a pig, a goat, and a cabbage must cross a river by boat. The boat can carry only the farmer and a single item. If left unattended together, the goat or the pig would eat the cabbage. How can they cross the river without anything being eaten?</p>
<h1>Exemplary output from ChatGPT 4</h1>
<p>The puzzle you've described is a classic river crossing problem, though it's typically presented with a wolf, a goat, and a cabbage. The solution to this problem, where the pig or the goat would eat the cabbage if left unattended, is as follows:
1. The farmer takes the goat (or pig, it doesn't matter which) across the river first and leaves it on the other side.
2. The farmer then goes back alone to the original side.
3. He takes the cabbage across the river next.
4. After leaving the cabbage on the other side, he takes the goat (or pig) that was left there during the first trip back with him to the original side.
5. He leaves the goat (or pig) on the original side and takes the pig (or goat, the one he hasn't taken across yet) to the other side.
6. The farmer leaves the pig (or goat) with the cabbage on the other side and goes back to the original side to get the goat (or pig).
7. Finally, he takes the goat (or pig) across the river.
Now, all three items − the pig, the goat, and the cabbage − are safely on the other side of the river. At no point were the cabbage and the animals left unattended together, so nothing was eaten.
ACKNOWLEDGMENTSWe like to thank Jan Niehues for his valuable comments, in particular for his hints on controlled text generation and controller architectures.This work has been supported by the project "Stay young with robots" (JuBot).The JuBot project was made possible by funding from the Carl Zeiss Foundation.
BlueSky: Combining Task Planning and Activity-Centric Access Control for Assistive Humanoid Robots. Saskia Bayreuther, Florian Jacob, Markus Grotz, Rainer Kartmann, Fabian Peller-Konrad, Fabian Paus, Hannes Hartenstein, Tamim Asfour, 10.1145/3532105.3535018Proceedings of the 27th ACM Symposium on Access Control Models and Technologies (SACMAT '22). the 27th ACM Symposium on Access Control Models and Technologies (SACMAT '22)New YorkACM2022</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, arXiv:1912.02164[cs.CL]Plug and Play Language Models: A Simple Approach to Controlled Text Generation. 2020</p>
<p>Neurosymbolic AI: The 3rd wave. Artur , Avila Garcez, Luis C Lamb, 10.1007/s10462-023-10448-wArtificial Intelligence Review. 2023. 2023</p>
<p>Chatbots reinlegen -Wie man KI-Sprachgeneratoren entlarvt. Arne Grävemeyer, 2023. 202316Fooling Chatbots -How to Expose AI Speech Generators</p>
<p>Towards Activity-Centric Access Control for Smart Collaborative Ecosystems. Maanak Gupta, Ravi Sandhu, 10.1145/3450569.3463559Proceedings of the 26th ACM Symposium on Access Control Models and Technologies (SACMAT '21). the 26th ACM Symposium on Access Control Models and Technologies (SACMAT '21)New YorkACM2021</p>
<p>Poster: How to Raise a Robot -Beyond Access Control Constraints in Assistive Humanoid Robots. Niklas Hemken, Florian Jacob, Fabian Peller-Konrad, Rainer Kartmann, Tamim Asfour, Hannes Hartenstein, 10.1145/3589608.3595078Proceedings of the 28th ACM Symposium on Access Control Models and Technologies. the 28th ACM Symposium on Access Control Models and TechnologiesTrento, Italy; New York, NY, USAAssociation for Computing Machinery2023SACMAT '23)</p>
<p>PDDL -The Planning Domain Definition Language. Adele Howe, Craig Knoblock, Drew Mcdermott, Ashwin Ram, Manuela Veloso, Daniel Weld, David Wilkins, 1998Yale Center for Computational Vision and ControlTechnical Report</p>
<p>Daniel Kahneman, Thinking, Fast and Slow. Farrar, Straus and Giroux. New York2011</p>
<p>The third AI summer. A Henry, Kautz, 10.1002/aaai.12036AAAI Robert S. Engelmore Memorial Lecture. AI Magazine. 432022. 2022</p>
<p>Nitish Shirish Keskar, Bryan Mccann, Lav R Varshney, Caiming Xiong, Richard Socher, arXiv:1909.05858[cs.CL]CTRL: A Conditional Transformer Language Model for Controllable Generation. 2019</p>
<p>Administration of Machine Learning Based Access Control. Mohammad Nur Nobi, Ram Krishnan, Yufei Huang, Ravi Sandhu, 10.1007/978-3-031-17146-8_10ESORICS 2022. ChamSpringer2022</p>
<p>Toward Deep Learning Based Access Control. Mohammad Nur Nobi, Ram Krishnan, Yufei Huang, Mehrnoosh Shakarami, Ravi Sandhu, 10.1145/3508398.3511497Proceedings of the Twelfth ACM Conference on Data and Application Security and Privacy (CODASPY '22). the Twelfth ACM Conference on Data and Application Security and Privacy (CODASPY '22)New YorkACM2022</p>
<p>Amit Sheth, Kaushik Roy, Manas Gaur, arXiv:2305.00813[cs.AI]Neurosymbolic AI -Why, What, and How. 2023</p>
<p>Wikipedia contributors. 2023. River crossing puzzle. </p>
<p>Ziyi Yang, Shreyas S Raman, Ankit Shah, Stefanie Tellex, arXiv:2309.09919[cs.RO]Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
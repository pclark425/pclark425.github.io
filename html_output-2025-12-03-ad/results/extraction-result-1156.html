<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1156 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1156</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1156</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-252780176</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.04642v1.pdf" target="_blank">Exploration via Planning for Information about the Optimal Trajectory</a></p>
                <p><strong>Paper Abstract:</strong> Many potential applications of reinforcement learning (RL) are stymied by the large numbers of samples required to learn an effective policy. This is especially true when applying RL to real-world control tasks, e.g. in the sciences or robotics, where executing a policy in the environment is costly. In popular RL algorithms, agents typically explore either by adding stochasticity to a reward-maximizing policy or by attempting to gather maximal information about environment dynamics without taking the given task into account. In this work, we develop a method that allows us to plan for exploration while taking both the task and the current knowledge about the dynamics into account. The key insight to our approach is to plan an action sequence that maximizes the expected information gain about the optimal trajectory for the task at hand. We demonstrate that our method learns strong policies with 2x fewer samples than strong exploration baselines and 200x fewer samples than model free methods on a diverse set of low-to-medium dimensional control tasks in both the open-loop and closed-loop control settings.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1156.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1156.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trajectory Information Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based MPC exploration method that plans action sequences to maximize the joint expected information gain about the optimal trajectory by sampling posterior dynamics, planning hypothetical optimal trajectories per sample, and computing the reduction in predictive entropy for planned queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Trajectory Information Planning (TIP)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based MPC agent using a Gaussian process posterior over transition dynamics, posterior-function sampling (Wilson et al. method), iCEM for planning, and a novel cost C_{τ*} that is the negative joint expected information gain about the optimal trajectory; updates dataset online and replans (MPC). Key components: GP dynamics model, posterior sampling, iCEM planner, JAX-backed covariance/entropy computations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information gain maximization about the optimal trajectory (Bayesian experimental design / expected information gain acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each planning step the agent samples posterior dynamics functions T_{ij} ∼ P(T|D), for each sample plans an approximately optimal trajectory τ*_{ij} (via planner on that sample), approximates the joint expected information gain about τ* for candidate query sets X by computing predictive entropies H[S|D] and H[S|D ∪ τ*_{ij}], and selects the action sequence that maximizes the expected reduction in entropy (i.e., maximizes EIG about τ*). It executes the first action, observes next state, updates dataset D, and repeats (MPC). This jointly accounts for redundancy across future observations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Evaluated on continuous control benchmarks: Pendulum-v0, Cartpole swing-up, Reacher-v2, β Tracking (plasma), β+Rotation (plasma), plus open-loop Lava Path and nonlinear regulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics (learned online), continuous state and action spaces, stochastic transitions modeled by GP, finite-horizon episodes, some tasks have sparse rewards (e.g., Cartpole), β+Rotation is multi-task (changing targets between episodes); experiments assumed full-state observations for GP (paper avoids partial-observability by fixing some inputs in plasma envs).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by task: Pendulum ~2D state / 1D action; Cartpole ~4D state / 1D action; Reacher ~10D state / 2D action; β Tracking 4D state / 1D action; β+Rotation 10D state / 2D actions; horizons are finite but task-dependent (short to medium horizons).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>High sample efficiency: median samples to reach 'solved' performance (across 5 seeds, averaged): Pendulum 21 samples; Cartpole 131 samples; β Tracking 46 samples; β+Rotation 201 samples; Reacher 251 samples. Authors also report TIP achieves ~2× fewer samples than strong exploration baselines and up to ~200× fewer samples than model-free methods on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines required substantially more data: example median samples for MPC (greedy) — Pendulum 46, Cartpole 201, β Tracking 76, β+Rotation >500, Reacher 751; model-free SAC: Pendulum ~7000, Cartpole tens of thousands; many ablations and baselines failed (>limit) on harder tasks (see paper Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very sample-efficient on the tested medium/low-dimensional control tasks: typically tens to a few hundred environment transitions to reach 'solved' performance (see per-task medians above).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances task and exploration by optimizing a task-specific acquisition: C_{τ*} explicitly aims to gain information that reduces uncertainty about the optimal trajectory (task-aware), rather than purely maximizing immediate reward (greedy) or purely exploring dynamics-uncertainty (task-agnostic). Uses MPC to exploit learned model for reward during test/evaluation (switches to reward-maximizing C_g at test time).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against sTIP, DIP, sDIP, EIG_T, BARL, MPC (greedy), PETS, HUCRL, Thompson Sampling (model-based TS), BPTT, FEEF, RHC, SAC, TD3, PPO, Bayesian Optimization (open-loop), and other ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>TIP substantially reduces sample complexity across closed- and open-loop tasks compared to both model-based and model-free baselines, often using a fraction or orders of magnitude fewer interactions; TIP attains lower modeling error on planner-used points (not necessarily uniform test set) and was the only GP-based method to solve the hardest plasma task (β+Rotation); joint EIG acquisition (C_{τ*}) outperforms summed/pointwise EIG approximations (sTIP) and task-agnostic exploration (DIP/sDIP); TIP can outperform BARL despite BARL's stronger data-access assumptions due to better optimization and initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computationally expensive (GP conditioning, many Cholesky decompositions; cubic in N+H in theory), JAX compilation overhead, relies on GP dynamics being appropriate (scales poorly to very high-dimensional states/actions), requires an ability to produce approximate optimal trajectories under sampled dynamics (planner cost), does not directly address partial observability (experiments assume observable state or fixed inputs to avoid POMDP issues).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1156.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1156.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>oTIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>open-loop Trajectory Information Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-loop variant of TIP that plans a single action sequence maximizing joint expected information gain about the optimal trajectory and then executes the sequence without replanning; retains the same GP model and C_{τ*} acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>oTIP (open-loop TIP)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Open-loop model-based controller: uses GP posterior and the same C_{τ*} acquisition to plan one action sequence (via iCEM) that maximizes expected information about the optimal trajectory, executes the entire sequence for the trial, and collects transitions to update the GP between trials.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information gain maximization about the optimal trajectory (Bayesian experimental design) applied in an open-loop planning context</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Plans a single sequence that maximizes joint EIG about τ* given current posterior, executes it (no replanning during episode), collects observed transitions, and updates the GP for the next trial; uses the same Monte Carlo procedure (sample posterior functions, plan τ* per sample) to compute C_{τ*}.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Open-loop benchmarks used in paper: Lava Path (navigation with hazards) and two nonlinear regulation problems; open-loop variant also evaluated on tasks suitable for open-loop control (stable, short horizon, fixed start).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Stable dynamics suitable for open-loop execution, continuous states/actions, unknown dynamics learned by GP, short horizons, fixed start states, possibly stochastic transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Examples: Lava Path ~4D state / 2D action; nonlinear regulation problems ~2D state / 2D action; relatively short horizon and fixed start state.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Paper reports oTIP as the most sample-efficient model-based method in the open-loop comparisons and outperforming Bayesian optimization baselines; exact per-task open-loop sample counts are reported in the paper's Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitatively highest sample efficiency among model-based open-loop methods in the paper's experiments (better than BO and other open-loop ablations); specific medians are provided in the paper's open-loop results tables.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Performs exploration via choosing an open-loop sequence that is maximally informative about τ* (task-aware information gain) rather than injecting random noise or optimizing pure reward during collection; trades off gathering task-relevant information in a single trial versus immediate exploitation (no within-trial replanning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against oDIP, oMPC, BO (open-loop Bayesian optimization), and other model-based / model-free baselines adapted to open-loop setting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>oTIP shows strong performance in open-loop tasks, being more sample-efficient than BO and other model-based open-loop approaches; demonstrates that the C_{τ*} acquisition generalizes to open-loop control.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Open-loop execution prevents adaptation within an episode (no replanning), so it is only suitable for stable short-horizon problems; paper provides fewer explicit numeric summaries for oTIP than for closed-loop TIP, and computational cost remains high due to GP computations and sampling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1156.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1156.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Active Reinforcement Learning (BARL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that introduced the EIG_{τ*} acquisition (expected information gain about the optimal trajectory) in a Transition Query RL (TQRL) setting where the agent may query the dynamics at arbitrary state-action pairs; used to sample points informative about optimal trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An experimental design perspective on model-based reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BARL (Bayesian Active Reinforcement Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Algorithm that uses a Bayesian posterior over dynamics and the EIG_{τ*} acquisition to select queries (state-action pairs) that are informative about the optimal trajectory; operates in the TQRL / generative-model setting allowing arbitrary state-action queries rather than trajectory rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Expected information gain about the optimal trajectory (EIG_{τ*}) in a TQRL (generative-model) setting</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Samples candidate state-action queries and evaluates the pointwise EIG_{τ*} acquisition to pick queries; relies on access to a simulator or generative model to query arbitrary state-action pairs and evaluate expected reductions in uncertainty about τ*.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Compared on the same continuous control benchmarks in this paper (when giving BARL TQRL access), e.g., Pendulum, Cartpole, Reacher, β Tracking, β+Rotation (with BARL allowed to make transition queries).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous state/action, unknown dynamics (Bayesian), finite-horizon tasks; BARL specifically assumes TQRL/generative access (not constrained to collect data only via rollouts).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same tasks as TIP; example median performance numbers reported in this paper: Pendulum 21 samples; Cartpole 111; β Tracking 186; β+Rotation >500 (failed median); Reacher 251 (see paper Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In the paper's comparisons, BARL sometimes matches or underperforms TIP: e.g., Pendulum 21 samples (comparable to TIP); on β+Rotation BARL failed to solve within budget (>500 median samples) while TIP solved in 201; BARL's performance depends on how the EIG acquisition is optimized (the paper notes BARL used uniform sampling over candidate points which may underperform TIP's optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Variable: can be sample-efficient when TQRL access is available, but in these experiments BARL required more samples or failed on harder tasks (β+Rotation) compared to TIP despite TQRL privileges, primarily due to optimization choices.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Purely exploration-oriented acquisition: selects queries to maximize expected information about τ*; not executed as trajectory rollouts in the standard MDP sense (requires generative access). Does not integrate exploitation via reward during query selection (TQRL-centric).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to TIP, sTIP, and other baselines in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BARL optimizes the same information-theoretic objective but in the TQRL setting; TIP outperforms BARL on some tasks despite BARL's stronger access assumptions, indicating the importance of efficient optimization and initialization of the acquisition (TIP's planner/initialization gave better practical collection of informative data).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires TQRL / generative-model access (unrealistic in many real-world settings where data must be collected via rollouts), and practical implementations may rely on crude optimization (uniform candidate sampling) which can reduce effectiveness; struggled on the hardest plasma control task in these comparisons.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1156.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1156.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamics Information Planning (DIP) [ablation]</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-agnostic planning ablation used in the paper that plans to maximize the joint predictive entropy (uncertainty) of the dynamics model over planned future observations (C_e), ignoring the specific task reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DIP (planned dynamics entropy maximization)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based planner (GP + iCEM) using cost C_e which is the negative joint entropy of the model predictive distribution over future next-states for the planned trajectory, i.e., plans to visit states that maximize model uncertainty without weighting by task.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Task-agnostic information gain (maximizing predictive entropy of dynamics model)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Plans trajectories that maximize joint predictive entropy H[T(S)|D] over the set of future state-action pairs expected to be visited, thereby adaptively seeking transitions where the dynamics are uncertain (but not task-specific).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same continuous control benchmarks used in paper (Pendulum, Cartpole, Reacher, β Tracking, β+Rotation, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous dynamics; optimization focuses on discovering dynamics structure globally rather than task-relevant regions; environments include sparse-reward and multi-task plasma control problems.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same per-task dimensionalities as TIP (Pendulum 2D/1D; Cartpole 4D/1D; Reacher ~10D/2D; β environments higher dimensional).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Underperforms TIP on task performance and sample complexity. Reported median samples to solve (Table 6): Pendulum 36; Cartpole 161; β Tracking 276; β+Rotation >500 (failed); Reacher >1000. Often collects data that is less useful for final control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Worse than TIP and many baselines in terms of data needed to solve tasks; collects broadly informative dynamics data but not task-focused, resulting in poor downstream sample efficiency for control.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure exploration objective (no explicit reward exploitation during exploration); the cost does not incorporate task reward, so it may explore dynamics-uncertain regions irrelevant to achieving the task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to TIP, sTIP, sDIP, EIG_T, BARL, MPC, PETS, model-free baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Task-agnostic information maximization is less effective for learning task policies than task-aware information gain (C_{τ*}); DIP collects dynamics information that is not necessarily relevant to task success, leading to worse sample complexity and control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Tends to gather data irrelevant for achieving task reward; failed to solve several harder tasks within allowed sample budgets; overexplores and thus is less sample-efficient for task learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1156.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1156.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sDIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summed Dynamics Information Planning (sDIP) [ablation]</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that approximates joint dynamics information by summing pointwise predictive entropies at future timesteps (i.e., an upper-bound/overestimate of joint information), which can lead to redundant queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>sDIP (summed pointwise dynamics entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Planner using cost equal to the negative sum over timesteps of pointwise predictive entropies H[T(s_i,a_i)|D], ignoring redundancy across timesteps (an overestimate of joint information). Implemented with GP + iCEM like other ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Pointwise predictive entropy summation (approximate info-gain about dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects actions by maximizing the sum of individual entropies at planned future points, failing to account for redundancy between nearby observations, potentially overvaluing local repeated information.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same set of continuous control benchmarks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous dynamics; similar task settings as DIP and TIP.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as other experiments (varied per task: Pendulum 2D/1D, Cartpole 4D/1D, Reacher ~10D/2D, plasma tasks higher-dim).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Worse than TIP; reported median samples to solve (Table 6): Pendulum 46; Cartpole 141; β Tracking 131; β+Rotation >500; Reacher >1000 (failed within budget on some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient than TIP and several baselines due to redundancy and overestimation of joint info.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure exploration; fails to consider task reward and double-counts information across timesteps leading to inefficient exploration sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared as an ablation against TIP, sTIP, DIP, EIG_T, BARL, MPC, PETS, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Summing pointwise entropies is a poor approximation of joint information (overestimates value), leading to redundant data collection and worse downstream control performance compared to joint-information-aware TIP.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Overestimates information due to ignoring redundancy; often fails on harder tasks within sample budget; not task-aware.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1156.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1156.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sTIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>summed Trajectory Information Planning (sTIP) [ablation]</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that plans using a cost equal to the negative sum of pointwise EIG_{τ*} values for future timesteps (i.e., sums individual expected information gains about τ* without accounting for joint redundancy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>sTIP (summed EIG_{τ*} ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses the same GP model and planner but replaces TIP's joint C_{τ*} with a cost that sums pointwise EIG_{τ*}(s_i,a_i) across planned timesteps; this ignores mutual-information overlap between observations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Pointwise expected information gain about τ* summed over timesteps (approximate EIG aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects action sequences by summing estimated EIG_{τ*} over each planned future observation and optimizing that summed score (computationally simpler but ignores redundancy), then executes via MPC like TIP.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmark suite as TIP (Pendulum, Cartpole, Reacher, β Tracking, β+Rotation, open-loop tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown dynamics continuous tasks, same as TIP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As per task (Pendulum 2D/1D; Cartpole 4D/1D; Reacher ~10D/2D; plasma tasks higher-dim).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Performs worse than TIP across tasks but better than some task-agnostic baselines; median samples to solve (Table 6): Pendulum 36; Cartpole 141; β Tracking 76; β+Rotation >500; Reacher >400 (failed on some seeds/tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient than TIP due to overestimation of joint information and redundant exploration; nevertheless often better than purely task-agnostic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Attempts to be task-aware (EIG_{τ*}) but aggregates gains naively (sum) leading to greedier/overcounting exploration relative to TIP's joint computation; exploitation handled by MPC replanning during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared as an ablation to TIP, DIP, sDIP, EIG_T, BARL, MPC and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>sTIP is outperformed by TIP across the board, demonstrating the importance of computing joint expected information (C_{τ*}) rather than summing pointwise EIG values to account for redundancy between future observations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Overestimates information by summing pointwise EIGs; collects redundant data and is less sample-efficient especially on harder tasks like β+Rotation and Reacher.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1156.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1156.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EIG_T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EIG_{τ*} (pointwise / TQRL variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The expected information gain about the optimal trajectory per state-action query (originally introduced in BARL/TQRL); used here as a baseline where the acquisition is evaluated per-point (TQRL) or as a simple ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An experimental design perspective on model-based reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EIG_T (pointwise expected info gain about τ*)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Acquisition function that computes expected information gain about τ* for a single state-action query (EIG_{τ*}(s,a)); in the TQRL setting it can be used to choose individual query points to reduce uncertainty about the optimal trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Pointwise expected information gain about optimal trajectory (EIG_{τ*})</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Evaluates EIG_{τ*} for candidate state-action queries and selects those with highest pointwise expected information; when used naively across a sequence or summed it may overcount redundant information.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used in TQRL experiments and as a baseline comparator on continuous control tasks in the paper (when TQRL access is allowed).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Assumes access to a generative model that can be queried at arbitrary state-action pairs (TQRL), continuous unknown dynamics, finite-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same benchmark environments when given TQRL access; in this paper EIG_T baseline often required more data and failed on harder tasks (see Table 6): Pendulum 56 samples; Cartpole 121; β Tracking >1000; β+Rotation >1000; Reacher >1500.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In the paper, EIG_T (as implemented) was often less sample-efficient and sometimes failed within budget on harder tasks compared to TIP and some other baselines; its TQRL use gives more flexible data-access but practical optimization/selection matters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Generally worse than TIP and in many cases worse than other model-based baselines when evaluated in the paper's settings (see per-task medians above).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Purely exploration-oriented acquisition; does not incorporate task reward into the acquisition itself and relies on selecting high-EIG points, which may be suboptimal for downstream control if optimization is crude or points are redundant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to TIP, sTIP, DIP variants, BARL (which uses EIG_{τ*} in TQRL), MPC, PETS, and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Pointwise EIG_{τ*} used in TQRL is theoretically attractive but in practice can underperform if joint effects and redundancy between points are not accounted for and if the acquisition optimization is naive; TIP's joint C_{τ*} is empirically superior in the trajectory-rollout setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on TQRL (generative) access often unavailable in real-world tasks; pointwise evaluation and naive optimization (uniform candidate sampling) can lead to suboptimal data collection and failure on harder tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An experimental design perspective on model-based reinforcement learning <em>(Rating: 2)</em></li>
                <li>Bayesian algorithm execution: Estimating computable properties of black-box functions using mutual information <em>(Rating: 2)</em></li>
                <li>Efficiently sampling functions from gaussian process posteriors <em>(Rating: 1)</em></li>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models <em>(Rating: 1)</em></li>
                <li>Planning to explore via self-supervised world models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1156",
    "paper_id": "paper-252780176",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "TIP",
            "name_full": "Trajectory Information Planning",
            "brief_description": "A model-based MPC exploration method that plans action sequences to maximize the joint expected information gain about the optimal trajectory by sampling posterior dynamics, planning hypothetical optimal trajectories per sample, and computing the reduction in predictive entropy for planned queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Trajectory Information Planning (TIP)",
            "agent_description": "Model-based MPC agent using a Gaussian process posterior over transition dynamics, posterior-function sampling (Wilson et al. method), iCEM for planning, and a novel cost C_{τ*} that is the negative joint expected information gain about the optimal trajectory; updates dataset online and replans (MPC). Key components: GP dynamics model, posterior sampling, iCEM planner, JAX-backed covariance/entropy computations.",
            "adaptive_design_method": "Information gain maximization about the optimal trajectory (Bayesian experimental design / expected information gain acquisition)",
            "adaptation_strategy_description": "At each planning step the agent samples posterior dynamics functions T_{ij} ∼ P(T|D), for each sample plans an approximately optimal trajectory τ*_{ij} (via planner on that sample), approximates the joint expected information gain about τ* for candidate query sets X by computing predictive entropies H[S|D] and H[S|D ∪ τ*_{ij}], and selects the action sequence that maximizes the expected reduction in entropy (i.e., maximizes EIG about τ*). It executes the first action, observes next state, updates dataset D, and repeats (MPC). This jointly accounts for redundancy across future observations.",
            "environment_name": "Evaluated on continuous control benchmarks: Pendulum-v0, Cartpole swing-up, Reacher-v2, β Tracking (plasma), β+Rotation (plasma), plus open-loop Lava Path and nonlinear regulation tasks",
            "environment_characteristics": "Unknown transition dynamics (learned online), continuous state and action spaces, stochastic transitions modeled by GP, finite-horizon episodes, some tasks have sparse rewards (e.g., Cartpole), β+Rotation is multi-task (changing targets between episodes); experiments assumed full-state observations for GP (paper avoids partial-observability by fixing some inputs in plasma envs).",
            "environment_complexity": "Varies by task: Pendulum ~2D state / 1D action; Cartpole ~4D state / 1D action; Reacher ~10D state / 2D action; β Tracking 4D state / 1D action; β+Rotation 10D state / 2D actions; horizons are finite but task-dependent (short to medium horizons).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "High sample efficiency: median samples to reach 'solved' performance (across 5 seeds, averaged): Pendulum 21 samples; Cartpole 131 samples; β Tracking 46 samples; β+Rotation 201 samples; Reacher 251 samples. Authors also report TIP achieves ~2× fewer samples than strong exploration baselines and up to ~200× fewer samples than model-free methods on these tasks.",
            "performance_without_adaptation": "Baselines required substantially more data: example median samples for MPC (greedy) — Pendulum 46, Cartpole 201, β Tracking 76, β+Rotation &gt;500, Reacher 751; model-free SAC: Pendulum ~7000, Cartpole tens of thousands; many ablations and baselines failed (&gt;limit) on harder tasks (see paper Table 6).",
            "sample_efficiency": "Very sample-efficient on the tested medium/low-dimensional control tasks: typically tens to a few hundred environment transitions to reach 'solved' performance (see per-task medians above).",
            "exploration_exploitation_tradeoff": "Balances task and exploration by optimizing a task-specific acquisition: C_{τ*} explicitly aims to gain information that reduces uncertainty about the optimal trajectory (task-aware), rather than purely maximizing immediate reward (greedy) or purely exploring dynamics-uncertainty (task-agnostic). Uses MPC to exploit learned model for reward during test/evaluation (switches to reward-maximizing C_g at test time).",
            "comparison_methods": "Compared against sTIP, DIP, sDIP, EIG_T, BARL, MPC (greedy), PETS, HUCRL, Thompson Sampling (model-based TS), BPTT, FEEF, RHC, SAC, TD3, PPO, Bayesian Optimization (open-loop), and other ablations.",
            "key_results": "TIP substantially reduces sample complexity across closed- and open-loop tasks compared to both model-based and model-free baselines, often using a fraction or orders of magnitude fewer interactions; TIP attains lower modeling error on planner-used points (not necessarily uniform test set) and was the only GP-based method to solve the hardest plasma task (β+Rotation); joint EIG acquisition (C_{τ*}) outperforms summed/pointwise EIG approximations (sTIP) and task-agnostic exploration (DIP/sDIP); TIP can outperform BARL despite BARL's stronger data-access assumptions due to better optimization and initialization.",
            "limitations_or_failures": "Computationally expensive (GP conditioning, many Cholesky decompositions; cubic in N+H in theory), JAX compilation overhead, relies on GP dynamics being appropriate (scales poorly to very high-dimensional states/actions), requires an ability to produce approximate optimal trajectories under sampled dynamics (planner cost), does not directly address partial observability (experiments assume observable state or fixed inputs to avoid POMDP issues).",
            "uuid": "e1156.0"
        },
        {
            "name_short": "oTIP",
            "name_full": "open-loop Trajectory Information Planning",
            "brief_description": "An open-loop variant of TIP that plans a single action sequence maximizing joint expected information gain about the optimal trajectory and then executes the sequence without replanning; retains the same GP model and C_{τ*} acquisition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "oTIP (open-loop TIP)",
            "agent_description": "Open-loop model-based controller: uses GP posterior and the same C_{τ*} acquisition to plan one action sequence (via iCEM) that maximizes expected information about the optimal trajectory, executes the entire sequence for the trial, and collects transitions to update the GP between trials.",
            "adaptive_design_method": "Information gain maximization about the optimal trajectory (Bayesian experimental design) applied in an open-loop planning context",
            "adaptation_strategy_description": "Plans a single sequence that maximizes joint EIG about τ* given current posterior, executes it (no replanning during episode), collects observed transitions, and updates the GP for the next trial; uses the same Monte Carlo procedure (sample posterior functions, plan τ* per sample) to compute C_{τ*}.",
            "environment_name": "Open-loop benchmarks used in paper: Lava Path (navigation with hazards) and two nonlinear regulation problems; open-loop variant also evaluated on tasks suitable for open-loop control (stable, short horizon, fixed start).",
            "environment_characteristics": "Stable dynamics suitable for open-loop execution, continuous states/actions, unknown dynamics learned by GP, short horizons, fixed start states, possibly stochastic transitions.",
            "environment_complexity": "Examples: Lava Path ~4D state / 2D action; nonlinear regulation problems ~2D state / 2D action; relatively short horizon and fixed start state.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Paper reports oTIP as the most sample-efficient model-based method in the open-loop comparisons and outperforming Bayesian optimization baselines; exact per-task open-loop sample counts are reported in the paper's Table 2.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Qualitatively highest sample efficiency among model-based open-loop methods in the paper's experiments (better than BO and other open-loop ablations); specific medians are provided in the paper's open-loop results tables.",
            "exploration_exploitation_tradeoff": "Performs exploration via choosing an open-loop sequence that is maximally informative about τ* (task-aware information gain) rather than injecting random noise or optimizing pure reward during collection; trades off gathering task-relevant information in a single trial versus immediate exploitation (no within-trial replanning).",
            "comparison_methods": "Compared against oDIP, oMPC, BO (open-loop Bayesian optimization), and other model-based / model-free baselines adapted to open-loop setting.",
            "key_results": "oTIP shows strong performance in open-loop tasks, being more sample-efficient than BO and other model-based open-loop approaches; demonstrates that the C_{τ*} acquisition generalizes to open-loop control.",
            "limitations_or_failures": "Open-loop execution prevents adaptation within an episode (no replanning), so it is only suitable for stable short-horizon problems; paper provides fewer explicit numeric summaries for oTIP than for closed-loop TIP, and computational cost remains high due to GP computations and sampling.",
            "uuid": "e1156.1"
        },
        {
            "name_short": "BARL",
            "name_full": "Bayesian Active Reinforcement Learning (BARL)",
            "brief_description": "Prior work that introduced the EIG_{τ*} acquisition (expected information gain about the optimal trajectory) in a Transition Query RL (TQRL) setting where the agent may query the dynamics at arbitrary state-action pairs; used to sample points informative about optimal trajectory.",
            "citation_title": "An experimental design perspective on model-based reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "BARL (Bayesian Active Reinforcement Learning)",
            "agent_description": "Algorithm that uses a Bayesian posterior over dynamics and the EIG_{τ*} acquisition to select queries (state-action pairs) that are informative about the optimal trajectory; operates in the TQRL / generative-model setting allowing arbitrary state-action queries rather than trajectory rollouts.",
            "adaptive_design_method": "Expected information gain about the optimal trajectory (EIG_{τ*}) in a TQRL (generative-model) setting",
            "adaptation_strategy_description": "Samples candidate state-action queries and evaluates the pointwise EIG_{τ*} acquisition to pick queries; relies on access to a simulator or generative model to query arbitrary state-action pairs and evaluate expected reductions in uncertainty about τ*.",
            "environment_name": "Compared on the same continuous control benchmarks in this paper (when giving BARL TQRL access), e.g., Pendulum, Cartpole, Reacher, β Tracking, β+Rotation (with BARL allowed to make transition queries).",
            "environment_characteristics": "Continuous state/action, unknown dynamics (Bayesian), finite-horizon tasks; BARL specifically assumes TQRL/generative access (not constrained to collect data only via rollouts).",
            "environment_complexity": "Same tasks as TIP; example median performance numbers reported in this paper: Pendulum 21 samples; Cartpole 111; β Tracking 186; β+Rotation &gt;500 (failed median); Reacher 251 (see paper Table 6).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In the paper's comparisons, BARL sometimes matches or underperforms TIP: e.g., Pendulum 21 samples (comparable to TIP); on β+Rotation BARL failed to solve within budget (&gt;500 median samples) while TIP solved in 201; BARL's performance depends on how the EIG acquisition is optimized (the paper notes BARL used uniform sampling over candidate points which may underperform TIP's optimization).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Variable: can be sample-efficient when TQRL access is available, but in these experiments BARL required more samples or failed on harder tasks (β+Rotation) compared to TIP despite TQRL privileges, primarily due to optimization choices.",
            "exploration_exploitation_tradeoff": "Purely exploration-oriented acquisition: selects queries to maximize expected information about τ*; not executed as trajectory rollouts in the standard MDP sense (requires generative access). Does not integrate exploitation via reward during query selection (TQRL-centric).",
            "comparison_methods": "Compared directly to TIP, sTIP, and other baselines in this paper.",
            "key_results": "BARL optimizes the same information-theoretic objective but in the TQRL setting; TIP outperforms BARL on some tasks despite BARL's stronger access assumptions, indicating the importance of efficient optimization and initialization of the acquisition (TIP's planner/initialization gave better practical collection of informative data).",
            "limitations_or_failures": "Requires TQRL / generative-model access (unrealistic in many real-world settings where data must be collected via rollouts), and practical implementations may rely on crude optimization (uniform candidate sampling) which can reduce effectiveness; struggled on the hardest plasma control task in these comparisons.",
            "uuid": "e1156.2"
        },
        {
            "name_short": "DIP",
            "name_full": "Dynamics Information Planning (DIP) [ablation]",
            "brief_description": "A task-agnostic planning ablation used in the paper that plans to maximize the joint predictive entropy (uncertainty) of the dynamics model over planned future observations (C_e), ignoring the specific task reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DIP (planned dynamics entropy maximization)",
            "agent_description": "Model-based planner (GP + iCEM) using cost C_e which is the negative joint entropy of the model predictive distribution over future next-states for the planned trajectory, i.e., plans to visit states that maximize model uncertainty without weighting by task.",
            "adaptive_design_method": "Task-agnostic information gain (maximizing predictive entropy of dynamics model)",
            "adaptation_strategy_description": "Plans trajectories that maximize joint predictive entropy H[T(S)|D] over the set of future state-action pairs expected to be visited, thereby adaptively seeking transitions where the dynamics are uncertain (but not task-specific).",
            "environment_name": "Same continuous control benchmarks used in paper (Pendulum, Cartpole, Reacher, β Tracking, β+Rotation, etc.).",
            "environment_characteristics": "Unknown continuous dynamics; optimization focuses on discovering dynamics structure globally rather than task-relevant regions; environments include sparse-reward and multi-task plasma control problems.",
            "environment_complexity": "Same per-task dimensionalities as TIP (Pendulum 2D/1D; Cartpole 4D/1D; Reacher ~10D/2D; β environments higher dimensional).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Underperforms TIP on task performance and sample complexity. Reported median samples to solve (Table 6): Pendulum 36; Cartpole 161; β Tracking 276; β+Rotation &gt;500 (failed); Reacher &gt;1000. Often collects data that is less useful for final control performance.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Worse than TIP and many baselines in terms of data needed to solve tasks; collects broadly informative dynamics data but not task-focused, resulting in poor downstream sample efficiency for control.",
            "exploration_exploitation_tradeoff": "Pure exploration objective (no explicit reward exploitation during exploration); the cost does not incorporate task reward, so it may explore dynamics-uncertain regions irrelevant to achieving the task.",
            "comparison_methods": "Compared to TIP, sTIP, sDIP, EIG_T, BARL, MPC, PETS, model-free baselines.",
            "key_results": "Task-agnostic information maximization is less effective for learning task policies than task-aware information gain (C_{τ*}); DIP collects dynamics information that is not necessarily relevant to task success, leading to worse sample complexity and control performance.",
            "limitations_or_failures": "Tends to gather data irrelevant for achieving task reward; failed to solve several harder tasks within allowed sample budgets; overexplores and thus is less sample-efficient for task learning.",
            "uuid": "e1156.3"
        },
        {
            "name_short": "sDIP",
            "name_full": "Summed Dynamics Information Planning (sDIP) [ablation]",
            "brief_description": "An ablation that approximates joint dynamics information by summing pointwise predictive entropies at future timesteps (i.e., an upper-bound/overestimate of joint information), which can lead to redundant queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "sDIP (summed pointwise dynamics entropy)",
            "agent_description": "Planner using cost equal to the negative sum over timesteps of pointwise predictive entropies H[T(s_i,a_i)|D], ignoring redundancy across timesteps (an overestimate of joint information). Implemented with GP + iCEM like other ablations.",
            "adaptive_design_method": "Pointwise predictive entropy summation (approximate info-gain about dynamics)",
            "adaptation_strategy_description": "Selects actions by maximizing the sum of individual entropies at planned future points, failing to account for redundancy between nearby observations, potentially overvaluing local repeated information.",
            "environment_name": "Same set of continuous control benchmarks in the paper.",
            "environment_characteristics": "Unknown continuous dynamics; similar task settings as DIP and TIP.",
            "environment_complexity": "Same as other experiments (varied per task: Pendulum 2D/1D, Cartpole 4D/1D, Reacher ~10D/2D, plasma tasks higher-dim).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Worse than TIP; reported median samples to solve (Table 6): Pendulum 46; Cartpole 141; β Tracking 131; β+Rotation &gt;500; Reacher &gt;1000 (failed within budget on some tasks).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Less sample-efficient than TIP and several baselines due to redundancy and overestimation of joint info.",
            "exploration_exploitation_tradeoff": "Pure exploration; fails to consider task reward and double-counts information across timesteps leading to inefficient exploration sequences.",
            "comparison_methods": "Compared as an ablation against TIP, sTIP, DIP, EIG_T, BARL, MPC, PETS, etc.",
            "key_results": "Summing pointwise entropies is a poor approximation of joint information (overestimates value), leading to redundant data collection and worse downstream control performance compared to joint-information-aware TIP.",
            "limitations_or_failures": "Overestimates information due to ignoring redundancy; often fails on harder tasks within sample budget; not task-aware.",
            "uuid": "e1156.4"
        },
        {
            "name_short": "sTIP",
            "name_full": "summed Trajectory Information Planning (sTIP) [ablation]",
            "brief_description": "An ablation that plans using a cost equal to the negative sum of pointwise EIG_{τ*} values for future timesteps (i.e., sums individual expected information gains about τ* without accounting for joint redundancy).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "sTIP (summed EIG_{τ*} ablation)",
            "agent_description": "Uses the same GP model and planner but replaces TIP's joint C_{τ*} with a cost that sums pointwise EIG_{τ*}(s_i,a_i) across planned timesteps; this ignores mutual-information overlap between observations.",
            "adaptive_design_method": "Pointwise expected information gain about τ* summed over timesteps (approximate EIG aggregation)",
            "adaptation_strategy_description": "Selects action sequences by summing estimated EIG_{τ*} over each planned future observation and optimizing that summed score (computationally simpler but ignores redundancy), then executes via MPC like TIP.",
            "environment_name": "Same benchmark suite as TIP (Pendulum, Cartpole, Reacher, β Tracking, β+Rotation, open-loop tasks).",
            "environment_characteristics": "Unknown dynamics continuous tasks, same as TIP experiments.",
            "environment_complexity": "As per task (Pendulum 2D/1D; Cartpole 4D/1D; Reacher ~10D/2D; plasma tasks higher-dim).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Performs worse than TIP across tasks but better than some task-agnostic baselines; median samples to solve (Table 6): Pendulum 36; Cartpole 141; β Tracking 76; β+Rotation &gt;500; Reacher &gt;400 (failed on some seeds/tasks).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Less sample-efficient than TIP due to overestimation of joint information and redundant exploration; nevertheless often better than purely task-agnostic methods.",
            "exploration_exploitation_tradeoff": "Attempts to be task-aware (EIG_{τ*}) but aggregates gains naively (sum) leading to greedier/overcounting exploration relative to TIP's joint computation; exploitation handled by MPC replanning during evaluation.",
            "comparison_methods": "Compared as an ablation to TIP, DIP, sDIP, EIG_T, BARL, MPC and other baselines.",
            "key_results": "sTIP is outperformed by TIP across the board, demonstrating the importance of computing joint expected information (C_{τ*}) rather than summing pointwise EIG values to account for redundancy between future observations.",
            "limitations_or_failures": "Overestimates information by summing pointwise EIGs; collects redundant data and is less sample-efficient especially on harder tasks like β+Rotation and Reacher.",
            "uuid": "e1156.5"
        },
        {
            "name_short": "EIG_T",
            "name_full": "EIG_{τ*} (pointwise / TQRL variant)",
            "brief_description": "The expected information gain about the optimal trajectory per state-action query (originally introduced in BARL/TQRL); used here as a baseline where the acquisition is evaluated per-point (TQRL) or as a simple ablation.",
            "citation_title": "An experimental design perspective on model-based reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "EIG_T (pointwise expected info gain about τ*)",
            "agent_description": "Acquisition function that computes expected information gain about τ* for a single state-action query (EIG_{τ*}(s,a)); in the TQRL setting it can be used to choose individual query points to reduce uncertainty about the optimal trajectory.",
            "adaptive_design_method": "Pointwise expected information gain about optimal trajectory (EIG_{τ*})",
            "adaptation_strategy_description": "Evaluates EIG_{τ*} for candidate state-action queries and selects those with highest pointwise expected information; when used naively across a sequence or summed it may overcount redundant information.",
            "environment_name": "Used in TQRL experiments and as a baseline comparator on continuous control tasks in the paper (when TQRL access is allowed).",
            "environment_characteristics": "Assumes access to a generative model that can be queried at arbitrary state-action pairs (TQRL), continuous unknown dynamics, finite-horizon tasks.",
            "environment_complexity": "Same benchmark environments when given TQRL access; in this paper EIG_T baseline often required more data and failed on harder tasks (see Table 6): Pendulum 56 samples; Cartpole 121; β Tracking &gt;1000; β+Rotation &gt;1000; Reacher &gt;1500.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In the paper, EIG_T (as implemented) was often less sample-efficient and sometimes failed within budget on harder tasks compared to TIP and some other baselines; its TQRL use gives more flexible data-access but practical optimization/selection matters.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Generally worse than TIP and in many cases worse than other model-based baselines when evaluated in the paper's settings (see per-task medians above).",
            "exploration_exploitation_tradeoff": "Purely exploration-oriented acquisition; does not incorporate task reward into the acquisition itself and relies on selecting high-EIG points, which may be suboptimal for downstream control if optimization is crude or points are redundant.",
            "comparison_methods": "Compared to TIP, sTIP, DIP variants, BARL (which uses EIG_{τ*} in TQRL), MPC, PETS, and other baselines.",
            "key_results": "Pointwise EIG_{τ*} used in TQRL is theoretically attractive but in practice can underperform if joint effects and redundancy between points are not accounted for and if the acquisition optimization is naive; TIP's joint C_{τ*} is empirically superior in the trajectory-rollout setting.",
            "limitations_or_failures": "Relies on TQRL (generative) access often unavailable in real-world tasks; pointwise evaluation and naive optimization (uniform candidate sampling) can lead to suboptimal data collection and failure on harder tasks.",
            "uuid": "e1156.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An experimental design perspective on model-based reinforcement learning",
            "rating": 2,
            "sanitized_title": "an_experimental_design_perspective_on_modelbased_reinforcement_learning"
        },
        {
            "paper_title": "Bayesian algorithm execution: Estimating computable properties of black-box functions using mutual information",
            "rating": 2,
            "sanitized_title": "bayesian_algorithm_execution_estimating_computable_properties_of_blackbox_functions_using_mutual_information"
        },
        {
            "paper_title": "Efficiently sampling functions from gaussian process posteriors",
            "rating": 1,
            "sanitized_title": "efficiently_sampling_functions_from_gaussian_process_posteriors"
        },
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "rating": 1,
            "sanitized_title": "deep_reinforcement_learning_in_a_handful_of_trials_using_probabilistic_dynamics_models"
        },
        {
            "paper_title": "Planning to explore via self-supervised world models",
            "rating": 1,
            "sanitized_title": "planning_to_explore_via_selfsupervised_world_models"
        }
    ],
    "cost": 0.025719,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploration via Planning for Information about the Optimal Trajectory</p>
<p>Viraj Mehta virajm@cs.cmu.edu 
Robotics Institute</p>
<p>Ian Char ichar@cs.cmu.edu 
Machine Learning Department
Carnegie Mellon University</p>
<p>Joseph Abbate jabbate@princeton.edu 
Princeton University</p>
<p>Rory Conlin wconlin@princeton.edu 
Princeton University</p>
<p>Mark D Boyer mboyer@pppl.gov 
Princeton Plasma Physics Laboratory</p>
<p>Stefano Ermon 
Computer Science Department
Stanford University</p>
<p>Jeff Schneider schneide@cs.cmu.edu 
Robotics Institute</p>
<p>Willie Neiswanger neiswanger@cs.stanford.edu 
Computer Science Department
Stanford University</p>
<p>Exploration via Planning for Information about the Optimal Trajectory</p>
<p>Many potential applications of reinforcement learning (RL) are stymied by the large numbers of samples required to learn an effective policy. This is especially true when applying RL to real-world control tasks, e.g. in the sciences or robotics, where executing a policy in the environment is costly. In popular RL algorithms, agents typically explore either by adding stochasticity to a reward-maximizing policy or by attempting to gather maximal information about environment dynamics without taking the given task into account. In this work, we develop a method that allows us to plan for exploration while taking both the task and the current knowledge about the dynamics into account. The key insight to our approach is to plan an action sequence that maximizes the expected information gain about the optimal trajectory for the task at hand. We demonstrate that our method learns strong policies with 2x fewer samples than strong exploration baselines and 200x fewer samples than model free methods on a diverse set of low-to-medium dimensional control tasks in both the open-loop and closed-loop control settings. 1</p>
<p>Introduction</p>
<p>The potential of reinforcement learning (RL) as a general-purpose method of learning solutions to sequential decision making problems is difficult to overstate. Ideally, RL could allow for agents that learn to accomplish all manner of tasks solely through a given reward function and the agent's experience; however, RL has so far broadly fallen short of this. Chief among the difficulties in realizing this potential is the fact that typical RL methods in continuous problems require very large numbers of samples to achieve a near-optimal policy. For many interesting applications of RL this problem is exacerbated by the fact that collecting samples from the true environment can incur huge costs. For example, expensive scientific experiments are needed for tokamak control [31,13,20] and design of molecules [70], and collecting experience on the road is both costly and runs the risk of car accidents for autonomous vehicles [66].</p>
<p>Though model-based methods like Deisenroth and Rasmussen [21], Chua et al. [15], and Curi et al. [16] are much more efficient than typical model-free methods, they do not explicitly reason about the prospective task-relevant information content of future observations. Moreover, these methods typically require thousands of timesteps of environment dynamics data to solve reasonably simple Markov decision processes (MDPs). Though progress has been made in using Thompson sampling [46], entropy bonuses [27], and upper confidence bounds (UCB) [16,5] to more intelligently explore determine where to explore next from its current state. To do so, in (b) the agent samples dynamics models T ∼ P (T | D) from its current posterior and finds approximately optimal trajectories τ * ∼ P (τ * | T ) for each sample. Then in (c) it pools these samples of posterior optimal trajectories τ * . In (d) it constructs a function that gives the joint expected information gain about the optimal trajectory τ * given a planned exploration trajectory (i.e. EIGτ * over the set of points visited). Finally, in (e) the agent can plan an action sequence which maximizes this joint expected information gain.</p>
<p>the state-action space of an MDP, these methods still do not explicitly reason about how information that the agent gathers will affect the estimated MDP solution. Furthermore, in continuous stateaction settings these methods must make coarse approximations to be computationally tractable (e.g. bootstrapped Q networks to approxiate a posterior or one-step perturbations for approximate UCB).</p>
<p>Many methods in the vein of Pathak et al. [47] and Shyam et al. [61] explore directly to gather new information based on current uncertainty about environment dynamics, but they do not in general specialize the information that they aim to acquire for a particular task specified by an initial state distribution and reward function. We believe that an ideal exploration strategy for sample efficiency should take into account this task, as well as uncertainty about the environment dynamics.</p>
<p>In this work, we start by showing how many methods can be cast as a Bayesian planning problem over a specific cost function. This framework helps illuminate the importance of the cost function and what impact it has on exploration. Viewed in this light, it becomes clear that many previous state-of-the-art methods rely on cost functions that either result in behavior that is too greedy-i.e. the policy tries to maximize returns during exploration-or too exploratory, i.e. the policy is incentivized to explore the environment dynamics and does not consider the task at hand. We therefore present a cost function that balances out these two extremes, by generalizing an information-gain acquisition function introduced in Mehta et al. [40] to apply to a set of future hypothetically acquired data. In particular, our cost function captures the amount of information that would be gained about the optimal trajectory, if the agent were to explore by following a particular planned trajectory. As depicted in Figure 1, this involves the agent sampling what it would hypothetically do given different realizations of the dynamics, and then planning actions that are informative about those possibilities.</p>
<p>In summary, the contributions of this work are as follows: we develop a novel cost function for exploration that explicitly accounts for both the specific task and uncertainty about environment dynamics, a method for planning which applies the cost function to explore in MDPs with continuous states and actions, and a thorough empirical evaluation of our method across 5 closed-loop and 3 open-loop environments (with a focus on expensive RL tasks in plasma physics) compared against 14 baselines. We find that our proposed method is able to learn policies that perform as well as an agent with access to the ground truth dynamics using half or fewer samples than comparison methods.</p>
<p>Related Work</p>
<p>Exploration in Reinforcement Learning The most common strategy for exploration in RL is to execute a greedy policy with some form of added stochasticity. The simplest approach, -greedy exploration as used in Mnih et al. [41], takes the current action thought to be best with probability 1 − and a random action with probability . Other methods use added Ornstein-Uhlenbeck action noise [37] to the greedy policy, or entropy bonuses [27] to the policy or value function objectives to add noise to a policy which is otherwise optimizing the RL objective.</p>
<p>Tabular RL is often solved by choosing actions based on upper confidence bounds on the value function [14,36], but explicitly computing and optimizing these bounds in the continuous setting is substantially more challenging. Recent work [16] approximates this method by computing one-step confidence bounds on the dynamics and training a 'hallucinated' policy which chooses perturbations within these bound to maximize expected policy performance. Another recent work [5] uses anticoncentration inequalities to approximate upper confidence bounds in MDPs with discrete actions.</p>
<p>Thompson sampling (TS) [55], which samples a realization of the MDP from the posterior and acts optimally as if the realization was the true model, can be applied for exploration in a model-free manner as in [45] or in a model-based manner as in [63]. As the posterior over MDP dynamics or value functions can be high-dimensional and difficult to represent, the performance of TS can be hindered by approximation errors using both Gaussian processes and ensembles of neural networks. Curi et al. [16] recently investigated this and found that this was potentially due to an insufficiently expressive posterior over entire transition functions, implying that it may be quite difficult to solve tasks using sampled models. Similarly, the posterior over action-value functions in Osband et al. [45] is only roughly approximated by training a bootstrapped ensemble of neural networks.</p>
<p>There is also a rich literature of Bayesian methods for exploration, which are typically computationally expensive and hard to use, though they have attractive theoretical properties. These methods build upon the fundamental idea of the Bayes-adaptive MDP [53], which we detail in Section E.1 alongside a discussion of this literature.</p>
<p>Additionally, a broad set of methods explore to learn about the environment without addressing a specified task. This line of work is characterized by Pathak et al. [47], which synthesizes a taskagnostic reward function from model errors. Other techniques include MAX [61], which optimizes the information gain about the environment dynamics, Random Network Distillation [11], which forces the agent to learn about a random neural network across the state space, and Plan2Explore [60], which prospectively plans to find areas of novelty where the dynamics are uncertain.</p>
<p>Bayesian Experimental Design: BOED, BO, BAX, and BARL There is a large literature on Bayesian optimal experiment design (BOED) [12] which focuses on efficiently querying a process or function to get maximal information about some quantity of interest. When the quantity of interest is the location of a function optimum, related strategies have been proposed as the entropy search family of Bayesian optimization (BO) algorithms [29,30]. Recently, a flexible framework known as Bayesian algorithm execution (BAX) [43] has been proposed to efficiently estimate properties of expensive black-box functions; this framework gives a general procedure for sampling points which are informative about the future execution of a given algorithm that computes the property of interest, thereby allowing the function property to be estimated with far less data.</p>
<p>A subsequent related work [40], known as Bayesian Active Reinforcement Learning (BARL), uses ideas from BOED and BAX to sample points that are maximally informative about the optimal trajectory in an MDP. However, BARL relies on a setting the authors call Transition Query Reinforcement Learning (TQRL), which assumes that the environment dynamics can be iteratively queried at an arbitrary sequence of state-action pairs chosen by the agent. TQRL is thus a highly restrictive setting which is not suitable when data can only be accessed via a trajectory (rollout) of environment dynamics; it typically relies on an accurate environment simulator of sufficient expense to warrant its use. Even then, there will likely be differences between simulators and ground truth dynamics for complex systems. Thus, one would ideally like to collect data in real environments. However, this often requires leaving the TQRL setting, and instead collecting data via trajectories only.</p>
<p>In this paper, we aim to apply the information-theoretic ideas from BARL but generalize them to the general MDP setting as well as learn open loop model-based controllers. The typical method for learning to solve open-loop control problems was demonstrated successfully in Tesch et al. [65], where a value function was learned from action sequences to task success. Our method takes a model-based approach to this problem, using similar exploration strategies as Bayesian optimization but benefitting from the more substantial supervision that is typical in dynamics model learning.</p>
<p>Problem Setting</p>
<p>In this work we deal with finite-horizon discrete-time Markov decision processes (MDPs) which consist of a sextuple S, A, T, r, p 0 , H where S is the state space, A is the action space, T is the transition function T : S × A → P (S) (using the convention that P (X ) is the set of probability measures over X ), r : S × A × S → R is a reward function, p 0 (s) is a distribution over S of start states, and H ∈ N is the horizon (i.e. the length of an episode). We always assume S, A, p 0 , and H are known. We also assume the reward r is known, though our development of the method can easily be generalized to the case where r is unknown. Our primary object of interest is the transition function T , which we learn from data. We address both open and closed loop control settings. In the more common closed loop setting, our aim is to find a policy π : S → A that maximizes Objective (1) below. We will denote trajectories as τ ∼ p(τ | π, T ) where τ = [(s 0 , a 0 ), . . . , (s H−1 , a H−1 ), s H ] generated by s 0 ∼ p 0 , a i = π(s i ), and s i+1 ∼ T (s i , a i ). We can write the return of a trajectory as
R(τ ) = H−1 i=0 r(s i , a i , s i+1 )
for the states and actions s i , a i that make up τ . The MDP objective can then be written as
J T (π) = E τ ∼p(τ |π,T ) [R(τ )] .(1)
We aim to maximize this objective while minimizing the number of samples from the ground truth transition function T that are required to reach good performance. We denote the optimal policy as π * = arg max π J T (π), which we can assume to be deterministic [64] but not necessarily unique. We use τ * to denote optimal trajectories, i.e. τ * ∼ p(τ | π * , T ).</p>
<p>Similarly, for the open-loop setting, we assume a fixed start state s 0 and aim to find an action sequence a 0 , . . . , a H−1 that maximizes the sum of rewards in an episode. We will slightly abuse notation and write τ ∼ p(τ | a 0:H−1 ) and J T (a 0:H−1 ) with these actions fixed in place of a reactive policy, and again use τ * to refer to the trajectories generated by an optimal action sequence.</p>
<p>We assume in this work that applying planning algorithms like [51] to a dynamics function T will result in a trajectory that approximates τ * . We will primarily focus on a Gaussian process (GP) model of the transition dynamics in order to take advantage of its expressive representation of uncertainty and grounded methods for sampling, conditioning, and joint inference. There is substantial prior work using GPs in RL-see Section E.2 for a discussion of this literature. Under this modeling choice, we assume that the dynamics are drawn from a GP prior P (T ) (see Section A.3 for further details on our GP model) and use P (T | D) for the posterior over transition functions given a dynamics dataset of triples D = {(s i , a i , s i )}. In this work, unions D ∪ τ or D ∪ S between the dataset D and trajectories τ or next state predictions S coerce τ and S into triples of dynamics data, prior to the union with the dataset.</p>
<p>Trajectory Information Planning</p>
<p>Our method consists of a generic framework for Bayesian (or approximately Bayesian) modelpredictive control and a novel cost function for planning that allows us to explicitly plan to find the maximal amount of new information relevant to our task. In Section 4.1, we describe the MPC framework and highlight that many prior methods approximate this framework while using a greedy cost function that corresponds to the future negative expected rewards or a pure exploration cost function that corresponds to future information about the dynamics. Afterwards, in Section 4.2, we derive our new cost function and describe how it is computed. The overall method we introduce simply applies this planning framework with our new cost function.</p>
<p>Model-Predictive Control in Bayesian Model-Based RL</p>
<p>In this section, we give a formulation of Bayesian planning for control that generalizes ideas from methods such as PILCO [21] and PETS [15]. This formulation highlights these methods' inherently greedy nature and hints at a possible solution. The objective of Bayesian planning is to find the h-step action sequence that maximizes the expected future returns under model uncertainty. That is,
argmin a0,...,a h−1 E T ∼P (T |D),τe∼P (τ |s0=s,a 0:h−1 ,T ) <a href="2">C(τ e )</a>
for some cost function C over trajectories and some start state s. If operating in the open-loop control setting, the agent executes the sequence of actions found without replanning. This procedure can also be extended to closed-loop control via model-predictive control (MPC), which involves re-planning (2) at every state the agent visits and playing the first action from the optimal sequence. Concretely, the MPC policy for our Bayesian setting is as follows:
π MPC (s) = arg min a0 min a1,...,a h−1 E T ∼P (T |D),τe∼P (τ |s0=s,a 0:h−1 ,T ) <a href="3">C(τ e )</a>
Whether we do open-loop control or closed-loop control via MPC, the cost function C, is integral to how the agent will behave. Prior work has predominantly focused on two types of cost function:
C g (τ ) = −R (τ ) Greedy Exploration C e (τ ) = − h i=0 H [T (s i , a i ) | D]
Task-Agnostic Exploration (4) Previous works such as Kamthe and Deisenroth [34] and PETS [15] use the greedy exploration cost function, C g . This cost function incentivizes trajectories that achieve high rewards over the next h transitions on average. In works that focus on task-agnostic exploration such as Sekar et al. [60] and Shyam et al. [61], the cost function C e (or similar) is used to encourage the agent to find areas of the state space in which the model is maximally uncertain. Note that we use π g to refer to the greedy policy given by using (3) with C g .</p>
<p>The optimization problem in (3) is typically approximately solved in one of three ways: Deisenroth and Rasmussen [21] and Curi et al. [16] directly backpropagate through the estimated dynamics and reward functions to find policy parameters that would generate good actions, Janner et al. [32] use an actor-critic method trained via rollouts in the model alongside the data collected to find a policy, and Chua et al. [15] and Mehta et al. [40] use the cross-entropy method [17] to find action sequences which directly maximize the reward over the estimated dynamics. In this work, we use a version of the last method given in Pinneri et al. [51], denoted iCEM, to directly find action sequences that optimize the cost function being used. We approximate the expectation by playing the actions on multiple samples from the posterior P (T | D). Algorithm 1 gives a formal description of the method and Section A.5 provides further details.</p>
<p>Algorithm 1 Bayesian Model-Predictive Control with Cost Function C</p>
<p>Inputs: transition function episode query budget b, number of posterior function samples k,
planning horizon h. Initialize D ← ∅. for i ∈ [1, . . . , b] do Sample start state s 0 ∼ p 0 . for t ∈ [0, . . . , H − 1] do Sample posterior functions {T } k =1 ∼ P (T | D). Approximately find arg min a0,...,a h−1 k =1 E τ ∼p(τ |T ,a0,...,a h−1 ) [C(τ )] via iCEM. Execute action a 0 by sampling s t+1 ∼ T (s t , a 0 ). Update dataset D ← D ∪ {(s t , a 0 , s t+1 }.
end for end for return π g for the posterior P (T | D).</p>
<p>A Task-Specific Cost Function based on Trajectory Information</p>
<p>In this work, we aim to explore by choosing actions that maximize the conditional expected information gain (EIG) about the optimal trajectory τ * . This is the same overall goal as that of Mehta et al. [40], where the EIG τ * acquisition function was introduced for this purpose. However, in this paper we generalize this acquisition function in order to allow for sequential information collection, and account for the redundant information that could be collected between timesteps. As discussed at length in Osband et al. [46], it is essential to reason about how an action taken at the current timestep will affect the possibility of learning something useful in future timesteps. In other words, exploration must be deep and not greedy. Explicit examples are given in Osband et al. [46] where the time to find an -optimal policy in a tabular MDP is exponential in the state size unless exploration can be coordinated over large numbers of timesteps rather than being conducted independently at each action. As the EIG τ * acquisition function is only defined over a single state-action pair and mutual information is submodular, we cannot naively use the acquisition function as is (or sum it over many datapoints) to choose actions that lead to good long-term exploration. This is clear in e.g. navigation tasks, where the nearby points visited over trajectories will provide redundant information about the local environment.</p>
<p>We therefore give a cost function that generalizes EIG τ * by taking a set of points to query and computing the joint expected information gain from observing the set. Our cost function is non-Markovian in the state space of the MDP, but it is Markovian in the dataset, which represents a point in the belief space of the agent about the dynamics. Let X = {x : x ⊆ S × A, |x| &lt; ∞} be the set of finite subsets of the set of all state-action pairs. Our cost function C τ * : X → R is defined below to be the negative joint expected information gain about the optimal trajectory τ * for a subset X ∈ X . In particular, assuming an existing dataset D, a set of h query points X = {(s i , a i )} i∈ [h] , and a random set of next states
S = {s i ∼ T (s i , a i ), i ∈ [h]}, C τ * (X) = E S ∼p(S |X,D) [H [τ * | D ∪ S ]] − H [τ * | D] .(5)
This formulation of C τ * forces our method to handle the redundant information among queries-it is likely that I(s 1 , τ * ) + I(s 2 , τ * ) &gt; I({s 1 , s 2 }, τ * ) and our method should avoid this overestimation. However, as written, this function relies on computing entropies on high-dimensional trajectories where the form of the joint distribution of the elements is unknown. To tractably estimate this quantity, we use the fact that C τ * (X) = −I(S , τ * ) = −I(τ * , S ) for the mutual information I. This allows us to exchange τ * and our set of queries so that τ * is giving information about the posterior predictive distribution of our set. In other words,
C τ * (X) = E τ * ∼p(τ * |D) [H [S | D ∪ τ * ]] − H [S | D] .(6)
In order to compute the right-hand term, we must take samples τ * ij ∼ P (τ * | D), i = 1, . . . , m, j = 1, . . . , n. To do this, we first sample m start states s (i) 0 from p 0 (we always set m = 1 in experiments but derive the procedure in general) and for each start state independently sample n posterior functions T ij ∼ P (T | D) from our posterior over dynamics models. We then run a planning procedure using iCEM [51] on each of the posterior functions from s (i) 0 using T ij for T (using our assumption that planning can generate approximately optimal trajectories given ground-truth dynamics), giving our sampled τ * ij . Formally, we can approximate C τ * via Monte-Carlo as
C τ * (X) ≈ 1 mn   i∈[m] j∈[n] H[S |D ∪ τ * ij ]   − H[S | D].(7)
Assuming the dynamics are modelled with a Gaussian process, we can compute the joint Gaussian probability of the next states S [52]. As the entropy of a multivariate Gaussian depends only on the log-determinant of the covariance, log |Σ|, we can tractably compute the joint entropy of the model predictions H [S | D] and optimize it with a zeroth order optimization algorithm. Finally, we must calculate the entropy H[S |D ∪ τ * ij ]. For this, we follow a similar strategy as Neiswanger et al. [43]: since τ * i is a set of states given by the transition model, we can treat them as additional noiseless datapoints for our dynamics model and condition on them before computing the joint covariance matrix for S . Given this newly generalized acquisition function, we can instantiate a method of planning in order to maximize future information gained. We give the concrete procedure for computing our acquisition function in Algorithm 2, noting that trajectories τ * ij do not depend on the query set X and can be cached for various values of X as long as the dataset D does not change.</p>
<p>Our ultimate procedure, which we name Trajectory Information Planning (TIP), is quite simple: run model-based RL using MPC as in Algorithm 1, but set the cost function to be C τ * (τ ) instead of C g or C e , and compute this cost function using Algorithm 2. At test time, we return to planning with C g as the cost function and greedily attempt to maximize returns rather than performing exploration. We can also formulate an open-loop variant of our method, oTIP, which involves planning once and then executing the entire action sequence.</p>
<p>Computational Cost and Implementation Details</p>
<p>Though the TIP algorithm is designed for settings where samples are expensive, it is important to understand, both theoretically and practically, the computational cost of this method. For ease of notation, we make the simplifying assumption that the planning algorithm used (in this case, iCEM from [51]) evaluates p action sequences consisting of h (the planning horizon) actions and that our current dataset is of size N . In order to efficiently sample functions from the posterior over dynamics functions, we use the method from Wilson et al. [69]. This reduces the naive complexity of querying these functions from O(N 3 ) to a one-time O(N ) cost and then O(1) for additional queries. As we derive in Section A.1, the computational complexity of one TIP planning iteration
is O nm (N + H) 3 + ph (N + H) 2 .
The two asymptotically expensive operations are (1) 
Algorithm 2 Computation of C τ * Inputs: dataset D = {(s k , a k , s k )},(i) 0 } m i=1 ∼ p 0 . for i ∈ [m] do Sample n posterior functions {T j } n j=1 ∼ P (T | D). for j ∈ [n] do
Set π * j ← π MPC using C g and a singleton posterior P (T | D) = δ(T j ) as in (3). Compute τ * ij by executing π * j on T j starting from s computing the Cholesky decompositions of the nm kernel matrices for datasets D ∪ τ * ij and (2) solving the triangular systems using the cached Cholesky decompositions in order to compute the covariance matrices Σ S ij | D ∪ τ * ij for each of the p action sequences used by the planning algorithm. However, our implementation choices mean that in practice these operations are not the most expensive step. The covariance matrix computations, which are the theoretical bottleneck, are implemented in JAX [9], allowing them to be compiled to much faster machine code and vectorized across large batches of queries. In fact, the most expensive operation in practice is planning on the sampled transition functions T i to sample optimal trajectories τ * ij . This is due to the fact that in practice p is large and we implemented the planner in NumPy [28] so it cannot be compiled together with the Tensorflow [1] code from Wilson et al. [69], which is used for predicting which states will be visited for the planner. We give further information on the implementation in Section A.  The aim of our development of the TIP algorithm and the C τ * acquisition function for RL is to reduce the sample complexity of learning effective policies in continuous MDPs given limited access to expensive dynamics. In this section we demonstrate the effectiveness of TIP in quickly learning a good policy by comparing against a variety of state-of-the-art reinforcement learning algorithms and strong baselines (including some that use the TQRL setting from [40], which is also known as RL with a generative model in Kakade [33] and other works [6,4]).</p>
<p>Experiments</p>
<p>Rollout</p>
<p>In particular, we compare the average return across five evaluation episodes across five random seeds of each algorithm on five closed-loop control problems. For sample complexity we assess the median amount of data taken by each algorithm to 'solve' the problem across five seeds with the threshold performance given by an MPC controller using the ground truth dynamics.  Table 1: Sample Complexity: Median number of samples across five seeds required to reach 'solved' performance, averaged across five trials. We determine 'solved' performance by running an MPC policy (similar to the one used for evaluation) on the ground truth dynamics to predict actions. We record &gt; n when the median run is unable to solve the problem by the end of training after collecting n datapoints. The methods in the rightmost section operate in the TQRL setting and therefore have more flexible access to the MDP dynamics for data collection. The full set of methods are shown in Section D as well as boxplots depicting the data in Figure 4.</p>
<p>Here too, we assess the average return as open-loop trials are conducted as well as the number of timesteps required to achieve 'solved' performance.</p>
<p>Comparison Methods We use several model-based and model-free comparison methods in this work. We compare to several published model-based methods. These include PETS [15], as implemented by Pineda et al. [50], which uses a probabilistic ensemble of neural networks and CEM over particle samples to do MPC. We also compare against three model-based techniques from the HUCRL [16] implementation: HUCRL itself, which relies on hallucinating dynamics perturbations as a way of realizing an upper confidence bound on the policy, model-based Thompson Sampling (TS), which samples from the posterior over models and chooses optimal actions for that sample, and a greedy model-based neural network method relying on backpropagation through time (BPTT). We also compare against the Free Energy of the Expected Future method from Tschantz et al. [67], which treats directed exploration as a process of actively collecting information for inference on a reward-biased generative model. A further comparison is with Receding Horizon Curiosity (RHC) [59], which does online Bayesian system identification over a linear model in order to quickly find a model of the environment dynamics. Our model-free comparison methods, Soft Actor-Critic (SAC) [27], an actor-critic method that uses an entropy bonus over the policy to encourage more exploration, and two others (TD3 and PPO), are in the appendix.</p>
<p>Finally, we compare against various ablations of the proposed method. These vary across two axes as described in Figure 2: the cost function they use and how they handle sequential queries. Besides these differences, they use the same GP model and iCEM planning algorithm with the same hyperparameters, so they are truly comparable methods. The three cost functions used are C g , C τ * , and C e . DIP, oDIP, and EIG T all use C e but compute, respectively, the expected joint entropy of the action sequence, the sum of the pointwise entropies of the action sequence, and the individual pointwise entropies in the TQRL setting. These methods are very similar in spirit to [61,48] in that they plan for future information gain about the dynamics, but we chose to compare in a way that controls for difference in the model and planning algorithm. MPC uses C g and is very close to the method in [34]. Like TIP, BARL [40] and sTIP use C τ * . BARL operates in the TQRL setting and can therefore use the simpler EIG τ * acquisition function. sTIP investigates the use of − si,ai∈S EIG τ * (s i , a i ) as a cost function for planning. This computes individual information gains for each future observation without accounting for the information they may have in common and is therefore an overestimate of the joint information gain. In the open-loop setting we compare against oDIP and oMPC, the open-loop variants of DIP and MPC, and Bayesian optimization (BO) as implemented by Pedregosa et al. [49]. oDIP plans an action sequence to minimize the joint C e and executes the actions found for each open-loop trial, while oMPC does the same thing using C g . We give additional details on the comparison methods in Section B. Control Problems Our closed loop control problems are the standard underactuated Pendulum swing-up task (Pendulum-v0 from Brockman et al. [10]) with 2D states and 1D actions, a Cartpole swing-up task with a sparse reward function, 2D s, and 1D actions, a 2-DOF robot arm control problem where the end effector is moved to a goal (Reacher-v2 from Brockman et al. [10]) with 10D states and 2D actions, a simplified β Tracking problem from plasma control [13,39] (similar in design but not identical to the one from Mehta et al. [40]) trained using with 4D states and 1D actions, and a more complicated problem in plasma control where β + Rotation are tracked with 10D states and 2D actions. Our open loop control problems are a navigation problem with hazards (Lava Path) from [40] and two regulation problems with different Nonlinear Gain functions. The Lava Path problem has 4D states and 2D actions and the nonlinear regulation problems have 2D states and 2D actions. Full details on these problems are available in Section C.  Results As can be seen in Table 1, TIP is able to reach solved performance more quickly across the board than the model-based and model-free external baselines, often using a fraction or even orders of magnitude less data than other methods. For many of our ablation methods we see failures to solve some of the problems even though the model is demonstrated by TIP to be able to sufficiently predict the dynamics. This is especially apparent on the harder plasma control environment, β+Rotation, where TIP is the only method using our GP which is able to solve the problem. We believe that this is because the data acquired through exploration by the ablation methods is less useful for control than the data TIP collects. This is underscored by the second column of Figure 3, where it is clear that TIP achieves the lowest modeling error on the points actually needed during the execution of the policy but not on the uniform test set. In particular we find it interesting that TIP outperforms BARL on the β + Rotation environment, as BARL should in principle have a strictly stronger access to the problem and is optimizing the same quantity with fewer constraints. We hypothesis that this may be due to the fact that BARL optimizes the acquisition function EIG τ * by simply uniformly sampling a set of points and choosing the one that evaluates to the largest value. Our more sophisticated optimization algorithm and forced initialization at the start state distribution seems to allow us to collect more information in this case. This interpretation is bolstered by the fact that on the problems where TIP outperforms BARL, we see that TIP is actually collecting more information per action than BARL as evidenced by larger EIG values. We also see clearly that there is value in computing the C τ * function rather than summing over EIG τ * values, as TIP outperforms sTIP across the board. Additionally, there is clear evidence for the value of task-specific exploration as the task-agnostic exploration methods (EIG T , DIP, sDIP) underperform both in returns and model error on the trajectories visited.</p>
<p>For the open-loop experiments (Table 2), we also see strong performance from oTIP. As the modelbased methods benefit from observing many model transitions for each open-loop trial, it is unsurprising that they are more sample-efficient than the BO method. Within the model-based techniques, oTIP is the most sample efficient. We believe that this is for much the same reasons as in the closed-loop case-exciting evidence that the C τ * cost function can be applied in a variety of settings.</p>
<p>Conclusion</p>
<p>In this work, we presented and evaluated a cost function designed for intelligent, task-aware exploration. Using this cost function in model-predictive control allows agents to solve continuous MDPs with far less data than comparison methods in open-and closed-loop settings. Though the method is effective in data reduction, it is computationally expensive and relies on dynamics that are well-modeled by a GP. In future work, we aim to scale the method to higher-dimensional and more complex control environments. We also aim to apply this method in the real world. In particular, we aim to address similar plasma control problems in a small number of trials on a real tokamak.</p>
<p>A Implementation Details</p>
<p>A.1 Derivation of Computational Cost</p>
<p>In this section, we derive the computational complexity of the TIP algorithm. For simplicity, we focus on a single TIP planning iteration as might be done at each replanning in closed-loop control or at the start of a trial in open-loop control. In order to keep the analysis general, we assume that the chosen planning algorithm requires p accesses to the model where h actions are sequentially executed, giving ph total queries per planner execution. We also assume that the numbers of inducing points and basis functions used in our GP posterior function sampling are constant as are the Monte Carlo hyperparameters m, n.</p>
<p>The TIP algorithm consists of the following major operations: 
• Sample T i for j ∈ [</p>
<p>A.2 Wall Times</p>
<p>Though TIP and oTIP are designed for applications where samples are expensive and computation is relatively inexpensive, we present in this section data on the running time of these methods. We ran all experiments on a shared research cluster available to us on large machines with hundreds of GB of memory and between 24 and 88 CPU cores. In general our implementation did not make use of more than 20 CPU cores concurrently. In Table 3, we give the running time of the phases of the TIP algorithm. We note that the bulk of the computation in the planning procedure actually goes towards the just-in-time compilation of the JAX code that computes the cost function C τ * on sampled future trajectories. In order to allow for this compilation cost, we modified the iCEM algorithm from [51] to take fixed batch sizes as the compilation (e.g. for the β tracking problem) takes approximately 90% of the time required for planning. Unfortunately this compilation process must be repeated at every iteration due to the limitations of the JAX compiler. We believe that a similarly JIT-compiled implementation of the planning algorithm for sampling τ * on posterior samples could lead to a substantial speedup and a more flexible compiler could do more still.</p>
<p>A.3 GP Model Details</p>
<p>For all of our experiments, we use a squared exponential kernel with automatic relevance determination [38,42]. The parameters of the kernel were estimated by maximizing the likelihood of the parameters after marginalizing over the posterior GP [68].</p>
<p>To optimize the transition function, we simply sampled a set of points from the domain, evaluated the acquisition function, and chose the maximum of the set. This set was chosed uniformly for every problem but β + Rotation and Reacher, for which we chose a random subset of ∪ i ∪ j τ * ij (the posterior  16  15  15  50  295  Total for TIP Iteration  40  46  22  75  425   Evaluation for one episode  5-20  2-10  2-5  3 -18  100-500   Table 3: Runtime in seconds for the phases of the TIP algorithm on all problems when run on the authors' CPU machines. The ranges given show the runtime for the operation at the beginning and at the end of training, as some operations run longer as more data is added.  samples of the optimal trajectory) since the space of samples is 10-dimensional and uniform random sampling will not get good coverage of interesting regions of the state space.</p>
<p>Control Problem</p>
<p>A.4 Cost Function Details</p>
<p>We set n = 15 and m = 1 for our Monte Carlo estimate of the cost function for each problem.</p>
<p>A.5 Details on Planning Method</p>
<p>As mentioned in the main text, we use the iCEM method from Pinneri et al. [51] with one major modification: a fixed sample batch size. This is in order to take advantage of the JIT compilation features of JAX and avoid recompiling code for each new batch size.</p>
<p>In Tables 4 and 5, we present the hyperparameters used for the planning algorithm across each problem. The same hyperparameters were used for the TIP, MPC, EIG T , DIP, sDIP, and sTIP methods. As recommended by the original paper, we use β = 3 for the scaling exponent of the power spectrum density of sampled noise for action sequences, γ = 1.25 for the exponential decay of population size, and ξ = 0.3 for the amount of caching.</p>
<p>B Description of Comparison Methods</p>
<p>We  as opposed to sTIP which sums the individual information expected at each timestep. oDIP is simply the open loop variant of DIP. EIG T uses the same objective as sDIP but operates in the TQRL setting, querying points that approximately maximize the predictive entropy of the dynamics model. BARL similarly operates in the TQRL setting but uses the EIG τ * acquisition function from Mehta et al. [40]. We use the authors' implementation of that work for comparison. MPC uses C g from (4) and plans to directly maximize expected rewards. This method can be seen as quite similar to Kamthe and Deisenroth [34] and a close cousin of Deisenroth and Rasmussen [21] in that it optimizes the same objective with a similar model. oMPC is simply the open loop variant of MPC.</p>
<p>Besides these methods which directly compare cost functions, we include 8 additional baselines from published work. PETS is a method given in Chua et al. [15] which uses a similar cross-entropy based planner and a probabilistic ensemble of neural networks for an uncertainty-aware estimate of the dynamics. PETS also plans to minimize C g . HUCRL [16] learns a policy via backpropagation through time using a hallucinated perturbation to the dynamics that maximizes discounted rewards subject to the one-step confidence interval of the dynamics. HUCRL also uses a probabilistic ensemble of neural networks. Using the same implementation we also tested Thompson Sampling (TS), which acts optimally according to a network drawn from the posterior over models, and BPTT which plans to minimize C g using a neural network policy and backpropagation through time. BPTT can also be viewed as a cousin of PILCO [21] as it attempts to take stochastic gradients of the expected cost. We also compare against SAC [27], TD3 [23], and PPO [58]. SAC uses entropy bonuses to approximate Boltzmann exploration in an actor-critic framework. TD3 and PPO include various tricks for stable learning and add Ornstein-Uhlenbeck noise in order to explore.</p>
<p>For our FEEF implementation, we took hyperparameters from the most similar comparison environments in that paper and used them for our results. We tried several values for 'expl_weight' in order to se whether we were inadequately balancing exploration and exploitation. Ultimately we saw an 'expl_weight' of 0.1 was the best value.</p>
<p>We used the author's implementation of RHC. RHC makes strong assumptions on the form of the reward function by assuming that all problems are regulation problems where the goal is to drive the system to a given state and keep it there (with some cost for actuation). We were able to pass the targets for all of our problems (which may change between episodes) to the RHC controller. We did a light hyperparameter search tuning the number of random Fourier features used in the Bayesian linear model in this method. Ultimately we were disappointed in the performance of RHC when applied to our problems. We believe that this might be due to its undirected uncertainty sampling objective and relatively constrained model of environment dynamics.</p>
<p>C Description of Control Problems</p>
<p>C.1 Plasma Control Problems</p>
<p>The plasma control problems are based on controlling a tokamak, a toroidally shaped device for confining a thermonuclear plasma using magnetic fields. Achieving net positive energy from fusion requires confining a plasma at high enough temperature and density long enough for hydrogen isotopes to collide and fuse. However, as the temperature and density are increased, a wide variety of instabilities can occur which degrade confinement, leading to a loss of energy. Full physics simulation of tokamak plasmas requires 10s-1000s of CPU hours to simulate a single trajectory, and often require hand tuning of different parameters to achieve accurate results. Following the work of Abbate et al. [2], each of our plasma control problems used neural networks trained on data as the ground truth dynamics models. We used the MDSPlus tool [62] to fetch historical discharges from the DIII-D tokamak in San Diego [22]. In total, we trained our models on 1,479 historical discharges. The data was pre-processed following the procedure outlined in Abbate et al. [2]. We describe how each environment was constructed in more detail below.</p>
<p>β Tracking In this environment the goal is to adjust the total injected power (PINJ) of the neutral beams so that the normalized plasma pressure, β N (defined as the ratio of thermal energy in the plasma to energy in the confining magnetic fields), reaches a target value of 2%. Reliably controlling plasmas to sustain high performance is a major goal of research efforts for fusion energy, so even this simple scenario is of interest. The ground-truth dynamics model takes in the current β N and PINJ, the β N and PINJ at some ∆t time in the past, and the PINJ at some ∆t time in the future (we assume that we have complete control over the values of PINJ at all times). Given these inputs, the model was trained to output what β N will be ∆t time into the future. In total, the state space is 4D and the action space is 1D. For this environment, we set ∆t = 200ms, and we specify the reward function to be the negative absolute difference between the next β N and the target β N = 2%.</p>
<p>β + Rotation Tracking This environment is a more complicated version of the β tracking environment in several ways. First of all, the controller now must simultaneously track both β N and the core toroidal rotation of the plasma. To do so, the controller is also allowed to set the total torque injected (TINJ) of the neutral beams (DIII-D has eight neutral beam injectors at different positions around the tokamak, so it is generally possible to control both total power and total torque independently). Controlling both of these quantities simultaneously is of interest since rotation shear often results in better confinement and less chance of instabilities in the plasma [8,25]. In addition, we assume a multi-task setting where the requested targets for β N and rotation can be set every trajectory. Specifically, the β N target is drawn from U (1.5%, 2.5%) and the rotation target is drawn from U (25, 125) krad/s every trajectory. These targets are appended to the state space.</p>
<p>The learned, ground-truth dynamics model is also more sophisticated here. In addition to the inputs and outputs used by the β tracking environment model, the inputs for this model also include rotation and TINJ at times t, t − ∆t, and t + ∆t for TINJ only. This model receives additional information about the plasma (e.g. the shape of the plasma); however, we have assumed these inputs are fixed to reasonable values in order to avoid partial observability problems. In total, the state space of this problem is 10D (targets plus current and past observations for β N , rotation, PINJ, and TINJ) and the action space is 2D (next PINJ and TINJ settings).</p>
<p>C.2 Robotics Problems</p>
<p>Pendulum The pendulum swing-up problem is the standard one found in the OpenAI gym [10]. The state space contains the angle of the pendulum and its first derivative and action space simply the scalar torque applied by the motor on the pendulum. The challenge in this problem is that the motor doesn't have enough torque to simply rotate the pendulum up from all positions and often requires a back-and-forth swing to achieve a vertically balanced position. The reward function here penalizes deviation from an upright pole and squared torque.</p>
<p>Cartpole The cartpole swing-up problem has 4-dimensional state (position of the cart and its velocity, angle of the pole and its angular velocity) and a 1-dimensional action (horizontal force applied to the cart). Here, the difficulty lies in translating the horizontal motion of the cart into effective torque on the pole. The reward function is a negative sigmoid function penalizing the distance betweent the tip of the pole and a centered upright goal position.</p>
<p>Reacher The reacher problem simulates a 2-DOF robot arm aiming to move the end effector to a randomly resampled target provided. The problem requires joint angles and velocities as well as an indication of the direction of the goal, giving an 8-dimensional state space along with the 2-dimensional control space.</p>
<p>D Additional Results</p>
<p>Due to space constraints in the main paper, we omitted results for the methods sDIP and BPTT. The are included alongside the rest in Table 6. They are outperformed across the board by TIP.</p>
<p>E Additional Related Work E.1 Bayesian Exploration Techniques</p>
<p>Given unlimited computation and an accurate prior, solving the Bayes-adaptive MDP [53] gives an optimal tradeoff between exploration and exploitation by explicitly accounting for the updated beliefs that would result from future observations and planning to find actions that result in high rewards as quickly as can be managed given the current posterior. However, this is computationally expensive even in small finite MDPs and totally intractable in continuous settings. Kolter and Ng  Environment   TIP  sTIP  DIP  sDIP  MPC  PETS  SAC  TD3  PPO  FEEF  HUCRL  TS  BPTT  BARL  EIG T   Pendulum  21  36  36  46  46  5.6k  7k  26k  14k  800  &gt;50k  &gt;50k  &gt;50k  21  56  Cartpole  131  141  161  141  201  1.63k  32k  18k  &gt;1M  &gt;2.5k  &gt;6k  &gt;6k  &gt;6k  111  121  β Tracking  46  76  276  131  76  330  12k  17k  39k  300  480  420  450  186  &gt;1k  β + Rotation  201  &gt;500  &gt;500  &gt;500  &gt;500  400  30k  &gt;50k  &gt;50k  &gt;2k  &gt;5k  &gt;5k  &gt;5k  &gt;500  &gt;1k  Reacher  251  &gt;400  &gt;1k  &gt;1k  751  700  23k  13k  &gt;100k  &gt;5k 6.6k 4.5k 3.7k 251 &gt;1.5k Table 6: Sample Complexity Comparison of All Methods: Median number of samples across 5 seeds required to reach 'solved' performance, averaged across 5 trials. We determine 'solved' performance by running an MPC policy (similar to the one used for evaluation) on the ground truth dynamics to predict actions. We record &gt; n when the median run is unable to solve the problem by the end of training after collecting n datapoints. The methods in the rightmost section operate in the TQRL setting and therefore have more flexible access to the MDP dynamics for data collection.  show for a given training run how many samples were needed to achieve the performance of an MPC controller given ground truth dynamics averaged across test episodes. We imputed the maximum number of samples for agents that failed to ever solve the problem on a given run.</p>
<p>[35] and Guez et al. [26] show that even approximating these techniques can result in substantial theoretical reductions in sample complexity compared to frequentist PAC-MDP bounds as in Kakade [33]. Another line of work [18,19] uses the myopic value of perfect information as a heuristic for similar Bayesian exploration in the tabular MDP setting. Further techniques for exploration include knowledge gradient policies [57,56], which approximate the value function of the Bayes-adaptive MDP and information-directed sampling (IDS) [54], which takes actions based on minimizing the ratio between squared regret and information gain over dynamics. This was extended to continuousstate finite-action settings using neural networks in Nikolov et al. [44]. Another very relevant recent paper [7] gives an acquisition strategy in policy space that iteratively trains a data-collection policy in the model that trades off exploration against exploitation using methods from active learning. Achterhold and Stueckler [3] use techniques from BOED to efficiently calibrate a Neural Process representation of a distribution of dynamics to a particular instance, but this calibration doesn't include information about the task. A tutorial on Bayesian RL methods can be found in Ghavamzadeh et al. [24] for further reference.</p>
<p>E.2 Gaussian Processes (GPs) in Reinforcement Learning</p>
<p>There has been substantial prior work using GPs [52] in reinforcement learning. Most well-known is PILCO [21], which computes approximate analytic gradients of policy parameters through the GP dynamics model while accounting for uncertainty. The original work is able to propagate the first 2 moments of the occupancy distribution through time using the GP dynamics and backpropagate gradients of the rewards to policy parameters. In [69], a method is developed for efficiently sampling functions from a GP posterior with high accuracy. One application show in their work is a method of using these samples to backpropagate gradients of rewards through time to policy paramters,</p>
<p>Figure 1 :
1A schematic depiction of Trajectory Information Planning (TIP). Suppose the agent in (a) aims to</p>
<p>Compute the joint posterior covariance Σ S | D across all points in X. Compute the joint posterior covariances Σ S ij | D ∪ τ ij ∀i ∈ [n], j ∈ [m] across all points in X. return log |Σ S | − 1 nm i∈[n],j∈[m] log |Σ S ij |.</p>
<p>Figure 2 :
2Our comparison methods can be broken down by the type of cost function used and how the methods do or do not handle sequential acquisition of information. As Cg is a sum, it naturally handles future timesteps jointly. For the other information quantities, it is possible to upper-bound information acquired by summing each separate mutual information, or to compute them jointly.</p>
<p>Figure 3 :
3Control and Modeling Details for TIP and Ablations. Column 1: Learning curves for our ablation methods, all of which use the same planner and model. Column 2: Dynamics model accuracy on the points used by the planner to choose actions during MPC. Column 3: Dynamics model accuracy on a uniformly random test set inS. Column 4: EIGτ * values normalized by the number of actions planned. sTIP was truncated on Reacher as it exceeded the wall time budget.</p>
<p>m]: O(nmN ) total cost from sampling algorithm, where N is the dataset size. • Sample τ * ij for i ∈ [n], j ∈ [m]: phHnm total cost from running the planner (ph posterior function queries) H times for each sampled τ * , where H is the MDP horizon. • Compute Cholesky decomposition for each D ∪ τ * ij . This takes a total of O(nm(N + H) 3 ) operations as the augmented dataset is of size N + H and Cholesky decompositions are O(d 3 ) in the matrix size d. • Compute posterior covariance Σ S | D ∪ τ ij for all τ ij . This involves several matrix operations but the most computationally intensive is solving h triangular systems of size (N + H) × (N + H), which each take O((N + H) 2 ) time. So the total computation here is O(pnmh(N + H) 2 ). • Compute determinants of covariance matrices for each of p queries and nm augmented datasets D ∪ τ ij . Each of these operations is over a matrix of size h × h and therefore costs O(h 3 ). So the total cost is O(pnmh 3 ). Summing these costs gives O(nmN + phHnm + nm(N + H) 3 + pnmh(N + H) 2 + pnmh 3 ). Clearly the third term dominates the first, the fourth dominates the second, and since H &gt; h, the fourth dominates the fifth. So, the computational cost can be summarized as O nm (N + H) 3 + ph(N + H) 2 .</p>
<p>compare against 14 different methods across open and closed-loop problems. Of these, 7 used the same model and planning algorithm (including hyperparameters) as TIP and oTIP. DIP and oDIP use the cost function C(τ ) = −H [T (S ) | D] and sDIP (summed DIP) uses the cost functionC(τ ) = − h i=0 H [T (s i , a i ) | D].These are all pure exploration methods, but DIP and oDIP are more sophisticated in that they plan for future observations with a large amount of joint information</p>
<p>Figure 4 :
4Box plots showing sample complexity figures across the 5 random seeds run. Each of these</p>
<p>query set X, number of start state samples m, number of posterior function samples n. Sample m start states {s</p>
<p>We evaluate the open-loop variant of our method, oTIP, against three comparison methods on three control problems suitable for open-loop control. In particular, to be suitable for open-loop control, the problem cannot be dynamically unstable (as Pendulum and Cartpole famously are) and must have a relatively short control horizon and fixed start state.Environment TIP sTIP </p>
<p>DIP MPC PETS SAC FEEF RHC HUCRL 
TS 
BARL EIG T </p>
<p>Pendulum 
21 
36 
36 
46 
5.6k 
7k 
800 </p>
<blockquote>
<p>40k 
50k 
50k 
21 
56 
Cartpole 
131 141 
161 
201 
1.63k 32k &gt;2.5k &gt;5k 
6k 
6k 
111 
121 
β Tracking 
46 
76 
276 
76 
330 
12k 
300 
3k 
480 
420 
186 
1k 
β + Rotation 201 &gt;500 &gt;500 &gt;500 
400 
30k 
2k 
2k 
5k 
5k 
500 
1k 
Reacher 
251 &gt;400 &gt;1k 
751 
700 
23k 
5k 
1.5k 
6.6k 
4.5k 
251 
1.5k </p>
</blockquote>
<p>Table 2 :
2Open Loop Sample Complexity: Median number of samples required to reach 'solved' performance, averaged across five trials. We determine 'solved' performance by running an MPC policy on the ground truth dynamics to predict actions. We record &gt; n when the median run is unable to solve the problem by the end of training after collecting n datapoints.</p>
<p>Table 4 :
4Hyperparameters used for optimization in MPC procedure for closed-loop control problems.</p>
<p>Table 5 :
5Hyperparameters used for optimization in MPC procedure for open-loop control problems.
Code is available at: https://github.com/fusion-ml/trajectory-information-rl 36th Conference on Neural Information Processing Systems (NeurIPS 2022).
AcknowledgementsThis work was supported in part by US Department of Energy grants under contract numbers DE-SC0021414 and DE-AC02-09CH1146.
TensorFlow: Largescale machine learning on heterogeneous systems. Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang ZhengRajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya SutskeverDandelion ManéMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large- scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow. org/. Software available from tensorflow.org.</p>
<p>Data-driven profile prediction for diii-d. Joseph Abbate, E Conlin, Kolemen, Nuclear Fusion. 61446027Joseph Abbate, R Conlin, and E Kolemen. Data-driven profile prediction for diii-d. Nuclear Fusion, 61(4):046027, 2021.</p>
<p>Explore the context: Optimal data collection for contextconditional dynamics models. Jan Achterhold, Joerg Stueckler, PMLRProceedings of The 24th International Conference on Artificial Intelligence and Statistics. Arindam Banerjee and Kenji FukumizuThe 24th International Conference on Artificial Intelligence and Statistics130Jan Achterhold and Joerg Stueckler. Explore the context: Optimal data collection for context- conditional dynamics models. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3529-3537. PMLR, 13-15 Apr 2021. URL https://proceedings.mlr.press/v130/achterhold21a.html.</p>
<p>Model-based reinforcement learning with a generative model is minimax optimal. Alekh Agarwal, Sham Kakade, Lin F Yang, PMLRProceedings of Thirty Third Conference on Learning Theory. Jacob Abernethy and Shivani AgarwalThirty Third Conference on Learning Theory125Alekh Agarwal, Sham Kakade, and Lin F. Yang. Model-based reinforcement learning with a generative model is minimax optimal. In Jacob Abernethy and Shivani Agarwal, ed- itors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Pro- ceedings of Machine Learning Research, pages 67-83. PMLR, 09-12 Jul 2020. URL https://proceedings.mlr.press/v125/agarwal20b.html.</p>
<p>Anticoncentrated confidence bonuses for scalable exploration. Jordan T Ash, Cyril Zhang, Surbhi Goel, Akshay Krishnamurthy, Sham M Kakade, International Conference on Learning Representations. Jordan T. Ash, Cyril Zhang, Surbhi Goel, Akshay Krishnamurthy, and Sham M. Kakade. Anti- concentrated confidence bonuses for scalable exploration. In International Conference on Learn- ing Representations, 2022. URL https://openreview.net/forum?id=RXQ-FPbQYVn.</p>
<p>Minimax pac bounds on the sample complexity of reinforcement learning with a generative model. Rémi Mohammad Gheshlaghi Azar, Hilbert J Munos, Kappen, Machine learning. 913Mohammad Gheshlaghi Azar, Rémi Munos, and Hilbert J Kappen. Minimax pac bounds on the sample complexity of reinforcement learning with a generative model. Machine learning, 91 (3):325-349, 2013.</p>
<p>Ready policy one: World building through active learning. Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, Stephen Roberts, PMLRProceedings of the 37th International Conference on Machine Learning. Hal Daumé III and Aarti Singhthe 37th International Conference on Machine Learning119Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts. Ready policy one: World building through active learning. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 591-601. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/ball20a.html.</p>
<p>Stabilization of external modes in tokamaks by resistive walls and plasma rotation. A Bondeson, D J Ward, Physical Review Letters. 7217A. Bondeson and D.J. Ward. Stabilization of external modes in tokamaks by resistive walls and plasma rotation. Physical Review Letters, 72(17):2709-2712, 1994.</p>
<p>JAX: composable transformations of Python+NumPy programs. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake Vanderplas, Skye Wanderman-Milne, Qiao Zhang, James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.</p>
<p>Openai gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.</p>
<p>Exploration by random network distillation. Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, International Conference on Learning Representations. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1lJJnR5Ym.</p>
<p>Bayesian experimental design: A review. Kathryn Chaloner, Isabella Verdinelli, Statistical Science. Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical Science, pages 273-304, 1995.</p>
<p>Offline contextual bayesian optimization. Ian Char, Youngseog Chung, Willie Neiswanger, Kirthevasan Kandasamy, O Andrew, Mark Nelson, Egemen Boyer, Jeff Kolemen, Schneider, Advances in Neural Information Processing Systems. 32Ian Char, Youngseog Chung, Willie Neiswanger, Kirthevasan Kandasamy, Andrew O Nelson, Mark Boyer, Egemen Kolemen, and Jeff Schneider. Offline contextual bayesian optimization. Advances in Neural Information Processing Systems, 32:4627-4638, 2019.</p>
<p>UCB and infogain exploration via q-ensembles. CoRR, abs/1706.01502. Richard Y Chen, Szymon Sidor, Pieter Abbeel, John Schulman, Richard Y. Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. UCB and infogain exploration via q-ensembles. CoRR, abs/1706.01502, 2017. URL http://arxiv.org/abs/ 1706.01502.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep re- inforcement learning in a handful of trials using probabilistic dynamics models. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar- nett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ 3de568f8597b94bda53149c7d7f5958c-Paper.pdf.</p>
<p>Efficient modelbased reinforcement learning through optimistic policy search and planning. Sebastian Curi, Felix Berkenkamp, Andreas Krause, NeurIPS, 2020. Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efficient model- based reinforcement learning through optimistic policy search and planning. In NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ a36b598abb934e4528412e5a2127b931-Abstract.html.</p>
<p>A tutorial on the cross-entropy method. Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, Reuven Y Rubinstein, Annals of operations research. 1341Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of operations research, 134(1):19-67, 2005.</p>
<p>Bayesian q-learning. Richard Dearden, Nir Friedman, Stuart Russell, Aaai/iaai. Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. In Aaai/iaai, pages 761-768, 1998.</p>
<p>Model-based bayesian exploration. CoRR, abs/1301. Richard Dearden, Nir Friedman, David Andre, 6690Richard Dearden, Nir Friedman, and David Andre. Model-based bayesian exploration. CoRR, abs/1301.6690, 1999. URL http://arxiv.org/abs/1301.6690.</p>
<p>Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Nature. 6027897Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897): 414-419, 2022.</p>
<p>Pilco: A model-based and data-efficient approach to policy search. Marc Peter Deisenroth, Carl Edward Rasmussen, Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML'11. the 28th International Conference on International Conference on Machine Learning, ICML'11Madison, WI, USA9781450306195Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on Interna- tional Conference on Machine Learning, ICML'11, page 465-472, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.</p>
<p>Diii-d research advancing the physics basis for optimizing the tokamak approach to fusion energy. E Max, J Fenstermacher, Abbate, Abe, M Abrams, Adams, N Adamson, Aiba, P Akiyama, Aleynikov, Allen, Nuclear Fusion. 62442024Max E Fenstermacher, J Abbate, S Abe, T Abrams, M Adams, B Adamson, N Aiba, T Akiyama, P Aleynikov, E Allen, et al. Diii-d research advancing the physics basis for optimizing the tokamak approach to fusion energy. Nuclear Fusion, 62(4):042024, 2022.</p>
<p>Addressing function approximation error in actor-critic methods. Scott Fujimoto, David Herke Van Hoof, Meger, PMLRProceedings of the 35th International Conference on Machine Learning. Jennifer Dy and Andreas Krausethe 35th International Conference on Machine Learning80Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1587-1596. PMLR, 10-15 Jul 2018. URL https://proceedings. mlr.press/v80/fujimoto18a.html.</p>
<p>Bayesian reinforcement learning: A survey. CoRR, abs/1609.04436. Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforce- ment learning: A survey. CoRR, abs/1609.04436, 2016. URL http://arxiv.org/abs/1609. 04436.</p>
<p>Role of edge electric field and poloidal rotation in the l-h transition. R J Groebner, K H Burrell, R P Seraydarian, 10.1103/PhysRevLett.64.3015Physical Review Letters. 6425R. J. Groebner, K. H. Burrell, and R. P. Seraydarian. Role of edge electric field and poloidal rotation in the l-h transition. Physical Review Letters, 64(25):3015-3018, 1990. ISSN 00319007. doi: 10.1103/PhysRevLett.64.3015.</p>
<p>Efficient bayes-adaptive reinforcement learning using sample-based search. Arthur Guez, David Silver, Peter Dayan, Proceedings of the 25th International Conference on Neural Information Processing Systems. the 25th International Conference on Neural Information Processing SystemsRed Hook, NY, USACurran Associates Inc1Arthur Guez, David Silver, and Peter Dayan. Efficient bayes-adaptive reinforcement learning using sample-based search. In Proceedings of the 25th International Conference on Neural Information Processing Systems -Volume 1, NIPS'12, page 1025-1033, Red Hook, NY, USA, 2012. Curran Associates Inc.</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, International conference on machine learning. PMLRTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861-1870. PMLR, 2018.</p>
<p>Array programming with NumPy. Charles R Harris, K Jarrod Millman, J Stéfan, Ralf Van Der Walt, Pauli Gommers, David Virtanen, Eric Cournapeau, Julian Wieser, Sebastian Taylor, Nathaniel J Berg, Robert Smith, Matti Kern, Stephan Picus, Marten H Hoyer, Matthew Van Kerkwijk, Allan Brett, Jaime Haldane, Mark Fernández Del Río, Pearu Wiebe, Pierre Peterson, Kevin Gérard-Marchant, Tyler Sheppard, Warren Reddy, Hameer Weckesser, Christoph Abbasi, Travis E Gohlke, Oliphant, 10.1038/s41586-020-2649-2Nature. 5857825Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Vir- tanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Hal- dane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357-362, September 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.</p>
<p>Entropy search for information-efficient global optimization. Philipp Hennig, J Christian, Schuler, Journal of Machine Learning Research. 136Philipp Hennig and Christian J Schuler. Entropy search for information-efficient global opti- mization. Journal of Machine Learning Research, 13(6), 2012.</p>
<p>Predictive entropy search for efficient global optimization of black-box functions. José Miguel Hernández-Lobato, W Matthew, Zoubin Hoffman, Ghahramani, Advances in neural information processing systems. 27José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. Advances in neural information processing systems, 27, 2014.</p>
<p>Novel aspects of plasma control in iter. David Humphreys, Ambrosino, Federico Peter De Vries, Felici, H Sun, Gary Kim, Jackson, Egemen Kallenbach, Kolemen, Lister, Moreau, Physics of Plasmas. 22221806David Humphreys, G Ambrosino, Peter de Vries, Federico Felici, Sun H Kim, Gary Jackson, A Kallenbach, Egemen Kolemen, J Lister, D Moreau, et al. Novel aspects of plasma control in iter. Physics of Plasmas, 22(2):021806, 2015.</p>
<p>When to trust your model: Model-based policy optimization. Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine, Advances in Neural Information Processing Systems. 32Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>On the sample complexity of reinforcement learning. Sham Machandranath Kakade, University of London, University College London (United KingdomSham Machandranath Kakade. On the sample complexity of reinforcement learning. University of London, University College London (United Kingdom), 2003.</p>
<p>Data-efficient reinforcement learning with probabilistic model predictive control. Sanket Kamthe, Marc Deisenroth, PMLRProceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics. Amos Storkey and Fernando Perez-Cruzthe Twenty-First International Conference on Artificial Intelligence and Statistics84Sanket Kamthe and Marc Deisenroth. Data-efficient reinforcement learning with probabilistic model predictive control. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1701-1710. PMLR, 09-11 Apr 2018. URL https://proceedings.mlr.press/v84/kamthe18a.html.</p>
<p>Near-bayesian exploration in polynomial time. Zico Kolter, Andrew Y Ng, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningJ Zico Kolter and Andrew Y Ng. Near-bayesian exploration in polynomial time. In Proceedings of the 26th annual international conference on machine learning, pages 513-520, 2009.</p>
<p>Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. Kimin Lee, Michael Laskin, Aravind Srinivas, Pieter Abbeel, International Conference on Machine Learning. PMLRKimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. In International Conference on Machine Learning, pages 6131-6141. PMLR, 2021.</p>
<p>P Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Bayesian nonlinear modeling for the prediction competition. J C David, Mackay, ASHRAE transactions. 1002David JC MacKay et al. Bayesian nonlinear modeling for the prediction competition. ASHRAE transactions, 100(2):1053-1062, 1994.</p>
<p>Neural dynamical systems: Balancing structure and flexibility in physical prediction. Viraj Mehta, Ian Char, Willie Neiswanger, Youngseog Chung, Andrew Nelson, Mark Boyer, Egemen Kolemen, Jeff Schneider, 2021 60th IEEE Conference on Decision and Control (CDC). IEEEViraj Mehta, Ian Char, Willie Neiswanger, Youngseog Chung, Andrew Nelson, Mark Boyer, Egemen Kolemen, and Jeff Schneider. Neural dynamical systems: Balancing structure and flexibility in physical prediction. In 2021 60th IEEE Conference on Decision and Control (CDC), pages 3735-3742. IEEE, 2021.</p>
<p>An experimental design perspective on model-based reinforcement learning. Viraj Mehta, Biswajit Paria, Jeff Schneider, Stefano Ermon, Willie Neiswanger, International Conference on Learning Representations. Viraj Mehta, Biswajit Paria, Jeff Schneider, Stefano Ermon, and Willie Neiswanger. An experimental design perspective on model-based reinforcement learning. In International Conference on Learning Representations, 2022.</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, arXiv:1312.5602arXiv preprintVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</p>
<p>Bayesian Learning for Neural Networks. M Radford, Neal, University of TorontoPhD thesisRadford M Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.</p>
<p>Bayesian algorithm execution: Estimating computable properties of black-box functions using mutual information. Willie Neiswanger, Alexander Ke, Stefano Wang, Ermon, PMLRInternational Conference on Machine Learning. Willie Neiswanger, Ke Alexander Wang, and Stefano Ermon. Bayesian algorithm execu- tion: Estimating computable properties of black-box functions using mutual information. In International Conference on Machine Learning. PMLR, 2021.</p>
<p>Informationdirected exploration for deep reinforcement learning. Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, Andreas Krause, International Conference on Learning Representations. Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Information- directed exploration for deep reinforcement learning. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Byx83s09Km.</p>
<p>Deep exploration via bootstrapped dqn. Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, Advances in Neural Information Processing Systems. D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. GarnettCurran Associates, Inc29Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep explo- ration via bootstrapped dqn. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Gar- nett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/ 8d8818c8e140c64c743113f563cf750f-Paper.pdf.</p>
<p>Deep exploration via randomized value functions. Ian Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, Journal of Machine Learning Research. 20124Ian Osband, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. Deep exploration via randomized value functions. Journal of Machine Learning Research, 20(124):1-62, 2019. URL http://jmlr.org/papers/v20/18-339.html.</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, International conference on machine learning. PMLRDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778- 2787. PMLR, 2017.</p>
<p>Self-supervised exploration via disagreement. Deepak Pathak, Dhiraj Gandhi, Abhinav Gupta, PMLRProceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning97Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via dis- agreement. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5062-5071. PMLR, 09-15 Jun 2019. URL https: //proceedings.mlr.press/v97/pathak19a.html.</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.</p>
<p>Mbrllib: A modular library for model-based reinforcement learning. Arxiv. Luis Pineda, Brandon Amos, Amy Zhang, Nathan O Lambert, Roberto Calandra, Luis Pineda, Brandon Amos, Amy Zhang, Nathan O. Lambert, and Roberto Calandra. Mbrl- lib: A modular library for model-based reinforcement learning. Arxiv, 2021. URL https: //arxiv.org/abs/2104.10159.</p>
<p>Sample-efficient cross-entropy method for real-time planning. Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, arXiv:2008.06389arXiv preprintJoerg Stueckler, Michal Rolinek, and Georg MartiusCristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal Rolinek, and Georg Martius. Sample-efficient cross-entropy method for real-time planning. arXiv preprint arXiv:2008.06389, 2020.</p>
<p>Gaussian processes in machine learning. Carl Edward Rasmussen, Summer school on machine learning. SpringerCarl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on machine learning, pages 63-71. Springer, 2003.</p>
<p>Bayes-adaptive pomdps. Stephane Ross, Brahim Chaib-Draa, Joelle Pineau, NIPS. Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive pomdps. In NIPS, pages 1225-1232, 2007.</p>
<p>Learning to optimize via information-directed sampling. Daniel Russo, Benjamin Van Roy, Advances in Neural Information Processing Systems. Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. WeinbergerCurran Associates, Inc27Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Wein- berger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/ 301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf.</p>
<p>A tutorial on thompson sampling. Foundations and Trends® in Machine Learning. J Daniel, Benjamin Russo, Abbas Van Roy, Ian Kazerouni, Zheng Osband, Wen, 11Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on thompson sampling. Foundations and Trends® in Machine Learning, 11(1):1-96, 2018.</p>
<p>Information collection on a graph. O Ilya, Warren B Ryzhov, Powell, Operations Research. 591Ilya O Ryzhov and Warren B Powell. Information collection on a graph. Operations Research, 59(1):188-201, 2011.</p>
<p>Bayesian exploration for approximate dynamic programming. Ilya O Ryzhov, R K Martijn, Mes, B Warren, Gerald Powell, Van Den, Berg, Operations research. 671Ilya O Ryzhov, Martijn RK Mes, Warren B Powell, and Gerald van den Berg. Bayesian exploration for approximate dynamic programming. Operations research, 67(1):198-214, 2019.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Receding horizon curiosity. Matthias Schultheis, Boris Belousov, Hany Abdulsamad, Jan Peters, Conference on robot learning. PMLRMatthias Schultheis, Boris Belousov, Hany Abdulsamad, and Jan Peters. Receding horizon curiosity. In Conference on robot learning, pages 1278-1288. PMLR, 2020.</p>
<p>Planning to explore via self-supervised world models. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak, International Conference on Machine Learning. PMLRRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pages 8583-8592. PMLR, 2020.</p>
<p>Model-based active exploration. Pranav Shyam, Wojciech Jaśkowski, Faustino Gomez, PMLR, 09-15Proceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning97Pranav Shyam, Wojciech Jaśkowski, and Faustino Gomez. Model-based active exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5779-5788. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/ shyam19a.html.</p>
<p>Mdsplus data acquisition system. Ja Stillerman, Tw Fredian, G Ka Klare, Manduchi, Review of Scientific Instruments. 681JA Stillerman, TW Fredian, KA Klare, and G Manduchi. Mdsplus data acquisition system. Review of Scientific Instruments, 68(1):939-942, 1997.</p>
<p>A bayesian framework for reinforcement learning. Malcolm Strens, ICML. Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, volume 2000, pages 943-950, 2000.</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, ISBN 0-262-19398-1MIT PressCambridge, MA, USARichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, USA, 1998. ISBN 0-262-19398-1. URL http://www.cs.ualberta.ca/ %7Esutton/book/ebook/the-book.html.</p>
<p>Expensive function optimization with stochastic binary outcomes. Matthew Tesch, Jeff Schneider, Howie Choset, Proceedings of the 30th International Conference on Machine Learning. Sanjoy Dasgupta and David McAllesterthe 30th International Conference on Machine LearningAtlanta, Georgia, USA28Matthew Tesch, Jeff Schneider, and Howie Choset. Expensive function optimization with stochastic binary outcomes. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 1283-1291, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/tesch13.html.</p>
<p>End-to-end model-free reinforcement learning for urban driving using implicit affordances. Marin Toromanoff, Emilie Wirbel, Fabien Moutarde, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionMarin Toromanoff, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free reinforce- ment learning for urban driving using implicit affordances. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7153-7162, 2020.</p>
<p>Alexander Tschantz, Beren Millidge, K Anil, Christopher L Seth, Buckley, arXiv:2002.12636Reinforcement learning through active inference. arXiv preprintAlexander Tschantz, Beren Millidge, Anil K Seth, and Christopher L Buckley. Reinforcement learning through active inference. arXiv preprint arXiv:2002.12636, 2020.</p>
<p>Gaussian processes for regression. K I Christopher, Carl Edward Williams, Rasmussen, Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for regression. 1996.</p>
<p>Efficiently sampling functions from gaussian process posteriors. James Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, Marc Deisenroth, International Conference on Machine Learning. PMLRJames Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Deisen- roth. Efficiently sampling functions from gaussian process posteriors. In International Confer- ence on Machine Learning, pages 10292-10302. PMLR, 2020.</p>
<p>which can be interpreted as a different sort of PILCO implementation. Most related to our eventual MPC-based method is [34], which gives a principled probabilistic model-predictive control algorithm for GPs. Zhenpeng Zhou, Steven Kearnes, Li Li, Patrick Richard N Zare, Riley, Scientific reports. 91Optimization of molecules via deep reinforcement learning. We combine ideas from this paper, PETS [15], and the ability to sample posterior functions discussed above to give our eventual MPC component as discussed in Section 4.1Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of molecules via deep reinforcement learning. Scientific reports, 9(1):1-10, 2019. which can be interpreted as a different sort of PILCO implementation. Most related to our eventual MPC-based method is [34], which gives a principled probabilistic model-predictive control algorithm for GPs. We combine ideas from this paper, PETS [15], and the ability to sample posterior functions discussed above to give our eventual MPC component as discussed in Section 4.1.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8649 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8649</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8649</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-276855039</p>
                <p><strong>Paper Title:</strong> Foundation models for materials discovery – current state and future directions</p>
                <p><strong>Paper Abstract:</strong> Large language models, commonly known as LLMs, are showing promise in tacking some of the most complex tasks in AI. In this perspective, we review the wider field of foundation models—of which LLMs are a component—and their application to the field of materials discovery. In addition to the current state of the art—including applications to property prediction, synthesis planning and molecular generation—we also take a look to the future, and posit how new methods of data capture, and indeed modalities of data, will influence the direction of this emerging field.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8649.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8649.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT: molecular generation using a transformer-decoder model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-decoder model that generates molecular SMILES strings autoregressively to propose novel chemical structures; presented as an application of GPT-style decoders to molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolGPT: molecular generation using a transformer-decoder model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-decoder (GPT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this perspective; typically trained on large SMILES corpora such as ZINC/ChEMBL/PubChem (textual molecular datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct autoregressive generation of SMILES strings from a decoder model (conditional or unconditional sampling possible).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Perspective reports MolGPT as a generative approach; specific novelty metrics (e.g., percentage outside training set, Tanimoto similarity) are not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Can be conditioned via prompts or fine-tuning to bias generation toward desired properties or tasks, but specific conditioning protocols are not detailed in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this perspective for MolGPT; typical metrics include validity, uniqueness, novelty, drug-likeness, and property scores (mentioned generically elsewhere in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of decoder-only transformer models applied to molecular generation; no experimental outcomes or quantitative results are reported in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned among transformer-based generative approaches; no direct quantitative comparison provided in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Noted limitations applicable to textual SMILES-based generators: loss of 3D information, potential for generating syntactically valid but chemically invalid or non-synthesizable molecules, and dataset biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundation models for materials discovery – current state and future directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8649.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8649.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regression Transformer (RT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A molecular language model that performs simultaneous sequence generation and regression (property prediction), enabling conditional generation of molecules with targeted properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regression transformer enables concurrent sequence regression and generation for molecular language modelling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regression Transformer (RT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based language model (multi-task: generation + regression)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in detail here; typically trained on molecular sequence corpora (SMILES/SELFIES) paired with property labels from datasets such as ChEMBL/ZINC and other curated property sets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation for drug discovery and materials design (property-driven design).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Concurrent sequence regression and conditional generation: the model can be prompted/conditioned on desired property values to generate molecules that aim to meet those properties.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Perspective notes RT has enabled the design of novel materials and improved over specialized single-task models in some cases, but specific novelty metrics are not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Generates molecules conditioned on numeric property targets (sequence regression signal) to bias generation toward specific application-driven properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Mentioned metrics for generative tasks broadly in the paper: property fulfillment, diversity, drug-likeness, synthesizability; RT is highlighted for concurrent regression/generation capability rather than specific benchmark numbers in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Described as successfully adopted to solve multimodal tasks and to accelerate design of novel materials, showing superior performance to specialized single-task models in cited works; quantitative outcomes are not given in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Reported to outperform specialized single-task models in some multimodal/multi-task settings according to the perspective's survey of the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>General challenges apply: dataset biases, the need for high-quality labeled property data, limited 3D/structural information when using 1D representations, and potential transferability issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundation models for materials discovery – current state and future directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8649.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8649.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Transformer: a model for uncertainty-calibrated chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence transformer model that frames reaction prediction as machine translation from reactants+reagents to products, providing uncertainty-calibrated predictions used in retrosynthesis and synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Sequence-to-sequence transformer (encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on reaction-annotated textual datasets; specific datasets are not enumerated in this perspective but are reaction corpora derived from literature and patent datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Retrosynthesis planning and organic synthesis prediction; supports synthesis planning for molecule design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Treats reaction prediction as translation; used for retrosynthetic suggestions and planning (decoder generates product or precursor SMILES sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Used primarily for predicting reaction outcomes and retrosynthesis rather than de novo molecule novelty metrics; perspective cites it as enabling improved retrosynthesis accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enables targeted retrosynthetic planning for specified target molecules; can be steered via prompts or disconnection prompts (cited elsewhere in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Retrosynthesis/reaction prediction accuracy, uncertainty calibration; exact numbers are not reproduced in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Presented as a milestone in applying language models to chemical reaction prediction and retrosynthesis; supports downstream synthesis of designed molecules but specific generation outcomes are not provided in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Framed as state-of-the-art at its introduction compared to earlier rule-based or template-based approaches for reaction prediction; no head-to-head benchmarks are presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Biases from training corpora, limited experimental detail in literature-derived recipes, and the general gap between predicted retrosynthesis and practical laboratory execution are discussed as challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundation models for materials discovery – current state and future directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8649.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8649.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cogmol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cogmol: target-specific and selective drug design for COVID-19 using deep generative models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep generative-model-based pipeline for target-specific molecule design applied to COVID-19 targets, demonstrating targeted de novo design using generative latent-space sampling and property optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cogmol: target-specific and selective drug design for covid-19 using deep generative models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cogmol (deep generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational autoencoder / latent-space generative models (and related deep generative architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not fully specified in this perspective; typical training corpora include SMILES/graph molecular datasets plus target-related bioactivity labels.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Therapeutic drug discovery (COVID-19 targets demonstrated).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Latent-space generation with property-guided optimization to produce target-specific molecules (conditional generation and optimization in latent space).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported application to produce candidate molecules for COVID-19 target proteins; specific novelty metrics are not provided here but the work was presented as producing actionable candidate compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to be target-specific and selective, using target-binding or predicted activity as conditioning/optimization criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Typical metrics include predicted binding/activity, selectivity, in silico docking/ADMET proxies; the perspective does not report exact values.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example where generative models produced target-directed candidates; some generated candidates were validated in silico (and similar works have progressed to in vitro validation), but explicit outcomes are not enumerated in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Shown as part of the wave of generative-model-based drug design efforts; no explicit numerical comparisons are included here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>General generative-model challenges: training-data bias, validation gap from in silico to in vitro, synthesizability concerns, and potential mode collapse or lack of chemical validity without appropriate constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundation models for materials discovery – current state and future directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8649.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8649.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DDR1 kinase inhibitor generative work (Zhavoronkov et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep learning enables rapid identification of potent DDR1 kinase inhibitors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early demonstration where deep generative models were used to propose kinase inhibitor candidates, some of which were validated experimentally, showing accelerated hit identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning enables rapid identification of potent ddr1 kinase inhibitors</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deep generative models (specific architectures vary across the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Deep generative architectures (RNNs/VAEs/GANs/transformers depending on study)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified here; typically public bioactivity datasets (e.g., ChEMBL), proprietary assay data and SMILES corpora are used for these workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — kinase inhibitor design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>De novo molecular generation guided by property predictors and scoring functions, followed by in silico filtering and experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Perspective cites this work as producing potent inhibitors validated experimentally; specific novelty/uniqueness metrics are not provided in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed specifically for DDR1 kinase activity, optimized against activity and potency criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>In the cited work, potency (IC50), binding assays, and in vitro validation were used; the perspective does not reproduce numerical results.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced as a successful example where generative models accelerated discovery and led to experimentally validated hits for a kinase target.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Highlighted as achieving rapid identification of active compounds compared to traditional discovery timelines; specific comparative numbers are not included here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Generalization limits, dependence on quality of bioactivity data, and synthesis/ADMET follow-up challenges are implied in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundation models for materials discovery – current state and future directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8649.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8649.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaccMannRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaccMannRL: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning-driven generative pipeline that uses transcriptomic signatures to bias de novo molecular generation toward compounds predicted to elicit desired cellular responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaccMannRL: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaccMannRL</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Reinforcement learning on top of generative molecular models (e.g., RNNs/sequence models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in the perspective; typically uses molecular corpora plus transcriptomic datasets linking compounds to gene-expression profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — anticancer molecule generation tailored to transcriptomic signatures</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Reinforcement learning to optimize generated molecules for predicted transcriptomic impact (reward based on modelled cellular response).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Perspective references this as an example of condition-specific generation; concrete novelty metrics are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioning on transcriptomic signatures provides application-specificity toward eliciting desired cellular responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reward-based optimization metrics, in silico predicted activity, and similarity to hit-like molecules are typical; explicit values are not given in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of RL applied to condition-driven molecule generation (anticancer use case); perspective does not give quantitative outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented among other data-driven generative approaches; no head-to-head numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Depends on the fidelity of transcriptomic predictive models, transferability to real biological systems, and synthesizability of generated compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundation models for materials discovery – current state and future directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8649.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8649.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-chem T5 / Multitask chemical foundation models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-chem T5 and related multi-task chemical-language foundation models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multitask transformer language models trained across chemical and natural language tasks to enable cross-domain transfer for tasks such as property prediction, retrosynthesis and generative chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text-chem T5 (example of a chemical multimodal/multitask foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Encoder-decoder transformer (T5-like multi-task language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on large-scale chemical corpora and natural language text; paper cites chemical datasets like ChEMBL/ZINC and broad text corpora as typical sources.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation, property prediction, retrosynthesis, and other cross-domain chemical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multi-task prompted training and instruction tuning enabling generation via text-conditioned prompts and cross-domain conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Perspective states such multi-task foundation models have recently shown promising results in multi-task generative settings, but does not provide concrete novelty metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Cross-domain training allows conditioning on natural-language descriptions and property targets to generate application-tailored molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Described improvements in downstream task performance (property prediction and generation) relative to single-task models; exact metrics and numbers are not supplied in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Text-chem T5 and similar multi-task models are cited as demonstrating promising performance and improved generalization across chemical tasks compared to specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Perspective claims multi-task foundation models can outperform specialized single-task models in some situations, reducing development time and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Challenges include need for very large and high-quality multimodal datasets, domain bias from dominant chemical subspaces, and integration of non-text modalities (3D/experiment data).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundation models for materials discovery – current state and future directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8649.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8649.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>15B-parameter retrosynthesis & generative chemistry model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large foundational model (15 billion parameters) trained for retrosynthesis prediction and generative chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale foundation model trained for retrosynthesis prediction and generative chemistry tasks; reported in the literature as a 15-billion-parameter model targeted at synthetic planning and molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unnamed 15B-parameter foundational chemistry model (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large-scale foundation model (likely transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>15B parameters (reported in the perspective)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in detail in this perspective; implied to be a mixture of chemical tasks and natural-language data suitable for retrosynthesis and generation (reaction corpora, molecular datasets, text).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Retrosynthesis prediction and generative chemistry (organic synthesis planning and molecule generation).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based and fine-tuning approaches for retrosynthetic planning and conditional molecule generation at foundation-model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Perspective notes the model as an advance toward foundation-scale generative chemistry but does not provide metrics of chemical novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>A large-scale multi-task model intended to support diverse chemistry tasks including targeted retrosynthesis and generation; methods to ensure application specificity are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in the perspective; likely includes retrosynthesis accuracy, route plausibility, and generation property metrics in the original cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of scaling foundation models into chemistry and retrosynthesis; specific outcomes and benchmarks are not reproduced in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Framed as a scaling-up step beyond smaller, task-specific models; perspective suggests foundation-scale models can match or exceed specialized approaches while reducing development cost/time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Large model scale raises concerns about data biases, the need for diverse high-quality multimodal datasets, reproducibility of literature-extracted synthesis steps, and practical validation in the laboratory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundation models for materials discovery – current state and future directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MolGPT: molecular generation using a transformer-decoder model <em>(Rating: 2)</em></li>
                <li>Regression transformer enables concurrent sequence regression and generation for molecular language modelling <em>(Rating: 2)</em></li>
                <li>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>Cogmol: target-specific and selective drug design for covid-19 using deep generative models <em>(Rating: 2)</em></li>
                <li>Deep learning enables rapid identification of potent ddr1 kinase inhibitors <em>(Rating: 2)</em></li>
                <li>PaccMannRL: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning <em>(Rating: 2)</em></li>
                <li>MolXPT: wrapping molecules with text for generative pre-training <em>(Rating: 1)</em></li>
                <li>Generative pre-training from molecules <em>(Rating: 1)</em></li>
                <li>Generative design of stable semiconductor materials using deep learning and density functional theory <em>(Rating: 1)</em></li>
                <li>Text-chem T5 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8649",
    "paper_id": "paper-276855039",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT: molecular generation using a transformer-decoder model",
            "brief_description": "A transformer-decoder model that generates molecular SMILES strings autoregressively to propose novel chemical structures; presented as an application of GPT-style decoders to molecular generation.",
            "citation_title": "MolGPT: molecular generation using a transformer-decoder model",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "transformer-decoder (GPT-style)",
            "model_size": null,
            "training_data": "Not specified in this perspective; typically trained on large SMILES corpora such as ZINC/ChEMBL/PubChem (textual molecular datasets).",
            "application_domain": "Drug discovery / molecular design",
            "generation_method": "Direct autoregressive generation of SMILES strings from a decoder model (conditional or unconditional sampling possible).",
            "novelty_of_chemicals": "Perspective reports MolGPT as a generative approach; specific novelty metrics (e.g., percentage outside training set, Tanimoto similarity) are not reported here.",
            "application_specificity": "Can be conditioned via prompts or fine-tuning to bias generation toward desired properties or tasks, but specific conditioning protocols are not detailed in this perspective.",
            "evaluation_metrics": "Not specified in this perspective for MolGPT; typical metrics include validity, uniqueness, novelty, drug-likeness, and property scores (mentioned generically elsewhere in the paper).",
            "results_summary": "Cited as an example of decoder-only transformer models applied to molecular generation; no experimental outcomes or quantitative results are reported in this perspective.",
            "comparison_to_other_methods": "Positioned among transformer-based generative approaches; no direct quantitative comparison provided in this perspective.",
            "limitations_and_challenges": "Noted limitations applicable to textual SMILES-based generators: loss of 3D information, potential for generating syntactically valid but chemically invalid or non-synthesizable molecules, and dataset biases.",
            "uuid": "e8649.0",
            "source_info": {
                "paper_title": "Foundation models for materials discovery – current state and future directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Regression Transformer (RT)",
            "name_full": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
            "brief_description": "A molecular language model that performs simultaneous sequence generation and regression (property prediction), enabling conditional generation of molecules with targeted properties.",
            "citation_title": "Regression transformer enables concurrent sequence regression and generation for molecular language modelling",
            "mention_or_use": "mention",
            "model_name": "Regression Transformer (RT)",
            "model_type": "Transformer-based language model (multi-task: generation + regression)",
            "model_size": null,
            "training_data": "Not specified in detail here; typically trained on molecular sequence corpora (SMILES/SELFIES) paired with property labels from datasets such as ChEMBL/ZINC and other curated property sets.",
            "application_domain": "Molecular generation for drug discovery and materials design (property-driven design).",
            "generation_method": "Concurrent sequence regression and conditional generation: the model can be prompted/conditioned on desired property values to generate molecules that aim to meet those properties.",
            "novelty_of_chemicals": "Perspective notes RT has enabled the design of novel materials and improved over specialized single-task models in some cases, but specific novelty metrics are not reported here.",
            "application_specificity": "Generates molecules conditioned on numeric property targets (sequence regression signal) to bias generation toward specific application-driven properties.",
            "evaluation_metrics": "Mentioned metrics for generative tasks broadly in the paper: property fulfillment, diversity, drug-likeness, synthesizability; RT is highlighted for concurrent regression/generation capability rather than specific benchmark numbers in this perspective.",
            "results_summary": "Described as successfully adopted to solve multimodal tasks and to accelerate design of novel materials, showing superior performance to specialized single-task models in cited works; quantitative outcomes are not given in this perspective.",
            "comparison_to_other_methods": "Reported to outperform specialized single-task models in some multimodal/multi-task settings according to the perspective's survey of the literature.",
            "limitations_and_challenges": "General challenges apply: dataset biases, the need for high-quality labeled property data, limited 3D/structural information when using 1D representations, and potential transferability issues.",
            "uuid": "e8649.1",
            "source_info": {
                "paper_title": "Foundation models for materials discovery – current state and future directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Molecular Transformer",
            "name_full": "Molecular Transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "brief_description": "A sequence-to-sequence transformer model that frames reaction prediction as machine translation from reactants+reagents to products, providing uncertainty-calibrated predictions used in retrosynthesis and synthesis planning.",
            "citation_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "mention_or_use": "mention",
            "model_name": "Molecular Transformer",
            "model_type": "Sequence-to-sequence transformer (encoder-decoder)",
            "model_size": null,
            "training_data": "Trained on reaction-annotated textual datasets; specific datasets are not enumerated in this perspective but are reaction corpora derived from literature and patent datasets.",
            "application_domain": "Retrosynthesis planning and organic synthesis prediction; supports synthesis planning for molecule design tasks.",
            "generation_method": "Treats reaction prediction as translation; used for retrosynthetic suggestions and planning (decoder generates product or precursor SMILES sequences).",
            "novelty_of_chemicals": "Used primarily for predicting reaction outcomes and retrosynthesis rather than de novo molecule novelty metrics; perspective cites it as enabling improved retrosynthesis accuracy.",
            "application_specificity": "Enables targeted retrosynthetic planning for specified target molecules; can be steered via prompts or disconnection prompts (cited elsewhere in the paper).",
            "evaluation_metrics": "Retrosynthesis/reaction prediction accuracy, uncertainty calibration; exact numbers are not reproduced in this perspective.",
            "results_summary": "Presented as a milestone in applying language models to chemical reaction prediction and retrosynthesis; supports downstream synthesis of designed molecules but specific generation outcomes are not provided in this perspective.",
            "comparison_to_other_methods": "Framed as state-of-the-art at its introduction compared to earlier rule-based or template-based approaches for reaction prediction; no head-to-head benchmarks are presented here.",
            "limitations_and_challenges": "Biases from training corpora, limited experimental detail in literature-derived recipes, and the general gap between predicted retrosynthesis and practical laboratory execution are discussed as challenges.",
            "uuid": "e8649.2",
            "source_info": {
                "paper_title": "Foundation models for materials discovery – current state and future directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Cogmol",
            "name_full": "Cogmol: target-specific and selective drug design for COVID-19 using deep generative models",
            "brief_description": "A deep generative-model-based pipeline for target-specific molecule design applied to COVID-19 targets, demonstrating targeted de novo design using generative latent-space sampling and property optimization.",
            "citation_title": "Cogmol: target-specific and selective drug design for covid-19 using deep generative models",
            "mention_or_use": "mention",
            "model_name": "Cogmol (deep generative models)",
            "model_type": "Variational autoencoder / latent-space generative models (and related deep generative architectures)",
            "model_size": null,
            "training_data": "Not fully specified in this perspective; typical training corpora include SMILES/graph molecular datasets plus target-related bioactivity labels.",
            "application_domain": "Therapeutic drug discovery (COVID-19 targets demonstrated).",
            "generation_method": "Latent-space generation with property-guided optimization to produce target-specific molecules (conditional generation and optimization in latent space).",
            "novelty_of_chemicals": "Reported application to produce candidate molecules for COVID-19 target proteins; specific novelty metrics are not provided here but the work was presented as producing actionable candidate compounds.",
            "application_specificity": "Designed to be target-specific and selective, using target-binding or predicted activity as conditioning/optimization criteria.",
            "evaluation_metrics": "Typical metrics include predicted binding/activity, selectivity, in silico docking/ADMET proxies; the perspective does not report exact values.",
            "results_summary": "Cited as an example where generative models produced target-directed candidates; some generated candidates were validated in silico (and similar works have progressed to in vitro validation), but explicit outcomes are not enumerated in this perspective.",
            "comparison_to_other_methods": "Shown as part of the wave of generative-model-based drug design efforts; no explicit numerical comparisons are included here.",
            "limitations_and_challenges": "General generative-model challenges: training-data bias, validation gap from in silico to in vitro, synthesizability concerns, and potential mode collapse or lack of chemical validity without appropriate constraints.",
            "uuid": "e8649.3",
            "source_info": {
                "paper_title": "Foundation models for materials discovery – current state and future directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DDR1 kinase inhibitor generative work (Zhavoronkov et al.)",
            "name_full": "Deep learning enables rapid identification of potent DDR1 kinase inhibitors",
            "brief_description": "An early demonstration where deep generative models were used to propose kinase inhibitor candidates, some of which were validated experimentally, showing accelerated hit identification.",
            "citation_title": "Deep learning enables rapid identification of potent ddr1 kinase inhibitors",
            "mention_or_use": "mention",
            "model_name": "Deep generative models (specific architectures vary across the cited work)",
            "model_type": "Deep generative architectures (RNNs/VAEs/GANs/transformers depending on study)",
            "model_size": null,
            "training_data": "Not specified here; typically public bioactivity datasets (e.g., ChEMBL), proprietary assay data and SMILES corpora are used for these workflows.",
            "application_domain": "Drug discovery — kinase inhibitor design",
            "generation_method": "De novo molecular generation guided by property predictors and scoring functions, followed by in silico filtering and experimental validation.",
            "novelty_of_chemicals": "Perspective cites this work as producing potent inhibitors validated experimentally; specific novelty/uniqueness metrics are not provided in the perspective.",
            "application_specificity": "Designed specifically for DDR1 kinase activity, optimized against activity and potency criteria.",
            "evaluation_metrics": "In the cited work, potency (IC50), binding assays, and in vitro validation were used; the perspective does not reproduce numerical results.",
            "results_summary": "Referenced as a successful example where generative models accelerated discovery and led to experimentally validated hits for a kinase target.",
            "comparison_to_other_methods": "Highlighted as achieving rapid identification of active compounds compared to traditional discovery timelines; specific comparative numbers are not included here.",
            "limitations_and_challenges": "Generalization limits, dependence on quality of bioactivity data, and synthesis/ADMET follow-up challenges are implied in the perspective.",
            "uuid": "e8649.4",
            "source_info": {
                "paper_title": "Foundation models for materials discovery – current state and future directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "PaccMannRL",
            "name_full": "PaccMannRL: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning",
            "brief_description": "A reinforcement-learning-driven generative pipeline that uses transcriptomic signatures to bias de novo molecular generation toward compounds predicted to elicit desired cellular responses.",
            "citation_title": "PaccMannRL: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "PaccMannRL",
            "model_type": "Reinforcement learning on top of generative molecular models (e.g., RNNs/sequence models)",
            "model_size": null,
            "training_data": "Not specified in the perspective; typically uses molecular corpora plus transcriptomic datasets linking compounds to gene-expression profiles.",
            "application_domain": "Drug discovery — anticancer molecule generation tailored to transcriptomic signatures",
            "generation_method": "Reinforcement learning to optimize generated molecules for predicted transcriptomic impact (reward based on modelled cellular response).",
            "novelty_of_chemicals": "Perspective references this as an example of condition-specific generation; concrete novelty metrics are not provided here.",
            "application_specificity": "Conditioning on transcriptomic signatures provides application-specificity toward eliciting desired cellular responses.",
            "evaluation_metrics": "Reward-based optimization metrics, in silico predicted activity, and similarity to hit-like molecules are typical; explicit values are not given in this perspective.",
            "results_summary": "Cited as an example of RL applied to condition-driven molecule generation (anticancer use case); perspective does not give quantitative outcomes.",
            "comparison_to_other_methods": "Presented among other data-driven generative approaches; no head-to-head numbers provided.",
            "limitations_and_challenges": "Depends on the fidelity of transcriptomic predictive models, transferability to real biological systems, and synthesizability of generated compounds.",
            "uuid": "e8649.5",
            "source_info": {
                "paper_title": "Foundation models for materials discovery – current state and future directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Text-chem T5 / Multitask chemical foundation models",
            "name_full": "Text-chem T5 and related multi-task chemical-language foundation models",
            "brief_description": "Multitask transformer language models trained across chemical and natural language tasks to enable cross-domain transfer for tasks such as property prediction, retrosynthesis and generative chemistry.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Text-chem T5 (example of a chemical multimodal/multitask foundation model)",
            "model_type": "Encoder-decoder transformer (T5-like multi-task language model)",
            "model_size": null,
            "training_data": "Trained on large-scale chemical corpora and natural language text; paper cites chemical datasets like ChEMBL/ZINC and broad text corpora as typical sources.",
            "application_domain": "Molecular generation, property prediction, retrosynthesis, and other cross-domain chemical tasks.",
            "generation_method": "Multi-task prompted training and instruction tuning enabling generation via text-conditioned prompts and cross-domain conditioning.",
            "novelty_of_chemicals": "Perspective states such multi-task foundation models have recently shown promising results in multi-task generative settings, but does not provide concrete novelty metrics.",
            "application_specificity": "Cross-domain training allows conditioning on natural-language descriptions and property targets to generate application-tailored molecules.",
            "evaluation_metrics": "Described improvements in downstream task performance (property prediction and generation) relative to single-task models; exact metrics and numbers are not supplied in the perspective.",
            "results_summary": "Text-chem T5 and similar multi-task models are cited as demonstrating promising performance and improved generalization across chemical tasks compared to specialized models.",
            "comparison_to_other_methods": "Perspective claims multi-task foundation models can outperform specialized single-task models in some situations, reducing development time and cost.",
            "limitations_and_challenges": "Challenges include need for very large and high-quality multimodal datasets, domain bias from dominant chemical subspaces, and integration of non-text modalities (3D/experiment data).",
            "uuid": "e8649.6",
            "source_info": {
                "paper_title": "Foundation models for materials discovery – current state and future directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "15B-parameter retrosynthesis & generative chemistry model",
            "name_full": "Large foundational model (15 billion parameters) trained for retrosynthesis prediction and generative chemistry",
            "brief_description": "A large-scale foundation model trained for retrosynthesis prediction and generative chemistry tasks; reported in the literature as a 15-billion-parameter model targeted at synthetic planning and molecule generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unnamed 15B-parameter foundational chemistry model (as cited)",
            "model_type": "Large-scale foundation model (likely transformer-based)",
            "model_size": "15B parameters (reported in the perspective)",
            "training_data": "Not specified in detail in this perspective; implied to be a mixture of chemical tasks and natural-language data suitable for retrosynthesis and generation (reaction corpora, molecular datasets, text).",
            "application_domain": "Retrosynthesis prediction and generative chemistry (organic synthesis planning and molecule generation).",
            "generation_method": "Prompt-based and fine-tuning approaches for retrosynthetic planning and conditional molecule generation at foundation-model scale.",
            "novelty_of_chemicals": "Perspective notes the model as an advance toward foundation-scale generative chemistry but does not provide metrics of chemical novelty.",
            "application_specificity": "A large-scale multi-task model intended to support diverse chemistry tasks including targeted retrosynthesis and generation; methods to ensure application specificity are not detailed here.",
            "evaluation_metrics": "Not specified in the perspective; likely includes retrosynthesis accuracy, route plausibility, and generation property metrics in the original cited work.",
            "results_summary": "Cited as an example of scaling foundation models into chemistry and retrosynthesis; specific outcomes and benchmarks are not reproduced in this perspective.",
            "comparison_to_other_methods": "Framed as a scaling-up step beyond smaller, task-specific models; perspective suggests foundation-scale models can match or exceed specialized approaches while reducing development cost/time.",
            "limitations_and_challenges": "Large model scale raises concerns about data biases, the need for diverse high-quality multimodal datasets, reproducibility of literature-extracted synthesis steps, and practical validation in the laboratory.",
            "uuid": "e8649.7",
            "source_info": {
                "paper_title": "Foundation models for materials discovery – current state and future directions",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MolGPT: molecular generation using a transformer-decoder model",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "Regression transformer enables concurrent sequence regression and generation for molecular language modelling",
            "rating": 2,
            "sanitized_title": "regression_transformer_enables_concurrent_sequence_regression_and_generation_for_molecular_language_modelling"
        },
        {
            "paper_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "rating": 2,
            "sanitized_title": "molecular_transformer_a_model_for_uncertaintycalibrated_chemical_reaction_prediction"
        },
        {
            "paper_title": "Cogmol: target-specific and selective drug design for covid-19 using deep generative models",
            "rating": 2,
            "sanitized_title": "cogmol_targetspecific_and_selective_drug_design_for_covid19_using_deep_generative_models"
        },
        {
            "paper_title": "Deep learning enables rapid identification of potent ddr1 kinase inhibitors",
            "rating": 2,
            "sanitized_title": "deep_learning_enables_rapid_identification_of_potent_ddr1_kinase_inhibitors"
        },
        {
            "paper_title": "PaccMannRL: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning",
            "rating": 2,
            "sanitized_title": "paccmannrl_de_novo_generation_of_hitlike_anticancer_molecules_from_transcriptomic_data_via_reinforcement_learning"
        },
        {
            "paper_title": "MolXPT: wrapping molecules with text for generative pre-training",
            "rating": 1,
            "sanitized_title": "molxpt_wrapping_molecules_with_text_for_generative_pretraining"
        },
        {
            "paper_title": "Generative pre-training from molecules",
            "rating": 1,
            "sanitized_title": "generative_pretraining_from_molecules"
        },
        {
            "paper_title": "Generative design of stable semiconductor materials using deep learning and density functional theory",
            "rating": 1,
            "sanitized_title": "generative_design_of_stable_semiconductor_materials_using_deep_learning_and_density_functional_theory"
        },
        {
            "paper_title": "Text-chem T5",
            "rating": 1,
            "sanitized_title": "textchem_t5"
        }
    ],
    "cost": 0.0156965,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>npj | computational materials Perspective</p>
<p>Edward O Pyzer-Knapp 
Matteo Manica 0000-0002-8872-0269
Peter Staar 
Lucas Morin 0000-0002-5829-5118
Patrick Ruch 
Teodoro Laino 
John R Smith 
Alessandro Curioni 
npj | computational materials Perspective
A5280227FED5FD50E9D8AA455C8084CC10.1038/s41524-025-01538-0Received: 18 September 2024; Accepted: 25 January 2025;</p>
<p>Large language models, commonly known as LLMs, are showing promise in tacking some of the most complex tasks in AI.In this perspective, we review the wider field of foundation models-of which LLMs are a component-and their application to the field of materials discovery.In addition to the current state of the art-including applications to property prediction, synthesis planning and molecular generation-we also take a look to the future, and posit how new methods of data capture, and indeed modalities of data, will influence the direction of this emerging field.</p>
<p>The story of AI is a story of data representations (Fig. 1).Early expert systems relied on hand-crafted symbolic representations which eventually evolved into task specific, hand-crafted representations for early machine learning applications 1 .Since hand-crafting a representation can act as a panacea for a lack of data, and capture a large amount of prior knowledge, this approach persisted for many years.As the availability of data grew, and the amount of compute available to apply to the problem grew with it, thoughts turned to more automated, data-driven ways to learn these representations utilizing the newly popular approach of deep learning [2][3][4] .This approach, whilst bereft of the injected prior knowledge of their hand-crafted cousins, started to address the related issue of the implicit inclusion of human biases.As this approach gained popularity, driven in part by the advent and enthusiastic uptake of GPUs for model training 5,6 , we saw a paradigm shift in the way data was considered, with significant effort being placed in the collection and curation of large data sets for training deep learning models 7,8 .Of course, there is a practical, if not fundamental, limit to the number of clean and large data sets which can be used for such tasks; and fundamental questions about the novelty of scientific discovery which can be brought to bear using models where so much data is already known, which brought into sharp focus the need for more generalizable representations.In 2017, the invention of the transformer architecture 9 , which was then developed into the generative pretrained transformer (GPT) models by OpenAI [10][11][12][13] , demonstrated that there was a route to generalized representations through the mechanism of self-supervised training on large corpora of text.Philosophically, this model can be thought of as harking back to the age of specific feature design, albeit through the lens of an oracle trained through exposure to phenomenal volumes of, often noisy and unlabeled, data.Through this decoupling, the task of representation learning, which is the most data hungry, is performed once-with smaller fine-tuning target-specific tasks now requiring little-or sometimes even no-additional training.</p>
<p>Whilst materials discovery can be a somewhat more nuanced task than language generation [14][15][16] , we have seen that techniques and technology developed in the AI realm for language are often transposed and translated to this important task 14,17,18 .In this perspective, we will chart the current state of foundation models-the general term for the newly evolved class of machine learning models of which large language models (LLMs) are a part-for materials discovery, and reflect on the challenges which should be addressed to maximize the impact of this important development to the scientific community.</p>
<p>Foundation models</p>
<p>The class of AI model commonly referred to as a "foundation model"of which LLMs are a specific incarnation-is defined as a "model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks" 19 .These models typically exist as a base model, which is often generated through unsupervised pre-training on a large amount of unlabeled data.This base model can then be fine-tuned using (often significantly less) labeled data to perform specific tasks.Optionally, this fine-tuned model can also undergo a process known as alignment.In this process, the outputs generated by the model are aligned to the preferences of the end user.In language tasks, this might take the form of reducing harmful outputs not aligned with human values (Fig. 1), whereas for a chemical task this might take the form of generating molecular structures which have improved synthesisability, or chemical correctness.Typically this is achieved through conditioning the exploration of the latent space to particular parts of a desired property distribution.A visual description of how the encoder and decoder tasks interact with a latent space, as well as how models trained from that latent space produce a property distribution is shown in Fig. 2, and we also point the interested reader at the following excellent articles which are dedicated to the subject [20][21][22][23] .</p>
<p>The separation of representation learning from the downstream tasks such as output generation can naturally be crystallized in the model architecture.Whilst the original transformer architecture encompassed both the encoding and decoding tasks, we now frequently see these components as being decoupled models, leading to encoderonly and decoder-only architectures becoming commonplace.Drawing from the success of Bidirectional Encoder Representations from Transformers (BERT) 24 , encoder-only models focus solely on understanding and representing input data, generating meaningful representations that can be used for further processing or predictions.Decoder-only models, on the other hand, are designed to generate new outputs by predicting and producing one token at a time based on the given input and previously generated token, making them ideally suited to the task of generating, for example, new chemical entities.Examples of how these types of models could be used in the context of materials discovery are shown in Table 1.</p>
<p>Data extraction</p>
<p>The starting point for successful pretraining and instruction tuning of foundational models is the availability of significant volumes of data, preferably at a high quality.For materials discovery, this principle is even more critical.Materials exhibit intricate dependencies where minute details can significantly influence their properties-a phenomenon known in the cheminformatics community as an "activity cliff".For instance, in the context of high-temperature superconductors like the high-temperature cuprate superconductors, the critical temperature (T c ) can be profoundly affected by subtle variations in hole-doping levels.Models which do not have the richness of information within their training data may miss these effects entirely, potentially leading to non-productive avenues of research inquiry.Chemical databases provide a wealth of structured information on materials and are therefore a first choice.Indeed resources such as PubChem 21 , ZINC 25 , and ChEMBL 26 are commonly used to train chemical foundation models [27][28][29] .However, these sources are often limited in scope and accessibility due to factors such as licensing restrictions (especially for proprietary databases 30 ), the relatively small size of datasets, and biased data sourcing.Furthermore, one must ensure the quality and reliability of the extracted data.Source documents often contain noisy, incomplete, or inconsistent information, which can propagate errors into downstream models and analyses.For instance, discrepancies in naming conventions, ambiguous property descriptions, or poor-quality images can hinder accurate extraction and association of materials data.To overcome these limitations, there is an imperative need for robust data-extraction models capable of operating at scale on one of the most common and ubiquitous data-sources, that is, the source documents themselves.</p>
<p>A significant volume of relevant materials information is represented in documents, be that public or proprietary scientific reports, patents or presentations.To extract the relevant materials data from these sources, any AI powered extraction models must efficiently parse and collect the materials information from a variety of habitats.Traditional data-extraction approaches primarily focus on text in the documents [31][32][33] , however, in the realm of materials science, significant information is also embedded in tables, images, and molecular structures.For example, in patent documents, some molecules are selected for their importance and represented by images, while the text can contain any irrelevant structures.Therefore, modern databases aim to extract molecular data not only from text, but from these multiple modalities 34,35 .Additionally, some of the most valuable data arises from the combination of text and images, such as Markush structures in patents, which encapsulate the key patented molecules.Thus, advanced data-extraction models must be adept at handling multimodal data, integrating textual and visual information to construct comprehensive datasets that accurately reflect the complexities of materials science.</p>
<p>Data extraction-based foundation models typically focus on two types of problems-on the one hand identifying the materials themselves and on the other hand identifying and associating described properties with these materials.</p>
<p>For the former, work to date has focused on leveraging the traditional named entity recognition (NER) approaches 36,37 although we note that this is only possible for data encoded within text.Some algorithms have also been developed to identify molecular structures from images in documents, using state-of-the-art computer vision such as Vision Transformers 38,39 and Graph Neural Networks 40 .Recent studies further aim to merge both modalities for extracting general knowledge from chemistry literature [41][42][43] .For the second type, i.e., property extraction and association, the latest progress in LLMs has allowed such tasks to become much more accurate and leverage schema based extraction 44,45 .</p>
<p>While traditional NER and multimodal approaches have shown promise in extracting materials data from diverse document formats, it is important to recognize that multimodal language models need not independently handle all forms of information.Instead, they can effectively integrate with specialized algorithms that act as intermediary tools to process specific types of content.For instance, Plot2Spectra 46 demonstrates how specialized algorithms can extract data points from spectroscopy plots in scientific literature, enabling large-scale analysis of material properties that would otherwise be inaccessible to text-based models.Similarly, DePlot 47 illustrates the utility of modular approaches by converting visual representations such as plots and charts into structured tabular data, which can then be used as input for reasoning by large language models.These examples emphasize that multimodal models can function as orchestrators, leveraging external tools for domain-specific tasks, thereby enhancing the overall efficiency and accuracy of data extraction pipelines in materials science.</p>
<p>Property prediction</p>
<p>The prediction of property from structure is a core component of the value that data-driven approaches can bring to materials discovery.Traditionally, property prediction has either been utilized as a highly approximate initial screen (for example, traditional QSPR methods), or based upon simulation of the fundamental physics of the systems, which can be prohibitively expensive.foundation models offer the opportunity to create powerful predictive capabilities based upon transferrable core components and begin to enable a truly data-driven approach to inverse design.</p>
<p>It is important to note that the current literature is dominated by models which are trained to predict properties from 2D representations of the molecule such as SMILES 48 or SELFIES 49 , which can lead to key information such as the 3D conformation of a molecule, being omitted.This is in part due to the significant delta in available datasets for these two respective modalities-with current foundation models being trained on datasets such as ZINC 25 and ChEMBL 26 which both offer datasets ~10 9 molecules-a size not readily available for 3D data.An exception is represented by inorganic solids, such as crystals, where property prediction models usually leverage 3D structures through graph-based or primitive cell feature representations 50,51 .</p>
<p>Many of the foundation models that are used for property prediction are encoder-only models based broadly on the BERT 24 architecture 27,52-55 , although Fig. 3 shows that other base architectures such as GPT [56][57][58][59] are becoming more prevalent.The reuse of both core models, and core architectural components is a strength of the foundation model approach, although there are analogies to be drawn to the limited number of handcrafted features that ultimately pushed the community into a more datadriven approach.</p>
<p>It is also possible to think of some of the recent class of machine learning based potentials-commonly known as MLIPs (machine learned interatomic potentials)-as foundation models 60 , with their core mode of operation being the prediction of energies and forces of a system, based upon a model pre-trained using a large amount of high-quality reference data-typically based on density functional theory (DFT).In much the same way that base representations can be leveraged to build powerful predictors, we are beginning to see pre-trained models such as MEGNET 61 , MACE 62 , ANI 63 and AIMNET 64,65 being tuned for more specific tasks 66 , or for more accurate datasets 67 , for which there is a lower available data volume.This is enhanced by efforts such as Optimade 68 , which reduces the barrier to bringing together different materials databases, including those built on simulated data.This offers a fundamentally different route to the use of foundation models for materials property prediction.Whilst many models approach the problem as a direct prediction of a property, this can be hindered by underrepresentation of, for example, rare events.By instead approximating the underlying potential, MLIPs enable traditional simulation techniques at a significantly reduced overhead, and thus the discovery of outcomes which need not be expressed in the original training data.</p>
<p>Of course, for both of these use cases, there is always the problem of understanding transferability and appropriateness of models 69,70 .To this end, we are beginning to see the emergence of techniques to evaluate properties such as model roughness 71,72 , which are able to provide quantitative measures which can be related to the likelihood of successful model application and the detection of so-called activity cliffs.We do note, however, that this is still not a solved problem and welcome further research into this area.</p>
<p>Molecular generation</p>
<p>Motivated by the need to overcome the limitations of traditional heuristic and grid-search approaches, AI generative models for material design have gained increasing popularity.</p>
<p>These models are trained to propose novel molecules with desired properties by relying on a variety of molecular structure representations, e.g., text-based SMILES 48 and SELFIES 49 or graph-based approaches 73 .</p>
<p>The field of machine learning-based algorithms for designing materials has seen a surge in diversity, with various techniques and applications being proposed.These include VAEs, GANs, GNNs, Transformer-based models, and Diffusion models, each offering unique contributions to the field.</p>
<p>From the end of the last decade, a series of seminal works [74][75][76][77][78] based on textual and graphical representations of molecular entities, have started showing promising application of deep generative models to design materials.</p>
<p>In the way paved by these early efforts, prominent examples have demonstrated how these models can be applied to conditionally design and optimize therapeutics successfully validated in silico or in vitro for a variety of applications: kinase inhibitors 79,80 , antivirals 81,82 , antimicrobials 83 , and disease-specific compounds 84,85 .</p>
<p>The usage of such machine learning-based approaches to molecule generation is not limited to the pharmaceutical domain but has soon shown remarkable results in the broader field of material discovery for propertydriven design, e.g., sugar/dye molecules via graph generation 86 , small molecules, peptides, and polymers generation leveraging language models 87,88 , and, semiconductors combining deep learning and DFT 89 .</p>
<p>The development of models has significantly reduced the barriers to accessing generative algorithms for material design.This is largely due to the release of open benchmarks and specialized toolkits for generative molecular design, which encompass a wide range of methods, including evolutionary approaches and generative models, such as GuacaMol 90 , Moses 91 , TDC (Therapeutics Data Common) 92,93 , and GT4SD 94 .</p>
<p>Despite the ease of access to these technologies, training generative models at the foundation model scale in material science still presents challenges 21 .Indeed, only recently have we seen attempts to train generative foundation models exhibit promising results in a multi-task setting that leverages extensive pretraining across various chemical tasks 95,96 .</p>
<p>Foundation models for materials synthesis</p>
<p>The emergence of foundation models in materials science represents a significant opportunity to revolutionize the synthesis of both inorganic and organic materials.The direct application of foundation models in these domains is still in its early stages, with key developments in the synthesis of both inorganic and organic materials thus far being more closely aligned with traditional machine learning approaches rather than foundation models.Nevertheless, compelling indications in the literature suggest that foundation models will play a pivotal role in the future, making it crucial to carefully analyze these trends to anticipate the forms and characteristics that future foundation models may take as they evolve.</p>
<p>Significant advancements in the synthesis of inorganic materials have been achieved through the application of machine learning and data-driven approaches.Recent studies have highlighted the potential of utilizing novel data sources, such as natural language text from scientific literature 97 , to predict synthesis protocols for inorganic materials 98 .For example, word embeddings and variational autoencoders have been used to generate synthesis strategies for perovskite materials 99 .Additionally, natural language processing techniques have been instrumental in designing novel synthesis heuristics derived from text-mined literature data.When combined with active learning, these heuristics optimize the synthesis of novel inorganic materials in powder form 100 .These efforts have led to the development of extraction and analysis pipelines for synthesis information from scientific publications, laying the groundwork for comprehensive, high-quality datasets for inorganic materials 101 and single-atom heterogeneous catalysis 102 .While natural language processing techniques have demonstrated significant potential, it is critical to observe that literature itself often presents inherent limitations that can affect the applicability of extracted synthesis recipes.For example, the quality and consistency of reporting in the scientific literature, as noted by David et al. 103 , can pose significant barriers.These challenges include incomplete data, inconsistent terminology, and insufficient experimental details, which can limit the reliability and generalizability of text-derived synthesis strategies.Addressing these limitations will require improved curation of datasets and the development of more robust processing frameworks to ensure accurate and actionable predictions.Supervised machine learning models have been developed to classify and predict suitable synthesis routes and conditions, such as calcination and sintering temperatures, based on target and precursor materials, outperforming traditional heuristics 104 .Reinforcement learning has also been successfully applied to predict optimal synthesis schedules, including time-sequenced reaction conditions for the synthesis of semiconducting monolayer MoS 2 using chemical vapor deposition 75 .Recent advancements have leveraged high-throughput thermochemical data and classical nucleation theory, identifying favorable reactions based on catalytic nucleation barriers and selectivity metrics, such as in Aykol et al. 105 .This method has been validated on well-known compounds such as LiCoO₂, BaTiO₃, and YBa₂Cu₃O₇, showcasing its applicability in identifying both established and unconventional synthetic routes.The integration of such frameworks into foundation models could further enhance their ability to predict and optimize complex synthesis processes by providing structured insights into thermodynamic and kinetic factors, opening up exciting possibilities for leveraging foundation models in both the analysis and prediction of time-series data 106 , as well as their application to chemical synthesis.The progress made in machine learning for inorganic synthesis underscores the immense potential of foundation models, particularly in expanding the capabilities of LLMs.Recent studies 107 have demonstrated the effectiveness of LLMs in predicting the synthesizability of inorganic compounds and selecting suitable precursors.In the long term, foundation models are likely to achieve performance levels comparable to specialized ML models, but with significantly reduced development time and costs.This suggests that the future of material synthesis may increasingly be driven by foundation models tailored to multiple tasks within this domain, making them a powerful tool for advancing the field.</p>
<p>The application of foundation models in the synthesis of organic materials follows a similar trajectory.In the realm of organic synthesis, these models hold immense potential for transforming synthetic pathways and optimizing reaction conditions.Early studies have shown that deep learning models can effectively predict reaction outcomes and retrosynthetic pathways, which are critical to the advancement of organic synthesis.A notable example is the Molecular Transformer model 108 , the first language model in chemical synthesis, which achieved state-of-the-art accuracy in predicting reaction products by treating reaction prediction as a machine translation problem.Recent advancements in prompt-based inference 109 have extended the capabilities of these models, enabling chemists to steer retrosynthetic predictions, thereby providing more diverse and creative disconnection strategies while overcoming biases in training data.The introduction of domain-specific LLMs 110,111 underscores the importance of fine-tuning with specialized data and integrating advanced chemistry tools 112 , significantly enhancing the predictive capabilities for synthetic tasks in organic chemistry.The introduction of the first foundation model 95 capable of addressing multiple tasks across both chemical 17,108,[113][114][115] and natural language domains 116 marks a breakthrough, demonstrating that sharing information across these domains can enhance model performance, particularly in crossdomain tasks.Furthermore, the implementation LLMs in organic chemistry, exemplified by recent advancements in training a foundational largescale model with 15 billion parameters for retrosynthesis prediction and generative chemistry 117 , highlights the potential of foundation models to extend beyond predictive capabilities, and eventually guide experimental efforts in the laboratory.These advancements underscore the potential for further refinement of foundation models, which could lead to enhanced predictive accuracy and, ultimately, more efficient synthesis of complex organic molecules.</p>
<p>The future application of foundation models in the synthesis of both inorganic and organic materials is particularly exciting when considering their ability to learn from diverse data modalities, including spectroscopic data 118 , crystallography 119 , and atomistic simulations 60 .This capability allows for a more holistic understanding of material behavior, facilitating the design of novel compounds with desired properties.The development of multimodal foundation models, which can simultaneously process and learn from these varied data sources, will represent a transformative approach in the synthesis of both organic and inorganic materials.By capturing complex interactions across different modalities, these models will not only improve predictive accuracy but also enable the generation of new hypotheses for experimental validation.This could lead to the discovery of materials with unprecedented properties, advancing fields such as catalysis, drug design, and energy storage.Moreover, their ability to generalize across different types of data positions them as powerful tools for tackling the intricate challenges in materials science, from understanding reaction mechanisms to optimizing synthesis pathways, all within a unique model.</p>
<p>Challenges and future look</p>
<p>Tackling the data challenge with multi-modal models In the wake of the rising popularity of LLMs' applications beyond natural language and the introduction of increasingly powerful foundation models, there has been a surge in interest in multimodal approaches.This has led to the development of vision-language models, such as Flamingo 120 , LLaVA 121 , and Idefics 122,123 which combine different data types to enhance the perception horizon of foundation models.Alongside modeling approaches, multimodal datasets play a crucial role in the success of such models.</p>
<p>Multiple efforts in building web-crawled datasets combining visuals and textual data have become central in machine learning research, culminating with extensive benchmarks promoting advances in multimodal modeling research, e.g., the Cauldron 124 or MMMU 125 .Besides generic datasets for vision-language modeling, there has been a growing interest in compiling data resources to push the boundaries of foundation model perception.Specifically, in the space of egocentric perception, two notable examples are represented by Ego4D 126 and Ego-Exo4D 127 , which provide rich, diverse, and annotated data for research in human activity recognition and exploration.</p>
<p>Despite the growing popularity of multimodal approaches, their application in material discovery remains limited.This limited application is primarily due to the lack of large-scale, high-quality datasets that cover a wide range of materials and their properties.Nevertheless, in recent years, inspired by the successful application of multi-task prompted training and instruction tuning 128 , models like Text-chem T5 95 and Regression Transformer (RT) 87,88 have been successfully adopted to solve multimodal tasks to accelerate the design of novel materials exhibiting superior performance to specialized single-task models.</p>
<p>These promising attempts at developing multi-purpose models for materials motivate further and deeper experimentation by combining various data sources in model training.Especially considering the wealth of high-quality data that can be generated by simulations, using the latter to fill gaps in real-world data represents a strategy to build more robust and accurate models.</p>
<p>Capturing new types of data for new types of models</p>
<p>The growing use of generative models for synthetic data generation and data augmentation in the domain of materials discovery remains grounded in data from experimental observations, which is indispensable to support the discovery of novel materials 14,16 .However, fundamental concerns regarding the reproducibility of experimental findings prevail 129 , and challenges with respect to effective dissemination of high-quality and findable, accessible, interoperable and reusable (FAIR) 130 experimental data cut across scientific disciplines.The lack of reproducibility, in particular, can usually be traced back to experimental data and metadata that is flawed or missing 131,132 .Additional key challenges are the disparity of data formats and schema that must be addressed in order to achieve data interoperability and standardization 133 , as well as the diversity of tools used to help digitize and organize experimental data 134,135 .</p>
<p>While the consolidation and organization of already digitized data may be facilitated by the use of LLMs 136 , the emergence of multi-modal foundation models capable of processing different data modalities at the same time offers new perspectives for data capture and experimental documentation.For example, the shift from convolutional neural networks (CNNs) to transformer-based foundation models for video and action recognition 10 has spawned a series of foundational vision-language models that exhibit proficiency in diverse video recognition tasks [137][138][139][140][141] .Such models, used to describe and document a wide range of real-world actions, have demonstrated significant gains in performance as exemplified by the increase in top-1 accuracy for action recognition on the Kinetics-400 dataset 142,143 from 73.9% in 2016 to 93.6% in 2024 144 .</p>
<p>So far, the training data for large vision-language models has mostly been restricted to common human activities 145 , but applications have also been studied in specialized domains such as endoscopic surgical procedures to provide information on surgical gestures, generate feedback for surgeons, and help study the relationship between intraoperative factors and postoperative outcomes 146 .The study and adaptation of multi-modal foundation models to the transcription of specialized procedures has the potential to alleviate data and metadata capture at the roots of the experimentation process, by automatically converting raw data streams of observations and sensor data to a reproducible transcript in the desired target format using multi-modal generative models.Such data capture could be realized with minimal burden on the experimentalist and implemented to generate documentation for any digital system of record, such as electronic laboratory notebooks (ELNs).This type of novel data capture stands to benefit both experimentalists who generate the data as well as data scientists and theoreticians who rely on the data to train and validate scientific models.While this concept is still nascent and lacking open datasets and benchmarks, the automatic transcription of laboratory procedures in real-time using multi-modal foundation models has already been demonstrated in principle 147 and may help standardize the process of documenting manual research, link procedural details with outcomes, facilitate science education and information sharing, and supply more consistent and reproducible data for the next generation of foundation models for materials discovery.</p>
<p>Exploiting the science of approximation through multifidelity models As both our model architectures, and the computational infrastructure on which they run, are able to ingest extreme volumes of data, the risk of data collection biases being transferred into the models increases.Whilst we can observe this to a degree in today's language models-for example through differential capability in English language models compared to other, less common, languages-we believe that this is a particular risk in materials discovery.We hold this view as the very task of discovery requires stepping into unknown chemistries, which may not be well represented in large volume datasets, which can be dominated by a particular part of chemical space (e.g., organic chemistry) or domain (e.g., drug discovery).We view multi-fidelity models as having a significant role to play in mitigating this risk.</p>
<p>A multi-fidelity model is one which can take as input data collected in several different ways, each with their own acquisition cost and accuracy.An example of this might be collecting molecular quantum-chemical data using a range of quantum mechanical approaches (e.g., semi-empirical methods, DFT, MP2).By being able to use data from a variety of sources, in a similar manner to which multi-modal models can draw data from a variety of modalities, multi-fidelity models are able to mitigate data sparsity concerns.</p>
<p>Multi-fidelity methods are a reasonably recent addition to the machine learning for materials discovery toolkit, including recent advances in their use for optimization 148 , and property prediction of both molecules [149][150][151] and materials [152][153][154] , and we believe that their extension to the foundation models arena will continue to drive progress.</p>
<p>Conclusion</p>
<p>Foundation models are already showing promise in tackling some of the hardest problems in materials discovery.In this perspective we outline some of the fundamentals of this new area of study, and their applicability to the task of materials discovery.We highlight some areas in which foundation models are already being used to significant impact-namely property prediction, retrosynthesis, and molecular generation-and also look to the future to outline areas which we believe are key to continuing to unlock value.These areas hinge on exploiting the natural multi-modality and multifidelity characteristics of materials data through increasingly powerful and elegant modeling approaches.We believe that building from the current state of the art into these areas will unlock a significant value stream, potentially enabling substantial acceleration of materials discovery by leveraging the ever-growing troves of data produced by the research community.</p>
<p>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.To view a copy of this licence, visit http://creativecommons.</p>
<p>Fig. 1 |
1
Fig. 1 | Timeline of the development of representations for machine learning.This timeline shows the evolution from hand crafted symbolic representations to today's foundation models through major milestones such as the advent of deep learning.</p>
<p>Fig. 2 |
2
Fig. 2 | A visual representation of the creation and utilization of a shared latent space, or embedding, via encoder, decoder and predictor models.Molecular representations are transformed into their latent space representation by an encoder model, which can be reversed by a decoder model.Additionally properties can be directly predicted from the encoded representation via the building of an additional property predictor model, which is itself capable of producing a property distribution for the latent space.</p>
<p>Fig. 3 |
3
Fig. 3 | Breakdown of common architecture types of models currently reported to perform property prediction.Colors have been determined by the "super type", or originating, architecture.Greens represent models based upon BERT, purples on GPT, oranges refer to architectures based directly on transformers, and blue XLNet.Data based on ref. 155.</p>
<p>https://doi.org/10.1038/s41524-025-01538-0Perspective npj Computational Materials | (2025) 11:61</p>
<p>https://doi.org/10.1038/s41524-025-01538-0Perspective npj Computational Materials | (2025) 11:61</p>
<p>org/licenses/by/4.0/.© The Author(s) 2025 https://doi.org/10.1038/s41524-025-01538-0Perspective npj Computational Materials | (2025) 11:61</p>
<p>Table 1 |
1
Examples of tasks in materials discovery which are currently using a foundation model approach
TaskModalitiesArchitectureExample usageData extractionText, ImageEncoderMolecular property extraction from patentProperty predictionText, Graph, Structural, Spectroscopic,Encoder, Encoder-DecoderDownstream prediction of molecular property from molecular graphMolecular generationText, GraphDecoderGenerate new molecular candidates for a desired taskSynthesis predictionTextEncoder-DecoderRetrosynthetic planning
IBM Research Europe, Rüschlikon, Switzerland.
Xyme, Oxford, UK.
IBM Research-TJ Watson Research Center, Yorktown Heights, NY, USA. e-mail: ed@xyme.ai npj Computational Materials | (2025) 11:61 1 1234567890():,; 1234567890():,;
AcknowledgementsThe authors would like to acknowledge their IBM colleagues for their continued contributions to this exciting area of research.Author contributionsAll authors contributed to the conception, structuring, and writing of this perspective.Competing interestsThe authors declare no competing interests.
An overview on data representation learning: from traditional feature learning to recent deep learning. G Zhong, L.-N Wang, X Ling, J Dong, J. Finance Data Sci. 22016</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5212015</p>
<p>Image matching from handcrafted to deep features: a survey. J Ma, X Jiang, A Fan, J Jiang, J Yan, Int. J. Comput. Vis. 1292021</p>
<p>The Goldilocks paradigm: comparing classical machine learning, large language models, and few-shot learning for drug discovery applications. S H Snyder, Commun. Chem. 72024</p>
<p>The transformational role of GPU computing and deep learning in drug discovery. M Pandey, Nat. Mach. Intell. 42022</p>
<p>. Y E Wang, G.-Y Wei, D Brooks, Tpu Benchmarking, Gpu Cpu, 10.48550/arXiv.1907.107012019</p>
<p>ImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. Curran Associates, Inc201225</p>
<p>. Y Lecun, C Cortes, MNIST handwritten digit database. 2010</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, OpenAI blog. 2018</p>
<p>paper/Language-Models-are-Unsupervised-Multitask-Learners. A Radford, Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe2018Preprint at Semantic ScholarLanguage models are unsupervised multitask learners</p>
<p>Language models are few-shot learners. T Brown, Adv. neural inf. process. syst. 332020</p>
<p>GPT-4 technical report. Openai, 10.48550/arXiv.2303.087742024Preprint</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. E O Pyzer-Knapp, Comput. Mater. 82022</p>
<p>Finding the needle in the haystack: materials discovery and design through computational ab initio high-throughput screening. G Hautier, Comput. Mater. Sci. 1632019</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 6202023</p>
<p>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. P Schwaller, Chem. Sci. 112020</p>
<p>Found in Translation": predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. P Schwaller, T Gaudin, D Lányi, C Bekas, T Laino, Chem. Sci. 92018</p>
<p>On the opportunities and risks of foundation models. R Bommasani, 2022Preprint</p>
<p>A review of large language models and autonomous agents in chemistry. M C Ramos, C Collison, A D White, 10.1039/D4SC03921AChem. Sci. 2024</p>
<p>Foundation model for material science. S Takeda, A Kishimoto, L Hamada, D Nakano, J R Smith, Proc. AAAI Conf. AAAI Conf202337</p>
<p>A tutorial survey of architectures, algorithms, and applications for deep learning. L Deng, APSIPA Trans. Signal Inf. Process. 3e22014</p>
<p>The Hitchhiker's guide to deep learning driven generative chemistry. Y Ivanenkov, ACS Med. Chem. Lett. 142023</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>ZINC-a free database of commercially available compounds for virtual screening. J J Irwin, B K Shoichet, J. Chem. Inf. Model. 452005</p>
<p>ChEMBL: a large-scale bioactivity database for drug discovery. A Gaulton, Nucleic Acids Res. 402012</p>
<p>ChemBERTa: largescale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, 10.48550/arXiv.2010.098852020Preprint at</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, Nat. Mach. Intell. 42022</p>
<p>MolGPT: molecular generation using a transformer-decoder model. V Bagal, R Aggarwal, P K Vinod, U D Priyakumar, J. Chem. Inf. Model. 622022</p>
<p>The making of reaxys-towards unobstructed access to relevant chemistry information. A J Lawson, J Swienty-Busch, T Géoui, D Evans, The Future of the History of Chemical Information. L R Mcewen, R E Buntrock, American Chemical Society20141164</p>
<p>Automatic identification of relevant chemical compounds from patents. Database. S A Akhondi, 2019. 20191</p>
<p>ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature. M C Swain, J M Cole, 10.1038/s41524-025-01538-0Perspective npj Computational Materials |. 56612016. 2025J. Chem. Inf. Model.</p>
<p>Chemical named entity recognition in patents by domain knowledge and unsupervised feature learning. Y Zhang, 2016. 201649</p>
<p>PatCID: an open-access dataset of chemical structures in patent documents. L Morin, V Weber, G I Meijer, F Yu, P W J Staar, Nat. Commun. 152024</p>
<p>SureChEMBL: a large-scale, chemically annotated patent document database. G Papadatos, Nucleic Acids Res. 442016</p>
<p>Named entity recognition and normalization applied to large-scale information extraction from the materials science literature. L Weston, J. Chem. Inf. Model. 592019</p>
<p>MatSciBERT: a materials domain language model for text mining and information extraction. T Gupta, M Zaki, N M A Krishnan, Mausam, Comput. Mater. 82022</p>
<p>an open platform for automated optical chemical structure identification, segmentation and recognition in scientific publications. K Rajan, H O Brinkhaus, M I Agea, A Zielesny, C Decimer Steinbeck, Ai, Nat. Commun. 1450452023</p>
<p>MolScribe: robust molecular structure recognition with image-to-graph generation. Y Qian, J. Chem. Inf. Model. 632023</p>
<p>MolGrapher: Graph-based Visual Recognition of Chemical Structures. L Morin, 10.1109/ICCV51070.2023.017912023 IEEE/CVF InternationalConference on Computer Vision (ICCV) 19495-19504. Paris, FranceIEEE2023</p>
<p>OpenChemIE: an information extraction toolkit for chemistry literature. V Fan, J. Chem. Inf. Model. 642024</p>
<p>Uni-SMART: universal science multimodal analysis and research transformer. H Cai, 10.48550/arXiv.2403.103012024Preprint at</p>
<p>Multi-modal chemical information reconstruction from images and texts for exploring the near-drug space. J Wang, Brief. Bioinforma. 234612022</p>
<p>A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing. P Shetty, NPJ Comput. Mater. 9522023</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, Nat. Commun. 1514182024</p>
<p>Plot2Spectra: an automatic spectra extraction tool. W Jiang, Digital Discov. 12022</p>
<p>DePlot: one-shot visual language reasoning by plot-totable translation. F Liu, Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Association for Computational Linguistics2023</p>
<p>SMILES. a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J. Chem. Inf. Comput. Sci. 281988</p>
<p>SELFIES and the future of molecular string representations. M Krenn, Patterns. 31005882022</p>
<p>Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. T Xie, J C Grossman, Phys. Rev. Lett. 1201453012018</p>
<p>Generative design of stable semiconductor materials using deep learning and density functional theory. E M D Siriwardane, Y Zhao, I Perera, J Hu, Comput. Mater. 82022</p>
<p>Catalyst energy prediction with CatBERTa: unveiling feature exploration strategies through large language models. J Ock, C Guntuboina, A Barati Farimani, ACS Catal. 132023</p>
<p>SELFormer: molecular representation learning via SELFIES language models. A Yüksel, U Erva, Ü Atabey, D Tunca, Mach. Learn. -Sci. 2250352023</p>
<p>SolvBERT for solvation free energy and solubility prediction: a demonstration of an NLP model for predicting the properties of molecular complexes. J Yu, Digital Discov. 22023</p>
<p>Mol-BERT: an effective molecular representation with BERT for molecular property prediction. J Li, X Jiang, Proc. Int. Wirel. Commun. Mob. Comput. Conf. 202171818152021</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nat. Mach. Intell. 62024</p>
<p>A smile is all you need: predicting limiting activity coefficients from SMILES with natural language processing. B Winter, C Winter, J Schilling, A Bardow, Digital Discov. 12022</p>
<p>Generative pre-training from molecules. S Adilov, 10.26434/chemrxiv-2021-5fwjd2021</p>
<p>MolXPT: wrapping molecules with text for generative pre-training. Z Liu, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Short Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20232</p>
<p>A foundation model for atomistic materials chemistry. I Batatia, 2024Preprint at</p>
<p>A universal graph deep learning interatomic potential for the periodic table. C Chen, S P Ong, Nat. Comput. Sci. 22022</p>
<p>MACE: higher order equivariant message passing neural networks for fast and accurate force fields. I Batatia, D P Kovacs, G Simm, C Ortner, G Csanyi, Adv. Neural Inf. Process. Syst. 352022</p>
<p>ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. J S Smith, O Isayev, A E Roitberg, Chem. Sci. 82017</p>
<p>AIMNet2: a neural network potential to meet your neutral, charged, organic, and elementalorganic needs. D Anstine, R Zubatyuk, O Isayev, 10.26434/chemrxiv-2023-296ch-v22024Preprint at</p>
<p>Accurate and transferable multitask prediction of chemical properties with an atoms-in-molecules neural network. R Zubatyuk, J S Smith, J Leszczynski, O Isayev, Sci. Adv. 564902019</p>
<p>Performance assessment of universal machine learning interatomic potentials: challenges and directions for materials' surfaces. B Focassio, L P M Freitas, G R Schleder, 10.48550/arXiv.2403.042172024Preprint at</p>
<p>Approaching coupled cluster accuracy with a general-purpose neural network potential through transfer learning. J S Smith, Nat. Commun. 1029032019</p>
<p>OPTIMADE, an API for exchanging materials data. C W Andersen, Sci. Data. 82172021</p>
<p>How big is big data?. D Speckhard, 10.1039/D4FD00102HFaraday Discuss. 2024</p>
<p>A critical examination of robustness and generalizability of machine learning prediction of materials properties. K Li, B Decost, K Choudhary, M Greenwood, J Hattrick-Simpers, npj Comput. Mater. 92023</p>
<p>A physics-inspired approach to the understanding of molecular representations and models. L Dicks, D Graff, K Jordan, C Coley, E Pyzer-Knapp, Mol. Syst. Des. Eng. 92024</p>
<p>Evaluating the roughness of structure-property relationships using pretrained molecular representations. E Graff, Digital Discov. 22023</p>
<p>the mathematical theory of isomers. O Cayley, Philos. Mag. 471874</p>
<p>Automatic chemical design using a data-driven continuous representation of molecules. R Gómez-Bombarelli, 10.1038/s41524-025-01538-0Perspective npj Computational Materials |. 4612018. 2025ACS Cent. Sci.</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. M H Segler, T Kogej, C Tyrchan, M P Waller, ACS Cent. Sci. 42018</p>
<p>Junction tree variational autoencoder for molecular graph generation. W Jin, R Barzilay, T Jaakkola, PMLR 2323-PMLR 2332International Conference on Machine Learning. 2018</p>
<p>Graph convolutional policy network for goal-directed molecular graph generation. J You, B Liu, Z Ying, V Pande, J Leskovec, Adv. Neural Inf. Process. Syst. 312018</p>
<p>A de novo molecular generation method using latent vector based generative adversarial network. O Prykhodko, J. Cheminform. 112019</p>
<p>Deep learning enables rapid identification of potent ddr1 kinase inhibitors. A Zhavoronkov, Nat. Biotechnol. 372019</p>
<p>Active site sequence representations of human kinases outperform full sequence representations for affinity prediction and inhibitor generation: 3D effects in a 1D model. J Born, J. Chem. Inform. 622022</p>
<p>Cogmol: target-specific and selective drug design for covid-19 using deep generative models. V Chenthamarakshan, Adv. Neural Inf. Process. Syst. 332020</p>
<p>Data-driven molecular design for discovery and synthesis of novel ligands: a case study on SARS-COV-2. J Born, Mach. Learn. Sci. Technol. 2250242021</p>
<p>Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. P Das, Nat. Biomed. Eng. 52021</p>
<p>De novo generation of hit-like molecules from gene expression signatures using artificial intelligence. O Méndez-Lucio, B Baillif, D.-A Clevert, D Rouquié, J Wichard, Nat. Commun. 112020</p>
<p>PaccMannRL: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning. J Born, 202124102269</p>
<p>Molecular inverse-design platform for material industries. S Takeda, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Regression transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, Nat. Mach. Intell. 52023</p>
<p>Artificial intelligence driven design of catalysts and materials for ring opening polymerization using a domain-specific language. N H Park, Nat. Commun. 1436862023</p>
<p>Generative design of stable semiconductor materials using deep learning and density functional theory. E M D Siriwardane, Y Zhao, I Perera, J Hu, Comput. Mater. 81642022</p>
<p>Guacamol: benchmarking models for de novo molecular design. N Brown, M Fiscato, M H Segler, A C Vaucher, J. Chem. Inf. Model. 592019</p>
<p>Molecular sets (Moses): a benchmarking platform for molecular generation models. D Polykovskiy, Front. Pharm. 1119312020</p>
<p>Therapeutics data commons: machine learning datasets and tasks for drug discovery and development. K Huang, Adv. Neural Inf. Process. Syst. 352021</p>
<p>Artificial intelligence foundation for therapeutic science. K Huang, Nat. Chem. Biol. 112022</p>
<p>Accelerating material design with the generative toolkit for scientific discovery. M Manica, npj Comput. Mater. 92023</p>
<p>Unifying molecular and textual representations via multi-task language modelling. D Christofidellis, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningJMLR.org2023202</p>
<p>Bidirectional generation of structure and properties through a single molecular foundation model. J Chang, J C Ye, Nat. Commun. 1523232024</p>
<p>Dataset of solution-based inorganic materials synthesis procedures extracted from the scientific literature. Z Wang, Sci. Data. 92312022</p>
<p>Precursor recommendation for inorganic synthesis by machine learning materials similarity from scientific literature. T He, Sci. Adv. 981802023</p>
<p>Inorganic materials synthesis planning with literaturetrained neural networks. E Kim, J. Chem. Inf. Model. 602020</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. N J Szymanski, Nature. 6242023</p>
<p>ULSA: unified language of synthesis actions for the representation of inorganic synthesis protocols. Z Wang, Digital Discov. 12022</p>
<p>Language models and protocol standardization guidelines for accelerating synthesis planning in heterogeneous catalysis. M Suvarna, A C Vaucher, S Mitchell, T Laino, J Pérez-Ramírez, Nat. Commun. 1479642023</p>
<p>A critical reflection on attempts to machine-learn materials synthesis insights from text-mined literature recipes. W Sun, N David, 10.1039/D4FD00112EFaraday Discuss. 2024</p>
<p>Interpretable machine learning enabled inorganic reaction classification and synthesis condition prediction. C Karpovich, E Pan, Z Jensen, E Olivetti, Chem. Mater. 352023</p>
<p>Rational solid-state synthesis routes for inorganic materials. M Aykol, J H Montoya, J Hummelshøj, J. Am. Chem. Soc. 1432021</p>
<p>Foundation models for time series analysis: a tutorial and survey. Y Liang, Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining. the 30th ACM SIGKDD conference on knowledge discovery and data mining2024</p>
<p>MatChat: a large language model and application service platform for materials science. Z Chen, Chin. Phys. B. 321181042023</p>
<p>Molecular transformer: a model for uncertaintycalibrated chemical reaction prediction. P Schwaller, ACS Cent. Sci. 52019</p>
<p>Unbiasing retrosynthesis language models with disconnection prompts. A Thakkar, ACS Cent. Sci. 92023</p>
<p>Inferring experimental procedures from textbased representations of chemical reactions. A C Vaucher, Nat. Commun. 1225732021</p>
<p>SynAsk: unleashing the power of large language models in organic synthesis. C Zhang, 10.48550/arXiv.2406.045932024Preprint at</p>
<p>Augmenting large language models with chemistry tools. A Bran, Nat. Mach. Intell. 62024</p>
<p>Standardizing chemical compounds with language models. M T Cretu, Mach. Learn. Sci. Technol. 4350142023</p>
<p>Growing strings in a chemical reaction space for searching retrosynthesis pathways. F Zipoli, C Baldassari, M Manica, J Born, T Laino, Comput. Mater. 102024</p>
<p>Prediction of chemical reaction yields using deep learning. P Schwaller, A C Vaucher, T Laino, J.-L Reymond, Mach. Learn. Sci. Technol. 2150162021</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. A C Vaucher, Nat. Commun. 1136012020</p>
<p>BatGPT-Chem: a foundation large model for chemical engineering. Y Yang, 10.26434/chemrxiv-2024-1p4xthttps://doi.org/10.1038/s41524-025-01538-0Perspective npj Computational Materials |. 11612024. 2025Preprint at</p>
<p>Leveraging infrared spectroscopy for automated structure elucidation. M Alberts, T Laino, A C Vaucher, 10.26434/chemrxiv-2023-5v27f2023Preprint at</p>
<p>Graph-text contrastive learning of inorganic crystal structure toward a foundation model of inorganic materials. K Ozawa, T Suzuki, S Tonogai, T Itakura, 10.26434/chemrxiv-2024-mpl8l2024Preprint at</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>OBELICS: an open web-scale filtered dataset of interleaved image-text documents. H Laurençon, 10.48550/arXiv.2306.165272023Preprint at</p>
<p>Improved baselines with visual instruction tuning. H Liu, C Li, Y Li, Y J Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>What matters when building vision-language models?. H Laurençon, L Tronchon, M Cord, V Sanh, Adv. Neural Inf. Process. Syst. 372025</p>
<p>MMMU: a massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. X Yue, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Ego4D: around the world in 3,000 h of egocentric video. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Ego-Exo4D: understanding skilled human activity from first-and third-person perspectives. K Grauman, Proceedings of the IEEE/ CVF Conference on Computer Vision and Pattern Recognition. the IEEE/ CVF Conference on Computer Vision and Pattern Recognition19383-19400 (2024</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, International Conference on Learning Representations. 2022</p>
<p>1500 scientists lift the lid on reproducibility. M Baker, Nature. 5332016</p>
<p>The FAIR guiding principles for scientific data management and stewardship. M D Wilkinson, Sci. Data. 31600182016</p>
<p>The variable quality of metadata about biological samples used in biomedical experiments. R S Gonçalves, M A Musen, Sci. Data. 61900212019</p>
<p>No raw data, no science: another possible source of the reproducibility crisis. T Miyakawa, Mol. Brain. 13242020</p>
<p>Making the collective knowledge of chemistry open and machine actionable. K M Jablonka, L Patiny, B Smit, Nat. Chem. 142022</p>
<p>Considerations for implementing electronic laboratory notebooks in an academic research environment. S G Higgins, A A Nogiwa-Valdez, M M Stevens, Nat. Protoc. 172022</p>
<p>Digital research environments: a requirements analysis. S Kanza, Digital Discov. 22023</p>
<p>14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon. K M Jablonka, Digital Discov. 22023</p>
<p>Expanding language-image pretrained models for general video recognition. B Ni, 10.48550/arXiv.2208.028162022Preprint at</p>
<p>Video-LLaVA: learning united visual representation by alignment before projection. B Lin, 10.48550/arXiv.2311.101222023Preprint at</p>
<p>VideoPrism: a foundational visual encoder for video understanding. L Zhao, 10.48550/arXiv.2402.132172024Preprint</p>
<p>How far are we to GPT-4V? Closing the gap to commercial multimodal models with open-source suites. Z Chen, Sci. China Inf. Sci. 672201012024</p>
<p>InternVideo2: scaling foundation models for multimodal video understanding. Y Wang, 10.48550/arXiv.2403.153772024Preprint at</p>
<p>The kinetics human action video dataset. W Kay, 10.48550/arXiv.1705.069502017Preprint at</p>
<p>Application of object detection and action recognition toward automated recognition of chemical experiments. R Sasaki, M Fujinami, H Nakai, Digital Discov. 32024</p>
<p>Action Classification on Kinetics-400. </p>
<p>Human activity recognition in artificial intelligence framework: a narrative review. N Gupta, Artif. Intell. Rev. 552022</p>
<p>A vision transformer for decoding surgeon activity from surgical videos. D Kiyasseh, Nat. Biomed. Eng. 72023</p>
<p>Using foundation models to promote digitization and reproducibility in scientific experimentation. A Thakkar, NeurIPS 2023 AI for Science Workshop. 2023</p>
<p>A multi-fidelity machine learning approach to high throughput materials screening. C Fare, P Fenner, M Benatan, A Varsi, E O Pyzer-Knapp, Comput. Mater. 82022</p>
<p>Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting. D Buterez, J P Janet, S J Kiddle, D Oglic, P Lió, Nat. Commun. 1515172024</p>
<p>Multi-fidelity prediction of molecular optical peaks with deep learning. K P Greenman, W H Green, R Gómez-Bombarelli, Chem. Sci. 132022</p>
<p>Multi-fidelity machine learning models for structure-property mapping of organic electronics. C.-H Yang, Comput. Mater. Sci. 2131115992022</p>
<p>Learning properties of ordered and disordered materials from multi-fidelity data. C Chen, Y Zuo, W Ye, X Li, S P Ong, Nat. Comput Sci. 12021</p>
<p>A multi-fidelity information-fusion approach to machine learn and predict polymer bandgap. A Patra, Comput. Mater. Sci. 1721092862020</p>
<p>Multi-fidelity machine learning models for accurate bandgap predictions of solids. G Pilania, J E Gubernatis, T Lookman, Comput. Mater. Sci. 1292017</p>
<p>A review of large language models and autonomous agents in chemistry. M C Ramos, C J Collison, A D White, Chem. Sci. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
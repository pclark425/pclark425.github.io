<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4748 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4748</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4748</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-258968043</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.19165v1.pdf" target="_blank">Strategic Reasoning with Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Strategic reasoning enables agents to cooperate, communicate, and compete with other agents in diverse situations. Existing approaches to solving strategic games rely on extensive training, yielding strategies that do not generalize to new scenarios or games without retraining. Large Language Models (LLMs), with their ability to comprehend and generate complex, context-rich language, could prove powerful as tools for strategic gameplay. This paper introduces an approach that uses pretrained LLMs with few-shot chain-of-thought examples to enable strategic reasoning for AI agents. Our approach uses systematically generated demonstrations of reasoning about states, values, and beliefs to prompt the model. Using extensive variations of simple matrix games, we show that strategies that are derived based on systematically generated prompts generalize almost perfectly to new game structures, alternate objectives, and hidden information. Additionally, we demonstrate our approach can lead to human-like negotiation strategies in realistic scenarios without any extra training or fine-tuning. Our results highlight the ability of LLMs, guided by systematic reasoning demonstrations, to adapt and excel in diverse strategic scenarios.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4748.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4748.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StrategicPrompt-CoDV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Strategic Prompt (search + value assignment + belief tracking) applied to code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured chain-of-thought prompting method that decomposes strategic reasoning into explicit search, value-assignment, and belief-tracking steps and supplies few-shot in-context demonstrations to LLMs (used primarily with code-davinci-002).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A code-focused large language model (Codex family) used here in instruction/few-shot settings, sampled greedily (temperature=0) for deterministic responses.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Structured strategic chain-of-thought (search + value + belief)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single, structured chain-of-thought prompt that demonstrates explicit search over opponent actions, natural-language value computation for leaf nodes, and belief formation over hidden states; provided as few-shot in-context examples to bias the LM's single reasoning trace.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Matrix games, Deal-or-No-Deal negotiation, partially observable sequential games</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Strategic reasoning tasks including 2x2 and larger matrix games (simultaneous and sequential), negotiation (Deal or No Deal split-items), and partially observable games requiring inference of hidden states or opponent values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Matrix games (generalization to new payoffs): accuracy 100% (35/35) on 2x2 simultaneous; 93% (28/30) on larger-action mixed-format games; 100% (15/15) on multi-player expansions. Negotiation (human study as negotiator): agent mean reward 7.6 vs 'no-belief' 6.7 and few-shot baseline 4.8. Belief/veracity task (communication setting): strategic-with-beliefs 1.00 vs no-belief 0.66 (33/50) on an evaluated set quoted in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>2-shot baseline (code-davinci-002): 0.37 (13/35) on the 2x2 simultaneous generalization set; text-davinci-003 2-shot: 0.66 (23/35); 0-shot chain-of-thought and few-shot baselines were substantially worse across these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing structured in-context demonstrations that explicitly show search, value assignment, and belief-tracking (a single, detailed CoT) substantially improves LLM accuracy and generalization across new game structures, objectives, and partial observability compared to vanilla few-shot or 0-shot CoT baselines; this single-chain structured CoT enabled near-perfect generalization in many matrix game settings and produced human-like negotiator behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Scaling to very large game trees can fail when presented monolithically as a single CoT due to context-window limits; performance degrades without additional factoring (see factored reasoning entry). Ablations removing search/value/belief components reduce reliability (i.e., success depends on these components).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4748.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4748.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FactoredReasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factored Reasoning (search & calculate tools / dispatch to subcontexts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular prompting approach that factors large strategic reasoning tasks into subproblems: a 'search' tool that spawns specialized LM contexts for subtrees and a 'calculate' tool for deterministic operations (mean, argmax), enabling recursion and context-window management.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (primary evaluation) and other text models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LM families as above; reasoning is factored by invoking separate LM contexts (or tool-like primitives) for subsearch and arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Factored search + calculate (modular / recursive)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Decomposes reasoning into multiple LM calls (different contexts) and simple calculator calls: 'search(agent, ...)' spawns specialized contexts to explore subtrees; 'calculate' performs deterministic ops. This produces multiple modular reasoning traces (diverse reasoning paths across subproblems) rather than one long monolithic chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Large/complex matrix games (many actions/players) and iterated strategic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Games whose full tree or long reasoning traces would exceed a single context window, including larger-action matrices and multi-stage games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When applied, factored reasoning restored generalization on large games: described in paper as achieving near-perfect generalization where monolithic prompts failed (e.g., strategic/factored prompts achieved 1.00 accuracy on many expanded settings where unfactored approaches degraded).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Unfactored monolithic reasoning/vanilla CoT: accuracy drops markedly as game size increases (examples: code-davinci-002 2-shot drops to 0.43/0.20 depending on task), whereas factored approach retained ~100% in reported evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Factoring reasoning into multiple, targeted contexts (i.e., creating diverse, modular reasoning subtraces) overcomes context-size limits and enables LLMs to generalize to much larger game trees and more players/actions; modular/diverse reasoning is essential for scaling strategic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No direct cases reported where factoring reduced performance relative to monolithic structured CoT; however, the method requires engineering (tool prompts / dispatch) and appropriate subtask design; authors note complexity in implementing retrieval or extended context solutions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4748.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4748.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IteratedProposals</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search with Proposals / Iterative Proposal-and-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative reasoning pattern where the model proposes candidate actions/deals, evaluates them (search + value), and revises proposals a fixed number of times, shown in prompts to encourage action refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (primary), applied to Deal-or-No-Deal broker and player roles</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Executed as a deterministic greedy LM with prompts that illustrate multiple proposal attempts and explicit evaluation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Iterative proposal-and-revision</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The LLM is shown demonstrations that perform a sequence of propose → evaluate → revise steps (multiple candidate reasoning traces). Diversity arises from trying multiple candidate proposals and keeping track of evaluations to select the best.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Deal-or-No-Deal negotiation (broker fairness optimization and player negotiation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Negotiation over discrete items with hidden opponent utilities; broker must propose fair splits given a fairness objective (equality or Rawlsian), players must propose/accept deals over limited rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Broker fairness optimization: iterated (proposal+revision) structure 'more than halved the error rate' compared to a model without iterative capability (paper statement). As negotiator (player) in human study, the full agent (with iterative reasoning + beliefs) achieved mean reward 7.6 vs few-shot baseline 4.8 and 'no-belief' 6.7; humans also received slightly higher reward when paired with this agent (6.30 vs 6.21).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Non-iterative (single-proposal) model or no-iteration ablation: substantially worse fairness (error more than doubled relative to iterated method) and lower negotiated outcomes in tests described qualitatively; few-shot baseline and no-belief variants had lower mean rewards in human study (4.8 and 6.7 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative proposal-and-revision (multiple attempted reasoning traces) improves search in large combinatorial negotiation spaces, leading to better optimization of fairness objectives and improved negotiation outcomes versus single-shot proposal methods.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Exact numerical fairness-table values are not fully enumerated in text, but authors caution iterative reasoning does not guarantee global optimality in arbitrarily large combinatorial spaces; effectiveness depends on demonstration quality and number of allowed proposals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4748.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4748.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-Cascade-LevelK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM Cascade for Iterated (Level-k) Strategic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cascade of LM contexts where each stage takes the previous stage's predicted opponent action as input and reasons about the next level of best response (implements recursive theory-of-mind / level-k reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (cascade of LM contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multiple sequential LM invocations (cascade) each running structured reasoning conditioned on earlier predicted actions to simulate higher-order reasoning (level-1, level-2, ...).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>LM-cascade (iterated level-k reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Creates diversity by chaining separate LM reasoning contexts: level-1 predicts naive opponent actions, level-2 conditions on level-1 predictions to compute best responses, etc. Each level is a distinct reasoning pass producing potentially different traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Matrix games against level-2 opponents (iterated strategic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>2x2 matrix games where the opponent is a level-2 reasoner (expects the agent to reason about it and best-respond accordingly).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Cascade generalizes successfully to level-2 opponents (paper reports success on evaluated 2x2 tasks; see Table 5 reference). The non-cascade, single-context combined prompt also performed well on the specific evaluated level-2 tasks, but authors note the cascade is expected to scale better as required reasoning depth increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>No-cascade single-context baseline: performed acceptably on level-2 tasks in the paper's experiments but authors anticipate degradation for higher levels; explicit numeric comparison for higher levels not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chaining LMs into a cascade (diverse, multistage reasoning) enables pragmatic implementation of recursive level-k reasoning and generalizes to level-2 opponents; modular cascades are more robust for increasing recursion depth than attempting to encode multiple levels in a single context.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For the low-depth (level-2) problems evaluated, the no-cascade baseline 'also performs well' (qualitative); authors caution that without cascading, performance likely degrades for higher-level recursion.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4748.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4748.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VanillaBaselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla baselines: 0-shot, few-shot, and 0-shot Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common prompting baselines where the LM is asked to output actions directly (0-shot or few-shot) or with an unstructured 'Let's think step by step' CoT, without the structured search/value/belief decomposition used by the strategic prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002, text-davinci-002, text-davinci-003, text-curie-001 (evaluated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard instruction-tuned text LMs (text-davinci family and curie) and code-davinci-002, sampled with greedy decoding; used as baseline comparators.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>0-shot / few-shot / 0-shot CoT (unstructured)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single output pass from the LM with no structured demonstration of search/value/belief; 0-shot CoT uses generic 'let's think step by step' phrasing rather than systematic demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same matrix games and negotiation tasks as above</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Strategic reasoning tasks: predicting best responses in matrix games, negotiation optimization, belief-based hidden-state reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples: code-davinci-002 2-shot on 2x2 simultaneous generalization: 0.37 (13/35); code-davinci-002 2-shot on larger/multi-stage: 0.43 (13/30) and 0.20 (3/15) on certain expansions; text-davinci-003 2-shot: 0.66 (23/35) on 2x2 set. Generally substantially worse than the strategic structured prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Structured strategic prompt (see StrategicPrompt-CoDV2) achieved 1.00 (35/35) on 2x2 simultaneous where 2-shot was 0.37; factored reasoning and iterative methods further closed scaling gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla few-shot and generic CoT baselines are often insufficient for reliable strategic generalization; they underperform compared to structured strategic prompts and factored/diverse reasoning approaches across the evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>In a few cases (e.g., some small tasks and some models like text-curie-001 on limited settings), simpler baselines had non-negligible performance (paper reports some baseline strengths), but they did not match the robustness and generalization of the structured methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4748.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4748.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BeliefTracking-vs-NoBelief</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit Belief-Tracking Prompts vs No-Belief Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison between prompts that explicitly demonstrate forming beliefs over hidden states/values (searching through hidden states and aggregating evidence) and ablations that omit explicit belief formation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (evaluated), other text models tested</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction/few-shot LMs prompted to either perform explicit belief searches over hidden states (strategic-with-beliefs) or to skip belief modeling (no-belief ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Belief-tracking (explicit searching over hidden states) vs No-belief</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Belief-tracking: prompts enumerate hidden states, evaluate opponent actions conditioned on each, and compare to form posterior-like beliefs; No-belief: action selection without this hidden-state search. Belief-tracking creates multiple hypothesized reasoning traces (one per hidden state), so it is 'diverse' relative to no-belief.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Partially observable matrix games and communication veracity tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Choose best responses when world state or opponent values are hidden and must be inferred from observed opponent actions and payoffs; includes tasks where opponent may lie about intended actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports: strategic-with-beliefs outperforms no-belief; example (communication veracity): strategic 1.00 (50/50 cases) vs no-belief 0.66 (33/50); other hidden-state experiments similarly show higher accuracy for belief-enabled prompts (see Table 4 references).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>No-belief ablation: lower accuracy (example above 0.66 vs 1.00); in negotiation human study the 'no-belief' agent achieved lower mean reward (6.7) than full belief-enabled agent (7.6).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit belief-tracking (enumerating hidden states and conditioning on opponent actions) meaningfully improves decision accuracy in partially observable strategic settings and yields more human-like negotiator behavior; forming beliefs via multiple hypothesized traces is effective.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No direct counterexample where omitting belief improved overall performance; authors note that belief-tracking increases prompt complexity and relies on evidence accumulation over interactions to be effective.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language model cascades <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4748",
    "paper_id": "paper-258968043",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "StrategicPrompt-CoDV2",
            "name_full": "Structured Strategic Prompt (search + value assignment + belief tracking) applied to code-davinci-002",
            "brief_description": "A structured chain-of-thought prompting method that decomposes strategic reasoning into explicit search, value-assignment, and belief-tracking steps and supplies few-shot in-context demonstrations to LLMs (used primarily with code-davinci-002).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex)",
            "model_description": "A code-focused large language model (Codex family) used here in instruction/few-shot settings, sampled greedily (temperature=0) for deterministic responses.",
            "reasoning_method_name": "Structured strategic chain-of-thought (search + value + belief)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single, structured chain-of-thought prompt that demonstrates explicit search over opponent actions, natural-language value computation for leaf nodes, and belief formation over hidden states; provided as few-shot in-context examples to bias the LM's single reasoning trace.",
            "task_name": "Matrix games, Deal-or-No-Deal negotiation, partially observable sequential games",
            "task_description": "Strategic reasoning tasks including 2x2 and larger matrix games (simultaneous and sequential), negotiation (Deal or No Deal split-items), and partially observable games requiring inference of hidden states or opponent values.",
            "performance": "Matrix games (generalization to new payoffs): accuracy 100% (35/35) on 2x2 simultaneous; 93% (28/30) on larger-action mixed-format games; 100% (15/15) on multi-player expansions. Negotiation (human study as negotiator): agent mean reward 7.6 vs 'no-belief' 6.7 and few-shot baseline 4.8. Belief/veracity task (communication setting): strategic-with-beliefs 1.00 vs no-belief 0.66 (33/50) on an evaluated set quoted in paper.",
            "comparison_with_other_method": true,
            "performance_other_method": "2-shot baseline (code-davinci-002): 0.37 (13/35) on the 2x2 simultaneous generalization set; text-davinci-003 2-shot: 0.66 (23/35); 0-shot chain-of-thought and few-shot baselines were substantially worse across these tasks.",
            "key_findings": "Providing structured in-context demonstrations that explicitly show search, value assignment, and belief-tracking (a single, detailed CoT) substantially improves LLM accuracy and generalization across new game structures, objectives, and partial observability compared to vanilla few-shot or 0-shot CoT baselines; this single-chain structured CoT enabled near-perfect generalization in many matrix game settings and produced human-like negotiator behavior.",
            "counter_examples_or_negative_results": "Scaling to very large game trees can fail when presented monolithically as a single CoT due to context-window limits; performance degrades without additional factoring (see factored reasoning entry). Ablations removing search/value/belief components reduce reliability (i.e., success depends on these components).",
            "uuid": "e4748.0"
        },
        {
            "name_short": "FactoredReasoning",
            "name_full": "Factored Reasoning (search & calculate tools / dispatch to subcontexts)",
            "brief_description": "A modular prompting approach that factors large strategic reasoning tasks into subproblems: a 'search' tool that spawns specialized LM contexts for subtrees and a 'calculate' tool for deterministic operations (mean, argmax), enabling recursion and context-window management.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (primary evaluation) and other text models",
            "model_description": "Same LM families as above; reasoning is factored by invoking separate LM contexts (or tool-like primitives) for subsearch and arithmetic.",
            "reasoning_method_name": "Factored search + calculate (modular / recursive)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Decomposes reasoning into multiple LM calls (different contexts) and simple calculator calls: 'search(agent, ...)' spawns specialized contexts to explore subtrees; 'calculate' performs deterministic ops. This produces multiple modular reasoning traces (diverse reasoning paths across subproblems) rather than one long monolithic chain.",
            "task_name": "Large/complex matrix games (many actions/players) and iterated strategic reasoning",
            "task_description": "Games whose full tree or long reasoning traces would exceed a single context window, including larger-action matrices and multi-stage games.",
            "performance": "When applied, factored reasoning restored generalization on large games: described in paper as achieving near-perfect generalization where monolithic prompts failed (e.g., strategic/factored prompts achieved 1.00 accuracy on many expanded settings where unfactored approaches degraded).",
            "comparison_with_other_method": true,
            "performance_other_method": "Unfactored monolithic reasoning/vanilla CoT: accuracy drops markedly as game size increases (examples: code-davinci-002 2-shot drops to 0.43/0.20 depending on task), whereas factored approach retained ~100% in reported evaluations.",
            "key_findings": "Factoring reasoning into multiple, targeted contexts (i.e., creating diverse, modular reasoning subtraces) overcomes context-size limits and enables LLMs to generalize to much larger game trees and more players/actions; modular/diverse reasoning is essential for scaling strategic reasoning.",
            "counter_examples_or_negative_results": "No direct cases reported where factoring reduced performance relative to monolithic structured CoT; however, the method requires engineering (tool prompts / dispatch) and appropriate subtask design; authors note complexity in implementing retrieval or extended context solutions.",
            "uuid": "e4748.1"
        },
        {
            "name_short": "IteratedProposals",
            "name_full": "Search with Proposals / Iterative Proposal-and-Refinement",
            "brief_description": "An iterative reasoning pattern where the model proposes candidate actions/deals, evaluates them (search + value), and revises proposals a fixed number of times, shown in prompts to encourage action refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (primary), applied to Deal-or-No-Deal broker and player roles",
            "model_description": "Executed as a deterministic greedy LM with prompts that illustrate multiple proposal attempts and explicit evaluation steps.",
            "reasoning_method_name": "Iterative proposal-and-revision",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "The LLM is shown demonstrations that perform a sequence of propose → evaluate → revise steps (multiple candidate reasoning traces). Diversity arises from trying multiple candidate proposals and keeping track of evaluations to select the best.",
            "task_name": "Deal-or-No-Deal negotiation (broker fairness optimization and player negotiation)",
            "task_description": "Negotiation over discrete items with hidden opponent utilities; broker must propose fair splits given a fairness objective (equality or Rawlsian), players must propose/accept deals over limited rounds.",
            "performance": "Broker fairness optimization: iterated (proposal+revision) structure 'more than halved the error rate' compared to a model without iterative capability (paper statement). As negotiator (player) in human study, the full agent (with iterative reasoning + beliefs) achieved mean reward 7.6 vs few-shot baseline 4.8 and 'no-belief' 6.7; humans also received slightly higher reward when paired with this agent (6.30 vs 6.21).",
            "comparison_with_other_method": true,
            "performance_other_method": "Non-iterative (single-proposal) model or no-iteration ablation: substantially worse fairness (error more than doubled relative to iterated method) and lower negotiated outcomes in tests described qualitatively; few-shot baseline and no-belief variants had lower mean rewards in human study (4.8 and 6.7 respectively).",
            "key_findings": "Iterative proposal-and-revision (multiple attempted reasoning traces) improves search in large combinatorial negotiation spaces, leading to better optimization of fairness objectives and improved negotiation outcomes versus single-shot proposal methods.",
            "counter_examples_or_negative_results": "Exact numerical fairness-table values are not fully enumerated in text, but authors caution iterative reasoning does not guarantee global optimality in arbitrarily large combinatorial spaces; effectiveness depends on demonstration quality and number of allowed proposals.",
            "uuid": "e4748.2"
        },
        {
            "name_short": "LM-Cascade-LevelK",
            "name_full": "LM Cascade for Iterated (Level-k) Strategic Reasoning",
            "brief_description": "A cascade of LM contexts where each stage takes the previous stage's predicted opponent action as input and reasons about the next level of best response (implements recursive theory-of-mind / level-k reasoning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (cascade of LM contexts)",
            "model_description": "Multiple sequential LM invocations (cascade) each running structured reasoning conditioned on earlier predicted actions to simulate higher-order reasoning (level-1, level-2, ...).",
            "reasoning_method_name": "LM-cascade (iterated level-k reasoning)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Creates diversity by chaining separate LM reasoning contexts: level-1 predicts naive opponent actions, level-2 conditions on level-1 predictions to compute best responses, etc. Each level is a distinct reasoning pass producing potentially different traces.",
            "task_name": "Matrix games against level-2 opponents (iterated strategic reasoning)",
            "task_description": "2x2 matrix games where the opponent is a level-2 reasoner (expects the agent to reason about it and best-respond accordingly).",
            "performance": "Cascade generalizes successfully to level-2 opponents (paper reports success on evaluated 2x2 tasks; see Table 5 reference). The non-cascade, single-context combined prompt also performed well on the specific evaluated level-2 tasks, but authors note the cascade is expected to scale better as required reasoning depth increases.",
            "comparison_with_other_method": true,
            "performance_other_method": "No-cascade single-context baseline: performed acceptably on level-2 tasks in the paper's experiments but authors anticipate degradation for higher levels; explicit numeric comparison for higher levels not provided.",
            "key_findings": "Chaining LMs into a cascade (diverse, multistage reasoning) enables pragmatic implementation of recursive level-k reasoning and generalizes to level-2 opponents; modular cascades are more robust for increasing recursion depth than attempting to encode multiple levels in a single context.",
            "counter_examples_or_negative_results": "For the low-depth (level-2) problems evaluated, the no-cascade baseline 'also performs well' (qualitative); authors caution that without cascading, performance likely degrades for higher-level recursion.",
            "uuid": "e4748.3"
        },
        {
            "name_short": "VanillaBaselines",
            "name_full": "Vanilla baselines: 0-shot, few-shot, and 0-shot Chain-of-Thought",
            "brief_description": "Common prompting baselines where the LM is asked to output actions directly (0-shot or few-shot) or with an unstructured 'Let's think step by step' CoT, without the structured search/value/belief decomposition used by the strategic prompt.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002, text-davinci-002, text-davinci-003, text-curie-001 (evaluated in paper)",
            "model_description": "Standard instruction-tuned text LMs (text-davinci family and curie) and code-davinci-002, sampled with greedy decoding; used as baseline comparators.",
            "reasoning_method_name": "0-shot / few-shot / 0-shot CoT (unstructured)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single output pass from the LM with no structured demonstration of search/value/belief; 0-shot CoT uses generic 'let's think step by step' phrasing rather than systematic demonstrations.",
            "task_name": "Same matrix games and negotiation tasks as above",
            "task_description": "Strategic reasoning tasks: predicting best responses in matrix games, negotiation optimization, belief-based hidden-state reasoning.",
            "performance": "Examples: code-davinci-002 2-shot on 2x2 simultaneous generalization: 0.37 (13/35); code-davinci-002 2-shot on larger/multi-stage: 0.43 (13/30) and 0.20 (3/15) on certain expansions; text-davinci-003 2-shot: 0.66 (23/35) on 2x2 set. Generally substantially worse than the strategic structured prompt.",
            "comparison_with_other_method": true,
            "performance_other_method": "Structured strategic prompt (see StrategicPrompt-CoDV2) achieved 1.00 (35/35) on 2x2 simultaneous where 2-shot was 0.37; factored reasoning and iterative methods further closed scaling gaps.",
            "key_findings": "Vanilla few-shot and generic CoT baselines are often insufficient for reliable strategic generalization; they underperform compared to structured strategic prompts and factored/diverse reasoning approaches across the evaluated tasks.",
            "counter_examples_or_negative_results": "In a few cases (e.g., some small tasks and some models like text-curie-001 on limited settings), simpler baselines had non-negligible performance (paper reports some baseline strengths), but they did not match the robustness and generalization of the structured methods.",
            "uuid": "e4748.4"
        },
        {
            "name_short": "BeliefTracking-vs-NoBelief",
            "name_full": "Explicit Belief-Tracking Prompts vs No-Belief Ablation",
            "brief_description": "Comparison between prompts that explicitly demonstrate forming beliefs over hidden states/values (searching through hidden states and aggregating evidence) and ablations that omit explicit belief formation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (evaluated), other text models tested",
            "model_description": "Instruction/few-shot LMs prompted to either perform explicit belief searches over hidden states (strategic-with-beliefs) or to skip belief modeling (no-belief ablation).",
            "reasoning_method_name": "Belief-tracking (explicit searching over hidden states) vs No-belief",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Belief-tracking: prompts enumerate hidden states, evaluate opponent actions conditioned on each, and compare to form posterior-like beliefs; No-belief: action selection without this hidden-state search. Belief-tracking creates multiple hypothesized reasoning traces (one per hidden state), so it is 'diverse' relative to no-belief.",
            "task_name": "Partially observable matrix games and communication veracity tasks",
            "task_description": "Choose best responses when world state or opponent values are hidden and must be inferred from observed opponent actions and payoffs; includes tasks where opponent may lie about intended actions.",
            "performance": "Paper reports: strategic-with-beliefs outperforms no-belief; example (communication veracity): strategic 1.00 (50/50 cases) vs no-belief 0.66 (33/50); other hidden-state experiments similarly show higher accuracy for belief-enabled prompts (see Table 4 references).",
            "comparison_with_other_method": true,
            "performance_other_method": "No-belief ablation: lower accuracy (example above 0.66 vs 1.00); in negotiation human study the 'no-belief' agent achieved lower mean reward (6.7) than full belief-enabled agent (7.6).",
            "key_findings": "Explicit belief-tracking (enumerating hidden states and conditioning on opponent actions) meaningfully improves decision accuracy in partially observable strategic settings and yields more human-like negotiator behavior; forming beliefs via multiple hypothesized traces is effective.",
            "counter_examples_or_negative_results": "No direct counterexample where omitting belief improved overall performance; authors note that belief-tracking increases prompt complexity and relies on evidence accumulation over interactions to be effective.",
            "uuid": "e4748.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language model cascades",
            "rating": 2,
            "sanitized_title": "language_model_cascades"
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        }
    ],
    "cost": 0.019859,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Strategic Reasoning with Language Models</p>
<p>Kanishk Gandhi kanishk.gandhi@stanford.edu 
Stanford University</p>
<p>Dorsa Sadigh 
Stanford University</p>
<p>Noah D Goodman 
Stanford University</p>
<p>Strategic Reasoning with Language Models</p>
<p>Strategic reasoning enables agents to cooperate, communicate, and compete with other agents in diverse situations. Existing approaches to solving strategic games rely on extensive training, yielding strategies that do not generalize to new scenarios or games without retraining. Large Language Models (LLMs), with their ability to comprehend and generate complex, context-rich language, could prove powerful as tools for strategic gameplay. This paper introduces an approach that uses pretrained LLMs with few-shot chain-of-thought examples to enable strategic reasoning for AI agents. Our approach uses systematically generated demonstrations of reasoning about states, values, and beliefs to prompt the model. Using extensive variations of simple matrix games, we show that strategies that are derived based on systematically generated prompts generalize almost perfectly to new game structures, alternate objectives, and hidden information. Additionally, we demonstrate our approach can lead to human-like negotiation strategies in realistic scenarios without any extra training or fine-tuning. Our results highlight the ability of LLMs, guided by systematic reasoning demonstrations, to adapt and excel in diverse strategic scenarios.</p>
<p>Introduction</p>
<p>Advances in game-playing AI, for games such as Chess, Go [31,32], Poker [17,7], and more recently diplomacy [3,2,13], have shown how algorithms trained with a combination of imitation, reinforcement learning, and planning can be used for strategic reasoning. Despite the increasing sophistication of these models, they are limited in their ability to generalize to new strategic scenarios because they are often extensively and solely trained on a given target game. Humans, on the other hand, can readily adapt to new scenarios: changes to the rules of a game (in chess, where the rules are changed so that the knight moves in a line); different styles of play (an expert player can be asked to play aggressively-like a sacrificial player such as Mikhail Tal-or with a low-risk approach-a positional player like Karpov); alternate goals (such as achieving the lowest score or even with some arbitrary definition of score-the first person to give a check wins the game) [15].</p>
<p>This paper aims to address the limitation of existing AI algorithms in their ability to adapt to new contexts by exploring the potential of language models to engage in strategic reasoning-the ability to foresee potential actions of others in the pursuit of possibly conflicting objectives, and to devise optimal strategies accordingly. This concept, central to game theory, entails reasoning about the interplay between multiple agents with divergent interests. Large language models (LLMs) have recently been shown to express human-like strategies [1,14] and flexibility in reasoning, potentially understanding nuanced and contextual information [35]. Further, since language models are trained on a variety of data sources, they can be adapted to different tasks and environments, making them suitable for flexible reasoning and potentially generalization to new scenarios. Despite these successes, LLMs can however be brittle and unreliable in their reasoning, especially when reasoning about agents, social contexts [28] and planning [37].</p>
<p>To enable LLMs to reason strategically, flexibly and reliably, we propose an approach that shows models how to search through states, evaluate actions and their effects, and form beliefs by using systematically generated demonstrations of strategic reasoning. The LLM can then generalize to new scenarios through few-shot in-context examples of these systematically generated prompts. To capture human-like strategic reasoning, an agent needs to 1) search through the space of states and actions: for example, a bot that negotiates with a vendor must understand the space of inventory and how its offers will affect the negotiation, 2) assign values to these states and actions: the bot must understand which items are valuable to it, and what the vendor values, 3) form beliefs about the partially-observable world: based on the vendor's actions, the bot must infer how much the vendor values the items. We develop an automated "prompt compiler" that can be used to systematically generate these demonstrations. The demonstrations generated by our compiler structure the chain-ofthought reasoning prompts [21,39] that bias the language model towards more flexible reasoning before selecting an action.</p>
<p>In the remainder of the paper, we first present our approach for a prompt compiler that systematically generates strategic reasoning prompts. We then evaluate it through experiments with matrix games and negotiation games. We use scenarios with new game structures, alternate objectives, partial information, and communication to demonstrate the model's ability to flexibly search, assign values and form beliefs in new contexts. Finally, we apply our approach to a realistic negotiation task [16] and create a human-like negotiation agent without any prior training. Our results demonstrate the potential of language models for flexible strategic reasoning and their ability to generalize to new scenarios with few or no additional examples.</p>
<p>Related Work</p>
<p>We review literature in two complementary areas: the progress of strategic reasoning with AI agents, particularly in game-playing environments, and the advancement of reasoning capabilities in LLMs.</p>
<p>Strategic Reasoning with AI Agents. Progress in game-playing AI, driven by breakthroughs in reinforcement learning (RL), self-play, and integrating them with tree search have led to successful strategic agents for Chess, Go, Starcraft, Poker and DOTA [31,32,38,17,5]. These approaches showed how self-play and reinforcement learning (RL) could help create policies that outperform humans. However, they were limited in producing agents that were adept at adapting to novel situations, such as new rules or objectives [15]. Recently, Cicero [2] demonstrated how language models could be used to create versatile agents capable of interacting and negotiating with humans through dialogue, by combining techniques in strategic reasoning and language modelling to create a dialogue-based agent that could play Diplomacy. Despite their success, Cicero employed a separate strategic planner, using an LLM only for translating predictions to dialogue (and some implicit planning). This limits the agent's flexibility; it would have to be retrained to be successful in a new version of the game or if a rule in the game changed. In this paper, we explore how we can guide the language model through systematic prompts so that it can be used as a flexible strategic planner.</p>
<p>Reasoning with Language Models. Large language models have been shown to be successful at reasoning [35] in a variety of contexts, especially when paired with prompting techniques that allow them to think through their steps [21,39]. Other techniques have shown how reasoning can further be improved by breaking down the steps in a problem [12], combining language models as modules or cascades [10,34], by fine-tuning/ bootstrapping [40,36] its own reasoning and by tuning through human feedback [24]. In spite of these successes, LLMs are limited in their reasoning about agents [28] and there have been few attempts to apply these models to complex strategic reasoning tasks. In this paper, we draw on these advances to amplify the reasoning capabilities of LLMs and design a method that systematically prompts them to be flexible, reliable strategic agents.</p>
<p>Strategic Reasoning with Language Models</p>
<p>The key idea of our method is to guide the decision making of language models towards better strategic reasoning and enable them to generalize reliably, and flexibly to new domains. We provide a systematic approach for generating prompts that incorporate structure based on common strategic reasoning techniques. Specifically, we will consider prompting strategies based on search, value assignment, and belief tracking. To demonstrate our approach and prompting strategies, let us first describe two game settings that we study in this work: matrix games and negotiation games. These games can be represented in the form of a multi-agent MDP. For a game with N agents, S represents the set of world states, A = {A i , ∀i ∈ N } represents the set of actions taken by each agent, R i : S × A 1 × · · · × A N → R is the set of rewards, and P : S × A 1 × · · · × A N → S is the search beliefs value (a) Game with hidden states: The world can be in states s 1 or s 2 . Gopher cannot see these states but Bob can. Bob plays an action (b 1 or b 2 ) after observing the state of the world. Gopher chooses an action (a 1 or a 2 ) after looking at Bob's action. Rewards for the game are shown at the bottom of the tree.   Figure 1: A partially observable game with two players Gopher and Bob is shown in (a), and the corresponding demonstration for strategic reasoning is shown in (b).</p>
<p>transition probability matrix. In addition, in a partially observable setting, parts of the state space or the rewards are not accessible to some or all of the agents.</p>
<p>Matrix Games: Game theorists have devised a vast array of simple matrix games to exemplify rational strategic behavior; these games involve players acting to maximize self-interest while possessing perfect foresight of others' strategies. Insights from these games have found applications in diverse fields such as economics, politics, and evolutionary biology. In matrix games, the state space, action space and rewards are defined by the size and values of the payoff matrix; a 2 × 2 matrix game has 2 players, where each player can take two actions. We vary the complexity of matrix games by changing the rewards, the objectives of the players, the action space, the number of players, the observability of the game and by adding multiple turns to a game.</p>
<p>Negotiation Games: In the negotiation setting, we use the Deal or No Deal game [16]. Each episode in this game has two players that must split a pot of three types of items (hats, books and balls) with each other. The players value these items differently and do not know the other player's values. Across episodes, the number of each of the items and the players' values for the items is varied.</p>
<p>Incorporating Structure in Strategic Reasoning Prompts</p>
<p>To enable a large language model to solve strategic reasoning games and generalize to its new instances, we propose an in-context learning approach, where demonstrations of solving strategic games are provided in the prompt. Our insight is that this prompt needs to incorporate structure for reasoning about the strategic games. enabling LLMs to reliably leverage cognitive tools useful for strategic reasoning such as search, value assignment, and belief modeling.</p>
<p>Our prompt compiler takes an example game and automatically generates an example of strategic reasoning. The compiler decomposes this example into components for search, value assignment, and belief tracking. In matrix games, the examples are generated for simple and restricted games allowing for systematically testing for generalization capabilities to novel scenarios. On the other hand, in the negotiation domain, the demonstrations are provided by a human player. These human demonstrations are then annotated with reasoning steps corresponding to search, value assignment and belief tracking by an expert demonstrator. In both settings, the compiler and the annotations leverage the decomposed games as contextual prompts to steer the behavior of the language model in a new strategic game. In Fig. 1, when the state of the world, hearts or spades is hidden from the player, we show how the reasoning is decomposed into search, value and belief formation to find the best response.</p>
<p>Prompting Strategy: Search. Searching flexibly is necessary for generalizing to changes in parts of a game, such as its rules, the number of players, or the action space. When possible, we perform an exhaustive search by exploring the values of all leaf nodes 2 in the game tree 3 (Alg. 1:3,9). For example, in a matrix game a player must iterate through the actions of the other players to find Algorithm 1: Exhaustive Search Input :game-tree Output :prompt 1 opp-actions ← opponent's actions from game-tree; 2 for action in opp-actions do 3 leaves ← get leaves for action ; 4 prompt ← action has expected-value for leaves; 5 end 6 prompt ← compare actions to get best one; 7 subtree ← game-tree if opponent takes best action; 8 for action in actions do 9 leaves ← get leaves for action in subtree ; 10 prompt ← action has expected-value for leaves; 11 end 12 prompt ← compare actions to get best response; Algorithm 2: Beliefs over hidden states Input :game-tree, opponent-action Output :prompt 1 hidden-states ← hidden in game-tree; 2 for hidden-state in hidden-states do 3 subtree ← get the subtree of hidden-state; 4 prompt ← add subtree of hidden-state; 5 value ← get expected value of opponent-action for subtree; 6 prompt ← hidden-state has value for opponent; 7 end 8 prompt ← compare values of hidden-states to form belief;</p>
<p>the right states to evaluate, "If player 1 plays a 1 and player 2 plays a 2 then ...". We automatically generate this prompt (as shown in Fig. 1 b) to demonstrate search (see example prompt: Fig. 6).</p>
<p>Search is conditioned on an opponent model, specifically, the levels of recursive reasoning (theory of mind) that a player performs about other players' actions. So, a naïve player i -a 0 th order theory of mind agent -only searches through the rewards obtained for their own actions:
a * i = argmax ai∈Ai [E s∈S R i (s, a i )]
, assuming that the other players pick their action randomly; since the naïve player does not consider what other agents are thinking. A player j that performs a higher level of reasoning (higher level theory of mind) first simulates the actions that the other player takes and then searches through their own actions to pick the best response a *
j = argmax aj ∈Aj [E s∈S R(s, a j , a * i )].
We consider a naïve opponent model for the matrix games (though see §A.2 for extensions).</p>
<p>As games increase in complexity, larger game trees and longer rule sets often surpass the context window of the LM, leading to less reliable reasoning. To address this, we propose factoring reasoning via two tools [29,34,19,10], 'search' and 'calculate'. Search is a cognitive tool that dispatches to a new language model context with a specialized prompt. This allows the model to recursively search through subtrees of a game by generating "search(agent, other agents, objective, action, other actions)". The calculate tools allow the model to call a simple calculator for either the 'mean' or 'argmax' operations, e.g. via "mean(a 1 =2, a 2 =1)". In-context examples are adjusted to illustrate use of these tools ( §B.2 for complete prompts and details). Factoring in this way can enhance language models' capabilities to tackle complex games by enabling modular, effective search and reasoning.</p>
<p>When the space of exhaustive search is too large, we carry out search with an iterative process of proposal and refinement of candidate actions [18,20,34,30]. In Alg. 3, an action is proposed a ∈ A; the choice of this action reflects both priors learned by the model in pre-training and in-context examples. This action is evaluated, and revised to a new action to try and generate a better candidate action (this relies on in-context demonstrations to show action revision; (Alg. 3: 1-4). This is repeated a predefined number of times which is specified in the demonstrations. For example, a step would look like: " Try 1/N, If we play a 1 , the expected reward would be r 1 . This is low, so we can try again. Try 2/N, We revise our action to a 2 . The reward would be r 2 ..." and so on (see sample prompt Fig. 9).</p>
<p>Prompting Strategy: Value Assignment. Understanding values flexibly is necessary for being able to generalize to new objectives. We alternate search with value assignment (Alg. 1: 3-4, 9-10); assigning values for each (state, action) pair: R i : S × A 1 · · · × A N → R. We provide natural language explanations for the model to understand how values are calculated (Alg. 4: 3-4); for example, "Bob wants to maximize his reward. As a 1 gives a higher reward compared to a 2 , Bob will choose a 1 ". These explanations help in generalizing to a new value presented during the evaluation phase.</p>
<p>Prompting Strategy: Belief Tracking. We estimate beliefs about the hidden information such as values of players or non-observable states by searching through the possible space of hidden information (Fig. 1 b) and by keeping track of evidence in multiple interactions. For example, when the state S is not visible but the other agents' actions A i and rewards R i are observable, we search through the states and form a belief over them based on the actions and rewards of the other agents (see Alg. 2 for pseudocode and Fig. 1 </p>
<p>b for the demonstration). Similarly, if the values of other agents</p>
<p>Algorithm 3: Search with proposals Input :game-tree, num-proposals Output :prompt 1 for attempt in num-proposals do 2 action ← propose or revise action (game-tree); 3 prompt ← action has expected-value(action); 4 end 5 prompt ← compare proposed actions to get best response;  Table 1: Accuracy of predicting the best response in novel games for different prompt structures. We compare our strategic prompt with common baselines and ablations. Numbers in parentheses indicate number of correct choices out of the total number of evaluations.</p>
<p>R i are hidden, we keep track of the actions to form a belief over what the rewards of an agent might be: "Player chose the book over balls 9/10 times, so they must like books more." (Fig. 3 b).</p>
<p>Experiments</p>
<p>Our experiments emphasize the utility of our prompting strategy by exploring the flexibility and reliability granted by its three main components: search §4.1, value assignemnt §4.2, and belief modelling §4.3. See project website 1 for all prompts and evaluations.</p>
<p>Language Models. We evaluate a code-based language model, specifically code-davinci-002 (CODEX [8]), and three text based models, curie-001, davinci-002, davinci-003 4 . We sample greedily from the language model with the most deterministic setting, temperature 0. We allow the LM to sample freely during the "reasoning" phase; when it generates a stop phrase ("agent action:") we then restrict the action space of the agent to valid actions using Synchromesh [26]. We convert the structure and rules of the games into text for the language model.</p>
<p>Baselines. We include the following baselines: 1) a 0-shot and a few-shot baseline where the model directly predicts the action without a scratchpad or chain-of-thought; 2) a 0-shot chain-ofthought baseline with a "Let's think step by step:" instruction [12]; 3) ablations where only specific components of the strategic prompts are used: only search, only value assignment, and search and value assignment together without belief modelling.</p>
<p>Metrics. We analyze performance by measuring the accuracy of choosing the best response.</p>
<p>Searching with Language</p>
<p>In this section, we show how language models guided with demonstrations generated from our method can learn to solve novel game trees reliably, generalizing to new rules, rewards, and structures.</p>
<p>Matrix Games: New Rewards</p>
<p>Experiment Setup. Here, we focus on testing generalization to new reward structures across different types of game structures; sequential games, simultaneours games and complex game formats involving both simultaneous and sequential play. The goal of the players in these games is to maximize their rewards by choosing optimal actions, either simultaneously or sequentially. We assume that the model is playing against a naïve (level-0) player (this is specified in the instruction).</p>
<p>We evaluate language models using simple 2 × 2 matrix games, focusing on single-stage simultaneous / sequential games and two-stage games.  We test games for generalization along two dimensions. First, larger action spaces, including 3 × 3, 4 × 3, 5 × 5, and 6 × 6 matrix games (sup. Fig. 12). As the action space grows, modeling the payoff structure and identifying optimal strategies becomes harder. Second, more players: 3, 4 and 5 player games. Multi-player games introduce more complex dynamics including alliances, competition, and social dilemmas between groups. They require sophisticated reasoning about how different players may interact and influence each other. The payoffs for these games are extended versions of their 2 × 2 versions. For example, we use a 3-way prisoner's dilemma and chicken game to evaluate generalization to games with more players. We still demonstrate strategic reasoning with only two trivial examples of solving 2 × 2 matrix games. So, the model now has to generalize 0-shot to a new game structure and rewards.</p>
<p>Results. As the size of the games increases, the language model is unable to keep up with the complexity of the problem, primarily due to its limited context size (see Fig. 2). When we use factored search and reasoning, this is not a limitation and the model generalizes to the new games perfectly. We see that reasoning from in-context demonstrations allows models to flexibly generalize to games with more players and actions without any demonstrations for the new game structures. This result is promising as it shows that factoring allows us to generalize to much more complex games. We also show how factoring can potentially be used for iterated strategic reasoning in §A.2.</p>
<p>Assigning Values with Language</p>
<p>In this section, we demonstrate how our method for evaluating states and actions in language can give large language models (LLMs) the flexibility to generalize and learn new objectives. We first show how our method can generalize to a new objective without any additional examples, then show how demonstrations can be used to teach the model to optimize for a new objective in a realistic setting.</p>
<p>Matrix Games with Varying Objectives</p>
<p>Experiment Setup. We consider four scenarios of objectives that the players seek to optimize: 1) the player tries to maximize its own reward knowing that the opponent is trying to help them; 2) the player tries to help the opponent maximize their reward given that the opponent is trying to maximize their reward; 3) Both players try to maximize the total welfare; and 4) The players maximize an  Table 2: Accuracy of choosing the best response for a new objective that has never been seen before. The goals in the first column indicate the objectives of the model and its opponent respectively; "max, help" means that the model is trying to maximize its payoff while its opponent is trying to help the model maximize the payoff. Daxity indicates a new objective that we define to be the difference between the payoff received by the player and its opponent.</p>
<p>arbitrary reward ('daxity'), defined as the advantage, which is the difference between their reward and the opponent's. We provide the same demonstrations as in section §4.1.1, where the players' objective is to maximize their reward. The model must generalize to the new objectives in a zero-shot manner; the model only sees demonstrations of players maximizing their payoffs and an instruction describing the new objective. The payoffs used to evaluate are the same as §4.1.1.</p>
<p>Results. Our reasoning structure with steps for inferring values in combination with search is able to generalize to alternate objectives (see Tab. 2). Here, language models can be seen as a powerful tool to optimize arbitrary new objectives. By generalizing to new objectives without re-training, we show that LMs can optimize not just for a fixed reward signal but for abstract goals. The ability to pursue new goals without losing competency is a hallmark of human-level reasoning, and success on this objective-generalization test is a promising sign, suggesting that LMs when prompted strategically, can show adaptable strategic reasoning.  Table 3: Difference from the optimal deal when the broker is optimizing for different fairness objectives. 'Random' represents the expected fairness of a deal when a deal is randomly proposed.  Table 4: Accuracy of predicting the best response when the model needs to form a belief over hidden information in the environment.</p>
<p>Model as a fair broker</p>
<p>Games</p>
<p>Experiment Setup. In this section, we consider a realistic negotiation game where the model acts as a broker and has to optimize various fairness objectives for a successful deal. We consider the Deal or No Deal dataset [16] to test the model. When the model acts as a broker, the model knows the values of each player and must propose a fair deal according to a fairness metric. The model is challenged to explore the complex action space of splitting the pot (where exhaustively searching all options is cumbersome) and adapting to new fairness objectives. We look at two different objectives that the broker has to optimize for: 1) equality: difference in the values that the players receive; 2) Rawlsian fairness: maximizing the minimum value that is given.</p>
<p>We quantify performance by measuring the difference between the fairness of the proposed deal with the optimal deal. The model learns to optimize these objectives through few-shot demonstrations. We show 5 episodes of in-context demonstrations (see §B.3) to teach the model to optimize an objective and teach it the reasoning structure of action proposal, evaluation, and revision. We test generalization to 100 new deals. The performance of the model is measured by the difference in the value of the proposed deal with the maximum possible value. We evaluate the model on 100 negotiation contexts from the Deal or No Deal dataset.</p>
<p>Results. We see (Tab. 3) that the iterated structure of reasoning enables the model to find more fair deals in comparison to a model without this capability, more than halving the error rate. This indicates the importance of iterative reasoning for negotiation models to converge on optimized solutions in complex search and value assignment problems.</p>
<p>Forming Beliefs with Language</p>
<p>We show how LLMs prompted with our method to track beliefs can solve games with a variety of partially observable information, like hidden states, veracity of communication and opponent values.  </p>
<p>Beliefs over States</p>
<p>Experiment Setup. In this section we introduce partial observability in the state of the world. We consider a sequential game where the player (model) cannot see the state of the world but can see the action that their opponent takes, and the payoffs that they receive for different world states. So, the player can observe the opponent's actions and values and must form a belief over the world state from these. We consider three types of game trees; (see Fig. 1) to show how language can be used for forming beliefs about hidden information. We show the prompt structure in Fig. 1. We evaluate their performance based on choosing the best response (response with the highest payoff). To demonstrate reasoning over hidden states of the world, we use 2 examples of games with trivial payoffs (similar to previous sections). We use three game structures with five variations of payoffs to test generalization. The first game structure is the same as the in-context demonstrations but with non-trivial payoffs. In the second structure, the world has three hidden states instead of two. For the final structure, the number of actions that the opponent can take is increased from two to three.</p>
<p>Results. Models with our structures of reasoning outperform baselines, generalizing to new structures (Tab. 4). This demonstrates how language enables models to reason under uncertainty and limited observability, a key capability for strategic reasoning and interaction. The ability to form beliefs and reason under partial observability suggests that language models can understand scenarios where information is imperfect and make inferences from limited signals. This type of flexible reasoning is crucial for interacting in complex, strategic environments. Our method shows one way of imparting this capability by using natural language to guide a model's search over possible hidden states.</p>
<p>(Reasoning over the honesty of an opponent's communication, as in §A.3, provides another example of how language can be used to form beliefs about hidden information based on limited signals.)</p>
<p>Realistic Scenario: Model as a negotiator</p>
<p>Experiment Setup. In this section, we show how we can create a human-like negotiation bot without retraining in a realistic negotiation setting by forming a belief over a human's values through multiple rounds of interaction. We use the Deal or No Deal environment [16] to test our method (Fig. 3). We restrict negotiations to a three turn (6 offers) setting. The human gets a chance to first propose an offer and then agents can propose a counter offer or accept/ reject the offer. When an agent rejects an offer, the player gets no reward. Demonstrations (Fig. 3b) are provided by an expert human player collected while playing against another human. These are then annotated systematically by the player to generate 7 demonstrations. The demonstrations capture the style of the human player.</p>
<p>We evaluate the model on new negotiating partners in settings with new values and item quantities. We consider two baselines to compare our models against: 1) A few shot baseline without chain of thought prompting, 2) prompting with strategic reasoning that does not explicitly form beliefs over the values of the other player. We conducted a user study with 11 human participants that were asked to interact with the three negotiation methods for three episodes each. After interacting with the agents, the participants are asked to fill out a survey, rating the agent on different aspects of their negotiating ability on a 7-point Likert scale. Participants rated how human-like, reasonable, aggressive or compromising an agent was.</p>
<p>Results. Our method outperforms baselines on multiple metrics (Fig. 4), achieving a higher average reward (7.6) compared to the 'no belief' model (6.7) and the few-shot baseline (4.8). Despite our model getting a higher reward, the humans also received a slightly higher reward (6.30 vs 6.21) when negotiating with our method. We are successfully able to build a negotiating agent with no additional training, only through few-shot demonstrations. Further, the agent mimics the style of the human which is reflected in the post-study survey-our agent was consistently rated to be more human-like, reasonable, compromising and less aggressive. This shows how our method of prompting a model to reason strategically in language can enable agents to handle complex, real-world tasks, like negotiation, without requiring large training datasets. With improved reasoning and context, agents like this could have long, open interactions and even match human-level performance on social and strategic tasks. This initial result highlights the exciting potential for AI agents to collaborate with and assist people in new ways.</p>
<p>Discussion</p>
<p>In this paper, we set out to show how language models can be made to reason strategically about other agents by using a structured prompt based on search, value assignment and belief-tracking strategies. Our demonstrations enabled language models to generalize to strategic settings that had novel game rules and structures, new objectives and required forming beliefs over a variety of hidden information. These insights led to a model that negotiated with people in a considerate, human-like manner while maximizing utility.</p>
<p>However, several challenges could arise in using language models for these domains. Complex games may require more demonstrations and instructions to understand the rules and game-specific behaviors; we are limited by the context length of the language model. Language models that rely on retrieval [27,11] could be used here, but extending them would be non-trivial. Complex games also require reasoning over longer time (i.e. more turns) than considered here, which requires much longer reasoning traces where LM performance may degrade. While our experiments on factored modeling are promising, other training strategies could help the model learn to tackle a new task quickly while maintaining flexibility by using language as a medium to "think." For instance, supervised fine-tuning over reasoning trajectories, using rewards to tune the model [6,33], or bootstrapping reasoning based on successful trajectories [40].</p>
<p>We did not focus on finding equilibria, stable states of a game where no player has incentive to deviate from their strategy, in this work. However, the search and best-response strategies we proposed could potentially be extended to level-k reasoning, which converges on equilibrium. A simple extension of our factoring approach implements such recursive reasoning (see Appendix A.2 for an illustration).</p>
<p>We hope our results will inspire further research into using language models for reasoning in complex multi-agent settings such as Diplomacy and Hanabi [25,4]; beyond merely being used for dialogue [2]. Our approach also has potential for modeling human-like behavior to form the basis for more realistic economic policy design, such as agent-based models of markets with human-like agents [41]. These models could help simulate social conventions and agent-specific models of desires and beliefs, enabling more accurate simulations of complex social interactions. We hope our research will motivate further exploration into the use of language models for reasoning in such settings.</p>
<p>LM-1 LM-n In this section, we consider games with sequential actions: the model plays an action, the other player observes this action and then takes an action with the highest payoffs. In this sequential setting, the first player has to search and reason through the opponent's actions, knowing that the opponent can see what they've played.</p>
<p>In-Context Demonstrations. The payoffs used for the in-context demos are the same as §4.1.1 but the structure of reasoning is different as the actions are sequential.</p>
<p>Evaluation Tasks. We evaluate generalization to novel payoffs. We test on sequential versions of the games from §4.1.1.</p>
<p>Results. Similar to the previous section, we observe that prompts based on strategically searching and evaluating actions are more reliable than the baselines. The model is successful at taking the sequential form of play into account and choose the best response.  Table 5: Accuracy of predicting the best response when playing against a level-2 opponent.</p>
<p>A.2 Iterated Strategic Reasoning</p>
<p>Method</p>
<p>We have so far only considered settings where the opponent was naïve and where the opponent did not think about the other players' strategies. Here, we remove this assumption and consider the case where the opponent knows that the player thinks about its action and chooses the best response according to this reasoning (level-2 reasoner). So, the player must iteratively find the best response to this opponent.</p>
<p>Method. To incorporate n-iterations of strategic reasoning through the opponents' choices, we use a cascade or chain of language models (see Fig. 5). For the first iteration, we assume that the opponent is naïve. The predicted action from this stage is passed on to a new language model context for search and reasoning to get the best response for the next level opponent: ''I thought through my opponent's actions and chose action 1. But if my opponent knows that I thought through their actions and that I will play action 1, then ...".</p>
<p>In-Context Demonstrations. We use the same in-context demonstrations from §4.1.1 for the level-1 model, and provide two demonstrations on trivial problems for the level-n model. The demonstrations for the level-n model take into account the action predicted by the model from the previous level. We consider a level-2 reasoner via both a LM cascade and by combining the reasoning prompts for the level-1 and level-2 model in a single context.</p>
<p>Evaluation Tasks. We consider 2 × 2 matrix games from §4.1.1 but the player plays against a level-2 opponent.</p>
<p>Results. The cascade generalizes successfully to a level-2 opponent (Tab. 5). This shows that our prompting strategy combined with the modularity of an LLM-cascade is a powerful tool for an arbitrary level of iterated strategic reasoning. The no-cascade baseline also performs well, though we expect performance would degrade for higher levels of reasoning. The level-1 reasoner is only effective in games where the optimal strategy is the same as that against a level-0 opponent.</p>
<p>A.3 Beliefs of Truthfulness</p>
<p>Here, we consider games where the opponent can communicate with the player and tell them what action they are going to play. The opponent can be faithful to what they say they are going to play or lie about their intentions. The player must infer if their opponent is telling the truth or is lying based on the payoffs, and then choose the best response to their opponents actions. We show how language can help understand and reason over the veracity of communication. Veracity can be seen as a hidden state of the world (the opponent) that the model must infer from the evidence. We use a reasoning structure similar to the previous section. The opponent in our evaluations is naïve, and assumes that the player will believe whatever they are told.</p>
<p>In-Context Demonstrations. We use one demonstration with trivial payoffs to teach the model to form a belief over the honesty of the communication. In this demonstration, the opponent says the truth. We use a second demonstration with trivial payoffs to teach the model how to break ties (same as §4.1.1).</p>
<p>Evaluation Tasks. We test the model on 5 types of 2 × 2 matrix games (cooperation, matching pennies, prisoner's dilemma, chicken, battle of the sexes; see  Table 6: Accuracy of predicting the best response when the model needs to form a belief over hidden information in the environment.</p>
<p>Results. We find that models that search through different states of the world based on the payoffs and actions are able to infer the veracity of the communication reliably. The model is successful in generalizing to new payoff structures and to scenarios where the opponent lies about their intentions.</p>
<p>In general, this section shows how language can be used to form beliefs over hidden mental states.</p>
<p>A.4 Evaluations with other LMs</p>
<p>Justification for choosing code-davinci-002. We chose a model which is state of the art for several long-form reasoning problems [35,39] other than GPT-4 [23] and PALM [9]. Moreover, it is also the best performing model (except for PALM, GPT-4) on mathematics and coding domains; both of which require long-form compositional reasoning similar to our problems [39]. Anecdotally, code-davinci-002 was also more consistent at reasoning compared to text-davinci-002 in our preliminary explorations.</p>
<p>Experiments with Other LMs. To ensure that our prompts hold for other language models, we have run three additional models on our evaluations for search and value assignment. We used text-curie-001 as a small instruction-tuned baseline, text-davinci-002 as an instruction-tuned text model (which uses code-davinci-002 as a base-model) [22], and text-davinci-003 which is an RLHF-tuned version of text-davinci-002. See the results of the models in Tab. 7 and in Tab. 8.</p>
<p>B Demonstration Prompts</p>
<p>Reproducibility We provide the prompts that are used to teach the models. Full prompts used in our experiments are available at the project homepage 5 Table 9: Token statistics for prompts used in different experiments. Tokens are counted based on the code-davinci-002 tokenizer.</p>
<p>Tab. 9. The models we test are hidden behind a paid API and code-davinci-002 is now available only as part of a researcher access program. The parameters of these models, the specifics of training these models, and the data used to train these models are not publicly accessible.</p>
<p>We include snippets of demonstrations here.</p>
<p>B.1 2x2 Matrix Games</p>
<p>See Fig. 6 for one demonstration from our prompt.</p>
<p>B.2 Factoring Reasoning</p>
<p>We have two prompts for factoring reasoning: the recursive case and the base case. The base case is shown in Fig. 7 and the base case is shown in Fig. 8.</p>
<p>B.3 Deal or No Deal: Broker</p>
<p>See Fig. 9 for one demonstration from our prompt for optimizing equality.</p>
<p>B.4 Deal or No Deal: Player</p>
<p>See Fig. 10 for an expert annotated demonstration of a negotiation. C Evaluation Tasks</p>
<p>C.1 Matrix Games</p>
<p>For tasks with 2 × 2 matrix games, we use the matrix games shown in Fig. 11. The games used in our experiments are:</p>
<p>• Prisoner's Dilemma: A game where both players have a dominant strategy of betraying the other, but they would both be better off cooperating. It demonstrates why cooperation can be difficult to achieve.</p>
<p>• Battle of the Sexes: A game where players have different preferences over outcomes, modeling a coordination problem.</p>
<p>• Market Entry: A game where one player must decide whether or not to enter a market based on the likely action of a competitor. It models predictive reasoning about other agents.</p>
<p>• Matching Pennies: A game where players want to match/mismatch a coin flip. It requires recognizing and exploiting the other player's incentives.</p>
<p>• Imbalanced Matching Pennies: A variant of Matching Pennies with unequal rewards, adding complexity to strategy selection.</p>
<p>• Deadlock: A game with no dominant strategies where any combination of actions leads to a tie. It demonstrates the limits of naive payoff-maximizing reasoning.</p>
<p>• Stag Hunt: A game demonstrating the tension between safety and social cooperation.</p>
<p>• Chicken: A game modeling escalation where players must decide whether to avoid or engage in a dangerous confrontation. The first to swerve away is the "chicken".</p>
<p>C.2 Complex Games</p>
<p>Complex 2-stage matrix games that have a sequential and simultaneous component are shown in Fig. 13.</p>
<p>Prompts for factored reasoning. Demonstrations to solve trivial 2 × 2 matrix games. </p>
<p>D Instructions given to participants</p>
<p>Here are the instructions that participants were shown for the negotiation experiment.</p>
<p>Description</p>
<p>You are invited to participate in a research study on strategic reasoning and negotiation with AI agents. We are interested in understanding how AI agents interact and negotiate with humans. The study will focus on how individuals negotiate with a bot and how the bot responds to different negotiation strategies. The study aims to understand how AI agents can be designed to be more effective in negotiation scenarios, by understanding the strategies that humans use and how the bot responds to them. Based on this information, we aim to develop AI agents that can adapt their behavior and improve their negotiation strategies.</p>
<p>Purpose</p>
<p>The purpose of this research study is to test and better understand interactions between humans and intelligent autonomous systems.</p>
<p>Procedures</p>
<p>In the first phase of the experiment, you will be asked to play a negotiation game with the ai agent. Next, we will ask you for feedback about the agent's behavior: e.g., did the agent act in a way that made sense, etc. The researchers will collect these responses, as well as data from the agent's performance. If you agree to be in this study, you will be asked to do the following:</p>
<p>You will play negotiation games with 3 different ai agents. You will play 3 games against each of them. In each game, you have to split a pot of hats, books and balls with the ai agent. The number of these objects and their values to you and the ai agent varies across the games. On every turn, you can take 3 actions: propose, where you say how many objects you want to keep, accept, when you want to accept a proposed deal, and reject, when you want to reject a deal and end the game. The ai agent can take the same actions too. Every lasts for at most six turns after which you must accept or reject the deal. Your goal is to maximize the value of the items that you get.</p>
<p>After this, you are asked to compare the behaviors with each other in terms of performance and capability. At the end, you will fill in a questionnaire. We will only record your preference responses in each run and the results of the negotiation interaction.</p>
<p>Study duration</p>
<p>Participation in this study will involve a total of 25 minutes of your time.</p>
<p>Risks/Discomforts</p>
<p>The risks and discomfort associated with participation in this study are no greater than those ordinarily encountered in daily life.</p>
<p>Confidentiality</p>
<p>Your study data will be handled as confidentially as possible. If results of this study are published or presented, individual names will not be used.</p>
<p>Compensation/Payment</p>
<p>You will receive $15 for your participation in the study in the form of an Amazon gift card.</p>
<p>Costs</p>
<p>You will not be charged for any of the study activities.</p>
<p>Rights</p>
<p>Participation is voluntary. Refusal to participate will not involve penalty or loss of benefits to which subjects are otherwise entitled. Subjects can discontinue participation at any time will not involve penalty or loss of benefits to which subjects are otherwise entitled. You may refuse to answer particular questions.    Figure 13: A visualization of a 2-stage game with different rules of playing actions in the rounds. In (a), players first play a simultaneous game and then play a sequential game if the both choose (a2, b2). In (b), players first play a sequential game and then play a simultaneous game if they both choose (a2, b2). Only games with trivial payoffs are shown here; these are used in the incontext demonstrations.</p>
<p>(</p>
<p>Figure 2 :
2Comparison of accuracies of predicting best response with &amp; without factored reasoning. Experiment Setup. To further challenge language models, we evaluate their strategic reasoning on matrix games with increased complexity, without more complex in-context examples.</p>
<p>Figure 3 :Figure 4 :
34A sample episode (a) from Deal or No Deal[16] with a (b) demonstration of the reasoning prompt; pink indicates reasoning over values, orange indicates search, and grey indicates beliefs. Comparison of different methods on different metrics when paired with real humans in the Deal or No Deal task. We show how our method outperforms baselines and ablations in human user studies. Error bars represent the standard deviation.</p>
<p>Figure 5 :
5A visualization of an LM-cascade. The demonstrations for LM-1 and LM-n are different as the LM-n takes a prediction of an action as input.</p>
<p>Fig. C.1) with 5 variations of payoffs. These games are designed such that some require deception while others rely on the honesty of communication. Games Strategic No Belief Beliefs: communication §A.3 1.00 (50/50) 0.66 (33/50)</p>
<p>Figure 6 :
6One demonstration of how to solve a trivial 2 × 2 matrix game.</p>
<p>Figure 7 :
7Prompts for factored reasoning. Demonstrations to solve trivial 2 × 2 matrix games.</p>
<p>Figure 8 :
8One demonstration of how to solve a trivial 2 × 2 matrix game.</p>
<p>Figure 9 :
9One demonstration of optimizing for equality as a broker in negotiation.</p>
<p>Figure 10 :Figure 11 :
1011A demonstration of negotiating a deal with an agent. Various 2 × 2 matrix games used in our evaluations.</p>
<p>Game. Payoffs from the left are used when c1 is played and payoffs from the right are used when c2 is played.</p>
<p>Figure 12 :
12Matrix games with novel structures, with more actions in (a),(b) and with more players in (c).</p>
<p>b) Demonstrated reasoning for prompting a large language model to reason strategically. Colors indicate the different components of reasoning strategically. Let's reason about what Bob wants. Bob wants to maximize reward. Bob doesn't think about others. Bob can see the state of the world. So, let's calculate the expected reward of Bob's actions for the different world states. If Bob plays b2, then Gopher can infer that the world is s1. … Gopher's action:a2 when Bob plays b1 or a1 when Bob plays b2s 1 
s 2 </p>
<p>If world is in state s1, If Bob plays b1, 
1. World: s1 then Bob:b1 then Gopher:a1, then; Bob's reward=1 
2. World: s1 then Bob:b1 then Gopher:a2, then; Bob's reward=3 
So, Bob's expected reward for action b1 in s1 is (1+3)/2=4/2=2.0. 
… 
So, 
Bob's 
expected 
reward 
for 
action 
b2 
in 
s1 
is 
(5+7)/2=12/2=6.0. 
As Bob's reward for b1 in s1=2.0, b2 in s1=6.0, 2.0 &lt; 6.0, Bob 
will play b2 in s1. 
… States hidden from </p>
<p>b 1 
b 2 </p>
<p>a 1 
a 2 
a 2 
a 2 
a 2 
a 1 
a 1 
a 1 </p>
<p>(1, 2) 
(3, 4) 
(5, 6) 
(7, 8) 
(0, -1) 
(-2, -3) 
(-4, -5) 
(-6, -7) </p>
<p>b 1 
b 2 </p>
<p>To teach strategic reasoning, we provide in-context examples of two basic matrix games with the appropriate format: First, we demonstrate a game with strictly descending payoffs (8, 7, . . . , 1) for all actions. Second, we demonstrate a game with payoff ties that matches the structure of the target evaluation game (simultaneous or sequential). These examples have the same game structure as, but simpler strategies than, that of the evaluation. These help teach the model to reason strategically before the main evaluation. To evaluate generalization to new rewards, we test language models on different variations of matrix games: For single-stage games, we use 7 game types (Prisoner's Dilemma, Chicken, Stag Hunt, Battle of the Sexes, Market Entry, Imbalanced Matching Pennies, Deadlock; see §B.1) with 5 payoff variations each, yielding 35 games. These games are chosen as they require different types of behaviors to be successful: cooperation, compromise, and adversarial action. For 2-stage games, we use 2 game trees (simultaneous-sequential, sequential-simultaneous) with 15 payoff variations, yielding 30 games(Fig. C.2). The range of games evaluates their ability to show different strategic behaviors.Results. Language models prompted to systematically search and assign values with our method generalize successfully to new payoffs (see Tab. 1). Based on the payoffs, models are able to express different types of behaviors. Vanilla strategies of prompting such as 0-shot, 2-shot, and 0-shot CoT prompts are less successful at generalizing to new payoffs. Further, when we ablate the strategic prompt to remove parts of search or value assignment, performance becomes less reliable; showing how parts of our reasoning structure help in making LLMs more reliable.4.1.2 Complex Game Structures: Factoring Strategic ReasoningMore Actions 
More Players </p>
<p>Success Rate </p>
<p>. Token statistics for the prompts are shown inSimultaneous 2x2 More actions 3x3, 4x3 More players 2x2x2 </p>
<p>code-davinci-002: 2-shot 
0.37 (13/35) 
0.43 (13/30) 
0.20 (3/15) 
code-davinci-002: Strategic 
1.00 (35/35) 
0.93 (28/30) 
1.00 (15/15) 
text-davinci-003: 2-shot 
0.66 (23/35) 
0.43 (13/30) 
0.13 (2/15) 
text-davinci-003: Strategic 
1.00 (35/35) 
0.66 (23/35) 
0.66 (10/15) 
text-davinci-002: 2-shot 
0.49 (17/35) 
0.40 (12/30) 
0.13 (2/15) 
text-davinci-002: Strategic 
0.80 (28/35) 
0.70 (21/30) 
0.27 (4/15) 
text-curie-001: 2-shot 
0.77 (27/35) 
0.40 (12/30) 
0.47 (7/15) 
text-curie-001: Strategic 
0.77 (27/35) 
-
-</p>
<p>Table 7 :
7Results for search with different language models.max,max 
max,help 
help,max 
welfare,welfare daxity,daxity </p>
<p>code-davinci-002: 2-shot 
0.37 (13/35) 0.49 (17/35) 0.43 (15/35) 
0.60 (21/35) 
0.43 (15/35) 
code-davinci-002: Strategic 1.00 (35/35) 1.00 (35/35) 1.00 (35/35) 
1.00 (35/35) 
0.94 (33/35) 
text-davinci-003: 2-shot 
0.66 (23/35) 0.60 (21/35) 0.49 (17/35) 
0.74 (26/35) 
0.71 (25/35) 
text-davinci-003: Strategic 1.00 (35/35) 1.00 (35/35) 0.86 (30/35) 
0.94 (33/35) 
0.89 (31/35) 
text-davinci-002: 2-shot 
0.49 (17/35) 0.69 (24/35) 0.43 (15/35) 
0.69 (24/35) 
0.83 (29/35) 
text-davinci-002: Strategic 0.80 (28/35) 1.00 (35/35) 0.74 (26/35) 
0.80 (28/35) 
0.80 (28/35) 
text-curie-001: 2-shot 
0.77 (27/35) 0.43 (15/35) 0.49 (17/35) 
0.60 (21/35) 
0.74 (26/35) 
text-curie-001: Strategic 
0.77 (27/35) 0.46 (16/35) 0.46 (16/35) 
0.67 (23/35) 
0.57 (20/35) </p>
<p>Table 8 :
8Results for value assignment with different language models.Prompt # Tokens </p>
<p>4.1.1 
1423 
4.1.2 
2292 (seq+sim), 2316 (sim+seq) 
4.1.3 
1423 (level 1), 1280 (level-n) 
4.2.1 
1423 
4.2.2 
4054 (equality), 2455 (rawlsian) 
4.3.1 
2640 
4.3.2 
1920 
4.3.3 
4268 </p>
<p>Project Website: https://sites.google.com/view/strategic-reasoning-llms/ Preprint. Under review. arXiv:2305.19165v1 [cs.AI] 30 May 2023
The terminal points of a game tree that have no subsequent decisions/actions, associated with payoffs.3  A standard tree structure that represents the game states/scenarios and possible transitions between them based on players' actions/decisions and the rules of the game.
the rationale behind our selection of these Language Models (LMs) and results with text models can be found in §A.4 
Prompts are available here https://sites.google.com/view/strategic-reasoning-llms/
AcknowledgementsThis worked was supported by DARPA YFA, NSF Award #2125511 and the NSF Expeditions Grant, Award Number (FAIN) 1918771. We would like to thank Minae Kwon, Gabriel Poesia, and Joseph Feffer for their feedback, insight, and discussions, which were instrumental in strengthening and improving this project.A:Let's reason about what Bob wants to first. Bob wants to maximize his reward. If Bob plays b1, 1. Gopher:a1, Bob:b1, Gopher reward gr11=8 and Bob reward br11=7: Bob's reward r11=(br11)=br11=7 3. Gopher:a2, Bob:b1, Gopher reward gr21=4 and Bob reward br21=3: Bob's reward r21=(br21)=br21=3 So, Bob's expected reward for b1 is (r11+r21)/2 = (7+3)/2 = 10/2 = 5 If Bob plays b2, 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5: Bob's reward r12=(br12)=br12=5 4. Gopher:a2, Bob:b2, then Gopher reward gr22=2 and Bob reward br22=1: Bob's reward r22=(br22)=br22=1 So, Bob's expected reward for b2 is (r12+r22)/2 = (5+1)/2 = 6/2 = 3 As b1=5, b2=3, 5&gt;3, b1&gt;b2, Bob will play b1. Now let's reason for Gopher. Gopher wants to maximize its reward. As we know Bob plays b1 we dont need to calculate expected reward for each action, listing actions where Bob plays b1 is enough. 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7: Gopher's reward r11=(gr11)=gr11=8 3. Gopher:a2, Bob:b1, then Gopher reward gr21=4 and Bob reward br21=3: Gopher's reward r21=(gr21)=gr21=4 We know Bob plays b1, so Gopher will pick the action with the highest payoff. As a1=8, a2=4, 8&gt;4, a1&gt;a2, Gopher will play a1. Gopher's action:a1
Using large language models to simulate multiple humans. Gati Aher, I Rosa, Adam Tauman Arriaga, Kalai, arXiv:2208.10264arXiv preprintGati Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans. arXiv preprint arXiv:2208.10264, 2022.</p>
<p>Human-level play in the game of <i>diplomacy</i> by combining language models with strategic reasoning. Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra, Science. 3786624Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of <i>diplomacy</i> by combining language models with strategic reasoning. Science, 378(6624):1067-1074, 2022.</p>
<p>No-press diplomacy from scratch. Anton Bakhtin, David Wu, Adam Lerer, Noam Brown, Advances in Neural Information Processing Systems. 34Anton Bakhtin, David Wu, Adam Lerer, and Noam Brown. No-press diplomacy from scratch. Advances in Neural Information Processing Systems, 34:18063-18074, 2021.</p>
<p>The hanabi challenge: A new frontier for ai research. Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, Artificial Intelligence. 280103216Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A new frontier for ai research. Artificial Intelligence, 280:103216, 2020.</p>
<p>Dota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.06680arXiv preprintChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.</p>
<p>Better rewards yield better summaries: Learning to summarise without references. Florian Böhm, Yang Gao, Christian M Meyer, Ori Shapira, Ido Dagan, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsFlorian Böhm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. Better rewards yield better summaries: Learning to summarise without references. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110-3120, Hong Kong, China, November 2019. Association for Computational Linguistics.</p>
<p>Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Noam Brown, Tuomas Sandholm, Science. 3596374Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418-424, 2018.</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374arXiv preprintet al. Evaluating large language models trained on codeMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, arXiv:2207.10342Jascha Sohl-Dickstein, et al. Language model cascades. Rif A SaurousarXiv preprintDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.</p>
<p>Colbert: Efficient and effective passage search via contextualized late interaction over bert. Omar Khattab, Matei Zaharia, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. the 43rd International ACM SIGIR conference on research and development in Information RetrievalOmar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextual- ized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39-48, 2020.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun ChoTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.</p>
<p>Thore Graepel, and Yoram Bachrach. Negotiation and honesty in artificial intelligence methods for the board game of diplomacy. János Kramár, Tom Eccles, Ian Gemp, Andrea Tacchetti, Mateusz Kevin R Mckee, Malinowski, Nature Communications. 131János Kramár, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin R McKee, Mateusz Malinowski, Thore Graepel, and Yoram Bachrach. Negotiation and honesty in artificial intelligence methods for the board game of diplomacy. Nature Communications, 13(1):1-15, 2022.</p>
<p>Reward design with language models. Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, International Conference on Learning Representations (ICLR. 2023Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. In International Conference on Learning Representations (ICLR), 2023.</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 40Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017.</p>
<p>Deal or no deal? end-to-end learning for negotiation dialogues. Mike Lewis, Denis Yarats, Devi Yann N Dauphin, Dhruv Parikh, Batra, Empirical Methods in Natural Language Processing. Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-to-end learning for negotiation dialogues. In Empirical Methods in Natural Language Processing (EMNLP), 2017.</p>
<p>Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisỳ, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling, Science. 3566337Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisỳ, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.</p>
<p>Generating options and choosing between them depend on distinct forms of value representation. Adam Morris, Jonathan Phillips, Karen Huang, Fiery Cushman, Psychological Science. 3211Adam Morris, Jonathan Phillips, Karen Huang, and Fiery Cushman. Generating options and choosing between them depend on distinct forms of value representation. Psychological Science, 32(11):1731-1746, 2021.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. arXiv preprintReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Evidence for dynamic consideration set construction in open-ended problems. Jonas Nelle, Fiery Cushman, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society44Jonas Nelle and Fiery Cushman. Evidence for dynamic consideration set construction in open-ended problems. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 44, 2022.</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, Deep Learning for Code Workshop. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop, 2022.</p>
<p>Openai platform: Model index for researchers. Openai, OpenAI. Openai platform: Model index for researchers. https://platform.openai.com/ docs/model-index-for-researchers. Accessed: May 8, 2023.</p>
<p>. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, L Carroll, Pamela Wainwright, Chong Mishkin, Sandhini Zhang, Katarina Agarwal, Alex Slama, Ray, arXiv:2203.02155arXiv preprintLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>No-press diplomacy: Modeling multi-agent gameplay. Philip Paquette, Yuchen Lu, Seton Steven Bocco, Max Smith, O-G Satya, Jonathan K Kummerfeld, Joelle Pineau, Satinder Singh, Aaron C Courville, Advances in Neural Information Processing Systems. 32Philip Paquette, Yuchen Lu, Seton Steven Bocco, Max Smith, Satya O-G, Jonathan K Kummer- feld, Joelle Pineau, Satinder Singh, and Aaron C Courville. No-press diplomacy: Modeling multi-agent gameplay. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Synchromesh: Reliable code generation from pre-trained language models. Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, Sumit Gulwani, arXiv:2201.11227arXiv preprintGabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. arXiv preprint arXiv:2201.11227, 2022.</p>
<p>Colbertv2: Effective and efficient retrieval via lightweight late interaction. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia, arXiv:2112.01488arXiv preprintKeshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488, 2021.</p>
<p>Neural theory-of-mind? on the limits of social intelligence in large lms. Maarten Sap, Le Ronan, Daniel Bras, Yejin Fried, Choi, EMNLP. 2022Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of social intelligence in large lms. In EMNLP, 2022.</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.04761arXiv preprintTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle- moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.11366arXiv preprintNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 5297587David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas- tering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Science. 3626419David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144, 2018.</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 33Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.</p>
<p>Factored cognition primer. Andreas Stuhlmüller, Justin Reppert, Luke Stebbing, Andreas Stuhlmüller, Justin Reppert, and Luke Stebbing. Factored cognition primer. https: //primer.ought.org, 2022.</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won, Aakanksha Chung, Chowdhery, V Quoc, Ed H Le, Denny Chi, Jason Zhou, Wei, arXiv:2210.09261arXiv preprintMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.10498arXiv preprintKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498, 2022.</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, H David, Richard Choi, Timo Powell, Petko Ewalds, Georgiev, Nature. 5757782Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Jun- young Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun ChoJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.</p>
<p>STar: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun ChoEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.</p>
<p>The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning. Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C Parkes, Richard Socher, Science Advances. 8182607Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C. Parkes, and Richard Socher. The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning. Science Advances, 8(18):eabk2607, 2022.</p>
<p>Gopher:a2, Bob:b1, then Gopher reward gr21=4 and Bob reward br21=3 4. Gopher:a2, Bob:b2, then Gopher reward gr22=2 and Bob reward br22=1 Gopher and Bob are trying to maximize their reward. What action should Gopher play? There are 3 functions that help in solving the problems: 1. search: search(agent, other_agent, objective, action, other_actions) returns the expected reward for the agent. Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5 3. other actions are passed when the agent thinks about other agents. 2. compare: compare(agent, objective. list of actions]) returns the action that maximizes the agent's objective. 3. mean: mean. list of numbers]) returns the mean of the list of numbersQ:Gopher and Bob are playing a game. Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5 3. Gopher:a2, Bob:b1, then Gopher reward gr21=4 and Bob reward br21=3 4. Gopher:a2, Bob:b2, then Gopher reward gr22=2 and Bob reward br22=1 Gopher and Bob are trying to maximize their reward. What action should Gopher play? There are 3 functions that help in solving the problems: 1. search: search(agent, other_agent, objective, action, other_actions) returns the expected reward for the agent. other actions are passed when the agent thinks about other agents. 2. compare: compare(agent, objective, [list of actions]) returns the action that maximizes the agent's objective. 3. mean: mean([list of numbers]) returns the mean of the list of numbers.</p>
<p>Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=-3 and Bob reward br11=-2 2. Gopher:a1, Bob:b2, then Gopher reward gr12=-1 and Bob reward. Q:Gopher and Bob are playing a game. Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=-3 and Bob reward br11=-2 2. Gopher:a1, Bob:b2, then Gopher reward gr12=-1 and Bob reward br12=-4</p>
<p>Gopher:a2, Bob:b1, then Gopher reward gr21=1 and Bob reward br21=2 4. Gopher:a2, Bob:b2, then Gopher reward gr22=3 and Bob reward br22=4. Gopher:a2, Bob:b1, then Gopher reward gr21=1 and Bob reward br21=2 4. Gopher:a2, Bob:b2, then Gopher reward gr22=3 and Bob reward br22=4</p>
<p>Gopher thinks about other players' reasoning. What action should Gopher play? Pick from a1, a2, a1 or a2? A:Let's reason about what Bob wants to first. Bob wants to maximize his reward: br. If Bob plays b1, exepected reward for b1 is search(Bob, Gopher, max, b1) = 0. If Bob plays b2, exepected reward for b2 is search. Bob, Gopher, max; Bob, maxBob does not think about their opponent and only plays the action with the highest expected reward. b2) = 0. So, Bob will play compare. b1=0, b2=0])= [b1,b2Gopher and Bob are trying to maximize their reward. Bob does not think about their opponent and only plays the action with the highest expected reward. Gopher thinks about other players' reasoning. What action should Gopher play? Pick from a1, a2, a1 or a2? A:Let's reason about what Bob wants to first. Bob wants to maximize his reward: br. If Bob plays b1, exepected reward for b1 is search(Bob, Gopher, max, b1) = 0. If Bob plays b2, exepected reward for b2 is search(Bob, Gopher, max, b2) = 0. So, Bob will play compare(Bob, max, [b1=0, b2=0])= [b1,b2].</p>
<p>Now let's reason for Gopher. Gopher wants to maximize its reward: gr. Now let's reason for Gopher. Gopher wants to maximize its reward: gr.</p>
<p>As Bob plays b1 or b2 we calculate the expected reward for each action, If Gopher plays a1, exepected reward for a1 is search. GopherBob, max, a1bob[b1, b2]]) = -2As Bob plays b1 or b2 we calculate the expected reward for each action, If Gopher plays a1, exepected reward for a1 is search(Gopher, Bob, max, a1, [bob[b1, b2]]) = -2.</p>
<p>If Gopher plays a2, exepected reward for a2 is search. GopherBob, max, a2; Gopher, maxbob[b1, b2]]) = 2. So, Gopher will play compare. a1=-2, a2=2]) = [a2If Gopher plays a2, exepected reward for a2 is search(Gopher, Bob, max, a2, [bob[b1, b2]]) = 2. So, Gopher will play compare(Gopher, max, [a1=-2, a2=2]) = [a2].</p>
<p>Gopher's action:a2. Gopher's action:a2.</p>
<p>Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5 3. Gopher:a2, Bob:b1, then Gopher reward gr21=4 and Bob. reward br21=3 4. Gopher:a2, Bob:b2, then Gopher reward gr22=2 and Bob reward br22=1Q:Gopher and Bob are playing a game. Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5 3. Gopher:a2, Bob:b1, then Gopher reward gr21=4 and Bob reward br21=3 4. Gopher:a2, Bob:b2, then Gopher reward gr22=2 and Bob reward br22=1</p>
<p>Gopher thinks about other players' reasoning. What action should Gopher play? Pick from a1, a2, a1 or a2? A:Let's reason about what Bob wants to first. Bob wants to maximize his reward: br. If Bob plays b1, exepected reward for b1 is search(Bob, Gopher, max, b1) = 5. If Bob plays b2, exepected reward for b2 is search. Bob, Gopher, max; Bob, maxBob does not think about their opponent and only plays the action with the highest expected reward. b2) = 3. So, Bob will play compare. b1=5, b2=3]) = [b1Gopher and Bob are trying to maximize their reward. Bob does not think about their opponent and only plays the action with the highest expected reward. Gopher thinks about other players' reasoning. What action should Gopher play? Pick from a1, a2, a1 or a2? A:Let's reason about what Bob wants to first. Bob wants to maximize his reward: br. If Bob plays b1, exepected reward for b1 is search(Bob, Gopher, max, b1) = 5. If Bob plays b2, exepected reward for b2 is search(Bob, Gopher, max, b2) = 3. So, Bob will play compare(Bob, max, [b1=5, b2=3]) = [b1].</p>
<p>Now let's reason for Gopher. Gopher wants to maximize its reward: gr. Now let's reason for Gopher. Gopher wants to maximize its reward: gr.</p>
<p>As Bob plays b1, we calculate the expected reward for each action, If Gopher plays a1, exepected reward for a1 is search. Gopher8Bob, max, a1; Bob, max, a2; Gopher, maxIf Gopher plays a2, exepected reward for a2 is search. bob[b1]]) = 4. So, Gopher will play compare. a1=8, a2=4]) = [a1As Bob plays b1, we calculate the expected reward for each action, If Gopher plays a1, exepected reward for a1 is search(Gopher, Bob, max, a1, [bob[b1]]) = 8. If Gopher plays a2, exepected reward for a2 is search(Gopher, Bob, max, a2, [bob[b1]]) = 4. So, Gopher will play compare(Gopher, max, [a1=8, a2=4]) = [a1].</p>
<p>Gopher's action:a1. Gopher's action:a1.</p>
<p>Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7 2. Gopher:a2, Bob:b1, then Gopher reward gr12=6 and Bob reward br12=5. Q:Gopher and Bob are playing a game. Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7 2. Gopher:a2, Bob:b1, then Gopher reward gr12=6 and Bob reward br12=5</p>
<p>Bob's objective: max. What is Bob's expected reward? A: Bob is maximizing his reward: br. If Gopher plays a1, 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7. Bob maximizes r11=(br11Bob's objective: max. What is Bob's expected reward? A: Bob is maximizing his reward: br. If Gopher plays a1, 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7; Bob maximizes r11=(br11)=br11=7</p>
<p>Gopher:a2, Bob:b1, then Gopher reward gr21=4 and Bob reward br21=3. If Gopher plays a2, 2. Bob maximizes r21=(br21If Gopher plays a2, 2. Gopher:a2, Bob:b1, then Gopher reward gr21=4 and Bob reward br21=3; Bob maximizes r21=(br21)=br21=3</p>
<p>Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5 Gopher's objective: max. What is Gopher's expected reward? A: Gopher is maximizing its reward: gr. 5Answer:5. Q:Gopher and Bob are playing a game. If Bob plays b1, 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7; Gopher maximizes r11=(gr11Expected reward for Bob = mean([r11, r21]) = mean([7, 3]) = 5. Answer:5. Q:Gopher and Bob are playing a game. Gopher and Bob get rewards according to these rules: 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5 Gopher's objective: max. What is Gopher's expected reward? A: Gopher is maximizing its reward: gr. If Bob plays b1, 1. Gopher:a1, Bob:b1, then Gopher reward gr11=8 and Bob reward br11=7; Gopher maximizes r11=(gr11)=gr11=8</p>
<p>If Bob plays b2, 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5. If Bob plays b2, 2. Gopher:a1, Bob:b2, then Gopher reward gr12=6 and Bob reward br12=5</p>
<p>Alice and Bob are trying to split books, hats and balls amongst themselves. The value of the books, hat and balls changes across scenarios. The items have a different value for Alice and Bob. Propose a deal where the Alice and Bob receive equal/ similar rewards. Try proposing 3 times at mostAlice and Bob are trying to split books, hats and balls amongst themselves. The value of the books, hat and balls changes across scenarios. The items have a different value for Alice and Bob. Propose a deal where the Alice and Bob receive equal/ similar rewards. Try proposing 3 times at most.</p>
<h2>New Deal items available: book=3 hat=1 ball=2 Bob's values: book=0 hat=2 ball=4. ## New Deal items available: book=3 hat=1 ball=2 Bob's values: book=0 hat=2 ball=4</h2>
<p>Alice's values: book=1 hat=3 ball=2 Q: What is a proposal that gives similar payoffs? Try 1/3. items: book=3, hat=1, ball=2 Bob values: ball:4 &gt; hat:2 &gt; book:0 Alice values: hat:3 &gt; ball:2 &gt; book:1 In a proposal with similar payoffs. Alice gets 3 books, 1 hats, 0 balls. So, Bob gets (3-3)=0 books, (1-1)=0 hatsAlice's values: book=1 hat=3 ball=2 Q: What is a proposal that gives similar payoffs? Try 1/3. items: book=3, hat=1, ball=2 Bob values: ball:4 &gt; hat:2 &gt; book:0 Alice values: hat:3 &gt; ball:2 &gt; book:1 In a proposal with similar payoffs, Alice gets 3 books, 1 hats, 0 balls. So, Bob gets (3-3)=0 books, (1-1)=0 hats, (2</p>
<p>Alice gets 6/10 and Bob gets 8/10. Difference in payoffs 8-6 = 2. This is a good deal but we might be able to do better. So, let's try againAlice gets 6/10 and Bob gets 8/10. Difference in payoffs 8-6 = 2. This is a good deal but we might be able to do better. So, let's try again.</p>
<p>Alice values: hat:3 &gt; ball:2 &gt; book:1 old proposal: Alice gets 3 books, 1 hats, 0 balls. Try 2/3. items: book=3, hat=1, ball=2 Bob values: ball:4 &gt; hat:2 &gt; book:0. Alice gets 6/10 and Bob gets 8/10. new proposal: Alice gets 3 books, 0 hats, 1 balls. So, Bob gets (3-3)=0 books, (1-0)=1 hatsTry 2/3. items: book=3, hat=1, ball=2 Bob values: ball:4 &gt; hat:2 &gt; book:0 Alice values: hat:3 &gt; ball:2 &gt; book:1 old proposal: Alice gets 3 books, 1 hats, 0 balls. Alice gets 6/10 and Bob gets 8/10. new proposal: Alice gets 3 books, 0 hats, 1 balls. So, Bob gets (3-3)=0 books, (1-0)=1 hats, (2</p>
<p>This is a good deal but we might be able to do better. So, let's try again. Try 3/3. last try. items: book=3, hat=1, ball=2 Bob values: ball:4 &gt; hat:2 &gt; book:0 Alice values: hat:3 &gt; ball:2 &gt; book:1 old proposal: Alice gets 3 books, 0 hats, 1 balls. Alice gets 5/10 and Bob gets 6/10. new proposal: Alice gets 2 books, 0 hats, 2 balls. Alice gets 5/10 and Bob gets 6/10. Difference in payoffs 6-5 = 1. So, Bob gets (3-2)=1 books, (1-0)=1 hats, (2-2)=0 balls. Value of proposal for AliceAlice gets 5/10 and Bob gets 6/10. Difference in payoffs 6-5 = 1. This is a good deal but we might be able to do better. So, let's try again. Try 3/3. last try. items: book=3, hat=1, ball=2 Bob values: ball:4 &gt; hat:2 &gt; book:0 Alice values: hat:3 &gt; ball:2 &gt; book:1 old proposal: Alice gets 3 books, 0 hats, 1 balls. Alice gets 5/10 and Bob gets 6/10. new proposal: Alice gets 2 books, 0 hats, 2 balls. So, Bob gets (3-2)=1 books, (1-0)=1 hats, (2-2)=0 balls. Value of proposal for Alice: (2*1</p>
<p>Summary of tries: Try 1/3: Alice gets 6/10 and Bob gets 8/10. Difference in payoffs 8-6 = 2. Try 2/3: Alice gets 5/10 and Bob gets 6/10. Alice gets 6/10 and Bob gets 2/10. Difference in payoffs 6-2 = 4. This is a bad deal as payoffs are not close. Difference in payoffs 6-5 = 1. Try 3/3: Alice gets 6/10 and Bob gets 2/10. Difference in payoffs 6-2 = 4. So, the best deal is the second try. Minimum difference is 2. So, we go with try 2/3. Alice gets 0 books, 1 hats, 2 balls. propose: book=0 hat=1 ball=2Alice gets 6/10 and Bob gets 2/10. Difference in payoffs 6-2 = 4. This is a bad deal as payoffs are not close. Summary of tries: Try 1/3: Alice gets 6/10 and Bob gets 8/10. Difference in payoffs 8-6 = 2. Try 2/3: Alice gets 5/10 and Bob gets 6/10. Difference in payoffs 6-5 = 1. Try 3/3: Alice gets 6/10 and Bob gets 2/10. Difference in payoffs 6-2 = 4. So, the best deal is the second try. Minimum difference is 2. So, we go with try 2/3. Alice gets 0 books, 1 hats, 2 balls. propose: book=0 hat=1 ball=2</p>
<p>Alice's values: book=4 hat=1 ball=2 prior over Bob's values: book=0 hat=0 ball=0 Bob: propose: book=0 hat=3 ball=1 over values: book: 0, hat: 0, balls: 0 Updated belief: book: 0+0=0, hat: 0+0.75=0.75, balls: 0+1=1 Old Proposals: Bob: propose: book=0 hat=3 ball=1 proposals: Bob values: balls:1 &gt; hats:0.75 &gt; books:0 Alice values: books:4 &gt; balls:2 &gt; hats. 1Alice's values: book=4 hat=1 ball=2 prior over Bob's values: book=0 hat=0 ball=0 Bob: propose: book=0 hat=3 ball=1 over values: book: 0, hat: 0, balls: 0 Updated belief: book: 0+0=0, hat: 0+0.75=0.75, balls: 0+1=1 Old Proposals: Bob: propose: book=0 hat=3 ball=1 proposals: Bob values: balls:1 &gt; hats:0.75 &gt; books:0 Alice values: books:4 &gt; balls:2 &gt; hats:1</p>
<p>Alice like books more than Bob, and can try to get more hats for a higher value. Bob does not like books but wants balls. Alice like books more than Bob, and can try to get more hats for a higher value. Bob does not like books but wants balls.</p>
<p>Alice: propose: book=1 hat=4 ball=0 Bob: propose: book=0 hat=2 ball=1 over values: book: 0, hat: 0.75, balls: 1 Updated belief: book: 0+0=0, hat: 0.75+0.5=1.25, balls: 1+1=2 Old Proposals: Alice: propose: book=1 hat=4 ball=0 proposals: Bob values: balls:2 &gt; hats:1.25 &gt; books:0 Alice values: books:4 &gt; balls:2 &gt; hats. 1Alice: propose: book=1 hat=4 ball=0 Bob: propose: book=0 hat=2 ball=1 over values: book: 0, hat: 0.75, balls: 1 Updated belief: book: 0+0=0, hat: 0.75+0.5=1.25, balls: 1+1=2 Old Proposals: Alice: propose: book=1 hat=4 ball=0 proposals: Bob values: balls:2 &gt; hats:1.25 &gt; books:0 Alice values: books:4 &gt; balls:2 &gt; hats:1</p>
<p>Alice can give take one more hat to increase her value. Bob likes balls, doesn't want books. Alice can give take one more hat to increase her value. Bob likes balls, doesn't want books.</p>
<p>Alice: propose: book=1 hat=3 ball=0 Bob:accept. Alice: propose: book=1 hat=3 ball=0 Bob:accept</p>            </div>
        </div>

    </div>
</body>
</html>
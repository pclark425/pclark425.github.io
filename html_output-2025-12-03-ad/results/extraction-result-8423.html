<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8423 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8423</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8423</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-50f44ef10335d59cec145b15effae20ff22c1fdb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/50f44ef10335d59cec145b15effae20ff22c1fdb" target="_blank">ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper seeks inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning, and proposes a symbolic memory framework instantiated as an LLM and a set of SQL databases.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8423.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8423.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments an LLM controller with an external SQL database used as symbolic memory; the LLM generates SQL to read/write structured tables and uses a chain-of-memory (CoM) to decompose multi-step tasks into sequential SQL operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent composed of an LLM controller (ChatGPT GPT-3.5 Turbo in experiments) that generates SQL statements to manipulate an external MySQL database as symbolic memory, orchestrated via a chain-of-memory process that decomposes user requests into intermediate SQL operations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 Turbo used as the LLM controller with temperature = 0 in experiments; it issues SQL and summary prompts but database executes symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fruit Shop Dataset (synthetic management records)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer 50 natural-language questions about a month of fruit-shop historical records (purchases, sales, price changes, returns) after ingesting 70 chronological records into the database; questions require retrieval, multi-hop reasoning, and precise calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-step reasoning with stateful record keeping</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external database (symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM generates SQL commands (INSERT/UPDATE/SELECT/DELETE) which are executed by an external MySQL database; chain-of-memory sequences multiple SQL steps, each step can be updated based on execution results.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured relational tables encoding historical records, entities (customers, fruits, sales, purchases), and intermediate results stored in DB rows/columns.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Symbolic SQL queries generated by LLM (selection, aggregation, joins, group-by) as part of the chain-of-memory; retrieval is explicit DB query execution rather than vector search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>82% accuracy (41/50 total; easy 13/15, hard 28/35) on Fruit Shop Dataset (reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared against a ChatGPT baseline that uses records in the prompt; Table 1 contrasts ChatDB's symbolic memory with prompt-based and matrix-based memories (qualitative comparison). No explicit ablation that removes the DB memory from ChatDB is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a structured external database as symbolic memory with chain-of-memory greatly improves multi-hop reasoning and numerical accuracy by delegating calculations and intermediate-state tracking to the DB; ChatDB outperforms ChatGPT baseline (82% vs 22%). Symbolic memory prevents error accumulation and enables precise state tracking and rollback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluation is on a synthetic dataset and with a single LLM (GPT-3.5 Turbo); no explicit ablation of components (e.g., CoM vs single-step SQL) is provided; schema creation may be manual or LLM-assisted; results limited to the presented benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8423.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8423.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo) baseline used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline agent that answers questions by including the chronological records directly in the prompt (no external symbolic memory in the experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard ChatGPT (GPT-3.5 Turbo) configured with temperature = 0 and a 4096 token limit; baseline treats the entire record history as part of the prompt and answers questions directly without using an external DB memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 Turbo used as-is, prompt concatenation of records and question; limited by context window and prone to arithmetic and multi-step errors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fruit Shop Dataset (same as ChatDB)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer the same 50 questions using the dataset placed into the prompt (no DB ingestion); requires multi-hop reasoning and numerical computations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt concatenation (implicit episodic in-prompt context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Records are concatenated in the model prompt; retrieval is implicit by the model attending to prompt tokens (no external memory module).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw textual records embedded in the prompt; no structured/tracked intermediate state.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Attention over concatenated prompt tokens and in-context reasoning; not explicit retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>22% accuracy overall (11/50; easy 10/15, hard 1/35) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Directly compared to ChatDB; paper highlights many failure modes for ChatGPT on hard multi-hop and numeric tasks and notes sequential error accumulation when relying solely on in-prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When large amounts of historical records are presented in the prompt, ChatGPT makes numerous arithmetic and multi-hop reasoning errors; prompt-based inclusion of history is brittle and leads to error accumulation on hard queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance degrades on hard multi-hop queries; limited by prompt length and numeric calculation reliability; no external structured state tracking or precise symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8423.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8423.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-GPT (tool-using agent, GitHub project)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous agent framework that chains LLM calls and external tools (e.g., search) to complete multi-step tasks; mentioned as an example of prompt-based memory/tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that uses LLM to orchestrate API calls and tooling (e.g., web search) and stores prompts/history; cited as an example of agents that use some memory module to keep track of history.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General autonomous task completion (cited example)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Chains tool calls and LLM responses to accomplish tasks; stores textual memory (prompt history) for continuity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>tool-using / multi-step task completion</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt-based memory (text and embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Stores content and/or vector embeddings of past prompts; selection via retrieval and inserting relevant bits into prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Historical prompt content and possibly vector embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval based on similarity/search over stored content (as described in related work discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Discussed qualitatively as a prompt-based memory approach in Table 1; contrasted against ChatDB's symbolic memory (ChatDB claims higher interpretability and state tracking).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt-based memory (as exemplified by Auto-GPT) stores text/embeddings and supports insertion/selection but lacks symbolic update/delete semantics and precise state tracking compared to databases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Non-symbolic (vector/similarity) operations can be inaccurate and lead to error accumulation; lacks native structured update/delete operations and explicit state interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8423.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8423.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent architecture that uses a memory module to store text prompts and retrieved memories to drive long-term agent behavior in simulations, cited as a prompt-based memory example.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that maintain a memory module storing textual experiences which are retrieved to inform future actions and dialogue; used as an example of memory-augmented agents in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Simulated agent behavior and long-term interaction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain coherent long-term behavior by storing/retrieving experience summaries and events.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>agent simulation / long-term interaction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt-based episodic memory (text summaries/embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store textual memories and retrieve relevant snippets for in-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual memory entries and possibly vector embeddings summarizing experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Similarity-based retrieval over memory entries to select relevant context for prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as an example of retrieval/prompt-based memory systems; contrasted conceptually with ChatDB's structured symbolic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt-based memory enables long-term behavior but relies on non-symbolic retrieval which can be less precise than symbolic DB operations for tasks requiring exact historical state.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential inaccuracy from similarity-based retrieval and lack of symbolic update/delete operations; limited interpretability relative to DBs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8423.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8423.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Memory Transformer (RMT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer augmentation that introduces extra memory tokens/matrices to store and exchange local and global information for very long sequences; presented as a matrix-based memory approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent memory transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recurrent Memory Transformer (RMT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Transformer variant that uses recurrent memory tokens or memory matrices to extend effective context and allow read/write interactions across segments, cited as an example of matrix-based memory augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-sequence modeling / extended context tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Handle very long input sequences by passing memory tokens/matrices between segments to preserve long-range information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-context modeling / memory-augmented sequence processing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>matrix-based memory tokens/matrices</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Learned memory tokens/matrices appended to inputs/outputs and updated by the model; memory operations are controlled by the neural network (non-symbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Trainable memory tokens or matrices (dense vectors) without explicit semantic labeling per-dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Attention and learned update dynamics within the Transformer architecture (implicit, neural-controlled).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Discussed in Table 1 as matrix-based memory contrasted with ChatDB's symbolic DB; matrix approach supports read/write but with low interpretability compared to DBs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Matrix-based memory can track state but is non-symbolic and less interpretable; ChatDB argues symbolic DBs have advantages for precise record manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Low interpretability and possible inaccuracies due to implicit neural control of memory; not inherently supporting symbolic CRUD semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8423.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8423.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented language models (Demonstrate-Search-Predict / RAG-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of approaches that retrieve relevant external documents or embeddings to augment LLM prompts; exemplified by Khattab et al.'s Demonstrate-Search-Predict and other retrieval-augmented generation works.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented models (RAG / Demonstrate-Search-Predict)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Models that use a retriever to fetch relevant documents or passages (often via dense/sparse vector search) and include them in prompts to the LLM; discussed as a prompt-based memory family.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive QA / long-context tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve and insert supporting external context to allow LLMs to answer questions beyond their parametric knowledge or context window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented question answering / long-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt-based retrieval (document store + vector embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Index of documents/embeddings; retrieval via similarity search and insertion of retrieved passages into model prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External documents and their vector embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic similarity search (embedding-based retrieval) and then prompt concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper contrasts retrieval-based methods with ChatDB, arguing retrieval (vector) methods are non-symbolic and prone to accumulated errors for multi-hop precise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented approaches help extend context but rely on approximate similarity and lack symbolic update semantics, which can be problematic for tasks needing precise state updates and multi-hop symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Similarity-based retrieval can be inaccurate and cause error accumulation, especially for long-horizon, stateful, numerically precise tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8423.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8423.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that equips LMs to call external APIs/tools by demonstrating tool usage in prompts; cited as a tool-using memory-related approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework that trains/permits LMs to use external APIs/tools (including memory-like tools) via self-supervised demonstrations; used as an example of treating DBs as tools.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tool-augmented reasoning / API usage</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use external tools (calculators, search, APIs) to improve LM capabilities and reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>tool-using / augmented reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external tools and APIs (can include memory stores)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM invokes external API/tool calls as part of its generated output; tools perform operations and return results to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Tool outputs (could be structured results) rather than internal embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Explicit API/tool invocation decided by LM and integrated into prompt/continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited as representative of tool-using LLM paradigms; ChatDB can be viewed similarly (DB as a tool), but emphasizes DB's symbolic storage/operations for correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tool-using approaches enable LMs to delegate computation/storage to external systems; ChatDB emphasizes that a database tool gives structured, symbolic state management beneficial for precise multi-hop tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires correct tool-invocation policies and interface handling; quality depends on tool reliability and integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memory augmented large language models are computationally universal <em>(Rating: 2)</em></li>
                <li>Recurrent memory transformer <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp <em>(Rating: 2)</em></li>
                <li>Binding language models in symbolic languages <em>(Rating: 1)</em></li>
                <li>Sql-palm: Improved large language modeladaptation for text-to-sql <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8423",
    "paper_id": "paper-50f44ef10335d59cec145b15effae20ff22c1fdb",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "ChatDB",
            "name_full": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
            "brief_description": "A framework that augments an LLM controller with an external SQL database used as symbolic memory; the LLM generates SQL to read/write structured tables and uses a chain-of-memory (CoM) to decompose multi-step tasks into sequential SQL operations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ChatDB",
            "agent_description": "An agent composed of an LLM controller (ChatGPT GPT-3.5 Turbo in experiments) that generates SQL statements to manipulate an external MySQL database as symbolic memory, orchestrated via a chain-of-memory process that decomposes user requests into intermediate SQL operations.",
            "model_name": "ChatGPT (GPT-3.5 Turbo)",
            "model_description": "GPT-3.5 Turbo used as the LLM controller with temperature = 0 in experiments; it issues SQL and summary prompts but database executes symbolic computation.",
            "task_name": "Fruit Shop Dataset (synthetic management records)",
            "task_description": "Answer 50 natural-language questions about a month of fruit-shop historical records (purchases, sales, price changes, returns) after ingesting 70 chronological records into the database; questions require retrieval, multi-hop reasoning, and precise calculations.",
            "task_type": "question answering / multi-step reasoning with stateful record keeping",
            "memory_used": true,
            "memory_type": "external database (symbolic)",
            "memory_mechanism": "LLM generates SQL commands (INSERT/UPDATE/SELECT/DELETE) which are executed by an external MySQL database; chain-of-memory sequences multiple SQL steps, each step can be updated based on execution results.",
            "memory_representation": "Structured relational tables encoding historical records, entities (customers, fruits, sales, purchases), and intermediate results stored in DB rows/columns.",
            "memory_retrieval_method": "Symbolic SQL queries generated by LLM (selection, aggregation, joins, group-by) as part of the chain-of-memory; retrieval is explicit DB query execution rather than vector search.",
            "performance_with_memory": "82% accuracy (41/50 total; easy 13/15, hard 28/35) on Fruit Shop Dataset (reported in Table 2).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared against a ChatGPT baseline that uses records in the prompt; Table 1 contrasts ChatDB's symbolic memory with prompt-based and matrix-based memories (qualitative comparison). No explicit ablation that removes the DB memory from ChatDB is reported.",
            "key_findings": "Using a structured external database as symbolic memory with chain-of-memory greatly improves multi-hop reasoning and numerical accuracy by delegating calculations and intermediate-state tracking to the DB; ChatDB outperforms ChatGPT baseline (82% vs 22%). Symbolic memory prevents error accumulation and enables precise state tracking and rollback.",
            "limitations_or_challenges": "Evaluation is on a synthetic dataset and with a single LLM (GPT-3.5 Turbo); no explicit ablation of components (e.g., CoM vs single-step SQL) is provided; schema creation may be manual or LLM-assisted; results limited to the presented benchmark.",
            "uuid": "e8423.0",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ChatGPT baseline",
            "name_full": "ChatGPT (GPT-3.5 Turbo) baseline used in experiments",
            "brief_description": "Baseline agent that answers questions by including the chronological records directly in the prompt (no external symbolic memory in the experiment).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ChatGPT (baseline)",
            "agent_description": "Standard ChatGPT (GPT-3.5 Turbo) configured with temperature = 0 and a 4096 token limit; baseline treats the entire record history as part of the prompt and answers questions directly without using an external DB memory.",
            "model_name": "ChatGPT (GPT-3.5 Turbo)",
            "model_description": "GPT-3.5 Turbo used as-is, prompt concatenation of records and question; limited by context window and prone to arithmetic and multi-step errors.",
            "task_name": "Fruit Shop Dataset (same as ChatDB)",
            "task_description": "Answer the same 50 questions using the dataset placed into the prompt (no DB ingestion); requires multi-hop reasoning and numerical computations.",
            "task_type": "question answering / multi-step reasoning",
            "memory_used": false,
            "memory_type": "prompt concatenation (implicit episodic in-prompt context)",
            "memory_mechanism": "Records are concatenated in the model prompt; retrieval is implicit by the model attending to prompt tokens (no external memory module).",
            "memory_representation": "Raw textual records embedded in the prompt; no structured/tracked intermediate state.",
            "memory_retrieval_method": "Attention over concatenated prompt tokens and in-context reasoning; not explicit retrieval.",
            "performance_with_memory": "22% accuracy overall (11/50; easy 10/15, hard 1/35) as reported in Table 2.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Directly compared to ChatDB; paper highlights many failure modes for ChatGPT on hard multi-hop and numeric tasks and notes sequential error accumulation when relying solely on in-prompt context.",
            "key_findings": "When large amounts of historical records are presented in the prompt, ChatGPT makes numerous arithmetic and multi-hop reasoning errors; prompt-based inclusion of history is brittle and leads to error accumulation on hard queries.",
            "limitations_or_challenges": "Performance degrades on hard multi-hop queries; limited by prompt length and numeric calculation reliability; no external structured state tracking or precise symbolic computation.",
            "uuid": "e8423.1",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Auto-GPT",
            "name_full": "Auto-GPT (tool-using agent, GitHub project)",
            "brief_description": "An autonomous agent framework that chains LLM calls and external tools (e.g., search) to complete multi-step tasks; mentioned as an example of prompt-based memory/tool use.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Auto-GPT",
            "agent_description": "An agent that uses LLM to orchestrate API calls and tooling (e.g., web search) and stores prompts/history; cited as an example of agents that use some memory module to keep track of history.",
            "model_name": null,
            "model_description": null,
            "task_name": "General autonomous task completion (cited example)",
            "task_description": "Chains tool calls and LLM responses to accomplish tasks; stores textual memory (prompt history) for continuity.",
            "task_type": "tool-using / multi-step task completion",
            "memory_used": true,
            "memory_type": "prompt-based memory (text and embeddings)",
            "memory_mechanism": "Stores content and/or vector embeddings of past prompts; selection via retrieval and inserting relevant bits into prompts.",
            "memory_representation": "Historical prompt content and possibly vector embeddings.",
            "memory_retrieval_method": "Retrieval based on similarity/search over stored content (as described in related work discussion).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Discussed qualitatively as a prompt-based memory approach in Table 1; contrasted against ChatDB's symbolic memory (ChatDB claims higher interpretability and state tracking).",
            "key_findings": "Prompt-based memory (as exemplified by Auto-GPT) stores text/embeddings and supports insertion/selection but lacks symbolic update/delete semantics and precise state tracking compared to databases.",
            "limitations_or_challenges": "Non-symbolic (vector/similarity) operations can be inaccurate and lead to error accumulation; lacks native structured update/delete operations and explicit state interpretability.",
            "uuid": "e8423.2",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents: Interactive simulacra of human behavior",
            "brief_description": "Agent architecture that uses a memory module to store text prompts and retrieved memories to drive long-term agent behavior in simulations, cited as a prompt-based memory example.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents",
            "agent_description": "Agents that maintain a memory module storing textual experiences which are retrieved to inform future actions and dialogue; used as an example of memory-augmented agents in related work.",
            "model_name": null,
            "model_description": null,
            "task_name": "Simulated agent behavior and long-term interaction",
            "task_description": "Maintain coherent long-term behavior by storing/retrieving experience summaries and events.",
            "task_type": "agent simulation / long-term interaction",
            "memory_used": true,
            "memory_type": "prompt-based episodic memory (text summaries/embeddings)",
            "memory_mechanism": "Store textual memories and retrieve relevant snippets for in-context prompting.",
            "memory_representation": "Textual memory entries and possibly vector embeddings summarizing experiences.",
            "memory_retrieval_method": "Similarity-based retrieval over memory entries to select relevant context for prompts.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as an example of retrieval/prompt-based memory systems; contrasted conceptually with ChatDB's structured symbolic memory.",
            "key_findings": "Prompt-based memory enables long-term behavior but relies on non-symbolic retrieval which can be less precise than symbolic DB operations for tasks requiring exact historical state.",
            "limitations_or_challenges": "Potential inaccuracy from similarity-based retrieval and lack of symbolic update/delete operations; limited interpretability relative to DBs.",
            "uuid": "e8423.3",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "RMT",
            "name_full": "Recurrent Memory Transformer (RMT)",
            "brief_description": "Transformer augmentation that introduces extra memory tokens/matrices to store and exchange local and global information for very long sequences; presented as a matrix-based memory approach.",
            "citation_title": "Recurrent memory transformer",
            "mention_or_use": "mention",
            "agent_name": "Recurrent Memory Transformer (RMT)",
            "agent_description": "A Transformer variant that uses recurrent memory tokens or memory matrices to extend effective context and allow read/write interactions across segments, cited as an example of matrix-based memory augmentation.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-sequence modeling / extended context tasks (general)",
            "task_description": "Handle very long input sequences by passing memory tokens/matrices between segments to preserve long-range information.",
            "task_type": "long-context modeling / memory-augmented sequence processing",
            "memory_used": true,
            "memory_type": "matrix-based memory tokens/matrices",
            "memory_mechanism": "Learned memory tokens/matrices appended to inputs/outputs and updated by the model; memory operations are controlled by the neural network (non-symbolic).",
            "memory_representation": "Trainable memory tokens or matrices (dense vectors) without explicit semantic labeling per-dimension.",
            "memory_retrieval_method": "Attention and learned update dynamics within the Transformer architecture (implicit, neural-controlled).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Discussed in Table 1 as matrix-based memory contrasted with ChatDB's symbolic DB; matrix approach supports read/write but with low interpretability compared to DBs.",
            "key_findings": "Matrix-based memory can track state but is non-symbolic and less interpretable; ChatDB argues symbolic DBs have advantages for precise record manipulation.",
            "limitations_or_challenges": "Low interpretability and possible inaccuracies due to implicit neural control of memory; not inherently supporting symbolic CRUD semantics.",
            "uuid": "e8423.4",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Retrieval-augmented models",
            "name_full": "Retrieval-augmented language models (Demonstrate-Search-Predict / RAG-style)",
            "brief_description": "A class of approaches that retrieve relevant external documents or embeddings to augment LLM prompts; exemplified by Khattab et al.'s Demonstrate-Search-Predict and other retrieval-augmented generation works.",
            "citation_title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-augmented models (RAG / Demonstrate-Search-Predict)",
            "agent_description": "Models that use a retriever to fetch relevant documents or passages (often via dense/sparse vector search) and include them in prompts to the LLM; discussed as a prompt-based memory family.",
            "model_name": null,
            "model_description": null,
            "task_name": "Knowledge-intensive QA / long-context tasks",
            "task_description": "Retrieve and insert supporting external context to allow LLMs to answer questions beyond their parametric knowledge or context window.",
            "task_type": "retrieval-augmented question answering / long-context reasoning",
            "memory_used": true,
            "memory_type": "prompt-based retrieval (document store + vector embeddings)",
            "memory_mechanism": "Index of documents/embeddings; retrieval via similarity search and insertion of retrieved passages into model prompt.",
            "memory_representation": "External documents and their vector embeddings.",
            "memory_retrieval_method": "Semantic similarity search (embedding-based retrieval) and then prompt concatenation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Paper contrasts retrieval-based methods with ChatDB, arguing retrieval (vector) methods are non-symbolic and prone to accumulated errors for multi-hop precise reasoning.",
            "key_findings": "Retrieval-augmented approaches help extend context but rely on approximate similarity and lack symbolic update semantics, which can be problematic for tasks needing precise state updates and multi-hop symbolic computation.",
            "limitations_or_challenges": "Similarity-based retrieval can be inaccurate and cause error accumulation, especially for long-horizon, stateful, numerically precise tasks.",
            "uuid": "e8423.5",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer: Language models can teach themselves to use tools",
            "brief_description": "A method that equips LMs to call external APIs/tools by demonstrating tool usage in prompts; cited as a tool-using memory-related approach.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "mention",
            "agent_name": "Toolformer",
            "agent_description": "Framework that trains/permits LMs to use external APIs/tools (including memory-like tools) via self-supervised demonstrations; used as an example of treating DBs as tools.",
            "model_name": null,
            "model_description": null,
            "task_name": "Tool-augmented reasoning / API usage",
            "task_description": "Use external tools (calculators, search, APIs) to improve LM capabilities and reliability.",
            "task_type": "tool-using / augmented reasoning",
            "memory_used": true,
            "memory_type": "external tools and APIs (can include memory stores)",
            "memory_mechanism": "LLM invokes external API/tool calls as part of its generated output; tools perform operations and return results to the model.",
            "memory_representation": "Tool outputs (could be structured results) rather than internal embeddings.",
            "memory_retrieval_method": "Explicit API/tool invocation decided by LM and integrated into prompt/continuation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Cited as representative of tool-using LLM paradigms; ChatDB can be viewed similarly (DB as a tool), but emphasizes DB's symbolic storage/operations for correctness.",
            "key_findings": "Tool-using approaches enable LMs to delegate computation/storage to external systems; ChatDB emphasizes that a database tool gives structured, symbolic state management beneficial for precise multi-hop tasks.",
            "limitations_or_challenges": "Requires correct tool-invocation policies and interface handling; quality depends on tool reliability and integration.",
            "uuid": "e8423.6",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memory augmented large language models are computationally universal",
            "rating": 2
        },
        {
            "paper_title": "Recurrent memory transformer",
            "rating": 2
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
            "rating": 2
        },
        {
            "paper_title": "Binding language models in symbolic languages",
            "rating": 1
        },
        {
            "paper_title": "Sql-palm: Improved large language modeladaptation for text-to-sql",
            "rating": 1
        }
    ],
    "cost": 0.01527625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatDB: Augmenting LLMs with Databases as Their SYMBOLIC MEMORY</h1>
<p>Chenxu Hu ${ }^{1 *}$ Jie Fu ${ }^{2 * \dagger}$ Chenzhuang Du ${ }^{1}$ Simian Luo ${ }^{1}$ Junbo Zhao ${ }^{3}$ Hang Zhao ${ }^{1 \dagger}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Beijing Academy of Artificial Intelligence ${ }^{3}$ Zhejiang University<br>fujie@baai.ac.cn hangzhao@mail.tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) with memory are computationally universal (Schuurmans, 2023). However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https: //chatdatabase.github.io/.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall workflow of ChatDB. The LLM controller controls the read and write operations to the memory. The memory stores historical information and provides relevant historical information to assist in responding to user input. In ChatDB, we focus on augmenting LLMs with databases as their symbolic memory.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>Large language models (LLMs), such as GPT-4 <em>(OpenAI, 2023)</em> and PaLM 2 <em>(Anil et al., 2023)</em>, have increasingly become an essential component of modern artificial intelligence (AI) systems, revolutionizing our understanding of natural language processing (NLP) and transforming various industries <em>(Hao et al., 2023; Wang et al., 2023)</em>. While LLMs have made significant strides in understanding and generating contextually relevant responses, they have limitations <em>(Chen et al., 2023)</em>. One of the main challenges is that multi-turn interactions with language models generate a large number of tokens, which can easily exceed the input token limit of LLMs. For example, GPT-4 (32K) can only handle 32,000 tokens. As the interaction progresses, the LLMs must maintain contextual information (e.g., user inputs, and previous responses) and generate responses based on the accumulated data. Simply concatenating all contextual information and cramming it into LLMs, however, can easily exceed the processing capabilities of LLMs and accumulate errors, causing the model to lose track of the conversation and produce less accurate responses.</p>
<p>Some neural memory mechanisms have been explored <em>(Wu et al., 2022a; Khattab et al., 2022; Zhong et al., 2022)</em> to overcome the limited token input issue of LLMs. The memory components serve as a storage and retrieval system for relevant information from previous interactions. However, augmenting LLMs with conventional neural memory usually leads to difficulties in storing, retrieving, and manipulating historical information in memory, especially for tasks requiring complex multi-hop reasoning. Two main causes are (a) They do not store historical information in a structured form; (b) Their manipulation of the information stored in memory is not symbolic, as they all rely on some vector similarity calculations, which can be inaccurate, thus leading to the accumulation of errors.</p>
<p>To address the aforementioned issues, we propose using databases as novel symbolic memory for LLMs. The whole framework is named ChatDB. As shown in Figure 1, ChatDB consists of two components: an LLM controller and its memory. The LLM controller can be any commonly used LLM <em>(OpenAI, 2023; Touvron et al., 2023; Du et al., 2022; Zeng et al., 2022)</em> and is responsible for controlling the read and write operations to the memory. The memory of LLMs, which can be symbolic or non-symbolic, or a combination of both, is responsible for storing historical information and providing information when needed to assist the LLM in responding to user input. In ChatDB, we focus on using databases as symbolic memory, which allows for the structured storage of historical information through the execution of a symbolic language, namely SQL statements. These SQL statements are generated by an LLM. Incorporating a database as symbolic memory is particularly useful in scenarios requiring precise recording, modification, querying, deletion, and analysis of historical data. For example, a store manager needs to maintain daily sales records, where using plain text or matrices as memory is unsuitable <em>(Chen et al., 2023)</em>. However, using a database as external symbolic memory is highly suitable. The database enables accurate operations, including data insertion, deletion, update, and selection, using SQL statements. Thus, employing databases as external symbolic memory ensures precision and efficiency in managing and manipulating historical data, significantly enhancing the performance of LLMs in scenarios that require high accuracy and long-term data recording and processing.</p>
<p>In the ChatDB framework, we propose the chain-of-memory (CoM) approach to manipulate the external symbolic memory more effectively, thereby further enhancing the reasoning capabilities of LLMs. The chain-of-memory approach transforms user input into a series of intermediate memory operation steps that lead to final results. Through the chain-of-memory approach, a complex problem is decomposed into multiple steps of memory operations, significantly reducing the complexity of problem-solving. In ChatDB, each intermediate step involves one or more SQL statements.</p>
<p>Our ChatDB makes several contributions to the field of LLMs. Firstly, we propose augmenting LLMs with databases as their external symbolic memory, allowing for structured storage of historical data and enabling symbolic and complex data operations using SQL statements. Secondly, our chain-of-memory approach enables effective memory manipulation by converting user input into multi-step intermediate memory operations, which enhance the performance of ChatDB, enabling it to handle complex, multi-table database interactions with improved accuracy and stability. Finally, our experiments demonstrate that augmenting LLMs with symbolic memory improves multi-hop reasoning capabilities and prevents error accumulation, thereby enabling ChatDB to significantly outperform ChatGPT on a synthetic dataset.</p>
<h2>2 Related Work</h2>
<p>Memory-Augmented Large Language Models (LLMs). LLMs, such as GPT-4 <em>(OpenAI, 2023)</em> and PaLM 2 <em>(Anil et al., 2023)</em>, have demonstrated powerful reasoning and decision-making abilities. However, LLMs are often hindered by their limited context window sizes (e.g., GPT-4 can only handle 32K tokens). Memory-augmented LLMs <em>(Wu et al., 2022a, b; Zhong et al., 2022; Lewis et al., 2020; Guu et al., 2020; Park et al., 2023; Khattab et al., 2022; Izacard et al., 2022)</em> incorporate a memory module that prevents the model from forgetting crucial information and allows it to handle long text inputs that exceed the context window size. Retrieval-augmented in-context learning <em>(Khattab et al., 2022)</em></p>
<p>uses retrieval models (RM) to retrieve relevant information that can be inserted into the LLM as a prompt. For example, Auto-GPT ${ }^{3}$ and Generative Agents (Park et al., 2023) utilize a memory module to store the text prompt directly, allowing the agent to keep track of its history. The past and current prompts are then input into the LLM for processing. Neural Turing Machines (NMT) (Graves et al., 2014), which incorporate the recurrent neural network (RNN) with external trainable memory resources and learn to interact with the memory module with gradient descent. Gated Graph Sequence Neural Network (GGS-NN) (Johnson, 2017) constructs and modifies graphs and utilizes the graphs to produce reasonable outputs. Recurrent Memory Transformer (RMT) (Bulatov et al., 2022) introduces additional memory tokens to the input and output sequences to store, process and exchange local and global information between segments of long sequences, and then train the model to control both memory operation and sequence representations processing.
Reasoning with LLMs. LLMs are known to struggle in complex reasoning tasks. Previous methods focus on incorporating specially designed supervisory signals or fine-tuning to enhance the reasoning ability of language models (Pikos et al., 2021; Ran et al., 2019; Andor et al., 2019; Cobbe et al., 2021; Chen et al., 2022). Recent methods mainly improve the reasoning ability of language models through In-Context Learning (Brown et al., 2020; Lester et al., 2021; Wei et al., 2021, 2022; Wang et al., 2022). The most representative of these is Chain-of-Thought (CoT) (Wei et al., 2022), which presents the intermediate reasoning process of solving sample problems to the language model, greatly enhancing its reasoning capabilities.
LLMs with DBs. LLMs have demonstrated an impressive capability in generating code, including Python code, execution commands for Excel, and Structured Query Language (SQL) for databases (OpenAI, 2023). ChatExcel ${ }^{4}$ uses LLMs to generate the Excel execution command, simplifying the user interaction process. BINDER (Cheng et al., 2022) proposes a framework that maps task inputs to executable programs in a programming language (e.g., Python code) bound with an API to call LLMs to perform a wide range of functionalities. SQL-PALM (Sun et al., 2023) proposes an LLM-based Text-to-SQL model, using the execution-based self-consistent prompting approach, and outperforms previous Text-2-SQL methods by a large margin. While previous works involve databases to some extent, our proposed ChatDB system differs significantly from these methods. In specific, ChatDB views the databases as the external symbolic memory module for the LLM, and then leverages the database for reading and writing essential data information to enhance the reasoning process via chain-of-memory, leading to more accurate reasoning results.
Tool-using LLMs. From the tool-using perspective, ChatDB can also be seen as an LLM utilizing DBs as a tool (Schick et al., 2023; Shen et al., 2023; Surs et al., 2023; Paranjape et al., 2023). Toolformer (Schick et al., 2023), through a series of demonstrations, instructs the language model that it can invoke some APIs to utilize external tools to solve the current problem. Another representative work is Auto-GPT ${ }^{5}$, which enables the language models to complete a series of impressive tasks using a search engine. The advantage of ChatDB, which uses a database as an external tool, is that it allows the language model to maintain more accurate records and use historical data, thereby addressing more complex problems, especially those that require accurate historical data for reasoning.</p>
<h1>3 ChatDB</h1>
<p>In this section, we first briefly introduce the task definition and setting. We then describe the overall framework of our proposed ChatDB. Finally, we delve into the details of the chain-of-memory approach, which is the main component of ChatDB.</p>
<h3>3.1 Task Definition</h3>
<p>Given a user input in natural language and details of existing tables in the database (not required if there are no existing tables), the goal is to manipulate the symbolic memory, i.e., the external database, to fulfill the user's request. For instance, if the user (e.g., a store manager) command is to record, modify, query, and delete specific data, the corresponding SQL operation should be to insert, update, select, and delete the relevant data in the appropriate tables, respectively. These operations often involve multiple tables in the database.</p>
<h3>3.2 Framework Overview</h3>
<p>ChatDB framework consists of three main stages: input processing, chain-of-memory, and response summary, as illustrated in Figure 2. Algorithm 1 provides a detailed illustration of the entire algorithmic procedure of ChatDB to respond to the user input.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: ChatDB framework. The red arrow lines represent the process flow of chain-of-memory, indicating the connection between multiple memory operations. The red arrow lines between database tables represent the reference relationships between primary keys and foreign keys, which start from primary keys to foreign keys. Only the first four columns of each table are shown for brevity. This example showcases the process of returning goods purchased on 2023-01-02 by a customer with the phone number 823451.</p>
<p>Input Processing. ChatDB generates a series of intermediate steps to manipulate the symbolic memory by utilizing LLMs if responding to the user input requires the use of memory. Otherwise, we use LLMs directly to generate a reply.
Chain-of-Memory. ChatDB executes a series of intermediate memory operation steps to interact with symbolic memory. ChatDB manipulates the symbolic memory in sequence according to a series of previously generated SQL statements, including operations such as insert, update, select, delete, etc. The external database executes the corresponding SQL statements, updates the database, and returns the results. It is worth noting that ChatDB decides whether to update the memory operation step based on the results of previous SQL statements before executing this operation. ChatDB executes the next step following the same procedure until all operations on the memory are completed.
Response Summary. ChatDB summarizes the final response to the user based on the results of a series of chain-ofmemory steps.</p>
<h1>3.3 Chain-of-Memory</h1>
<p>Chain-of-thought (Wei et al., 2022) emphasizes decomposing complex reasoning into a series of intermediate steps. Chain-of-memory (CoM) can be seen as a way to augment chain-of-thought by providing a symbolic memory mechanism to support the storage associated with those intermediate steps.
The purpose of chain-of-memory is to enhance the reasoning capabilities and robustness of LLMs when manipulating symbolic memory. The approach involves converting the user input into a sequence of intermediate memory operations, enabling LLMs to more accurately and effectively manipulate the memory in a symbolic way. The ability to manipulate symbolic memory is particularly valuable for real-world applications that involve complex and accurate interactions with historical data, such as record-keeping and data analysis in a management setting.
To enhance the performance and stability of our approach, we employ in-context learning (Brown et al., 2020), providing prompt exemplars of several sequences of chain-of-memory steps, and chain-of-thought prompting. A robust and accurate chain-of-memory process enables LLMs to better reason over symbolic memory and handle more complex scenarios.</p>
<p>The advantages of chain-of-memory are twofold. Firstly, it enables LLMs to perform complex database manipulations with greater accuracy, enhancing their multi-hop reasoning ability over symbolic memory. Secondly, by decomposing complex operations into a sequence of intermediate memory operations, the chain-of-memory approach enhances the capability of LLMs when handling complex, multi-table interactions. This approach enables LLMs to handle edge cases and unexpected scenarios better, making it a promising method for real-world applications.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">algorithm</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">ChatDB</span>
<span class="nv">Input</span>:<span class="w"> </span><span class="nv">userInput</span>,<span class="w"> </span><span class="nv">dataBase</span>
<span class="nv">Output</span>:<span class="w"> </span><span class="nv">reply</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">manipulate</span><span class="w"> </span><span class="nv">memory</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">respond</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">user</span><span class="w"> </span><span class="nv">Input</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nv">memOps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">getSteps</span><span class="ss">(</span><span class="nv">userInput</span><span class="ss">)</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span><span class="nv">reply</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="ss">(</span><span class="nv">userInput</span><span class="ss">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nv">reply</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="nv">sqlResults</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>[]
<span class="w">    </span><span class="nv">newMemOps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>[]
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">memOp</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">memOps</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">update</span><span class="w"> </span><span class="nv">memOp</span><span class="w"> </span><span class="nv">based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">sqlResults</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="nv">newMemOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">updateOperation</span><span class="ss">(</span><span class="nv">memOp</span>,<span class="w"> </span><span class="nv">sqlResults</span><span class="ss">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="nv">newMemOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">memOp</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="nv">sqlResult</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">executeOperation</span><span class="ss">(</span><span class="nv">newMemOp</span>,<span class="w"> </span><span class="nv">dataBase</span><span class="ss">)</span>
<span class="w">        </span><span class="nv">sqlResults</span>.<span class="nv">append</span><span class="ss">(</span><span class="nv">sqlResult</span><span class="ss">)</span>
<span class="w">        </span><span class="nv">newMemOps</span>.<span class="nv">append</span><span class="ss">(</span><span class="nv">newMemOp</span><span class="ss">)</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="nv">reply</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">summary</span><span class="ss">(</span><span class="nv">userInput</span>,<span class="w"> </span><span class="nv">newMemOps</span>,<span class="w"> </span><span class="nv">sqlResults</span><span class="ss">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nv">reply</span>
</code></pre></div>

<p>$\triangleright$ Begin of Input Processing
$\triangleright$ Generate intermediate steps using LLMs
$\triangleright$ Generate reply using LLMs directly
$\triangleright$ Begin of Chain-of-Memory</p>
<h1>3.4 Comparison with previous memory-augmented LLMs</h1>
<p>Table 1: Comparison with prompt-based memory and matrix-based memory.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Memory format</th>
<th style="text-align: center;">Supported operations</th>
<th style="text-align: center;">Memory storage</th>
<th style="text-align: center;">Memory execution</th>
<th style="text-align: center;">Interpretability</th>
<th style="text-align: center;">State tracking</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">ChatDB</td>
<td style="text-align: center;">Symbolic storage <br> (e.g., database)</td>
<td style="text-align: center;">Insert, Delete Update, Select</td>
<td style="text-align: center;">Structured</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-based</td>
<td style="text-align: center;">Auto-GPT</td>
<td style="text-align: center;">Content and its vector embeddings</td>
<td style="text-align: center;">Insert, Select</td>
<td style="text-align: center;">Semi-Structured</td>
<td style="text-align: center;">Non-Symbolic</td>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;">Matrix-based</td>
<td style="text-align: center;">RMT</td>
<td style="text-align: center;">Memory tokens/ memory matrices</td>
<td style="text-align: center;">Read, Write</td>
<td style="text-align: center;">Semi-Structured</td>
<td style="text-align: center;">Non-Symbolic</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Yes</td>
</tr>
</tbody>
</table>
<p>In this subsection, we provide a comprehensive comparison between ChatDB and recent approaches that augment Transformer-based language models with memory module. The memory modules of language models proposed in previous work can be broadly categorized into two types. The first type of memory stores context and uses a retrieval model to find content from past interactions that's most relevant to the current conversation, and then uses it as a prompt for the language model (Khattab et al., 2022). We refer to this type of memory as prompt-based memory. The second type of approach utilizes additional memory tokens or memory matrices as memory (Bulatov et al., 2022), which we refer to as matrix-based memory. We compare ChatDB with these approaches based on the following aspects:</p>
<ol>
<li>Memory Format. This aspect relates to the format used for storing memory. ChatDB utilizes databases as its memory. Prompt-based memory (Park et al., 2023) stores relevant interaction content and/or their corresponding vector embeddings. Matrix-based memory employs additional trainable memory tokens (Bulatov et al., 2022, 2023) or trainable memory matrices (Graves et al., 2014).</li>
<li>Supported Operations. This aspect refers to the operations supported for manipulating memory. ChatDB supports operations such as insertion, deletion, update, and selection of data in the database memory. The prompt-based memory primarily supports insertion and selection operations but lacks complete support for updates and deletions. Matrix-based memory supports reading (selection) and writing (insertion, updating, deletion) operations. However, the exact operations performed by the neural network are not explicitly known.</li>
<li>Memory Storage. This aspect refers to the format in which data is stored in memory, specifically whether it is structured or not. ChatDB stores memory in a structured format using databases, while both the prompt-based memory</li>
</ol>
<p>and matrix-based memory are viewed as semi-structured. The reason is that vector embeddings and memory matrices have specific dimensions and sizes, but each individual dimension does not carry a specific and explicit meaning.
4. Memory Execution. This aspect focuses on how memory operations are executed, specifically whether they are symbolic or not. ChatDB executes operations on its database memory using SQL, which is a symbolic language, thus making it inherently symbolic. Prompt-based memory performs selections based on a similarity metric using vector embeddings and employs a language encoder to obtain vector embeddings for insertions. Both of these are considered non-symbolic executions. In matrix-based memory augmentated LLMs, memory operations are entirely controlled by the neural network, resulting in non-symbolic executions as well.
5. Interpretability. This aspect refers to the degree of interpretability of memory. In ChatDB, memory is stored in a structured and explicit format, and its operations are symbolic, leading to a high level of interpretability. In the prompt-based memory, interpretability is generally limited due to inherent challenges in interpreting vector embeddings. For matrix-based memory approach, the degree of interpretability is low since memory is completely controlled by the neural network implicitly.
6. State Tracking. This aspect refers to whether the memory effectively tracks the current state of the LLMs. In the case of ChatDB, its memory accurately tracks the current state of the LLMs. The fruit shop experiment serves as a demonstration, where after processing each record, ChatDB's database memory is updated to reflect the latest state of the fruit shop. This showcases how ChatDB's memory effectively tracks its current state. Thanks to the symbolic memory execution, ChatDB's memory allows for easy rollback to any desired timestamp, providing greater flexibility and controllability. In matrix-based memory approach, memory is continuously updated and altered by the model itself, enabling it to track the current state of the LLMs. However, the prompt-based memory approach simply stores the historical context and only knows what has happened in the past without a clear understanding of the current state.
By examining these aspects, we observe the distinctive features and capabilities of ChatDB in comparison to existing approaches. The superiority of ChatDB highlights the advantages of using symbolic memory to augment LLMs.</p>
<h1>4 Evaluation</h1>
<p>In this section, we conduct experiments to evaluate the effectiveness of augmenting LLMs with databases as their symbolic memory. Our experiment results demonstrate that ChatDB significantly outperforms the baseline model ChatGPT, highlighting the advantages of symbolic memory integration.</p>
<h3>4.1 Experimental Setup</h3>
<p>As previously mentioned, using a database as symbolic memory is particularly suitable for scenarios that require precise recording and processing of historical information, such as various data management scenarios. To adapt to the use case of ChatDB and enable quantitative comparisons with other models, we constructed a synthetic dataset simulating the management of a fruit shop.
Furthermore, in order to evaluate the performance of the models, we collected a set of 50 questions with annotated standard answers. These questions varied in difficulty, ranging from hard ones that required multi-hop reasoning to easy ones that only required retrieving information from historical data. There are 15 easy questions and 35 hard questions. Each question is independently answered by the models.</p>
<h3>4.1.1 Model Configuration</h3>
<p>ChatDB. The LLM used in ChatDB is ChatGPT (GPT-3.5 Turbo), and the hyperparameter temperature is set to 0 . We used the MySQL database as the external symbolic memory.
Baseline. We use ChatGPT (GPT-3.5 Turbo) as the baseline model with a maximum token length of 4096. Similar to ChatDB, we set the temperature to 0 .</p>
<h3>4.1.2 Dataset</h3>
<p>We synthesize a dataset of fruit shop management records, referred to as the "Fruit Shop Dataset". This dataset simulates four common operations in a shop: purchasing, selling, changing prices, and goods returns. We ensure that all historical records are valid and do not encounter issues such as negative inventory. We generate 70 records arranged in chronological order, totaling approximately 3.3 k tokens, which is within the maximum token length limit of ChatGPT (4096 tokens).</p>
<p>(a) Purchasing goods: the shop purchases goods from suppliers.</p>
<p>Record: We restocked our store on 2023-01-01 with a new supply of fruits from "ABC" (abc_sup@example.com, 10080). The purchased quantities include 24 kg cherry, at unit prices of 0.8 . Our intended selling price of cherry is 1.3 dollars per unit.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Insert supplier 'ABC' if not exists
INSERT INTO suppliers (supplier_name, contact_number, email)
SELECT 'ABC', "10080", 'abc_sup@example.com'
WHERE NOT EXISTS (SELECT 1 FROM suppliers WHERE supplier_name $\sim$ - "ABC");
Step2: Insert fruit (set the selling price to NULL and stock quantity to 0 ) if not exists</p>
<p>INSERT INTO fruits (fruit_name, selling_price, stock_quantity, $\sim$ fruit_type, shelf_life)
SELECT 'cherry', NULL, 0, NULL, NULL
WHERE NOT EXISTS (SELECT 1 FROM fruits WHERE fruit_name $\sim$ 'cherry');
Step3: Insert purchase
INSERT INTO purchases (supplier_id, purchase_date, $\sim$ total_cost)
VALUES (SELECT supplier_id FROM suppliers WHERE
$\sim$ supplier_name - "ABC"), "2023-01-01", 24 - 0.8);
Step4: Insert purchase item
INSERT INTO purchase_items (purchase_id, fruit_id,
$\sim$ quantity_purchased, cost_per_item, item_total_cost)
VALUES (SELECT MAX(purchase_id) FROM purchases), (SELECT
$\sim$ fruit_id FROM fruits WHERE fruit_name - "cherry"), 24,
$\sim 0.8,24$ - 0.8);
Step5: Update the stock quantity of cherry
UPDATE fruits
SET stock_quantity = stock_quantity + 24
WHERE fruit_name = "cherry";
Step6: Update the selling price of cherry if given new selling price
UPDATE fruits
SET selling_price $=1.3$
WHERE fruit_name = "cherry";
(c) Return of goods: customers bring back purchased items to the shop.</p>
<p>Record: Because the customer returned their purchase, we are required to undo the sales transaction made by customer 'John Doe' (phone: 123-4567890, email: john.doe@example.com) on 2023-01-05.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Find the sale_id for this customer on this date
SELECT sale_id FROM sales
WHERE customer_id = (SELECT customer_id FROM customers WHERE
$\sim$ phone_number = "123-456-7890" AND email =
"john.doe@example.com") AND sale_date = "2023-01-05";
Database response:
$\qquad$
| sale_id |
$+$............
1 9
$+$............
Step2: Get all the fruit_id and quantity_sold for this sale, replace <sale_id> with the results from the previous queries
SELECT fruit_id, quantity_sold FROM sale_items
WHERE sale_id = 9 [<sale_id>];
Database response:
$+$.............
| fruit_id | quantity_sold |
$+$.............
| 7 | 10
1 0
$+$............
$\square$</p>
<h2>(b) Selling goods: customers buy goods from the shop</h2>
<p>Record: A sale was made on 2023-01-02 to 'Bob Smith' (contact details: 123-456-7893, bob.smith@example.com). The items purchased were 9 kg apple, 4 kg cherry.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Insert customer 'Bob Smith' if not exists
INSERT INTO customers (first_name, last_name, phone_number, $\sim$ email)
SELECT 'Bob', 'Smith', "123-456-7893",
$\sim$ 'bob.smith@example.com'
WHERE NOT EXISTS (SELECT 1 FROM customers WHERE phone_number $\sim$ = "123-456-7893");
Step2: Insert sale
INSERT INTO sales (customer_id, sale_date, total_price)
VALUES (SELECT customer_id FROM customers WHERE phone_number
$\sim$ = "123-456-7893"), "2023-01-02", (SELECT selling_price
$\sim$ FROM fruits WHERE fruit_name = "apple") + 9 + (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "cherry") + $\sim$ 4);</p>
<p>Step3: Insert sale item
INSERT INTO sale_items (sale_id, fruit_id, quantity_sold,
$\sim$ price_per_item, item_total_price)
VALUES (SELECT MAX(sale_id) FROM sales), (SELECT fruit_id
$\sim$ FROM fruits WHERE fruit_name = "apple"), 9, (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "apple"),
$\sim$ (SELECT selling_price FROM fruits WHERE fruit_name =
$\sim$ "apple") + 9),
((SELECT MAX(sale_id) FROM sales), (SELECT fruit_id FROM
$\sim$ fruits WHERE fruit_name = "cherry"), 4, (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "cherry"),
$\sim$ (SELECT selling_price FROM fruits WHERE fruit_name =
$\sim$ "cherry") + 4);
Step4: Update the stock quantity of apple and cherry
UPDATE fruits
SET stock_quantity = CASE
WHERE fruit_name = "apple" THEN stock_quantity = 9
WHERE fruit_name = "cherry" THEN stock_quantity = 4
ELSE stock_quantity
END
WHERE fruit_name IN ("apple", "cherry");</p>
<p>Step3: Increase the stock_quantity for each fruit sold in this sale, replace <quantity_sold> <fruit_id> with the results from the previous queries
UPDATE fruits
SET stock_quantity = stock_quantity + 10 [<quantity_sold>]
WHERE fruit_id = 7 [<fruit_id>];
UPDATE fruits
SET stock_quantity = stock_quantity + 9 [<quantity_sold>]
WHERE fruit_id = 6 [<fruit_id>];
Step4: Delete the sale items for this sale, replace <sale_id> with the results from the previous queries
DELETE FROM sale_items WHERE sale_id = 9 [<sale_id>];
Step5: Delete the sale record, replace <sale_id> with the results from the previous queries
DELETE FROM sales WHERE sale_id = 9 [<sale_id>];
(d) Changing prices: modify the price of the goods.</p>
<p>Record: On 2023-01-05, the sale price of pear in the store was changed to 1.6 dollar per unit.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Update the selling price of pear
UPDATE fruits
SET selling_price $=1.6$
WHERE fruit_name = "pear";</p>
<p>Figure 3: Examples showcasing ChatDB's response to the four common operations in the records of the Fruit Shop Dataset. " 9 [<sale_id>]" represents that <sale_id> is replaced with 9 in the updated SQL statement. Due to page size limitations, some unnecessary details and ChatDB's summary are omitted.</p>
<p>Suppose you are a fruit shop manager and good at analyzing history records.
The fruit shop newly opened on January 1, 2023. Given the history records for the fruit shop in January 2023, which include customer names, transaction dates, fruit prices, quantities purchased, and whether the items were returned, you need to answer some questions.
By default, exclude the sales transactions that have been returned when performing calculations.
Here are the historical records of the fruit shop, which are arranged in chronological order based on the occurrence time, surrounded by triple backticks:
$\cdots$
${$ records $}$
$\cdots$
Based on the history records, answer the question about the fruit shop:
${$ question $}$
Figure 4: Prompt for ChatGPT to answer questions in the Fruit Shop Dataset. The placeholders "records" and "question" will be replaced by specific details when it is actually used.</p>
<p>Why do we limit the token length of the dataset? If the token length of the dataset exceeds the maximum token length of ChatGPT, memory becomes necessary. However, the mainstream memory retrieval methods based on vector embedding are prone to errors. This inevitably leads to a decline in the performance of ChatGPT, which is not desired. Therefore, we deliberately design the token length of the dataset to be within the maximum token length of ChatGPT to avoid using memory and maximize the model's performance. Note that ChatDB's performance is generally unaffected by the token length of the dataset. Thus, if ChatDB outperforms ChatGPT when the dataset is small, it indicates that ChatDB is also superior to memory-augmented ChatGPT when the dataset is large.</p>
<h1>4.1.3 Processing records</h1>
<p>For ChatDB, the first step is to initialize the database. We need to generate a reasonable database schema for the specific task scenario and create tables in the database. The generation of the database schema can be done manually or using LLMs. Next, for each record in the dataset, ChatDB processes them one by one. Using the LLM controller, ChatDB manipulates the external database (i.e., symbolic memory) following Algorithm 1. We provide examples of ChatDB's response to the four common operations in the Fruit Shop Dataset, namely purchasing, selling, changing prices, and goods returns, as shown in Figure 3. It is worth emphasizing that ChatDB processes record one by one, so it is not sensitive to the total number of records. Furthermore, each step of the database operation in ChatDB is symbolic and free from errors. Therefore, in theory, ChatDB can handle an infinite number of historical records without sacrificing performance. However, for ChatGPT or existing memory-augmented LLMs, excessively long historical records can significantly degrade performance. In this experiment, as for the ChatGPT baseline, since the records are not long, we simply treat them as part of the prompt.</p>
<h3>4.1.4 Answering questions</h3>
<p>When answering questions, ChatDB no longer requires records to be part of the prompt. After processing the records, the information is stored in symbolic memory. Following Algorithm 1, ChatDB utilizes SQL statements to perform a series of database queries (including calculations) in order to answer the question. On the other hand, ChatGPT includes records as part of the prompt and directly asks the question. The prompt template is shown in Figure 4.</p>
<h3>4.2 Results</h3>
<p>The experimental results are presented in Table 2, which clearly demonstrate that ChatDB outperforms ChatGPT with significantly higher accuracy. While ChatGPT is capable of answering easy questions, it falls short in handling hard questions that necessitate multi-hop reasoning and precise calculations. Consequently, ChatGPT exhibits a low accuracy rate for these hard questions. In contrast, ChatDB exhibits a notably high accuracy rate, underscoring the advantages of utilizing a database as symbolic memory. This approach not only prevents error accumulation but also enhances the multi-hop reasoning and precise calculation capabilities of LLMs.
We present several examples of the two models answering questions in Figure 5 for comparison. In all these examples, ChatDB correctly answers the questions while ChatGPT fails. ChatGPT often exhibits errors in calculating the total price of each sale transaction, as observed in Figure 5(a). Sometimes, the formulas are correct but the calculations are wrong, while other times, even the formulas are incorrect. In addition, ChatGPT struggles to find all valid sale</p>
<p>(a) Question: What was the total revenue for January 2023?</p>
<h2>ChatGPT:</h2>
<p>To calculate the total revenue for January 2023, we need to sum up the revenue generated from each sales transaction. We can use the following formula to calculate the revenue for each transaction: revenue $=$ quantity $*$ selling price We also need to exclude the sales transactions that have been returned. Based on the history records, the following sales transactions have been returned:</p>
<ul>
<li>Alice Smith on 2023-01-17 - Bob Smith on 2023-01-05 [...]</li>
</ul>
<p>Therefore, we need to exclude these transactions when calculating the total revenue. Here is the calculation for each sales transaction:</p>
<ul>
<li>Bob Smith on 2023-01-02: $(9 \mathrm{~kg} * 3.8 \mathrm{USD} / \mathrm{kg})+(4 \mathrm{~kg} * 1.3 \mathrm{USD} / \mathrm{kg})=39.1$ USD [WRONG]: calculation error, should be 39.4</li>
<li>Sue Davis on 2023-01-02: $1 \mathrm{~kg} * 3.8$ USD/kg $=3.8$ USD</li>
<li>Bob Smith on 2023-01-05: $(5 \mathrm{~kg} * 1.8 \mathrm{USD} / \mathrm{kg})+(2 \mathrm{~kg} * 3.0 \mathrm{USD} / \mathrm{kg})+(6 \mathrm{~kg}$ * 2.8 USD/kg) $=47.6$ USD [WRONG]: calculation error and this transaction has been returned.
[...] [WRONG]: most of the sale transactions are calculated incorrectly.
[STOP]: Because it exceeds the maximum token length.
(b) Question: Which day in January 2023 had the highest revenue?</li>
</ul>
<h2>ChatGPT:</h2>
<p>To calculate the revenue for each day, we need to sum up the total sales for that day (excluding any returned items) and multiply by the selling price. We can then compare the revenue for each day to determine which day had the highest revenue.
Here is the breakdown of sales for each day:</p>
<ul>
<li>January 2: 9 kg apple at $3.8=34.2,4 \mathrm{~kg}$ cherry at $1.3=5.2,3 \mathrm{~kg}$ cherry at $1.3=$ $3.9,5 \mathrm{~kg}$ orange at $2.1=6.3$, total sales $=49.6$ [WRONG]: should be 43.2</li>
<li>January 4: 2 kg apple at $1.5=3,4 \mathrm{~kg}$ pear at $1.6=6.4,7 \mathrm{~kg}$ cherry at $1.3=9.1$, total sales $=18.5$ [WRONG]: should be 27.9</li>
<li>January 5: 5 kg cherry at $1.9=9.5,10 \mathrm{~kg}$ orange at $1.8=18,1 \mathrm{~kg}$ cherry at 1.9 $=1.9,1 \mathrm{~kg}$ grape at $3.2=3.2$, total sales $=32.6$ [WRONG]: should be 83</li>
<li>January 6: 3 kg apple at $1.5=4.5,10 \mathrm{~kg}$ grape at $3.2=32$, total sales $=36.5$ [WRONG]: should be 29.4</li>
<li>January 7: 4 kg pear at $1.6=6.4$, total sales $=6.4$ [WRONG]: should be 37.2</li>
<li>January 8: 6 kg peach at $3=18$, total sales $=18$ [WRONG]: should be 21.6
[...] [WRONG]: most of the daily revenues are calculated incorrectly.
[STOP]: Because it exceeds the maximum token length.</li>
</ul>
<h2>ChatDB:</h2>
<p>Begin to interact with the external database (symbolic memory).
Step1: Calculate the total revenue for January 2023
SELECT SUM (total_price) AS total_revenue
FROM sales
WHERE sale_date &gt;= <em>2023-01-01</em> AND sale_date &lt; <em>2023-02-01</em>;
Database response:
{
\total_revenue
$\mid$
707.8
}</p>
<h2>ChatDB:</h2>
<p>Begin to interact with the external database (symbolic memory).
Step1: Calculate the total revenue for each day in January 2023
SELECT sale_date, SUM (total_price) AS revenue
FROM sales
WHERE sale_date BETWEEN <em>2023-01-01</em> AND <em>2023-01-31</em> GROUP BY sale_data;</p>
<p>Database response:
{
\sale_date | revenue
}
2023-01-02 43.2
2023-01-04 27.9
Step2: Find the day with the highest revenue
SELECT sale_date, SUM (total_price) AS revenue
FROM sales
WHERE sale_date BETWEEN <em>2023-01-01</em> AND <em>2023-01-31</em> GROUP BY sale_date ORDER BY revenue DESC LIMIT 1;</p>
<p>Database response:
}
\sale_date | revenue
}
2023-01-30 119.7
}
(c) Question: What was the average weight of fruit per sales transaction in January 2023?</p>
<p>ChatGPT: To calculate the average weight of fruit per sales transaction, we need to first identify all the sales transactions in January 2023 and calculate the total weight of fruit sold in those transactions. Then, we can divide the total weight by the number of sales transactions to get the average weight per transaction. Here are the sales transactions in January 2023:</p>
<ul>
<li>Bob Smith on 2023-01-02: 9 kg apple, 4 kg cherry</li>
<li>Sue Davis on 2023-01-02: 1 kg apple</li>
<li>Bob Smith on 2023-01-05: 5 kg orange, 2 kg peach, 6 kg pear [WRONG]: this transaction has been returned.</li>
<li>Sue Davis on 2023-01-05: 5 kg pear, 10 kg orange, 1 kg cherry</li>
<li>Alice Smith on 2023-01-05: 5 kg cherry
[...] [WRONG]: not find out all valid sale transactions.
To calculate the total weight of fruit sold, we can add up the quantities of each fruit sold in the above transactions:</li>
<li>Apples: 16 kg - Cherries: 12 kg - Oranges: 10 kg - Pears: 18 kg - Kiwis: 12 kg Grapes: 18 kg - Bananas: 27 kg - Strawberries: 7 kg - Peaches: 9 kg - Plums: 8 kg
The total weight of fruit sold in January 2023 is 127 kg . [WRONG]: It should be 278. Even according to the calculation above, the total should be 137, not 127. To calculate the average weight per sales transaction, we need to divide the total weight by the number of sales transactions. There are 18 sales transactions in January 2023, [WRONG]: It should be 28 valid sale transactions in total. so the average weight [STOP]: Because it exceeds the maximum token length.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">ChatDB:</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Begin to interact with the external database (symbolic memory).</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Step1: Calculate the total weight of fruit sold in January 2023</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SELECT SUM(si.quantity_sold) as</td>
<td style="text-align: left;">Database response:</td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$ total_weight</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FROM sale_items si</td>
<td style="text-align: left;">total_weight</td>
</tr>
<tr>
<td style="text-align: left;">JOIN sales $x$ ON si.sale_id = s.sale_id</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WHERE s.sale_date BETWEEN <em>2023-01-01</em></td>
<td style="text-align: left;">278</td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$ AND <em>2023-01-31</em></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Step2: Count the number of sales transactions in January 2023
SELECT COUNT (DISTINCT s.sale_id) as
$\rightarrow$ num_sales
FROM sales $x$
WHERE s.sale_date BETWEEN <em>2023-01-01</em> $\rightarrow$ AND <em>2023-01-31</em></p>
<div class="codehilite"><pre><span></span><code>\s avg_weight_per_sale
</code></pre></div>

<p>Step3: Calculate the average weight of fruit per sales transaction in January 2023, replace <total_weight> and <num_sales> with the results from the previous queries</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SELECT 278</td>
<td style="text-align: left;"><total_weight></td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$</td>
<td style="text-align: left;">$&lt;$ <num_sales></td>
</tr>
<tr>
<td style="text-align: left;">as avg_weight_per_sale</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">9.93</td>
</tr>
</tbody>
</table>
<p>Figure 5: Examples of ChatDB and ChatGPT answering questions. While ChatGPT fails to answer any of the questions correctly, ChatDB successfully answers all of them. The issues with ChatGPT are highlighted in red. Some irrelevant details and ChatDB's summary are omitted for brevity. The gray ellipsis [...] indicates that the response is trimmed.</p>
<p>Table 2: The experimental results of answering questions in the Fruit Shop Dataset. There are 50 questions in total, with 15 being easy questions and 35 being hard questions.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Easy</th>
<th>Hard</th>
<th>All</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>$10 / 15$</td>
<td>$1 / 35$</td>
<td>$11 / 50$</td>
<td>$22 \%$</td>
</tr>
<tr>
<td>ChatDB (ours)</td>
<td>$13 / 15$</td>
<td>$28 / 35$</td>
<td>$41 / 50$</td>
<td>$82 \%$</td>
</tr>
</tbody>
</table>
<p>transactions, leading to errors in its answering process. This issue is commonly encountered and evident in all these examples. Furthermore, ChatGPT tends to make sequential errors, resulting in significant error accumulation.</p>
<p>In contrast, ChatDB performs quite well in these examples. During the initial processing of records, symbolic operations (i.e., SQL operations) are applied to manipulate the database (i.e., symbolic memory), ensuring that all information is stored in a structured form within the database. When answering questions, ChatDB generates SQL statements to query the database. The three examples demonstrate the effectiveness of ChatDB in solving problems requiring one, two, and three chain-of-memory steps, respectively. We can observe that ChatDB accurately answers the questions, and the execution logic of the chain-of-memory is clear, with each step tightly connected and approaching the ultimate answer. From these examples, the advantages of ChatDB are evident in two aspects:</p>
<ol>
<li>Through the chain-of-memory approach, complex problems are decomposed into multiple steps of memory operations, simplifying the problem's complexity. Each step's result is accurately stored as an intermediate outcome and used in subsequent steps, which greatly assists in complex reasoning.</li>
<li>Symbolic memory enables precise operations and calculations. ChatDB delegates many calculational tasks to the external database by executing SQL statements, ensuring the accuracy of each step and preventing error accumulation.</li>
</ol>
<p>In summary, by leveraging external databases as symbolic memory, ChatDB significantly outperforms ChatGPT in this experiment.</p>
<h1>5 Conclusion</h1>
<p>In this paper, we introduce ChatDB, a framework that augments LLMs with symbolic memory in the form of databases. We demonstrate the advantages and capabilities of symbolic memory and chain-of-memory approach in enhancing complex reasoning and preventing error accumulation. By providing a precise storage mechanism for intermediate results, symbolic memory enables accurate and reliable operations. Moreover, the use of symbolic languages, such as SQL, allows symbolic computation and manipulation of stored information. Through experimental evaluation, we observe a significant improvement in performance with ChatDB compared to ChatGPT. The integration of symbolic memory in ChatDB substantially enhances the model's ability to handle various queries and reasoning tasks in management settings. This improvement highlights the benefits and effectiveness of leveraging symbolic memory in LLMs.</p>
<h2>References</h2>
<p>Andor, D., He, L., Lee, K., and Pitler, E. (2019). Giving bert a calculator: Finding operations and arguments with reading comprehension. arXiv preprint arXiv:1909.00109.</p>
<p>Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. (2023). Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Bulatov, A., Kuratov, Y., and Burtsev, M. (2022). Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079-11091.</p>
<p>Bulatov, A., Kuratov, Y., and Burtsev, M. S. (2023). Scaling transformer to 1m tokens and beyond with rmt. arXiv preprint arXiv:2304.11062.</p>
<p>Chen, A., Phang, J., Parrish, A., Padmakumar, V., Zhao, C., Bowman, S. R., and Cho, K. (2023). Two failures of self-consistency in the multi-step reasoning of llms. arXiv preprint arXiv:2305.14279.</p>
<p>Chen, W., Ma, X., Wang, X., and Cohen, W. W. (2022). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L., et al. (2022). Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. (2022). Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. (2020). Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR.</p>
<p>Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. (2023). Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992.</p>
<p>Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. (2022). Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.</p>
<p>Johnson, D. D. (2017). Learning graphical state transitions. In International conference on learning representations.
Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022). Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024.</p>
<p>Lester, B., Al-Rfou, R., and Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kttler, H., Lewis, M., Yih, W.-t., Rocktschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>OpenAI (2023). Gpt-4 technical report.
Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., and Ribeiro, M. T. (2023). Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014.</p>
<p>Park, J. S., OBrien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>Pikos, P., Michalewski, H., and Malinowski, M. (2021). Measuring and improving bert's mathematical abilities by predicting the order of reasoning. arXiv preprint arXiv:2106.03921.</p>
<p>Ran, Q., Lin, Y., Li, P., Zhou, J., and Liu, Z. (2019). Numnet: Machine reading comprehension with numerical reasoning. arXiv preprint arXiv:1910.06701.</p>
<p>Schick, T., Dwivedi-Yu, J., Dess, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Schuurmans, D. (2023). Memory augmented large language models are computationally universal. arXiv preprint arXiv:2301.04589.</p>
<p>Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2023). Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.</p>
<p>Sun, R., Arik, S. O., Nakhost, H., Dai, H., Sinha, R., Yin, P., and Pfister, T. (2023). Sql-palm: Improved large language modeladaptation for text-to-sql.</p>
<p>Surs, D., Menon, S., and Vondrick, C. (2023). Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozire, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al. (2023). Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. (2022). Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. (2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022). Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. (2022a). Memorizing transformers. arXiv preprint arXiv:2203.08913.</p>
<p>Wu, Y., Zhao, Y., Hu, B., Minervini, P., Stenetorp, P., and Riedel, S. (2022b). An efficient memory-augmented transformer for knowledge-intensive nlp tasks. arXiv preprint arXiv:2210.16773.</p>
<p>Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. (2022). Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Zhong, Z., Lei, T., and Chen, D. (2022). Training language models with memory augmentation. arXiv preprint arXiv:2205.12674.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/Significant-Gravitas/Auto-GPT
${ }^{4}$ https://chatexcel.com/
${ }^{5}$ https://github.com/Significant-Gravitas/Auto-GPT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
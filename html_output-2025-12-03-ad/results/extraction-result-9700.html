<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9700 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9700</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9700</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-272969418</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.18170v1.pdf" target="_blank">Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models have advanced clinical Natural Language Generation, creating opportunities to manage the volume of medical text. However, the high-stakes nature of medicine requires reliable evaluation, which remains a challenge. In this narrative review, we assess the current evaluation state for clinical summarization tasks and propose future directions to address the resource constraints of expert human evaluation.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9700.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9700.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model used as an evaluator/judge for generated clinical summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses using LLMs (via prompting, in-context learning, PEFT, and human-aware fine-tuning) to automatically evaluate clinical summarization outputs as a scalable complement or alternative to expert human evaluation, highlighting both promise and significant drawbacks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Clinical summarization / medical domain summarization</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Not a single model evaluated in this paper (discussion references general instruction-tuned LLMs and examples such as GPT-4 and other modern LLMs), model/version not specified for any empirical judge role in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Conceptual setups described include hard prompting (zero-shot and few-shot) with an anatomy of an evaluator prompt (Prompt, Information, Evaluation sections), soft prompting / prompt tuning, PEFT (quantization, LoRA), and alignment via RLHF or Direct Preference Optimization (DPO) / HALO; no empirical protocol or dataset for an LLM-as-judge was executed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluations described generically as gold standard: expert clinical annotators using established rubrics (SaferDx, PDQI-9, Revised-IDEA and adapted rubrics for LLM output) with varied scoring formats (binary, Likert, counts, edit-distance, penalty/reward); noted constraints: limited number of trained clinical experts, resource/time intensive, inter-rater variability unless trained.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Not reported numerically in this paper; discussion is qualitative — the paper notes that comparisons between automated metrics and human judgments often use measures like correlation/agreement in the literature but provides no specific agreement values comparing LLM judges to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When replacing or complementing humans with LLM-based judges, the paper identifies potential losses/degradations including: (1) decreased validation and psychometric rigour because models evolve rapidly and are often insufficiently validated; (2) variability and instability of judgments due to sensitivity to prompts, prompt phrasing, and model/version differences; (3) biases (e.g., egocentric bias favoring LLM-like phrasing) and fairness/safety risks including potential for stigmatizing language; (4) risk of reduced trustworthiness if LLM evaluators are not stringently tested and cannot demonstrate construct/criterion/content validity and inter-rater reliability comparable to trained human experts; and (5) residual inability (or unproven ability) to reliably assess nuanced clinical reasoning, prioritization, and contextual judgments unless specifically fine-tuned and validated.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper does not present empirical case studies with quantitative divergences; it provides conceptual/hypothetical examples and literature-based observations: (a) LLM evaluators can produce highly variable judgments across prompt formulations and model versions (prompt-sensitivity and version drift); (b) 'egocentric bias' where LLM judges might preferentially score text that resembles LLM-generated language, thus mis-estimating quality as more LLM content proliferates; (c) automated (pre-LLM) metrics historically overlooked hallucinations and clinical reasoning — the review warns LLM judges may also fail or be unreliable for these aspects without careful alignment and validation; these are described qualitatively rather than reported as measured failures in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The paper emphasizes caveats and potential strengths: (1) LLM-based evaluators could combine the speed and consistency of automated metrics with richer, content-aware judgments (e.g., detecting hallucinations, omissions, factual accuracy) if properly prompted and aligned; (2) early studies (cited in the review) show utility of LLMs as scalable evaluators, suggesting they may complement human experts rather than fully replace them; (3) parameter-efficient fine-tuning and human-aware loss functions (DPO/HALO and variants) are proposed as strategies to improve alignment with human preferences and mitigate some losses, though empirical validation in clinical settings is still needed.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'FUTURE DIRECTIONS: LLMs as Evaluators to Complement Human Expert Evaluators', 'Zero-Shot and In-Context Learning', 'Parameter Efficient Fine-Tuning', 'Parameter Efficient Fine-Tuning with Human-Aware Loss Function', and 'Drawbacks of LLMs as Evaluators' (within the provided paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>Benchmarking Cognitive Biases in Large Language Models as Evaluators <em>(Rating: 2)</em></li>
                <li>Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation <em>(Rating: 2)</em></li>
                <li>Med-Flamingo: a Multimodal Medical Few-shot Learner <em>(Rating: 1)</em></li>
                <li>Med-HALT: Medical Domain Hallucination Test for Large Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9700",
    "paper_id": "paper-272969418",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "Large Language Model used as an evaluator/judge for generated clinical summaries",
            "brief_description": "The paper discusses using LLMs (via prompting, in-context learning, PEFT, and human-aware fine-tuning) to automatically evaluate clinical summarization outputs as a scalable complement or alternative to expert human evaluation, highlighting both promise and significant drawbacks.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Clinical summarization / medical domain summarization",
            "llm_judge_model": "Not a single model evaluated in this paper (discussion references general instruction-tuned LLMs and examples such as GPT-4 and other modern LLMs), model/version not specified for any empirical judge role in this review.",
            "llm_judge_setup": "Conceptual setups described include hard prompting (zero-shot and few-shot) with an anatomy of an evaluator prompt (Prompt, Information, Evaluation sections), soft prompting / prompt tuning, PEFT (quantization, LoRA), and alignment via RLHF or Direct Preference Optimization (DPO) / HALO; no empirical protocol or dataset for an LLM-as-judge was executed in this paper.",
            "human_evaluation_setup": "Human evaluations described generically as gold standard: expert clinical annotators using established rubrics (SaferDx, PDQI-9, Revised-IDEA and adapted rubrics for LLM output) with varied scoring formats (binary, Likert, counts, edit-distance, penalty/reward); noted constraints: limited number of trained clinical experts, resource/time intensive, inter-rater variability unless trained.",
            "agreement_metric": "Not reported numerically in this paper; discussion is qualitative — the paper notes that comparisons between automated metrics and human judgments often use measures like correlation/agreement in the literature but provides no specific agreement values comparing LLM judges to humans.",
            "losses_identified": "When replacing or complementing humans with LLM-based judges, the paper identifies potential losses/degradations including: (1) decreased validation and psychometric rigour because models evolve rapidly and are often insufficiently validated; (2) variability and instability of judgments due to sensitivity to prompts, prompt phrasing, and model/version differences; (3) biases (e.g., egocentric bias favoring LLM-like phrasing) and fairness/safety risks including potential for stigmatizing language; (4) risk of reduced trustworthiness if LLM evaluators are not stringently tested and cannot demonstrate construct/criterion/content validity and inter-rater reliability comparable to trained human experts; and (5) residual inability (or unproven ability) to reliably assess nuanced clinical reasoning, prioritization, and contextual judgments unless specifically fine-tuned and validated.",
            "examples_of_loss": "The paper does not present empirical case studies with quantitative divergences; it provides conceptual/hypothetical examples and literature-based observations: (a) LLM evaluators can produce highly variable judgments across prompt formulations and model versions (prompt-sensitivity and version drift); (b) 'egocentric bias' where LLM judges might preferentially score text that resembles LLM-generated language, thus mis-estimating quality as more LLM content proliferates; (c) automated (pre-LLM) metrics historically overlooked hallucinations and clinical reasoning — the review warns LLM judges may also fail or be unreliable for these aspects without careful alignment and validation; these are described qualitatively rather than reported as measured failures in this paper.",
            "counterexamples_or_caveats": "The paper emphasizes caveats and potential strengths: (1) LLM-based evaluators could combine the speed and consistency of automated metrics with richer, content-aware judgments (e.g., detecting hallucinations, omissions, factual accuracy) if properly prompted and aligned; (2) early studies (cited in the review) show utility of LLMs as scalable evaluators, suggesting they may complement human experts rather than fully replace them; (3) parameter-efficient fine-tuning and human-aware loss functions (DPO/HALO and variants) are proposed as strategies to improve alignment with human preferences and mitigate some losses, though empirical validation in clinical settings is still needed.",
            "paper_reference": "Sections: 'FUTURE DIRECTIONS: LLMs as Evaluators to Complement Human Expert Evaluators', 'Zero-Shot and In-Context Learning', 'Parameter Efficient Fine-Tuning', 'Parameter Efficient Fine-Tuning with Human-Aware Loss Function', and 'Drawbacks of LLMs as Evaluators' (within the provided paper).",
            "uuid": "e9700.0",
            "source_info": {
                "paper_title": "Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
            "rating": 2,
            "sanitized_title": "benchmarking_cognitive_biases_in_large_language_models_as_evaluators"
        },
        {
            "paper_title": "Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation",
            "rating": 2,
            "sanitized_title": "human_evaluation_and_correlation_with_automatic_metrics_in_consultation_note_generation"
        },
        {
            "paper_title": "Med-Flamingo: a Multimodal Medical Few-shot Learner",
            "rating": 1,
            "sanitized_title": "medflamingo_a_multimodal_medical_fewshot_learner"
        },
        {
            "paper_title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
            "rating": 2,
            "sanitized_title": "medhalt_medical_domain_hallucination_test_for_large_language_models"
        }
    ],
    "cost": 0.00955825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review
26 Sep 2024</p>
<p>Emma Croxford 
University of Wisconsin
MadisonUSA</p>
<p>PhDYanjun Gao 
University of Colorado
AuroraUSA</p>
<p>Nicholas Pellegrino 
Epic Systems
VeronaWIUSA</p>
<p>MD MPH MIDSKaren K Wong 
Epic Systems
VeronaWIUSA</p>
<p>PhDGraham Wills 
UW Health
MadisonWIUSA</p>
<p>Elliot First 
Epic Systems
VeronaWIUSA</p>
<p>PhDFrank J Liao 
University of Wisconsin
MadisonUSA</p>
<p>UW Health
MadisonWIUSA</p>
<p>MBACherodeep Goswami 
UW Health
MadisonWIUSA</p>
<p>MD MPHBrian Patterson 
University of Wisconsin
MadisonUSA</p>
<p>UW Health
MadisonWIUSA</p>
<p>MDMajid Afshar 
University of Wisconsin
MadisonUSA</p>
<p>Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review
26 Sep 2024341390646565BA5C5F95EC04711F76C7arXiv:2409.18170v1[cs.CL]
Large Language Models have advanced clinical Natural Language Generation, creating opportunities to manage the volume of medical text.However, the high-stakes nature of medicine requires reliable evaluation, which remains a challenge.In this narrative review, we assess the current evaluation state for clinical summarization tasks and propose future directions to address the resource constraints of expert human evaluation.</p>
<p>Introduction</p>
<p>The rapid development of Large Language Models (LLMs) has led to significant advancements in the field of Natural Language Generation (NLG).In the medical domain, LLMs have shown promise in reducing documentation-based cognitive burden for healthcare providers, particularly in NLG tasks such as summarization and question answering.Summarizing clinical documentation has emerged as a critical NLG task as the volume of medical text in Electronic Health Records (EHRs) continues to expand [1].</p>
<p>Recent advancements, like the introduction of larger context windows in LLMs (e.g., Google's Gemini 1.5 Pro with a 1 million-token capacity [2]), allow for the processing of extensive textual data, making it possible to summarize entire patient histories in a single input.However, a major challenge in applying LLMs to highstakes environments like medicine is ensuring the reliable evaluation of their performance.Unlike traditional approaches, generative AI (GenAI) offers greater flexibility by generating natural language narratives that use language dynamically to fulfill tasks.Yet, this flexibility introduces added complexity in assessing the accuracy, reliability, and quality of the generated output where the desired response is not as static.</p>
<p>The evaluation of clinical summarization by LLMs must address the intricacies of complex medical texts and tackle LLM-specific challenges such as relevancy, hallucinations, omissions, and ensuring factual accuracy [3].Healthcare data can further complicate the LLM-specific challenges because they can contain conflicting or incorrect information.Current metrics, like n-gram overlap and semantic scores, used in summarization tasks are insufficient for the nuanced needs of the medical domain [4].While these metrics may perform adequately for simple extractive summarization, they fall short when applied to abstractive summarization [5], where complex reasoning and in-depth medical knowledge are required.They are also unable to differentiate in the needs of various users and provide evaluations that account for the relevancy of generations.</p>
<p>In the era of GenAI, automation bias further complicates the potential risks posed by LLMs, particularly in clinical settings where the consequences of inaccuracies can be severe.Therefore, efficient and automated evaluation methods are essential.In this review, we examine the current state of LLM evaluation in summarization tasks, highlighting both its applications and limitations in the medical domain.We also propose a future direction to overcome the labor-intensive process of expert human evaluation, which is time-consuming, costly, and requires specialized training.</p>
<p>Human Evaluations in Electronic Health Record Documentation</p>
<p>The current human evaluation frameworks for human-authored clinical notes are largely based on pre-GenAI rubrics that assess clinical documentation quality.These frameworks vary depending on the type of evaluators, content, and the analysis required to generate evaluative scores.Such flexibility allows for tailored evaluation methods, capturing task-specific aspects that ensure quality generation.Expert evaluators, with their field-specific knowledge, play a crucial role in maintaining high standards of assessment.Some commonly used pre-GenAI rubrics include the SaferDx [6], Physician Documentation Quality Instrument (PDQI-9) [7], and Revised-IDEA [8] rubrics.The SaferDx rubric focuses on identifying diagnostic errors and analyzing missed opportunities in EHR documentation through a 12-question retrospective survey aimed at improving diagnostic decision-making and patient safety.The PDQI-9 evaluates physician note quality across nine criteria questions, ensuring continuous improvement in clinical documentation and patient care.The Revised-IDEA tool offers feedback on clinical reasoning documentation through a 4-item assessment.All three of these rubrics place emphasis on the omission of relevant diagnoses throughout the differential diagnosis process and the relevant objective data, processes, and conclusions associated with those diagnoses.They also require clinical documentation to be free of incorrect, inappropriate, or incomplete information emphasizing the importance of the quality of evidence and reasoning that is present in clinical documentation.Each rubric includes additional questions based on the origin and usage of specific clinical documentation -like the PDQI-9's assessment of organization to ensure a reader is able to understand the clinical course of a patient.Each of the three also uses different assessment styles based on the granularity of the questions and intention behind the assessment.For instance, the Revised-IDEA tool uses a count style assessment for 3 of the 4-items to guarantee the inclusion of a minimum number of objective data points and inclusion of required features for a high-quality diagnostic reasoning documentation.In recent publications, the SaferDx tool has been used as a retrospective analysis of the use of GenAI in clinical practice [9], whereas the PDQI-9 and Revised-IDEA tools have been utilized to compare the quality of clinical documentation that is written by clinicians versus GenAI methods [10,11,12].While each of these rubrics was not originally designed to evaluate LLM-generated content, they offer valuable insights into the essential criteria for evaluating text generated in the medical domain.</p>
<p>Human evaluations remain the gold standard for LLM outputs [13].However, because these rubrics were initially developed for evaluating clinician-generated notes, they may need to be adapted for the specific purpose of evaluating LLM-generated output.Several new and modified evaluation rubrics have emerged to address the unique challenges posed by LLM-generated content, including evaluating the consistency and factual accuracy (i.e., hallucinations) of the generated text.Common themes in these adapted rubrics include safety [14], modality [15,16], and correctness [17,18].</p>
<p>Criteria for Human Evaluations</p>
<p>In general, the criteria that are used to make up evaluation rubrics for LLM output fall into seven broad criteria: (1) Hallucination [4,17,18,19,20,21,22], (2) Omission [14,19], (3) Revision [23], (4) Faithfulness/Confidence [15,16,23], (5) Bias/Harm [14,16,22], (6) Groundedness [14,15], and (7) Fluency [15,17,20,23].Hallucination encompasses any evaluative questions that intend to capture when information in a generated text does not follow from the source material.Unsupported claims, nonsensical statements, improbable scenarios, and incorrect or contradictory facts would be flagged by questions in this criteria.Omission-based questions are used to identify missing information in a generated text.Medical facts, important information, and critical diagnostic decisions can all be considered omitted when not included in generated text, if those items would have been included by a medical professional.When an evaluator is asked to make revisions or estimate the number of revisions needed for a generated text, the evaluative question would fall under Revision.Generated texts are revised until they meet the standards set forth by a researcher, hospital system, or larger government body.Faithfulness/Confidence is generally characterized by questions that capture whether a generated text has preserved the content of the source text and presented conclusions that reflect the confidence and specificity present in the source text.Questions about Bias/Harm evaluate whether generated text is introducing potential harm to a patient or reflecting bias in the response.Information that is inaccurate, inapplicable, or poorly applied would be captured by questions that fall under this criteria.Groundedness refers to evaluative questions that grade the quality of the source-based evidence for a generated text.Any evidence that contains poor reading comprehension, recall of knowledge, reasoning steps, or is antithetical to scientific consensus would result in a poor groundedness score.In addition to the content of a generated text, the Fluency of a generated text is also included in evaluations.Coherency, readability, grammatical correctness, and lexical correctness fall under this criteria.In many cases, Fluency is assumed to be adequate in favor of focusing on content-based evaluative criteria.</p>
<p>Analysis of Human Evaluations</p>
<p>The method of analysis for evaluation rubrics can also vary based upon the setting and task.Evaluative scores can be calculated using binary/Likert categorizations [14,15], counts/proportions of pre-specified instances [22], edit distance [23], or penalty/reward schemes similar to those used for medical exams [24].Binary categorizations answer evaluative questions using True/False or Yes/No response schema.This set-up allows complex evaluations to be broken down into simpler and potentially more objective decisions.A binary categorization places more penalization on smaller errors by pushing responses to be either acceptable or unacceptable.Likert-scaled categorizations allow for a higher level of specificity in the score by providing an ordinal scale.These scales can consist of as many levels as necessary, and in many cases there are between 3 and 9 levels including a neutral option for unclear responses.Scales with a higher number of levels introduce more problems with meeting assumptions of a normal distribution into an analysis, along with complexity and disagreement amongst reviewers.Count/proportion-based evaluations require an evaluator to identify pre-specified instances of correct or incorrect key phrases related to a particular evaluative criteria.A precision, recall, f-score, or rate can then be computed from an evaluator's annotations to establish a numerical score for a generated text.Edit distance evaluations also require an evaluator to make annotations on the generated text that is being evaluated.In these cases, an evaluator makes edits to the generated text until it is satisfactory or no longer contains critical errors.These edits can be corrections on factual errors, inclusion of omissions, or removal of irrelevant items.The evaluative score is the distance from the original generated text and the edited version based upon the number of characters, words, etc. that required editing.The Levenshtein distance [25] is an example of an algorithm used to calculate the distance between the generated text and its edited version.This distance is calculated as the minimum number of substitutions, insertions, and deletions of individual characters required to change the original to the edited version.Finally, one of the more complex ways to compute evaluative scores is to use a Penalty/Reward schema.These schema award points for positive outcomes to evaluative questions and penalize negative outcomes.This schema is similar to those seen on national exams which account for positive and negative scores, using the importance and difficulty associated with different questions.For example, the schema used to evaluate LLMs on the Med-HALT dataset is an average of the correct and incorrect answers which are assigned +1 and -0.25 points respectively [24].This evaluation schema provides a high level of specificity for assigning weights representative of the trade-off between false positives and false negatives.</p>
<p>Drawbacks of Human Evaluations</p>
<p>While human evaluations provide nuanced assessments, they are resource-intensive and heavily reliant on the recruitment of evaluators with clinical domain knowledge.The experience and background of an evaluator can significantly influence how they interpret and evaluate generated text.Additionally, the level of guidance and specificity in evaluative instructions determines how much of the assessment is shaped by the evaluators' personal interpretations and beliefs about the task.Although increasing the number of evaluators could mitigate some of these biases, resources-both time and financial-often limit the scale of human evaluations.These evaluations also require substantial manual effort, and without clear guidelines and training, inter-rater agreement may suffer.Ensuring that human evaluators align with the evaluation rubric's intent requires training, much like annotation guidelines for NLP shared tasks [26,27,28].In the clinical domain, medical professionals are typically used as expert evaluators, but their time constraints limit their availability for large-scale evaluations.The difficulty of recruiting more medical professionals, compounded by the time needed for thorough assessments, makes frequent, rapid evaluations impractical.</p>
<p>Another concern is the validity of the evaluation rubric itself.A robust human evaluation framework must possess strong psychometric properties, including construct validity, criterion validity, content validity, and inter-rater reliability, to ensure reproducibility and generalizability.Unfortunately, many frameworks used in clinical evaluations do not provide sufficient details about their creation, making it difficult to assess their validity [15,24].Often, human evaluation frameworks are developed for specific projects with only one evaluator, and while metrics like inter-rater reliability are crucial to establish validity, they are not always reported [18,23].Moreover, clinically relevant evaluation rubrics have not been specifically designed to assess LLM-generated summaries.Most existing evaluation rubrics focus on assessing human-authored note quality, and they do not encompass all the elements required to evaluate the unique aspects of LLM-generated outputs [6,7,8].</p>
<p>Pre-LLM Automated Evaluations</p>
<p>Automated metrics offer a practical solution to the resource constraints of human evaluations, particularly in fields like Natural Language Processing (NLP), where tasks such as question answering, translation, and summarization have long relied on these methods.Automated evaluations employ algorithms, models, or heuristic techniques to assess the quality of generated text without the need for continuous human intervention, making them far more efficient in terms of time and labor.These metrics, however, depend heavily on the availability of high-quality reference texts, often referred to as "gold standards."The generated text is compared against these gold standard reference texts to evaluate its accuracy and how well it meets the task's requirements.Despite their efficiency, automated metrics may struggle to capture the nuance and contextual understanding required in more complex domains, such as clinical diagnosis, where subtle differences in phrasing or reasoning can have significant implications.Therefore, while automated evaluations are valuable for their scalability, their effectiveness is closely tied to the quality and relevance of the reference texts used in the evaluation.</p>
<p>Categories of Automated Evaluation</p>
<p>Automated evaluations in the clinical domain can be categorized into five primary types (Figure 1), each tailored to specific evaluation goals and dependent on the availability of reference and source material for the generated text: (1) Word/Character-based, (2) Embedding-based, (3) Learned metrics, (4) Probability-based, (5) and Pre-Defined Knowledge Base.Word/Character-based evaluations rely on comparisons between a reference text and the generated text to compute an evaluative score.These evaluations can be based on character, word, or sub-sequence overlaps depending on the need of the evaluation and the nuance that may be present in the text.Recall Oriented Understudy for Gisting Evaluation (ROUGE) [29] is a prime example of a word/character-based metric.The many variants of ROUGE -N-gram Co-Occurrence (N), Longest Common Sub-sequence (L), Weighted Longest Common Sub-sequence (W), Skip-Bigram Co-Occurrence (S) -represent the level of comparison between the reference and generated texts.ROUGE-L is the current gold standard for automated evaluation, especially in summarization, and relies on the longest common subsequence between the reference and generated texts.The evaluative score is computed as the fraction of words in the text that are in the longest common subsequence.Edit distance metrics [25] would also fall under this category as they are based on the number of words or characters that would need to be changed to match the reference and generated texts.Edits can be classified as insertions, deletions, substitutions, or transpositions of the words/characters in the generated text.</p>
<p>Embedding-based evaluations create contextualized or static embeddings for the reference and generated texts for comparison rather than relying on exact matches between words or characters.These embedding-based metrics are able to capture semantic similarities between two texts since the embedding for a word or phrase would be based on the text that surrounds it as well as itself.The BERTScore [30] is a commonly used metric that falls under this category.For this metric, a Bidirectional Encoder Representations from Transformers (BERT) model [31] is used to generate the contextualized embeddings before computing a greedy cosine similarity score based on those embeddings.Learned metric-based evaluations rely on training a model to compute the evaluations.These metrics can be trained on example evaluation scores or directly on the reference and generated text pairs.Regression and neural network models are the foundation of these metrics providing varying degrees of complexity for the learnable parameters.The Crosslingual Optimized Metric for Evaluation of Translation (COMET) [51] is a metric that would fall under this category as it is a neural model trained for evaluation.It was originally created for evaluation of machine translations, but has since been applied to other generative tasks.COMET uses a neural network with the generated text as input to produce an evaluative score.This metric can be applied to datasets that are reference-less as well as those with reference texts.</p>
<p>Probability evaluations rely on calculating the likelihood of a generated text based on domain knowledge, reference texts, or source material.These metrics equate high-quality generations with those that have a high probability of being coherent or relevant to the reference or source text.They also penalize the inclusion of off-topic or unrelated information.An example is BARTScore [74], which calculates the sum of log probabilities for the generated output based on the reference text.In this case, the log probabilities are computed using the Bidirectional and Auto-Regressive Transformer (BART) model, which assesses how well the generated text aligns with the expected content [78].</p>
<p>Pre-Defined Knowledge Base metrics rely on established databases of domain-specific knowledge to inform the evaluation of generated text.These metrics are particularly valuable in specialized fields like healthcare, where general language models may lack the necessary depth of knowledge.By incorporating domain-specific knowledge bases, such as the National Library of Medicine's Unified Medical Language System (UMLS) [79], these metrics provide more accurate and contextually relevant evaluations.Predefined knowledge bases can enhance other evaluation methods, such as contextual embedding, machine learning, or probability-based metrics, by grounding them in the specialized terminology and relationships unique to the domain.This combination ensures that evaluations account for both linguistic accuracy and the specialized knowledge required in fields like clinical medicine.BERTScore has a variant that was trained on the UMLS called the SapBERTScore [80].The score functions similarly to the general domain BERTScore but leverages a BERT model fine-tuned using UMLS data to generate more domain-specific embeddings.Other metrics based on the UMLS include the CUI F-Score [50] and UMLS Scorer [73].The UMLS Scorer utilizes UMLS-based knowledge graph embeddings to assess the semantic quality of the text [19], providing a more structured approach to evaluating clinical content.Meanwhile, the CUI F-Score represents text using Concept Unique Identifiers (CUIs) from the UMLS, calculating F-scores that reflect how well the generated text aligns with key medical concepts.This enables a more granular evaluation of the relevance and accuracy of medical terminology within the generated content.</p>
<p>Drawbacks of Automated Metrics</p>
<p>Prior to the advent of LLMs, automated metrics would generate a single score meant to represent the quality of a generated text, regardless of its length or complexity.This single-score approach can make it difficult to pinpoint specific issues in the text, and in the case of LLMs, it is nearly impossible to understand the 1 The taxonomy includes Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [29], Metric for Evaluation of Translation with Explicit Ordering (METEOR) [32], Jensen-Shannon (JS) Divergence [33], Consensus-based Image Description Evaluation (CIDEr) [34], PyrEval [35], Standardized Bilingual Evaluation Understudy (sacreBLEU) [36], Summarization Evaluation by Relevance Analysis (SERA) [37], POURPRE [38], Basic Elements (BE) [39], Bilingual Evaluation Understudy (BLEU) [36], General Text Matcher (GTM) [40], Word Error Rate (WER) [41]/ Translation Edit Rate (TER) [42], Improving Translation Edit Rate (ITER) [43]/CDER (Cover-Disjoint Error Rate) [44], chrF (character n-gram F-score) [45], charac-TER (Character Level Translation Edit Rate) [46], Extended Edit Distance (EED) [47], YiSi [48], Q-metrics [49], Concept Unique Identifier (CUI) F-Score [50], Crosslingual Optimized Metric for Evaluation of Translation (COMET) [51], Bilingual Evaluation Understudy with Representations from Transformers (BLEURT) [52], Combined Regression Model for Evaluating Responsiveness (CREMER) [53], Better Evaluation as Ranking (BEER) [54], BLEND [55], Composite [56], Neural Network Based Evaluation Metric (NNEval) [56], Enhanced Sequential Inference Model (ESIM) [57], Regressor Using Sentence Embeddings (RUSE) [58], Bidirectional Encoder Representations from Transformers for Machine Translation Evaluation (BERT for MTE) [59], ClinicalBLEURT [19], Conditional Bilingual Mutual Information (CBMI) [60], NIST [61], BERTScore [30], MoverScore [62], AUTOmatic SUMMary Evaluation based on N-gram Graphs (AutoSumm ENG) [63], Merge Model Graph (MeMoG) [63], Semantic Propositional Image Caption Evaluation (SPICE) [64], BERTr [65], Word Embedding-based automatic MT evaluation using Word Position Information (WEWPI) [66], Word Mover-Distance (WMD) [67], SIMILE [68], NeUral Based Interchangeability Assessor (NUBIA) [69], SapBERTScore [70], ClinicalBERTScore [71], PubMedBERTScore [72], UMLSScorer [73], MIST [19], Summary-Input Similarity Metrics (SIMetrix) [33], BARTScore [74], Hallucination Risk Measure-ment+ (HARiM+) [75], ClinicalBARTScore [74], MedBARTScore [19], Semantic Normalized Cumulative Gain (SEM-nCG) [76], Intrinsic Knowledge Graph [77] precise factors contributing to a particular score [13].While automated metrics offer the benefit of speed, this comes at the cost of relying on surface-level heuristics, such as lexicographic and structural measures, that fail to capture more abstract summarization challenges in medical text such as needing to apply clinical reasoning and knowledge to appropriately prioritize and synthesize medical information.</p>
<p>FUTURE DIRECTIONS: LLMs as Evaluators to Complement</p>
<p>Human Expert Evaluators: Prompt Engineering LLMs as Judges LLMs are versatile tools capable of performing a wide range of tasks, including evaluating the outputs of other LLMs.This concept, where an LLM acts as a model of a human expert evaluator, has gained traction with the advent of instruction tuning and reinforcement learning with human feedback (RLHF) [81].These advancements have significantly improved the ability of LLMs to align their outputs with human preferences, as seen in the transition from GPT-3 to GPT-4, which marked a paradigm shift in LLM accuracy and performance [82].</p>
<p>An effective LLM evaluator would be able to respond to evaluative questions with precision and accuracy comparable to that of human experts, following frameworks like those used in human evaluation rubrics.LLM-based evaluations could provide many of the same advantages as traditional automated metrics, such as speed and consistency, while potentially overcoming the reliance on high-quality reference texts.Moreover, LLMs could evaluate complex tasks by directly engaging with the content, bypassing the need for simplistic heuristics and offering more information into factual accuracy, hallucinations, and omissions.</p>
<p>Although the use of LLMs as evaluators is still emerging in research, early studies have demonstrated their utility as an alternative to human evaluations, offering a scalable solution to the limitations of manual assessment [83].As the methodology continues to develop, LLM-based evaluations hold promise for addressing the shortcomings of both traditional automated metrics and human evaluations, particularly in complex, context-rich domains such as clinical text generation.</p>
<p>Zero-Shot and In-Context Learning</p>
<p>One method for designing LLMs to perform evaluations is through the use of manually curated prompts (Figure 3).A prompt consists of the task description and instructions provided to an LLM to guide its responses.Two primary prompting strategies are employed in this context: Zero-Shot and Few-Shot [3].In Zero-Shot prompting, the LLM is given only the task description without any examples before being asked to perform evaluations.Few-Shot prompting provides the task description alongside a few examples to help guide the LLM in generating output.The number of examples varies based on the LLM's architecture, input window limitations, and the point at which the model performs optimally.Typically, between one and five few-shot examples are used.Prompt engineering, through both Zero-Shot and Few-Shot ("in-context learning") approaches (collectively referred to as "hard prompting"), enables an LLM to perform tasks that it was not explicitly trained to do.However, performance can vary significantly depending on the model's pre-training and its relevance to the new task.</p>
<p>Figure 3: Anatomy of an Evaluator Prompt An evaluator prompt consists of three sections: Prompt, Information, and Evaluation.All three components are essential for an LLM serving as an evaluator.The Evaluator Prompt needs to instruct the LLM on the task (Prompt), provide the LLM will all the necessary information to make an evaluation (Information), and all the information that defines the guidelines and formatting of the evaluation (Evaluation).</p>
<p>Beyond these manual approaches, a more adaptive strategy involves "soft prompting," also known as machine-learned prompts, which includes techniques like prompt tuning and p-tuning [84].Soft prompts are learnable parameters added as virtual tokens to a model's input to signal task-specific instructions.Unlike hard prompts, soft prompts are trained and incorporated into the model's input layer, enabling the model to handle a broader range of specialized tasks.Soft prompting has been shown to outperform Few-Shot prompting, especially in large-scale models, as it fine-tunes the model's behavior without altering the core weights.When prompting alone does not achieve the desired performance, fine-tuning the entire LLM may be necessary for optimal task execution.</p>
<p>Parameter Efficient Fine-Tuning</p>
<p>Even though an LLM may be pre-trained on a vast corpus, it can struggle with tasks requiring domainspecific knowledge or handling nuanced inputs.To address these challenges, Supervised fine-tuning (SFT) methods with Parameter Efficient Fine-Tuning (PEFT) using quantization and low rank adaptors can be employed, where the model is trained on a specialized dataset of prompt/response pairs tailored to the task at hand.Fine-tuning every weight in a LLM can require a large amount of time and computational resources.In these instances, quantization and low rank adaptors are added to the fine-tuning process for PEFT.Quantization reduces the time and memory costs of training by using lower precision data types, generally 4-bit and 8-bit, for the LLMs weights [85].Low rank adaptors (LoRA) freeze the weights of a LLM and decompose them into a smaller number of trainable parameters ultimately also reducing the costs of SFT [86].PEFT helps refine an LLM by embedding task-specific knowledge, ensuring the model can respond accurately in specialized contexts.The creation of these datasets is critical-performance improvements are directly tied to the quality and relevance of the prompt/response pairs used for fine-tuning.The goal is to adjust the LLM to perform better in specific use cases, such as medical diagnosis or legal reasoning, by narrowing its focus to task-specific behaviors through PEFT.</p>
<p>Parameter Efficient Fine-Tuning with Human-Aware Loss Function</p>
<p>In certain applications, the focus of fine-tuning is to align the LLM with human values and preferences, especially when the model risks generating biased, incorrect, or harmful content.This alignment, known as Human Alignment training, is driven by high-quality human feedback integrated into the training process.A widely recognized approach in this domain is Reinforcement Learning with Human Feedback (RLHF) [87].RLHF is applied to update the LLM, guiding it toward outputs that score higher on the reward scale.In the reward model stage, a dataset annotated with human feedback is used to establish the reward, typically scalar in nature, of a particular response.The LLM is then trained to produce responses that will receive higher rewards through a process known as Proximal Policy Optimization (PPO) [88].This iterative process ensures the model aligns with human expectations, but it can be resource-intensive, requiring significant memory, time, and computational power.</p>
<p>To address these computational challenges, newer paradigms have emerged that streamline Human Alignment training by directly optimizing the LLM-based on human preferences, without the need for a reward model with Direct Preference Optimization (DPO) [89].DPO reformulates the alignment process into a human-aware loss function (HALO), optimized on a dataset of human preferences where prompts are paired with preferred and dis-preferred responses (Figure 4).This method is particularly promising for aligning LLMs with human preferences and can be applied to ordinal responses, such as the Likert scales commonly seen in human evaluation rubrics.While PPO improves LLM performance by aligning outputs with human preferences, it is often sample-inefficient and can suffer from reward hacking [90].DPO, in contrast, directly optimizes model outputs based on human preferences without needing an explicit reward model, making it more sample-efficient and better aligned with human values.DPO simplifies the training process by focusing directly on the desired outcomes, leading to more stable and interpretable alignment.While these methods have been successfully applied in other domains [91,92,93], their use in the medical field is under-explored.Training data from the human evaluation rubric on a much smaller scale to overcome labor constraints can be incorporated into a loss function designed for human alignment using DPO.</p>
<p>In the last year, many variants of DPO have emerged for alignment training methods that can prevent over-fitting and circumvent DPO's modeling assumptions with modifications to the underlying model and loss function (Figure 5).Alternative methods such as Joint Preference Optimization (JPO) [94] and Simple Preference Optimization (SimPO) [95] were derived from DPO.These methods introduce regularization terms and modifications to the loss function to prevent premature convergence and ensure more robust alignment over a broader range of inputs.Other alternative methods such as Kahneman-Tversky Optimization (KTO) [96] and Pluralistic Alignment Framework (PAL) [97] use alternatives to the Bradley-Terry preferences model that underlies DPO.The alternative modeling assumptions used in these methods can prevent the breakdown of DPO's alignment in situations without direct preference data and heterogeneous human preferences.</p>
<p>Drawbacks of LLMs as Evaluators</p>
<p>LLMs hold promise for automating evaluation, but as with other automated evaluation methods, there are significant challenges to consider.One major issue is the rapid pace at which LLMs and their associated training strategies have evolved.This rapid development often outpaces the ability to thoroughly validate LLM-based evaluators before they are used in practice.In some cases, new optimization techniques are introduced before their predecessors have undergone peer review, and these advancements may lack sufficient mathematical justification.The speed of LLM evolution can make it difficult to allocate time and resources for proper validation, which can compromise their reliability.</p>
<p>Moreover, despite their advancements, LLMs remain sensitive to the prompts and inputs they receive.As LLMs continue to update and change their internal knowledge representations and as their prompts also change, the output can be highly variable.The exact LLM, or model version, that is used can also add another layer of variability.The same prompts and inputs can produce different results based on the LLM's internal structure and pre-training schema.LLMs have also been noted for egocentric bias which could affect evaluations as more and more LLM generated text appears in source texts [112].As a result, the use of LLMs as evaluators must be accompanied by stringent testing and safety checks to mitigate risks.Ensuring fairness in their responses is also critical, particularly in sensitive domains like healthcare, where biased or stigmatizing language could have serious consequences.These challenges highlight the need for continuous evaluation, testing, and refinement to make LLM-based evaluators both reliable and safe for medical evaluations.</p>
<p>Rejection Sampling [98], IPO: Identity Preference Optimization [99], cDPO: Conservative DPO [100], KTO: Kahneman Tversky Optimization [96], JPO: Joint Preference Optimization [94], ORPO: Odds Ratio Preference Optimization [101], rDPO: Robust DPO [102], BCO: Binary Classifier Optimization [103], DNO: Direct Nash Optimization [97], TR-DPO: Trust Region DPO [104], CPO: Contrastive Preference Optimization [105], SPPO: Self-Play Preference Optimization [106], PAL: Pluralistic Alignment Framework [97], EXO: Efficient Exact Optimization [107], AOT: Alignment via Optimal Transport [108], RPO: Iterative Reasoning Preference Optimization [109], NCA: Noise Contrastive Alignment [110], RTO: Reinforced Token Optimization [111], SimPO: Simple Preference Optimization [95]  The development of reliable evaluation strategies is becoming increasingly important as the pace of innovation in GenAI outstrips the speed at which these technologies are validated.In healthcare, the focus on clinical safety must also contend with the time constraints placed on healthcare professionals.While human evaluation rubrics offer a high degree of reliability and accuracy, they are significantly limited by the time commitment required from medical professionals serving as evaluators.Ironically, the technologies being evaluated often aim to reduce the cognitive load on these same professionals, yet they demand further time investment for their performance evaluation.</p>
<p>Automated evaluations, if properly designed for the clinical domain, present a promising alternative to human evaluations.However, traditional non-LLM automated evaluations have thus far fallen short, failing to consistently match the rigor of human evaluation rubrics [5,13].These metrics frequently overlook hallucinations, fail to assess reasoning quality, and struggle to determine the relevance of generated texts.</p>
<p>As LLMs are introduced as potential alternatives for human evaluators, it is critical to consider the unique requirements of the clinical domain.A well-designed LLM evaluator-an "LLM-as-a-judge"-could potentially combine the high reliability of human evaluations with the efficiency of automated methods, while avoiding the pitfalls that have limited existing automated metrics.If executed effectively, such LLM-based evaluations could offer the best of both worlds, ensuring clinical safety without sacrificing the quality of assessments.</p>
<p>Figure 1 :
1
Figure 1: Pre-LLM Automated Evaluation Metric Taxonomy A structured organization of pre-LLM automated evaluation metrics categorized by their bases and the need for ground truth references.Those metrics that were built for or have been applied in the clinical domain are in bold. 1</p>
<p>Figure 2 :
2
Figure 2: Stages of Prompt Engineering LLMs as Judges The three different aspects of prompt engineering expanded upon in section 5.The three sections -Zero-Shot and In-Context Learning (ICL), Parameter Efficient Fine Tuning (PEFT), and PEFT with Human Aware Loss Function (HALO) -fit together into a larger schema for training and prompting an LLM to serve as an evaluator to complement human expert evaluators.</p>
<p>Figure 4 :
4
Figure 4: Alignment Workflow: PPO v. DPO An overview of the processes for aligning an LLM through Reinforcement Learning Human Feedback (RLHF) with Proximal Policy Optimization (PPO) and Direct Policy Optimization (DPO).</p>
<p>Figure 5 : 2 6
52
Figure 5: Human Aware Loss Functions (HALOs) from PPO to Present The development timeline for HALOs from the advent of Proximal Policy Optimization (PPO) in 2017 through 2024.HALOs are classified on an algorithmic basis and on their data requirements.2</p>
<p>The figure includes PPO: Proximal Policy Optimization[88], DPO: Direct Preference Optimization[89], RSO: Statistical
AcknowledgementsWe thank Anne Glorioso, Leslie Christensen, and Paije Wilson for their assistance with database selection and search query assistance as UW-Madison subject librarians.EC was supported by an NLM training grant to the Computation and Informatics in Biology and Medicine Training Program (NLM 5T15LM007359).YG was supported by NIH/NLM R00 LM014308-02 and MA was supported by R01LM012973.Author ContributionsEC, YG, NP, EF, BP, and MA conceptualized the review.EC completed the literature search and prepared the first draft of the manuscript.EC, YG, NP, EF, KW, BP, and MA contributed to the revision of the manuscript.All authors read through and approved the final manuscript.Competing InterestsThe authors declare no competing interests.
Call me Dr Ishmael: trends in electronic health record notes available at emergency department visits and admissions. B W Patterson, D J Hekman, F J Liao, A G Hamedani, M N Shah, M Afshar, JAMIA Open. 72e0392024 Apr</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. G Team, P Georgiev, V I Lei, R Burnell, L Bai, A Gulati, ArXiv:2403.055302024 Aug</p>
<p>A Survey of Large Language Models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, ArXiv:2303.182232023 Jun</p>
<p>Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation. F Moramarco, Papadopoulos Korfiatis, A Perera, M Juric, D Flann, J Reiter, E , Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. S Muresan, P Nakov, A Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Development of a Human Evaluation Framework and Correlation with Automated Metrics for Natural Language Generation of Medical Diagnoses. E Croxford, Y Gao, B Patterson, D To, S Tesch, D Dligach, 10.1101/2024.03.20.24304620v22024 Apr:2024.03.20.24304620</p>
<p>Recommendations for using the Revised Safer Dx Instrument to help measure and improve diagnostic safety. H Singh, A Khanna, C Spitzmueller, And Meyer, Diagnosis. 62019 Nov</p>
<p>Assessing Electronic Note Quality Using the Physician Documentation Quality Instrument (PDQI-9). P D Stetson, S Bakken, J O Wrenn, E L Siegler, Applied Clinical Informatics. 322012 Apr</p>
<p>Development of a Clinical Reasoning Documentation Assessment Tool for Resident and Fellow Admission Notes: a Shared Mental Model for Feedback. V Schaye, L Miller, D Kudlowitz, J Chun, J Burk-Rafel, P Cocks, Journal of General Internal Medicine. 3732022 Feb</p>
<p>Incidence of Diagnostic Errors Among Unexpectedly Hospitalized Patients Using an Automated Medical History-Taking System With a Differential Diagnosis Generator: Retrospective Observational Study. R Kawamura, Y Harada, S Sugimoto, Y Nagase, S Katsukura, T Shimizu, Company: JMIR Medical Informatics Distributor: JMIR Medical Informatics Institution: JMIR Medical Informatics Label: JMIR Medical Informatics publisher. Toronto, CanadaJMIR Publications Inc2022 Jan10e35225</p>
<p>Ambient Artificial Intelligence Scribes to Alleviate the Burden of Clinical Documentation. NEJM Catalyst. A A Tierney, G Gayre, B Hoberman, B Mattern, M Ballesca, P Kipnis, CAT.23.04042024 Feb5</p>
<p>Comparison of clinical note quality between an automated digital intake tool and the standard note in the emergency department. R Eshel, F Bellolio, A Boggust, N I Shapiro, A F Mullan, H A Heaton, The American Journal of Emergency Medicine. 632023</p>
<p>Clinical Reasoning of a Generative Artificial Intelligence Model Compared With Physicians. S Cabral, D Restrepo, Z Kanjee, P Wilson, B Crowe, R E Abdulnour, JAMA Internal Medicine. 18452024 May</p>
<p>A Survey of Evaluation Metrics Used for NLG Systems. A B Sai, A K Mohankumar, M M Khapra, ACM Computing Surveys. 5522023</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, Nature. 2023 Jul</p>
<p>The patient is more dead than alive: exploring the current state of the multi-document summarisation of the biomedical literature. Y Otmakhova, K Verspoor, T Baldwin, J H Lau, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>. Ireland Dublin, 2022Association for Computational Linguistics</p>
<p>A Meta-Evaluation of Faithfulness Metrics for Long-Form Hospital-Course Summarization. G Adams, J Zucker, N Elhadad, ArXiv:2303.039482023 Mar</p>
<p>Automated Lay Language Summarization of Biomedical Scientific Reviews. Y Guo, W Qiu, Y Wang, T Cohen, ArXiv:2012.125732022 Jan</p>
<p>Generating (Factual?) Narrative Summaries of RCTs: Experiments with Neural Multi-Document Summarization. B C Wallace, S Saha, F Soboczenski, I J Marshall, 2020</p>
<p>An Investigation of Evaluation Metrics for Automated Medical Note Generation. A B Abacha, Yim Ww, G Michalopoulos, T Lin, ArXiv:2305.173642023 May</p>
<p>Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards. S Yadav, D Gupta, A B Abacha, D Demner-Fushman, ArXiv:2107.001762021 Jun</p>
<p>Med-Flamingo: a Multimodal Medical Few-shot Learner. M Moor, Q Huang, S Wu, M Yasunaga, C Zakka, Y Dalmia, ArXiv:2307.151892023 Jul</p>
<p>Multimodal Generation of Radiology Reports using Knowledge-Grounded Extraction of Entities and Relations. Dalla Serra, F Clackett, W Mackinnon, H Wang, C Deligianni, F Dalton, J , Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Long Papers. the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics20221</p>
<p>Generation of Patient After-Visit Summaries to Support Physicians. P Cai, F Liu, A Bajracharya, J Sills, A Kapoor, W Liu, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaInternational Committee on Computational Linguistics2022</p>
<p>Med-HALT: Medical Domain Hallucination Test for Large Language Models. L K Umapathi, A Pal, M Sankarasubbu, ArXiv:2307.153432023 Jul</p>
<p>Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady. V I Levenshtein, 1966 Feb10707</p>
<p>Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding. Y Gao, D Dligach, T Miller, S Tesch, R Laffin, M M Churpek, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2022</p>
<p>Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of Biomedical Research Articles. T Goldsack, C Scarton, M Shardlow, C ; Lin, D Demner-Fushman, S Ananiadou, M Miwa, K Roberts, J Tsujii, Proceedings of the 23rd Workshop on Biomedical Natural Language Processing. the 23rd Workshop on Biomedical Natural Language ProcessingBangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Overview of the MedVidQA 2022 Shared Task on Medical Video Question-Answering. D Gupta, D Demner-Fushman, D Demner-Fushman, K B Cohen, S Ananiadou, J Tsujii, Proceedings of the 21st Workshop on Biomedical Language Processing. the 21st Workshop on Biomedical Language ProcessingDublin, IrelandAssociation for Computational Linguistics2022</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. C Y Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Evaluating Text Generation with BERT. T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, Bertscore, ArXiv:1904.096752020 Feb</p>
<p>Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M W Chang, K Lee, K Toutanova, Bert, ArXiv:1810.048052019 May</p>
<p>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. S Banerjee, A Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. J Goldstein, A Lavie, C Y Lin, C Voss, the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, MichiganAssociation for Computational Linguistics2005</p>
<p>Automatically Assessing Machine Summary Content Without a Gold Standard. A Louis, A Nenkova, Computational Linguistics. 3922013 Jun</p>
<p>CIDEr: Consensus-based image description evaluation. R Vedantam, C L Zitnick, D Parikh, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Boston, MA, USAIEEE2015</p>
<p>Automated Pyramid Summarization Evaluation. Y Gao, C Sun, R J Passonneau, 2019</p>
<p>BLEU: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W J Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. ACL '02. the 40th Annual Meeting on Association for Computational Linguistics. ACL '02USAAssociation for Computational Linguistics2002</p>
<p>Revisiting Summarization Evaluation for Scientific Articles. A Cohan, N Goharian, 2016</p>
<p>Automatically Evaluating Answers to Definition Questions. J Lin, D Demner-Fushman, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. R Mooney, C Brew, L F Chien, K Kirchhoff, Human Language Technology Conference and Conference on Empirical Methods in Natural Language ProcessingVancouver, British Columbia, CanadaAssociation for Computational Linguistics2005</p>
<p>Automated Summarization Evaluation with Basic Elements. E Hovy, C Y Lin, L Zhou, J Fukumoto, N Calzolari, K Choukri, A Gangemi, B Maegaard, J Mariani, J Odijk, Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06). the Fifth International Conference on Language Resources and Evaluation (LREC'06)Genoa, ItalyEuropean Language Resources Association (ELRA2006</p>
<p>Evaluation of machine translation and its evaluation. J P Turian, L Shen, I D Melamed, Proceedings of Machine Translation Summit IX: Papers. Machine Translation Summit IX: PapersNew Orleans, USA2003</p>
<p>A New Quantitative Quality Measure for Machine Translation Systems. K Y Su, M W Wu, J S Chang, The 14th International Conference on Computational Linguistics; 1992. 19922</p>
<p>A Study of Translation Edit Rate with Targeted Human Annotation. M Snover, B Dorr, R Schwartz, L Micciulla, J Makhoul, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers. the 7th Conference of the Association for Machine Translation in the Americas: Technical PapersCambridge, Massachusetts, USA2006Association for Machine Translation in the Americas</p>
<p>ITER: Improving Translation Edit Rate through Optimizable Edit Costs. J Panja, S K Naskar, O Bojar, R Chatterjee, C Federmann, M Fishel, Y Graham, B Haddow, Proceedings of the Third Conference on Machine Translation: Shared Task Papers. the Third Conference on Machine Translation: Shared Task PapersBelgium, BrusselsAssociation for Computational Linguistics2018</p>
<p>CDER: Efficient MT Evaluation Using Block Movements. G Leusch, N Ueffing, H Ney, 11th Conference of the European Chapter. D Mccarthy, S Wintner, Trento, ItalyAssociation for Computational Linguistics2006</p>
<p>chrF: character n-gram F-score for automatic MT evaluation. M Popović, O Bojar, R Chatterjee, C Federmann, B Haddow, C Hokamp, M Huck, Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>W Wang, J T Peter, H Rosendahl, H Ney, Character, O Bojar, C Buck, R Chatterjee, C Federmann, L Guillou, B Haddow, Proceedings of the First Conference on Machine Translation. the First Conference on Machine TranslationBerlin, GermanyAssociation for Computational Linguistics20162Shared Task Papers</p>
<p>Extended Edit Distance Measure for Machine Translation. P Stanchev, Wang W Ney, H Eed ; Bojar, O Chatterjee, R Federmann, C Fishel, M Graham, Y Haddow, B , Proceedings of the Fourth Conference on Machine Translation. the Fourth Conference on Machine TranslationFlorence, ItalyAssociation for Computational Linguistics20192Shared Task Papers, Day 1)</p>
<p>YiSi -a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources. Lo Ck, ; Bojar, O Chatterjee, R Federmann, C Fishel, M Graham, Y Haddow, B , Proceedings of the Fourth Conference on Machine Translation. the Fourth Conference on Machine TranslationFlorence, ItalyAssociation for Computational Linguistics20192Shared Task Papers, Day 1)</p>
<p>Towards a Better Metric for Evaluating Question Generation Systems. P Nema, M M Khapra, E Riloff, D Chiang, J Hockenmaier, J Tsujii, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Summarizing Patients Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models. Y Gao, D Dligach, T Miller, D Xu, M M Churpek, M Afshar, ArXiv:2208.084082022 Sep</p>
<p>COMET: A Neural Framework for MT Evaluation. R Rei, C Stewart, A C Farinha, A Lavie, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2020Online</p>
<p>BLEURT: Learning Robust Metrics for Text Generation. T Sellam, D Das, A P Parikh, ArXiv:2004.046962020 May</p>
<p>Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation. Z Lin, C Liu, H T Ng, My ; Kan, H Li, C Y Lin, M Osborne, G G Lee, J C Park, Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 50th Annual Meeting of the Association for Computational LinguisticsJeju Island, KoreaAssociation for Computational Linguistics20121</p>
<p>Fitting Sentence Level Translation Evaluation with Many Dense Features. M Stanojević, Sima ' , Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). A Moschitti, B Pang, W Daelemans, the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational Linguistics2014</p>
<p>Blend: a Novel Combined MT Metric Based on Direct Assessment -CASICT-DCU submission to WMT17 Metrics Task. Q Ma, Y Graham, S Wang, Q Liu, O Bojar, C Buck, R Chatterjee, C Federmann, Y Graham, B Haddow, Proceedings of the Second Conference on Machine Translation. the Second Conference on Machine TranslationCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Learning-based Composite Metrics for Improved Caption Evaluation. N Sharif, L White, M Bennamoun, Ali Shah, Sa ; Shwartz, V Tabassum, J Voigt, R , Che W De Marneffe, M C Nissim, M , Proceedings of ACL 2018, Student Research Workshop. ACL 2018, Student Research WorkshopMelbourne, AustraliaAssociation for Computational Linguistics2018</p>
<p>Enhanced LSTM for Natural Language Inference. Q Chen, X Zhu, Z H Ling, S Wei, H Jiang, D Inkpen, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>RUSE: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation. H Shimanaka, T Kajiwara, M Komachi, O Bojar, R Chatterjee, C Federmann, M Fishel, Y Graham, B Haddow, Proceedings of the Third Conference on Machine Translation: Shared Task Papers. the Third Conference on Machine Translation: Shared Task PapersBelgium, BrusselsAssociation for Computational Linguistics2018</p>
<p>Machine Translation Evaluation with BERT Regressor. H Shimanaka, T Kajiwara, M Komachi, ArXiv:1907.126792019 Jul</p>
<p>Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation. S Zhang, Y Liu, F Meng, Y Chen, J Xu, J Liu, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. S Muresan, P Nakov, A Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. G Doddington, Proceedings of the second international conference on Human Language Technology Research. the second international conference on Human Language Technology ResearchSan Diego, CaliforniaAssociation for Computational Linguistics2002138</p>
<p>Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. W Zhao, M Peyrard, F Liu, Y Gao, C M Meyer, S Eger, Moverscore, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. K Inui, J Jiang, V Ng, Wan X , the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>AutoSummENG and MeMoG in Evaluating Guided Summaries. G Giannakopoulos, V Karkaletsis, 2011</p>
<p>Semantic Propositional Image Caption Evaluation. P Anderson, B Fernando, M Johnson, S Gould, Spice, Computer Vision -ECCV 2016. B Leibe, J Matas, N Sebe, M Welling, ChamSpringer International Publishing2016</p>
<p>Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation. N Mathur, T Baldwin, T Cohn, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Word Embedding-Based Automatic MT Evaluation Metric using Word Position Information. H Echizen'ya, K Araki, E Hovy, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>From Word Embeddings To Document Distances. M Kusner, Y Sun, N Kolkin, K Weinberger, Proceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine Learning2015</p>
<p>Beyond BLEU: Training Neural Machine Translation with Semantic Similarity. J Wieting, T Berg-Kirkpatrick, K Gimpel, G Neubig, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>NUBIA: NeUral Based Interchangeability Assessor for Text Generation. H Kane, M Y Kocyigit, A Abdalla, P Ajanoh, M Coulibali, ArXiv:2004.146672020 May</p>
<p>Self-Alignment Pretraining for Biomedical Entity Representations. F Liu, E Shareghi, Z Meng, M Basaldella, N Collier, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>. E Alsentzer, J R Murphy, W Boag, W Weng, Jin D Naumann, T , abs/1904.03323Publicly Available Clinical BERT Embeddings. CoRR. 2019</p>
<p>Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, 2020</p>
<p>. J B Delbrouck, Scorer, 2023</p>
<p>Evaluating Generated Text as Text Generation. W Yuan, G Neubig, P Liu, Bartscore, ArXiv:2106.115202021 Oct</p>
<p>S Son, J Park, Hwang Ji, J Lee, H Noh, Lee Y Harim+, Evaluating Summary Quality with Hallucination Risk. 2022</p>
<p>Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE?. M Akter, N Bansal, S K Karmaker, Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>A Knowledge-Graph-Based Intrinsic Test for Benchmarking Medical Concept Embeddings and Pretrained Language Models. C Aracena, F Villena, M Rojas, J Dunstan, 2022</p>
<p>BART: Denoising Sequenceto-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, Mohamed A Levy, O , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>The Unified Medical Language System. Da Ma Lindberg, B L Humphreys, Yearb Med Inform. 141993</p>
<p>Self-Alignment Pretraining for Biomedical Entity Representations. F Liu, E Shareghi, Z Meng, M Basaldella, N Collier, K Toutanova, A Rumshisky, L Zettlemoyer, D Hakkani-Tur, I Beltagy, S Bethard, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies. Online: Association for Computational Linguistics2021</p>
<p>Deep reinforcement learning from human preferences. P Christiano, J Leike, T B Brown, M Martic, S Legg, D Amodei, 2017</p>
<p>GPT-4 Technical Report. Achiam J Openai, S Adler, S Agarwal, L Ahmad, I Akkaya, ArXiv:2303.087742024 Mar</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. L Zheng, W L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, ArXiv:2306.056852023 Dec</p>
<p>The Power of Scale for Parameter-Efficient Prompt Tuning. B Lester, R Al-Rfou, N Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. M F Moens, X Huang, L Specia, Yih Swt, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>QLoRA: Efficient Finetuning of Quantized LLMs. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, ArXiv:2305.143142023 May</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, ArXiv:2106.096852021 Oct</p>
<p>Fine-Tuning Language Models from Human Preferences. D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, 2019</p>
<p>Proximal Policy Optimization Algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, ArXiv:1707.063472017 Aug</p>
<p>Direct Preference Optimization: Your Language Model is Secretly a Reward Model. R Rafailov, A Sharma, E Mitchell, S Ermon, C D Manning, C Finn, ArXiv:2305.182902023 May</p>
<p>Language Models Learn to Mislead Humans via RLHF. J Wen, R Zhong, A Khan, E Perez, J Steinhardt, M Huang, ArXiv:2409.128222024 Sep</p>
<p>Research on Large Language Model for Coal Mine Equipment Maintenance Based on Multi-Source Text. X Cao, W Xu, J Zhao, Y Duan, X Yang, APPLIED SCIENCES-BASEL. 1472024 Apr</p>
<p>Reinforcement Learning Based Optimal Energy Management of A Microgrid. S Iqbal, K Mehran, Ieee , 2022</p>
<p>Improving Contextual Query Rewrite for Conversational AI Agents through User-preference Feedback Learning. Z Sun, Y Zhou, J Hao, Fan X Lu, Y Ma, C , Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. M Wang, I Zitouni, the 2023 Conference on Empirical Methods in Natural Language Processing: Industry TrackSingaporeAssociation for Computational Linguistics2023</p>
<p>Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization. H Bansal, A Suvarna, G Bhatt, N Peng, K W Chang, A Grover, ArXiv:2404.005302024 Mar</p>
<p>Simple Preference Optimization with a Reference-Free Reward. Y Meng, M Xia, Chen D Simpo, ArXiv:2405.147342024 May</p>
<p>KTO: Model Alignment as Prospect Theoretic Optimization. K Ethayarajh, W Xu, N Muennighoff, D Jurafsky, D Kiela, ArXiv:2402.013062024 Jun</p>
<p>Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences. C Rosset, C A Cheng, Mitra A Santacroce, M Awadallah, A Xie, T , ArXiv:2404.037152024 Apr</p>
<p>Statistical Rejection Sampling Improves Preference Optimization. T Liu, Y Zhao, R Joshi, M Khalman, M Saleh, P J Liu, ArXiv:2309.066572024 Jan</p>
<p>A General Theoretical Paradigm to Understand Learning from Human Preferences. M G Azar, M Rowland, B Piot, D Guo, D Calandriello, M Valko, ArXiv:2310.120362023 Nov</p>
<p>A note on DPO with noisy preferences and relationship to IPO. E Mitchell, V1.12023</p>
<p>ORPO: Monolithic Preference Optimization without Reference Model. J Hong, N Lee, J Thorne, ArXiv:2403.076912024 Mar</p>
<p>Provably Robust DPO: Aligning Language Models with Noisy Feedback. S R Chowdhury, A Kini, N Natarajan, ArXiv:2403.004092024 Apr</p>
<p>Binary Classifier Optimization for Large Language Model Alignment. S Jung, G Han, D W Nam, K W On, ArXiv:2404.046562024 Apr</p>
<p>Learn Your Reference Model for Real Good Alignment. A Gorbatovski, B Shaposhnikov, A Malakhov, N Surnachev, Y Aksenov, I Maksimov, ArXiv:2404.096562024 May</p>
<p>Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. H Xu, A Sharaf, Y Chen, W Tan, L Shen, B Van Durme, ArXiv:2401.084172024 Jun</p>
<p>Self-Play Preference Optimization for Language Model Alignment. Y Wu, Z Sun, H Yuan, Ji K Yang, Y Gu, Q , ArXiv:2405.006752024 Juncs, stat</p>
<p>Towards Efficient Exact Optimization of Language Model Alignment. H Ji, C Lu, Y Niu, P Ke, H Wang, J Zhu, ArXiv:2402.008562024 Jun</p>
<p>Distributional Preference Alignment of LLMs via Optimal Transport. I Melnyk, Y Mroueh, B Belgodere, M Rigotti, A Nitsure, M Yurochkin, ArXiv:2406.058822024 Jun</p>
<p>Iterative Reasoning Preference Optimization. R Y Pang, W Yuan, K Cho, H He, S Sukhbaatar, J Weston, ArXiv:2404.197332024 Jun</p>
<p>Noise Contrastive Alignment of Language Models with Explicit Rewards. H Chen, G He, L Yuan, G Cui, H Su, J Zhu, ArXiv:2402.053692024 Jul</p>
<p>DPO Meets PPO: Reinforced Token Optimization for RLHF. H Zhong, G Feng, W Xiong, X Cheng, L Zhao, D He, ArXiv:2404.189222024 Julcs, stat</p>
<p>Benchmarking Cognitive Biases in Large Language Models as Evaluators. R Koo, M Lee, V Raheja, J I Park, Z M Kim, D Kang, ArXiv:2309.170122024 Aug</p>            </div>
        </div>

    </div>
</body>
</html>
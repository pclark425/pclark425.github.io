<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5185 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5185</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5185</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269614031</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.03845v1.pdf" target="_blank">Self-Improving Customer Review Response Generation Based on LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Previous studies have demonstrated that proactive interaction with user reviews has a positive impact on the perception of app users and encourages them to submit revised ratings. Nevertheless, developers encounter challenges in managing a high volume of reviews, particularly in the case of popular apps with a substantial influx of daily reviews. Consequently, there is a demand for automated solutions aimed at streamlining the process of responding to user reviews. To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs). Our solution, named SCRABLE, represents an adaptive customer review response automation that enhances itself with self-optimizing prompts and a judging mechanism based on LLMs. Additionally, we introduce an automatic scoring mechanism that mimics the role of a human evaluator to assess the quality of responses generated in customer review domains. Extensive experiments and analyses conducted on real-world datasets reveal that our method is effective in producing high-quality responses, yielding improvement of more than 8.5% compared to the baseline. Further validation through manual examination of the generated responses underscores the efficacy our proposed system.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5185.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5185.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCRABLE (LLM self-improve)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCRABLE - Self-Improving Customer Review Response Automation Based on LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end system that generates responses to customer reviews using retrieval-augmented generation (RAG) with GPT-4 and iteratively improves its prompt and outputs by using an LLM-as-a-Judge to score responses and provide improvement suggestions, producing an optimized prompt via repeated generate-then-judge cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as both the response generator and as the judging/refinement engine; no model size or parameter count provided in the paper (treated as a proprietary large language model).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative prompt optimization with LLM-as-a-Judge (generate-then-judge-and-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For all reviews the system initially generates responses using a base prompt. An LLM-as-a-Judge evaluates each generated response on four categories (Relevancy, Application Specificity, Accuracy, Grammatical Correctness), returning a numeric score and textual suggestions. Reviews with lowest judge scores (bottom n% = 30%) plus a small random sample (m% = 10%) are selected for improvement. The system asks an LLM to revise the original prompt (prompt generator) using judge feedback and expert answers, then regenerates responses with the revised prompt. The loop repeats until an average-score threshold or max iterations is reached. The judge is also used to weight overall score (accuracy weighted double). The pipeline uses RAG to provide application knowledge to both generation and judge steps.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Customer review response generation (app store responses)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate polite, accurate, app-specific responses to user reviews of an application; evaluate quality along relevancy, application specificity, accuracy, and grammatical correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Human-evaluated overall score on test set: improved from 0.78 (base prompt) to 0.85 with LLM-optimized prompt (+8.97% reported; paper also states 'more than 8.5%'); per-category human improvements on test set: App Specificity 0.77->0.87 (+12.99%), Accuracy 0.60->0.68 (+13.33%), Relevancy 0.76->0.84 (+10.53%). LLM-as-a-Judge scores also improved (train overall LLM scores: 0.87 base -> 0.90 LLM-optimized; LLM-as-a-Judge prompt comparison overall: 0.81 base -> 0.91 LLM-optimized).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base-prompt (no iterative LLM-driven refinement) performance: human overall 0.78 (normalized) on test set; LLM-as-a-Judge overall score 0.87 on train (base prompt) and 0.81 when comparing judge prompts in Table 3 (base).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: blind human raters scored test-set responses generated with base vs LLM-optimized prompts and found >8.5% overall improvement (Table 5/Table 6). LLM-as-a-Judge numeric scores also show consistent improvement from base to human-optimized to LLM-optimized prompts (Tables 3 and 4). Qualitative: the paper shows example iterations with prompt, response, judge score and suggestions illustrating how judge feedback leads to concrete prompt edits and higher judge scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Several limitations reported: small dataset (49 reviews total; 28 train / 21 test) limits statistical power; for the training set one judge category showed nearly zero correlation with human scores (Grammar had negligible variance); on the test set LLM judge and human scores diverged for Relevancy and Application Specificity (weak/negative correlation reported), indicating judge misalignment in some categories; human expert responses are used by the judge for Accuracy scoring (in real-time deployment expert answers would be unavailable and must be omitted, which may change judge behavior); mitigation against overfitting is heuristic (select bottom 30% + 10% random); number of iterations and stopping criteria unspecified (thresholds like Judge(IR_j) ≥ 0.95 are arbitrary); system is domain- and app-specific (RAG knowledge base required) so generalization to other domains may be limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Improving Customer Review Response Generation Based on LLMs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5185.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5185.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE (Iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced iterative framework where a single LLM generates initial outputs, evaluates them, and refines them using self-generated feedback, enabling self-improvement without additional external data or training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM (unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced generically as 'large language models' used as generator, refiner, and feedback provider; no size or architecture specified in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refinement / generate-evaluate-refine (self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative process: model generates initial output, evaluates that output (using its own feedback), and refines the output based on the self-evaluation; designed to operate without externally labeled data or additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General iterative self-improvement (as reported in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General-purpose text generation tasks where iterative self-feedback is used to improve model outputs (no specific benchmark reported in this paper's discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited in related work as a prior method that enables models to iteratively improve outputs using self-feedback; the present paper references the approach but does not report experiment-level numbers from that method.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No experiment-specific limitations reported within this paper for Self-Refine; the SCRABLE authors note general concerns about judge-model alignment and the need to prevent overfitting, which are relevant to any self-refinement system.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Improving Customer Review Response Generation Based on LLMs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5185.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5185.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Rewarding LMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Rewarding Language Models (SLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced approach where models create instruction-following training data and train reward models by generating and judging their own outputs, iteratively improving instruction-following and reward prediction abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rewarding language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pre-trained language model (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as starting from a base pre-trained language model with a small human-annotated seed dataset; models then generate instructions and judge their outputs to bootstrap improved training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-rewarding / self-training with LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative cycles where the model generates instruction–response pairs, evaluates its own outputs using an internal judge (LLM-as-a-Judge), and uses the judged/generated data to retrain or fine-tune models (including reward models), aiming to improve instruction-following without large labeled corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction-following and reward modeling (as reported in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Bootstrapping models' instruction-following and reward prediction abilities by repeatedly generating and judging synthetic training data; not applied as an experiment in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Described in related work as producing iterative gains in instruction-following and reward modeling; this paper cites it as background motivation but does not present new numerical comparisons for that method.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail in this paper; general concerns include reliance on initial seed quality, potential for feedback loop amplification of errors, and need to validate self-generated training data against human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Improving Customer Review Response Generation Based on LLMs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-rewarding language models <em>(Rating: 2)</em></li>
                <li>Self-Instruct: Aligning language model with self generated instructions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5185",
    "paper_id": "paper-269614031",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "SCRABLE (LLM self-improve)",
            "name_full": "SCRABLE - Self-Improving Customer Review Response Automation Based on LLMs",
            "brief_description": "An end-to-end system that generates responses to customer reviews using retrieval-augmented generation (RAG) with GPT-4 and iteratively improves its prompt and outputs by using an LLM-as-a-Judge to score responses and provide improvement suggestions, producing an optimized prompt via repeated generate-then-judge cycles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used as both the response generator and as the judging/refinement engine; no model size or parameter count provided in the paper (treated as a proprietary large language model).",
            "reflection_method_name": "Iterative prompt optimization with LLM-as-a-Judge (generate-then-judge-and-refine)",
            "reflection_method_description": "For all reviews the system initially generates responses using a base prompt. An LLM-as-a-Judge evaluates each generated response on four categories (Relevancy, Application Specificity, Accuracy, Grammatical Correctness), returning a numeric score and textual suggestions. Reviews with lowest judge scores (bottom n% = 30%) plus a small random sample (m% = 10%) are selected for improvement. The system asks an LLM to revise the original prompt (prompt generator) using judge feedback and expert answers, then regenerates responses with the revised prompt. The loop repeats until an average-score threshold or max iterations is reached. The judge is also used to weight overall score (accuracy weighted double). The pipeline uses RAG to provide application knowledge to both generation and judge steps.",
            "num_iterations": null,
            "task_name": "Customer review response generation (app store responses)",
            "task_description": "Generate polite, accurate, app-specific responses to user reviews of an application; evaluate quality along relevancy, application specificity, accuracy, and grammatical correctness.",
            "performance_with_reflection": "Human-evaluated overall score on test set: improved from 0.78 (base prompt) to 0.85 with LLM-optimized prompt (+8.97% reported; paper also states 'more than 8.5%'); per-category human improvements on test set: App Specificity 0.77-&gt;0.87 (+12.99%), Accuracy 0.60-&gt;0.68 (+13.33%), Relevancy 0.76-&gt;0.84 (+10.53%). LLM-as-a-Judge scores also improved (train overall LLM scores: 0.87 base -&gt; 0.90 LLM-optimized; LLM-as-a-Judge prompt comparison overall: 0.81 base -&gt; 0.91 LLM-optimized).",
            "performance_without_reflection": "Base-prompt (no iterative LLM-driven refinement) performance: human overall 0.78 (normalized) on test set; LLM-as-a-Judge overall score 0.87 on train (base prompt) and 0.81 when comparing judge prompts in Table 3 (base).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: blind human raters scored test-set responses generated with base vs LLM-optimized prompts and found &gt;8.5% overall improvement (Table 5/Table 6). LLM-as-a-Judge numeric scores also show consistent improvement from base to human-optimized to LLM-optimized prompts (Tables 3 and 4). Qualitative: the paper shows example iterations with prompt, response, judge score and suggestions illustrating how judge feedback leads to concrete prompt edits and higher judge scores.",
            "limitations_or_failure_cases": "Several limitations reported: small dataset (49 reviews total; 28 train / 21 test) limits statistical power; for the training set one judge category showed nearly zero correlation with human scores (Grammar had negligible variance); on the test set LLM judge and human scores diverged for Relevancy and Application Specificity (weak/negative correlation reported), indicating judge misalignment in some categories; human expert responses are used by the judge for Accuracy scoring (in real-time deployment expert answers would be unavailable and must be omitted, which may change judge behavior); mitigation against overfitting is heuristic (select bottom 30% + 10% random); number of iterations and stopping criteria unspecified (thresholds like Judge(IR_j) ≥ 0.95 are arbitrary); system is domain- and app-specific (RAG knowledge base required) so generalization to other domains may be limited.",
            "uuid": "e5185.0",
            "source_info": {
                "paper_title": "Self-Improving Customer Review Response Generation Based on LLMs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "SELF-REFINE (Iterative refinement with self-feedback)",
            "brief_description": "A referenced iterative framework where a single LLM generates initial outputs, evaluates them, and refines them using self-generated feedback, enabling self-improvement without additional external data or training.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "LLM (unspecified in this paper)",
            "model_description": "Referenced generically as 'large language models' used as generator, refiner, and feedback provider; no size or architecture specified in this paper's discussion.",
            "reflection_method_name": "Self-refinement / generate-evaluate-refine (self-feedback)",
            "reflection_method_description": "Iterative process: model generates initial output, evaluates that output (using its own feedback), and refines the output based on the self-evaluation; designed to operate without externally labeled data or additional training.",
            "num_iterations": null,
            "task_name": "General iterative self-improvement (as reported in related work)",
            "task_description": "General-purpose text generation tasks where iterative self-feedback is used to improve model outputs (no specific benchmark reported in this paper's discussion).",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited in related work as a prior method that enables models to iteratively improve outputs using self-feedback; the present paper references the approach but does not report experiment-level numbers from that method.",
            "limitations_or_failure_cases": "No experiment-specific limitations reported within this paper for Self-Refine; the SCRABLE authors note general concerns about judge-model alignment and the need to prevent overfitting, which are relevant to any self-refinement system.",
            "uuid": "e5185.1",
            "source_info": {
                "paper_title": "Self-Improving Customer Review Response Generation Based on LLMs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Self-Rewarding LMs",
            "name_full": "Self-Rewarding Language Models (SLMs)",
            "brief_description": "Referenced approach where models create instruction-following training data and train reward models by generating and judging their own outputs, iteratively improving instruction-following and reward prediction abilities.",
            "citation_title": "Self-rewarding language models",
            "mention_or_use": "mention",
            "model_name": "Pre-trained language model (unspecified)",
            "model_description": "Described as starting from a base pre-trained language model with a small human-annotated seed dataset; models then generate instructions and judge their outputs to bootstrap improved training sets.",
            "reflection_method_name": "Self-rewarding / self-training with LLM-as-a-Judge",
            "reflection_method_description": "Iterative cycles where the model generates instruction–response pairs, evaluates its own outputs using an internal judge (LLM-as-a-Judge), and uses the judged/generated data to retrain or fine-tune models (including reward models), aiming to improve instruction-following without large labeled corpora.",
            "num_iterations": null,
            "task_name": "Instruction-following and reward modeling (as reported in related work)",
            "task_description": "Bootstrapping models' instruction-following and reward prediction abilities by repeatedly generating and judging synthetic training data; not applied as an experiment in the present paper.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Described in related work as producing iterative gains in instruction-following and reward modeling; this paper cites it as background motivation but does not present new numerical comparisons for that method.",
            "limitations_or_failure_cases": "Not discussed in detail in this paper; general concerns include reliance on initial seed quality, potential for feedback loop amplification of errors, and need to validate self-generated training data against human judgments.",
            "uuid": "e5185.2",
            "source_info": {
                "paper_title": "Self-Improving Customer Review Response Generation Based on LLMs",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-rewarding language models",
            "rating": 2,
            "sanitized_title": "selfrewarding_language_models"
        },
        {
            "paper_title": "Self-Instruct: Aligning language model with self generated instructions",
            "rating": 1,
            "sanitized_title": "selfinstruct_aligning_language_model_with_self_generated_instructions"
        }
    ],
    "cost": 0.011209499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Improving Customer Review Response Generation Based on LLMs
6 May 2024</p>
<p>Guy Azov 
Intel Corporation</p>
<p>Tatiana Pelc 
Intel Corporation</p>
<p>Adi Fledel Alon 
Intel Corporation</p>
<p>Gila Kamhi 
Intel Corporation</p>
<p>Self-Improving Customer Review Response Generation Based on LLMs
6 May 2024C38ECFB60BA9CD3D667E435051109C84arXiv:2405.03845v1[cs.CL]Review Response GenerationLLM-as-a-JudgePrompt OptimizationSelf Improving System
Previous studies have demonstrated that proactive interaction with user reviews has a positive impact on the perception of app users and encourages them to submit revised ratings.Nevertheless, developers encounter challenges in managing a high volume of reviews, particularly in the case of popular apps with a substantial influx of daily reviews.Consequently, there is a demand for automated solutions aimed at streamlining the process of responding to user reviews.To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).Our solution, named SCRABLE, represents an adaptive customer review response automation that enhances itself with self-optimizing prompts and a judging mechanism based on LLMs.Additionally, we introduce an automatic scoring mechanism that mimics the role of a human evaluator to assess the quality of responses generated in customer review domains.Extensive experiments and analyses conducted on real-world datasets reveal that our method is effective in producing high-quality responses, yielding improvement of more than 8.5% compared to the baseline.Further validation through manual examination of the generated responses underscores the efficacy our proposed system.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated remarkable performance in a wide range of tasks related to comprehending and generating natural language, text, and code (Devlin et al., 2019;Raffel et al., 2019;Brown et al., 2020), (Zhang et al., 2022;et al, 2022;Chung et al., 2022).The most notable advancement is that these tasks are executed using few-shot or in-context learning (Xie et al., 2022;Dong et al., 2023;Roberts et al., 2020), reducing the need for construction of traditional labeled datasets for supervised learning.Through their ability to efficiently store and apply knowledge, LLMs have shown outstanding capabilities in tasks involving information-seeking questions, where the question cannot be answered easily by the person asking it (Tunstall et al., 2022).Large Language Models (LLMs) are advanced AI systems designed to understand, generate, and manipulate human language.They are trained on vast amounts of text data, allowing them to perform a wide range of language-related tasks.In the contemporary digital age, customer reviews have become a cornerstone of consumer decision-making.Prospective buyers frequently rely on online reviews as a principal source for obtaining insights into various products and services.Empirical research indicates a robust and positive correlation between the numerical rating of a mobile application and the number of downloads it garners.Furthermore, it has been observed that users exhibit a tendency to modify their ratings following the reception of *Equal contribution †Joint supervision responses from developers.Consequently, the act of responding to user reviews is considered imperative in the realm of app development.However, crafting an appropriate response to an online review is a complex task that demands expertise to ensure it matches the customer's feedback in both content and tone.A response must cater to different audiences: the reviewer seeking resolution or acknowledgment, potential customers who use reviews to inform their buying choices, and search engines that use this content for search ranking purposes.The sheer volume and diversity of customer reviews across platforms like e-commerce sites, social media, and review websites present both a treasure trove of information and a daunting challenge for consumers seeking answers and for businesses.The latter often struggle with the resources and time to manage this feedback effectively, and they may not have staff skilled in crafting responses.In this paper, we introduce a scalable automatic end-to-end customer review response generation methodology based on LLMs.We aim to get high-quality responses leveraging an optimization strategy that relies on LLM-as-a-Judge, capable of iteratively scoring and proposing response improvements.Subsequently, these proposals are fed into a prompt generator that generates an improved prompt for each iteration in the response generation process.Our methodology, which deploys custom-tailored prompts for every customer support category, has demonstrated superior performance over the general prompt as per the research conducted by (Yuan et al., 2024).</p>
<p>• We propose (SCRABLE -Self-Improving Customer Review Response Automation Based on LLMs), an LLM-based approach to automatically generate high quality responses to given user reviews.We demonstrate the power of customized prompt engineering to lead the LLM-based solutions to responses that raise customer satisfaction, engagement and delight.Furthermore, we employ automatic prompt engineering, using an LLM to improve a prompt, which is then evaluated against an objective function evaluator.We achieve an optimal review response prompt for inference via a two step method, 1) Review -Response generating LLM (calibrated by human evaluation) 2) Automatic prompt optimization using LLM-as-a-Judge.</p>
<p>• We conduct both manual and automatic evaluation on the performance of the proposed models and baselines.The experimental results indicate that our optimized prompt increased the human score of our test set response generations by more than 8.5% comapred to the generations obtained by using our initial base prompt.</p>
<p>• The results demonstrate that our proposed LLM-as-a-Judge approach achieves 3-5 times stronger correlation with human evaluation compared to (Yuan et al., 2024).</p>
<p>The rest of this paper is organized as follows.Section 2 surveys the related work.Section 3 introduces an overview of the proposed approach and the detailed design of the approach.Section 4 elaborates on the experimental results, including the results from the automatic LLM based evaluation and manual human evaluation.Sections 5 and 6 discuss conclude our work, summarizing the proposed future work.</p>
<p>Related Work</p>
<p>Customer Reviews Analysis</p>
<p>As noted in Pagano et al., user feedback and user involvement are crucial for modern software organizations (Pagano D, 2013).Data mining of user reviews has attracted significant research attention owing to the pivotal role reviews play in shaping consumer perceptions and decision-making regarding applications.Researchers have applied various techniques to analyze these reviews, ranging from fundamental structural features, such as review length and TF-IDF (Term Frequency-Inverse Document Frequency), which are frequently used to automatically classify user review emotions at a high level.Furthermore, more in-depth analyses have been pursued through the extraction of content features, including sentiment, topic, and keywords, often achieved through the application of machine learning methods (Guzman and Maalej, 2014;Martin et al., 2017;Gao et al., 2019;Palomba et al., 2017;Bharti and Babu, 2017).Other papers provide a unified summary of multiple customer reviews using machine learning models (Bražinskas et al., 2020;Brazinskas et al., 2022;Bhaskar et al., 2023).</p>
<p>Customer Reviews Response Generation</p>
<p>In addition to the process and analysis of the reviews, it is crucial to properly respond to the user.In addition to being informative, such response should be polite, address user's concerns, be empathic, leave a positive impression about the product, etc.</p>
<p>It is important for developers to carefully respond to each and every customer review.Hassan et al. indicate that the chances that a user will revisit their review score are six times higher if the review gets a timely and to-the-point response from the product team (Hassan et al., 2018).However, some applications have so many users and reviews such that human responses are not always possible for all of the reviews.In recent years, efforts were made to automatically generate responses to customer reviews using machine learning techniques.</p>
<p>Gao et al. suggest an RNN-based model named</p>
<p>RRGen to encode the review with high level features such as occurrences of specific keywords, rating, review sentiment, review length and app category towards an automatic response generation (Gao et al., 2020).Zhang et al.(Zhang et al., 2023) propose a transformer (Vaswani et al., 2017) based model named TRRGen for automatic app review response generation.TRRGen fuses the features of app category and ratings and demonstrates that the fusion of app category feature and rating feature into token semantics is helpful for generating high-quality responses (competitive with human app expert responses).Gao et al. aim to address two limitations of the method they previously suggested, namely its lack of flexibility and generalization, which often leads to the generation of non-informative responses (Gao et al., 2021).Their proposed solution, named CoRe, leverages app details and responses from similar reviews.In addition, Farooq et al. train a seq2seq model with a retrieval component that merges user reviews with pertinent app descriptions and known user reviews, using specific app features to generate app-aware responses (Farooq et al., 2020).Cao et al. evaluate the performance of selected pre-trained language models against a transformer model trained from scratch in the context of automatic customer review response generation.They find that although pre-trained language models may score lower than baseline models in their experiments, they still prove effective in generating responses and show considerable robustness relative to the amount of training data used (Cao and Fard, 2022).Finally, Chen et al. propose a multi aspect attentive network to automatically attend different aspects of the review, ensuring most of the issues are being answered (Chen et al., 2022)</p>
<p>Response Evaluation</p>
<p>Assessing the quality of generated responses in the context of generative AI models involves multiple parameters such as relevance, coherence, and human-likeness.In the study by Katsiuba et al. (Katsiuba et al., 2023), an online experiment involving 502 participants was leveraged to determine the effectiveness of large language models (LLMs) in generating responses to customer feedback.The experiment's findings indicate that LLMs' responses were not only effective in achieving communicative goals but also held up well when compared to responses written by humans.One key methodology employed to evaluate the responses was the Turing test approach (Turing, 2009), which involves human evaluators to determine the humanlike quality of an utterance generated by an AI.</p>
<p>Traditional automatic evaluation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) often do not correlate well with human judgment due to their focus on lexical matching.Consequently, there is a pressing need for more advanced automatic evaluation techniques that better mirror human assessments.One approach is to employ semantic evaluation methods that measure the similarity between the ground truth and model-generated responses (Zhang et al., 2019;Zhao et al., 2019;Risch et al., 2021).Another emerging strategy is to utilize Large Language Models (LLMs) as evaluators to assess the quality of text and the overall performance of language language models, a practice known as LLM-as-a-Judge (Fu et al., 2023;Gao et al., 2023a;Chiang and Lee, 2023;Liu et al., 2023;Shen et al., 2023;Wang et al., 2023a,c;Peng et al., 2023;Gudibande et al., 2023;Zhou et al., 2023;Dettmers et al., 2023;Dubois et al., 2023;Bubeck et al., 2023;Chan et al., 2023;Yuan et al., 2021;Li et al., 2023;Fernandes et al., 2023;Bai et al., 2023;Saha et al., 2023;Kim et al., 2023a;Zheng et al., 2023;Kim et al., 2023b).While the focus has been on the automatic evaluation of responses, the integration of retrieval-augmented generation (RAG) frameworks (Lewis et al., 2020;Guu et al., 2020;Izacard et al., 2022) has become increasingly prevalent to boost LLM performance.This integration necessitates the development of an automated evaluation system tailored for the comprehensive RAG process (Es et al., 2023;Saad-Falcon et al., 2023).(Wang et al., 2022) propose Self-Instruct, a method for bootstrapping LLMs using instruction-response pairs that they generate themselves.Lastly, Yuan et al. (Yuan et al., 2024) investigate Self-Rewarding Language Models, which are capable of self-improvement by evaluating and training on their own outputs.These models not only employ LLM-as-a-Judge for self-assessment; but also use training data to create instructions that enhance the quality of the target output.Iterative methods use a single LLM to act as the generator, refiner, and feedback provider or to generate and judge its own responses to improve both its response quality and reward prediction ability.The SELF-REFINE framework allows large language models to iteratively improve their output by generating initial output, evaluating it, and then refining it based on self-generated feedback, all without the need for additional or external data or training.This method harnesses the model's own feedback to enact self-improvement, similar to human revision processes (Madaan et al., 2023).The self-improving process involves Self-Rewarding Language Models (SLMs) starting with a base pre-trained language model and a small amount of human-annotated seed data, which then engage in self-instruction creation to generate and judge new training data (Yuan et al., 2024).Each iterative cycle aims to surpass the previous models by using refined training sets from the model's own generations and evaluations, leading to both improved instruction-following abilities and a dynamic, improving reward model-ing capacity.Integrating human expertise with AI, in customer feedback management improves the generation of human-like responses.Human-AI collaborative configurations, such as a combination of deep learning models with human edits, showcased better performance in Turing tests, suggesting they were more human-like than responses from AI alone (Katsiuba et al., 2023).The significant amplification in communicative effectiveness, offering responses that align more closely with customer expectations in terms of quality, fairness, and personalization.Our approach integrates artificial intelligence to enhance customer review analysis by focusing on key elements like accuracy, relevance, and empathy, essential for the customer support domain.By incorporating the LLM-as-a-Judge system, we've introduced an intermediate prompt creation step, which allows for a more controlled and nuanced adjustment process.This strategy involves selectively choosing categories for review and methodically suggesting on which reviews to base new prompts, ensuring a more tailored and impactful response to users.Moreover, our system is designed with stability in mind; the feedback and LLM-as-a-Judge mechanisms are fixed, eliminating the need for generating new training data.The collaboration between the judge LLM and the nuanced prompts across different categories delivers more rounded, human-like responses.Additionally, we have implemented an automated scoring method for the model which correlates well with human judgment, ensuring that our automatic assessment and scoring align closely with human perspectives.</p>
<p>LLM Self Improvement</p>
<p>Methods</p>
<p>LLM as a Judge of Customer Review Response</p>
<p>Due to the limited availability, the challenge of obtaining, and the expenses related to human evaluations, our aim was to create an automated system, LLM-as-a-Judge, that is designed to evaluate customer feedback responses just like a human judge would.Undoubtedly, such a tool provides us with the capability to evaluate online reviews and enhance our services in real-time.It not only facilitates immediate feedback but also paves the way for ongoing enhancements in the way we provide our services.Our approach assumes that for a given collection of customer reviews, denoted as {R i } N i=1 , there exist corresponding responses crafted by human experts, denoted as {ExpertResponse[R i ]} N i=1 .These expert responses act as ideal examples, illustrating the optimal reply for each particular review.In developing an LLM-as-a-Judge intended to serve as a proxy to for actual human evaluation, we initially requested each author of the paper to evaluate the responses based on four criteria -Relevancy -how relevant the response is regarding to the review, Application Specificity -how specific the response is regarding the application, at hand, Accuracy -how accurate the response is and Grammatical Correctness of the response.These criteria were selected because they are widely recognized in the customer review domain ( (Zhang et al., 2023), (Gao et al., 2020), (Gao et al., 2021), (Farooq et al., 2020)) .Evaluators assign ratings to each aspect individually on a scale from 1 to 5. Drawing on the works of (Liu et al., 2023) and (Yuan et al., 2024), we devised specific evaluation prompts for each category that reflect the guidelines given to the human responders (detailed prompts are included in the appendix).These prompts are inputted to the LLM-as-a-Judge, which then generates scores and justifications for each category's evaluation, adopting the prompt structure of (Yuan et al., 2024).To assess Relevancy, Application Specificity, and Grammar, the LLM primarily considers the review and the model's prediction, without referencing the expert's answer.However, for Accuracy, the LLM does reference the expert-provided ground truth answer (i.e.,{ExpertResponse[R i ]} N i=1 ).To further improve the accuracy assessments by the LLM, we integrate the knowledge base of our application and implement the RAG pipeline outlined in 3.5, aiming to make the judgments more credible and precise.It is worth noting that to deploy our LLM-as-a-Judge in real-time, where human expert responses are unavailable, one should omit the human expert response from the evaluation prompts.</p>
<p>Iterative Refinement of Customer Review Response</p>
<p>Once the LLM has demonstrated its capability to assess customer review responses with accuracy comparable to human evaluators, we utilize our optimized LLM as a judge utility to enhance the quality of response generation flow.This time around, we iterate on only the M which represents the reviews with the lowest scores from human evaluation, indicating areas where improvement is needed.To prevent overfitting, we also include a small proportion of randomly selected reviews in this subset.We refer to this curated set of reviews, where the response generation process has not performed optimally, as
{IR j } M j=1 ⊂ {R i } N i=1 (1)
when IR stands for improvement required.We iterate until the score meets the quality criteria or we reach a fixed point.</p>
<p>Judge(IR j ) ≥ 0.95 (2)</p>
<p>Figure 1: Prompt Optimization Flow driven via feedback of LLM as a judge utility</p>
<p>At each iteration, we modify the prompt guiding the LLM for response generation, instructing it to enhance its performance by utilizing insights from the human expert's answer.This adaptive strategy is designed with the aim that such improvements will be applicable more broadly to the generation of responses for future reviews.</p>
<p>LLM as a Response Generator</p>
<p>Our iterative self improving flow illustrated in Algorithm 1 and Figure 1 initially generates response predictions for all N reviews {R i } N i=1 via instructing LLM (in our case GPT4) via the base prompt.The predictions are then evaluated, with scores and feedback, including suggestions for improvements being collected for all reviews as depicted in Algorithm 2 and Figure 2.This results in a collection of (score, suggestions) tuples for all reviews.Reviews with lowest scores (those for which an improvement is required) are flagged.For these reviews, feedback is specifically sought to refine the responses; denoted as {IR j } M j=1 .Following the refinement of the prompt, we calculate the average score for new response predictions across all reviews using this updated prompt.The process is repeated until no further improvements may be achieved or we have reached the quality threshold.The end product of this self improving iterative flow is a customized optimized prompt (i.e., revisedPrompt) that yields the highest score for customer review response predictions through GPT).We adopt a dual strategy approach 1 Aim to improve reviews with the most need for improvement by selecting n % (in our case n = 30) of the lowest scoring responses (based on judge scores; 2 Incorporate stochasticity to combat overfitting via targeting to improve m% (in our case m = 10) additional random response predictions.</p>
<p>As can be seen in Algorithm 2 and respective Figure 2, we leverage LLM both as a generator
prediction i ← ResponseGen(R i ) 7: (score i , suggestions i ) ← Judge(R i , prediction i , ExpertResponse[R i ]) 8:
append (score i , suggestions i ) to feedback 9:</p>
<p>i + + 10: end for 11: return feedback</p>
<p>LLM as a Prompt Generator</p>
<p>The process of refining prompts through LLM is preceded by a rigorous selection of inputs.Ini- tially, all responses generated by the initial prompt are inspected, and only those with the lowest average scores are chosen for further analysis, as detailed in the previous section.To ensure focused improvement, each response category is further filtered to include only those areas where performance falls below a specific threshold, indicating considerable room for improvement.After the selection has been refined, the LLM embarks on the optimization phase, where it reassesses the original agent's prompt within the context of the selected analyses.The aim here is to enhance clarity, eliminate redundancy, and focus on rectifying the identified weaknesses.This custom-made optimization ensures that the most crucial areas of communication are addressed, thereby augmenting the effectiveness of future responses.By focusing on the response's most critical points, the refined prompt is engineered to bolster the system's overall performance.This vital stage in the continuous loop of prompt optimization also acts as a safeguard against overfitting.It transforms a compilation of specific case responses into concise, actionable prompt instructions that can be generalized across various interactions.</p>
<p>Offline and Online Information Retrieval</p>
<p>In the RAG system, as can be seen in Figure 3, two distinct yet interconnected workflows, offline and online, are utilized to provide a seamless information retrieval and response generation process.</p>
<p>Offline Flow:</p>
<p>The offline flow is dedicated to preparing and structuring the data for the RAG system.This involves the following series of steps:</p>
<p>Document Segmentation:</p>
<p>Through the LangChain Character Text Splitter, documents are segmented into 500-token pieces to facilitate easier model interpretation.</p>
<p>Generating Embeddings:</p>
<p>The OpenAI textembedding-ada-002 model is employed to transform document segments into embeddings for better comparison capability.</p>
<p>Storing Documents and Embeddings:</p>
<p>Finally, documents and embeddings are securely stored in a vector store, with Azure AI Search providing straightforward retrieval.</p>
<p>Online Flow:</p>
<p>The online flow is activated when the system interacts with a user's query.It employs a dynamic approach:</p>
<ol>
<li>
<p>Hybrid Retrieval: Using Azure Cognitive Search, the system retrieves the top four segments most relevant to the user's query.</p>
</li>
<li>
<p>Response Generation: GPT-4 integrates the query with the retrieved information to craft a comprehensive and contextually accurate response.By combining these offline and online methods, the RAG system ensures the provision of relevant, accurate, and app-specific responses, including useful references and links, in real-time, leveraging both the vast indexed knowledge and the generative capabilities of the advanced AI model.</p>
</li>
</ol>
<p>Experiments and Results</p>
<p>In this section, we provide detailed information about our experiments and their corresponding results.The findings from our study suggest that employing a GPT4 LLM can effectively:</p>
<p>• Generate automatic responses to customer reviews.</p>
<p>• Achieve good (close to human) evaluations of the quality of customer review responses.</p>
<p>• Automate the enhancement of the LLM's ability to generate responses to customer reviews, ultimately competing with outcomes obtained from human-optimized prompts.</p>
<p>Customer Review Data</p>
<p>We collected forty nine real-life customer reviews pertaining to <OUR APP NAME > 1 in addition to expert responses from various online platforms, and then split them into train (28 reviews) and test (21 reviews) datasets.Additionally, we created an extensive knowledge base that includes the application's documentation, such as user manuals and instructional guides to be used in our RAG flow.</p>
<p>Human Evaluation</p>
<p>Analogous to the methodology employed by (Bhaskar et al., 2023), the authors of the present study were tasked with evaluating responses to customer reviews that were produced by a manually refined prompt.Our focus was targeted on various key aspects, namely Relevancy, Application Specificity, Accuracy, and Grammatical Correctness.The authors received detailed instructions on how to rate each category separately.The scores given by the human judges are compiled in Table 1, which includes metrics such as Krippendorff's alpha and Fleiss kappa.Ultimately, the average score for each category, as determined by the labelers, was calculated and normalized to a 0 − 1 scale using the min-max normalization.</p>
<p>LLM as a Judge</p>
<p>Like the human assessment process, the scores from LLM-as-a-Judge are also normalized.It is important to note, however, that while the "overall score" from human evaluations is an average of the four categories (after normalization), our observations indicated that placing additional emphasis on the accuracy aspect made the LLM's overall scores more aligned with effective outcomes.Thus, the "overall" score of the LLM-as-a-Judge is computed by a weighted average of the categories, with accuracy being given twice the weight of the other categories.A comparison of our LLM and human scores is presented in Table 2. Within the training data set, our LLM-as-a-Judge shows moderate to strong positive correlation with human scores in the same category in three categories (Relevancy,</p>
<p>1 Application name has been left out Accuracy, and Application Specificity) and in the overall score.The fourth category, which presents nearly zero correlation, still exhibits a negligible variance between the LLM and human scores.Moreover, only a few human scores in the Grammar category are less than 5, high grammatical quality generation by GPT-4.For the test dataset, the Accuracy and the overall scores moderately correlate to those from humans, paired with a nearly exact match in Grammatical Correctness.We note that that for the test set, all human scores were at the 5, thus calculations of Krippendorff's Alpha and Fleiss Kappa are irrelevant.However, unlike the training set, the Relevancy and Application Specificity scores of the LLM showed a weak (and negative) correlation with human assessments.To demonstrate the strength of our LLM-as-a-Judge we compared the overall score obtained using our evaluation prompts and the prompt of (Yuan et al., 2024) against the human grades (Table 3).Our experiments imply that a tailored evaluation prompt for each category, specifically related to customer support, is more advantageous than a single broad evaluation prompt.To make the comparison as fair as possible, we made few changes to the original prompt of (Yuan et al., 2024).First, we made the prompt more suitable to customer review domain, for example, we replaced the word 'question' with the word 'review'.We also add the product context to the prompt, similarly to our prompt, enhancing the judge capabilities.Lastly, we tested how adding a reference to the ground truth expert response, affect the scores.The assessment was conducted by calculating the correlation and divergence between these LLM-assigned scores and the scores obtained from human assessments of responses generated by the manually optimized prompt.</p>
<p>LLM as a Response Generator</p>
<p>Our study utilized various examples to showcase the strength of our refined response generation mechanism.Initially, Tables 4 and 5 illustrate that the outputs crafted using our LLM-tailored prompts outperform responses generated with human-tailored and foundational prompts in almost every aspect.This superior performance is consistently observed across both train and test datasets, as evaluated by our LLM-as-a-Judge.Further, in Figure 4 we provide an insight on the improvements obtained in each iteration of our self-improving response generation flow via providing details of prompt, response, score and suggestions of the LLM for an iteration step.To impartially assess the improvement in the results achieved using the base prompt versus our optimized prompt, we enlisted four team members, unaffiliated with this project, to manually score the test set generation obtained using the base and optimized prompts.</p>
<p>Discussion</p>
<p>Building on prior research in the domain of customer review response creation, our study inte-</p>
<p>Conclusions</p>
<p>In summary, our comprehensive preparation of customer review data for both training and testing, combined with the utilization of human evaluators, has enabled us to thoroughly assess the ability of the LLM (GPT4 in particular) to act as an effective response generator to customer reviews of <OUR APP NAME > at an app store.Our experimental results provide strong evidence of LLMs dual functionality.Not only can they effectively generate predictive responses to customer reviews, but they also show a commendable capacity to evaluate the quality of those response predictions.This dual functionality enhances the system's adaptability and versatility, making it a valuable tool in the realm of customer service and communication.The outcomes from our assessments provide a promising foundation for further exploration and improvement of LLMs capabilities in practical real-world settings.</p>
<p>Human Optimized Prompt</p>
<p>As a customer support chatbot assistant, your task is to respond to the review received on the <OUR APP NAME > application.Your goal is to craft a response that will satisfy and delight the customer who wrote the review.Please follow the steps below:</p>
<ol>
<li>
<p>Analyze the customer's question and the context provided.</p>
</li>
<li>
<p>Formulate a response that addresses their concerns or queries.1.Is the response relevant and provides some information related to the user's review ?</p>
</li>
<li>
<p>Is the response addressing the user's review directly?</p>
</li>
<li>
<p>If not specifically mentioned, you may assume that the user is using the <OUR APP FULL NAME > app.</p>
</li>
</ol>
<p>Assign a score ranging from 1.0 to 5.0, where 1.0 signifies a non relevant response and 5.0 indicates a very relevant response.Dont refer the quality of the answer, only refer to its relevancy to the user review.Your output must be a single number between 1.0 to 5.0.Assign a score ranging from 1.0 to 5.0, where 1.0 signifies a response that is not specific to <OUR APP NAME > and 5.0 indicates a response that is very specific for <OUR APP NAME >.Dont refer the quality of the answer, only refer to its specifically relates to<OUR APP NAME >.Your output must be a single number between 1.0 to 5.0.Customer review : query Agent response: result Expert Response : answer After examining the user's review and the agent's response: Briefly justify your total score, up to 150 words.Conclude with the score using the format: 'Total Score: <total points>'</p>
<p>Algorithm 1 #
1
Customer Service Chatbot Assistant -Iteratively self improving customer review response generation based on feedback 1: prompt ← Basic Prompt Template 2: reviews ← {R i } N i=1 3: feedback ← ScoredResponseGen(prompt, reviews) 4: # a list of score &amp; suggestion pairs for each review 5: avgScore ← AverageScore(feedback) feedback for response gen.via the revised prompt 14: avgScore ← AverageScore(feedback) 15: until (avgScore ≥ THRESHOLD) or MAX_ITER 16: return prompt of response predictions (i.e., ResponseGen), and also judge the quality of the predictions according to four categories Relevancy, Application Specificity, Accuracy and Grammatical Correctness (i.e., Judge).Algorithm 2 Get scored response predictons for the reviews at hand including improvement suggestions for each 1: function ScoredResponseGen(prompt, reviews) 2: # Generate scored response predictions 3: feedback ← Empty List 4: i ← 1 5: for each R i in reviews do 6:</p>
<p>Figure 2 :
2
Figure 2: ScoredResponseGen : Given reviews of interest, a prompt and optionally respective expert responses, LLM predicts responses via (Re-sponseGen) utility.Scoring of the quality of the response and improvement suggestions are handled via (Judge) utility.The feedback ouput is a list of score and suggestions pairs for each review.</p>
<p>Figure 3 :
3
Figure 3: Our Retrieval Augmented Generation Pipeline</p>
<p>Figure 4 :
4
Figure 4: Iterative Self-Improving Response Generation Step</p>
<p>Table 2 :
2
LLM-as-a-Judge Compared to Human Scores -Train/Test Sets
The scor-</p>
<p>Table 3 :
3
LLM-as-a-Judge Prompt Comparison : Train/Test Sets
CategoryLLM Scoring (Base) LLM Scoring (Human Optimized) LLM Scoring (LLM Optimized)App Specificity0.760.930.99Accuracy0.720.780.84Relevancy0.940.990.97Grammatical Correctness0.981.001.00Overall0.810.870.91</p>
<p>Table 4 :
4
LLM Scores of Generated Responses -Train Set
CategoryLLM Scoring (Base) LLM Scoring (Human Optimized) LLM Scoring (LLM Optimized)App Specificity0.920.990.99Accuracy0.780.790.81Relevancy0.990.990.99Grammatical Correctness0.991.001.00Overall0.870.890.90</p>
<p>Table 5 :
5
LLM Scores of Generated Responses -Test Set
Category(Normalized) Averaged Human Scoring (Base) (Normalized) Averaged Human Scoring (LLM Optimized)App Specificity0.770.87 (+12.99%)Accuracy0.600.68 (+13.33%)Relevancy0.760.84 (+10.53%)Grammatical Correctness11Overall0.780.85 (+8.97%)</p>
<p>Table 6 :
6
Human Scores of Generated Responses -Test Set
ers were kept blind to the origin of the results, i.e.examples. The scores obtained were noteworthy,which were derived from which prompts. Althoughaveraging 4.68 for relevancy, 4.8 for accuracy, 4.7the LLM exhibited a slight improvement with the re-for application specificity and 4.32 for grammaticalfined prompt, Table 6 reveals a significantly largercorrectness.improvement regarding to human scoring of morethan 8.5% overall. Finally, we generated responsesfor 50 new reviews and solicited a domain expertto evaluate the results, aiming to gain a generalunderstanding of the result quality for new, unseen</p>
<p>After examining the user's review, the agent's response and the expert's response: Briefly justify your total score, up to 150 words.If possible, use the Data Source Context to establish your claims.Conclude with the score using the format: 'Total Score: <total points>' LLM-as-a-Judge -Relevancy Prompt You get a customer review and a corresponding customer service agent response.Your role is to rate the relevancy of the agent's response.Your score should be based on the following criteria:
LLM Prompt OptimizationLLM Optimized Prompt LLM-as-a-Judge -Accuracy PromptYour task is to enhance the effectivenessInstruction: You get a customer review, a correspond-of customer service interactions. Begin byAs a customer support chatbot assistant, ing customer service agent response, andreviewing the original agent's prompt andyour role is to respond to the feedback re-a best possible response designed by anthe analysis of the responses it generated.ceived about the <OUR APP NAME > ap-expert. Your role is to rate how accurate theUse the insights from the analysis to refineplication. Tailor your responses to the cus-agent's response, based on the context andthe agent's prompt, aiming to improvetomer's specific issue, offering helpful solu-the expert response. Your score should bethe agent's overall performance for futuretions and resources. based on the following criteria -Accuracy -interactions. Your revised prompt should be clear, concise, and non-repetitive. Your revised prompt should be focused on addressing the identified areas forContext: {context} Customer review: {question} Answer: In your response, ensure you address the customer's primary issue and provide im-1. Based on the expert's response, does the agent's response answer the user concerns regarding to the <OUR APP NAME > app accurately?3. Only for the issues that may necessi-tate professional intervention, please include this message in your response: improvement while retaining the structure of the original prompt. Be sure to enclose all variables in curly brackets as in the original prompt.mediate, actionable solutions. Refer to the <OUR APP NAME > app and its features, and guide them to the technical support team if the issue persists. Should the cus-2. Does the agent's response lack some information from the expert's response?'Should the problem continue, we en-Begin your revision process here:tomer's query be unclear, clarify by asking 3. Does the agent's response aligned withcourage you to contact our technical Original Agent's Prompt: {question}for more information. If the customer can't the expert response?support team for expert help. [support url]' 4. If the context contains useful informa-Responses Analysis: {context} Your Improved Prompt: tion that can assist the user, incorporate it into your response, add helpfull links from the context, if link is added do not add the same link again as a reference.locate the <OUR APP NAME > app, provide direct links to different app stores. Address 4. Does the agent use the Data Source Context correctly to generate the an-all potential issues related to the <OUR APP swer? NAME > app by providing clear troubleshoot-ing steps. If the customer mentions bugs in 5. Does the agent use the Data Source the <OUR APP NAME > app, direct them Context accurately when addressing to resources for common troubleshooting or provide contact information for technical the user concerns?5. In case the context does not provide any relevant information, use your gen-eral knowledge to formulate a helpful response.support. Assign a score ranging from 1.0 to 5.0, Remember to highlight key information such where 1.0 signifies inaccurate response and as the app's design to conserve resources, 5.0 indicates very accurate response. Dont its unavailability on certain platforms, and refer the quality of the answer, only refer to recent updates. If the customer expresses its accuracy. Your output must be a single6. Start your response by thanking userdisappointment about certain app capabil-number between 1.0 to 5.0.for their feedback, and ensure that yourities, acknowledge their feedback, explain Customer review : {query}response is short and highlights thethe current app capabilities, and hint at fu-Agent response: {result}positive features of the <OUR APPture updates if applicable. Also, don't forget Expert Response : {answer}NAME > application.to mention the recently added tablet sup-Context: {context} Question: {question} Answer:port. Your response should be grammatically cor-rect, free of spelling errors, and maintain a polite and professional tone. Use thedata source context effectively without be-ing overly lengthy or repetitive. Focus ondirectly addressing the user's review andproviding a concise, relevant response.</p>
<p>Customer review : query Agent response: result Expert Response : answer After examining the user's review and the agent's response: Briefly justify your total score, up to 150 words.Conclude with the score using the format: 'Total Score: <total points>'LLM-as-a-Judge -Grammatical Correctness Prompt You get a customer review and a corresponding customer service agent response.Your role is to rate the grammar of the agent's response.Your score should be based on the following criteria:After examining the user's review and the agent's response: Briefly justify your total score, up to 150 words.Conclude with the score using the format: 'Total Score: <total points>'LLM-as-a-Judge -App Specificity You get a customer review and a corresponding customer service agent response.Your role is to rate the agent's response regarding whether it specifically addresses to <OUR APP FULL NAME > app.Your score should be based on the following criteria:1.Is the response specifically tailored to <OUR APP FULL NAME > and its functionalities?2.Do the opening and the end of the response relate to <OUR APP NAME >?3.If not specifically mentioned, you may assume that the user is using the<OUR APP FULL NAME > 4. For your concern, <OUR APP NAME > and <OUR APP FULL NAME > are the acronyms.5.If not specifically mentioned, you may assume that the user is using the <OUR APP FULL NAME > app.
1. Is the response grammatically correct?2. Does the response has no spelling er-rors?Assign a score ranging from 1.0 to 5.0,where 1.0 signifies a wrongly spelled,low quality response and 5.0 indicates agrammatically correct high quality response.Your output must be a single numberbetween 1.0 to 5.0.Customer review : queryAgent response: resultExpert Response : answer
AcknowledgmentsWe wish to express our sincere gratitude to Nady Gorovetsky and Tomer Achdut for their invaluable contributions in this work.Their expert responses to customer reviews and insightful feedback were instrumental.This research greatly benefited from their knowledge and dedication.Their involvement has been a significant factor in the progress of this project.
Benchmarking foundation models with language-model-as-anexaminer. Bibliographical References, Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, arXiv:2306.041812023arXiv preprint</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Automatic keyword extraction for text summarization: A survey. Santosh Kumar, Bharti , Korra Sathya, Babu , 2017</p>
<p>. Adithya Bhaskar, Alexander R Fabbri, Greg Durrett, 2023Prompted opinion summarization with gpt-3.5</p>
<p>Few-shot learning for opinion summarization. Arthur Bražinskas, Mirella Lapata, Ivan Titov, 10.18653/v1/2020.emnlp-main.337Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Efficient fewshot fine-tuning for opinion summarization. Arthur Brazinskas, Ramesh Nallapati, Mohit Bansal, Markus Dreyer, 2022</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Pre-trained neural language models for automatic mobile app user feedback answer generation. Yue Cao, H Fatemeh, Fard, 2022</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Generating persuasive responses to customer reviews with multi-source prior knowledge in e-commerce. Bo Chen, Jiayi Liu, Mieradilijiang Maimaiti, Xing Gao, Ji Zhang, Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. the 31st ACM International Conference on Information &amp; Knowledge Management2022</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.019372023arXiv preprint</p>
<p>Learning phrase representations using rnn encoderdecoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 2014</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Scaling instruction-finetuned language models</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2305.14314Qlora: Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2019</p>
<p>. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, 2023A survey on in-context learning</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, arXiv:2305.14387Alpacafarm: A simulation framework for methods that learn from human feedback. 2023arXiv preprint</p>
<p>Ragas: Automated evaluation of retrieval augmented generation. Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert, arXiv:2309.15217Aakanksha Chowdhery et al. 2022. Palm: Scaling language modeling with pathways. 2023arXiv preprint</p>
<p>App-aware response synthesis for user reviews. Umar Farooq, Fuad Siddique, Zhijia Jamour, Vagelis Zhao, Hristidis, 2020 IEEE International Conference on Big Data (Big Data). IEEE2020</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, F T André, Graham Martins, Ankush Neubig, Jonathan H Garg, Markus Clark, Orhan Freitag, Firat, arXiv:2308.072862023arXiv preprint</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Cuiyun Gao, Jichuan Zeng, Xin Xia, David Lo, Michael R Lyu, Irwin King, Automating app review response generation. 2020</p>
<p>Emerging app issue identification from user feedback: Experience on wechat. Cuiyun Gao, Wujie Zheng, Yuetang Deng, David Lo, Jichuan Zeng, Michael R Lyu, Irwin King, 10.1109/ICSE-SEIP.2019.000402019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). 2019</p>
<p>Automating app review response generation based on contextual knowledge. Cuiyun Gao, Wenjie Zhou, Xin Xia, David Lo, Qi Xie, Michael R Lyu, 10.1145/3464969ACM Trans. Softw. Eng. Methodol. 1312021</p>
<p>Humanlike summarization evaluation with chatgpt. Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, Xiaojun Wan, arXiv:2304.025542023aarXiv preprint</p>
<p>. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang, 2023baugmented generation for large language models: A survey</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, arXiv:2305.15717The false promise of imitating proprietary llms. 2023arXiv preprint</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>How do users like this feature? a fine grained sentiment analysis of app reviews. Emitza Guzman, Walid Maalej, 10.1109/RE.2014.69122572014 IEEE 22nd International Requirements Engineering Conference (RE). 2014</p>
<p>Studying the dialogue between users and developers of free apps in the google play store. Safwat Hassan, Chakkrit Tantithamthavorn, Cor-Paul Bezemer, Ahmed E Hassan, Empirical Software Engineering. 232018</p>
<p>Few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, arXiv:2208.032992022arXiv preprint</p>
<p>Billion-scale similarity search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 732019</p>
<p>Artificially human: Examining the potential of textgenerating technologies in online customer feedback management. Dzmitry Katsiuba, Tannon Kew, Mateusz Dolata, Matej Gurica, Gerhard Schwabe, 2023</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, arXiv:2310.08491Prometheus: Inducing fine-grained evaluation capability in language models. 2023aarXiv preprint</p>
<p>Tae Soo, Kim , Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim, arXiv:2309.13633Evallm: Interactive evaluation of large language model prompts on user-defined criteria. 2023barXiv preprint</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019BartarXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Alpacaeval: An automatic evaluator of instructionfollowing models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Text summarization branches out. 2023. 2004Rouge: A package for automatic evaluation of summaries</p>
<p>Geval: Nlg evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.166342023. may 20236arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>A survey of app store analysis for software engineering. William Martin, Federica Sarro, Yue Jia, Yuanyuan Zhang, Mark Harman, 10.1109/TSE.2016.2630689IEEE Transactions on Software Engineering. 4392017</p>
<p>User feedback in the appstore: An empirical study. D Maalej W Pagano, Proceedings of the 2013 21st IEEE international requirements engineering conference. the 2013 21st IEEE international requirements engineering conference2013</p>
<p>Recommending and localizing change requests for mobile apps based on user reviews. Fabio Palomba, Pasquale Salza, Adelina Ciurumelea, Sebastiano Panichella, Harald Gall, Filomena Ferrucci, Andrea De, Lucia , 10.1109/ICSE.2017.182017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). 2017</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint</p>
<p>Automatic prompt optimization with gradient descent and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023arXiv preprint</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 10.48550/arXiv.1910.10683arXiv:1910.106832019arXiv e-prints</p>
<p>Semantic answer similarity for evaluating question answering models. Julian Risch, Timo Möller, Julian Gutsch, Malte Pietsch, arXiv:2108.061302021arXiv preprint</p>
<p>How much knowledge can you pack into the parameters of a language model?. Adam Roberts, Colin Raffel, Noam Shazeer, 2020</p>
<p>Ares: An automated evaluation framework for retrievalaugmented generation systems. Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia, arXiv:2311.094762023arXiv preprint</p>
<p>Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li, arXiv:2310.15123Branch-solve-merge improves large language model evaluation and generation. 2023arXiv preprint</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, Lidong Bing, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, V Quoc, Le, 2014</p>
<p>Moshe Wasserblat, and Oren Pereg. 2022. Efficient few-shot learning without prompts. Lewis Tunstall, Nils Reimers, Eun Seo Unso, Luke Jo, Daniel Bates, Korat, </p>
<p>Computing machinery and intelligence. Alan M Turing, 2009Springer</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023aarXiv preprint</p>
<p>Promptagent: Strategic planning with language models enables expert-level prompt optimization. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, Zhiting Hu, arXiv:2310.164272023barXiv preprint</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Raghavi Khyathi, David Chandu, Kelsey Wadden, Noah A Macmillan, Iz Smith, Beltagy, arXiv:2306.047512023carXiv preprint</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.10560Self-instruct: Aligning language model with self generated instructions. 2022arXiv preprint</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, 2020</p>
<p>An explanation of incontext learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, 2022</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, Self-rewarding language models. 2024</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022Opt: Open pre-trained transformer language models</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>
<p>A transformer-based approach for improving app review response generation. Weizhe Zhang, Wenchao Gu, Cuiyun Gao, Michael R Lyu, Software: Practice and Experience. 5322023</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger ; Lianmin, Wei-Lin Zheng, Ying Chiang, Siyuan Sheng, Zhanghao Zhuang, Yonghao Wu, Zi Zhuang, Zhuohan Lin, Dacheng Li, Li, arXiv:1909.02622arXiv:2306.05685Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. 2019arXiv preprint</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, arXiv:2305.11206Lima: Less is more for alignment. 2023arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. A. Appendix A.1. Prompts Base Prompt Instruction: As a customer support chatbot assistant, your task is to respond to the review received on the <OUR APP NAME > application based on the context information. Lianghui Zhu, Xinggang Wang, Xinlong Wang, 2023Context: context Question: question Answer</p>            </div>
        </div>

    </div>
</body>
</html>
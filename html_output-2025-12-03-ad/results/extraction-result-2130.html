<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2130 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2130</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2130</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-55.html">extraction-schema-55</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-280650168</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.10795v1.pdf" target="_blank">Beyond “Not Novel Enough”: Enriching Scholarly Critique with LLM-Assisted Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Novelty assessment is a central yet understud-ied aspect of peer review, particularly in high-volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence-based assessment. Our method is informed by a large-scale analysis of human-written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions—substantially outperforming existing LLM-based baselines. The method produces detailed, literature-aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM-assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available. 1</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2130.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2130.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PeerReview_NoveltyProxy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Peer-review novelty judgments as a proxy for scientific value</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper studies peer-review novelty assessments (human and LLM-assisted) as proxy metrics for scientific value, comparing system-generated novelty assessments to style-normalized human novelty assessments treated as ground truth and quantifying alignment across reasoning and conclusion metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>peer review novelty judgments / reviewer scores</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>style-normalized human novelty assessments (human reviews synthesized into unified novelty assessments via GPT-4.1)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>human-annotated novelty assessment statements and qualitative depth categories (Surface / Moderate / Deep) used to distinguish incremental versus more substantive novel claims; judged contributions compared individually (Novelty Delta Analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Reported as alignment percentages between system outputs (proxy) and the human ground truth: Reasoning Alignment and Conclusion Agreement. Examples: Our system: Reasoning Alignment 86.5% ±0.20, Conclusion Agreement 75.3% ±0.85; Human vs Human baseline: Reasoning Alignment 65.1% ±1.05, Conclusion Agreement 62.8% ±0.40; OpenReviewer: 42.4% / 46.8%; DeepReviewer: 50.6% / 51.5%. (These figures quantify how closely proxy reviewer judgments match the chosen ground truth.)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Not reported as a function of novelty level (incremental vs transformational). The paper reports overall proxy-ground-truth gaps via alignment scores (e.g., ~10–40 percentage-point differences across systems relative to human ground truth) but does not stratify gap magnitude specifically by degree of novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (ICLR submissions; NLP/ML conference)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not reported — evaluation restricted to ICLR 2025 (CS/NLP); authors explicitly note performance on other domains remains untested.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>The proposed LLM-assisted pipeline substantially outperformed baselines on alignment metrics: Reasoning Alignment 86.5% and Conclusion Agreement 75.3% (Ours) vs. Human vs Human 65.1% / 62.8%, DeepReviewer 50.6% / 51.5%, OpenReviewer 42.4% / 46.8%. Depth-of-analysis and prior-work engagement distributions show the proposed system produced 0% surface-level analyses and 52.1% deep analyses (vs. human baseline deep 11.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>A three-stage pipeline (document processing, related-work retrieval + reranking, and structured novelty assessment) informed by human-analysis prompts; component ablation shows human-informed prompt design gave the largest gains (+40.7% reasoning, +46.8% conclusion), structured extraction added +3.3% / +4.5%, landscape analysis added +3.2% reasoning and -0.7% conclusion. The pipeline therefore acts as an effective correction mechanism for reviewer-proxy gaps in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Discussed qualitatively: DeepReviewer was trained on ICLR 2025 data which includes the evaluation set, which may bias its apparent performance. The paper notes potential bias of systems trained on available corpora and warns about domain/training-distribution limitations, but provides no numerical estimate of this bias beyond noting DeepReviewer's training overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td>The paper documents variability among human reviewers (human-human disagreement ~35–40% for novelty judgments in their analyzed set) and cites a prior study where two NeurIPS committees disagreed on 23% of identical papers (Beygelzimer et al., 2023), illustrating cases where peer-review proxies diverge from consensus ground truth; however it does not present systematic counterexamples where proxies reliably recognize transformational work vs incremental work.</td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Empirical evaluation on 182 ICLR 2025 submissions with human-annotated novelty assessment statements (dataset curated from OpenReview). Ground truth formed by GPT-4.1 synthesis of human novelty fragments; automated evaluation used LLM-as-Judge (GPT-4.1) across four dimensions (novelty conclusion alignment, novelty reasoning alignment, prior work engagement, depth of analysis). Human validation: 3 PhD evaluators performed 100 pairwise comparisons (25 overlapping per evaluator plus unique samples), with inter-rater reliability reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2130.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2130.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PosNeg_ShiftMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Positive Shift and Negative Shift sentiment metrics for novelty conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Defined in this paper as derived metrics measuring directional shifts between system-generated novelty conclusions and human reference (Positive Shift = cases where system assessment is more positive than human; Negative Shift = more negative), used to quantify optimism/pessimism biases of systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>directional sentiment shift of novelty verdicts (Positive/Negative Shift)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>style-normalized human novelty assessments (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>not directly a novelty measure — used to indicate bias in proxy verdict direction relative to human ground truth; complements depth/engagement measures</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Table 2 reports Positive and Negative Shift percentages per system: OpenReviewer Positive Shift 16.3% ±0.27, Negative Shift 15.3% ±0.40; DeepReviewer Positive Shift 21.7% ±1.89, Negative Shift 9.1% ±0.00; Human vs Human Positive Shift 6.7% ±0.79, Negative Shift 15.0% ±0.40; Scideator Positive Shift 0.0% ±0.00, Negative Shift 20.5% ±0.00; Ours Positive Shift 16.3% ±1.28, Negative Shift 3.0% ±0.43. These quantify how often systems are more optimistic/pessimistic than human reference.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Provides overall magnitude of directional bias (e.g., DeepReviewer shows higher Positive Shift ~21.7% vs human-human ~6.7%; proposed system lowered Negative Shift to 3.0% but Positive Shift remained ~16.3%), but not stratified by novelty class.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (ICLR submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>These metrics are diagnostic of system bias: DeepReviewer shows largest Positive Shift (over-optimism), while the proposed system shows low Negative Shift (less over-criticism) and moderate Positive Shift; no accuracy-by-novelty-level reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>The pipeline reduced Negative Shift to 3.0% (lowest among compared systems) by improving depth and evidence-based comparisons; no temporal correction mechanism reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Computed across the same 182-paper evaluation set; Percentages reported with uncertainty intervals in Table 2 using the LLM-as-Judge framework.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2130.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2130.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DepthEngagement_Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-of-analysis and Prior-Work Engagement distributions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper operationalizes 'Depth of Analysis' (Surface / Moderate / Deep) and 'Prior Work Engagement' (None / Limited / Extensive) as structured measures that relate proxy novelty assessments to the thoroughness and literature grounding that approximates scientific value.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>depth-of-analysis and prior-work engagement of novelty assessments (as proxies for assessment quality/value)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>human novelty assessment depth and literature citations (style-normalized human novelty assessments)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Depth categories serve as a proxy for how well novelty/transformational claims are substantiated (Deep analyses implying stronger verification of novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Reported distributions (Table 3 & 4). For 'Depth': Ours — Surface 0.0%, Moderate 47.9%, Deep 52.1%; Human vs Human — Surface 22.3%, Moderate 66.2%, Deep 11.5%; OpenReviewer — Surface 67.4%, Moderate 31.3%, Deep 1.2%. For 'Prior Work Engagement': Ours — None 0.0%, Limited 3.9%, Extensive 60.9%; Human vs Human — None 19.6%, Limited 65.2%, Extensive 15.2%. These figures connect proxy assessment thoroughness to the human ground truth benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Substantial gaps: many baseline systems produce high rates of surface-level analyses (OpenReviewer 67.4% surface) while human ground truth contains more moderate analyses; proposed system flips distribution to majority deep analyses (52.1%) unlike human reviewers (11.5% deep). No breakdown by novelty magnitude (incremental vs transformational) provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (ICLR 2025 submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>The proposed system attains far higher rates of 'Deep' analyses and 'Extensive' prior-work engagement than baselines, suggesting automated systems can be tuned to increase evidence-based proxy quality (numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Structured extraction (six components per paper) and explicit landscape + delta analysis steps increase depth and prior-work engagement; ablations attribute modest gains to structured extraction and larger gains to human-informed prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Distributional analysis across the 182-paper dataset; categorizations produced by LLM-as-Judge evaluation using pre-extracted human core judgments as reference.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment <em>(Rating: 2)</em></li>
                <li>A billion-dollar donation: estimating the cost of researchers' time spent on peer review <em>(Rating: 1)</em></li>
                <li>The ability of different peer review procedures to flag problematic publications <em>(Rating: 2)</em></li>
                <li>Understanding peer review of software engineering papers <em>(Rating: 1)</em></li>
                <li>Is ChatGPT good at search? investigating large language models as re-ranking agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2130",
    "paper_id": "paper-280650168",
    "extraction_schema_id": "extraction-schema-55",
    "extracted_data": [
        {
            "name_short": "PeerReview_NoveltyProxy",
            "name_full": "Peer-review novelty judgments as a proxy for scientific value",
            "brief_description": "This paper studies peer-review novelty assessments (human and LLM-assisted) as proxy metrics for scientific value, comparing system-generated novelty assessments to style-normalized human novelty assessments treated as ground truth and quantifying alignment across reasoning and conclusion metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "peer review novelty judgments / reviewer scores",
            "ground_truth_measure": "style-normalized human novelty assessments (human reviews synthesized into unified novelty assessments via GPT-4.1)",
            "novelty_transformation_measure": "human-annotated novelty assessment statements and qualitative depth categories (Surface / Moderate / Deep) used to distinguish incremental versus more substantive novel claims; judged contributions compared individually (Novelty Delta Analysis)",
            "quantitative_relationship": "Reported as alignment percentages between system outputs (proxy) and the human ground truth: Reasoning Alignment and Conclusion Agreement. Examples: Our system: Reasoning Alignment 86.5% ±0.20, Conclusion Agreement 75.3% ±0.85; Human vs Human baseline: Reasoning Alignment 65.1% ±1.05, Conclusion Agreement 62.8% ±0.40; OpenReviewer: 42.4% / 46.8%; DeepReviewer: 50.6% / 51.5%. (These figures quantify how closely proxy reviewer judgments match the chosen ground truth.)",
            "gap_magnitude": "Not reported as a function of novelty level (incremental vs transformational). The paper reports overall proxy-ground-truth gaps via alignment scores (e.g., ~10–40 percentage-point differences across systems relative to human ground truth) but does not stratify gap magnitude specifically by degree of novelty.",
            "temporal_pattern": null,
            "field_studied": "Computer science (ICLR submissions; NLP/ML conference)",
            "field_differences": "Not reported — evaluation restricted to ICLR 2025 (CS/NLP); authors explicitly note performance on other domains remains untested.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "The proposed LLM-assisted pipeline substantially outperformed baselines on alignment metrics: Reasoning Alignment 86.5% and Conclusion Agreement 75.3% (Ours) vs. Human vs Human 65.1% / 62.8%, DeepReviewer 50.6% / 51.5%, OpenReviewer 42.4% / 46.8%. Depth-of-analysis and prior-work engagement distributions show the proposed system produced 0% surface-level analyses and 52.1% deep analyses (vs. human baseline deep 11.5%).",
            "correction_mechanism": "A three-stage pipeline (document processing, related-work retrieval + reranking, and structured novelty assessment) informed by human-analysis prompts; component ablation shows human-informed prompt design gave the largest gains (+40.7% reasoning, +46.8% conclusion), structured extraction added +3.3% / +4.5%, landscape analysis added +3.2% reasoning and -0.7% conclusion. The pipeline therefore acts as an effective correction mechanism for reviewer-proxy gaps in this setting.",
            "training_distribution_bias": "Discussed qualitatively: DeepReviewer was trained on ICLR 2025 data which includes the evaluation set, which may bias its apparent performance. The paper notes potential bias of systems trained on available corpora and warns about domain/training-distribution limitations, but provides no numerical estimate of this bias beyond noting DeepReviewer's training overlap.",
            "counterexamples": "The paper documents variability among human reviewers (human-human disagreement ~35–40% for novelty judgments in their analyzed set) and cites a prior study where two NeurIPS committees disagreed on 23% of identical papers (Beygelzimer et al., 2023), illustrating cases where peer-review proxies diverge from consensus ground truth; however it does not present systematic counterexamples where proxies reliably recognize transformational work vs incremental work.",
            "study_design": "Empirical evaluation on 182 ICLR 2025 submissions with human-annotated novelty assessment statements (dataset curated from OpenReview). Ground truth formed by GPT-4.1 synthesis of human novelty fragments; automated evaluation used LLM-as-Judge (GPT-4.1) across four dimensions (novelty conclusion alignment, novelty reasoning alignment, prior work engagement, depth of analysis). Human validation: 3 PhD evaluators performed 100 pairwise comparisons (25 overlapping per evaluator plus unique samples), with inter-rater reliability reported.",
            "uuid": "e2130.0"
        },
        {
            "name_short": "PosNeg_ShiftMetrics",
            "name_full": "Positive Shift and Negative Shift sentiment metrics for novelty conclusions",
            "brief_description": "Defined in this paper as derived metrics measuring directional shifts between system-generated novelty conclusions and human reference (Positive Shift = cases where system assessment is more positive than human; Negative Shift = more negative), used to quantify optimism/pessimism biases of systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "directional sentiment shift of novelty verdicts (Positive/Negative Shift)",
            "ground_truth_measure": "style-normalized human novelty assessments (as above)",
            "novelty_transformation_measure": "not directly a novelty measure — used to indicate bias in proxy verdict direction relative to human ground truth; complements depth/engagement measures",
            "quantitative_relationship": "Table 2 reports Positive and Negative Shift percentages per system: OpenReviewer Positive Shift 16.3% ±0.27, Negative Shift 15.3% ±0.40; DeepReviewer Positive Shift 21.7% ±1.89, Negative Shift 9.1% ±0.00; Human vs Human Positive Shift 6.7% ±0.79, Negative Shift 15.0% ±0.40; Scideator Positive Shift 0.0% ±0.00, Negative Shift 20.5% ±0.00; Ours Positive Shift 16.3% ±1.28, Negative Shift 3.0% ±0.43. These quantify how often systems are more optimistic/pessimistic than human reference.",
            "gap_magnitude": "Provides overall magnitude of directional bias (e.g., DeepReviewer shows higher Positive Shift ~21.7% vs human-human ~6.7%; proposed system lowered Negative Shift to 3.0% but Positive Shift remained ~16.3%), but not stratified by novelty class.",
            "temporal_pattern": null,
            "field_studied": "Computer science (ICLR submissions)",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "These metrics are diagnostic of system bias: DeepReviewer shows largest Positive Shift (over-optimism), while the proposed system shows low Negative Shift (less over-criticism) and moderate Positive Shift; no accuracy-by-novelty-level reporting.",
            "correction_mechanism": "The pipeline reduced Negative Shift to 3.0% (lowest among compared systems) by improving depth and evidence-based comparisons; no temporal correction mechanism reported.",
            "training_distribution_bias": null,
            "counterexamples": null,
            "study_design": "Computed across the same 182-paper evaluation set; Percentages reported with uncertainty intervals in Table 2 using the LLM-as-Judge framework.",
            "uuid": "e2130.1"
        },
        {
            "name_short": "DepthEngagement_Metrics",
            "name_full": "Depth-of-analysis and Prior-Work Engagement distributions",
            "brief_description": "The paper operationalizes 'Depth of Analysis' (Surface / Moderate / Deep) and 'Prior Work Engagement' (None / Limited / Extensive) as structured measures that relate proxy novelty assessments to the thoroughness and literature grounding that approximates scientific value.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "depth-of-analysis and prior-work engagement of novelty assessments (as proxies for assessment quality/value)",
            "ground_truth_measure": "human novelty assessment depth and literature citations (style-normalized human novelty assessments)",
            "novelty_transformation_measure": "Depth categories serve as a proxy for how well novelty/transformational claims are substantiated (Deep analyses implying stronger verification of novelty)",
            "quantitative_relationship": "Reported distributions (Table 3 & 4). For 'Depth': Ours — Surface 0.0%, Moderate 47.9%, Deep 52.1%; Human vs Human — Surface 22.3%, Moderate 66.2%, Deep 11.5%; OpenReviewer — Surface 67.4%, Moderate 31.3%, Deep 1.2%. For 'Prior Work Engagement': Ours — None 0.0%, Limited 3.9%, Extensive 60.9%; Human vs Human — None 19.6%, Limited 65.2%, Extensive 15.2%. These figures connect proxy assessment thoroughness to the human ground truth benchmarks.",
            "gap_magnitude": "Substantial gaps: many baseline systems produce high rates of surface-level analyses (OpenReviewer 67.4% surface) while human ground truth contains more moderate analyses; proposed system flips distribution to majority deep analyses (52.1%) unlike human reviewers (11.5% deep). No breakdown by novelty magnitude (incremental vs transformational) provided.",
            "temporal_pattern": null,
            "field_studied": "Computer science (ICLR 2025 submissions)",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "The proposed system attains far higher rates of 'Deep' analyses and 'Extensive' prior-work engagement than baselines, suggesting automated systems can be tuned to increase evidence-based proxy quality (numbers above).",
            "correction_mechanism": "Structured extraction (six components per paper) and explicit landscape + delta analysis steps increase depth and prior-work engagement; ablations attribute modest gains to structured extraction and larger gains to human-informed prompt design.",
            "training_distribution_bias": null,
            "counterexamples": null,
            "study_design": "Distributional analysis across the 182-paper dataset; categorizations produced by LLM-as-Judge evaluation using pre-extracted human core judgments as reference.",
            "uuid": "e2130.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment",
            "rating": 2
        },
        {
            "paper_title": "A billion-dollar donation: estimating the cost of researchers' time spent on peer review",
            "rating": 1
        },
        {
            "paper_title": "The ability of different peer review procedures to flag problematic publications",
            "rating": 2
        },
        {
            "paper_title": "Understanding peer review of software engineering papers",
            "rating": 1
        },
        {
            "paper_title": "Is ChatGPT good at search? investigating large language models as re-ranking agents",
            "rating": 1
        }
    ],
    "cost": 0.014470249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback</p>
<p>Osama Mohammed Afzal 
UKP Lab
TU Darmstadt and Hessian Center for AI (hessian.AI</p>
<p>Preslav Nakov 
MBZUAI</p>
<p>Tom Hope 
The Allen Institute for AI (AI2)</p>
<p>Iryna Gurevych 
UKP Lab
TU Darmstadt and Hessian Center for AI (hessian.AI</p>
<p>Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback
24F24D2F823EE2CEB7FFDA43159ED443
Novelty assessment is a central yet understudied aspect of peer review, particularly in highvolume fields like NLP where reviewer capacity is increasingly strained.We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence-based assessment.Our method is informed by a large-scale analysis of humanwritten novelty reviews and captures key patterns such as independent claim verification and contextual reasoning.Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions-substantially outperforming existing LLM-based baselines.The method produces detailed, literature-aware analyses and improves consistency over ad hoc reviewer judgments.These results highlight the potential for structured LLM-assisted approaches to support more rigorous and transparent peer review without displacing human expertise.Data and code are made available. 1</p>
<p>Introduction</p>
<p>The peer review system is collapsing under its own success.Two independent committees at NeurIPS 2021 disagreed on 23% of identical papers (Beygelzimer et al., 2023)-a breakdown in consistency that signals deeper problems than mere capacity constraints.With manuscript submissions doubling every 15 years (Larsen and Ins, 2010) and reviewers now handling 14 evaluations annually (Díaz et al., 2024), the system's 15 million annual reviewing hours (Aczel et al., 2021) are producing increasingly unreliable outcomes.</p>
<p>1 https://ukplab.github.io/arxiv2025-assessing-paper-novelty Among peer review tasks, novelty assessment stands out as one of the most problematic (Ernst et al., 2020) (Horbach and Halffman, 2018).Novelty assessment requires reviewers to determine whether a submission makes sufficiently original contributions by identifying what specific advances it makes beyond existing work, evaluating whether these advances are significant enough to warrant publication, and verifying that the authors have accurately characterized their contributions relative to prior research.This knowledge-intensive process demands that reviewers maintain comprehensive awareness of related work across their field and can precisely distinguish between meaningful innovations and incremental modifications-a task that becomes exponentially more difficult as publication rates accelerate and research domains specialize.Overwhelmed reviewers often resort to superficial analyses, producing vague feedback like "not novel enough" without clear justification.The challenge compounds when reviewers encounter papers outside their specific expertise, leading to either overly conservative rejections or 1 arXiv:2508.10795v2[cs.CL] 17 Aug 2025 inadequate assessments that fail to catch incremental work (Kuznetsov et al., 2024).</p>
<p>Recent advances in large language models present an unprecedented opportunity to address these novelty assessment challenges at scale.These breakthrough technologies have revolutionized text processing and demonstrated remarkable performance across knowledge-intensive tasks (Raiaan et al., 2024), with recent technical advancements expanding capabilities to specialized reasoning and efficient inference (Li et al., 2024a;Zhang et al., 2025).</p>
<p>While recent LLM advances create this opportunity, no existing work specifically addresses novelty assessment as a dedicated task within the peer review process.Prior research incorporates novelty evaluation within idea generation pipelines (Radensky et al., 2025;Lu et al., 2024;Li et al., 2024b), generates peer reviews with novelty assessments occurring as a result of them existing in peer reviews from training data (Idahl and Ahmadi, 2025;D'Arcy et al., 2024), or adds novelty assessment steps to review synthesis pipelines for improvement (Zhu et al., 2025).However, these approaches either operate on synthetic ideas rather than real research contributions or fail to evaluate novelty assessment capabilities in isolation.This represents a critical gap requiring specialized methodologies for peer review novelty assessment.</p>
<p>To address this gap, we propose an end-to-end novelty assessment pipeline for peer review submissions.Our approach consists of three stages: document processing and content extraction, related work retrieval and ranking, and structured novelty assessment.The final stage implements four sequential steps: novelty related content selection from the submission pdf, building comprehensive understanding of related work from retrieved papers, comparing claimed novelty against the comprehensive analysis from the prior step, and generating a summary with cited evidence from the comparison.This pipeline operates on real research papers and directly evaluates novelty assessment capabilities, addressing the limitations of existing approaches.Importantly, we conduct the first evaluation of LLMs for novelty assessment using actual human data, including annotated novelty assessment statements, and provide comprehensive evaluation across multiple dimensions.</p>
<p>Research Questions and Contributions This work aims to address the following research ques-tions:</p>
<ol>
<li>
<p>How does our human-informed novelty assessment pipeline compare to existing approaches?</p>
</li>
<li>
<p>How well do our assessments align with human reviewer preferences across key evaluation dimensions?</p>
</li>
<li>
<p>Can automated evaluation reliably substitute for human judgment in assessing novelty assessment quality?</p>
</li>
</ol>
<p>Our contributions are threefold:</p>
<p>• Human Analysis Dataset and Insights: A systematically curated dataset of 182 papers with annotated human novelty assessments from ICLR 2025, along with empirical insights into expert reviewer reasoning patterns, evaluation criteria, and argument structures that inform AI system design for novelty assessment.</p>
<p>• Human-Informed Pipeline: A literaturegrounded pipeline that incorporates insights from human novelty assessment practices, featuring structured prompting strategies and targeted content extraction informed by observed expert reviewer behavior.</p>
<p>• Comprehensive Evaluation and Analysis: Systematic comparison of our humaninformed approach against existing baselines and human reviewers, with fine-grained evaluation across multiple dimensions and validation of automated assessment methods.</p>
<p>Related Work</p>
<p>AI-Assisted Peer Review Systems Our work is positioned at the peer review stage of scientific research, where our system operates when a manuscript is submitted for evaluation.While previous works (D 'Arcy et al., 2024) (Idahl and Ahmadi, 2025) (Zhu et al., 2025) (Chitale et al., 2025) (Chang et al., 2025) (Nemecek et al., 2025) have developed end-to-end peer review generation pipelines that may implicitly include novelty assessment steps, we are the first to focus specifically on building a dedicated pipeline for novelty assessment and the first to systematically evaluate LLMs on this task.A related line of work operates at the ideation stage of research (Radensky et al., 2025) (Shahid et al., 2025) (Li et al., 2024b) (Lu et al., 2024), developing pipelines for research idea generation that aim to improve novelty through feedback loops from a novelty assessor.In contrast, we operate at a more mature stage where ideas have been fully executed and comparative analyses are well-formulated.The evaluation in ideation-stage works focuses on synthetic ideas that are typically abstract and loosely defined, whereas we evaluate concrete, polished research contributions that have undergone the refinement process of execution and manuscript preparation.</p>
<p>Scientific Literature Analysis &amp; Retrieval Our work employs an extensive related work discovery pipeline that collects papers cited within the submission and additionally retrieves related papers by querying with prompts generated by GPT-4.1.Papers are then ranked using an embeddingbased method and reranked using RankGPT.We adapt this general approach from existing work (Radensky et al., 2025) (Shahid et al., 2025) (Li et al., 2024b) with modifications to ranking and filtering for our specific task.Similar retrieval-rankrerank pipelines have been used for related work generation (Agarwal et al., 2025).Another retrieval approach is OpenScholar (Asai et al., 2024), which uses an LLM-RAG based approach to answer scientific queries by identifying relevant passages from 45 million open-access papers.Works like DeepReviewer (Zhu et al., 2025) incorporate OpenScholar for novelty validation.However, our primary criticism of OpenScholar for novelty assessment is that it provides only generic comparisons rather than the granular analysis across methodology, problem formulation, evaluation approaches, and novelty claims that our task requires.</p>
<p>Evaluation of LLM Generated Text Prior works evaluating generated peer reviews have adopted either quantitative evaluations, where they compare LLM-assigned scores (such as Overall Score, Soundness, etc.) against human-assigned scores on review forms, or qualitative evaluations using traditional metrics like BERTScore (Zhang et al., 2020), ROUGE (Lin, 2004), and BLEU (Papineni et al., 2002), or more recent approaches like LLM-as-Judge (Zheng et al., 2023).We adopt the LLM-as-Judge approach for our evaluation.Notably, no prior work has specifically evaluated LLM performance on novelty assessment as a dedicated task, making our evaluation framework the first of its kind.</p>
<p>Methodology</p>
<p>Human Analysis for Prompt Design</p>
<p>To understand how humans conduct novelty assessment, we analyzed reviews from ICLR 2025, which explicitly requires novelty evaluation with dedicated review sections, making novelty discussions more frequent than in other venues.We sourced submissions from OpenReview and used keyword-based search for terms including "novel", "original", "research gap", "innovation", "incremental", "prior work", and "existing work".Papers were ranked using a scoring function prioritizing: (1) reviews with &gt;4 novelty keywords, (2) consistent novelty discussion patterns across reviews, and (3) total review count.We selected the top 200 papers for analysis.</p>
<p>To speed up the annotation process, we employed multiple instances of GPT-4o mini to perform sentence-level classification, determining whether individual sentences discussed novelty.This classification helped human annotators identify general areas where novelty discussions take place, after which humans selected all sentences containing actual novelty assessments.This process revealed that 18 of the 200 sampled papers (9%) contained limited genuine novelty assessments, often triggered by keyword matches referring to paper components rather than novelty evaluation.The remaining 182 papers formed our final dataset for analysis.We systematically analyzed the selected assessments to identify recurring patterns in reviewer reasoning, evaluation criteria, and argument structures.This analysis focused on how reviewers structure their novelty arguments, what evidence they prioritize, and how they compare submissions to prior work.</p>
<p>This analysis revealed several key patterns in how expert reviewers assess novelty:</p>
<p>Verification over acceptance: Rather than accepting author claims at face value, reviewers independently verify relationships with prior work and critically examine how authors characterize related research, often distinguishing between author framing and actual technical relationships.Our prompt explicitly instructs models to "independently verify relationships" and "distinguish between authorclaimed differences and independently observed differences," mirroring this critical verification approach, as shown in Figures 10 and 11.</p>
<p>Variable granularity: Reviewers assess contributions with varying detail-some providing global novelty assessments while others examine each contribution separately against relevant prior work.(We address this through the "Contribution Delta Analysis" section that systematically examines each claimed contribution individually against the most similar prior work, ensuring comprehensive coverage regardless of author presentation style, as detailed in Figure 11.)</p>
<p>Different analytical lenses: Some reviewers focus on methodological innovations while others evaluate systems holistically, calibrating expectations based on field maturity.Our prompt incorporates multiple analytical perspectives through separate sections for research positioning, methodological relationships, and field context considerations that help calibrate novelty expectations based on area maturity, shown across Figures 10 and 11.</p>
<p>Gap identification: Reviewers systematically identify gaps in related work discussions and distinguish between implementation-level improvements and genuine conceptual advances.(The "Related Work Considerations" section specifically instructs models to identify missing comparisons and assess whether improvements stem from "implementation details rather than conceptual advances," directly addressing this reviewer behavior in Figure 11.)These insights informed both our prompt task design and the input to the LLM.</p>
<p>Our Approach</p>
<p>Overview Our pipeline processes submission PDFs and generates structured novelty assessments through three stages (Figure 2): (i) Document Processing extracts key content from submissions, (ii) Related Work Discovery identifies and ranks relevant prior work, and (iii) Novelty Assessment performs comparative analysis to generate evidencebased novelty evaluations.</p>
<p>Stage 1: Document Processing</p>
<p>We extract structured content from submission PDFs using GROBID2 to obtain titles, abstracts, bibliographies, and citation contexts required for subsequent stages.</p>
<p>Stage 2: Related Work Discovery</p>
<p>This stage identifies and ranks related work through a multi-step retrieval pipeline designed to capture both explicitly cited works and potentially relevant uncited research.</p>
<p>Cited Work Processing Bibliography entries are matched against Semantic Scholar to obtain standardized metadata (title, abstract, authors, publication date, venue) for consistent downstream processing.</p>
<p>Uncited Work Discovery To identify relevant work not cited by authors, we generate 5 keyword queries using GPT-4.1 and search Semantic Scholar.Results are filtered to remove exact title matches with the submission (avoiding potential preprints) and papers published after the submission date.</p>
<p>Embedding-based Ranking We generate embeddings for all collected papers using SPECTER v2 (Singh et al., 2022) on concatenated titles and abstracts.Papers are ranked by cosine similarity to the submission's embedding to identify semantically similar work.</p>
<p>LLM-based Reranking</p>
<p>To prioritize papers with conceptual rather than purely semantic similarity, we employ LLM-based reranking (Sun et al., 2023b,a) with prompts emphasizing methodological approaches, novelty claims, and problem statements.We select the top-K (k=20) papers for novelty assessment.</p>
<p>Content Extraction For selected papers, we retrieve PDFs through a hierarchical search across Semantic Scholar, ACL Anthology, and arXiv.Retrieved papers are processed using MinerU (Wang et al., 2024;He et al., 2024) to extract introduction sections, with Nougat OCR (Blecher et al., 2023) as fallback for processing failures.We use these tools for OCRs here as they output more accurate OCRs and we will be using this paper content in the next stage.</p>
<p>Stage 3: Novelty Assessment</p>
<p>We use GPT-4.1 (OpenAI, 2024) for its improved instruction-following capabilities.This stage consists of four sequential steps.</p>
<p>Structured Extraction Processing retrieved papers as raw text creates context optimization challenges that degrade LLM performance.Recent research demonstrates that model performance consistently degrades with increasing input length, even when task complexity remains constant (Hong et al., 2025).This occurs because either overwhelming models with unrelated information reduces accuracy (Zhu et al., 2025;Idahl and Ahmadi, 2025) or insufficient context through heavy truncation limits understanding (Radensky et al., 2025).</p>
<p>We extract six structured components aligned with novelty assessment requirements from each paper's title, abstract, introduction: (i) Methods, (ii) Problems addressed, (iii) Datasets, (iv) Results, (v) Evaluation approaches, and (vi) Novelty Claims.This preserves essential information while reducing context length to mitigate the performance degradation observed with longer, unstructured inputs (Figure 8).</p>
<p>Landscape Analysis Expert reviewers are typically assigned papers within their areas of expertise, providing them with comprehensive domain knowledge of established benchmarks, common techniques, evaluation metrics, and recent devel- Using GPT-4.1, we perform cross-paper synthesis to identify methodological clusters, trace problem evolution over time, map evaluation ecosystems, and establish technical relationships between approaches (Figure 9).The landscape analysis produces a hierarchical organization of the research space with explicit connections between related approaches, competing methods, and complementary techniques.</p>
<p>This structured representation serves as contextual background for subsequent novelty assessment, mimicking the organized domain understanding that expert reviewers naturally possess when evaluating papers in their field.</p>
<p>Novelty Delta Analysis This step performs comparative analysis between the submission and prior work using three inputs: (1) the research landscape, (2) the submission's claimed contributions, and (3) citation contexts-sentences where the submission cites related work.Citation contexts reveal how authors position their contributions, enabling verification of claimed distinctions versus rhetorical framing.</p>
<p>Using GPT-4.1 with prompts informed by our human analysis (Section 3.1), the system implements key reviewer patterns: independent verification of author claims, variable granularity examination of contributions, and identification of gaps in related work discussions (Figures 10 and 11).</p>
<p>Assessment Report Generation The final step generates a concise paragraph long summary that appears similar to actual peer review novelty assessments, enabling direct comparison with humanwritten assessments (Figure 12).</p>
<p>Evaluation</p>
<p>We use the data we annotated before the prompt design stage when studying human patterns.We prompt GPT-4.1 with each human review and its corresponding annotated novelty assessment statements to generate a coherent novelty assessment using the prompt in Figure 13.This step is necessary because novelty-related comments typically appear scattered throughout reviews rather than as unified assessments.Simply concatenating these fragments would introduce stylistic biases during evaluation, as the disjointed human comments would differ markedly from the coherent assessments our system generates.The GPT-4.1 synthesized assessments serve as our evaluation ground truth.</p>
<p>Evaluation Methods</p>
<p>Automated Evaluation Evaluating novelty assessment systems presents significant challenges due to the subjective and knowledge-intensive nature of the task.What constitutes "novel" depends heavily on the evaluator's familiarity with the surrounding research landscape.Even when human reviewers reach similar novelty conclusions, they may arrive at these decisions through different reasoning paths and evidence bases.Given these challenges, we employ an LLMas-Judge framework using our style-normalized human novelty assessments as ground truth.We evaluate AI-generated assessments across four key dimensions using the prompts in Figures 14 and 15 with GPT-4.1 as our Judge:</p>
<p>Novelty Conclusion Alignment: Whether the AI assessment reaches similar novelty conclusions as human reviewers.</p>
<p>Novelty Reasoning Alignment: Whether the AI's reasoning process and justifications align with human reviewer logic.</p>
<p>Prior Work Engagement: Whether the assessment demonstrates adequate engagement with relevant literature rather than superficial analysis.</p>
<p>Depth of Analysis: Whether the assessment provides substantive, detailed evaluation rather than surface-level observations.These dimensions ensure that AI assessments not only align with human judgments but also meet quality standards for thorough, evidence-based novelty evaluation.Our evaluation employs a twostage process to ensure consistency.First, we extract core judgments (key novelty strengths and weaknesses) from human reviews using GPT-4.1 with the prompt in Figure 14.We perform this extraction separately to establish stable reference judgments, as combining extraction with evaluation would risk the LLM identifying different claims across comparisons.In the second stage, we evaluate AI-generated assessments against these preextracted judgments using the prompt in Figure 15.This evaluation quantifies four aspects: (1) judgment similarity, measuring whether the AI identifies the same specific novelty aspects with confidence scores; (2) conclusion alignment, checking whether bottom-line novelty sufficiency verdicts match; (3) prior work engagement, categorized as None, Limited (1-2 citations), or Extensive (3+); and (4) depth of analysis, rated as Surface Level, Moderate (1-2 aspects), or Deep (3+ detailed comparisons).Table 2 reports the resulting alignment scores across these dimensions.</p>
<p>Human Evaluation To validate our automated evaluation, we conduct human evaluation using three PhD students (two third-year, one first-year) specializing in NLP and AI for Science, all with multiple conference publications.The evaluation uses pairwise comparison across the same four dimensions.</p>
<p>Evaluators compare side-by-side novelty assessments from different systems (human ground truth, our approach, or baselines) with each pair representing different system types.We collected 100 total comparisons: 25 overlapping samples per evaluator (for inter-annotator agreement) and 25 unique samples each.</p>
<p>For each dimension, evaluators select: Candidate A wins, Candidate B wins, Tie, or Unclear.A comment box captures specific observations and assumptions, enabling both quantitative preference measurement and qualitative insights.The web interface used for human preference collection can be seen in Figures 6 and 7.</p>
<p>Inter-Rater Reliability Table 6 shows moderate inter-rater agreement (0.493-0.560) with fair kappa scores (0.287-0.368), reflecting the inherent subjectivity in evaluating novelty assessment quality.</p>
<p>Baseline Methods</p>
<p>We compare our approach against three existing systems, adapting each for novelty assessment evaluation.</p>
<p>Scideator (Radensky et al., 2025) Scideator includes a novelty classification module that uses GPT-4o with few-shot examples and task definition to classify ideas as 'novel' or 'not novel'.Originally designed for idea synthesis pipelines where LLMs iteratively refine ideas based on novelty feedback, we adapt it by translating Scideator's "idea" input to "title and abstract" input-recognizing these as the crystallized form of a scientific idea.This preserves Scideator's classification approach while shifting from assessing nascent ideas to evaluating completed scientific contributions.</p>
<p>OpenReviewer (Idahl and Ahmadi, 2025) OpenReviewer generates comprehensive peer reviews using Llama-OpenReviewer-8B, trained on 79,000 expert reviews from top conferences.Since it generates complete reviews rather than targeted novelty assessments, we extract novelty-related content from its outputs using the same LLM-based approach applied to human review normalization as shown in prompt in figure 13.</p>
<p>DeepReviewer (Zhu et al., 2025) DeepReviewer is a multi-stage review framework that combines literature retrieval done with OpenScholar (Asai et al., 2024) with evidence-based argumentation, powered by DeepReviewer-14B trained on structured review annotations.We extract novelty assessments from its outputs using the same LLM-based extraction approach employed for OpenReviewer and human reviews.Notably, DeepReviewer was trained on ICLR 2025 data, which encompasses our entire evaluation dataset-a critical consideration when interpreting its performance.</p>
<p>Results and Analysis</p>
<p>We evaluated each system by comparing its novelty assessments against human novelty assessments as reference.For papers with multiple human reviewers, we also conducted human-vs-human comparisons to establish a baseline.Table 2 presents the overall results.</p>
<p>Overall Performance</p>
<p>Our system significantly outperforms both AI baselines and the human-vs-human baseline across key metrics.For Reasoning Alignment, our system achieves scores that are 44.1 and 35.9 percentage points higher than OpenReviewer (Idahl and Ahmadi, 2025) and DeepReviewer (Zhu et al., 2025), respectively, and 21.4 percentage points above the human baseline.For Conclusion Agreement, our system again leads all three baselines, with the human baseline performing closest at approximately 13 percentage points below our system.Outputs of our pipeline in comparison to the baselines can be seen in Tables 7, 8 and 9.</p>
<p>Sentiment Shift Analysis</p>
<p>We analyze two derived metrics from novelty conclusions.Positive Shift measures assessments changing from neutral/negative to positive sentiment compared to human reference, while Negative Shift measures the opposite direction.Higher Positive Shift indicates overly optimistic assessment, while higher Negative Shift suggests excessive criticism.AI systems generally demonstrate optimistic bias, as evidenced by DeepReviewer's high Positive Shift score.Our system shows lower Positive Shift than DeepReviewer, though OpenReviewer aligns most closely with human rates.For Negative Shift, humans tend to be more critical-a pattern mirrored by OpenReviewer, followed by DeepReviewer.Our approach achieves the lowest Negative   Shift rate.</p>
<p>Depth and Prior Work Engagement</p>
<p>Tables 3 &amp; 4 show our system achieves the highest scores for both dimensions, producing no surfacelevel analyses unlike all baselines.This stems from our specialized multi-step pipeline targeting novelty assessment, while other systems generate complete peer reviews where novelty is a minor component.OpenReviewer performs worst, lacking retrieval and relying on parametric knowledge.Deep-Reviewer uses OpenScholar retrieval but fails at comparative analysis.Human reviewers show high variance, with some engaging extensively while others provide minimal analysis.</p>
<p>Human Evaluation Validation</p>
<p>We conducted human evaluations to validate our LLM-as-Judge evaluation framework.Figures 3 and 4 show the pairwise comparison results.Against OpenReviewer, our system wins 74% of the time.Performance against DeepReviewer and human reviewers is more mixed (39% and 36% win rates), but high tie rates (30% and 41%) indicate many assessments were judged comparable.Loss rates remain low across all comparisons (16-26%).By dimension (Figure 4), Claim Substantiation and Analytical Quality perform best (56% and 55% win rates).Novelty Decision shows the most ties (31%), suggesting different approaches often yield similar conclusions.These patterns align with our automated results, supporting our evaluation approach's validity.</p>
<p>Analysis: Understanding Human Alignment Patterns</p>
<p>Our system's higher agreement scores compared to human-human baselines warrant careful examination.To investigate this, we analyzed papers with multiple human reviewers to understand the sources of disagreement.</p>
<p>Sources of Human Reviewer Variability</p>
<p>Qualitative analysis reveals several factors contributing to reviewer disagreement: Different Evaluation Lenses: Reviewers often focus on different aspects of novelty.In submission Ipe4fMCBXk, half the reviewers emphasized methodological contributions while others focused on application novelty, leading to opposite conclusions from the same paper.Varying Domain Expertise: Reviewers' background knowledge affects assessments.For instance, in a protein design paper, reviewers familiar with the field's history correctly identified prior work on recombination techniques, while others assessed these as novel contributions.Assessment Granularity: Some reviewers provide high-level judgments ("innovative approach") while others focus on specific technical details.This variation in granularity contributes to disagreement even when reviewers might agree on underlying facts.</p>
<p>The Role of Systematic Evaluation</p>
<p>Our system's approach differs from human review in applying consistent evaluation criteria.It evaluates multiple dimensions (methodology, application, prior work) for every paper, maintains uniform depth of analysis across assessments, and applies consistent thresholds for novelty judgments.This systematic approach may explain the alignment patterns: when human reviewers disagree due to focusing on different aspects, our system's comprehensive evaluation can align partially with each perspective.</p>
<p>Component Analysis</p>
<p>Table 5 shows the incremental contribution of each pipeline component.Our human-informed prompt design provides the largest gains (+40.7% reasoning, +46.8% conclusion), reflecting the importance of structured evaluation criteria derived from our human analysis.Structured extraction adds moder-ate improvements (+3.3% reasoning, +4.5% conclusion) but reduces overall computation costs and time by a lot, while landscape analysis contributes minimally (+3.2% reasoning, -0.7% conclusion).</p>
<p>Conclusion</p>
<p>We present a human-informed pipeline for automated novelty assessment in peer review, addressing a critical gap in AI-assisted review systems.</p>
<p>Our approach combines systematic related work retrieval with structured evaluation criteria derived from analysis of expert reviewer patterns.Experimental results demonstrate that our system outperforms existing AI baselines and achieves higher agreement rates than human-human comparisons across key evaluation dimensions.The system produces consistently deep analyses with strong prior work engagement, while human evaluations show our assessments are comparable to human reviews in 41% of cases with low loss rates (14-21%) across all dimensions.These findings highlight both the potential for systematic AI assistance in novelty assessment and the inherent subjectivity in human novelty judgments.Our work provides a foundation for more reliable, transparent novelty evaluation in scientific peer review.</p>
<p>Limitations</p>
<p>Despite achieving strong performance, our system has several important limitations: Evaluation Scope: Our evaluation focuses on computer science papers from ICLR 2025.The system's performance on other scientific domains remains untested and likely requires domain-specific adaptations.</p>
<p>Consistency vs. Diversity: While our analysis shows that systematic evaluation reduces reviewer disagreement, this consistency might eliminate valuable diversity in perspectives.The 35-40% human-human disagreement rate may reflect legitimate differences in expertise and viewpoint rather than mere inconsistency.</p>
<p>Nuanced Novelty: Breakthrough ideas often challenge conventional evaluation criteria.Our system's consistent approach might miss paradigmshifting contributions that human experts would recognize through intuition or deep domain expertise.</p>
<p>Language Scope: Our study evaluates the system only on English-language manuscripts and reviews.As a result, we cannot claim that the ap-proach generalizes to submissions written in other languages or rooted in different academic conventions; assessing cross-lingual performance remains future work.tions (A/B/Tie/Unclear), time spent per evaluation, and comments for flagged cases.</p>
<p>B Output Examples</p>
<p>Output of our pipeline can be seen in Tables 7, 8 and 9.It is quite evident that our system aligns better with the human as compared to the baselines across all four dimensions.The submission overstates its novelty , as the core ideas (token selection, attention sparsity, training-free deployment) are already well-explored , and similar methods (e.g., RazorAttention, PyramidKV, L2 Norm) achieve comparable goals without model changes or fine-tuning.Several highly relevant recent works are omitted from the discussion , and the claims of being the first to balance compression and performance or to preserve long-range dependencies are not substantiated by the literature .</p>
<p>The main technical delta lies in the specific heuristics (CGE, RGL) and their empirical performance, rather than in a conceptual advance .</p>
<p>Reviewers should view IntelLLM as a routine, incremental contribution and may wish to request more comprehensive comparisons and a more accurate positioning within the current research landscape.</p>
<p>Table 7: Full novelty assessments from the human reviewer (reference), the Scideator baseline, and our proposed system.Key phrases are highlighted to show verdict alignment: positive novelty claims , limited/incremental novelty , comparative analysis , and critical issues .</p>
<p>Research Paper Information Extraction Prompt</p>
<p>You are tasked with extracting key information from a research paper for building a knowledge representation.Paper title: {title} Based on the paper content provided below, extract the following information: -"methods": [List of methods/approaches proposed in the paper], -"problems": [List of problems the paper addresses], -"datasets": [List of datasets used for evaluation], -"metrics": [List of evaluation metrics used], -"results": [List of objects with 'metric' and 'value' fields representing key quantitative results], -"novelty_claims":  Overall, the submission's primary strengths lie in evaluation rigor and empirical findings, while its conceptual contributions represent a natural progression of the field rather than a fundamental shift .Novelty Delta Analysis for Reviewer Support -Part 2 5. CRITICAL ASSESSMENT CONSIDERATIONS -Identify aspects where claimed novelty may be overstated -Analyze whether authors' characterizations of their own novelty align with evidence -Consider whether empirical improvements might result from factors other than claimed innovations -Assess whether terminology differences might mask conceptual similarities -Identify instances where "extensions" might be routine adaptations -Note: Frame these as considerations rather than definitive judgments 6. RELATED WORK CONSIDERATIONS -Identify potentially relevant work not addressed in the submission -Highlight areas where additional comparisons are necessary -Note incomplete or potentially misleading characterizations of prior work -Identify when claimed "limitations" of prior work may be exaggerated -Compare how authors cite specific works versus how they actually relate -Note: Present these as information that might help complete the picture 7. KEY OBSERVATION SUMMARY -Highlight the most significant independently verified differences from prior work -Summarize the main relationships to existing research -Identify which claimed contributions have the strongest and weakest differentiation -Note the most important discrepancies between author characterizations and independent assessment -Note: Frame as observations to inform the reviewer's independent judgment ## Evidence Standards For each observation, provide: -Specific references to prior work -Clear distinction between author claims and independently verified differences -Explicit identification of similarities and differences based on technical details -Assessment of whether differences appear substantive or superficial -Analysis of accuracy in how authors characterize related work ## Example Format for Citation Analysis "For [Paper X], the authors characterize it as 'limited to simple datasets' and claim their work 'extends X to complex scenarios.'The citation sentences appear in the following contexts: -'Unlike X, which only works on simple datasets, our approach handles complex scenarios' (Introduction) -'X proposed the basic framework, but did not address challenge Y' (Related Work) Independent analysis suggests that Paper X actually did address complex scenarios in Section 3.2, though using different terminology.The authors' characterization appears to understate X's capabilities to emphasize their contribution.The actual primary difference appears to be [specific technical difference] rather than the complexity of supported scenarios."</p>
<p>Remember that your role is to provide objective analysis that helps reviewers make informed judgments about novelty.Carefully examine both what authors explicitly claim and how they implicitly position their work through their characterizations of prior research.</p>
<p>Figure 1 :
1
Figure 1: Comparison of a surface level human (Top) vs. LLM-written novelty assessment (Bottom).</p>
<p>Figure 2 :
2
Figure 2: Automated novelty assessment pipeline.The system processes manuscripts through three stages: (1) Document Processing extracts content using GROBID, (2) Related Work Discovery identifies and ranks relevant papers via embedding similarity and LLM reranking, and (3) Novelty Assessment performs structured analysis to generate evidence-based novelty evaluations.</p>
<p>Figure 3 :
3
Figure 3: Overall performance comparison between our system and three baseline systems based on human evaluation (n values indicates number of comparisons)</p>
<p>Figure 4 :
4
Figure 4: Performance breakdown across evaluation categories, aggregated across all baseline comparisons.</p>
<p>Figure 5 :
5
Figure 5: Distribution of the number of reviews per paper.Most papers received 1 to 4 reviews.</p>
<p>Figure 6 :
6
Figure 6: Screenshot of the custom-built interface used for human evaluation.Annotators compared AI-generated and human-written novelty assessments across multiple dimensions, including reasoning depth, prior work engagement, and conclusion alignment.</p>
<p>Figure 7 :
7
Figure 7: Screenshot (2) of the custom-built interface used for human evaluation.Annotators compared AI-generated and human-written novelty assessments across multiple dimensions, including reasoning depth, prior work engagement, and conclusion alignment.</p>
<p>Figure 10: Novelty Delta Analysis for Reviewer Support -Part 1</p>
<p>Figure 11 :
11
Figure 11: Novelty Delta Analysis for Reviewer Support -Part 2</p>
<p>Table 1 :
1
Distribution of papers and reviews with novelty discussions by ICLR 2025 decision outcomes opments.To approximate this contextual foundation, we incorporate a landscape analysis step that systematically organizes the previously extracted structured components from retrieved related work.
DecisionPapers Reviews Words/rev Rev/paperNo Decision / Withdrawn5111010022.16Reject811959192.41Accept (Poster)451029622.27Accept (Spotlight)4109972.50Accept (Oral)1211822.00Total1824199592.30</p>
<p>Table 2 :
2
Summary of Reasoning Alignment, Conclusion Agreement, Positive Shift, and Negative Shift Metrics
SystemReasoning Alignment (%↑) Conclusion Agreement (%↑) Positive Shift (%↓) Negative Shift (%↓)OpenReviewer (Idahl and Ahmadi, 2025)42.4 ± 0.3946.8 ± 0.716.3 ± 0.2715.3 ± 0.40DeepReviewer (Zhu et al., 2025)50.6 ± 0.6751.5 ± 1.2421.7 ± 1.899.1 ± 0.00Human vs. Human65.1 ± 1.0562.8 ± 0.406.7 ± 0.7915.0 ± 0.40Scideator (Radensky et al., 2025)23.7 ± 0.0022.4 ± 0.000.0 ± 0.0020.5 ± 0.00Ours86.5 ± 0.2075.3 ± 0.8516.3 ± 1.283.0 ± 0.43</p>
<p>Table 3 :
3
Reasoning Depth Distribution (Percentages)
SystemSurface-Level (%) Moderate (%) Deep (%)OpenReviewer67.431.31.2DeepReviewer43.456.60.0Human vs. Human22.366.211.5Scideator44.954.50.6Ours0.047.952.1SystemNone (%) Limited (%) Extensive (%)OpenReviewer39.953.17.0DeepReviewer24.775.30.0Human vs. Human19.665.215.2Scideator0.075.924.1Ours0.039.160.9Table 4: Prior Work Engagement Distribution (Percent-ages)</p>
<p>Table 5 :
5
Component Analysis: Incremental Contribution of Pipeline Components</p>
<p>Table 6 :
6
Inter-Rater Reliability Metrics Across Categories
CategoryAgreement Kappa ComparisonsNovelty Reasoning Alignment0.5200.34175Novelty Decision Alignment0.5330.34675Claim Substantiation0.4930.28775Analytical Quality0.5600.36875</p>
<p>[Claims about what is novel in this work] Be precise and specific.
Human (Reference)DeepReviewer (Baseline)Ours (Proposed)The paper "DuRND: RewardingThis paper proposesDuRND isfrom Novelty to Contribution fora novel framework, Dual Random,a direct extension of Random Network Dis-,Reinforcement Learning via DualNetworks Distillation (DuRND)tillation (RND)Random Networks Distillation" proposesto address the exploration-exploitation dilemma inintroducing a dual random network architecture to compute both noveltyan extension to classical RNDreinforcement learning by(exploration) and "contribution" (exploitation)by introducing two distinct randomcombining novelty-basedrewards, with a dynamic schedulingnetwork modules-one for statesexploration and value-basedmechanism that shifts emphasis over time.deemed "successful" and anotherexploitation in a unified framework.While the use of random networks for bothfor states associated with "failure."The novelty and contributionsignals isThis innovation allows for therewards in DuRND are based ona novel implementation detail,derivation of both a "novelty" and a "contribution" reward signal, striking a balance between ex-ploratory and exploitative behav-ior While the additional novelty introduced by DuRND is incremental com-pared to classical RND I still believe the contribution is valuable and fills a gap in the current literature. , .prediction errors from random network modules, which may not capture complex state relationships in certain environments, and the reliance on prediction error as a proxy for novelty might be insufficient where the state space has a complex structure or the prediction error does not align with actual information gain. The approach introduces an interesting combination of nov-elty and contribution rewards but relies on computationally efficient heuristics that might not capture the true novelty in complex envi-ronments While the framework demonstrates effectiveness and low computational overhead,.the underlying concepts-combining explo-ration and exploitation signals and dynam-ically adjusting their balance-are well-established in the literature (e.g., SORS, ROSA, Sibling Rivalry, SERENE). The authors accurately position DuRND as a low-overhead, scalable method, but somewhat overstate its conceptual novelty and the dichotomy between prior approaches Empirical results show improved performance . on standard sparse-reward benchmarks, though such gains may be attributable to implemen-. tation choices rather than fundamental inno-vation Overall, DuRND offers a practical and well-engineered architectural , tweak to RND with its main contribution beingthe linear adjustment of weights wards is a heuristic that may not for novelty and contribution re-.tual breakthrough sic reward computation, rather than a concep-a simple, dual-network design for joint intrin-.be optimal or universally applica-bleOverall,the originality lies in the integra-,tion of novelty and value-basedsignalsbut there remainsignificant limitations in the ro-bustness and generalizability ofthe contributiondue to dependency on specificproxy measures and heuristicschedules.Paper content:{abstract}{introduction}Figure 8: Research Paper Information Extraction Prompt</p>
<p>Table 8 :
8
Full novelty assessments from the human reviewer (reference), the DeepReviewer baseline, and our proposed system for the DuRND paper.Key phrases are highlighted to show verdict alignment: novel/valuable aspects , limited/incremental novelty , technical details , and overstated claims .
Human (Reference)OpenReviewer (Baseline)Ours (Proposed)This paper introduces a method forThis paper introducesThis assessment finds that the submissiongenerating image inputs to Visiona novel method of embedding hid-addresses indirect, cross-modal promptLanguage Models (VLMs) thatden meta-instructions in imagesinjection in Visual Language Models (VLMs)embed "meta-instructions," enabling the model to respond to any user query appended to the image with a specified "spin," such as a particular sentiment or language. While the concept of providing meta-instructions through image inputs is not entirely novel-prior work, no-tably Bailey et al. (2023), has ex-plored related prompt injection at-tacks via images this paper offers the most thorough and comprehensive treatment of the subject to date. The general algorithm employed, which involves creating a dataset of input-output pairs and training an image via projected gradient descent to maximize the likelihood over this dataset, closely aligns with the "Behavior Matching" approach described by Bailey et al. (2023) the methodological contribution is incremental rather than fundamen-As such,. .as a way to attack visual language models, which is distinct from traditional jailbreak-ing or adversarial example attacks While the approach is new, the paper could provide more insight into how the proposed attacks compare to existing forms of adversarial attacks on VLMs, highlighting the unique aspects of meta-instruction attacks..by embedding hidden meta-instructions in images, aiming to steer model outputs while preserving image semantics. The work is most closely related to recent studies on ad-versarial image prompting (e.g., Qi et al. 2024, Bagdasaryan et al. 2023) but distinguishes itself through more systematic optimization for semantic preservation and a broader range of meta-instructions beyond jailbreaking The main substantive contributions are a rigorous, multi-metric evaluation of attack effectiveness and semantic preservation and empirical evidence that image-based meta-instructions can be more effective than explicit text prompts. However, the assessment notes that the conceptual advances are incremental as the core idea of cross-modal prompt injection and semantic preservation has been explored in prior work and overstated frame VLM users as victims) are somewhat some novelty claims (e.g., being the first to, . , , , .tally newHowever, the paper distinguishesitself throughits in-depth analysis and experi-,mental resultsparticularly the study of semanticchanges in images resulting fromvarious attacks, with a focus on howmeta-instruction attacks canpreserve the original imagemeaning. The transferabilityexperiments and the breadth ofevaluation across multiple VLMsfurther strengthen the contribution.Overall,the novelty of this work lies less,in the method itself and more inthe scope, depth, and clarity of itsempirical investigationas well as the new insights itprovides into the vulnerabilities ofVLMs to image-based promptinjection. The paper would benefitfrom more explicitlyacknowledging the overlap withprior methods and more clearlyarticulating how its results advancethe understanding of this threatmodel.</p>
<p>Table 9 :
9
Full novelty assessments from the human reviewer (reference), the OpenReviewer baseline, and our proposed system for the Meta-Instructions in VLMs paper.Key phrases are highlighted to show verdict alignment: novel/strength claims , limited/incremental novelty , prior work comparison , and overstated claims .
Research Landscape Analysis# Research Landscape Analysis## TaskAnalyze the collection of research papers provided below to create acomprehensive map of the research landscape they represent. The submissionpaper is the focus of our analysis, and the related papers provide context.## Input FormatYou will be provided with structured information extracted from multipleresearch papers including:-A submission paper that is the focus of our analysis-Multiple related papers that form the research contextEach paper contains:-methods: List of methods/approaches proposed-problems: List of problems addressed-datasets: List of datasets used-metrics: List of evaluation metrics-results: Key quantitative results-novelty_claims: Claims about what is novel in the work## Output FormatProvide a comprehensive analysis with the following sections:1. METHODOLOGICAL LANDSCAPE-Identify and describe the main methodological approaches across the papers-Group similar or related methods into clusters-Highlight methodological trends or patterns-Describe relationships between different methodological approaches2. PROBLEM SPACE MAPPING-Identify the key problems being addressed across the papers-Analyze how different papers approach similar problems-Highlight patterns in problem formulation3. EVALUATION LANDSCAPE-Analyze the common datasets and evaluation methods-Identify patterns in how performance is measured-Compare evaluation approaches across papers4. RESEARCH CLUSTERS-Identify groups of papers that appear closely related-Describe the key characteristics of each cluster-Analyze relationships between clusters5. TECHNICAL EVOLUTION-Identify any visible progression or evolution of ideas-Highlight building blocks and their extensions-Note any competing or complementary approaches## Example Output FormatMETHODOLOGICAL LANDSCAPE-Cluster 1: [Description of similar methods across papers]-Papers X, Y, Z employ transformer-based approaches with variations in...-These methods share characteristics such as...-They differ primarily in...PROBLEM SPACE MAPPING-Problem Area 1: [Description of a common problem addressed]-Papers A, B, C all address this problem but differ in...-The problem is formulated differently in Paper D which focuses on...... [additional sections] ...Ensure your analysis is comprehensive, identifying significant patternsand relationships across the collection of papers.## Papers:{papers}Figure 9: Research Landscape Analysis Prompt
https://github.com/kermitt2/grobid
A Human Evaluation Protocol: Novelty Assessment ComparisonA.1 Task DesignWe conducted a comparative evaluation where human evaluators assessed the quality of AIgenerated novelty assessments against expertwritten reference assessments.Each evaluator pared pairs of AI-generated assessments (labeled A and B) to a human expert's gold-standard novelty review of the same research paper.A.2 Evaluation FrameworkA.2.1 Materials ProvidedFor each evaluation, evaluators received: (1) an expert-written gold-standard novelty review as reference, (2) two novelty assessments (A and B) with system identities hidden.A.2.2 Evaluation DimensionsEvaluators assessed each pair across four dimensions:1. Reasoning Alignment: For each dimension, evaluators selected one of four options: A wins, B wins, Tie (both equally good/poor), or Unclear (cannot determine).A.3 Evaluation GuidelinesA.3.1 Instructions for EvaluatorsEvaluators were instructed to read the reference assessment thoroughly before evaluating A and B, evaluate each dimension independently, and base judgments on substantive content rather than stylistic differences.They allocated 4-7 minutes per example to ensure thorough evaluation and flagged ambiguous cases with explanatory comments when necessary.A.3.2 Evaluation FocusEvaluators were instructed to prioritize substance and accuracy of novelty reasoning, alignment with reference judgments (particularly for Dimensions 1-2), quality and depth of technical analysis (particularly for Dimensions 3-4), and specific evidence and citations supporting claims.They were instructed to disregard writing style, grammar, or formatting differences; suggestions for paper improvement unrelated to novelty; minor phrasing variations with equivalent meaning; and length differences if content quality was comparable.A.4 Implementation DetailsA.4.1 Evaluation PlatformThe evaluation was conducted through a custom web interface presenting materials in a standardized format (see Figures6 and 7).Each evaluator received a unique evaluator ID, 50 randomly assigned paper-assessment pairs, and the ability to save progress and flag unclear cases.A.4.2 Quality ControlWe calculated inter-evaluator agreement using Cohen's kappa reported in Table6.A.4.3 Data CollectionCompleted evaluations were submitted as structured JSON files containing dimension-wise selec-Reviewer Summary PromptSummarize the following assessment in 5 sentences for a reviewer reviewing at an AI conference.## Delta Assessment {novelty_assessment}Figure12: Reviewer Summary Prompt Novelty Assessment Normalization Prompt I'll provide you with a novelty assessment extracted from an academic peer review, along with the full review for context.Please reformat the novelty assessment into a standardized paragraph that begins with a brief description of the paper's contribution before analyzing its novelty.Example of desired format: "This paper presents a method for neural network compression using knowledge distillation with a focus on mobile applications.The approach has limited novelty, as it largely builds upon existing techniques in the literature.While the authors claim their technique is the first to combine layerwise distillation with quantization-aware training, similar combinations have been explored in prior work bySmith et al. (2022)andJones et al. (2023).The main contribution appears to be a specific implementation detail in how gradient flows are managed during the distillation process, but this incremental advance does not significantly push the boundaries of the field.The paper would benefit from more clearly articulating the specific differences from existing approaches to better establish its contribution."Full review (for context): {full_review}Extracted novelty assessment to be reformatted: {novelty_statements} Important guidelines: 1. Begin with a clear description of what the paper presents/proposes (drawn from the full review if needed) 2. Create a cohesive paragraph that flows from describing the contribution to analyzing its novelty 3. Maintain all novelty claims and critiques from the original assessment 4. Preserve references to prior work and comparisons 5. Keep the reviewer's judgment of novelty level 6.Incorporate relevant context from the full review to provide a complete picture of the novelty assessment 7. Follow the structure of the example paragraph: description first, then novelty analysis 8. Preserve all critical analysis regarding limitations or strengths of novelty claims Provide the reformatted novelty assessment: For each judgment, explain why it's considered a core novelty assessment.Provide rationale for your selection of these specific judgments.Reviewer Novelty Evaluation PromptCompare reviewer assessment against reference using these core judgments: Core Judgments: {extracted_core_judgments} Reference: {reference_assessment} Reviewer: {reviewer_assessment} Evaluate three dimensions:1. JUDGMENT SIMILARITY: Do they identify same novelty strengths/weaknesses? -For each core judgment, find corresponding judgment in reviewer assessment -Assess similarity and provide detailed explanation of alignment/differences -Include confidence score for each comparison -If the core judgement is referring to a very specific aspect of the methodology and the reviewer assessment does not mention it, then the core judgment is not similar to the reviewer assessment.DEPTH_OF_ANALYSIS:-Assesses how deeply specific novelty aspects are compared to prior work (SURFACE LEVEL: vague; MODERATE: 1 to 2 aspects; DEEP: 3+ or highly detailed comparisons)Provide explanations for all assessments to support reasoning.
A billion-dollar donation: estimating the cost of researchers' time spent on peer review. Balazs Aczel, Barnabas Szaszi, Alex O Holcombe, 10.1186/s41073-021-00118-2Research Integrity and Peer Review. 61142021</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, arXiv:2402.017882025Preprint</p>
<p>Openscholar: Synthesizing scientific literature with retrievalaugmented lms. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, arXiv:2411.141992024Preprint</p>
<p>Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment. Alina Beygelzimer, Yann N Dauphin, Percy Liang, Jennifer Wortman Vaughan, arXiv:2306.032622023Preprint</p>
<p>Nougat: Neural optical understanding for academic documents. Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic, arXiv:2308.134182023Preprint</p>
<p>Treereview: A dynamic tree of questions framework for deep and efficient llm-based scientific peer review. Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Zhijiang Guo, Ngai Wong, ArXiv, abs/2506.076422025</p>
<p>Autorev: Automatic peer review system for academic research papers. Maitreya Prafulla Chitale, Ketaki Mangesh Shetye, Harshit Gupta, Manav Chaudhary, Vasudeva Varma, arXiv:2505.143762025arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024Preprint</p>
<p>Streamlining the review process: Ai-generated annotations in research manuscripts. Oscar Díaz, Xabier Garmendia, Juanan Pereira, 10.48550/arXiv.2412.002812024Preprint available on arXiv</p>
<p>Understanding peer review of software engineering papers. Neil A Ernst, Jeffrey C Carver, Daniel Méndez, Marco Torchiano, Empirical Software Engineering. 262020</p>
<p>Conghui He, Wei Li, Zhenjiang Jin, Chao Xu, Bin Wang, Dahua Lin, arXiv:2407.13773Opendatalab: Empowering general artificial intelligence with open datasets. 2024arXiv preprint</p>
<p>Context rot: How increasing input tokens impacts llm performance. Kelly Hong, Anton Troynikov, Jeff Huber, 2025ChromaTechnical report</p>
<p>The ability of different peer review procedures to flag problematic publications. P J M Serge, Willem Horbach, Halffman, Scientometrics. 1182018</p>
<p>OpenReviewer: A specialized large language model for generating critical scientific paper reviews. Maximilian Idahl, Zahra Ahmadi, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations). the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations)Albuquerque, New MexicoAssociation for Computational Linguistics2025</p>
<p>Thamar Solorio, and 5 others. 2024. What can natural language processing. Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Margot Mausam, Aurélie Mieskes, Danish Névéol, Lizhen Pruthi, Roy Qu, Noah A Schwartz, Smith, arXiv:2405.06563Preprint</p>
<p>The rate of growth in scientific publication and the decline in coverage provided by. Peder Olesen, Larsen , Markus Ins, 10.1007/s11192-010-0202-zScience Citation Index. Scientometrics. 8432010</p>
<p>Llm inference serving: Survey of recent advances and opportunities. Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari, IEEE High Performance Extreme Computing Conference (HPEC). 2024a. 2024</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, arXiv:2410.13185Tian Feng, and Lidong Bing. 2024b. Chain of ideas: Revolutionizing research via novel idea development with llm agents. Preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards fully automated open-ended scientific discovery. 2024arXiv preprint</p>
<p>The feasibility of topic-based watermarking on academic peer reviews. Alexander Nemecek, Yuzhou Jiang, Erman Ayday, arXiv:2505.216362025Preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135OpenAI. 2024. Gpt-4.1Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguisticsjune 2024 version. 2002Large language model</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342025Preprint</p>
<p>A review on large language models: Architectures, applications, taxonomies, open issues and challenges. IEEE Access. Mohaimenul Azam Khan Raiaan, Md. Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad Fahad, Sadman Jashim Sakib, Most. Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami Azam122024</p>
<p>Simra Shahid, Marissa Radensky, Raymond Fok, Pao Siangliulue, Tom Daniel S Weld, Hope, arXiv:2506.22026Literature-grounded novelty assessment of scientific ideas. 2025arXiv preprint</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Instruction distillation makes large language models efficient zero-shot rankers. Weiwei Sun, Zheng Chen, Xinyu Ma, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, ArXiv, abs/2311.015552023a</p>
<p>Is ChatGPT good at search? investigating large language models as re-ranking agents. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, 10.18653/v1/2023.emnlp-main.923Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023b</p>
<p>Mineru: An open-source solution for precise document content extraction. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, Conghui He, arXiv:2409.188392024Preprint</p>
<p>Kag-thinker: Interactive thinking and deep reasoning in llms via knowledge-augmented generation. Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, Qiwei Wang, Xiaorui Wang, Xinkai Du, Yangyang Hou, Yu Ao, Zhaoyang Wang, Zhengke Gui, Zhiying Yi, Zhongpu Bo, Haofen Wang, Huajun Chen, ArXiv, abs/2506.177282025</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.096752020Preprint</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>DeepReview: Improving LLM-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 63rd Annual Meeting of the Association for Computational LinguisticsVienna, AustriaAssociation for Computational Linguistics20251</p>            </div>
        </div>

    </div>
</body>
</html>
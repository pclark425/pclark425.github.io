<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8378 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8378</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8378</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b" target="_blank">What Algorithms can Transformers Learn? A Study in Length Generalization</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task and provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.</p>
                <p><strong>Paper Abstract:</strong> Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming language designed for the computational model of a Transformer -- and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). On the theoretical side, we give a simple example where the"min-degree-interpolator"model of learning from Abbe et al. (2023) does not correctly predict Transformers' out-of-distribution behavior, but our conjecture does. Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8378.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8378.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Addition (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decimal multi-digit addition (standard formatting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multi-digit decimal addition trained with standard prompt/answer formatting on decoder-only Transformers trained from scratch; used as a canonical hard arithmetic task that typically fails to length-generalize under this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only causal Transformer (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard autoregressive decoder-only Transformer with learned positional embeddings; trained from scratch on task-specific synthetic data with context packing and random shifts; greedy sampling at inference; model size not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit decimal addition (variable-length), tested on longer lengths than training; evaluations include 'easy' carry (random summands) and 'hard' carry (maximal carry-chains).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Naive addition requires index-arithmetic (precise positional lookups) and non-causal carry propagation, operations that are unnatural in a single forward Transformer pass and not representable compactly in RASP-L without interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Empirical training/evaluation experiments comparing standard formatting vs. reformattings (index-hints, reverse-order), and varied training distributions (balanced-carry sampling); no linear-probe or neuron-level mechanistic probing reported for baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline: models trained on standard-format addition show little-to-no length generalization; test performance on examples 5+ tokens longer than training is at or near random chance (no numeric EM reported beyond 'no generalization observed').</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails to generalize to longer lengths; sensitive to carry-chains (especially 'hard' carry cases); failure attributed to inability to do global index-arithmetic and to propagate carry non-causally in one forward pass.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Comparison experiments: lack of generalization contrasted with success after targeted formatting interventions (index hints, reverse-order) which reduce required index arithmetic and make carry causal; RASP-L analysis shows standard addition needs long/non-uniform program in RASP-L.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Although a standard-order RASP-L program exists, it is much longer; length generalization can be enabled via training-data diversity and formatting (so failure is not absolute), showing that negative baseline is surmountable with interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Algorithms can Transformers Learn? A Study in Length Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8378.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8378.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Addition (index-hints + reverse)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Addition with index-hints and reverse-order outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reformatted addition where each digit is annotated with index-hints (tokens that allow induction-style lookup) and the output is reversed (least-significant digit first), enabling short RASP-L implementations and strong length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only causal Transformer (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same experimental model family as above; trained on reformatted addition data with index-hints and reverse-order targets, with context packing and random shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit decimal addition with per-digit index hints and reverse-order output; evaluated on both 'easy' and 'hard' carry distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Mechanistically framed as: index-hints allow digit correspondence via induction-head-like behavior (no index arithmetic), and reverse-order makes carry propagation causal by referencing previous output digit as scratchpad, enabling a single-pass Transformer implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Intervention via prompt/output reformatting (index-hints, reverse-order); training with balanced carry sampling for diversity to expose long carry-chains; comparison of forward vs reverse formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reverse addition with index hints: strong length generalization — many runs generalize perfectly to 50-digit test once training max length >~40 (Figure 5b). Forward addition with index hints: generalizes on 'easy' carry but fails on 'hard' carry unless trained with balanced-carry sampling; after balanced-carry training, forward addition can also length-generalize (claimed first strong generalization on decimal addition trained from scratch).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Forward-format still struggles on 'hard' carry when training lacks diverse long carry examples; success depends on training diversity and formatting choices.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: dramatic performance improvement when using index-hints + reverse output compared to baseline; faster optimization (Figure 6a) for reverse addition; existence of short RASP-L program (Listing 9) for reverse addition demonstrates representability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Forward-order addition is implementable but requires a much longer RASP-L program (Listing 10) and more diverse training (balanced carry) to learn; thus format helps but does not fully eliminate algorithmic difficulty without sufficient training diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Algorithms can Transformers Learn? A Study in Length Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8378.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8378.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parity (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bitwise parity (no scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Computing parity of a bit sequence (XOR of bits) presented as a next-token task without intermediate scratchpad steps; considered a canonical hard boolean/sequential task for Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only causal Transformer (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as above; trained on parity examples sampled uniformly up to a max training length, with context packing and random shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity computation (binary sequence parity), multi-length sequences, tested out-of-distribution at longer lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Natural algorithm is sequential finite-state iteration (running parity state), which cannot be implemented in a single constant-depth parallel Transformer pass without unrolling or scratchpad; computing parity in parallel would require a global sum and stable numeric parity detection, which is unnatural for Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Empirical training/evaluation; attempted augmentation with scratchpads (see separate entry); no fine-grained neuron probing reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline: no length generalization observed; test performance does not exceed random chance when evaluated 5+ tokens longer than training.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails to generalize; cannot stably compute parity for arbitrarily long sequences; may fail to even fit training set beyond some lengths (as noted in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Argument based on RASP-L expressivity and causal masking: parity requires n sequential steps or a numerically unstable global sum; empirical failure corroborates theoretical limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>When provided a carefully designed scratchpad that writes intermediate parity state per symbol (unrolling the loop into the context), Transformers can learn to generalize parity (see parity with scratchpad), showing the task is learnable with suitable external state encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Algorithms can Transformers Learn? A Study in Length Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8378.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8378.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parity (with scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parity with index-hints and iterative scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parity reformatted to include index-hints and a scratchpad that records running parity per 1-bit encountered; this externalizes the sequential state and enables strong length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only causal Transformer (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same experimental family; trained on parity examples augmented with index-hints and a scratchpad that logs running parity symbols (+/-) associated with indices.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity computation over binary sequences, using scratchpad tokens representing intermediate parity states; evaluated on longer-length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Scratchpad encodes the sequential state externally so that each autoregressive next-token prediction only needs to perform one-step updates; this converts an n-step internal loop into n single-step next-token predictions representable by the Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Intervention via prompt/scratchpad design; training experiments varying max training length to observe generalization thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Parity with scratchpad: many runs trained up to length 30 generalize perfectly to length 50; when training length reaches 40, all runs achieve perfect length generalization on length-50 tests (Figure 5c).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Performance sensitive to scratchpad formatting; poorly designed scratchpads may not help or can slow convergence; success requires proper index hints and scratchpad layout.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: strong length generalization achieved only when scratchpad unrolls sequential state; training speed (Figure 6b) correlates with scratchpad quality; RASP-L analysis explains why scratchpad reduces RASP-L program complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not all scratchpads help — some human-intuitive scratchpads (e.g., outputting decimal counts for mode) made tasks harder; scratchpad design must match Transformer's representational strengths (e.g., avoid introducing complex index arithmetic or decimal parsing).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Algorithms can Transformers Learn? A Study in Length Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8378.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8378.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Counting task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Count from a to b inclusive (digit generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence-generation task where model must output the sequence of integers between two given endpoints a and b; used as an example of an algorithmic task that Transformers can easily learn and length-generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only causal Transformer (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above; trained on packed contexts containing multiple count examples sampled across lengths up to a max train length (e.g., up to 50).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Counting sequences (incrementing integers between endpoints), variable output length.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Represented by a short RASP-L program: model finds SoS token, reads a and b, then autoregressively outputs next number by reading latest output token and incrementing (x+1) until b then EoS; implemented via simple attention and tokenwise ops rather than complex index arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Empirical training/evaluation; Appendix C contains supporting evidence suggesting models implement the described algorithm (no detailed neuron-level probes in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Near-perfect length generalization: models trained on counts up to length 50 generalize near perfectly to length 100 (Figure 1b); EM nearly 100% for sufficiently large training max length.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Requires diversity in training lengths (uniform sampling 1..max) — training on fixed-length examples prevents generalization; otherwise robust.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Short RASP-L program exists (Figure 2) mapping directly to a single-transformer-layer-per-line compile; empirical success and faster convergence consistent with RASP-L simplicity principle; Appendix C shows evidence that trained models implement algorithmic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>If positional embeddings or training procedures differ (e.g., not packing/shifting contexts) or training data lacks diversity, generalization deteriorates; RASP-L reasoning suggests representation depends on allowed token/index ops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Algorithms can Transformers Learn? A Study in Length Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8378.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8378.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy unique / Induction heads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copy task with unique tokens (using induction-head mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Copying the prompt exactly in the output when prompt tokens are guaranteed unique; task is solved by induction-head-like attention which locates prior occurrence of token and predicts the following token.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only causal Transformer (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same family; trained on prompt->copy examples where input tokens are unique within each example; learned positional embeddings with packed contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Not strictly arithmetic, but a sequential token-copying task that demonstrates an attention-based algorithmic mechanism relevant to arithmetic index lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Induction heads: attention pattern that finds previous occurrence of the current query token, retrieves the token that followed it, and uses that as the prediction; this is naturally expressible in RASP-L via the induct helper and easy for Transformers to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Empirical training/evaluation; RASP-L standard-library includes an induct implementation used to reason about mechanism; no neuron-level probing reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Models trained on unique-token copy up to length 40 generalize perfectly to length 50 (Figure 3b); EM near 100% for sufficient train max length.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When tokens repeat, induction-head strategy breaks and models fail to generalize unless alternative indexing or scratchpad mechanisms are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong empirical generalization consistent with induction-head functioning; RASP-L provides a one-line implementation (Listing 6) showing representability; prior work (Olsson et al., 2022) corroborates induction-head emergence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Copy with repeating tokens is hard and fails to generalize because it requires precise index-arithmetic that induction heads cannot disambiguate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Algorithms can Transformers Learn? A Study in Length Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8378.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8378.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RASP-L / RASP-Generalization Conjecture</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RASP-L language and the RASP-Generalization Conjecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RASP-L is a restricted RASP variant designed to capture algorithms easy for Transformers to represent; the RASP-Generalization Conjecture posits Transformers will length-generalize when the true next-token function has a short RASP-L program and training data prevents shorter spurious programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (theoretical/analysis tool for Transformer representability)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RASP-L programs correspond to causal Transformer computations (one RASP-L line ≈ one Transformer layer); RASP-L restricts index arithmetic and numeric ranges to reflect learnable, uniform transformer-implementable operations.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Provides representational analysis for arithmetic-like tasks (addition, parity, counting) by expressing whether algorithms are short/uniform in RASP-L.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Frames mechanisms in terms of attention-based operations (kqv causal attention), tokenwise maps, min/max/mean aggregations, and restricted index operations (order, successor/predecessor) rather than arbitrary index arithmetic; explains when induction heads, scratchpads, or attention searches suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used as an explanatory and predictive tool: write RASP-L programs for tasks, compile to Transformer weights in prior works, and design interventions (index hints, scratchpads, reverse-output) to reduce RASP-L complexity and empirically test effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Phenomenological claim: tasks with short RASP-L programs empirically show strong length generalization and faster in-distribution optimization; specific task metrics reported in paper (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>RASP-L is not complete: some transformer-representable numerical algorithms are poorly captured; RASP-L restrictions (no index arithmetic, bounded ints) may exclude some valid transformer solutions; finding minimum RASP-L program is intractable in general.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical alignment across tasks: tasks with short RASP-L solutions (count, mode, copy-unique, sort) generalize well; tasks without short RASP-L solutions (addition, parity, copy-repeat) fail unless reformatted; experiments manipulating RASP-L complexity (scratchpads, hints) change generalization predictably.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>There are representable functions not captured succinctly by RASP-L; the conjecture is phenomenological (predictive) rather than a mechanistic proof that trained weights match compiled RASP-L weights; limits in capturing numeric/float operations and some attention behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Algorithms can Transformers Learn? A Study in Length Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Thinking like transformers <em>(Rating: 2)</em></li>
                <li>Tracr: Compiled transformers as a laboratory for interpretability <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>In-context learning and induction heads <em>(Rating: 2)</em></li>
                <li>Generalization on the unseen, logic reasoning and degree curriculum <em>(Rating: 1)</em></li>
                <li>Length generalization in arithmetic transformers <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8378",
    "paper_id": "paper-1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Addition (baseline)",
            "name_full": "Decimal multi-digit addition (standard formatting)",
            "brief_description": "Multi-digit decimal addition trained with standard prompt/answer formatting on decoder-only Transformers trained from scratch; used as a canonical hard arithmetic task that typically fails to length-generalize under this setup.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only causal Transformer (trained from scratch)",
            "model_description": "Standard autoregressive decoder-only Transformer with learned positional embeddings; trained from scratch on task-specific synthetic data with context packing and random shifts; greedy sampling at inference; model size not specified.",
            "arithmetic_task_type": "Multi-digit decimal addition (variable-length), tested on longer lengths than training; evaluations include 'easy' carry (random summands) and 'hard' carry (maximal carry-chains).",
            "mechanism_or_representation": "Naive addition requires index-arithmetic (precise positional lookups) and non-causal carry propagation, operations that are unnatural in a single forward Transformer pass and not representable compactly in RASP-L without interventions.",
            "probing_or_intervention_method": "Empirical training/evaluation experiments comparing standard formatting vs. reformattings (index-hints, reverse-order), and varied training distributions (balanced-carry sampling); no linear-probe or neuron-level mechanistic probing reported for baseline.",
            "performance_metrics": "Baseline: models trained on standard-format addition show little-to-no length generalization; test performance on examples 5+ tokens longer than training is at or near random chance (no numeric EM reported beyond 'no generalization observed').",
            "error_types_or_failure_modes": "Fails to generalize to longer lengths; sensitive to carry-chains (especially 'hard' carry cases); failure attributed to inability to do global index-arithmetic and to propagate carry non-causally in one forward pass.",
            "evidence_for_mechanism": "Comparison experiments: lack of generalization contrasted with success after targeted formatting interventions (index hints, reverse-order) which reduce required index arithmetic and make carry causal; RASP-L analysis shows standard addition needs long/non-uniform program in RASP-L.",
            "counterexamples_or_challenges": "Although a standard-order RASP-L program exists, it is much longer; length generalization can be enabled via training-data diversity and formatting (so failure is not absolute), showing that negative baseline is surmountable with interventions.",
            "uuid": "e8378.0",
            "source_info": {
                "paper_title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Addition (index-hints + reverse)",
            "name_full": "Addition with index-hints and reverse-order outputs",
            "brief_description": "Reformatted addition where each digit is annotated with index-hints (tokens that allow induction-style lookup) and the output is reversed (least-significant digit first), enabling short RASP-L implementations and strong length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only causal Transformer (trained from scratch)",
            "model_description": "Same experimental model family as above; trained on reformatted addition data with index-hints and reverse-order targets, with context packing and random shifts.",
            "arithmetic_task_type": "Multi-digit decimal addition with per-digit index hints and reverse-order output; evaluated on both 'easy' and 'hard' carry distributions.",
            "mechanism_or_representation": "Mechanistically framed as: index-hints allow digit correspondence via induction-head-like behavior (no index arithmetic), and reverse-order makes carry propagation causal by referencing previous output digit as scratchpad, enabling a single-pass Transformer implementation.",
            "probing_or_intervention_method": "Intervention via prompt/output reformatting (index-hints, reverse-order); training with balanced carry sampling for diversity to expose long carry-chains; comparison of forward vs reverse formatting.",
            "performance_metrics": "Reverse addition with index hints: strong length generalization — many runs generalize perfectly to 50-digit test once training max length &gt;~40 (Figure 5b). Forward addition with index hints: generalizes on 'easy' carry but fails on 'hard' carry unless trained with balanced-carry sampling; after balanced-carry training, forward addition can also length-generalize (claimed first strong generalization on decimal addition trained from scratch).",
            "error_types_or_failure_modes": "Forward-format still struggles on 'hard' carry when training lacks diverse long carry examples; success depends on training diversity and formatting choices.",
            "evidence_for_mechanism": "Empirical: dramatic performance improvement when using index-hints + reverse output compared to baseline; faster optimization (Figure 6a) for reverse addition; existence of short RASP-L program (Listing 9) for reverse addition demonstrates representability.",
            "counterexamples_or_challenges": "Forward-order addition is implementable but requires a much longer RASP-L program (Listing 10) and more diverse training (balanced carry) to learn; thus format helps but does not fully eliminate algorithmic difficulty without sufficient training diversity.",
            "uuid": "e8378.1",
            "source_info": {
                "paper_title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Parity (baseline)",
            "name_full": "Bitwise parity (no scratchpad)",
            "brief_description": "Computing parity of a bit sequence (XOR of bits) presented as a next-token task without intermediate scratchpad steps; considered a canonical hard boolean/sequential task for Transformers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only causal Transformer (trained from scratch)",
            "model_description": "Same as above; trained on parity examples sampled uniformly up to a max training length, with context packing and random shifts.",
            "arithmetic_task_type": "Parity computation (binary sequence parity), multi-length sequences, tested out-of-distribution at longer lengths.",
            "mechanism_or_representation": "Natural algorithm is sequential finite-state iteration (running parity state), which cannot be implemented in a single constant-depth parallel Transformer pass without unrolling or scratchpad; computing parity in parallel would require a global sum and stable numeric parity detection, which is unnatural for Transformers.",
            "probing_or_intervention_method": "Empirical training/evaluation; attempted augmentation with scratchpads (see separate entry); no fine-grained neuron probing reported.",
            "performance_metrics": "Baseline: no length generalization observed; test performance does not exceed random chance when evaluated 5+ tokens longer than training.",
            "error_types_or_failure_modes": "Fails to generalize; cannot stably compute parity for arbitrarily long sequences; may fail to even fit training set beyond some lengths (as noted in prior work).",
            "evidence_for_mechanism": "Argument based on RASP-L expressivity and causal masking: parity requires n sequential steps or a numerically unstable global sum; empirical failure corroborates theoretical limitation.",
            "counterexamples_or_challenges": "When provided a carefully designed scratchpad that writes intermediate parity state per symbol (unrolling the loop into the context), Transformers can learn to generalize parity (see parity with scratchpad), showing the task is learnable with suitable external state encoding.",
            "uuid": "e8378.2",
            "source_info": {
                "paper_title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Parity (with scratchpad)",
            "name_full": "Parity with index-hints and iterative scratchpad",
            "brief_description": "Parity reformatted to include index-hints and a scratchpad that records running parity per 1-bit encountered; this externalizes the sequential state and enables strong length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only causal Transformer (trained from scratch)",
            "model_description": "Same experimental family; trained on parity examples augmented with index-hints and a scratchpad that logs running parity symbols (+/-) associated with indices.",
            "arithmetic_task_type": "Parity computation over binary sequences, using scratchpad tokens representing intermediate parity states; evaluated on longer-length generalization.",
            "mechanism_or_representation": "Scratchpad encodes the sequential state externally so that each autoregressive next-token prediction only needs to perform one-step updates; this converts an n-step internal loop into n single-step next-token predictions representable by the Transformer.",
            "probing_or_intervention_method": "Intervention via prompt/scratchpad design; training experiments varying max training length to observe generalization thresholds.",
            "performance_metrics": "Parity with scratchpad: many runs trained up to length 30 generalize perfectly to length 50; when training length reaches 40, all runs achieve perfect length generalization on length-50 tests (Figure 5c).",
            "error_types_or_failure_modes": "Performance sensitive to scratchpad formatting; poorly designed scratchpads may not help or can slow convergence; success requires proper index hints and scratchpad layout.",
            "evidence_for_mechanism": "Empirical: strong length generalization achieved only when scratchpad unrolls sequential state; training speed (Figure 6b) correlates with scratchpad quality; RASP-L analysis explains why scratchpad reduces RASP-L program complexity.",
            "counterexamples_or_challenges": "Not all scratchpads help — some human-intuitive scratchpads (e.g., outputting decimal counts for mode) made tasks harder; scratchpad design must match Transformer's representational strengths (e.g., avoid introducing complex index arithmetic or decimal parsing).",
            "uuid": "e8378.3",
            "source_info": {
                "paper_title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Counting task",
            "name_full": "Count from a to b inclusive (digit generation)",
            "brief_description": "Sequence-generation task where model must output the sequence of integers between two given endpoints a and b; used as an example of an algorithmic task that Transformers can easily learn and length-generalize.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only causal Transformer (trained from scratch)",
            "model_description": "As above; trained on packed contexts containing multiple count examples sampled across lengths up to a max train length (e.g., up to 50).",
            "arithmetic_task_type": "Counting sequences (incrementing integers between endpoints), variable output length.",
            "mechanism_or_representation": "Represented by a short RASP-L program: model finds SoS token, reads a and b, then autoregressively outputs next number by reading latest output token and incrementing (x+1) until b then EoS; implemented via simple attention and tokenwise ops rather than complex index arithmetic.",
            "probing_or_intervention_method": "Empirical training/evaluation; Appendix C contains supporting evidence suggesting models implement the described algorithm (no detailed neuron-level probes in main text).",
            "performance_metrics": "Near-perfect length generalization: models trained on counts up to length 50 generalize near perfectly to length 100 (Figure 1b); EM nearly 100% for sufficiently large training max length.",
            "error_types_or_failure_modes": "Requires diversity in training lengths (uniform sampling 1..max) — training on fixed-length examples prevents generalization; otherwise robust.",
            "evidence_for_mechanism": "Short RASP-L program exists (Figure 2) mapping directly to a single-transformer-layer-per-line compile; empirical success and faster convergence consistent with RASP-L simplicity principle; Appendix C shows evidence that trained models implement algorithmic behavior.",
            "counterexamples_or_challenges": "If positional embeddings or training procedures differ (e.g., not packing/shifting contexts) or training data lacks diversity, generalization deteriorates; RASP-L reasoning suggests representation depends on allowed token/index ops.",
            "uuid": "e8378.4",
            "source_info": {
                "paper_title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Copy unique / Induction heads",
            "name_full": "Copy task with unique tokens (using induction-head mechanism)",
            "brief_description": "Copying the prompt exactly in the output when prompt tokens are guaranteed unique; task is solved by induction-head-like attention which locates prior occurrence of token and predicts the following token.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only causal Transformer (trained from scratch)",
            "model_description": "Same family; trained on prompt-&gt;copy examples where input tokens are unique within each example; learned positional embeddings with packed contexts.",
            "arithmetic_task_type": "Not strictly arithmetic, but a sequential token-copying task that demonstrates an attention-based algorithmic mechanism relevant to arithmetic index lookup.",
            "mechanism_or_representation": "Induction heads: attention pattern that finds previous occurrence of the current query token, retrieves the token that followed it, and uses that as the prediction; this is naturally expressible in RASP-L via the induct helper and easy for Transformers to learn.",
            "probing_or_intervention_method": "Empirical training/evaluation; RASP-L standard-library includes an induct implementation used to reason about mechanism; no neuron-level probing reported here.",
            "performance_metrics": "Models trained on unique-token copy up to length 40 generalize perfectly to length 50 (Figure 3b); EM near 100% for sufficient train max length.",
            "error_types_or_failure_modes": "When tokens repeat, induction-head strategy breaks and models fail to generalize unless alternative indexing or scratchpad mechanisms are provided.",
            "evidence_for_mechanism": "Strong empirical generalization consistent with induction-head functioning; RASP-L provides a one-line implementation (Listing 6) showing representability; prior work (Olsson et al., 2022) corroborates induction-head emergence.",
            "counterexamples_or_challenges": "Copy with repeating tokens is hard and fails to generalize because it requires precise index-arithmetic that induction heads cannot disambiguate.",
            "uuid": "e8378.5",
            "source_info": {
                "paper_title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RASP-L / RASP-Generalization Conjecture",
            "name_full": "RASP-L language and the RASP-Generalization Conjecture",
            "brief_description": "RASP-L is a restricted RASP variant designed to capture algorithms easy for Transformers to represent; the RASP-Generalization Conjecture posits Transformers will length-generalize when the true next-token function has a short RASP-L program and training data prevents shorter spurious programs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (theoretical/analysis tool for Transformer representability)",
            "model_description": "RASP-L programs correspond to causal Transformer computations (one RASP-L line ≈ one Transformer layer); RASP-L restricts index arithmetic and numeric ranges to reflect learnable, uniform transformer-implementable operations.",
            "arithmetic_task_type": "Provides representational analysis for arithmetic-like tasks (addition, parity, counting) by expressing whether algorithms are short/uniform in RASP-L.",
            "mechanism_or_representation": "Frames mechanisms in terms of attention-based operations (kqv causal attention), tokenwise maps, min/max/mean aggregations, and restricted index operations (order, successor/predecessor) rather than arbitrary index arithmetic; explains when induction heads, scratchpads, or attention searches suffice.",
            "probing_or_intervention_method": "Used as an explanatory and predictive tool: write RASP-L programs for tasks, compile to Transformer weights in prior works, and design interventions (index hints, scratchpads, reverse-output) to reduce RASP-L complexity and empirically test effect.",
            "performance_metrics": "Phenomenological claim: tasks with short RASP-L programs empirically show strong length generalization and faster in-distribution optimization; specific task metrics reported in paper (see other entries).",
            "error_types_or_failure_modes": "RASP-L is not complete: some transformer-representable numerical algorithms are poorly captured; RASP-L restrictions (no index arithmetic, bounded ints) may exclude some valid transformer solutions; finding minimum RASP-L program is intractable in general.",
            "evidence_for_mechanism": "Empirical alignment across tasks: tasks with short RASP-L solutions (count, mode, copy-unique, sort) generalize well; tasks without short RASP-L solutions (addition, parity, copy-repeat) fail unless reformatted; experiments manipulating RASP-L complexity (scratchpads, hints) change generalization predictably.",
            "counterexamples_or_challenges": "There are representable functions not captured succinctly by RASP-L; the conjecture is phenomenological (predictive) rather than a mechanistic proof that trained weights match compiled RASP-L weights; limits in capturing numeric/float operations and some attention behaviors.",
            "uuid": "e8378.6",
            "source_info": {
                "paper_title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Thinking like transformers",
            "rating": 2
        },
        {
            "paper_title": "Tracr: Compiled transformers as a laboratory for interpretability",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2
        },
        {
            "paper_title": "In-context learning and induction heads",
            "rating": 2
        },
        {
            "paper_title": "Generalization on the unseen, logic reasoning and degree curriculum",
            "rating": 1
        },
        {
            "paper_title": "Length generalization in arithmetic transformers",
            "rating": 2
        }
    ],
    "cost": 0.014615,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What Algorithms can Transformers Learn? A Study in Length Generalization</h1>
<p>Hattie Zhou ${ }^{\wedge 1,2}$, Arwen Bradley ${ }^{1}$, Etai Littwin ${ }^{1}$, Noam Razin ${ }^{\wedge 1,3}$, Omid Saremi ${ }^{1}$, Josh Susskind ${ }^{1}$, Samy Bengio ${ }^{1}$, and Preetum Nakkiran ${ }^{1}$<br>${ }^{1}$ Apple<br>${ }^{2}$ Mila, Université de Montréal<br>${ }^{3}$ Tel Aviv University</p>
<h4>Abstract</h4>
<p>Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021)- a programming language designed for the computational model of a Transformerand introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). On the theoretical side, we give a simple example where the "min-degree-interpolator" model of learning from Abbe et al. (2023) does not correctly predict Transformers' out-of-distribution behavior, but our conjecture does. Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown impressive abilities in natural language generation, reading comprehension, code-synthesis, instruction-following, commonsense reasoning, and many other tasks (Brown et al., 2020; Chen et al., 2021; Chowdhery et al., 2022; Lewkowycz et al., 2022b; Gunasekar et al., 2023; Touvron et al., 2023). However, when evaluated in controlled studies, Transformers often struggle with out-of-distribution generalization (Nogueira et al., 2021; Ontañón et al., 2022; Dziri et al., 2023; Wu et al., 2023; Saparov et al., 2023). It is thus not clear how to reconcile Transformers' seemingly-impressive performance in some settings with their fragility in others.</p>
<p>In this work, we aim to understand when standard decoder-only Transformers can generalize systematically beyond their training distribution. We adopt the approach of recent studies and focus on length generalization on algorithmic tasks as a measure of how well language models can learn to reason (Nogueira et al., 2021; Kim et al., 2021; Anil et al., 2022; Lee et al., 2023; Dziri et al., 2023; Welleck et al., 2022; Liu et al., 2023). Length generalization evaluates the model on problems that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: a A selection of tasks studied in this paper partitioned by whether they can be solved by programs in the RASP-L programming language (discussed in Section 3). Test EM denotes the exact match accuracy (EM) for test inputs of length 10 greater than train. We show that all tasks which admit a short solution in the RASP-L programming language also exhibit strong length generalization performance, and vice versa. For certain hard tasks we construct "new" versions which admit RASP-L solutions, by carefully modifying the input and scratchpad format, and these versions length generalize. We also show how poor scratchpad formats can make tasks harder, by giving an example for the Mode task. b Length generalization for the counting task (described below), measured by exact match accuracy (EM). Transformers are trained on sequences of varying length, and tested at different levels of out-of-distribution over the maximum training length. Models trained on sequences of length 60 or more exhibit near perfect length generalization up to length 150 (max evaluation length).
are longer (and harder) than seen in the training set- for example, training on 10 digit decimal addition problems, and testing on 20 digit problems. This challenging evaluation setting gives an indication of whether the model has learned the correct underlying algorithm for a given task.</p>
<p>There is currently no clear answer in the literature about when (or even if) Transformers length generalize. Transformers trained from scratch on addition and other arithmetic tasks exhibit little to no length generalization in prior work (Nye et al., 2021; Nogueira et al., 2021; Lee et al., 2023), and even models finetuned from pretrained LLMs struggle on simple algorithmic tasks (Anil et al., 2022). Going even further, Dziri et al. (2023) hypothesize that Transformers solve tasks via "analogical pattern matching", and thus fundamentally cannot acquire systematic problem-solving capabilities. On the other hand, length generalization can sometimes occur for particular architectural choices and scratchpad formats (Jelassi et al., 2023; Kazemnejad et al., 2023; Li \&amp; McClelland, 2023). A number of papers have studied this question for specific classes of problems, such as decimal addition (Lee et al., 2023), Dyck-1 languages (Bhattamishra et al., 2020), Boolean functions (Abbe et al., 2023), structural recursion (Zhang et al., 2023), and finite state automaton (Liu et al., 2023). However, there is no unifying framework for reasoning about length generalization in Transformers which captures both their surprising failures and surprising capabilities, and applies to a broad class of symbolic reasoning tasks.</p>
<p>As a starting point of our work, we show that there exist algorithmic tasks where Transformers trained from scratch generalize naturally far outside of the training distribution. This observation contradicts the conventional wisdom in much of the existing literature, and suggests that length generalization is not inherently problematic for the Transformer architecture- though it clearly does not occur for all tasks. Why then do Transformers exhibit strong length generalization on certain tasks and not others, and what are the mechanisms behind generalization when it occurs? In the following sections, we will propose a unifying framework to predict cases of successful length generalization and describe a possible underlying mechanism.</p>
<p>Preview of Length Generalization. We begin by introducing a simple task that exhibits strong length generalization, as a minimal example of the phenomenon. The task is "counting": given a prompt SoS a b &gt; for numbers $a, b$, the model must count from $a$ to $b$ inclusive, and terminate with "EoS". An example is: SoS 2 5&gt;2345 EoS. We train a Transformer with learned positional embeddings on count sequences of lengths up to 50, with random endpoints $a$ and $b$ in $[0,155]$. We "pack the context" with i.i.d. sequences during training, following standard practices in LLM pipelines. This trained model then length-generalizes near perfectly when prompted to count sequences of length 100 (see Figure 1b).</p>
<p>Possible Mechanisms. To understand the above example, it is helpful to first consider: why should length generalization be possible at all? The crucial observation is that the Transformer architecture is already equipped with a natural notion of length-extension. If we omit positional encodings for simplicity, then a fixed setting of Transformer weights defines a sequence-to-sequence function on sequences of arbitrary length. If this function applies the correct transformation for inputs of any length in the context, then we can expect it to length generalize.
For the count task, length generalization is possible if the model somehow learns a correct algorithm to solve the count task. One such algorithm is as follows. To predict the next token:</p>
<ol>
<li>Search for the most-recent SoS token, and read the following two numbers as $a, b$.</li>
<li>Read the latest output token as $x$. If ( $\mathrm{x}==\left.'&gt;\right)$, output $a$. If ( $\mathrm{x}==\mathrm{b}$ ), output EoS.</li>
<li>Otherwise, output $(x+1)$.</li>
</ol>
<p>This program applies to sequences of all lengths. Thus, if the model ends up learning the algorithm from short sequences, then it will automatically length-generalize to long sequences. The discussion so far could apply to any auto-regressive model, not just Transformers. What is special about the Transformer architecture and the count task, though, is that a Transformer can easily represent the above program, uniformly for all input lengths. This is not trivial: we claim that the same exact Transformer weights which solve the task at length 20, can also solve the task at length 50 and greater lengths (Figure 1a). Interestingly, we find supporting evidence in Appendix C that the trained models are actually implementing this algorithm.</p>
<p>Overview. The core message of this work is that it is actually possible for Transformers to approximately learn a length-generalizing algorithm, if the correct algorithm is both possible and "simple" to represent with a Transformer. In our main conjecture in Section 2, we propose a set of conditions that determine whether a Transformer model trained from scratch is likely to generalize on a given symbolic task. Our conjecture builds upon the following intuition:</p>
<p>Intuition. Algorithms which are simple to represent by Transformers are also easy to learn for Transformers, and vice versa.</p>
<p>This intuition tells us that to reason about whether Transformers will length-generalize on a given task, we should consider whether the underlying algorithm that solves the task is naturally representable by the Transformer architecture. To do so, we leverage the recently introduced RASP programming language (Weiss et al., 2021; Lindner et al., 2023), which is essentially an "assembly language for Transformers." We describe RASP, and our variant of it RASP-L, in Section 3. For now, it suffices to say RASP-L is a human-readable programming language which defines programs that can be compiled into Transformer weights ${ }^{1}$, such that each line of RASP-L compiles into at most one Transformer layer. RASP-L lets us reason about Transformer-representable algorithms at a familiar level of abstraction - similar to standard programming languages like Python, but for programs which "run on a Transformer" instead of running on a standard von Neumann computer.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We leverage the RASP-L language to state a more precise version of our intuition, which predicts exactly which functions Transformers learn:</p>
<p>Toy Model (RASP MDL Principle). For symbolic tasks, Transformers tend to learn the shortest RASP-L program which fits the training set. Thus, if this minimal program correctly lengthgeneralizes, then so will the Transformer.</p>
<p>This toy model is similar to Minimum-Description-Length principles (e.g. Shalev-Shwartz \&amp; BenDavid (2014)) and Solomonoff induction (Solomonoff, 1964). The key insight is that we propose using a measure of complexity that specifically corresponds to the unique information-flow inside Transformer architectures. Although many prior works have conjectured similar "simplicity bias" in Transformers (Abbe et al., 2023; Bhattamishra et al., 2023), the notion of simplicity we use here is tailor-made for the Transformer architecture. This distinction is important: In Section 6, we show a simple setting where the popular "minimum polymomial degree" notion of simplicity (Abbe et al., 2023) does not correctly predict Transformer generalization, whereas our RASP-based notion of simplicity does. Finally, notice that we use a notion of simplicity over programs, rather than over functions with fixed input dimension, which makes it especially suitable for understanding algorithmic tasks.</p>
<h1>1.1 Organization and Contributions</h1>
<p>In Section 2, we introduce the RASP-Generalization Conjecture (RASP conjecture for short), which uses the RASP-L programming language to predict when Transformers are likely to length-generalize. To help users develop intuition about RASP-L algorithms, we present a "standard library" of RASP-L functions (Section 3) that can be used as modular components of programs. This includes an implementation of "induction heads" (Olsson et al., 2022), for example. In Section 4, we show that the RASP conjecture is consistent with experiments on a variety of algorithmic tasks. Then, in Section 5, we leverage our conjecture to improve length generalization, which results in the first instance of strong length generalization on parity and addition for Transformer models trained from scratch. Finally, on the theoretical side, we give an example where the "min-degree-interpolator" model of learning from Abbe et al. (2023) does not produce correct predictions for Transformers, but our conjecture does (Section 6). We conclude with a discussion of limitations and open questions, and discuss additional related works in Appendix A.</p>
<h2>2 Main Conjecture</h2>
<p>We first set some notation. A Transformer (Vaswani et al., 2017) refers to an instance of a decoderonly causal Transformer architecture, with any fixed setting of weights, along with any computable positional embedding scheme ${ }^{2}$. As a technical point, we allow the transformer weights to take values in the extended real line $\mathbb{R} \cup{ \pm \infty}$, to allow saturating the softmax at arbitrarily large context lengths ${ }^{3}$. We consider only greedy sampling throughout, since our tasks are deterministic. In this conjecture, and throughout the paper, we consider Transformers "trained to completion," meaning trained to near-optimal performance on their training distribution. That is, we assume that in-distribution generalization is achieved nearly-optimally, and focus our attention on the induced out-of-distribution generalization. The exact training procedure we consider is given in Section 4.1. We now describe our main conjecture.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>RASP-Generalization Conjecture. A decoder-only autoregressive Transformer is likely to lengthgeneralize when trained to completion on an algorithmic task if the following conditions hold.</p>
<ol>
<li>Realizability. The true next-token function for the task can be represented by a single causal Transformer which works on all input lengths.</li>
<li>Simplicity. This representation is "simple", meaning it can be written in RASP-L (a learnable subset of RASP defined in Section 3).</li>
<li>Diversity. The training data is sufficiently diverse, such that there does not exist any shorter RASP-L program which agrees with the task in-distribution but not out-of-distribution.</li>
</ol>
<p>Remarkably, the above features are empirically correlated with: (1) generalization to longer lengths out-of-distribution, and (2) faster train optimization in-distribution.</p>
<p>The first condition of realizability is actually quite stringent, because it requires a single Transformer to be able to solve the task at all lengths. Causal Transformers define a particular computational model, and not all sequence-to-sequence tasks can be solved within this model. For example, nexttoken functions which require $\Omega\left(n^{3}\right)$ computation time on inputs of length $n$ provably cannot be represented by a Transformer, however large ${ }^{4}$. Now, the realizability condition may seem stronger than required, because in practice, we do not actually need length generalization for arbitrary unbounded lengths- only lengths up to some maximum context size. Nevertheless, we find that considering representability in the unbounded length setting is a good heuristic for learnability in bounded length settings. Intuitively, if a task requires a different Transformer for each input length, then it may be an "unnatural" task for Transformers, and unlikely to generalize well.</p>
<p>We emphasize that our conjecture is primarily phenomenological, as opposed to mechanistic. That is, we do not claim Transformers will actually learn weights which are close to the compiled weights of the RASP-L program. We only claim that RASP-L is a useful predictive tool: empirically, if a task can be solved by a RASP-L program, then it can be learned easily by a Transformer, and vice versa. Although we believe the toy model is a plausible mechanism that implies our conjecture, we leave investigating this more fully as an important question for future work.</p>
<h1>2.1 Experimental Validation</h1>
<p>We empirically evaluate the conjecture by training Transformers on a set of algorithmic tasks in Section 4. The conjecture proposes three conditions: 1) realizability, 2) simplicity, and 3) diversity. Simplicity implies realizability, as being able to come up with a RASP-L program for a task guarantees realizability. In our experiments, we propose a set of "easy" tasks for which we could write simple RASP-L solutions for, and a set of "hard" tasks for which we could not, and evaluate the relationship between task difficulty (per RASP conjecture) and length generalization performance. Moreover, we evaluate the hypothesis that simple-to-represent programs are more easily learned by looking at the train convergence speed of tasks with varying RASP-L complexity. Lastly, although task diversity is a rich area for investigation, here we focus on varying the range of lengths seen during training as a simple proxy for diversity.</p>
<p>There are many other factors that may influence the generalization performance of Transformer models, including architectural innovations, positional embeddings, and training methodology (Dehghani et al., 2019; Furrer et al., 2021; Ontañón et al., 2022; Press et al., 2022; Ruoss et al., 2023). Such factors can also influence how robustly the correct solution is learned, if indeed it is. Since our study focuses on the relevant characteristics of tasks, and not of architectures, we employ standard decoder-only causal Transformers with learned positional embeddings throughout this paper. If models trained in this simple setting can still exhibit non-trivial length generalization on a given task, we can conclude that this task is amenable to generalization. For completeness, we also include results using rotary positional embedding in Appendix D.3.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Note, we do not expect (and indeed do not observe) perfect length generalization over arbitrary lengths. Some level of degradation is likely fundamental due to issues of the noisy optimization process, continuous weight space, finite-precision, etc. Moreover, the model may learn a program that functionally approaches the target program, but does not exactly match it. Thus, we study the external phenomena of non-trivial length generalization performance in this paper, and leave the mechanistic investigation of the learned programs to future work.</p>
<h1>3 RASP-L: What Algorithms Can Transformers Learn?</h1>
<p>We will now define a version of the RASP programming language (Weiss et al., 2021), which we call RASP-L. RASP-L is essentially a restricted version of RASP, with one additional feature. Since programming in RASP-L is fairly non-intuitive, we also introduce a "standard library" of useful RASP-L functions, which can be composed to solve more complex tasks. We first review the original RASP language.</p>
<h3>3.1 Background: A RASP Primer (in Python)</h3>
<p>The original RASP language can be thought of as a domain-specific-language for specifying Transformer weights, in human-readable form (Weiss et al., 2021). Importantly, RASP was designed for the computational model of Transformers, so short RASP programs define functions which are "easy to represent" for Transformers. Although RASP was conceived as a separate language with its own syntax, it is possible to realize RASP as a restricted subset of Python where only a few operations are allowed. We do this explicitly in Listing 1, briefly described here ${ }^{5}$.</p>
<p>Every RASP program accepts an input sequence of length $n$, for all $n \in \mathbb{N}$, and returns an output sequence of the exact same length- just like a Transformer. No control flow is allowed; all programs must be straight-line programs, with no branching or loops. Concretely, every line of a RASP program must be a call to one of the core functions defined in Listing 1, or to another RASP program. The core operations allowed in RASP are: arbitrary elementwise operations over sequences (map and seq_map), and a very particular type of non-elementwise operation kqv, which simulates a causal attention layer. The kqv function takes as input three sequences and a binary predicate (such as greater-than), then constructs a boolean attention matrix by applying the predicate to all pairs of key and query, and applies this matrix to the value sequence (see Listing 1).</p>
<p>Causality. Since we study autoregressive decoder-only Transformers, we must use the causal version of RASP, where all sequence-to-sequence operations are executed causally. Moreover, while RASP programs define sequence-to-sequence functions, we interpret them as autoregressive functions by taking the last token of output sequence as the next-token prediction. This setting differs from most prior literature on RASP, which typically consider non-causal models, and these differences significantly change the nature of RASP programming.</p>
<p>Intuition. A key characteristic of RASP is that it only allows parallelizable operations, because Transformers are an inherently parallel model of computation. This makes performing inherentlysequential computation, such as iterating through each input symbol and updating an internal state, tricky if not impossible to write in RASP. This is why loops are not allowed in RASP: a Transformer has only constant depth, and cannot directly simulate an arbitrary number of loop iterations. One way to bypass this limitation is to exploit the autoregressive inference procedure ${ }^{6}$. Since the model is called iteratively at inference time, this effectively provides an "outer-loop" that</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">tok_map</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="k">func</span><span class="p">):</span>
<span class="w">    </span><span class="mi">2</span><span class="w"> </span><span class="n">tokenwise</span><span class="w"> </span><span class="n">map</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="k">func</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">xi</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">x</span><span class="p">])</span>
<span class="n">def</span><span class="w"> </span><span class="n">seq_map</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="k">func</span><span class="p">):</span>
<span class="w">    </span><span class="mi">2</span><span class="w"> </span><span class="n">tokenwise</span><span class="w"> </span><span class="n">map</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">sequences</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="k">func</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="w"> </span><span class="n">yi</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">xi</span><span class="p">,</span><span class="w"> </span><span class="n">yi</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)])</span>
<span class="n">def</span><span class="w"> </span><span class="n">indices</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">def</span><span class="w"> </span><span class="n">full</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="p">):</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="p">)</span>
<span class="n">def</span><span class="w"> </span><span class="n">sel_width</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
<span class="w">    </span><span class="mi">0</span><span class="w"> </span><span class="n">returns</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">selected</span><span class="w"> </span><span class="n">keys</span>
<span class="w">    </span><span class="mi">0</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">query</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">A</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="k">select</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="mi">0</span><span class="w"> </span><span class="n">constructs</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">causal</span><span class="w"> </span><span class="nc">binary</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">matrix</span>
<span class="w">    </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="w">    </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">bool</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="w">            </span><span class="n">A</span><span class="o">[</span><span class="n">i, j</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pred</span><span class="o">[</span><span class="n">k[j</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">A</span>
<span class="n">def</span><span class="w"> </span><span class="n">aggr</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="mi">0</span><span class="w"> </span><span class="n">applies</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="k">sequence</span><span class="w"> </span><span class="n">v</span>
<span class="w">    </span><span class="k">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">)</span>
<span class="w">    </span><span class="n">norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sel_width</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">divide</span><span class="p">(</span><span class="k">out</span><span class="p">,</span><span class="w"> </span><span class="n">norm</span><span class="p">,</span><span class="w"> </span><span class="k">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v</span><span class="p">),</span>
<span class="w">        </span><span class="k">where</span><span class="o">=</span><span class="n">norm</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">))</span>
<span class="n">def</span><span class="w"> </span><span class="n">kqv</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="mi">0</span><span class="w"> </span><span class="n">convenience</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">composes</span>
<span class="w">    </span><span class="mi">0</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">aggr</span><span class="p">,</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">layer</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">aggr</span><span class="p">(</span><span class="k">select</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="p">),</span><span class="w"> </span><span class="n">v</span><span class="p">)</span>
</code></pre></div>

<p>Listing 1: RASP. Basic Numpy implementation of the RASP core functions. See Listing 2 in the Appendix for the full RASP-L core, which is slightly more expressive.
can enable a certain kind of sequential computation, where the sequential state is encoded into the prior context. This is exactly what scratchpads enable, as we elaborate in Section 5.</p>
<h1>3.2 RASP-L: Learnable RASP</h1>
<p>Motivation. The original RASP language defines a large set of programs which can be represented by Transformers, by hard-coding specific model weights. However, there are two immediate problems with RASP for our purposes: first, RASP is "too expressive," and includes programs which are in theory possible to represent, but difficult to learn from scratch. (For example, RASP allows arbitrarily-complex tokenwise operations $\mathbb{R} \rightarrow \mathbb{R}$ ). Second, the RASP representation is technically "non-uniform" ${ }^{7}$ , meaning it requires a different set of Transformer weights for each input length- primarily because of how it handles indexing. We thus introduce RASP-L, which is essentially a simplification of RASP that is intended to be easy-to-learn and uniform. In the experimental section, we will argue that RASP-L programs can in fact be learned by Transformers, in a length-generalizing way.</p>
<p>To address these concerns, we define a variant of RASP which we call RASP-L, which is a restricted subset of RASP along with one additional feature. The technical details of RASP-L are in Appendix E but the primary restrictions are: all variables are bounded integers (int8) to avoid arbitrary-precision and numerical stability issues, and token indices are treated specially. Roughly, token indices in RASP-L can only be operated on in simple ways (order comparison, predecessor, successor)- arbitrary arithmetic involving indices is not allowed. We will elaborate on the reasons for this below. Finally, RASP-L has one added feature: it allows "min" and "max" aggregations in the aggr function, in addition to the standard "mean" aggregation of RASP. These aggregations can be represented by an attention layer as well, similar to mean-aggregation; we give an explicit construction in Appendix G.</p>
<p>Example and Standard Library. In Figure 2 we walk through a detailed example of a RASP-L program that solves the counting task from the Introduction. To help write such programs, in Appendix F. 3 we provide a small library of useful helper-functions built on the RASP-L core. This library includes, for example, an induct function mirroring the "induction heads" identified by Olsson et al. (2022). The library notably does not include the C function at oi, which parses a sequence of tokens as a decimal integer- because operating on decimal representations is nontrivial: it is not clear if Transformers can implement this parsing with a single algorithm that works for numbers of all lengths.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: RASP-L program for Counting. A RASP-L program that solves the counting task from the Introduction, along with an autoregressive Transformer that implements this program. Here the input $x$ is a context window that contains the prompt [SoS, 1, 4], for counting from 1 to 4 . For each input token, the program must output the correct next-token, just as an autoregressive Transformer would. We show all intermediate variables of the RASP-L program evaluated on $x$; note that these variables are all sequences, and since every RASP-L operation is causal, all intermediate variables are causal functions of the input as well. The where helper-function is implemented in the standard library of Listing 3. We tokenise SoS as -1 and EoS as -2 .</p>
<p>Index Operations. Our restrictions on token indices are important because they rule out certain operations that seem natural in common programming languages like Python, but that would be challenging for a Transformer model to learn or represent. For example, standard RASP allows arbitrary arithmetic operations on indices, such as division-by-two: $i \mapsto\lfloor i / 2\rfloor$. While such index operations can in theory be represented by Transformers, this representation is not "uniform", meaning that input sequences of different lengths may need entirely different positional embeddings. Even in theory, it is non-trivial to construct positional embeddings which encode all indices in $\mathbb{N}$, while allowing arithmetic [ring] operations on indices to be performed in an "easy" and numerically-stable way. Thus we may intuitively expect that positional embeddings which were learned to support index operations over say length 20 inputs will not remain valid for length 50 inputs, because there is no "natural" representation which works for both these lengths ${ }^{8}$. To reflect this in RASP-L, we only allow the following operations on indices: order comparisons with other indices, and computing successor/predecessor of an index. This is formalized by a type system in Appendix E.</p>
<h1>4 Experiments: Length Generalization</h1>
<p>In this section, we experimentally evaluate 4 tasks that have simple RASP-L programs and 3 tasks that do not. We show that the tasks with simple RASP-L programs length-generalize well, while the remaining tasks length-generalize poorly. The four easy tasks we consider are: count, mode, copy with unique tokens, and sort. We provide the RASP-L program for these tasks in Appendix F. The three hard tasks we consider are: copy with repeat tokens, addition, and parity.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: a Length generalization performance for the mode task. All models generalize perfectly to 10 or more additional tokens at test time. b Performance on copy tasks for test sequences of length 50, and varying train lengths. Copying unique tokens makes length generalization much easier, since it can be solved by an induction head. c Performance on mode task for test sequences of length 50, and varying training lengths. The scratchpad hurts in this setting (both final answer accuracy and exact match), since it is not computable by a short RASP-L program.</p>
<h1>4.1 Experimental Setup</h1>
<p>Our experimental setup is designed to mirror standard training procedure for LLMs as closely as possible. Notably, this deviates from the typical setup used for synthetic tasks in the following way: at train time, we "pack the context", filling the Transformer's context window with multiple independent samples of the task, and we randomly shift the Transformer along its context window. This procedure of packing and shifting the context mirrors standard practice in LLM training (Karpathy, 2023; Brown et al., 2020), but is typically not done in prior works using synthetic tasks. It is an important detail: packing and shifting the context allows all positional embeddings to be trained, and encourages the transformer to treat all positions symmetrically. At test time, we evaluate examples without packing and shifting. We measure the exact match (EM) on all outputs, which is 1 if the entire output sequence is correct, and 0 otherwise.</p>
<p>All other experimental details are routine: we train decoder-only Transformers with learned positional embeddings, trained from scratch, with the standard autoregressive loss on all tokens including the prompt. We train all of our models to convergence on the train distribution where possible, and we sample independent train batches from this distribution (instead of using a finitesize train set). For all tasks, the length of training examples is sampled uniformly from length 1 up to the max training length. The detailed experimental setup and hyperparameters are provided in Appendix B.</p>
<h3>4.2 Successful Length Generalization</h3>
<p>Count. We described the count task in the Introduction and showed results in Figure 1b. The task outputs the sequence between a start and end token in the prompt, for example: 2 5&gt;2345. Length here refers to the number of output tokens. This task can be solved by the RASP-L program in Figure 2. We find that models trained on count can generalize near perfectly to double the training lengths. It is crucial that our training distribution contain samples ranging from length 1 to maximum training length, which adds diversity as we scale up training length. These factors are necessary in preventing shortcut programs from being learned. For example, no generalization is observed if we train on sequences of all the same length. Moreover, as we scale up training length, we observe a corresponding increase in the number of tokens beyond the training length that the model can successfully generalize to.</p>
<p>Mode. The mode task identifies the most frequent element in a sequence. Merrill et al. (2022) studied a binary-only version of this task and demonstrated strong length generalization. We constrain the sequences such that the answer is unique. An example is: a b c b a c b b. Figure 3a shows the results on mode, when training and testing on random sequences from an alphabet of 52 symbols. We find that models trained on mode generalize strongly to sequences far longer than the training lengths. Similar to count, we also observe here that increasing data diversity in the form of maximum training length leads to better length generalization. Interestingly, the improvement due to training set complexity in this task is much more subtle: even models trained on sequence length up to 10 can achieve a median test accuracy of $50 \%$ of sequences of length 60 .</p>
<p>Copy with unique tokens. The copy task repeats the prompt sequence in the output. We constrain the sequences to have all unique tokens in the prompt. An example is: a c d b &gt; a c d b. Figure 3b shows the results on copy with unique tokens. For models trained on sequence length up to 40 , we find that they can generalize perfectly to length of 50 . Intuitively, this task is easy because we can leverage what is called an "induction head" (Olsson et al., 2022). Induction heads work by identifying a previous instance of the current token, finding the token that came after it, and predicting the same completion to the current token. Olsson et al. (2022) found that induction heads are reliably learned even by simple Transformers, and conjectured them to be a component of what enables in-context learning in LLMs. Induction heads are simple to implement in RASP-L, as the induct function in Listing 3. Thus, the next token can be generated by simply using an induction head on the current token, since all tokens are unique. This is exactly what the RASP-L program does, in Listing 6.</p>
<p>Sort. The sort task takes in a sequence and outputs the same sequence sorted in ascending order. For example: $41237&gt;34712$. This task has been studied by Li \&amp; McClelland (2023); Awasthi \&amp; Gupta (2023), and showed signs of strong length generalization. Indeed, there is a oneline RASP-L program for this task (Listing 7). Figure 9 shows the experimental results for sort. We observe strong length generalization on length 50 for models trained with sequences of length 35 or more.</p>
<h1>4.3 Unsuccessful Length Generalization</h1>
<p>Next, we study three tasks that do not admit simple RASP-L solutions: addition, parity, and copy with repeating tokens. We discuss reasons why Transformer models struggle to generalize on these tasks by highlighting the operations that these algorithms require, but that are unnatural for a Transformer to represent.</p>
<p>Addition \&amp; Parity. Addition and parity have both been studied extensively as difficult tasks for Transformers. Models trained from scratch show little to no length generalization on addition (Nye et al., 2021; Lee et al., 2023) and parity (Bhattamishra et al., 2020; Chiang \&amp; Cholak, 2022; Ruoss et al., 2023; Delétang et al., 2023), and even pretrained LLMs cannot solve these tasks robustly (Brown et al., 2020; Chowdhery et al., 2022; Anil et al., 2022) without careful prompting (Zhou et al., 2022b).</p>
<p>Indeed, addition is also difficult to write in RASP-L. To see why, consider the standard addition program shown in Appendix F.8. This algorithm requires the carry value to be propagated in reverse order from least- to most-significant digit, but this is difficult to simulate due to causal masking. Moreover, the most prohibitive aspect is the index-related operations. The standard addition algorithm requires index-arithmetic (e.g. finding the middle of the prompt sequence) and precise indexing operations (e.g. look up the corresponding summand digits for the current output</p>
<p>digit). Such operations are forbidden in RASP-L, as they require index-arithmetic which are difficult to represent in a global, length-generalizing way.</p>
<p>Similarly, parity without any scratchpad requires operations that are forbidden under RASP-L. The natural algorithm for parity on $n$ is to run a finite-state-machine for $n$ steps, keeping the runningparity of prior bits as state. However, arbitrary finite-state-machines cannot be naturally simulated by one forward-pass of a transformer, since one forward-pass involves only a constant number of "parallel" operations, not $n$ sequential ones. Intuitively, solving parity "in parallel" requires taking the sum of the entire sequence, then determining the parity of the sum. This cannot naturally be computed in a numerically stable way for arbitrarily large sums. Moreover, we cannot expect to learn a 'sum' operation which generalizes to numbers larger than the training sequences. Many works have shown that a Transformer cannot even fit the training set of parity sequences over some minimal length (Hahn, 2020; Delétang et al., 2023).</p>
<p>Under our experimental settings, we find that no length generalization is observed for both addition and pairity tasks: test performance does not exceed random chance when evaluated on examples 5 or more tokens longer than the training set.</p>
<p>Copy with repeating tokens. For this task, we constrain the sequences to consist only of 2 possible tokens. An example is: a a b a &gt; a a b a. Since the tokens are no longer unique, the induction-head is no longer helpful. Instead, the model must perform precise index-arithmetic, which are prohibited under RASP-L. We show in Figure 3b that models fail to generalize to longer lengths on this task.</p>
<h1>5 Application: Improving Length Generalization</h1>
<p>In this section, we demonstrate how our RASP conjecture can go beyond post-hoc explanations, by constructing interventions that predictably change length generalization performance. We study how reformatting tasks to allow shorter RASP-L programs can improve generalization performance, and how increasing diversity in the training data allows the model to perform well on tasks that require more complex RASP-L programs.</p>
<h3>5.1 Deep Dive on Addition</h3>
<p>Reducing the RASP-L complexity for addition. In the previous section, we noted two aspects of a naive addition algorithm that pose problems for RASP-L: index-arithmetic (to query the summand digits for the current output digit), and non-causality (for the carry). To address the difficulty with indexing operations, we can leverage induction heads to simplify the addition algorithm for a Transformer by adding "index hints" to the prompt and answer: For example, 54 +37 &gt;91 becomes a5b4+a3b7&gt;a9b1. This enables us to get the corresponding digits for each sum step by calling induct on its index hint (a or b), thus sidestepping the need to precisely access and manipulate positional indices. During training, we sample the index hints as a random slice from a longer contiguous block of tokens, to encourage learning all hints and their linear ordering. This is similar to our training strategy for the count task. Adding index hints thus allows us to avoid index-arithmetic, which is the most prohibitive aspect of representing the addition algorithm in RASP-L.</p>
<p>To address non-causality of the carry operation, we can format the output in reverse-order (from least- to most-significant digit). For example, a5b4+a3b7&gt;a9b1 becomes a5b4+a3b7&gt;b1a9. This enables simple and causal propagation of the carry where each step can reference the previous output to determine the current carry, similar to the standard addition algorithm. A RASP-L program for reverse-order addition, with index-hints, is provided in Listing 9. Although reversing the answer digits greatly simplifies the carrying procedure, it is still</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Intuition for why reverse order addition is simpler to implement in RASP than forward order. In reverse order, the carry calculation is simple with the help of the most recent output digit, which acts like a built-in scratchpad. In forward order, the carry requires a more complex calculation involving all remaining input digits. This intuition is reflected in the RASP-L program for forward addition, which is much longer than that of reverse addition (see Listings 9 and 10).
possible to implement an algorithm for addition in standard order. This algorithm is nontrivial, because we essentially need to propagate a carry through all $n$ input digits in order to determine the first digit of output, the most significant digit (see Figure 4). Although arbitrary sequential operations cannot be implemented in a Transformer, we can parallelize these operations due to their simplicity. We show how to construct a RASP-L program for standard-order addition in Listing 10. Comparing Listing 9 and Listing 10 reveals how much more complicated the forward algorithm is - it results in a much longer RASP-L program. The observation that reverse-order addition is easier for autoregressive Transformers was made in Lee et al. (2023), and is usually explained by claiming the reverse-order algorithm is "simpler" than the forward-order one. Here we make this explanation more concrete, by using a notion of "simpler" that is specific to the Transformer's computational model (that is, RASP-L program length).</p>
<p>Index hints enables generalization on addition. We evaluate addition in two settings: "easy" carry and "hard" carry. In easy carry, the two numbers are sampled randomly and independentlythis is what is typically done in the literature. However, uniformly random summands will only produce addition instances with short carry-chains (in expectation) - and for such instances, each output digit only depends on a small number of input digits. We thus also test "hard" carry instances, where we constrain the examples to have the longest possible carry chain for the given length. For example, a hard carry instance of length 3 is $381+619=1000$, which requires the model to compute the carry over a chain of 3 digit positions. The performance on "easy" carry is shown in Figure 14, and the performance on "hard" carry in Figure 5. We find that index hints allow both forward and reverse addition to length generalize on "easy" carry. However, on "hard" carry questions that involve carry-chains longer than seen at training, reverse addition maintains strong length generalization while forward addition exhibits no generalization. Moreover, we observe in Figure 6a that reverse addition optimizes more quickly than forward addition during training. These observations are all consistent with our claim that length generalization is "easier" on tasks which admit simple RASP-L programs.</p>
<p>Diversity enables generalization on forward addition. As we saw previously, one lever to improve generalization is to convert the task into one that has a simpler RASP-L program. Another lever suggested by the RASP conjecture is to increase training data diversity, such that shortcut</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Length generalization on addition and parity. Plot shows 20 individual trials per length, as well as their median (solid line). a Shows generalization performance for forward addition with index hints on hard carry examples of length 50 . No length generalization is observed. b Shows the generalization of reverse addition with index hints on hard carry examples of length 50. Most runs start to length generalize perfectly on 50 digit addition once the training length is greater than 40 . c Shows generalization performance for parity with scratchpad, on length 50 inputs. Most runs start to generalize perfectly on 50 digit addition once the training length is greater than 35 .
programs can no longer fit the training set. Since forward addition does admit a RASP-L program, albeit a more complex one, we would expect it is possible to learn if we "try harder," e.g. use a more careful and diverse train distribution. We explore this by training with balanced carry samplinginstead of sampling the two numbers independently, we first sample the length of the carry chain uniformly between 0 and question length, then sample a random question that contains the given carry chain length. This ensures that the model sees a significant percentage of questions containing long carry chains, thus increasing the diversity and difficulty of the training data. The results of the balanced carry training approach for both forward and reverse addition are shown in Figure 15. We see that this more careful training unlocks the model's ability to length generalize on forward addition, even under the hard carry evaluation. To our knowledge, these results demonstrate the first instance of strong length generalization on decimal addition for Transformer models trained from scratch.</p>
<h1>5.2 Why do Scratchpads Help?</h1>
<p>The RASP conjecture provides a natural way to understand why scratchpads (Nye et al., 2021; Wei et al., 2022) can be helpful: scratchpads can simplify the next-token prediction task, making it amenable to a short RASP-L program. One especially common type of simplification is when a scratchpad is used to "unroll" a loop, turning a next-token problem that requires $n$ sequential steps into $n$ next-token problems that are each only one step. The intuition here is that Transformers can only update their internal state in very restricted ways - given by the structure of attentionbut they can update their external state (i.e. context) in much more powerful ways. This helps explain why parity does not have a RASP-L program, but addition with index hints does. Both tasks require some form of sequential iteration, but in the case of addition, the iteration's state is external: it can be decoded from the input context itself.</p>
<p>In the following examples, we construct "good" scratchpads which convert the original problem into one that can be solved by a simple RASP-L program. Conversely, we construct "bad" scratchpads which seem natural to humans but require a more complex RASP-L program than the original task. We show that these interventions lead to predictable changes in length generalization, per the RASP conjecture.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: We compare the training speed as measured by exact match on a maximum training length of 45 . a compares the convergence speed of models trained on forward addition vs reverse addition with index hints. $\mathbf{b}$ compares the convergence speed of models trained on different scratchpads on parity.</p>
<p>Scratchpad enables generalization on parity. The natural algorithm for parity is to iterate through all input tokens, updating some internal state along the way. Unfortunately, this algorithm cannot be directly implemented in RASP-L, because RASP-L does not allow loops - fundamentally because one forward-pass of a Transformer cannot directly simulate $n$ sequential iterations of the loop. However, if the internal state is written on the scratchpad each iteration, then the Transformer only needs to simulate one iteration in one forward-pass, which is now possible.</p>
<p>We leverage this intuition to design a scratchpad for parity. Similar to addition, we add index hints to the prompt to simplify the indexing operation. In the scratchpad output, we locate index hints that precede each 1 in the prompt, and keep track of the running parity with symbols + (even) and - (odd). The last output token corresponds to the final answer. For example: a|0|b|0|c|1|d|1|e|0|&gt;|+|c|-|d|+. Figure 5c shows the exact match performance of the proposed parity scratchpad. We find that some of the runs trained with sequences up to 30 in length can generalize perfectly on sequences of length 50 . When training length reaches 40 , all models achieve perfect length generalization on length 50. Lastly, in Figure 6b, we compare the training curves of parity using various scratchpads, and we show that training speed is also correlated with how difficult the task is under RASP-L. Details can be found in Appendix D.1. To our knowledge, these results demonstrate the first instance of strong length generalization on parity for Transformer models trained from scratch.</p>
<p>Scratchpad hurts generalization on mode. Now we consider the mode task and look at how scratchpad might affect a task that a Transformer is naturally amenable to. A natural algorithm one might come up with is to calculate the counts of each unique token in the sequence, then output the token with the maximum count. To encourage the model to learn this algorithm, we might utilize the following scratchpad, where we output the frequency of each token in ascending order: a|b|b|c|b|a|c|b|&gt;|2|a|2|c|4|b|b. The last token in the scratchpad is then the correct answer. However, although this scratchpad provides more supervision for what algorithm the model should learn, it is a more difficult task when considered in terms of RASP-L. Intuitively, instead of internally counting element-frequencies and outputting the maximum, the model must now explicitly sort the frequencies, and convert these into decimal representations to produce its scratchpad.</p>
<p>We show in Figure 3c that the scratchpad performs significantly worse than no scratchpad, both when measured on exact match and also on the accuracy of the final answer. This shows that not only is the scratchpad difficult to generalize on (low EM), but it also reduces the likelihood that the final token corresponding to the answer learns the correct solution. Moreover, we show in Figure 6c that models trained on scratchpad converges much more slowly during training than</p>
<p>no scratchpad. In Appendix D.2, we evaluate another variant of the scratchpad with the tokens presented in order of appearance, such that no sorting is required, and observe a similar effect.</p>
<h1>6 Comparison to Min-Degree-Interpolators</h1>
<p>An essential aspect of our work is our Transformer-specific notion of function complexity: the minimum RASP-L program length. Here we show why this choice is important, by contrasting it with another popular notion of complexity: minimum polynomial degree. Concretely, Abbe et al. (2023) recently proposed a model of learning in which Transformers learn the minimum-degree function which interpolates their train set. We will give a simple example where our RASP toy model correctly predicts a Transformer's out-of-distribution generalization behavior, but the min-degree-interpolator model does not. We emphasize that these results are not inconsistent with Abbe et al. (2023): neither Abbe et al. (2023) nor our current work claim to apply in all settings. Rather, this section illustrates how a Transformer-specific measure of complexity can be more predictive than architecture-agnostic measures of complexity, in certain settings.</p>
<h3>6.1 The Setting: Boolean Conjunction</h3>
<p>We consider the "Generalization-on-the-Unseen" setting of Abbe et al. (2023), for direct comparison. Our target function is simply boolean conjunction. Given $n=20$ input bits $x_{i} \in{0,1}$, the ground truth function is the boolean AND of all bits: $f^{<em>}\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\bigwedge_{i \in[n]} x_{i}$. That is, $f^{</em>}(x)=1$ iff all bits $x_{i}=1$, and $f^{*}(x)=0$ otherwise. Now, the train distribution is supported on inputs $x$ where the last $k=5$ bits of $x$ are always 1 . Specifically: with probability $1 / 2, x$ is the all-ones vector, otherwise $x$ is all ones with a single 0 in a random location among the first 15 bits. That is,</p>
<p>$$
x \sim\left(1^{n}-B(1 / 2) e_{i}\right) ; \quad i \sim \operatorname{Unif}[1,15]
$$</p>
<p>where $e_{i} \in{0,1}^{n}$ is the $i$ th standard basis vector. Note the ground-truth label $y=f^{*}(x)$ for this distribution is balanced. The unseen test distribution is identical to the train distribution, except the ' 0 ' is only among the last 5 bits. That is,</p>
<p>$$
x^{(\text {Unseen) }} \sim\left(1^{n}-B(1 / 2) e_{i}\right) ; \quad i \sim \operatorname{Unif}[16,20]
$$</p>
<p>We now ask: when a Transformer is trained on the above train distribution, what does it predict on the unseen test distribution?</p>
<p>Experimental Result. We train a standard decoder-only Transformer autoregressively in the above setting, with sequence distribution $\left[x, f^{*}(x)\right]$ for $x$ sampled from the train distribution. The trained Transformer reaches $100 \%$ test accuracy on the unseen test set. That is, the Transformer correctly computes the boolean AND, even on bits which were irrelevant at train time. Experimental details can be found in Appendix B.</p>
<h3>6.2 The Min-Degree Interpolator</h3>
<p>We now claim that the minimum-degree-interpolator of the train set does not behave like the Transformer in the above experiment. To see this, observe that the minimum-degree-interpolator will not depend on the last $k=5$ bits of the input, since these bits are constant on the train set. This can be formalized via the following simple lemma, which uses the same notion of "degree profile" (DegP) as Abbe et al. (2023) ${ }^{9}$.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Lemma 1. For all subsets $S \subseteq{0,1}^{n}$ and all boolean functions $f:{0,1}^{n} \rightarrow \mathbb{R}$, the following holds. Let $g^{*}:{0,1}^{n} \rightarrow \mathbb{R}$ be the boolean function of minimum degree-profile which agrees with $f$ on $S$. That is,</p>
<p>$$
g^{*}=\underset{\substack{g:{0,1}^{n} \rightarrow \mathbb{R} \ \text { s.t. } g \mid_{S}=f \mid_{S}}}{\arg \min } \operatorname{DegP}(g)
$$</p>
<p>Let $I \subseteq[n]$ be the subset of indices (if any) on which $S$ is constant. That is, $\pi_{I}(S)$, the projection of $S$ to coordinates $I$, is a singleton. Then, the minimum-degree-interpolator $g^{*}$ also does not depend on indices $I$. That is,</p>
<p>$$
x_{i}=y_{i} \forall i \notin I \Longrightarrow g^{<em>}(x)=g^{</em>}(y)
$$</p>
<p>This lemma follows from the fact that if an interpolator depends on bits which are always constant on the train set $S$, then its degree-profile could be reduced without changing its output on $S$, and thus it cannot be a min-degree-interpolator. For completeness, we state this formally as Lemma 3 in the Appendix.</p>
<p>RASP-Length. On the other hand, there is a one-line RASP-L program which computes the boolean AND of all input bits:
def output(x): kqv(x, full(x, 0), full(x, 0), equals, default=1) This program can be represented by a 1-layer Transformer, where the attention layer simply "searches for a 0 " in the prior context. Thus, our RASP toy model of learning predicts the correct experimental result in this setting. The intuition here is that it is actually "easier" for a Transformer to represent the boolean conjection of all input bits, and so it learns to do so. Learning to ignore the last 5 bits would have been possible, but is an additional complication, and does not help in fitting the train set in any case. Notably, this intuition (and indeed, the experimental result) does not hold for MLPs; it is specific to the Transformer architecture.</p>
<h1>7 Discussion, Limitations, and Open Questions</h1>
<p>Limitations of RASP-L. The RASP and RASP-L programming languages were remarkably useful in our work, for reasoning about which algorithms are easy to represent and learn. However, they have clear limitations. For one, RASP and RASP-L are not complete: not all functions which can be efficiently represented by Transformers have efficient RASP implementations. A notable class of Transformer algorithms which are ill-suited for RASP are numerical algorithms, such as incontext gradient-descent and linear regression, which involve high-dimensional matrix and vector floating-point operations (Akyürek et al., 2022; Garg et al., 2022; Charton, 2021). Moreover, RASP supports only deterministic outputs and binary attention masks. In addition, some of the RASP-L restrictions may seem somewhat contrived (such as no floating-points and index restrictions) there may be a more natural way to capture the set of easily-representable algorithms for standard Transformers. Nonetheless, we view RASP and RASP-L as important steps towards reasoning about Transformer algorithms, and we hope to see future work in this direction.</p>
<p>Limitations of Complexity Measures. We considered for simplicity a basic notion of function complexity, which is the minimum RASP-L program length. This intuitive notion captures many empirical behaviors, as we demonstrated. However, depending on the operations, it can be possible to represent multiple lines of RASP-L in a single layer of a Transformer, and so RASP program length does not perfectly correspond to Transformer-complexity. There are likely more refined notions of complexity, such as the "parallel depth" of RASP-L programs, or lower-level measures like the minimum-weight-norm among all weight-noise-robust Transformers which implement the function. Many of these notions, including ours, have the drawback of being likely intractable to compute-intuitivly, it may be difficult to find the minimum RASP-L program for a task for similar</p>
<p>reasons that Kolmogorov complexity is uncomputable (Kolmogorov, 1963). We leave investigating more refined notions of complexity to future works.</p>
<p>Limitations of Scope and Strength. We acknowledge that our Main Conjecture is not fully formal, because there are aspects we do not fully understand. For example, we cannot precisely predict the extent of length generalization for different tasks. Moreover, since it is likely intractable to determine the minimum RASP-L program that fits a given training set, we cannot predict a priori what forms of "data diversity" are required to ensure strong length generalization, even if our conjecture holds true. Nonetheless, we view our conjecture as a step forward in understanding the implicit bias of Transformers, as it has more predictive power than many prior theories. Developing more formal and precise conjectures is an important question for future work.</p>
<p>On Formal Language Characterizations. One important question that remains open is whether there exists a natural complexity-theoretic definition of the class of tasks which are "simple" for autoregressive Transformers to represent. For example, it is well-known that RNNs tend to length generalize on tasks equivalent to regular languages, like Parity (e.g. Delétang et al. (2023)). This is intuitively because regular languages can be decided by a class of algorithms (deterministic finite automata) which are "simple" to represent by RNNs, and thus plausibly easy to learn. We would like an analogous characterization of which tasks admit algorithms with a simple and natural Transformer representation. Recent works have characterized which functions are possible to represent by a Transformer ${ }^{10}$, but this representation is not always "simple" enough to be learnable, and not always uniform (Hahn, 2020; Merrill et al., 2022; Chiang \&amp; Cholak, 2022; Pérez et al., 2021; Bhattamishra et al., 2020; Ebrahimi et al., 2020). Our presentation of RASP-L is meant to be one way of defining algorithms which are "simple" to represent- those expressable as short RASP-L programs - but this definition is not explicit (and likely not complete).</p>
<p>Relations to Mechanistic Interpretability. The line of work on mechanistic interpretability seeks to understand which algorithms (or "circuits") Transformers internally learn to implement, by inspecting their internals (Olsson et al., 2022; Nanda et al., 2023; Zhong et al., 2023; Conmy et al., 2023; Zhang et al., 2023; Hanna et al., 2023). Our work is thematically related, but differs in scope and level of abstraction. Our toy model is a mechanistic model and provides intuition for the RASP conjecture, but we focus the scope of this paper on the phenomenological level. We aim to establish the predictive power of the conjecture in this work, and thus focus on the external performance of the trained models in the out-of-distribution setting; whether trained Transformers' internals are at all similar to the internals of a compiled RASP program remains an open question (some such investigation was done in Lindner et al. (2023)). Finally, our motivation can be seen as dual to mechanistic interpretability: while interpretability typically takes models which are known to "work" experimentally, and then investigates how they work, we attempt to predict whether models will work or not before training them on a given task.</p>
<h1>8 Conclusion</h1>
<p>There has been significant interest recently in understanding and improving the reasoning abilities of Transformer-based models (Cobbe et al., 2021; Magister et al., 2023; Wei et al., 2022; Dziri et al., 2023; Suzgun et al., 2022; Lewkowycz et al., 2022a; Press et al., 2023). An important step towards this goal is to understand the conditions under which a Transformer model can learn general problemsolving strategies or algorithms simply by training on examples- if at all possible. We study this</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>question in a controlled setting by focusing on length generalization as an instance of algorithm learning, with standard decoder-only Transformers trained from scratch on synthetic algorithmic tasks.</p>
<p>The guiding philosophy behind our work is that we should think of Transformers not not as functions with a fixed input size, but as programs, defined for inputs of all lengths. With this perspective, we conjecture that algorithms which are simple-to-represent by a Transformer are also more likely to be learned. To operationalize this, we constructed the RASP-L language based on Weiss et al. (2021), and then defined "simple-to-represent" as "can be written as a short RASP-L program." Remarkably, our RASP conjecture captures most if not all known instances of length generalization, and provides a unifying perspective on when length generalization and algorithm learning are possible. The tools developed here could in theory apply to compositional generalization more broadly, beyond length generalization; we consider this a fruitful direction for future work. Overall, we hope our results help demystify certain observations of the surprising "reasoning-like" abilities of Transformers, by showing that some of these behaviors can actually emerge for simple, unsurprising reasons.</p>
<h1>Acknowledgements</h1>
<p>We thank (alphabetically) Samira Abnar, Madhu Advani, Jarosław Błasiok, Stefano Cosentino, Laurent Dinh, Fartash Faghri, Spencer Frei, Yejin Huh, Vaishaal Shankar, Vimal Thilak, Russ Webb, Jason Yosinski, and Fred Zhang for feedback on early drafts and discussions throughout the project.</p>
<h2>References</h2>
<p>Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum, 2023.</p>
<p>Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.</p>
<p>Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. In Advances in Neural Information Processing Systems, 2022. URL https : / /openreview.net/forum?id=zSkYVeX7bC4.</p>
<p>Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.</p>
<p>Pranjal Awasthi and Anupam Gupta. Improving length-generalization in transformers via task hinting, 2023.</p>
<p>Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020.</p>
<p>Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. Simplicity bias in transformers and their ability to learn sparse boolean functions, 2023.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>François Charton. Linear algebra with transformers. arXiv preprint arXiv:2112.01898, 2021.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>David Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention, 2022.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.</p>
<p>Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability, 2023.</p>
<p>Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers, 2019.</p>
<p>Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A. Ortega. Neural networks and the chomsky hierarchy, 2023.</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023.</p>
<p>Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. How can self-attention networks recognize dyck-n languages? In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 43014306, 2020.</p>
<p>Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Schärli. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures, 2021.</p>
<p>Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022.</p>
<p>Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023.</p>
<p>Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020.</p>
<p>Michael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model, 2023.</p>
<p>Samy Jelassi, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and François Charton. Length generalization in arithmetic transformers, 2023.</p>
<p>Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.</p>
<p>Andrej Karpathy. nanogpt. https://github.com/karpathy/nanoGPT, 2023.</p>
<p>Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 5156-5165. PMLR, 2020.</p>
<p>Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers, 2023.</p>
<p>Jeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. Have you seen that number? investigating extrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021.</p>
<p>Andrei N Kolmogorov. On tables of random numbers. Sankhyā: The Indian Journal of Statistics, Series A, pp. 369-376, 1963.</p>
<p>Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers, 2023.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022a.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Martin Dohan, Ethan S Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. 2022b. URL https://arxiv.org/abs/2206.14858.</p>
<p>Yuxuan Li and James McClelland. Representations and computations in transformers that support generalization on structured tasks. Transactions on Machine Learning Research, 2023. ISSN 28358856. URL https://openreview.net/forum?id=oFC2LAqS6Z.</p>
<p>David Lindner, János Kramár, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as a laboratory for interpretability. arXiv preprint arXiv:2301.05062, 2023.</p>
<p>Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata, 2023.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango, 2022.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason, 2023.</p>
<p>Eran Malach. Auto-regressive next-token predictors are universal learners. arXiv preprint arXiv:2309.06979, 2023.</p>
<p>William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843-856, 2022.</p>
<p>Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= 9XFSbDPmdW.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks, 2021.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.</p>
<p>Santiago Ontañón, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Making transformers solve compositional tasks, 2022.</p>
<p>Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing complete. The fournal of Machine Learning Research, 22(1):3463-3497, 2021.</p>
<p>Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2023.</p>
<p>Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. Toronto, Canada, 2023. Association for Computational Linguistics.</p>
<p>Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, and He He. Testing the general deductive reasoning capacity of large language models using ood examples, 2023.</p>
<p>Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.</p>
<p>Ray J Solomonoff. A formal theory of inductive inference. part i. Information and control, 7(1):1-22, 1964.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them, 2022.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Petar Veličković and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273, 2021.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers, 2021.
Sean Welleck, Peter West, Jize Cao, and Yejin Choi. Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics. In AAAI, 2022. URL https://arxiv. org/pdf/2109.13986.pdf.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ Note that the exact set of which functions are representable depends on certain definitional details of Transformers such as finite vs. infinite precision, bounded vs. unbounded weights, etc, which is why some of these references arrive at different conclusions.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
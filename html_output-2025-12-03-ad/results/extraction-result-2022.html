<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2022 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2022</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2022</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-276318034</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.08991v2.pdf" target="_blank">Task Generalization With AutoRegressive Compositional Structure: Can Learning From $D$ Tasks Generalize to $D^{T}$ Tasks?</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) exhibit remarkable task generalization, solving tasks they were never explicitly trained on with only a few demonstrations. This raises a fundamental question: When can learning from a small set of tasks generalize to a large task family? In this paper, we investigate task generalization through the lens of autoregressive compositional structure, where each task is a composition of $T$ operations, and each operation is among a finite family of $D$ subtasks. This yields a total class of size $D^T$. We first show that generalization to all $D^T$ tasks is theoretically achievable by training on only $\widetilde{O}(D)$ tasks. Empirically, we demonstrate that Transformers achieve such exponential task generalization on sparse parity functions via In-context Learning (ICL) and chain-of-thought (CoT) reasoning. We further show generalization in arithmetic and translation, beyond parity functions.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2022.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2022.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoRegressive Compositional task class</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal task-class in which each task is an autoregressive composition of T conditional subtasks, each chosen from a finite family of size D, yielding D^T total tasks; used to analyze when training on Õ(D) tasks suffices to generalize to D^T tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Theoretical framework / task-class (not a neural model): tasks produce output sequences y_1..y_T via y_t ~ P_{θ_t}(· | x, y_<t) where each θ_t ∈ Θ_t and |Θ_t| = D.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>modular autoregressive decomposition of tasks into T independent conditional subtasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>theoretical / general compositional tasks (applies across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ARC(D, T) (AutoRegressive Compositional class)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Each task f_θ is a tuple of conditional distributions (P_{θ1},...,P_{θT}); for any input x the model generates y_1..y_T autoregressively. Composition is across timesteps (sequential composition of subtasks); breadth D per step yields D^T tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>T (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>sequential/function composition (autoregressive composition of conditional subtasks)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>train on subset of tasks F_train ⊂ F and test on unseen tasks F_test (task-level OOD generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>theoretical analysis: MLE to identify subtasks from training tasks and discrimination tests at inference; empirical instantiations use in-context learning with CoT</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Theorem: under mild identifiability, sampling n_θ = Õ(D) tasks uniformly at random suffices to identify all subtasks and generalize to D^T tasks (formal sample-complexity bounds in Thm 3.3 and Corollary 3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Defines conditions (finite subtask families, identifiability via TV gaps) under which training on Õ(D) tasks yields exponential (D^T) task generalization; provides explicit sample-size bounds (n_θ ≥ D ln(100 D^T) etc.) and inference-sample bounds via TV-based discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Theory assumes per-component coverage (training tasks must cover each subtask at every timestep); adversarial selection of training tasks can violate coverage and break generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Per-component coverage in training (each subtask appears in F_train), identifiability (total-variation separation c>0 between different subtasks), sufficient training tasks n_θ = Õ(D), and sufficient inference demonstration samples ℓ = O(log(...)/c^2).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2022.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2022.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parity-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse parity with Chain-of-Thought (CoT) representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical instantiation of ARC: parity functions Parity(d,k) are represented with CoT as an autoregressive sequence of partial XORs (y_1 = x_{i1}, y_2 = x_{i1}⊕x_{i2}, ...), converting an otherwise breadth-exponential single-step task into ARC(d,k) with breadth D=d and depth T=k; Transformers trained with CoT generalize to unseen parity tasks with Õ(d) training tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 style Transformer (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer trained from scratch on next-token prediction of demonstration sequences; CoT implemented by using intermediate tokens (sequence of partial XORs) as supervision and prediction targets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small (3-layer, 1-head configuration used for parity/arithmetic experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer decoder/attention; uses Chain-of-Thought as output token sequence (intermediate tokens) to expose autoregressive compositional structure</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical / boolean function reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sparse Parity (Parity(d, k))</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given binary input x ∈ {0,1}^d, parity task selects k secret indices (i1..ik) and outputs the XOR of those positions. With CoT the target is the sequence of cumulative XORs of the secret indices across T=k steps, enabling autoregressive decomposition with D=d choices per step.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>T = k (they evaluate k ∈ {3,4,5,6,7} and up to larger values in scaling experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>nested/exhaustive function composition (sequential XOR accumulation / function composition across timesteps)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>task-level split: training tasks F_train sampled i.i.d. from Parity(d,k) and F_test is held-out unseen parity functions (novel secret-index tuples); also input-space split X_train/X_test for monitoring in-distribution generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>In-context learning (next-token prediction) on sequences of demonstrations (m examples per sequence); with CoT loss averaged over intermediate steps; trained from scratch (no pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>High in-distribution accuracy reported when tasks are in train; specifics vary by setting but model achieves strong IID performance with sufficient samples (context length 40).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>With CoT, near-perfect generalization (>95% test accuracy on held-out tasks) when trained on ≈3·d·ln(d) tasks (example: d=15, ~122 training tasks yields >95% accuracy across k up to 7).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Very large: ICL without CoT remains near chance (slightly above 50%) while CoT-enabled model achieves >95% on unseen tasks — an effective gap of ~45+ percentage points between no-CoT and CoT conditions on task-level generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Empirically stable across tested depths: for fixed d=15, k ∈ {3..7} the CoT model attains similar high accuracy (>95%) when training tasks ≈3 d ln(d); i.e., required number of training tasks remains roughly constant with increasing k in the tested range.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Baseline: standard ICL (no CoT) using the same Transformer architecture; result: ICL without CoT fails to generalize to unseen parity tasks (performance only slightly above chance) even as the number of training tasks increases. With CoT: near-perfect generalization. Also in-distribution ICL (no CoT) fails (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>No alternative architectures compared in parity experiments beyond same Transformer with/without CoT representation.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Task-sample scaling: empirical results match theoretical Õ(d) scaling — training task count proportional to d ln(d) suffices; increasing d (10,15,20) and using proportional training tasks (3·d·ln(d)) yields consistent generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-Thought representation exposes autoregressive compositional structure and enables exponential task generalization: training on Õ(d) i.i.d. sampled tasks suffices to generalize to D^T parity tasks; empirical evidence: ~122 training tasks for d=15 generalize to ~6,400 unseen tasks with >95% accuracy, stable across k.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>When training tasks are adversarially selected (non-i.i.d.), CoT+Transformer can fail — e.g., excluding all tasks with a particular positional coordinate in training causes chance-level performance on those withheld positional cases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>CoT representation of intermediate steps, i.i.d. random coverage of subtasks across training tasks (per-component coverage), sufficient number of distinct training tasks ≈ Õ(d), adequate context length (empirically context=40).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2022.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2022.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL-no-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning without Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline condition where the Transformer is trained to map input x directly to final output y (no intermediate CoT tokens); empirically fails to generalize to unseen parity tasks and even to some in-distribution settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 style Transformer (same 3-layer,1-head configuration used in main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard next-token prediction training where outputs are single-step final labels rather than sequences of intermediate reasoning tokens (no CoT representation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small (3-layer, 1-head)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer decoder; no explicit structural modifications beyond omission of intermediate tokens</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical / boolean function reasoning (parity) and other synthetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Parity(d,k) and related tasks (no CoT condition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transformer is given demonstration pairs (x_j, f(x_j)) and must predict f(x_query) given context, with final-output-only supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>T = 1 (single-step final output representation)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>non-decomposed single-step mapping (no autoregressive decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>same task-level splits as CoT experiments (train tasks sampled, test tasks withheld)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>ICL training on sequences; loss only on final labels (no intermediate-step supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Poor: even for in-distribution settings (trained and tested on same tasks but different input sequences) ICL without CoT struggles (Figure 6 shows failure in Parity(10,2) in-distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Very poor on unseen tasks: performance remains near chance even as number of training tasks increases (contrasts sharply with CoT condition).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>From near-chance (no CoT) to >95% (with CoT) on held-out parity tasks — large positive effect from CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared directly with same architecture trained with CoT; CoT vastly outperforms no-CoT on task-level generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Increasing number of training tasks does not substantially improve task-level generalization without CoT (remains near chance).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard ICL without explicit intermediate reasoning (CoT) fails to exploit compositional structure required for task generalization in parity and related hierarchical problems; representation matters.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Fails particularly on problems requiring hierarchical/hinted intermediate steps (e.g., parity).</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>N/A — this is the failure baseline; success requires conversion to CoT representation or other inductive bias that exposes subtasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2022.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2022.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial-Task-Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial / non-i.i.d. training task selection experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments showing that when training tasks are non-i.i.d. (adversarially excluding specific positional configurations), Transformers with CoT can fail to generalize to withheld configurations despite large training set size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 style Transformer (3-layer, 1-head)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as parity experiments; CoT representation used unless noted.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small (3-layer, 1-head)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer decoder with CoT when applicable</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical / boolean function reasoning (parity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Parity(d=10, k=3) adversarial splits</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Two adversarial removal setups: (1) Missing Coordinate: remove all training tasks where second coordinate s2=5; (2) Missing Pair: remove all training tasks that contain both indices 4 and 6 together. Evaluate generalization to tasks that include those excluded patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>T = 3 (k=3)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel combination of seen primitives vs unseen positional configurations</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>non-iid / adversarial exclusion of specific coordinate or pair from training tasks</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard ICL+CoT training but with biased task selection that omits specific patterns</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Dramatic dependency on which patterns are excluded: Missing Coordinate scenario (no training tasks with s2=5) leads to chance-level performance on tasks with s2=5; Missing Pair scenario (never seeing 4 and 6 together) still generalizes with high accuracy (96.2% reported in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Between adversarial exclusion cases: Missing Coordinate case yields ~chance vs Missing Pair yields ~96.2%, demonstrating that specific positional coverage matters more than mere element-level coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Shows sensitivity to positional/configurational coverage (positional diversity matters).</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared two adversarial selection regimes; both remove exponentially many tasks from training but have different generalization outcomes (chance vs 96.2%). Also contrasted with i.i.d. sampled training tasks which generalize well.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Even very large F_train (exponentially large) can fail to generalize if per-component coverage/positional diversity is missing; i.i.d. sampling avoids these adversarial gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Task selection matters: i.i.d. sampling of training tasks that ensures per-component coverage is crucial; adversarial exclusion of specific positional configurations (e.g., missing a coordinate) can break generalization even with many training tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Model fails primarily when an entire subtask (a given position value at a particular timestep) is never observed in training; positional diversity is essential.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Per-component coverage in training tasks (each subtask at each timestep seen at least once) — random i.i.d. sampling with n_θ = Õ(D) typically achieves this with high probability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2022.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2022.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arithmetic-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured arithmetic tasks with Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Arithmetic family where d binary inputs are combined left-to-right via ops in {+,×}; with CoT the task is ARC(2, d-1) and Transformers trained on Õ(2) tasks scale empirically as predicted, enabling generalization across exponentially many operator sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 style Transformer (3-layer, 1-head configuration used in parity/arithmetic experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer trained on next-token prediction with CoT supervision across intermediate arithmetic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small (3-layer, 1-head)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer decoder; uses CoT intermediate tokens representing each binary operation result</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical / arithmetic sequence evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic (sequence of operations over binary inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given binary inputs b1..bd and a sequence of d-1 operations op_t ∈ {+,×}, evaluate the left-to-right expression using CoT intermediate outputs; with D=2 and T=d-1 this is ARC(2, d-1).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>T = d-1 (varies with input dimension d); experiments scale T and D accordingly</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition of binary arithmetic operations (sequential operations)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>task-level i.i.d. sampling across operator sequences for training vs withheld operator sequences for testing (task-level OOD)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>In-context learning with CoT; reported total training examples: 25,000 (equally distributed across training tasks) for arithmetic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Not separately enumerated in text, but reported empirical scaling closely follows theoretical predictions under CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Empirical scaling matches theoretical O(D ln(DT)) dependence; authors state that training on a few hundred tasks enables generalization to very large exponential task spaces (example: 2^25 > 33M tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Authors report that when scaling with T, empirical task scaling follows theoretical O(D log(DT)) dependency (no detailed per-depth accuracy numbers given in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Matches theoretical scaling with D and T in practice for arithmetic tasks; D is small (2) so required training tasks scale modestly with T.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT converts arithmetic tasks into ARC(2,T) and empirical task-sample scaling follows theory: a modest number of training tasks suffices to generalize to exponentially many operator sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>CoT decomposition into intermediate results, sufficient number of training tasks sampled i.i.d., and adequate demonstration context length.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2022.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2022.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MultiStep-Translation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-step language translation chain experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task: translate a token through a randomly sampled chain of T languages (L1→L2→...→L_T); forms an ARC(D, T-1) with D number of languages. Empirical D-scaling follows theory but T-scaling exhibits linear degradation due to error accumulation in long chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (2-layer, 3-head, embedding dim 768) trained from scratch</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small encoder-decoder style Transformer? (paper states a 2-layer Transformer with 3 heads and embedding dim 768; trained on language-labeled tokens in next-token prediction setup for chain translation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small (2-layer, 3-head, emb=768)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer; per-step conditional generation depends on target language token prepended in demonstrations (autoregressive chain of translations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic / sequential translation</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step language translation chains</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Starting from a word in a language, translate it sequentially through T languages sampled with replacement (L1→L2→...→L_T); each step is a conditional generation depending on target language, giving ARC(D, T-1) structure with D languages.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>T-1 (sequence length of translations), experiments vary T and D</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>sequential composition of translation functions (language-to-language mapping composed across steps)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>task-level split: training chains sampled from language combinations vs testing on held-out chains (task-level OOD generalization); D and T scaling experiments performed.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised next-token prediction trained from scratch; total training samples 1e5, trained for 6250 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>D-scaling empirical behavior matches theoretical O(D ln(DT)). For T-scaling, empirical performance degrades approximately linearly with T (worse than theoretical logarithmic dependence), attributed to error accumulation across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Performance decreases approximately linearly with increasing T (reported deviation from theoretical O(ln T) due to compounding errors), D-scaling follows theoretical predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Empirical D-scaling consistent with ARC theory; T-scaling shows stronger dependence (linear) likely due to compounding errors in finite-sample regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Autoregressive compositional structure arises in multi-step translation; training on relatively few tasks allows generalization to huge combinatorial sets (authors report generalization to ~10^6 tasks in experiments), but long chains accumulate errors causing T to have a roughly linear negative effect on accuracy rather than logarithmic.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Longer translation chains suffer from cumulative error across steps; theoretical asymptotic bounds do not account for finite-sample error compounding.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Diverse per-step language coverage in training and moderate chain lengths to avoid compounding errors; sufficient training data (authors used 1e5 samples).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2022.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2022.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear-Probe-SecretIndex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear probing of hidden states to predict secret indices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Post-hoc linear probes trained on final-attention-layer hidden states to predict the i-th secret index in CoT parity tasks, used to test whether intermediate compositional structure is represented in model activations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Linear classifier probe on top of frozen Transformer checkpoints</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A linear classifier appended to final attention-layer hidden states; only the linear classifier is trained (Transformer weights frozen) to predict secret index tokens during CoT generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>linear probe (diagnostic classifier) interacting with transformer hidden states</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>representation analysis / interpretability for compositional tasks (parity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Linear probe prediction of secret indices in parity CoT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Train a linear classifier to predict the i-th secret index from the hidden state when generating the i-th CoT token; used to assess whether the model computes or encodes the indices during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>diagnostic probing of intermediate representations</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>training and validation tasks disjoint; probes trained on training tasks and validated on held-out tasks (details in D. Experiment Details)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Linear probe trained with Adam (lr=4e-5), batch size 32; transformer weights frozen; probe training sizes depend on d (20k, 20k, 50k for d=10,15,20 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Validation accuracies reported in Table 2 (paper indicates validation accuracy (%) per secret position but explicit numeric table entries not reproduced in main text); demonstrates that intermediate indices are represented to varying degrees.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Probe experiments run across d=10,15,20 with corresponding probe training sample sizes; specific numeric trends in Table 2 (not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Linear probes indicate that models trained with CoT encode information predictive of secret indices in their hidden states (supporting that CoT elicits internal representations aligned with the ARC decomposition), though exact accuracies are reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Sufficient probe training data and frozen transformer checkpoints trained with CoT; tasks and probe datasets kept disjoint.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Understanding in-context learning in transformers and LLMs by learning to learn discrete functions <em>(Rating: 2)</em></li>
                <li>How far can transformers reason? the locality barrier and inductive scratchpad <em>(Rating: 2)</em></li>
                <li>Transformers as statisticians: Provable in-context learning with in-context algorithm selection <em>(Rating: 1)</em></li>
                <li>How do in-context examples affect compositional generalization? <em>(Rating: 1)</em></li>
                <li>Do large language models have compositional ability? an investigation into limitations and scalability <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2022",
    "paper_id": "paper-276318034",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "ARC",
            "name_full": "AutoRegressive Compositional task class",
            "brief_description": "A formal task-class in which each task is an autoregressive composition of T conditional subtasks, each chosen from a finite family of size D, yielding D^T total tasks; used to analyze when training on Õ(D) tasks suffices to generalize to D^T tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Theoretical framework / task-class (not a neural model): tasks produce output sequences y_1..y_T via y_t ~ P_{θ_t}(· | x, y_&lt;t) where each θ_t ∈ Θ_t and |Θ_t| = D.",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": "modular autoregressive decomposition of tasks into T independent conditional subtasks",
            "task_domain": "theoretical / general compositional tasks (applies across domains)",
            "task_name": "ARC(D, T) (AutoRegressive Compositional class)",
            "task_description": "Each task f_θ is a tuple of conditional distributions (P_{θ1},...,P_{θT}); for any input x the model generates y_1..y_T autoregressively. Composition is across timesteps (sequential composition of subtasks); breadth D per step yields D^T tasks.",
            "compositional_depth": "T (generic)",
            "composition_type": "sequential/function composition (autoregressive composition of conditional subtasks)",
            "split_type": "train on subset of tasks F_train ⊂ F and test on unseen tasks F_test (task-level OOD generalization)",
            "training_strategy": "theoretical analysis: MLE to identify subtasks from training tasks and discrimination tests at inference; empirical instantiations use in-context learning with CoT",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": null,
            "baseline_comparisons": null,
            "architectural_comparison": null,
            "scale_effects": "Theorem: under mild identifiability, sampling n_θ = Õ(D) tasks uniformly at random suffices to identify all subtasks and generalize to D^T tasks (formal sample-complexity bounds in Thm 3.3 and Corollary 3.5).",
            "transfer_results": null,
            "key_findings": "Defines conditions (finite subtask families, identifiability via TV gaps) under which training on Õ(D) tasks yields exponential (D^T) task generalization; provides explicit sample-size bounds (n_θ ≥ D ln(100 D^T) etc.) and inference-sample bounds via TV-based discrimination.",
            "failure_analysis": "Theory assumes per-component coverage (training tasks must cover each subtask at every timestep); adversarial selection of training tasks can violate coverage and break generalization.",
            "success_conditions": "Per-component coverage in training (each subtask appears in F_train), identifiability (total-variation separation c&gt;0 between different subtasks), sufficient training tasks n_θ = Õ(D), and sufficient inference demonstration samples ℓ = O(log(...)/c^2).",
            "uuid": "e2022.0"
        },
        {
            "name_short": "Parity-CoT",
            "name_full": "Sparse parity with Chain-of-Thought (CoT) representation",
            "brief_description": "Empirical instantiation of ARC: parity functions Parity(d,k) are represented with CoT as an autoregressive sequence of partial XORs (y_1 = x_{i1}, y_2 = x_{i1}⊕x_{i2}, ...), converting an otherwise breadth-exponential single-step task into ARC(d,k) with breadth D=d and depth T=k; Transformers trained with CoT generalize to unseen parity tasks with Õ(d) training tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 style Transformer (trained from scratch)",
            "model_description": "Decoder-only Transformer trained from scratch on next-token prediction of demonstration sequences; CoT implemented by using intermediate tokens (sequence of partial XORs) as supervision and prediction targets.",
            "model_size": "small (3-layer, 1-head configuration used for parity/arithmetic experiments)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer decoder/attention; uses Chain-of-Thought as output token sequence (intermediate tokens) to expose autoregressive compositional structure",
            "task_domain": "mathematical / boolean function reasoning",
            "task_name": "Sparse Parity (Parity(d, k))",
            "task_description": "Given binary input x ∈ {0,1}^d, parity task selects k secret indices (i1..ik) and outputs the XOR of those positions. With CoT the target is the sequence of cumulative XORs of the secret indices across T=k steps, enabling autoregressive decomposition with D=d choices per step.",
            "compositional_depth": "T = k (they evaluate k ∈ {3,4,5,6,7} and up to larger values in scaling experiments)",
            "composition_type": "nested/exhaustive function composition (sequential XOR accumulation / function composition across timesteps)",
            "split_type": "task-level split: training tasks F_train sampled i.i.d. from Parity(d,k) and F_test is held-out unseen parity functions (novel secret-index tuples); also input-space split X_train/X_test for monitoring in-distribution generalization.",
            "training_strategy": "In-context learning (next-token prediction) on sequences of demonstrations (m examples per sequence); with CoT loss averaged over intermediate steps; trained from scratch (no pretraining).",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "High in-distribution accuracy reported when tasks are in train; specifics vary by setting but model achieves strong IID performance with sufficient samples (context length 40).",
            "compositional_performance": "With CoT, near-perfect generalization (&gt;95% test accuracy on held-out tasks) when trained on ≈3·d·ln(d) tasks (example: d=15, ~122 training tasks yields &gt;95% accuracy across k up to 7).",
            "generalization_gap": "Very large: ICL without CoT remains near chance (slightly above 50%) while CoT-enabled model achieves &gt;95% on unseen tasks — an effective gap of ~45+ percentage points between no-CoT and CoT conditions on task-level generalization.",
            "performance_by_depth": "Empirically stable across tested depths: for fixed d=15, k ∈ {3..7} the CoT model attains similar high accuracy (&gt;95%) when training tasks ≈3 d ln(d); i.e., required number of training tasks remains roughly constant with increasing k in the tested range.",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Baseline: standard ICL (no CoT) using the same Transformer architecture; result: ICL without CoT fails to generalize to unseen parity tasks (performance only slightly above chance) even as the number of training tasks increases. With CoT: near-perfect generalization. Also in-distribution ICL (no CoT) fails (Figure 6).",
            "architectural_comparison": "No alternative architectures compared in parity experiments beyond same Transformer with/without CoT representation.",
            "scale_effects": "Task-sample scaling: empirical results match theoretical Õ(d) scaling — training task count proportional to d ln(d) suffices; increasing d (10,15,20) and using proportional training tasks (3·d·ln(d)) yields consistent generalization.",
            "transfer_results": null,
            "key_findings": "Chain-of-Thought representation exposes autoregressive compositional structure and enables exponential task generalization: training on Õ(d) i.i.d. sampled tasks suffices to generalize to D^T parity tasks; empirical evidence: ~122 training tasks for d=15 generalize to ~6,400 unseen tasks with &gt;95% accuracy, stable across k.",
            "failure_analysis": "When training tasks are adversarially selected (non-i.i.d.), CoT+Transformer can fail — e.g., excluding all tasks with a particular positional coordinate in training causes chance-level performance on those withheld positional cases.",
            "success_conditions": "CoT representation of intermediate steps, i.i.d. random coverage of subtasks across training tasks (per-component coverage), sufficient number of distinct training tasks ≈ Õ(d), adequate context length (empirically context=40).",
            "uuid": "e2022.1"
        },
        {
            "name_short": "ICL-no-CoT",
            "name_full": "In-Context Learning without Chain-of-Thought",
            "brief_description": "Baseline condition where the Transformer is trained to map input x directly to final output y (no intermediate CoT tokens); empirically fails to generalize to unseen parity tasks and even to some in-distribution settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 style Transformer (same 3-layer,1-head configuration used in main experiments)",
            "model_description": "Standard next-token prediction training where outputs are single-step final labels rather than sequences of intermediate reasoning tokens (no CoT representation).",
            "model_size": "small (3-layer, 1-head)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer decoder; no explicit structural modifications beyond omission of intermediate tokens",
            "task_domain": "mathematical / boolean function reasoning (parity) and other synthetic tasks",
            "task_name": "Parity(d,k) and related tasks (no CoT condition)",
            "task_description": "Transformer is given demonstration pairs (x_j, f(x_j)) and must predict f(x_query) given context, with final-output-only supervision.",
            "compositional_depth": "T = 1 (single-step final output representation)",
            "composition_type": "non-decomposed single-step mapping (no autoregressive decomposition)",
            "split_type": "same task-level splits as CoT experiments (train tasks sampled, test tasks withheld)",
            "training_strategy": "ICL training on sequences; loss only on final labels (no intermediate-step supervision)",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Poor: even for in-distribution settings (trained and tested on same tasks but different input sequences) ICL without CoT struggles (Figure 6 shows failure in Parity(10,2) in-distribution).",
            "compositional_performance": "Very poor on unseen tasks: performance remains near chance even as number of training tasks increases (contrasts sharply with CoT condition).",
            "generalization_gap": "From near-chance (no CoT) to &gt;95% (with CoT) on held-out parity tasks — large positive effect from CoT.",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared directly with same architecture trained with CoT; CoT vastly outperforms no-CoT on task-level generalization.",
            "architectural_comparison": null,
            "scale_effects": "Increasing number of training tasks does not substantially improve task-level generalization without CoT (remains near chance).",
            "transfer_results": null,
            "key_findings": "Standard ICL without explicit intermediate reasoning (CoT) fails to exploit compositional structure required for task generalization in parity and related hierarchical problems; representation matters.",
            "failure_analysis": "Fails particularly on problems requiring hierarchical/hinted intermediate steps (e.g., parity).",
            "success_conditions": "N/A — this is the failure baseline; success requires conversion to CoT representation or other inductive bias that exposes subtasks.",
            "uuid": "e2022.2"
        },
        {
            "name_short": "Adversarial-Task-Selection",
            "name_full": "Adversarial / non-i.i.d. training task selection experiments",
            "brief_description": "Experiments showing that when training tasks are non-i.i.d. (adversarially excluding specific positional configurations), Transformers with CoT can fail to generalize to withheld configurations despite large training set size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 style Transformer (3-layer, 1-head)",
            "model_description": "Same architecture as parity experiments; CoT representation used unless noted.",
            "model_size": "small (3-layer, 1-head)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer decoder with CoT when applicable",
            "task_domain": "mathematical / boolean function reasoning (parity)",
            "task_name": "Parity(d=10, k=3) adversarial splits",
            "task_description": "Two adversarial removal setups: (1) Missing Coordinate: remove all training tasks where second coordinate s2=5; (2) Missing Pair: remove all training tasks that contain both indices 4 and 6 together. Evaluate generalization to tasks that include those excluded patterns.",
            "compositional_depth": "T = 3 (k=3)",
            "composition_type": "novel combination of seen primitives vs unseen positional configurations",
            "split_type": "non-iid / adversarial exclusion of specific coordinate or pair from training tasks",
            "training_strategy": "standard ICL+CoT training but with biased task selection that omits specific patterns",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "Dramatic dependency on which patterns are excluded: Missing Coordinate scenario (no training tasks with s2=5) leads to chance-level performance on tasks with s2=5; Missing Pair scenario (never seeing 4 and 6 together) still generalizes with high accuracy (96.2% reported in Table 3).",
            "generalization_gap": "Between adversarial exclusion cases: Missing Coordinate case yields ~chance vs Missing Pair yields ~96.2%, demonstrating that specific positional coverage matters more than mere element-level coverage.",
            "performance_by_depth": null,
            "performance_by_composition_type": "Shows sensitivity to positional/configurational coverage (positional diversity matters).",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared two adversarial selection regimes; both remove exponentially many tasks from training but have different generalization outcomes (chance vs 96.2%). Also contrasted with i.i.d. sampled training tasks which generalize well.",
            "architectural_comparison": null,
            "scale_effects": "Even very large F_train (exponentially large) can fail to generalize if per-component coverage/positional diversity is missing; i.i.d. sampling avoids these adversarial gaps.",
            "transfer_results": null,
            "key_findings": "Task selection matters: i.i.d. sampling of training tasks that ensures per-component coverage is crucial; adversarial exclusion of specific positional configurations (e.g., missing a coordinate) can break generalization even with many training tasks.",
            "failure_analysis": "Model fails primarily when an entire subtask (a given position value at a particular timestep) is never observed in training; positional diversity is essential.",
            "success_conditions": "Per-component coverage in training tasks (each subtask at each timestep seen at least once) — random i.i.d. sampling with n_θ = Õ(D) typically achieves this with high probability.",
            "uuid": "e2022.3"
        },
        {
            "name_short": "Arithmetic-CoT",
            "name_full": "Structured arithmetic tasks with Chain-of-Thought",
            "brief_description": "Arithmetic family where d binary inputs are combined left-to-right via ops in {+,×}; with CoT the task is ARC(2, d-1) and Transformers trained on Õ(2) tasks scale empirically as predicted, enabling generalization across exponentially many operator sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 style Transformer (3-layer, 1-head configuration used in parity/arithmetic experiments)",
            "model_description": "Decoder-only Transformer trained on next-token prediction with CoT supervision across intermediate arithmetic steps.",
            "model_size": "small (3-layer, 1-head)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer decoder; uses CoT intermediate tokens representing each binary operation result",
            "task_domain": "mathematical / arithmetic sequence evaluation",
            "task_name": "Arithmetic (sequence of operations over binary inputs)",
            "task_description": "Given binary inputs b1..bd and a sequence of d-1 operations op_t ∈ {+,×}, evaluate the left-to-right expression using CoT intermediate outputs; with D=2 and T=d-1 this is ARC(2, d-1).",
            "compositional_depth": "T = d-1 (varies with input dimension d); experiments scale T and D accordingly",
            "composition_type": "function composition of binary arithmetic operations (sequential operations)",
            "split_type": "task-level i.i.d. sampling across operator sequences for training vs withheld operator sequences for testing (task-level OOD)",
            "training_strategy": "In-context learning with CoT; reported total training examples: 25,000 (equally distributed across training tasks) for arithmetic experiments",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Not separately enumerated in text, but reported empirical scaling closely follows theoretical predictions under CoT.",
            "compositional_performance": "Empirical scaling matches theoretical O(D ln(DT)) dependence; authors state that training on a few hundred tasks enables generalization to very large exponential task spaces (example: 2^25 &gt; 33M tasks).",
            "generalization_gap": null,
            "performance_by_depth": "Authors report that when scaling with T, empirical task scaling follows theoretical O(D log(DT)) dependency (no detailed per-depth accuracy numbers given in text).",
            "performance_by_composition_type": null,
            "has_baseline_comparison": false,
            "baseline_comparisons": null,
            "architectural_comparison": null,
            "scale_effects": "Matches theoretical scaling with D and T in practice for arithmetic tasks; D is small (2) so required training tasks scale modestly with T.",
            "transfer_results": null,
            "key_findings": "CoT converts arithmetic tasks into ARC(2,T) and empirical task-sample scaling follows theory: a modest number of training tasks suffices to generalize to exponentially many operator sequences.",
            "failure_analysis": null,
            "success_conditions": "CoT decomposition into intermediate results, sufficient number of training tasks sampled i.i.d., and adequate demonstration context length.",
            "uuid": "e2022.4"
        },
        {
            "name_short": "MultiStep-Translation",
            "name_full": "Multi-step language translation chain experiments",
            "brief_description": "Task: translate a token through a randomly sampled chain of T languages (L1→L2→...→L_T); forms an ARC(D, T-1) with D number of languages. Empirical D-scaling follows theory but T-scaling exhibits linear degradation due to error accumulation in long chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (2-layer, 3-head, embedding dim 768) trained from scratch",
            "model_description": "Small encoder-decoder style Transformer? (paper states a 2-layer Transformer with 3 heads and embedding dim 768; trained on language-labeled tokens in next-token prediction setup for chain translation).",
            "model_size": "small (2-layer, 3-head, emb=768)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer; per-step conditional generation depends on target language token prepended in demonstrations (autoregressive chain of translations).",
            "task_domain": "linguistic / sequential translation",
            "task_name": "Multi-step language translation chains",
            "task_description": "Starting from a word in a language, translate it sequentially through T languages sampled with replacement (L1→L2→...→L_T); each step is a conditional generation depending on target language, giving ARC(D, T-1) structure with D languages.",
            "compositional_depth": "T-1 (sequence length of translations), experiments vary T and D",
            "composition_type": "sequential composition of translation functions (language-to-language mapping composed across steps)",
            "split_type": "task-level split: training chains sampled from language combinations vs testing on held-out chains (task-level OOD generalization); D and T scaling experiments performed.",
            "training_strategy": "Supervised next-token prediction trained from scratch; total training samples 1e5, trained for 6250 steps.",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "D-scaling empirical behavior matches theoretical O(D ln(DT)). For T-scaling, empirical performance degrades approximately linearly with T (worse than theoretical logarithmic dependence), attributed to error accumulation across steps.",
            "generalization_gap": null,
            "performance_by_depth": "Performance decreases approximately linearly with increasing T (reported deviation from theoretical O(ln T) due to compounding errors), D-scaling follows theoretical predictions.",
            "performance_by_composition_type": null,
            "has_baseline_comparison": false,
            "baseline_comparisons": null,
            "architectural_comparison": null,
            "scale_effects": "Empirical D-scaling consistent with ARC theory; T-scaling shows stronger dependence (linear) likely due to compounding errors in finite-sample regimes.",
            "transfer_results": null,
            "key_findings": "Autoregressive compositional structure arises in multi-step translation; training on relatively few tasks allows generalization to huge combinatorial sets (authors report generalization to ~10^6 tasks in experiments), but long chains accumulate errors causing T to have a roughly linear negative effect on accuracy rather than logarithmic.",
            "failure_analysis": "Longer translation chains suffer from cumulative error across steps; theoretical asymptotic bounds do not account for finite-sample error compounding.",
            "success_conditions": "Diverse per-step language coverage in training and moderate chain lengths to avoid compounding errors; sufficient training data (authors used 1e5 samples).",
            "uuid": "e2022.5"
        },
        {
            "name_short": "Linear-Probe-SecretIndex",
            "name_full": "Linear probing of hidden states to predict secret indices",
            "brief_description": "Post-hoc linear probes trained on final-attention-layer hidden states to predict the i-th secret index in CoT parity tasks, used to test whether intermediate compositional structure is represented in model activations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Linear classifier probe on top of frozen Transformer checkpoints",
            "model_description": "A linear classifier appended to final attention-layer hidden states; only the linear classifier is trained (Transformer weights frozen) to predict secret index tokens during CoT generation.",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": "linear probe (diagnostic classifier) interacting with transformer hidden states",
            "task_domain": "representation analysis / interpretability for compositional tasks (parity)",
            "task_name": "Linear probe prediction of secret indices in parity CoT",
            "task_description": "Train a linear classifier to predict the i-th secret index from the hidden state when generating the i-th CoT token; used to assess whether the model computes or encodes the indices during generation.",
            "compositional_depth": null,
            "composition_type": "diagnostic probing of intermediate representations",
            "split_type": "training and validation tasks disjoint; probes trained on training tasks and validated on held-out tasks (details in D. Experiment Details)",
            "training_strategy": "Linear probe trained with Adam (lr=4e-5), batch size 32; transformer weights frozen; probe training sizes depend on d (20k, 20k, 50k for d=10,15,20 respectively).",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Validation accuracies reported in Table 2 (paper indicates validation accuracy (%) per secret position but explicit numeric table entries not reproduced in main text); demonstrates that intermediate indices are represented to varying degrees.",
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": false,
            "baseline_comparisons": null,
            "architectural_comparison": null,
            "scale_effects": "Probe experiments run across d=10,15,20 with corresponding probe training sample sizes; specific numeric trends in Table 2 (not fully enumerated in text).",
            "transfer_results": null,
            "key_findings": "Linear probes indicate that models trained with CoT encode information predictive of secret indices in their hidden states (supporting that CoT elicits internal representations aligned with the ARC decomposition), though exact accuracies are reported in Table 2.",
            "failure_analysis": null,
            "success_conditions": "Sufficient probe training data and frozen transformer checkpoints trained with CoT; tasks and probe datasets kept disjoint.",
            "uuid": "e2022.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Understanding in-context learning in transformers and LLMs by learning to learn discrete functions",
            "rating": 2
        },
        {
            "paper_title": "How far can transformers reason? the locality barrier and inductive scratchpad",
            "rating": 2
        },
        {
            "paper_title": "Transformers as statisticians: Provable in-context learning with in-context algorithm selection",
            "rating": 1
        },
        {
            "paper_title": "How do in-context examples affect compositional generalization?",
            "rating": 1
        },
        {
            "paper_title": "Do large language models have compositional ability? an investigation into limitations and scalability",
            "rating": 1
        }
    ],
    "cost": 0.020722499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Task Generalization With AutoRegressive Compositional Structure: Can Learning From D Tasks Generalize to D T Tasks?
9 Jun 2025</p>
<p>Amirhesam Abedsoltan <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#97;&#98;&#101;&#100;&#115;&#111;&#108;&#116;&#97;&#110;&#64;&#117;&#99;&#115;&#100;&#46;&#101;&#100;&#117;">&#97;&#97;&#98;&#101;&#100;&#115;&#111;&#108;&#116;&#97;&#110;&#64;&#117;&#99;&#115;&#100;&#46;&#101;&#100;&#117;</a>. 
Department of Computer Science and Engineering
UC San Diego</p>
<p>Huaqing Zhang 
Institute for Interdisciplinary In-formation Sciences
Tsinghua University</p>
<p>Kaiyue Wen 
Stanford University</p>
<p>Hongzhou Lin 
Jingzhao Zhang 
Institute for Interdisciplinary In-formation Sciences
Tsinghua University</p>
<p>Mikhail Belkin 
Department of Computer Science and Engineering
UC San Diego</p>
<p>Halicioglu Data Science Institute
UC San Diego</p>
<p>Task Generalization With AutoRegressive Compositional Structure: Can Learning From D Tasks Generalize to D T Tasks?
9 Jun 2025FF791948FF1DD90371AD7C8DB0842445arXiv:2502.08991v2[cs.LG]
Large language models (LLMs) exhibit remarkable task generalization, solving tasks they were never explicitly trained on with only a few demonstrations.This raises a fundamental question: When can learning from a small set of tasks generalize to a large task family?In this paper, we investigate task generalization through the lens of autoregressive compositional structure, where each task is a composition of T operations, and each operation is among a finite family of D subtasks.This yields a total class of size D T .We first show that generalization to all D T tasks is theoretically achievable by training on only Õ(D) tasks.Empirically, we demonstrate that Transformers achieve such exponential task generalization on sparse parity functions via In-context Learning (ICL) and chain-of-thought (CoT) reasoning.We further show generalization in arithmetic and translation, beyond parity functions.</p>
<p>Introduction</p>
<p>Large language models (LLMs) demonstrate a remarkable ability to solve tasks they were never explicitly trained on.Unlike classical supervised learning, which typically assumes that the test data distribution follows the training data distribution, LLMs can generalize to new task distributions with just a few demonstrations-a phenomenon known as incontext learning (ICL) (Brown et al., 2020;Wei et al., 2022;Garg et al., 2022).Recent studies suggest that trained Transformers implement algorithmic learners capable of solving various statistical tasks-such as linear regression-at in-Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025.Copyright 2025 by the author(s).In this prototype experiment, tasks are sampled from the parity function family P arity(10, 2) with secret length k = 2 and bit length d = 10, totaling 45 tasks.To evaluate task generalization, we withhold a subset of tasks and train only on different subset of the remaining ones.Consistent with prior work (Bhattamishra et al., 2024), we observe that standard ICL fails to generalize across tasks.In contrast, incorporating Chain-of-Thought (CoT) reasoning significantly improves performance on unseen tasks.ference time in context (Li et al., 2023;Bai et al., 2023).Despite their success in tasks such as learning conjunctions or linear regression, Transformers relying solely on ICL struggle with more complex problems, particularly those requiring hierarchical reasoning.</p>
<p>A notable case where Transformers struggle with ICL is the learning of parity functions, as examined in (Bhattamishra et al., 2024).In this setting, a Transformer is provided with a sequence of demonstrations (x 1 , f (x 1 )), . . ., (x n , f (x n )) and is required to predict f (x query ) for a new input x query .Specifically, they focused on parity functions from the class Parity(10, 2), where each function is defined by a secret key of length k = 2 within a length space of d = 10.Each function f corresponds to a distinct learning task, resulting in 45 possible tasks.To assess generalization, a subset of tasks was held out during training.Their results demonstrate that Transformers trained via ICL fail to generalize to unseen tasks, even when the new tasks require only a simple XOR operation.These findings, along with other empirical studies (An et al., 2023;Xu et al., 2024), suggest that standard ICL struggles with tasks requiring hierarchical or compositional reasoning.</p>
<p>In contrast, we found that incorporating Chain-of-Thought (CoT) reasoning-introducing intermediate reasoning steps to the model-allows Transformers to easily generalize to unseen tasks, as illustrated in Figure 1.Consistent with (Bhattamishra et al., 2024), we observe that Transformers without CoT perform only slightly better than chance level, no matter how many training tasks are presented to the model.However, as the number of training tasks increases, Transformers with CoT achieve near-perfect generalization on the held-out set of unseen tasks.We see that the extra information provided by CoT enables the model to exploit the compositional structure of the parity problem.</p>
<p>Motivated by this example, we aim to systematically analyze how models can leverage autoregressive compositional structures to extend their capabilities beyond the training tasks.Conventionally, learning involves approximating a target function f * drawn from a function class F using examples from a training distribution over the input space X ; generalization is then measured by testing f * on new examples.In contrast, our focus is on task generalization, where training is restricted to a subset of functions or "tasks" F train ⊂ F, leaving the remaining functions, unseen during training.Our goal is to investigate whether a model trained on tasks from F train (with inputs from X ) can generalize to all tasks, including unseen tasks.This notion of task generalization goes beyond the standard out-of-distribution (OOD) settings (see, e.g., (Zhou et al., 2022) for review) by shifting the focus from adapting to new input distributions to learning entirely new tasks.Specifically, we ask:</p>
<p>How can we quantify the number of tasks a model must be trained on to generalize to the entire class F?</p>
<p>To analyze task generalization, we consider a finite set of functions F, where each function maps an input x ∈ X to a tuple of random variables y = (y 1 , . . ., y T ).We assume each function can be characterized by a parameter tuple θ = (θ 1 , θ 2 , . . ., θ T ).The outputs are generated autoregressively: first, y 1 is produced from x; then y 2 is generated from x and y 1 ; and then y 3 is generated from x, y 1 and y 2 ; and this process continues until y T is produced.Specifically, the sequence is generated sequentially as:
y t ∼ P θt (y t | x, y &lt;t ), for t = 1, . . . , T,
where y &lt;t = (y 1 , . . ., y t−1 ) denotes the previously generated outputs, and P θt is some conditional probability distribution that is parametrized by θ t and is conditioned on y &lt;t and x.This structure can also be interpreted as a sequence of compositions,
x P θ 1 − − → y 1 x, y 1 P θ 2 − − → y 2 . . . x, y 1 , . . . , y T −1 P θ T −1 − −−− → y T .
We will call this function class AutoRegressive Compositional structure.Assuming that the cardinality of the set of possible values for each parameter θ t is finite and is equal to D, we will use the notation F = ARC(T, D).The cardinality of this class is D T .</p>
<p>For the sparse parity problem with k secret keys in this framework, the output sequence has length T = k.Given an input x ∈ X = {0, 1} n , let the secret keys correspond to indices i 1 , i 2 , . . ., i k (in a predetermined order).The output sequence y = (y 1 , y 2 , . . ., y k ) is defined as follows,
y 1 = x i1 , y 2 = x i1 ⊕x i2 , . . . , y k = x i1 ⊕x i2 ⊕• • •⊕x i k .
That is, each y t recovers the XOR of the first t secret coordinates.In this example, the output distribution at each step is deterministic, assigning probability 1 to the correct XOR value and 0 to all other values.</p>
<p>We can now address the following fundamental question: How many tasks in F train must a model be trained on to generalize to all tasks in F, including those it has not seen?In particular, can a model trained on Õ(D) tasks generalize across the entire set of D T tasks?</p>
<p>Our main contributions are:</p>
<p>• We define AutoRegressive Compositional structure and introduce a framework to quantitatively analyze task generalization when the function class follows an Au-toRegressive Compositional structure.(Sections 3.2 and 3.3)</p>
<p>• We establish that under this structure, task generalization to all D T tasks is theoretically achievable by training on Õ(D) tasks up to logarithmic terms (Section 3.4).</p>
<p>• We demonstrate how the parity problem aligns with our framework and empirically show that Transformers trained on i.i.d.sampled tasks exhibit exponential task generalization via chain-of-thought (CoT) reasoning, consistent with theoretical scaling (Section 4).</p>
<p>• Finally, we show that the selection of training tasks significantly impacts generalization to unseen tasks.If tasks are chosen adversarially, training on even nearly all D T of the tasks with CoT may fail to generalize to the remaining tasks (Section 5.1).</p>
<p>Related Works</p>
<p>Composition and Generalization</p>
<p>The role of composition in reasoning for language models has been widely studied.(Saparov et al., 2023) explores various out-of-distribution (OOD) generalization formats, including compositional generalization, showing that a neural network's ability to generalize compositionally is highly dependent on both architecture and task properties.Similar conclusions have been drawn in prior works (Lake &amp; Baroni, 2018;Keysers et al., 2020).Further, (Bhattamishra et al., 2024;Dziri et al., 2023;An et al., 2023;Xu et al., 2024) examine compositional generalization in in-context learning (ICL) and find that generalization to composing multiple steps is in general hard for LLMs.One notable observation is that LLMs succeed in compositional generalization for clause satisfaction problems but not for parity problems.</p>
<p>Another line of research investigates composition as a mechanism underlying emergent abilities in language models.(Arora &amp; Goyal, 2023) demonstrates that language modeling can lead to learning tuples of skills, which are small compositions of fundamental capabilities.Building on this idea, (Kaur et al., 2025;Zhao et al., 2024) leverage compositional structures to generate supervised fine-tuning (SFT) data, leading to improved language model performance.</p>
<p>Beyond sequential composition, other forms of compositionality in neural networks have been explored.(Song et al., 2025) investigates layer-wise composition in transformers, while (Schug et al., 2024) proposes a modular neural architecture for learning hidden compositional representations.Additionally, (Wiedemer et al., 2024) examines compositional structures in image reconstruction, and (Lippl &amp; Stachenfeld, 2025) provides a theoretical analysis of composition in kernel and linear models.</p>
<p>While prior work has largely focused on qualitative insights into compositional generalization, our work takes a quantitative approach: studying how many training tasks are needed to achieve task generalization over an entire function class.</p>
<p>Learning and Testing with Multiple Distributions</p>
<p>Our work aims to analyze generalization when the training and testing distributions differ.This problem has been studied from various perspectives in the statistical learning community.One approach is to frame it as learning a shared representation across multiple tasks.(Ye et al., 2021) defines variation and informativeness between different environments based on a common representation, while (Arjovsky et al., 2019) addresses the problem by designing specific training objectives.Earlier studies on linear and kernel models also explore this direction (Du et al., 2017;Lei et al., 2021).</p>
<p>Another perspective considers the testing environment as a distribution shift, where the model may sample during inference to achieve domain adaptation.(Mansour et al., 2009) analyzes generalization error when a model is trained on distribution P but tested on a different distribution Q, introducing an error bias dependent on the distance d(P, Q).To mitigate this bias, (Cortes et al., 2010) proposes reweighting training samples when test samples from Q are available.</p>
<p>A related line of research investigates scenarios where both training and test samples are accessible.Notable setups include covariate shift (Kpotufe &amp; Martinet, 2021;Ma et al., 2023) and domain adaptation (Sugiyama et al., 2007;Ben-David &amp; Urner, 2014).When direct sampling from the test distribution is not feasible, alternative strategies focus on training robustly against worst-case shifts.This can be achieved through adversarial perturbations or min-max optimization formulations (Madry et al., 2018;Raghunathan et al., 2020;Duchi et al., 2023).</p>
<p>In</p>
<p>Theoretical Framework for Task Generalization</p>
<p>In this section, we present a theoretical framework to study task generalization with autoregressive compositional structure.When the compositional structure holds, we show that there exists a learning algorithm that is only trained on Õ(D) different tasks, but can generalize to exponentially many unseen tasks.</p>
<p>Preliminaries and Notations</p>
<p>For a positive integer n, denote
[n] = {1, 2, • • • , n}.
For a finte set S, we denote by ∆(S) the probability simplex over with support S. Given t sets S 1 , S 2 , . . ., S t , their Cartesian product is defined as
t × i=1 S i := {(s 1 , s 2 , . . . , s t ) | s i ∈ S i for all i ∈ [t]} .
We further denote S t := × t i=1 S. For two probability distributions P and Q over a discrete space S, the total variation distance is defined as
TV(P, Q) := 1 2 s∈S |P (s) − Q(s)| .
We let the bold letter y denote a sequence, and the subscripted y j denote the j th sequence / example.Within each sequence y j = (y 1 , ..., y T ), the regular letter y t denote the t th token in the sequence.</p>
<p>AutoRegressive Compositional Structure</p>
<p>In the following definition, we formally introduce the Au-toRegressive Compositional (ARC) task class, which models structured sequence generation through a composition of conditional distributions.Definition 3.1.(AutoRegressive Compositional task class).</p>
<p>Let X and Y denote the finite input and output spaces, respectively.The AutoRegressive Compositional (ARC) task class consists of sequential generation processes:
F := {f θ = (P θ1 , . . . , P θ T ) | P θt ∈ P Θt for all t ∈ [T ]} ,
where each task f θ ∈ F for any input x ∈ X generates an output sequence y = (y 1 , . . ., y T ) ∈ Y through an autoregressive sampling process:
y t ∼ P θt (• | x, y &lt;t ), for all t ∈ [T ].
At each step t, the conditional probability distribution P θt is drawn from a subtask family P Θt , parametrized by θ t :
P Θt := P θt (• | x, y &lt;t ) : X × Y t−1 → ∆(Y) | θ t ∈ Θ t .
Here, Θ t represents the parameter space at step t, and the overall task parameter space is Θ := × T t=1 Θ t .Assuming each step has a finite number of possible subtasks, i.e.,
|Θ t | = d for all t ∈ [T ], the AutoRegressive Compositional task class ARC(d, T ) consists of |F| = |Θ| = d T tasks.
Given any input x ∈ X and a sequence y ∈ Y T , the joint distribution for a task f θ = (P θ1 , • • • , P θ T ) ∈ F is:
P θ (x, y) = P (x) T s=1 P θs (y s | x, y &lt;s ),(1)
and for partial sequences up to any t ∈ [T ]:
P θ1:t (x, y 1:t ) = P x (x) t s=1 P θs (y s | x, y &lt;s ).(2)
At a high level, an AutoRegressive Compositional task class ARC(D, T ) is characterized by two key properties:</p>
<p>• Modularity.The generation process is decomposed into T sequential steps, each governed by an independent conditional distribution P θt ∈ P Θt .This modular structure allows tasks to be constructed by combining different components at each step.</p>
<p>• Exponential Growth.The task class size grows exponentially in T as |F| = D T , despite each step having only D choices.This reflects the combinatorial nature of task construction, where variations at each step lead to an exponentially large set of possible tasks.</p>
<p>Task Generalization</p>
<p>Under the autoregressive task learning setup, there are two levels of generalization:</p>
<ol>
<li>
<p>Generalizing to unseen inputs within a task.</p>
</li>
<li>
<p>Generalizing to unseen tasks in the class F.</p>
</li>
</ol>
<p>We focus on the latter one, referred as task generalization.</p>
<p>Training Phase During training, the model can only access to a small subset of tasks
F train = {f θ 1 , . . . , f θ n θ } ⊆ F with n θ = |F train |.
For each task f θ i ∈ F train , we observe n x i.i.d.demonstration samples:
D i = (x i,j , y i,j ) nx j=1 i.i.d. ∼ P θ i (x, y),
where P θ i is defined as in Equation ( 1).The full training dataset is the union of D i denoted by D train = {D i } n θ i=1 .We assume the learner does not know the true subtask conditional distribution families {P Θt } T t=1 a priori.Instead, it accesses to a larger hypothesis class:
P Ξt := P ζt (• | x, y &lt;t ) ζ t ∈ Ξ t ⊇ P Θt ,
where Ξ t parameterizes the learner's model class at step t.The goal of training is to identify the true subtask families P Θt from P Ξt through D train .</p>
<p>Inference Phase At test time, the learner is given ℓ inference-time demonstration samples:
D infer = {( xi , ỹi )} ℓ i=1 i.i.d. ∼ P θ (x, y),
where f θ ∈ F is an unseen task.The learner must identify the true conditionals {P θt } T t=1 from P Θt for each step t.Formally, the learner A that is trained on D train and given D infer as input, produces an output sequence of conditional distributions:
A (D infer ; D train ) ∈ {(P ξ1 , • • • , P ξ T ) | P ξt ∈ P Ξt , t ∈ [T ]}.</p>
<p>Main Result: Exponential Task Generalization</p>
<p>We now establish our main theoretical result: with the compositional structure in Definition 3.1, a learner can achieve exponential task generalization with only Õ(D) training tasks.This demonstrates how compositional structure fundamentally reduces the sample complexity of task learning from exponential to polynomial in D. Our results hold under the following mild assumptions: Assumption 3.2 (Compositional Identifiability).The autoregressive task class F satisfies:</p>
<ol>
<li>
<p>Finite Subtask Families.For each t ∈ [T ], the hypothesis class P Ξt is finite and the subtask conditional distribution family P Θt ⊆ P Ξt has size |P Θt | = D.</p>
</li>
<li>
<p>Task Identifiability.For any t ∈ [T ], θ 1:t−1 ∈ × t−1 s=1 Θ s , and θ t ∈ Θ t , ζ t ∈ Ξ t , P ζt ̸ = P θt , the induced distributions stasify:</p>
</li>
</ol>
<p>TV P θ1:t−1,θt , P θ1:t−1,ζt &gt; 0.</p>
<p>Furthermore, for any t ∈ [T ], θ 1:t−1 ∈ × t−1 s=1 Θ s , and θ t ̸ = θ ′ t ∈ Θ t , the induced distributions satisfy:
TV P θ1:t−1,θt , P θ1:t−1,θ ′ t ≥ c &gt; 0.
Under these conditions, we establish our main theorem: In other words, Theorem 3.3 shows that the learner can generalize from only Õ(D) tasks to an exponentially large number of unseen tasks, on the order of D T .The learning algorithm A operates in two stage.In the training stage, it applies a maximum-likelihood estimation (MLE) procedure to D i in order to identify the subtasks of the i-
th training task (P θ i 1 , • • • , P θ i T ).
In the inference stage, it then uses a total-variation-based distribution discrimination test (Lemma A.1) on inference-time demonstration samples D infer to recover f θ = (P θ1 , • • • , P θT ).The proof is deferred to Appendix A.1.Remark 3.4.If we additionally assume that every subtask distribution is separated from any incorrect hypothesis by a fixed total-variation margin, i.e. for all t ∈ [T ], θ 1:t−1 ∈ × t−1 s=1 Θ s , and θ t ∈ Θ t , ζ t ∈ Ξ t with P ζt ̸ = P θt , TV P θ1:t−1, θt , P θ1:t−1, ζt ≥ r &gt; 0, then one can replace the MLE procedure used in the training stage with the same distribution-discrimination approach from the inference stage (Lemma A.1).Under this condition, we can derive a non-asymptotic bound on the n x needed per task for accurate identification.See Appendix B for details.
parity S (b 1 , b 2 , . . . , b d ) = b i1 ⊕ b i2 ⊕ • • • ⊕ b i k ,
where ⊕ denotes the XOR (exclusive OR) operation.We define P arity(d, k) as the set of all parity functions with d variables and k secret indices, yielding a total of
|P arity(d, k)| = d k = O(d k ).
Representation Matters.Without Chain-of-Thought (CoT), the sparse parity problem P arity(d, k) is of ARC( d k , 1), but with CoT, it becomes an autoregressive compositional structure of ARC(d, k).
• No CoT → ARC d k , 1 . • With CoT → ARC(d, k).
Indeed, without CoT, the model maps input In contrast, with CoT (Abbe et al., 2024;Wen et al., 2025), the parity computation is decomposed into small steps:
x = (b 1 , . . . , b d ) directly to output y = b i1 ⊕ b i2 ⊕ • • • ⊕ b i k in a singley = (b i1 , b i1 ⊕ b i2 , . . . , b i1 ⊕ • • • ⊕ b i k ),
enabling a structured representation that reduces the breadth.</p>
<p>More precisely, at step t, the class of subtask is exactly defined by the XOR operation with previous token and one secret index :
P (t,it) (y t |x, y &lt;t ) = 1[y t = y t−1 ⊕ b it ].</p>
<p>Experiments: Parity Problem Case Study</p>
<p>As we have shown, there exists a learning algorithm that is only trained on Õ(d) different tasks to fully generalize on all the tasks in P arity(d, k).However, from a practical standpoint, it is not clear whether a Transformer can actually match this task complexity.In this section, we present empirical evidence demonstrating that a standard Transformer can indeed learn the sparse parity function with CoT using Õ(d) training tasks.The paper's GitHub repository can be found online.</p>
<p>Experimental Setup: In-Context Learning</p>
<p>Our empirical setup is a refinement of the theoretical framework presented in Section 3.3, and closely follows that of (Garg et al., 2022;Bhattamishra et al., 2024).In this setup, a sequence model M (such as Transformers) is trained using N sequences, each sequence consisting of m demonstration samples (x 1 , y 1 , . . ., x m , y m ).The model is trained for the next token-prediction task, except that we only consider y j in loss optimization: for each context (x 1 , y 1 , . . ., x j−1 , y j−1 , x j ), the model predicts ŷj , and the loss is given by 1 m m j=1 ℓ( ŷj , y j ).In our experiment, we use cross-entropy loss to measure the discrepancy between the predicted output ŷj and the true output y j .</p>
<p>When Chain of Thought (CoT) is used, each y j is itself a sequence of length k representing intermediate reasoning steps.In this case, the loss is the average on all these intermediate steps.</p>
<p>Training Data Generation.We split both the task space and input space into training and testing.In other words, the parity tasks are split into F train and F test ; and the binary sequences are split into X train and X test .The split in the input space helps us monitor the in-distribution training process while as the split in the task space aims to measure generalization to unseen parity functions.</p>
<p>To construct the ICL data, we sample m points x 1 , . . ., x m uniformly at random from X train .Similarly, we sample a function f uniformly at random from F train , and generate the sequence (x 1 , f (x 1 ), . . ., x m , f (x m )).This process is repeated N times, each time with a fresh sample of m points and a new function f .Evaluating Task Generalization.For evaluation, we sample f randomly from the held-out tasks F test and sample inputs uniformly at random from X test .We report the accuracy of the prediction f (x m ) given the demonstration (x 1 , f (x 1 ), . . ., x m−1 , f (x m−1 ), x m ).This setting challenges the model to generalize to novel tasks beyond those encountered during training.To investigate this, we conduct experiments on:</p>
<p>Experimental Results</p>
<p>As</p>
<ol>
<li>Scaling T (= k), i.e. the length of secret indices.</li>
</ol>
<p>Scaling D(= d), i.e. the ambient dimension of input.</p>
<p>Scaling T for a Fixed D. We examine how the number of training tasks affects test accuracy while keeping the ambient dimension fixed at d = 15.Specifically, we evaluate test accuracy for k = {3, 4, 5, 6, 7} under varying numbers of training tasks.As k increases, the size of the parity class grows significantly-from approximately 500 for k = 3 to around 6500 for k = 7.</p>
<p>Remarkably, despite this increase, the test accuracy follows a similar trajectory.With just 3d ln(d) ≈ 122 training tasks, the model generalizes to unseen cases with high accuracy (&gt; 95%).For k = 7, this means training on 122 tasks enables generalization to about 6,400 unseen ones!This empirical results suggests that the required number of training tasks remains roughly the same, regardless of k, consistent with the theoretical scaling of Õ(d) tasks.</p>
<p>Scaling D for a fixed T .We examine the effect of increasing d ∈ {10, 15, 20} while keeping k = 3 fixed.For each d, we train on a total number of tasks proportional to d ln(d), up to 4 × d ln(d).the model is trained on a total of 3 × d ln(d) i.i.d.tasks.Table 1 shows that generalization performance remains consistent across these settings, providing further evidence that Õ(d) training tasks are sufficient to generalize to unseen parity tasks using CoT prompting.</p>
<p>Task Generalization Beyond i.i.d. Sampling and Parity Functions</p>
<p>In this section, we extend our experiments beyond i.i.d.task sampling and parity functions.We show an adversarial example where biased task selection substantially hinders task generalization for sparse parity problem.In addition, we demonstrate that exponential task scaling extends to a nonparity tasks including arithmetic and multi-step language translation.</p>
<p>Task Generalization Beyond i.i.d. Task Sampling</p>
<p>In previous sections, we focused on i.i.d.settings, where the set of training tasks F train were sampled uniformly at random from the entire class F. Here, we explore scenarios that deliberately break this uniformity to examine the effect of task selection on out-of-distribution (OOD) generalization.</p>
<p>How does the selection of training tasks influence a model's ability to generalize to unseen tasks?Can we predict which setups are more prone to failure?</p>
<p>To investigate this, we consider two cases parity problems with d = 10 and k = 3, where each task is represented by its tuple of secret indices (s 1 , s 2 , s 3 ):</p>
<ol>
<li>
<p>Generalization with a Missing Coordinate.In this setup, we exclude all training tasks where the second coordinate takes the value s 2 = 5, such as (1, 5, 7).At test time, we evaluate whether the model can generalize to unseen tasks where s 2 = 5 appears.</p>
</li>
<li>
<p>Generalization with Missing Pair.Here, we remove all training tasks that contain both 4 and 6 in the tuple (s 1 , s 2 , s 3 ), such as (2, 4, 6) and (4, 5, 6).At test time, we assess whether the model can generalize to tasks where both 4 and 6 appear together.</p>
</li>
</ol>
<p>If you had to guess.Which scenario is more challenging for generalization to unseen tasks?We provide the experimental result in Table 3.</p>
<p>In the first scenario, despite being trained on all tasks except those where s 2 = 5, which is of size O(D T ), the model struggles to generalize to these excluded cases, with prediction at chance level.This is intriguing as one may expect model to generalize across position.The failure suggests that positional diversity plays a crucial role in the task generalization of Transformers.</p>
<p>In contrast, in the second scenario, though the model has never seen tasks with both 4 and 6 together, it has encountered individual instances where 4 appears in the second position (e.g., (1, 4, 5)) or where 6 appears in the third position (e.g., (2,3,6)).This exposure appears to facilitate generalization to test cases where both 4 and 6 are present.</p>
<p>As a result, when the training tasks are not i.i.d, an adversarial selection such as exclusion of specific positional configurations may lead to failure to unseen task generalization even though the size of F train is exponentially large.</p>
<p>Task Generalization</p>
<p>MULTI-STEP LANGUAGE TRANSLATION TASK</p>
<p>In this task, we study a sequential translation process across multiple languages (Garg et al., 2022).Given a set of D languages, we construct a translation chain by randomly sampling a sequence of T languages with replacement: L 1 , L 2 , . . ., L T , where each L t is a sampled language.</p>
<p>Starting with a word, we iteratively translate it through the sequence:
L 1 → L 2 → L 3 → • • • → L T .
For example, if the sampled sequence is EN → FR → DE → FR, translating the word "butterfly" follows:</p>
<p>butterfly → papillon → schmetterling → papillon.</p>
<p>This task follows an AutoRegressive Compositional structure by itself, specifically ARC(D, T − 1), where at each step, the conditional generation only depends on the target language, making D as the number of languages and the total number of possible tasks is D T −1 .This example illustrates that autoregressive compositional structures naturally arise in real-world languages, even without explicit CoT.</p>
<p>We examine task scaling along D (number of languages) and T (sequence length).As shown in Figure 4, empirical D-scaling closely follows the theoretical O(D ln DT ).However, in the T -scaling case, we observe a linear dependency on T rather than the logarithmic dependency O(ln T ).A possible explanation is error accumulation across sequential steps-longer sequences require higher precision in intermediate steps to maintain accuracy.This contrasts with our theoretical analysis, which focuses on asymptotic scaling and does not explicitly account for compounding errors in finite-sample settings.</p>
<p>Despite this, the task scaling is still remarkable -training on a few hundred tasks enables generalization to 4 10 ≈ 10 6 tasks!</p>
<p>Conclusions</p>
<p>In this work, we quantitatively investigated task generalization under the autoregressive compositional structure, demonstrating both theoretically and empirically that exponential task generalization to D T tasks can be achieved with training on only Õ(D) tasks.To summerize:</p>
<p>• Theoretical Framework for Task Generalization.</p>
<p>We introduced the AutoRegressive Compositional (ARC) framework to model structured task learning, demonstrating that a model trained on only Õ(D) tasks can generalize to an exponentially large space of D T tasks.</p>
<p>• Formal Sample Complexity Bound.We established a fundamental scaling law that quantifies the number of tasks required for generalization, proving that exponential generalization is theoretically achievable with only a logarithmic increase in training samples.</p>
<p>• Empirical Validation on Parity Functions.We showed that Transformers struggle with standard incontext learning (ICL) on parity tasks but achieve exponential generalization when Chain-of-Thought (CoT) reasoning is introduced.Our results provide the first empirical demonstration of structured learning enabling efficient generalization in this setting.</p>
<p>• Scaling Laws in Arithmetic and Language Translation.Extending beyond parity functions, we demonstrated that the same compositional principles hold for arithmetic operations and multi-step language translation, confirming that structured learning significantly reduces the task complexity required for generalization.</p>
<p>• Impact of Training Task Selection.We analyzed how different task selection strategies affect generalization, showing that adversarially chosen training tasks can hinder generalization, while diverse training distributions promote robust learning across unseen tasks.</p>
<p>We introduce a framework for studying the role of compositionality in learning tasks and how this structure can significantly enhance generalization to unseen tasks.Additionally, we provide empirical evidence on learning tasks, such as the parity problem, demonstrating that transformers follow the scaling behavior predicted by our compositionality-based theory.Future research will explore how these principles extend to real-world applications such as program synthesis, mathematical reasoning, and decision-making tasks.</p>
<p>By establishing a principled framework for task generalization, our work advances the understanding of how models can learn efficiently beyond supervised training and adapt to new task distributions.We hope these insights will inspire further research into the mechanisms underlying task generalization and compositional generalization.</p>
<p>A. Proofs in Section 3</p>
<p>Notations For a distribution over finite class S, denote supp(P ) ⊆ S as the support of P .For two distributions P, Q over finite set S, denote the cross entropy as
H(P, Q) = − x∈S P (x) log Q(x),
and denote the KL divergence as
KL(P, Q) = − x∈S P (x) log Q(x) P (x) .
A.1.Proof of Theorem 3.3</p>
<p>To identify the true distribution θ from Θ at inference time, we need the following lemma to provide a non-asymptotic bound for distribution discrimination.Lemma A.1 (Non-Asymptotic Discrimination of Two Distributions).Let S be any finite set and P, Q ∈ ∆(S) be two distributions with total variation distance TV(P, Q) = c &gt; 0. Suppose we observe n independent samples X 1 , . . ., X n from an unknown distribution Y , where Y is either P or Q.Then, there exists a testing algorithm that identifies Y with probability at least 1 − δ provided that
n ≥ 2 ln(1/δ) c 2 .
Proof.Testing Procedure.Define the testing statistic ϕ as follows:
ϕ = 1 n n i=1 (−1) 1[P (Xi)&lt;Q(Xi)] .
Equivalently, ϕ can be written in terms of the empirical distribution Ŷn :
ϕ = x∈S Ŷn (x) • (−1) 1[P (x)&lt;Q(x)] ,
where
Ŷn (x) = 1 n n i=1 1[X i = x].
Compute the expected values of ϕ under Y = P and Y = Q:
µ P = x∈S P (x) • (−1) 1[P (x)&lt;Q(x)] , µ Q = x∈S Q(x) • (−1) 1[P (x)&lt;Q(x)] .</p>
<p>The algorithm reports
Ŷ = P if |ϕ − µ P | &lt; |ϕ − µ Q |, and Ŷ = Q otherwise.
Proof of Correctness.Without loss of generality, assume Y = P .We analyze the probability of error:
Pr Ŷ ̸ = P = Pr (|ϕ − µ P | ≥ |ϕ − µ Q |) .
Note that E[ϕ] = µ P under Y = P .From the definition of total variation distance:
µ Q − µ P = x∈S (Q(x) − P (x)) • (−1) 1[P (x)&lt;Q(x)] = 2 • d TV (P, Q) = 2c.
Thus, the error probability can be bounded as:
Pr (|ϕ − µ P | ≥ |ϕ − µ Q |) ≤ Pr (ϕ ≥ µ P + c) .
Since ϕ is the average of n independent random variables taking values in {−1, +1}, by Hoeffding's inequality:
Pr (ϕ ≥ µ P + c) ≤ exp − c 2 n 2 .
Setting exp − c 2 n 2 ≤ δ gives the required sample complexity n ≥ 2 ln(1/δ) c 2 .</p>
<p>To prove Theorem 3.3, we introduce the following lemma:</p>
<p>Lemma A.2. Consider a compositional task class F satisfying Assumption 3.2.Suppose the training tasks F train = {f θ i } n θ i=1 satisfy the per-component coverage condition: for every timestep t ∈ [T ],
{P θ i t } n θ i=1 = P Θt .
Then there exists a learner A such that when trained on D train = {D i } n θ i=1 with D i i.i.d.</p>
<p>∼ P θ i where D i consists of n x i.i.d.samples, and given ℓ ≥ 2 ln(100T n θ ) c 2 inference-time demonstration samples i.i.d.sampled from unseen task P θ , denoted by D infer , then it holds that: lim nx→∞ Pr A(D infer ; D train ) ̸ = (P θ1 , . . ., P θT ) ≤ 0.01, where the probability is over the randomness in D train and D infer .</p>
<p>Proof.</p>
<p>Step 1: Training Stage.</p>
<p>We construct a learning algorithm that recovers for t = 1 to T do 3:
f θ i = P θ i 1 , . . . , P θ i T from D i = {(x i,j , y i,j )} nx j=1 i.i.d. ∼ P θ i (x, y),
Observed Support t ← {(x i,j , y i,j 1:t )} We now show that, as n x → ∞,
Pr P Θt = P Θt ∀ t ∈ [T ] → 1. Assume θi 1:t−1 = θ i 1:t−1 . As n x → ∞, Pr Observed Support t ̸ = supp P θ i 1:t ≤ (x,y1:t) ∈ supp P θ i 1:t Pr (x i,j , y i,j 1:t ) ̸ = (x, y 1:t ) ∀ j ∈ [n x ] → 0.
Conditioned on Observed Support t = supp P θ i 1:t , any P ξt in P ′ Ξt must share the same support:
supp P θ i 1:t−1 , ξt = supp P θ i 1:t−1 , θ i t .
Thus the cross-entropy is finite.By the law of large numbers and Pinsker's inequality, for P ξt ̸ = P θ i t we have</p>
<p>Proof of Theorem 3.3.By Lemma A.2, it suffices to verify that per-component coverage condition holds for each timestep with high probability when n θ = D ln 100 D T .</p>
<p>For each timestep t ∈ [T ], observe
Pr {P θ i t } n θ i=1 ̸ = P Θt ≤ P θ t ∈PΘ t Pr P θ i t ̸ = P θt ∀ i ∈ [n θ ] = D 1 − 1 D n θ .
Hence,
Pr ∃ t ∈ [T ] : {P θ i t } n θ i=1 ̸ = P Θt ≤ T t=1 Pr {P θ i t } n θ i=1 ̸ = P Θt ≤ D T 1 − 1 D n θ &lt; D T e −n θ /D = 0.01.
This completes the proof.</p>
<p>A.2. Proof of Corollary 3.5</p>
<p>Proof of Corollary 3.5.It suffices to verify that for the sparse parity problem, the constant c in Assumption 3.2 is 1 2 .We denote
P i1,••• ,it (x, y &lt;t ) = 1 2 d × 1[y 1 = x i1 ] t s=2 1[y s = y s−1 ⊕ x is ]. For any i 1 , • • • , i t−1 ∈ [d], and i t ̸ = i ′ t ∈ [d] and it holds that TV(P i1,••• ,it−1,it , P i1,••• ,it−1,i ′ t ) = 1 2 n x∈{0,1} d 1[x i1 ⊕ • • • ⊕ x it−1 ⊕ x it ̸ = x i1 ⊕ • • • ⊕ x it−1 ⊕ x i ′ t ] = 1 2 n x∈{0,1} d 1[x it ̸ = x i ′ t ] = 1 2 .
This completes the proof.</p>
<p>B. Non-Asymptotic Analysis of Training-Time Demonstration Sample Complexity</p>
<p>In Theorem 3.3, we established only an asymptotic result, showing that as the number of demonstration samples per task at training time n x → ∞, the probability of correctly identifying subtask families P Θt tends to one.However, by imposing an additional assumption on the total variation gap between the true distributions and any other hypotheses, it is possible to derive a non-asymptotic guarantee on how large n x must be for accurate subtask identification.</p>
<p>Although maximum-likelihood estimation (MLE) does not directly yield such a non-asymptotic bound in this setting, we can use the same distribution discrimination approach introduced in the inference stage (Lemma A.1). for t = 1 to T do  ICL with no CoT fails in even in-distribution generalziation.We observe in Figure 6 that transformers with ICL and no CoT struggle to generalize even in simpler in-distribution settings as the number of tasks increases.In the parity task, we refer to in-distribution generalization as a setting where the model is trained on F train tasks and S train sequences, and then evaluated on the same set of tasks F train but with entirely new sequences S test that were not seen during training.</p>
<p>Here, the setting is the same as in (Bhattamishra et al., 2024) for Parity(10, 2), but we used the same tasks during both training and testing.We trained on half of the total sequences, 2 9 and tested on unseen sequences while keeping the tasks unchanged.</p>
<p>D. Experiment Details</p>
<p>Model and optimization.We used the transformers library from Hugging Face (Wolf et al., 2020) to instantiate and train our GPT-2 model from scratch.In all experiments, we used a 3-layer, 1-head configuration.We used the Wadam optimizer (Kingma &amp; Ba, 2015) with a learning rate of 8 × 10 −5 and a batch size of 64.</p>
<p>Parity and arithmetic.In all experiments shown in Figures 2 and 3 for both parity and arithmetic tasks, we used a context length of 40.</p>
<p>For the arithmetic problem, across all dimensions, we used a total of 25,000 training examples, equally distributed across the training tasks.</p>
<p>For the parity problem, we used 20,000 training samples, equally distributed across the training tasks for dimensions up to 15.For dimension 20, we increased the total number of training samples to 50,000.</p>
<p>At testing time, we always randomly select the minimum between 200 subsets and all remaining tasks, each containing 500 different sequences with the same context length of 40.</p>
<p>Language experiments.For the translation experiments, we train a 2-layer Transformer with 3 heads and embedding dimension 768.We use an Adam optimizer with betas being 0.9, 0.95 and learning rate 3e-4.We will keep the number of total training samples to be 1e5 and train for 1 pass for 6250 steps.We choose the languages randomly from the following set {English, F rench, Spanish, Chinese, German, Italian, Japanese, Russian, P ortuguese, Arabic} and meanings (in English) from {cat, dog, house, apple, sky, car, road , tree, bed, water, sun, moon}.We use a GPT-2 tokenizer and in our demonstrations, we will prepend the language of the corresponding word before each word in the following format like "English: cat".</p>
<p>Linear Probing We append a linear classifier to the checkpoints of models of "Increasing D for a fixed T " tasks, trained on the hidden states of the final attention layer when generating the i-th token in the Chain-of-Thought, with the goal of predicting the i-th "secret index."The models are trained on a total number of of 20, 000, 20, 000, and 50, 000 training samples for d = 10, 15, and 20, respectively.The tasks used for training and validation are disjoint.Only the linear classifier is trained, while the parameters of the transformer are frozen.We use the Adam optimizer with a learning rate of 4 × 10 −5 , and the batch size is set to be 32.</p>
<p>Figure 1 .
1
Figure 1.We train a Transformer to learn parity functions through In-Context Learning (ICL): given a demonstration sequence (x1, f (x1)), . . ., (xn, f (xn)), infer the target f (xquery) from a new input xquery.Each function f defines a distinct learning task.In this prototype experiment, tasks are sampled from the parity function family P arity(10, 2) with secret length k = 2 and bit length d = 10, totaling 45 tasks.To evaluate task generalization, we withhold a subset of tasks and train only on different subset of the remaining ones.Consistent with prior work(Bhattamishra et al., 2024), we observe that standard ICL fails to generalize across tasks.In contrast, incorporating Chain-of-Thought (CoT) reasoning significantly improves performance on unseen tasks.</p>
<p>Theorem 3.3 (Exponential Task Generalization).Let F be an AutoRegressive Compositional(ARC) task class satisfying Assumption 3.2.Then there exists a learner A with the following property: if during training, one samples n θ ≥ D ln 100 D T tasks uniformly and independently from F, each provided with n x i.i.d.demonstration samples as the training dataset, and if at inference one observes ℓ ≥ 2 ln 100 T n θ c 2 i.i.d.demonstration samples from a previously unseen task P θ ∈ F, then lim nx→∞ Pr A D infer ; D train ̸ = P θ1 , . . ., P θT ≤ 0.02, where D train and D infer denote the training dataset and inference-time demonstration samples respectively, and the probability is taken over the random selection of training tasks F train ⊆ F, the training data D train , and the inference-time demonstration samples D infer .</p>
<p>3.5.Example: Sparse Parity Problem To illustrate the role of the AutoRegressive Compositional structure, we use sparse parity problem as an example.Sparse Parity Problem.Given d binary variables x = (b 1 , b 2 , . . ., b d ), a sparse parity function selects k secret indices S = {i 1 , i 2 , . . ., i k } and outputs 1 if the sum of the corresponding variables is odd, and 0 otherwise:</p>
<p>step, hence we have length T = 1 and the breadth D = |P arity(d, k)| = O(d k ).Such exponential dependency on the breadth suggests that one would need to train on O(d k ) tasks, explaining what we are observing unsuccessful task generalization in the introduction figure.</p>
<p>where i t is the t-th secret index, by default lying ∈ [1, d].Therefore, the breadth D at each step is exactly d.This said, the CoT effectively reduces the breadth from O(d k ) to d.According to Theorem 3.3, Corollary 3.5.For the sparse parity problem described above, we can show that the parameter c in Assumption 3.2 is 1 2 , thus when n θ ≥ d ln(100kd), ℓ ≥ 8 ln(100kn θ ) , it holds that lim nx→∞ Pr A (D infer ; D train ) ̸ = (P (1,i1) , • • • , P (k,i k ) ) ≤ 0.02.In other words, the family of P arity(d, k) with CoT is learnable with O(d log(d)) training tasks.</p>
<p>discussed in Section 3.5, introducing CoT transforms the parity problem class P arity(d, k) into an AutoRegressive Compositional structure ARC(D, T ) with D = d and T = k.A key empirical question is: how does the number of training tasks scale with d and T to achieve a fixed target accuracy on unseen task sets?</p>
<p>Figure 2 ,
2
Panel B, shows similar task generalization performance across different ambien dimension of d, providing further evidence that Õ(d) i.i.d.training tasks are sufficient for generalization to unseen tasks on praity functions with CoT.Scaling D and T together.We examine the effect of jointly increasing d and k, with (d, k) ∈ {(10, 5), (15, 7), (20, 10), (25, 12), (30, 15)}.For each d,</p>
<p>Figure 2 .
2
Figure 2. Test accuracy on unseen tasks.For parity task: D = d as the ambient dimension and T = k as the number of secret indices.We show that the empirical scaling closely follows the theoretical scaling of D ln(D).(A) For a fixed D = 15, as T increases, the test accuracy on unseen tasks remains similar, even though the total number of tasks (∼ D T ) grows exponentially with T .(B) For a fixed secret length is 3, as D increases, the number of tasks grows polynomially with D, yet the number of tasks required to generalize reasonably to unseen tasks remains in ∝ D log D.</p>
<p>Beyond Parity Problems 5.2.1.ARITHMETIC TASK We introduce the family of Arithmetic task that, like the sparse parity problem, operates on d binary inputs b 1 , b 2 , . . ., b d .The task involves computing a structured arithmetic expression over these inputs using a sequence of addition and multiplication operations.Formally, we define the function: Arithmetic S : {0, 1} d → {0, 1, . . ., d}, where S = (op 1 , op 2 , . . ., op d−1 ) is a sequence of d − 1 operations, each op k chosen from {+, ×}.The function evaluates the expression by applying the operations sequentially from left-to-right order: for example, if S = (+, ×, +), then the arithmetic function would compute Arithmetic S (b 1 , b 2 , b 3 , b 4 ) = ((b 1 + b 2 ) × b 3 ) + b 4 .By introducing a step-by-step CoT, arithmetic class belongs to ARC(2, d − 1): this is because at every step, there is only D = |Θ t | = 2 choices (either + or ×) while the length is T = d − 1, resulting a total number of 2 d−1 tasks.Task generalization for the arithmetic task with CoT.It has d = 2 and T = d − 1 as the ambient dimension, hence D ln(DT ) = 2 ln(2T ).We show that the empirical scaling closely follows the theoretical scaling.Notably, when scaling with T , we observe in the figure above that the task scaling closely follow the theoretical O(D log(DT )) dependency.Given that the function class grows exponentially as 2 T , it is truly remarkable that training on only a few hundred tasks enables generalization to an exponentially larger space-on the order of 2 25 &gt; 33 Million tasks.This exponential scaling highlights the efficiency of structured learning, where a modest number of training examples can yield vast generalization capability.</p>
<p>Figure 4 .
4
Figure 4. Task generalization for language translation task: D is the number of languages and T is the length of steps.</p>
<p>which is part of the training set D train .The procedure is shown below: Algorithm 1 Training Stage Require: Training set D train = {D i } n θ i=1 1: for i = 1 to n θ do 2:</p>
<p>Length.The theory assumes access to an infinite number of examples for each training task but does not require infinite demonstrations during inference.However, in practice, we cannot train on an infinite number of examples.Figure5shows that providing sufficient context length during both training and inference is crucial for strong performance.Empirically, we observed that a context length of 40 works reasonably well across all experiments with dimensions up to d = 20.</p>
<p>Figure 5 .
5
Figure 5.The effect of context length on performance.</p>
<p>Figure 6 .
6
Figure 6.ICL without CoT even fails to generalize in distribution.</p>
<p>Table 2 .
2
Validation accuracy (%) of linear probes trained to predict each secret token position from the final hidden state.</p>
<p>Table 3 .
3
Generalization Results for Scenarios 1 and 2 for d = 10, k = 3.ScenarioTasks excluded from training Generalization accuracy Generalization with Missing Pair {4, 6} ⊆ {s 1 , s 2 , s 3 }
96.2%</p>
<p>Assumption B.1 (Compositional Identifiability with fixed tv marigin).The autoregressive task class F satisfies: 1. Finite Subtask Families: For each t ∈ [T ], the hypothesis class P Ξt is of size at most H and the subtask conditional distribution family P Θt ⊆ P Ξt has size |P Θt | = D. 2. Task Identifiability: For any t ∈ [T ], θ 1:t−1 ∈ × t−1 s=1 Θ s , and θ t ∈ Θ t , ζ t ∈ Ξ t , P ζt ̸ = P θt , the induced distributions stasify: Theorem B.2 (Exponential Task Generalization).Let F be an autoregressive compositional task class satisfying Assumption 3.2.Then there exists a learner A with the following property: if during training, one samples n θ tasks uniformly and independently from F, each provided with n x i.i.d.demonstration samples as the training dataset, and if at inference one observes ℓ i.i.d.demonstration samples from a previously unseen task P θ ∈ F, then Pr A D infer ; D train ̸ = P θ1 , . . ., P θT ≤ DT e −n θ /D + n θ T e −c 2 ℓ/2 + n θ T He −r 2 nx/2 .where D train and D infer denote the training dataset and inference-time demonstration samples respectively, and the probability is taken over the random selection of training tasks F train ⊆ F, the training data D train , and the inference time demonstration samples D infer .Proof.Denote the hypothesis class P Ξt = {P ξt,1 , • • • , P ξ t,|Ξ t | }, we present the training stage of the learner.Algorithm 3 Training Stage with Distribution Dislimination Require: Training set D train = {D i } n θ</p>
<p>TV P θ1:t−1,θt , P θ1:t−1,ζt ≥ r &gt; 0.Furthermore, for any timestep t ∈ [T ], θ 1:t−1 ∈ × t−1 s=1 Θ s , and θ t ̸ = θ ′ t ∈ Θ t , the induced distributions satisfy:TV P θ1:t−1,θt , P θ1:t−1,θ ′ t ≥ c &gt; 0. i=1 1: for i = 1 to n θ do 2:</p>
<p>P θ1:t−1 , θi t
AcknowledgementsWe acknowledge support from the National Science Foundation (NSF) and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and #814639 as well as the TILOS institute (NSF CCF-2112665) and the Office of Naval Research (ONR N000142412631).This work used the programs (1) XSEDE (Extreme science and engineering discovery environment) which is supported by NSF grant numbers ACI-1548562, and (2) ACCESS (Advanced cyberinfrastructure coordination ecosystem: services &amp; support) which is supported by NSF grants numbers #2138259, #2138286, #2138307, #2137603, and #2138296.Specifically, we used the resources from SDSC Expanse GPU compute nodes, and NCSA Delta system, via allocations TG-CIS220009.Impact StatementThis paper advances the theoretical understanding of task generalization, investigating how learning on a small subset of tasks enables generalization to a much larger set of unseen tasks.Our study is primarily theoretical, supported by synthetic experiments designed to isolate key structural properties of task composition.By analyzing autoregressive compositional structures, we provide insights into sample complexity and task transferability.While our work lays foundational groundwork for broader applications in machine learning, its immediate impact remains theoretical, with no direct societal implications requiring emphasis.almost surely.Thus,Recursively applying this argument from t = 1 to T and taking a union bound over all iFinally, by the assumption that {P θ 1 t , . . ., P θ n θ t } = P Θt for each t, we concludeStep 2: Inference Stage Assume that, in the training phase, the learner identifies the true distribution sets. ., P θn θ t = P Θt for all t = 1, . . ., T.We now construct an algorithm that, given ℓ ≥ 2 ln 100 T n θ c 2 independent samples from the unseen composite task f θ = P θ1 , . . ., P θT , outputs the same distribution tuple P θ1 , . . ., P θT with probability at least 0.99.To achieve this, we leverage the distribution discrimination technique of Lemma A.1 to distinguish between candidates in P Θt , given sufficiently many demonstration samples.end for 9: end for 10: return P ζ1 , . . ., P ζ T .Error Analysis.Let P ζt be the chosen distribution at step t.Given P ζ1:t−1 = P θ1:t−1 , Lemma A.1 ensures that any incorrect candidate can be detected with high probability, as long as P θt ∈ P Θt .Specifically,A union bound over t = 1, . . ., T then implies, we getStep (x i,j ,y i,j 1:t ) &lt; P θ1:t−1 , ξ t,k (x i,j ,y The remainder of the proof then proceeds exactly as in Theorem 3.3.
How far can transformers reason? the locality barrier and inductive scratchpad. E Abbe, S Bengio, A Lotfi, C Sandon, O Saremi, Advances in Neural Information Processing Systems. 2024</p>
<p>Understanding intermediate layers using linear classifier probes. Alain , G Bengio, Y , 5th International Conference on Learning Representations, ICLR 2017. Toulon, FranceApril 24-26, 2017. 2017Workshop Track Proceedings</p>
<p>How do in-context examples affect compositional generalization?. S An, Z Lin, Q Fu, B Chen, N Zheng, J.-G Lou, D Zhang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>M Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, arXiv:1907.02893Invariant risk minimization. 2019arXiv preprint</p>
<p>A theory for emergence of complex skills in language models. S Arora, A Goyal, arXiv:2307.159362023arXiv preprint</p>
<p>Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Y Bai, F Chen, H Wang, C Xiong, S Mei, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Domain adaptation-can quantity compensate for quality?. S Ben-David, R Urner, Annals of Mathematics and Artificial Intelligence. 702014</p>
<p>Understanding in-context learning in transformers and LLMs by learning to learn discrete functions. S Bhattamishra, A Patel, P Blunsom, V Kanade, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Learning bounds for importance weighting. C Cortes, Y Mansour, M Mohri, Advances in neural information processing systems. 232010</p>
<p>Hypothesis transfer learning via transformation functions. Advances in neural information processing systems. S S Du, J Koushik, A Singh, B Póczos, 201730</p>
<p>Distributionally robust losses for latent covariate mixtures. J Duchi, T Hashimoto, H Namkoong, Operations Research. 7122023</p>
<p>Faith and fate: Limits of transformers on compositionality. N Dziri, X Lu, M Sclar, X L Li, L Jiang, B Y Lin, S Welleck, P West, C Bhagavatula, R Le Bras, Advances in Neural Information Processing Systems. 202336</p>
<p>What can transformers learn in-context? a case study of simple function classes. S Garg, D Tsipras, P S Liang, G Valiant, Advances in Neural Information Processing Systems. 202235</p>
<p>Instruct-skillmix: A powerful pipeline for llm instruction tuning. S Kaur, S Park, A Goyal, S Arora, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>. D Keysers, N Schärli, N Scales, H Buisman, D Furrer, S Kashubin, N Momchev, D Sinopalnikov, L Stafiniak, T Tihon, D Tsarkov, X Wang, Van Zee, </p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. O Bousquet, International Conference on Learning Representations. 2020</p>
<p>A method for stochastic optimization. D P Kingma, J Ba, Adam, International Conference on Learning Representations. 2015</p>
<p>Marginal singularity and the benefits of labels in covariate-shift. S Kpotufe, G Martinet, The Annals of Statistics. 4962021</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. B Lake, M Baroni, International conference on machine learning. PMLR2018</p>
<p>Near-optimal linear regression under distribution shift. Q Lei, W Hu, J Lee, International Conference on Machine Learning. PMLR2021</p>
<p>Transformers as algorithms: Generalization and stability in in-context learning. Y Li, M E Ildiz, D Papailiopoulos, S Oymak, International Conference on Machine Learning. PMLR2023</p>
<p>When does compositional structure yield compositional generalization? a kernel theory. S Lippl, K Stachenfeld, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Optimally tackling covariate shift in rkhs-based nonparametric regression. C Ma, R Pathak, M J Wainwright, The Annals of Statistics. 5122023</p>
<p>Towards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, International Conference on Learning Representations. 2018</p>
<p>Domain adaptation: Learning bounds and algorithms. Y Mansour, M Mohri, A Rostamizadeh, Annual Conference on Learning Theory. 2009</p>
<p>Understanding and mitigating the tradeoff between robustness and accuracy. A Raghunathan, S M Xie, F Yang, J Duchi, P Liang, International Conference on Machine Learning. PMLR2020</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. A Saparov, R Y Pang, V Padmakumar, N Joshi, M Kazemi, N Kim, H He, Advances in Neural Information Processing Systems. 202336</p>
<p>Discovering modular solutions that generalize compositionally. S Schug, S Kobayashi, Y Akram, M Wolczyk, A M Proca, J Von Oswald, R Pascanu, J Sacramento, A Steger, International Conference on Learning Representations. 2024</p>
<p>Out-of-distribution generalization via composition: a lens through induction heads in transformers. J Song, Z Xu, Y Zhong, Proceedings of the National Academy of Sciences. 1226e24171821222025</p>
<p>Covariate shift adaptation by importance weighted cross validation. M Sugiyama, M Krauledat, K.-R Müller, Journal of Machine Learning Research. 852007</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, Transactions on Machine Learning Research. 2835-88562022</p>
<p>From sparse dependence to sparse attention: Unveiling how chainof-thought enhances transformer sample efficiency. K Wen, H Zhang, H Lin, J Zhang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Compositional generalization from first principles. T Wiedemer, P Mayilvahanan, M Bethge, W Brendel, Advances in Neural Information Processing Systems. 202436</p>
<p>Transformers: State-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Brew, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2020</p>
<p>Do large language models have compositional ability? an investigation into limitations and scalability. Z Xu, Z Shi, Y Liang, Conference on Language Modeling. 2024</p>
<p>Towards a theoretical framework of out-of-distribution generalization. H Ye, C Xie, T Cai, R Li, Z Li, L Wang, Advances in Neural Information Processing Systems. 202134</p>
<p>Can models learn skill composition from examples?. H Zhao, S Kaur, D Yu, A Goyal, S Arora, Advances in Neural Information Processing Systems. 202437</p>
<p>Domain generalization: A survey. K Zhou, Z Liu, Y Qiao, T Xiang, C C Loy, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4542022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9577 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9577</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9577</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-276632282</p>
                <p><strong>Paper Title:</strong> Large language models for scientific discovery in molecular property prediction</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are a form of artificial intelligence system encapsulating vast knowledge in the form of natural language. These systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization and computer code generation. Although LLMs have seen initial applications in natural sciences, their potential for driving scientific discovery remains largely unexplored. In this work, we introduce LLM4SD, a framework designed to harness LLMs for driving scientific discovery in molecular property prediction by synthesizing knowledge from literature and inferring knowledge from scientific data. LLMs synthesize knowledge by extracting established information from scientific literature, such as molecular weight being key to predicting solubility. For inference, LLMs identify patterns in molecular data, particularly in Simplified Molecular Input Line Entry System-encoded structures, such as halogen-containing molecules being more likely to cross the blood–brain barrier. This information is presented as interpretable knowledge, enabling the transformation of molecules into feature vectors. By using these features with interpretable models such as random forest, LLM4SD can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. We foresee it providing interpretable and potentially new insights, aiding scientific discovery in molecular property prediction. Zheng et al. developed LLM4SD, a framework using large language models to predict molecular properties. The method leverages the ability of large language models to synthesize knowledge from literature and to reason about scientific data with domain expertise.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9577.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9577.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4SD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs for Scientific Discovery (LLM4SD) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses large language models to (1) synthesize knowledge from pretrained scientific literature and (2) infer rules from experimental molecular data, converting those rules into executable feature functions used to train interpretable models for molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLM4SD (framework using multiple LLM backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Not a single model but a modular pipeline that queries and aggregates outputs from various LLM backbones (GPT-4, Galactica, Falcon, ChemLLM, ChemDFM) to synthesize human-readable rules and to infer data-derived rules; rules are converted to code functions (via GPT-4) and applied with cheminformatics tools (RDKit) to produce feature vectors for interpretable learners (random forest / linear models).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry / molecular property prediction (physiology, biophysics, physical chemistry, quantum mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Two inputs: (1) the LLMs' own pretrained scientific corpora (unspecified large text corpora / scientific literature embedded in each backbone) used implicitly for literature synthesis; (2) supervised molecular datasets from MoleculeNet (58 tasks across BBBP, ClinTox, Tox21 (12), SIDER (27), HIV, BACE, ESOL, FreeSolv, Lipophilicity, QM9 (12)) provided as SMILES+labels for data-driven inference. Dataset sizes and splits are reported (e.g., QM9: 133,885 instances; Tox21: 7,831; HIV: 17,930; BBBP: 2,039), and scaffold/random splits were used per task.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Structure–activity / molecular property rules (interpretable decision rules and feature definitions), including both literature-derived heuristics and data-inferred regularities</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Example synthesized rule: 'Molecules with molecular weight under 500 Da are more likely to cross the blood–brain barrier (BBB)'. Example inferred rule: 'Molecules containing halogen atoms are observed more frequently among BBB-permeable compounds.' Another inferred example: 'presence of carbonyl groups and ring fragments associated with BBB permeability (possibly via effect on cross-sectional area).' </td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompting-based knowledge synthesis and data inference: (a) literature synthesis—prompt LLMs to adopt an 'experienced chemist' persona to enumerate quantifiable/categorical features known from literature; (b) data inference—present LLMs with batches of SMILES + labels or property values and ask for discriminative rules, repeating over many sampled batches and using the LLM's summarization to deduplicate rules; (c) require rules to have numeric/categorical measures so they can be transcribed into functions; (d) GPT-4 is used to generate Python/RDKit code implementing each rule as a feature extractor; (e) features from both sources are combined into vectors and used to train interpretable models (random forest/linear).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Multiple evaluation streams: (1) predictive performance: train random-forest models on LLM-derived features and compare to nine state-of-the-art baselines (various pretrained GNNs and RF+ECFP4) on 58 tasks using domain-appropriate metrics (AUC-ROC for physiology/biophysics, RMSE for physical chemistry, MAE for quantum mechanics); (2) rule-level validation: statistical tests (two-sided Mann–Whitney U for classification; two-sided linear regression t-test for regression) with p < 0.05 to test association between a rule and target label/value; (3) literature review by two pharmacologists via Google Scholar to determine whether statistically significant rules are documented in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM4SD (features from literature + data) outperformed baselines across 58 benchmark tasks. Combining synthesized and inferred rules gave better results than either alone in all four domains (e.g., physiology combined AUC-ROC 73.62 vs synthesis-only 70.46 and inference-only 69.25 averaged across backbones). Synthesized rules: ~85% statistically significant across tasks and nearly always found in existing literature. Inferred rules: ~91.3% statistically significant on average; ~74% documented in literature and ~17.3% not found in literature (novel / under-documented data patterns). The authors report domain-specific improvements (physiology AUC-ROC improved from previous best 74.53% to 76.60% (+2.07%); quantum mechanics average MAE reported as 5.8233 vs baseline 11.2450, described as a 48.2% improvement). Case studies show inferred rules can identify plausible 'second-order' features (e.g., carbonyls & rings affecting BBB permeation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to nine specialized pretrained GNN baselines (AttrMask, GraphCL, MolCLR, 3DInfomax, GraphMVP, MoleBERT, Grover, UniMol) and RF+ECFP4, LLM4SD achieved superior average performance across domains and achieved state-of-the-art on physiology and biophysics tasks; reported quantitative examples include physiology AUC-ROC improvement to 76.60% (from previous 74.53%) and large gains in certain quantum-mechanics regressions. The authors emphasize interpretability as an advantage over opaque GNN embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Noted limitations include: (1) reliance on the LLMs' pretrained literature (synthesis is an aggregation of existing knowledge rather than new discovery without data); (2) varying backbone performance: smaller/general LLMs (e.g., Falcon-7b) produced gibberish or failed in some domains, and larger/domain-pretrained models or very large general models (GPT-4) performed better; (3) challenges scaling to biological sequences (longer contexts needed for proteins/genes); (4) inferred novel rules require further experimental/biological validation; (5) some metric reporting in the paper mixes metric types in prose (authors caution metric domain-appropriate choices).</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>The paper reports model failure modes (e.g., Falcon-7b producing incoherent inference outputs) and emphasizes the need for expert validation; it also distinguishes between literature-aggregation (synthesized rules that largely match prior literature) and data-driven inference (some statistically significant inferred rules not found in literature), arguing that these are not mere memorization but require further validation. The authors do not report systematic hallucination quantification but mitigate risk via statistical tests and expert literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9577.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9577.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large transformer-based language model used in this work to (a) generate executable Python code (RDKit-based) that implements LLM-extracted rules as feature functions, and (b) as one of the LLM backbones evaluated for rule synthesis and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A very large transformer LLM; paper states GPT-4 was 'said to have been trained on approximately 1.76 trillion tokens' and is used both as a backbone for rule synthesis/inference and specifically to generate Python/RDKit code converting textual rules into functions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry / molecular property prediction (as part of LLM4SD pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Implicit: GPT-4's large-scale pretraining corpora including extensive web and scientific text (paper cites '~1.76 trillion tokens'); for inference tasks the model also received sampled SMILES + labels as prompted inputs during rule-inference stage.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Literature-derived heuristics and data-inferred molecular rules (structure–activity relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Same as pipeline examples: 'molecular weight < 500 Da correlates with BBB permeability'; GPT-4 used to codify such rules into RDKit feature functions.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompt-based role-play and batch data analysis; GPT-4 was used to produce executable code from textual rules (i.e., implement 'if molecular weight < 500 then feature=1' as RDKit function).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Predictions from features implemented via GPT-4-generated code were used to train models whose predictive performance was benchmarked across 58 tasks; additionally rule significance tested statistically and checked by expert literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 was a strong-performing backbone in the LLM4SD experiments and was used successfully to translate rules into code; the authors note GPT-4 'consistently performs well on all tasks' and draw a link to its very large pretraining scale.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>As an LLM backbone within LLM4SD, GPT-4-based features contributed to models that outperformed all specialized GNN baselines on multiple domains; GPT-4's scale is credited for consistently strong results relative to smaller backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>No model-specific limitations beyond general pipeline limitations; authors note the need for expert review of generated rules and caution about context-length limitations for some biological sequence tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Not explicitly quantified for GPT-4, but general concerns addressed via statistical validation and literature review; GPT-4 served partly to reduce implementation errors by producing code, but the paper does not quantify hallucination rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9577.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9577.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-6.7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica 6.7 billion parameter model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific LLM pretrained primarily on scientific literature; used in LLM4SD and selected for detailed rule validation due to strong performance and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Galactica-6.7b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A 6.7-billion-parameter transformer LLM pretrained on scientific literature (paper states Galactica-6.7b was built from scratch using 106 billion tokens of scientific literature).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry / molecular property prediction (scientific literature-focused LLM for rule synthesis/inference)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Pretrained on ~106 billion tokens of scientific literature (domain-specific pretraining); used in prompts both for knowledge synthesis (literature-derived rules) and for data inference from SMILES+labels.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Literature-synthesized rules and data-inferred molecular substructure rules (structure–activity relationships and second-order features)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Examples produced by Galactica: molecular weight, lipophilicity (logP), topological polar surface area, hydrogen bond counts for BBBP; inferred: carbonyl presence and ring fragments linked to BBB permeability.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Role-playing prompt ('experienced chemist') for literature rule synthesis; batch-based data analysis prompts for inference from SMILES + labels; summarization to deduplicate rules; requirement that outputs be numeric/categorical to allow codification.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Galactica-6.7b's generated rules were validated via statistical association tests (Mann–Whitney U for classification; linear regression t-test for regression), and subject-matter expert literature review; Galactica-6.7b was selected for rule-level analysis because of its reproducibility and strong results.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Galactica-6.7b produced a high proportion of statistically significant rules that frequently matched existing literature; used for the paper's rule validation analyses (e.g., reported that ~85% of synthesized rules were statistically significant, and that Galactica-derived inferred rules included a notable share of rules not found in literature but statistically significant).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Galactica models often outperformed other open general-purpose LLMs of similar or larger size (the paper highlights Galactica-6.7b rivalled Galactica-30b in several domains despite much smaller parameter count), illustrating the value of domain-specific pretraining for scientific rule extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Performance varied by domain (e.g., quantum mechanics tasks favored larger Galactica-30b in some cases); domain-specific pretraining yields benefits but does not guarantee superiority across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>No explicit hallucination metrics reported; authors mitigate risk by statistical tests and literature review. Some inferred rules lacked prior literature support and thus were flagged as requiring further validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9577.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9577.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon series</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon LLMs (Falcon-7b and Falcon-40b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose LLM backbones evaluated within LLM4SD; varying scale showed marked performance differences with Falcon-40b outperforming Falcon-7b for scientific rule extraction and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Falcon-7b / Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>General-purpose transformer LLM family; Falcon-7b is a smaller model (~7B params) and Falcon-40b a larger model (~40B params); trained for broad utility rather than scientific-specialised corpora per paper discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry / molecular property prediction (evaluated as LLM backbones for rule synthesis/inference)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Pretrained on general web and text corpora (not primarily scientific literature); for inference tasks, provided SMILES+labels in prompts as per pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>General literature-aggregated heuristics and data-inferred rules when capable</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>When functional, Falcon backbones produced similar kinds of rules (e.g., molecular descriptors like MW, H-bond counts) but the smaller Falcon-7b often failed to generate coherent inference outputs in certain domains (physiology and quantum mechanics).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompt-based synthesizing and data inference identical to other backbones (persona prompts, batch sampling and summarization), but model quality depended on scale and pretraining domain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Backbone-specific performance was evaluated by training features generated by each Falcon model and benchmarking predictive performance across the 58 tasks, and by inspecting coherence of generated rules.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Falcon-40b (larger model) could bridge the scientific knowledge gap and produced useful rules; Falcon-7b frequently underperformed and produced incoherent/gibberish inference outputs in some domains, indicating that general LLMs need larger scale to match domain-specific performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Falcon-40b's LLM4SD features achieved better domain performance than Falcon-7b; however, domain-specialised models (Galactica) often outperformed or matched Falcon-series models of larger size in several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Smaller general models (Falcon-7b) failed to perform inference reliably in some domains and produced gibberish outputs; general pretraining requires more scale to be effective for scientific rule extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>The paper documents incoherent outputs from under-sized general models as a failure mode rather than explicit hallucination of factual claims; recommended mitigation is larger model scale or domain-specific pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9577.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9577.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemLLM-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemLLM (7 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemistry-adapted LLM evaluated as an LLM4SD backbone; despite being domain-specific it underperformed relative to Galactica-6.7b, likely due to limited fine-tuning data scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChemLLM-7b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A 7-billion-parameter LLM adapted from a general LLM (InternLM2-7b) and fine-tuned on chemical data; paper reports ChemLLM-7b used only ~7 million tokens of chemical data for adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry / molecular property prediction (domain-specific backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Fine-tuned on a relatively small chemical corpus (~7 million tokens) on top of a general LLM; used with SMILES+labels in the data inference stage and for literature-synthesis prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Chemistry heuristics and data-inferred structure–activity rules (subject to the model's limited fine-tuning data)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Not enumerated in detail for ChemLLM specifically; general types mirror other backbones (MW, logP, polar surface area, substructure presence).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Same LLM4SD prompting pipeline (persona prompts, batch inference, summarization); however the model's small adaptation corpus limited its rule extraction quality relative to Galactica.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Performance of features produced by ChemLLM-7b was benchmarked across tasks and compared to other LLM backbones and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemLLM-7b substantially underperformed compared to Galactica-6.7b; authors attribute this to the small scale of chemical fine-tuning data and adaptation approach (fine-tuning an existing general LLM rather than training from scratch on a large scientific corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Underperformed relative to Galactica-6.7b and GPT-4 within the LLM4SD pipeline; illustrates that small domain-specific fine-tuning data can limit rule-extraction capability.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Limited chemical pretraining data (~7M tokens) likely constrained performance; the adaptation-from-general-LM approach was less effective than from-scratch domain training with larger corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Not specifically quantified; overall lower-quality outputs are reported as a limitation rather than specific hallucination metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9577.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9577.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemDFM-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemDFM 13 billion parameter model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter chemistry-focused foundation model (ChemDFM) evaluated as an LLM4SD backbone; it underperformed compared to Galactica-6.7b despite larger size, likely due to training/adaptation approach differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChemDFM-13b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A 13-billion-parameter model adapted from LLaMa-13b and trained/fine-tuned on chemistry-related tokens (paper states ChemDFM-13b trained on ~34 billion tokens), used as a domain-specific backbone within LLM4SD.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry / molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Adapted from a general LLaMa-13b backbone and trained on ~34 billion chemical tokens (per the paper); used with standard LLM4SD prompts and datasets (SMILES + labels).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Chemistry heuristics and data-inferred rules (structure–activity relationships), similar to other backbones</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Paper does not enumerate unique ChemDFM-derived laws, but ChemDFM was part of backbone comparison for rule synthesis/inference output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Used within the same prompting, batch inference, and summarization pipeline; rules were required to be measurable for codification.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Backbone-specific outputs were converted to features and evaluated by training random-forest models and comparing across the 58 tasks; coherence and statistical significance of rules also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemDFM-13b underperformed relative to Galactica-6.7b in the LLM4SD experiments despite larger token count in its chemical training; authors speculate adaptation-from-general-model and differences in training corpus quality/scale explain the discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>ChemDFM-13b performed worse than Galactica-6.7b and GPT-4 in many tests; indicates that size plus domain data does not guarantee better rule extraction if pretraining/fine-tuning strategies differ.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Performance discrepancies attributed to training approach (adaptation vs from-scratch) and differences in training data composition; suggests data scale and pretraining strategy critical for scientific rule extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>No detailed hallucination analysis reported; lower quality of inferred/synthesized rules relative to Galactica is discussed as a practical limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for scientific discovery in molecular property prediction', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: a large language model for science <em>(Rating: 2)</em></li>
                <li>Chemllm: a chemical large language model <em>(Rating: 2)</em></li>
                <li>ChemDFM: a large language foundation model for chemistry <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9577",
    "paper_id": "paper-276632282",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "LLM4SD",
            "name_full": "LLMs for Scientific Discovery (LLM4SD) pipeline",
            "brief_description": "A pipeline that uses large language models to (1) synthesize knowledge from pretrained scientific literature and (2) infer rules from experimental molecular data, converting those rules into executable feature functions used to train interpretable models for molecular property prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "LLM4SD (framework using multiple LLM backbones)",
            "llm_model_description": "Not a single model but a modular pipeline that queries and aggregates outputs from various LLM backbones (GPT-4, Galactica, Falcon, ChemLLM, ChemDFM) to synthesize human-readable rules and to infer data-derived rules; rules are converted to code functions (via GPT-4) and applied with cheminformatics tools (RDKit) to produce feature vectors for interpretable learners (random forest / linear models).",
            "application_domain": "Chemistry / molecular property prediction (physiology, biophysics, physical chemistry, quantum mechanics)",
            "input_corpus_description": "Two inputs: (1) the LLMs' own pretrained scientific corpora (unspecified large text corpora / scientific literature embedded in each backbone) used implicitly for literature synthesis; (2) supervised molecular datasets from MoleculeNet (58 tasks across BBBP, ClinTox, Tox21 (12), SIDER (27), HIV, BACE, ESOL, FreeSolv, Lipophilicity, QM9 (12)) provided as SMILES+labels for data-driven inference. Dataset sizes and splits are reported (e.g., QM9: 133,885 instances; Tox21: 7,831; HIV: 17,930; BBBP: 2,039), and scaffold/random splits were used per task.",
            "qualitative_law_type": "Structure–activity / molecular property rules (interpretable decision rules and feature definitions), including both literature-derived heuristics and data-inferred regularities",
            "qualitative_law_example": "Example synthesized rule: 'Molecules with molecular weight under 500 Da are more likely to cross the blood–brain barrier (BBB)'. Example inferred rule: 'Molecules containing halogen atoms are observed more frequently among BBB-permeable compounds.' Another inferred example: 'presence of carbonyl groups and ring fragments associated with BBB permeability (possibly via effect on cross-sectional area).' ",
            "extraction_methodology": "Prompting-based knowledge synthesis and data inference: (a) literature synthesis—prompt LLMs to adopt an 'experienced chemist' persona to enumerate quantifiable/categorical features known from literature; (b) data inference—present LLMs with batches of SMILES + labels or property values and ask for discriminative rules, repeating over many sampled batches and using the LLM's summarization to deduplicate rules; (c) require rules to have numeric/categorical measures so they can be transcribed into functions; (d) GPT-4 is used to generate Python/RDKit code implementing each rule as a feature extractor; (e) features from both sources are combined into vectors and used to train interpretable models (random forest/linear).",
            "evaluation_method": "Multiple evaluation streams: (1) predictive performance: train random-forest models on LLM-derived features and compare to nine state-of-the-art baselines (various pretrained GNNs and RF+ECFP4) on 58 tasks using domain-appropriate metrics (AUC-ROC for physiology/biophysics, RMSE for physical chemistry, MAE for quantum mechanics); (2) rule-level validation: statistical tests (two-sided Mann–Whitney U for classification; two-sided linear regression t-test for regression) with p &lt; 0.05 to test association between a rule and target label/value; (3) literature review by two pharmacologists via Google Scholar to determine whether statistically significant rules are documented in prior literature.",
            "results_summary": "LLM4SD (features from literature + data) outperformed baselines across 58 benchmark tasks. Combining synthesized and inferred rules gave better results than either alone in all four domains (e.g., physiology combined AUC-ROC 73.62 vs synthesis-only 70.46 and inference-only 69.25 averaged across backbones). Synthesized rules: ~85% statistically significant across tasks and nearly always found in existing literature. Inferred rules: ~91.3% statistically significant on average; ~74% documented in literature and ~17.3% not found in literature (novel / under-documented data patterns). The authors report domain-specific improvements (physiology AUC-ROC improved from previous best 74.53% to 76.60% (+2.07%); quantum mechanics average MAE reported as 5.8233 vs baseline 11.2450, described as a 48.2% improvement). Case studies show inferred rules can identify plausible 'second-order' features (e.g., carbonyls & rings affecting BBB permeation).",
            "comparison_to_baseline": "Compared to nine specialized pretrained GNN baselines (AttrMask, GraphCL, MolCLR, 3DInfomax, GraphMVP, MoleBERT, Grover, UniMol) and RF+ECFP4, LLM4SD achieved superior average performance across domains and achieved state-of-the-art on physiology and biophysics tasks; reported quantitative examples include physiology AUC-ROC improvement to 76.60% (from previous 74.53%) and large gains in certain quantum-mechanics regressions. The authors emphasize interpretability as an advantage over opaque GNN embeddings.",
            "reported_limitations": "Noted limitations include: (1) reliance on the LLMs' pretrained literature (synthesis is an aggregation of existing knowledge rather than new discovery without data); (2) varying backbone performance: smaller/general LLMs (e.g., Falcon-7b) produced gibberish or failed in some domains, and larger/domain-pretrained models or very large general models (GPT-4) performed better; (3) challenges scaling to biological sequences (longer contexts needed for proteins/genes); (4) inferred novel rules require further experimental/biological validation; (5) some metric reporting in the paper mixes metric types in prose (authors caution metric domain-appropriate choices).",
            "bias_or_hallucination_issues": "The paper reports model failure modes (e.g., Falcon-7b producing incoherent inference outputs) and emphasizes the need for expert validation; it also distinguishes between literature-aggregation (synthesized rules that largely match prior literature) and data-driven inference (some statistically significant inferred rules not found in literature), arguing that these are not mere memorization but require further validation. The authors do not report systematic hallucination quantification but mitigate risk via statistical tests and expert literature review.",
            "uuid": "e9577.0",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A very large transformer-based language model used in this work to (a) generate executable Python code (RDKit-based) that implements LLM-extracted rules as feature functions, and (b) as one of the LLM backbones evaluated for rule synthesis and inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4",
            "llm_model_description": "A very large transformer LLM; paper states GPT-4 was 'said to have been trained on approximately 1.76 trillion tokens' and is used both as a backbone for rule synthesis/inference and specifically to generate Python/RDKit code converting textual rules into functions.",
            "application_domain": "Chemistry / molecular property prediction (as part of LLM4SD pipeline)",
            "input_corpus_description": "Implicit: GPT-4's large-scale pretraining corpora including extensive web and scientific text (paper cites '~1.76 trillion tokens'); for inference tasks the model also received sampled SMILES + labels as prompted inputs during rule-inference stage.",
            "qualitative_law_type": "Literature-derived heuristics and data-inferred molecular rules (structure–activity relationships)",
            "qualitative_law_example": "Same as pipeline examples: 'molecular weight &lt; 500 Da correlates with BBB permeability'; GPT-4 used to codify such rules into RDKit feature functions.",
            "extraction_methodology": "Prompt-based role-play and batch data analysis; GPT-4 was used to produce executable code from textual rules (i.e., implement 'if molecular weight &lt; 500 then feature=1' as RDKit function).",
            "evaluation_method": "Predictions from features implemented via GPT-4-generated code were used to train models whose predictive performance was benchmarked across 58 tasks; additionally rule significance tested statistically and checked by expert literature review.",
            "results_summary": "GPT-4 was a strong-performing backbone in the LLM4SD experiments and was used successfully to translate rules into code; the authors note GPT-4 'consistently performs well on all tasks' and draw a link to its very large pretraining scale.",
            "comparison_to_baseline": "As an LLM backbone within LLM4SD, GPT-4-based features contributed to models that outperformed all specialized GNN baselines on multiple domains; GPT-4's scale is credited for consistently strong results relative to smaller backbones.",
            "reported_limitations": "No model-specific limitations beyond general pipeline limitations; authors note the need for expert review of generated rules and caution about context-length limitations for some biological sequence tasks.",
            "bias_or_hallucination_issues": "Not explicitly quantified for GPT-4, but general concerns addressed via statistical validation and literature review; GPT-4 served partly to reduce implementation errors by producing code, but the paper does not quantify hallucination rates.",
            "uuid": "e9577.1",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Galactica-6.7b",
            "name_full": "Galactica 6.7 billion parameter model",
            "brief_description": "A domain-specific LLM pretrained primarily on scientific literature; used in LLM4SD and selected for detailed rule validation due to strong performance and reproducibility.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "Galactica-6.7b",
            "llm_model_description": "A 6.7-billion-parameter transformer LLM pretrained on scientific literature (paper states Galactica-6.7b was built from scratch using 106 billion tokens of scientific literature).",
            "application_domain": "Chemistry / molecular property prediction (scientific literature-focused LLM for rule synthesis/inference)",
            "input_corpus_description": "Pretrained on ~106 billion tokens of scientific literature (domain-specific pretraining); used in prompts both for knowledge synthesis (literature-derived rules) and for data inference from SMILES+labels.",
            "qualitative_law_type": "Literature-synthesized rules and data-inferred molecular substructure rules (structure–activity relationships and second-order features)",
            "qualitative_law_example": "Examples produced by Galactica: molecular weight, lipophilicity (logP), topological polar surface area, hydrogen bond counts for BBBP; inferred: carbonyl presence and ring fragments linked to BBB permeability.",
            "extraction_methodology": "Role-playing prompt ('experienced chemist') for literature rule synthesis; batch-based data analysis prompts for inference from SMILES + labels; summarization to deduplicate rules; requirement that outputs be numeric/categorical to allow codification.",
            "evaluation_method": "Galactica-6.7b's generated rules were validated via statistical association tests (Mann–Whitney U for classification; linear regression t-test for regression), and subject-matter expert literature review; Galactica-6.7b was selected for rule-level analysis because of its reproducibility and strong results.",
            "results_summary": "Galactica-6.7b produced a high proportion of statistically significant rules that frequently matched existing literature; used for the paper's rule validation analyses (e.g., reported that ~85% of synthesized rules were statistically significant, and that Galactica-derived inferred rules included a notable share of rules not found in literature but statistically significant).",
            "comparison_to_baseline": "Galactica models often outperformed other open general-purpose LLMs of similar or larger size (the paper highlights Galactica-6.7b rivalled Galactica-30b in several domains despite much smaller parameter count), illustrating the value of domain-specific pretraining for scientific rule extraction.",
            "reported_limitations": "Performance varied by domain (e.g., quantum mechanics tasks favored larger Galactica-30b in some cases); domain-specific pretraining yields benefits but does not guarantee superiority across all tasks.",
            "bias_or_hallucination_issues": "No explicit hallucination metrics reported; authors mitigate risk by statistical tests and literature review. Some inferred rules lacked prior literature support and thus were flagged as requiring further validation.",
            "uuid": "e9577.2",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Falcon series",
            "name_full": "Falcon LLMs (Falcon-7b and Falcon-40b)",
            "brief_description": "General-purpose LLM backbones evaluated within LLM4SD; varying scale showed marked performance differences with Falcon-40b outperforming Falcon-7b for scientific rule extraction and inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "Falcon-7b / Falcon-40b",
            "llm_model_description": "General-purpose transformer LLM family; Falcon-7b is a smaller model (~7B params) and Falcon-40b a larger model (~40B params); trained for broad utility rather than scientific-specialised corpora per paper discussion.",
            "application_domain": "Chemistry / molecular property prediction (evaluated as LLM backbones for rule synthesis/inference)",
            "input_corpus_description": "Pretrained on general web and text corpora (not primarily scientific literature); for inference tasks, provided SMILES+labels in prompts as per pipeline.",
            "qualitative_law_type": "General literature-aggregated heuristics and data-inferred rules when capable",
            "qualitative_law_example": "When functional, Falcon backbones produced similar kinds of rules (e.g., molecular descriptors like MW, H-bond counts) but the smaller Falcon-7b often failed to generate coherent inference outputs in certain domains (physiology and quantum mechanics).",
            "extraction_methodology": "Prompt-based synthesizing and data inference identical to other backbones (persona prompts, batch sampling and summarization), but model quality depended on scale and pretraining domain.",
            "evaluation_method": "Backbone-specific performance was evaluated by training features generated by each Falcon model and benchmarking predictive performance across the 58 tasks, and by inspecting coherence of generated rules.",
            "results_summary": "Falcon-40b (larger model) could bridge the scientific knowledge gap and produced useful rules; Falcon-7b frequently underperformed and produced incoherent/gibberish inference outputs in some domains, indicating that general LLMs need larger scale to match domain-specific performance.",
            "comparison_to_baseline": "Falcon-40b's LLM4SD features achieved better domain performance than Falcon-7b; however, domain-specialised models (Galactica) often outperformed or matched Falcon-series models of larger size in several tasks.",
            "reported_limitations": "Smaller general models (Falcon-7b) failed to perform inference reliably in some domains and produced gibberish outputs; general pretraining requires more scale to be effective for scientific rule extraction.",
            "bias_or_hallucination_issues": "The paper documents incoherent outputs from under-sized general models as a failure mode rather than explicit hallucination of factual claims; recommended mitigation is larger model scale or domain-specific pretraining.",
            "uuid": "e9577.3",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ChemLLM-7b",
            "name_full": "ChemLLM (7 billion parameter variant)",
            "brief_description": "A chemistry-adapted LLM evaluated as an LLM4SD backbone; despite being domain-specific it underperformed relative to Galactica-6.7b, likely due to limited fine-tuning data scale.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "ChemLLM-7b",
            "llm_model_description": "A 7-billion-parameter LLM adapted from a general LLM (InternLM2-7b) and fine-tuned on chemical data; paper reports ChemLLM-7b used only ~7 million tokens of chemical data for adaptation.",
            "application_domain": "Chemistry / molecular property prediction (domain-specific backbone)",
            "input_corpus_description": "Fine-tuned on a relatively small chemical corpus (~7 million tokens) on top of a general LLM; used with SMILES+labels in the data inference stage and for literature-synthesis prompts.",
            "qualitative_law_type": "Chemistry heuristics and data-inferred structure–activity rules (subject to the model's limited fine-tuning data)",
            "qualitative_law_example": "Not enumerated in detail for ChemLLM specifically; general types mirror other backbones (MW, logP, polar surface area, substructure presence).",
            "extraction_methodology": "Same LLM4SD prompting pipeline (persona prompts, batch inference, summarization); however the model's small adaptation corpus limited its rule extraction quality relative to Galactica.",
            "evaluation_method": "Performance of features produced by ChemLLM-7b was benchmarked across tasks and compared to other LLM backbones and baselines.",
            "results_summary": "ChemLLM-7b substantially underperformed compared to Galactica-6.7b; authors attribute this to the small scale of chemical fine-tuning data and adaptation approach (fine-tuning an existing general LLM rather than training from scratch on a large scientific corpus).",
            "comparison_to_baseline": "Underperformed relative to Galactica-6.7b and GPT-4 within the LLM4SD pipeline; illustrates that small domain-specific fine-tuning data can limit rule-extraction capability.",
            "reported_limitations": "Limited chemical pretraining data (~7M tokens) likely constrained performance; the adaptation-from-general-LM approach was less effective than from-scratch domain training with larger corpora.",
            "bias_or_hallucination_issues": "Not specifically quantified; overall lower-quality outputs are reported as a limitation rather than specific hallucination metrics.",
            "uuid": "e9577.4",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ChemDFM-13b",
            "name_full": "ChemDFM 13 billion parameter model",
            "brief_description": "A 13B-parameter chemistry-focused foundation model (ChemDFM) evaluated as an LLM4SD backbone; it underperformed compared to Galactica-6.7b despite larger size, likely due to training/adaptation approach differences.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "ChemDFM-13b",
            "llm_model_description": "A 13-billion-parameter model adapted from LLaMa-13b and trained/fine-tuned on chemistry-related tokens (paper states ChemDFM-13b trained on ~34 billion tokens), used as a domain-specific backbone within LLM4SD.",
            "application_domain": "Chemistry / molecular property prediction",
            "input_corpus_description": "Adapted from a general LLaMa-13b backbone and trained on ~34 billion chemical tokens (per the paper); used with standard LLM4SD prompts and datasets (SMILES + labels).",
            "qualitative_law_type": "Chemistry heuristics and data-inferred rules (structure–activity relationships), similar to other backbones",
            "qualitative_law_example": "Paper does not enumerate unique ChemDFM-derived laws, but ChemDFM was part of backbone comparison for rule synthesis/inference output quality.",
            "extraction_methodology": "Used within the same prompting, batch inference, and summarization pipeline; rules were required to be measurable for codification.",
            "evaluation_method": "Backbone-specific outputs were converted to features and evaluated by training random-forest models and comparing across the 58 tasks; coherence and statistical significance of rules also evaluated.",
            "results_summary": "ChemDFM-13b underperformed relative to Galactica-6.7b in the LLM4SD experiments despite larger token count in its chemical training; authors speculate adaptation-from-general-model and differences in training corpus quality/scale explain the discrepancy.",
            "comparison_to_baseline": "ChemDFM-13b performed worse than Galactica-6.7b and GPT-4 in many tests; indicates that size plus domain data does not guarantee better rule extraction if pretraining/fine-tuning strategies differ.",
            "reported_limitations": "Performance discrepancies attributed to training approach (adaptation vs from-scratch) and differences in training data composition; suggests data scale and pretraining strategy critical for scientific rule extraction.",
            "bias_or_hallucination_issues": "No detailed hallucination analysis reported; lower quality of inferred/synthesized rules relative to Galactica is discussed as a practical limitation.",
            "uuid": "e9577.5",
            "source_info": {
                "paper_title": "Large language models for scientific discovery in molecular property prediction",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: a large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Chemllm: a chemical large language model",
            "rating": 2,
            "sanitized_title": "chemllm_a_chemical_large_language_model"
        },
        {
            "paper_title": "ChemDFM: a large language foundation model for chemistry",
            "rating": 2,
            "sanitized_title": "chemdfm_a_large_language_foundation_model_for_chemistry"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 1,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.017529999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>nature machine intelligence</p>
<p>Yizhen Zheng 
Huan Yee Koh 0000-0002-0488-2616
Jiaxin Ju 0000-0003-3503-5708
An T N Nguyen 0000-0003-0528-9416
Lauren T May 0000-0002-4412-1707
Geoffrey I Webb 0000-0001-9963-5169
&amp; Shirui Pan 0000-0003-0794-527X
nature machine intelligence
C1D42EBB99874670B140BC3D2A2925D710.1038/s42256-025-00994-z
Large language models (LLMs) are a form of artificial intelligence system encapsulating vast knowledge in the form of natural language.These systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization and computer code generation.Although LLMs have seen initial applications in natural sciences, their potential for driving scientific discovery remains largely unexplored.In this work, we introduce LLM4SD, a framework designed to harness LLMs for driving scientific discovery in molecular property prediction by synthesizing knowledge from literature and inferring knowledge from scientific data.LLMs synthesize knowledge by extracting established information from scientific literature, such as molecular weight being key to predicting solubility.For inference, LLMs identify patterns in molecular data, particularly in Simplified Molecular Input Line Entry System-encoded structures, such as halogen-containing molecules being more likely to cross the blood-brain barrier.This information is presented as interpretable knowledge, enabling the transformation of molecules into feature vectors.By using these features with interpretable models such as random forest, LLM4SD can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties.We foresee it providing interpretable and potentially new insights, aiding scientific discovery in molecular property prediction.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-zCalcExactMolWt (mol) &lt; 500', which will return a binary result.Here, rdMolDescriptors is a module in the RDKit 13 library.If the result is 0, it means the molecule has a weight greater than 500 Da and vice versa.In practice, human experts can review these generated rules to drop duplicate or non-functional parts.However, the experimental results presented were obtained without any human intervention or modification.</p>
<p>With these rules and functions, molecules can be transformed into vectorized representations: that is, features.These rule-based features can then be used to train an interpretable model, such as a random forest or linear classifier (Fig. 1c).Remarkably, we noted that when enhanced with LLM4SD, these traditional interpretable models can surpass state-of-the-art baselines.</p>
<p>Once the interpretable models are trained (Fig. 1d), we can gain valuable insights.In the BBBP prediction task, we can see the prediction results, the rules content, how each molecule aligns with those rules by looking into their vector representation and how important each rule is for the final prediction.For example, a random-forest model builds decision trees, and if these trees rely heavily on certain features, those features are assessed to have higher importance for the task.This helps us understand which rules are most critical in the model's predictions.As shown in Fig. 1d, the input molecule has a molecular weight under 500 Da, and molecular weight is a key factor in predicting BBBP.To improve usability for researchers, we have created a web-based application that offers knowledge synthesis, inference and prediction with interpretation (Supplementary Information Section 3).</p>
<p>Experimental results</p>
<p>In this section, we offer a synopsis of LLM4SD's performance spanning the four domains of physiology, biophysics, quantum mechanics and physical chemistry.Subsequently, we investigate the key components of LLM4SD, examining its performance across various LLM backbones of differing scales and pretraining datasets.</p>
<p>Overall performance on four domains.To evaluate LLM4SD's versatility, we conducted a comprehensive analysis of its performance across 58 molecular prediction tasks across four domains (Fig. 2).We compared LLM4SD's performance with nine specialized, state-of-the-art supervised machine learning models.These are advanced graph or geometric neural networks (GNNs): AttrMask 14 , GraphCL 15 , MolCLR 16 , 3DInfomax 17 , GraphMVP 18 , MoleBERT 19 , Grover 20 and UniMol 21 .Each model was pretrained on large datasets with diverse molecular knowledge and then fine-tuned for specific tasks (Methods).As a standard baseline, we implemented random forest with ECFP4 (ref.22) as input set features.</p>
<p>Benchmarking LLM4SD against the baseline, LLM4SD demonstrated its superior efficacy and performance (Fig. 2).This performance spanned 58 diverse tasks, from physiology (Extended Data Figs.1-3) and biophysics (Extended Data Fig. 4) to physical chemistry (Extended Data Fig. 5) and quantum mechanics (Extended Data Fig. 6).The detailed descriptions of these tasks is illustrated in the Methods section (Methods, 'Datasets').</p>
<p>In both physiology and biophysics, our model outperformed all existing baselines (Fig. 2a).Notably, we attained state-of-the-art results in physiology, raising the area under the receiver operating characteristic curve (AUC-ROC) from a previous best of 74.53% to 76.60%, a gain of 2.07%.In biophysics, our model also achieved the best performance.These advancements in physiology and biophysics emphasize the robustness and precision of LLM4SD in tasks that demand intricate biological understanding and modelling.</p>
<p>On tasks in quantum mechanics and physical chemistry, LLM4SD demonstrated substantial advancements (Fig. 2b).In the domain of quantum mechanics, it showed a profound improvement of 48.2% over the best performing baseline, registering an average mean absolute error (MAE) of 5.8233 across 12 tasks as opposed to 11.2450 achieved by factors-knowledge derived from the literature they are trained on.Additionally, LLMs can understand formal scientific languages, such as the Simplified Molecular Input Line Entry System (SMILES), which describes molecular structures and is used for storing molecular property data.For example, to represent a molecule's solubility, its SMILES string and logP score can be stored together.Given these two key abilities-understanding molecular property prediction tasks and interpreting SMILES-we are motivated to explore two questions: can LLMs leverage their prior knowledge and reasoning abilities to facilitate scientific discovery?Can LLMs be effectively used to help predicting the properties of molecules?</p>
<p>In this work, we propose LLM4SD (LLMs for Scientific Discovery).LLM4SD functions by performing two main tasks: synthesizing knowledge from existing literature and inferring knowledge by observing experimental data.First, LLM4SD retrieves known rules to predict molecular properties based on its pretrained literature, such as molecules with molecular weight under 500 Da being more likely to pass the blood-brain barrier (BBB).Second, using its understanding of SMILES notation and chemistry knowledge, LLM4SD identifies patterns from experimental data, such as molecules containing halogens being more likely to pass the BBB.These rules are then used to create interpretable feature vectors for each molecule.By training an interpretable machine learning model using these vectors, we show that this pipeline achieves the current state of the art on molecular property prediction across 58 benchmark tasks from the MoleculeNet dataset curated by the Stanford PANDE group 9 .These tasks, encompassing both classification and regression, span four domains: physiology, biophysics, physical chemistry and quantum mechanics.</p>
<p>Although these molecular property predictions address complex challenges, such as predicting a molecule's BBB permeability, they represent only a small corner of the breadth of scientific endeavour.The findings of LLM4SD highlight the broader potential of using LLMs for scientific discovery.</p>
<p>Results</p>
<p>LLM4SD pipeline</p>
<p>Our scientific discovery pipeline, LLM4SD, shown in Fig. 1, consists of four main components: (1) knowledge synthesis from the scientific literature, (2) knowledge inference from data, (3) model training and (4) interpretable insights.</p>
<p>In the knowledge synthesis from literature phase (Fig. 1a), LLMs use their pretrained understanding from extensive scientific literature 5,10,11 to synthesize rules for predicting molecular properties.For example, in predicting BBB permeability (BBBP), LLMs can apply established knowledge, such as the Lipinski rule of five 12 , which assesses drug-likeness based on rules like molecular weight being under 500 Da or there being fewer than five hydrogen bond donors.In the knowledge inference from data phase (Fig. 1b), LLMs can utilize their inferential and analytical abilities to identify patterns in scientific data: for example, SMILES strings and their corresponding labels.For instance, LLMs can detect that molecules containing halogens are more likely to pass the BBB, as this trait is frequently observed in the provided data for molecules that successfully cross the barrier.</p>
<p>In both the knowledge synthesis and inference stages, we require that the identified rules have either a numerical or categorical measure associated with them.This ensures that the rules can be readily transformed into corresponding code functions, which in turn can convert each molecule into a vector of values.Prompts used for both knowledge synthesis and inference in LLM4SD are shown in Extended Data Tables 1-3.</p>
<p>To convert these rules into corresponding executable code functions, we utilize GPT-4 to generate Python code using cheminformatics software such as RDKit 13 .For example, the rule 'if the molecular weight is smaller than 500 Da, the molecule is likely more permeable to the blood-brain barrier' can be converted into code as 'rdMolDescriptors.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-z the second-best baseline GraphMVP.Similarly, in physical chemistry, LLM4SD observed a noteworthy enhancement, with the model reaching an MAE of 1.28, marking an 12.9% advancement over the baseline root mean square error (RMSE) of 1.47.These substantial improvements in regression tasks affirm the refined capability of our approach in continuous prediction.</p>
<p>Compared to these advanced GNN baselines, LLM4SD has two primary advantages.First, it leverages a wealth of prior knowledge accumulated from decades of scientific literature.Although GNNs can learn general patterns from extensive molecular datasets through pretraining, incorporating specific scientific knowledge typically requires explicit effort, such as the careful curation and integration of domain-specific features into the model's input, unless that knowledge is hand-coded for their use.Developers must manually decide both what knowledge to include and how to integrate it effectively into the model 23,24 .In contrast, LLMs, pretrained on vast amounts of literature across fields like chemistry, inherently embed substantial amounts of scientific knowledge that can be leveraged directly without additional intervention beyond natural language interaction.Second, GNNs, although effective in encoding molecules into embeddings, often lack interpretability.This limits their utility in generating clear scientific hypotheses because their interpretive mechanisms, such as attention mechanisms, tend to be opaque.</p>
<p>Study of key components.</p>
<p>To delve deeper into the intricacies of the LLM4SD pipeline, we studied the influence of scale and pretraining datasets on its performance.In addition, we assessed the relative contributions of knowledge synthesis and inference.Our evaluation spanned across a spectrum of foundational LLM backbones, notably the GPT-4 (ref.ChemDFM-13b (ref.26).These backbones can be categorized into two classes, general LLMs and domain-specific LLMs.In particular, GPT-4 and the Falcon models are trained for a broad range of applications, imbibing a more general context during their pretraining phase, whereas the Galactica 10 models, ChemLLM 25 and ChemDFM 26 are pretrained or fine-tuned on mainly scientific literature.</p>
<p>Effect of scale.</p>
<p>The comparison of seven LLM4SD backbones revealed substantial differences among the different LLMs (Fig. 3a,b).Within the Falcon series, performance disparities were conspicuous.Falcon-7b, a smaller model, fell short compared to Falcon-40b in its range of domain expertise.Notably, it failed to conduct tasks in two key areas-physiology and quantum mechanics-indicating a weaker understanding of scientific challenges and data interpretation.Specifically, it produced gibberish responses when asked to conduct inference.</p>
<p>Conversely, the Galactica series painted a more nuanced picture.Unlike the Falcon series, a larger model did not necessarily translate to superior performance.In disciplines such as physiology, biophysics and physical chemistry, Galactica-6.7brivalled the performance of Galactica-30b, despite the latter having more than four times the number of parameters.However, in the domain of quantum mechanics, the larger Galactica-30b surged ahead, outperforming Galactica-6.7bby a margin of 14%.This variance could be attributed to the intricate and abstract Assume you are an experienced chemist.Please come up with 20/30 rules that are important to predict blood-brain barrier permeability.</p>
<p>Prompt for LLMs:</p>
<p>Prompt for LLMs:</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-znature of quantum mechanics, where the depth and breadth of knowledge encapsulated in the larger model might offer a discernible advantage.Surprisingly, ChemLLM-7b and ChemDFM-13b substantially underperform compared to Galactica-6.7b,despite all of them being domain-specific models.Unlike Galactica-6.7b,which was built from scratch using 106 billion tokens of scientific literature, ChemLLM-7b and ChemDFM-13b are adapted from general LLMs-InternLM2-7b (ref.27) and LLaMa-13b (ref.28), respectively-through fine-tuning.Moreover, Galactica benefits from a rich training dataset, whereas ChemLLM-7b utilizes only 7 million tokens of chemical data and ChemDFM-13b is trained on 34 billion tokens.We conjecture that this discrepancy in data scale and training approach leads to the performance difference.</p>
<p>GPT-4 consistently performs well on all tasks, which is reasonable considering its enormous scale.It is said to have been trained on approximately 1.76 trillion tokens 29 , which is more than 100 times larger than all baselines.</p>
<p>Effect of pretraining datasets of LLMs.It becomes evident that an LLM steeped in scientific literature, even if smaller in scale, exhibits a commendable prowess in scientific tasks (Fig. 3a,b).Conversely, the Falcon series, designed for general utility, necessitates a more substantial scale to effectively navigate scientific challenges.We postulate that this phenomenon is underpinned by the emergent capabilities 30 inherent in large-scale LLMs.These capabilities empower the more expansive Falcon-40b to bridge the knowledge gap and adapt to scientific tasks.In a broader perspective, despite their relatively modest scale, the Galactica models consistently outperformed the Falcon series, underscoring the pivotal role of proper domain-specific pretraining.This is further supported by the fact that, despite its smaller scale, the Galactica series achieves performance comparable to that of GPT-4.</p>
<p>Contributions of knowledge synthesis and inference.</p>
<p>It is important to assess the relative contributions of the features synthesized from literature and those inferred from data.To this end, we trained each of the classifiers using just one or the other or both forms of feature.Overall values for these three types of models were obtained by averaging over results for all tasks in a domain (Fig. 3c).</p>
<p>Across the 58 tasks, the combination of synthesis and inference features consistently outperformed individual methods.Specifically, in the field of physiology, an average AUC-ROC of 73.62 was achieved using both methods, compared to 70.46 with synthesis alone and 69.25 with inference.Similarly, in biophysics, combining both methods yielded an average AUC-ROC of 79.10, surpassing the scores of 76.30 and 75.76 obtained from synthesis and inference features, respectively.In physical chemistry, the combined approach resulted in an average RMSE of 1.54, which is notably better than the 1.99 from synthesis features and 1.77 from inference features.Finally, in quantum mechanics, the use of both synthesis and inference features produced an MAE of 10.42, improving on the values of 37.67 and 68.17 recorded with synthesis and inference alone.</p>
<p>These observations highlight the value of combining knowledge synthesis from scientific literature with inference from data.Literature imparts foundational theoretical insights, whereas empirical data identifies further regularities.The fusion of these knowledge facets equips the models with a comprehensive understanding, empowering them to excel across varied tasks and domains.</p>
<p>Validation of established rules</p>
<p>With LLM4SD outperforming specialized, state-of-the-art methods, we further validated the rules generated by Galactica-6.7bdue to its superior performance and ease of reproducibility.Individual rules were validated in two ways: statistical tests to confirm their association with
A t t r M a s k G r a p h C L G r a p h M V P M o l C L R M o l e B E R T R F + E C F P 4 G r o v e r U n iM o l L L M 4 S D 3 D I n f o m a x A t t r M a s k G r a p h C L G r a p h M V P M o l C L R M o l e B E R T R F + E C F P 4 G r o v e r U n iM o l L L M 4 S D 3 D I n f o m a x A t t r M a s k G r a p h C L G r a p h M V P M o l C L R M o l e B E R T R F + E C F P 4 G r o v e r U n iM o l LLM4SD Avg result 3DInfomax AttrMask GraphCL GraphMVP MolCLR MoleBERT RF + ECFP4 Grover UniMol LLM4SD Avg result 3DInfomax AttrMask GraphCL GraphMVP MolCLR MoleBERT RF + ECFP4 Grover UniMol L L M 4 S D MAE RMSE G r a p h C L G r a p h M V P M o l C L R M o l e B E R T R F + E C F P 4 G r o v e r U n iM o l 3 D I n f o m a x A t t r M a s k</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-z the target molecular attribute and a literature review to assess whether they are discussed in existing scientific literature.We employed the Mann-Whitney U-test 31 of association for classification tasks and the linear correlation t-test for regression tasks.The Mann-Whitney U-test 31 compared the distributions of a chosen rule across the two classes of the target variable, thereby evaluating the statistical relevance of the rule's ability to distinguish classes.Conversely, the linear regression t-test treated the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to regression prediction.</p>
<p>We further carried out a comprehensive review with in-domain experts of statistically significant rules to evaluate whether they are already identified in existing literature (Supplementary Information Section 4).Specifically, the evaluation process follows a two-step approach.First, two pharmacologists independently perform a literature review using Google Scholar for any studies related to the identified statistically significant rules for the downstream task.After that, if both experts found no related articles, we categorize the rule as 'statistically significant and not found in literature'.If either identified the rule in the literature, it would be classified as 'statistically significant and found in literature'.Following this process, we categorized each rule into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; or statistically insignificant (Fig. 4).</p>
<p>Knowledge synthesis from scientific literature.We discovered that most of the synthesized rules we examined are readily available in existing scholarly works.Notably, across all selected tasks, an overwhelming majority (85%) of these rules had statistically significant association with the target labels, affirming our pipeline's ability to summarize rules from scientific literature that were the most important for different tasks and domains.</p>
<p>Importantly, except for BACE 32 and Tox21-NR-Ahr9, we found no instances where statistically significant synthesized rules were absent from existing literature (Fig. 4).This aligns with the design of our pipeline: without analysing the data, LLMs tend to aggregate and summarize existing knowledge.To illustrate, in the context of BBBP, the rules generated by our pipeline were consistent with well-established determinants such as molecular weight, lipophilicity, distribution coefficient, topological polar surface area and hydrogen bonds [33][34][35] .These findings validate the robustness and reliability of our pipeline in leveraging LLMs to summarize existing scientific literature.</p>
<p>Biophysics</p>
<p>Quantum mechanics Physical chemistry Fig. 3 | Study of LLM4SD's components.a, Performance comparison of LLM4SD using seven LLM backbones for physiology and biophysics.b, Performance comparison of LLM4SD using seven LLM backbones for physical chemistry and quantum mechanics.c, Examining the influence of both synthesized and inferred knowledge on the average (across all backbones) model performance across all four domains.The triangle's colour signifies the metric employed for domainspecific tasks.A (+) next to the metric name indicates that higher values yield better results, whereas a (−) suggests the contrary.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-zKnowledge inference from data.We found that an average of 91.3% of the inferred rules were statistically significant, higher than synthesized rules (Fig. 4).Of these, an average of 74% rules were already documented in existing scientific literature, and we were unable to find prior mention of 17.3%.These latter rules were primarily associated with data patterns that are not widely discussed in the literature but can be inferred from our task dataset, such as obscure molecular substructures that influence target labels like the Gibbs free energy (ΔG) of a molecule.</p>
<p>In contrast to the knowledge synthesized from literature, we found that six out of eight tasks have statistically significant rules that we could not identify in the existing literature.This suggests that the inferred rules produced by LLM4SD are not merely a result of the LLM's textual memorization during pretraining.Instead, the inferred rules reflect a genuine capability to derive meaningful rules from data based on the specific task.</p>
<p>Our case studies further substantiated the utility of these unidentified but statistically significant rules.For instance, in BBBP, where 38% of rules are statistically significant but unidentified, Galactica-6.7bpinpointed the carbonyl functional group and fragment rings as key determinants of a molecule's BBBP.We hypothesize that these features are crucial for calculating a molecule's cross-sectional area, which in turn influences its orientation in lipid-water interfaces-factors vital for membrane partitioning and permeation 35 .Intriguingly, this suggests that our pipeline enables LLMs to infer 'second-order features'.</p>
<p>These are features that may not be immediately obvious or widely recognized but are consistent with established scientific principles in literature.In doing so, LLMs not only corroborate existing knowledge but also apply existing knowledge in interpreting data, thereby enriching the current scientific discourse.An additional case study was conducted to assess the effectiveness of several lesser-explored rules (Supplementary Information Section 6).</p>
<p>By leveraging LLMs, our pipeline not only validates well-established scientific principles but also uncovers less documented and even potentially new rules.This facilitates a more effective and transparent interaction between scientists and the artificial intelligence (AI) system, enhancing both the quality and trustworthiness of the research output.Moreover, the statistically robust but underrepresented rules we identified could serve as promising avenues for future scientific exploration, thereby advancing the frontier of collaborative, AI-assisted research.</p>
<p>Discussion</p>
<p>Our exploration unveiled unexpected capabilities of LLM4SD through our specially designed pipeline, enabling LLMs to excel in scientific discovery in molecular prediction.Through seamless integration with our proposed architecture, LLMs exhibit state-of-the-art performance across 58 downstream tasks drawn from four domains in the Mole-culeNet benchmark.The inherent versatility of LLM4SD stands as a testament to its potential for broader molecular applications across varied domains.Fig. 4 | Literature review and statistical analysis of LLM rules.a-d, We conducted statistical analysis and a comprehensive literature review on rules generated by Galactica-6.7bacross all four scientific domains, with two tasks evaluated for each domain: quantum mechanics (a), physical chemistry (b), physiology (c) and biophysics (d).In the statistical analysis, the significance of a rule is determined based on the task type: for classification, the two-sided Mann-Whitney U-test 31 compares the difference in distributions of chosen rule across the two classes of the target variable; and for regression, the two-sided linear regression t-test 44 treats the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to prediction.In both cases, we used a 0.05 P value threshold to determine rule significance.In the literature review, we assessed the prevalence of a rule in existing literature.With statistical analysis and literature review, each rule is categorized into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; or statistically insignificant.Across all tasks, literature-synthesized knowledge rules were generally both prevalent in existing literature and statistically significant.</p>
<p>In contrast, empirically inferred data rules yielded mixed results, with some easily found in existing literature and others not identified by the researchers.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-00994-zScientific discovery, vast in scope, is constantly evolving with our expanding understanding of the universe.Our study only focused on molecular property prediction.However, LLM4SD's promising results in molecular property prediction hold promise for the direct application of LLMs in more advanced applications, such as protein sequence or gene sequence analysis.Protein and gene sequences are much more complicated than the SMILES strings in our study, which typically comprise up to dozens of characters.For example, protein sequences normally have 300-500 amino acids, whereas gene sequences typically contain many thousands of nucleotides.The complexity of these data poses great challenges for LLMs to understand long context domain-specific information, which requires extensive prior knowledge.In addition, LLMs need a longer and effective context window to process this data.Even for models that support large input, they often cannot use the long context input effectively in practice 36,37 .To enhance LLMs' ability to handle intricate biological data, pretraining them on vast, diverse datasets of protein or gene sequences may help the models understand the complex patterns more effectively.Additionally, incorporating retrieval-augmented generation with specialized biological knowledge bases, such as UniProt 38 and GenBank 39 , can provide additional knowledge to improve understanding and contextual accuracy.Furthermore, developing efficient tokenization methods specifically tailored for biological sequences may enhance the model's ability to process and analyse this type of data, leading to more accurate and insightful results.We envision further expansion, integrating more tasks and domains, pushing LLMs to their full potential and reshaping the boundaries of scientific inquiry.</p>
<p>As we gaze towards the horizon, the potential trajectory for LLM4SD is compelling.We anticipate a future where the nexus between LLMs and advanced scientific toolkits deepens.As computational capabilities grow and scientific knowledge expands, our pipeline stands poised for evolutionary enhancements.Our steadfast goal is to harmoniously incorporate LLMs with myriad scientific arenas, unlocking insights and pioneering avenues previously unimagined.</p>
<p>Methods</p>
<p>Datasets</p>
<p>We conducted a thorough evaluation of LLM4SD, covering 58 subtasks across four unique domains for a robust assessment.The physiology domain included 41 tasks like BBBP, ClinTox and the 12-task Tox21, ranging from NR-AR to SR-p53, along with the 27-task SIDER suite covering various medical conditions.Biophysics offered two classification tasks: BACE and HIV.In physical chemistry, we addressed three regression tasks: ESOL, FreeSolv and Lipophilicity; and the quantum mechanics domain presented 12 regression tasks within the QM9 dataset, exploring properties from mu to G, providing a comprehensive insight into LLM4SD's capabilities.</p>
<p>Physiology</p>
<p>BBBP.</p>
<p>The BBBP dataset contains 2,039 instances, each representing unique compounds labelled based on their permeability properties.Predicting which molecules can cross this barrier is paramount for drug development, especially for neurological conditions.</p>
<p>ClinTox.The ClinTox dataset, with 1,478 instances, provides comprehensive information on the toxicological properties of various compounds.</p>
<p>Tox21.With 7,831 instances, the Tox21 dataset is a collaborative effort to identify environmental toxicants.Its 12 classification tasks focus on specific biological targets or pathways.The nuclear receptor (NR) tasks, namely NR-AhR, NR-AR, NR-AR-LBD, NR-Aromatase, NR-ER, NR-ER-LBD and NR-PPAR-gamma, examine interactions with intracellular proteins influencing gene expression and potential toxic effects.The stress response (SR) tasks, including SR-ARE, SR-ATAD5, SR-HSE, SR-MMP and SR-p53, explore the impact of chemicals on stress-related cellular pathways.</p>
<p>SIDER.</p>
<p>The SIDER dataset, with 1,427 instances, offers detailed data on medication side effects.Each task in this dataset relates to a specific adverse drug reaction, aiding researchers in understanding and predicting potential drug side effects.The 27 classification tasks are (1) hepatobiliary disorders; (2) metabolism and nutrition disorders;</p>
<p>(3) product issues; (4) eye disorders; (5)</p>
<p>Biophysics</p>
<p>HIV.With a collection of 17,930 instances, the HIV dataset offers a comprehensive repository of molecules represented in the SMILES format.This dataset is instrumental in the classification of compounds based on their potential inhibitory effects against HIV.</p>
<p>BACE.</p>
<p>The BACE dataset, comprising 11,908 instances, is a curated collection of molecules, each represented in the SMILES format.This dataset is tailored for classification tasks, aiming to discern molecules that can inhibit the BACE-1 enzyme.By analysing the molecules within this dataset, researchers can glean insights into the structural features that confer inhibitory properties against BACE-1.</p>
<p>Physical chemistry</p>
<p>ESOL.The ESOL dataset, comprising 1,128 instances, is a curated collection that delves into the solubility of molecules in water.By analysing the ESOL dataset, researchers can gain a deeper understanding of the molecular features that dictate solubility, thereby aiding in the design of compounds with optimal solubility profiles.Each entry in this dataset is represented using the SMILES notation, a universal language for describing the structure of chemical species.</p>
<p>FreeSolv.With 642 instances, the FreeSolv dataset provides comprehensive data on the hydration free energy of molecules.This dataset is pivotal for researchers aiming to predict how molecules interact with water, which has implications for drug solubility and stability.Each molecule in the FreeSolv dataset is also represented using the SMILES notation.</p>
<p>Lipophilicity.Lipophilicity is a fundamental property that influences the absorption, distribution, metabolism and excretion of drugs.The Lipophilicity dataset, with 4,200 compounds, offers a rich resource for understanding this property.Analysing this dataset allows researchers to discern the molecular attributes that contribute to a compound's lipophilicity, guiding the synthesis of molecules with desired pharmacokinetic properties.Like the other datasets in this domain, each entry is denoted using the SMILES notation.</p>
<p>Quantum mechanics</p>
<p>QM9.The quantum mechanics domain, central to understanding the fundamental properties of matter, is exemplified in our evaluation through the QM9 dataset.Comprising 133,885 instances, the QM9 Article https://doi.org/10.1038/s42256-025-00994-zdataset provides a comprehensive exploration of molecules' quantum mechanical attributes, essential for diverse applications from material science to pharmaceuticals.It includes 12 tasks: μ (dipole moment), α (polarizability), R 2 (squared radius), ZPVE (zero-point vibrational energy), C v (heat capacity at constant volume), Δϵ (energy gap), ϵ HOMO (highest occupied molecular orbital energy), ϵ LUMO (lowest unoccupied molecular orbital energy), U 0 (internal energy at 0 Kelvin), U (internal energy at standard state), H (enthalpy) and G (Gibbs free energy).</p>
<p>Baselines</p>
<p>We rigorously assessed our pipeline in comparison to specialized, state-of-the-art supervised machine learning methods.For conventional approaches, we employed random forest 40 , using ECFP4 (ref.22) as the input feature set.We also considered state-of-the-art GNNs, including attribute masking (AttrMask) 14 , GraphCL 15 , MolCLR 16 , 3DInfomax 17 , GraphMVP 18 , MoleBERT 19 , Grover 20 and UniMol 21 .Each of these models was initialized with pretrained weights and subsequently fine-tuned for specific tasks.</p>
<p>In summary, AttrMask pretraining involves teaching the GNN to predict randomly masked atom and bond attributes within molecular graphs.GROVER pretraining focuses on contextual predictions of an atom's surroundings and predicts graph-level motifs.GraphCL and MolCLR use graph augmentations for a contrastive learning objective, aimed at maximizing the similarity between augmentations originating from the same molecule while minimizing similarity between augmentations from different molecules.GraphMVP and 3DInfomax leverage existing three-dimensional (3D) molecular datasets to pretrain models capable of deducing 3D molecular geometry from two-dimensional graphs, by optimizing mutual information between 3D summary vectors and GNN graph representations.MoleBERT, the recent state-of-the-art method, employs a vector quantized variational autoencoder-based tokenizer to diversify atom vocabulary, thereby balancing dominant and rare atoms.It uses masked atoms modelling and triplet masked contrastive learning for node and graph-level pretraining, respectively.Finally, UniMol pioneers a universal 3D molecular representation learning method, which pretrained with 3D position recovery and masked atom prediction.</p>
<p>LLM4SD in the molecular prediction pipeline</p>
<p>In this section, we detail the proposed pipeline and the techniques used to align with the requirements of molecular property prediction tasks.Instead of merely prompting LLMs to generate scientific hypotheses 41 or training them for direct predictions 42 , LLM4SD emulates how human experts conduct scientific research.This includes synthesizing knowledge from literature, inferring hypotheses from datasets, validating findings through experiments and elucidating the rationale behind predictions.</p>
<p>Knowledge synthesis from the scientific literature.LLMs are usually pretrained on large corpora of text data that include books, articles, websites and other written content.This extensive pretraining helps LLMs to learn the structure of the language, recognize patterns, understand context and acquire a wide-ranging knowledge of facts and concepts.Thus, the goal of the knowledge synthesis process is to extract relevant features from the vast pool of the knowledge that an LLM possesses from the pretraining stage.</p>
<p>To achieve this, we first instruct the LLM to adopt the persona of an experienced chemist and then engage it to identify pertinent features based on its existing knowledge.This form of role-playing prompt facilitates the knowledge mining process to mimic how human experts solve real-world challenges.For example, when a chemist needs to predict the bioactivity or BBBP of a molecule, they often apply feature-related rules such as number of hydrogen bond donors/acceptors, molecular weight and logP.We require that the features identified by LLMs can be measured with a numerical or categorical measure to enable their transcription into corresponding functions.</p>
<p>Knowledge inference from data.The objective of knowledge inference form data is to harness the powerful reasoning skills of LLMs to identify relevant features by analysing the given data.Given their impressive ability to solve mathematical problems and identify patterns, we conjecture that LLMs have the capacity to discern common patterns within groups of molecules based on their scientific understanding.To validate this hypothesis, we provide LLMs with an instruction and several batches of sampled instances with their corresponding class labels or instance property values.In the instruction, the LLM is tasked with analysing patterns from provided data to identify features that effectively discriminate between two classes of instances or predict their property values.As a result, LLMs will come up with rules distilled from the analysis for each batch.Because the generated rules in different batches may contain duplicates, we ultimately employ the LLMs' summarization capability to condense the rules and eliminate duplicates, resulting in the final list of features.</p>
<p>Model training.In this stage, all the features identified in the first two stages are transcribed into corresponding functions.All these functions take a scientific instance as input-for example, a SMILES string for molecules-and return a feature value.Consequently, the final representation of an instance resides in an r-dimensional space, where r is the number of features that have been identified.</p>
<p>These vector representations function as the feature vectors for the model training.Employing interpretable models like a linear layer or random forest enables quantification of each rule's importance in prediction, thus elucidating their contribution to the model's final decision.This transparency fosters an intuitive comprehension of the decision-making process, enhancing trust and usability among domain experts.</p>
<p>Interpretable insights.Following interpretable model training, our pipeline delivers clear insights, including prediction results, vector representations of molecules, rules and their corresponding importance scores.This information helps users identify key factors influencing the prediction, thereby improving interpretability.</p>
<p>Metrics</p>
<p>We assessed LLM4SD across 58 molecular property prediction tasks spanning four domains, utilizing distinct evaluation metrics tailored to each task's nature.For the domains of physiology and biophysics, the AUC-ROC metric was employed.AUC-ROC measures the ability of the model to distinguish between classes, with a range from 0 to 1, where a higher value indicates better performance.In the domain of physical chemistry, the RMSE was used.RMSE quantifies the difference between predicted and observed values, with a lower value indicating a closer fit to the true data.For quantum mechanics, we utilized the MAE metric.MAE measures the average magnitude of errors between predicted and true values, with smaller values denoting better accuracy.</p>
<p>Related work</p>
<p>Molecular property prediction.</p>
<p>Research in molecular property prediction has explored a range of computational methods.Preliminary attempts use extended connectivity fingerprint 22 to encode molecular characteristics for traditional machine learning methods such as random forest 40 .Transitioning into neural network approaches, GROVER 20 , GraphCL 15 and AttrMask 14 employ pretraining on large-scale unlabelled molecular graphs and then fine-tune for property prediction.Specifically, GROVER 20 focuses on contextual predictions of an atom's surroundings and predicts graph-level motifs, whereas GraphCL 15 uses a contrastive learning approach to maximize the similarity between Article https://doi.org/10.1038/s42256-025-00994-zdifferent augmented views of a molecule.AttrMask masks and predicts the attributes of nodes, such as atom type.</p>
<p>Recent advancements have introduced more sophisticated frameworks.MolCLR 16 , for instance, extends contrastive learning with three molecular graph augmentations, atom masking, bond deletion and subgraph removal.Meanwhile, GraphMVP 18 and 3Dinfomax 17 enhance contrastive learning by incorporating 3D information about the molecules, contrasting two-dimensional and 3D views of molecules.UniMol 21 pioneers a universal 3D molecular representation learning method, which pretrains with 3D position recovery and masked atom prediction.Additionally, MolBERT 19 employs a vector quantized variational autoencoders framework to encode atoms into chemically meaningful discrete codes and trains through masked atom prediction.</p>
<p>Although previous methods like GNN that encode information into vectors are effective, they often lack interpretability.This makes them less useful for generating clear hypotheses, as their interpretative mechanisms, like attention, offer only a vague understanding.In contrast, our LLM4SD method produces comprehensible rules akin to human analytical processes, offering clearer and more actionable insights.Additionally, previous models cannot effectively harness prior knowledge of chemistry, which remains a challenging task.LLMs pretrained on vast amounts of knowledge, including chemistry, possess the potential to act as human domain-specific experts.Despite their promise, the application of LLMs in scientific discovery is still underexplored.Our proposed LLM4SD method leverages LLMs to drive scientific discovery in molecular property prediction, demonstrating improvements by synthesizing prior knowledge and inferring principles from data.</p>
<p>Experimental settings</p>
<p>The experimental setting consists of splitting datasets and evaluation.Splitting datasets.We followed the MolCLR setup by using the code from MolCLR's GitHub repository to partition the dataset into an 80/10/10 split for training, validation and test sets.Specifically, for the physiology, biophysics and physical chemistry tasks, we employed a scaffold split for molecular compounds, whereas for quantum mechanics tasks, we used a random split.To ensure a fair comparison across all baselines, we reran all baseline methods using the exact same data split (that is, all models share the same SMILES-label pairs in the train, validation and test sets).This guarantees consistency in the training, validation and test sets across all baselines.</p>
<p>Evaluation.After constructing the dataset, molecules were converted into numerical features using the LLM4SD framework, leveraging four distinct LLM backbones: Falcon-7b, Falcon-40b, Galactica-6.7band Galactica-30b.For each backbone, two separate sets of inferred rules were generated, corresponding to 30 and 50 iterations of data sampling, respectively.These numerical features served as input for training a random-forest model.To optimize the model's performance, a grid search was performed to identify the best hyperparameters.The optimized random-forest model was then employed to predict molecular properties on the test set.Each prediction was repeated ten times, and the final test results reflect the backbone and sampling times combination that achieved the best performance on the validation set.</p>
<p>Reporting summary</p>
<p>Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.</p>
<p>a</p>
<p>Knowledge</p>
<p>Fig. 1 |
1
Fig. 1 | LLMs for scientific discovery in molecular prediction pipeline.a, Knowledge synthesis from the literature.In this phase, LLMs synthesized knowledge based on their pretrained literature for tasks like predicting BBBP.For example, molecules with a molecular weight under 500 Da are more likely to pass through the BBB.b, Knowledge inference from data.Here, LLMs analyse data, such as SMILES strings with labels (1 for BBB permeable, 0 for non-BBB permeable), to identify patterns.For instance, they may observe that molecules containing halogens have a higher chance of crossing the BBB.c, Model</p>
<p>Fig. 2 |
2
Fig. 2 | Comparison between LLM4SD and baselines across four domains.The red dashed line represents the average performance of all baselines.a, Comparative analysis of model performance versus baselines in physiology and biophysics.b, Comparative analysis of regression performance: LLM4SD versus baselines in quantum mechanics and physical chemistry.</p>
<p>4 F
4
a l c o n -7 b F a l c o n -4 0 b G a l a c t i c a -6 .7 b G a l a c t i c a -3</p>
<p>Extended Data Fig. 1 |
1
Detailed performance comparison between 'LLM4SD' and nine baselines in the physiology domain.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD outperformed other models in 3 out of 4 datasets using the AUC-ROC metric and consistently surpassing the average across all datasets.The results for Tox21 and SIDER are average scores from 12 and 27 tasks respectively (see Extended Data Figs. 2 and 3 for detailed breakdown).Extended Data Fig. 2 | Detailed performance comparison between 'LLM4SD' and nine baselines on Tox21 Dataset.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD ranks among the top three methods in 8 out of 12 tasks and consistently outperformed the average in all tasks.Extended Data Fig. 4 | Detailed performance comparison between 'LLM4SD' and nine baselines in the biophysics domain.The red dashed line shows the average result across all methods, in terms of AUC-ROC.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD outperformed the top-performing baseline by roughly 1% on the HIV dataset and closely matched the best performing method, UniMol.In both cases, LLM4SD delivered a visibly superior outcome compared to the average performance.Extended Data Fig. 5 | Detailed performance comparison between 'LLM4SD' and nine baselines in the physical chemistry domain.The red dashed line shows the average result across all methods.The physical chemistry domain encompasses three datasets: the ESOL dataset with 1,128 instances, the FreeSolv dataset with 642 instances, and the Lipophilicity dataset comprising 4,200 compounds.Each marker's error bar represents the method's standard deviation, calculated based on 10 independent runs (n=10).These data points are overlaid on the plot in grey colour.LLM4SD substantially outperformed all baseline methods on ESOL, demonstrating a 57% improvement over the average outcome for that dataset, and achieved state-of-the-art results on the additional datasets, FreeSolv and Lipophilicity.Extended Data Fig. 6 | Detailed performance comparison between 'LLM4SD' and nine baselines in the quantum mechanics domain.The red dashed line shows the average result across all methods.The quantum mechanics domain includes the QM9 datasets with 12 subtasks, comprising 133,885 instances.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs (n=10).These data points are overlaid on the plot in grey colour.LLM4SD excelled in predicting properties such as U0, U, H, and G, showing substantial enhancements.In other tasks, the results from LLM4SD were comparable to the average of all methods.</p>
<p>Extended Data Table 1 | General prompt for Knowledge Synthesis from the Scientific Literature and Knowledge Inference from Data https</p>
<p>://doi.org/10.1038/s42256-025-00994-z</p>
<p>Extended Data Table 2 | Classification task descriptions for the general prompt in Extended Data Table 1 Extended Data Table 3 | Regression task descriptions for the general prompt in Extended Data Table 1</p>
<p>Scientific productivity is facing a notable decline, with progress in many fields roughly halving every 13 years 1 . As scientific discovery becomes increasingly complex and challenging, traditional methodologies struggle to keep pace, necessitating innovative approaches. Scientific discovery relies on building on existing knowledge to analyse experimental data, recognize data patterns and formulate well-reasoned hypotheses2 . This process requires two essential abilities: prior knowledge understanding and reasoning abilities. Large language models
Extended Data Fig.3| Detailed performance comparison between 'LLM4SD' and nine baselines on Sider Dataset. The red dashed line shows the average result across all methods. Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs. LLM4SD ranks among the top three methods in 22 out of 27 tasks, and consistently outperforms the average in all tasks with the exception of the 'Psychiatric disorders' task.
Acknowledgements H.Y.K.'s scholarship is supported by the Australian Government Research Training Programme (RTP) Scholarship and Monash University as a cocontribution to Australian Research Council grant no.ARC DP210100072.L.T.M.'s, G.I.W.'s and A.T.N.N.'s research into AI applications for drug discovery is supported by a National Health and Medical Research Council (NHMRC) of Australia Ideas grant (grant no.APP2013629).L.T.M.'s research is also supported by the National Heart Foundation of Australia (grant no.101857).L.T.M.'s and A.T.N.N.'s research is also funded by the NHMRC of Australia and the Department of Health and Aged Care through the Medical Research Future Fund (MRFF) Stem Cell Therapies Mission (grant no.MRF2015957). Computational resources were generously provided by the Nectar Research Cloud, a collaborative Australian research platform supported by the NCRIS-funded Australian Research Data Commons (ARDC) and the MASSIVE HPC facility.We also gratefully acknowledge the support of the Griffith University eResearch Service &amp; Specialized Platforms Team and the use of the High-Performance Computing Cluster 'Gowonda'.S.P. is supported by ARC Future Fellowship (grant no.FT210100097) and ARC grant no.DP240101547.Data availabilityThe split datasets utilized in this study are entirely open source and have been made publicly available to ensure straightforward replication of our findings.We have provided presplit datasets for a variety of tasks, including BBBP, ClinTox, Tox21 (12 subtasks), SIDER (27 subtasks), HIV, BACE, ESOL, Lipophilicity, FreeSolv and QM9 (12 subtasks).These datasets are divided into training, validation and test sets based on our experimental settings.You can access them at the following GitHub repository: https://github.com/zyzisastudyreallyhardguy/LLM4SD/tree/main/scaffold_datasets.For the original raw datasets provided by MoleculeNet 9 , the corresponding links are as follows: BBBP, https:// deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv;Clin-Tox, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/clintox.csv.gz;Tox21, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/tox21.csv.gz;SIDER, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/sider.csv.gz;HIV, https://deepchemdata. s3-us-west-1.amazonaws.com/datasets/HIV.csv;BACE, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/bace.csv;ESOL, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv;FreeSolv, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/SAMPL.csv; Lipophilicity, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv;QM9, https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv.Source data are provided with this paper.Code availabilityIn our commitment to transparency and reproducibility, we have released our code showing our implementation.This encompasses methodologies for literature knowledge mining, knowledge inference rule mining and interpretable model training.Throughout this work, we have employed several open-source libraries, including Hugging Face, numpy, rdkit, pytorch, scipy, bitsandbytes and accelerate.The GitHub link of the model is https://github.com/zyzisastudyreallyhardguy/LLM4SD (https://doi.org/10.5281/zenodo.13986921)43.Furthermore, we are in the process of deploying a website to facilitate scientists in utilizing LLM4SD.The site features three core functionalities for scientific users: knowledge synthesis, knowledge inference and prediction with explanations.Examples of user interactions with the website can be found in the Supplementary Information.Author contributionsS.P. and G.I.W. supervised the project.Y.Z., H.Y.K. and J.J. contributed to the conception and design of the work.Y.Z., H.Y.K. and J.J. contributed to the technical implementation.Y.Z., H.Y.K. and J.J. prepared the figures.Y.Z.contributed to the design of the web-based application.A.T.N.N. and L.T.M. provided domain expertise for the literature review and validation of rules.Y.Z., H.Y.K., A.T.N.N. and L.T.M. contributed to the design of the rule validation test.All authors edited and revised the manuscript.Competing interestsThe authors declare no competing interests.Additional informationExtended data is available for this paper at https://doi.org/10.1038/s42256-025-00994-z.Supplementary informationThe online version contains supplementary material available at https://doi.org/10.1038/s42256-025-00994-z.Articlehttps://doi.org/10.1038/s42256-025-00994-zPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.
Are ideas getting harder to find?. N Bloom, C I Jones, J Reenen, M Webb, Am. Econ. Rev. 1102020</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 6202023</p>
<p>Baby steps in evaluating the capacities of large language models. M Frank, Nat. Rev. Psychol. 22023</p>
<p>Language models are few-shot learners. T Brown, Adv. Neural Inf. Process Syst. 332020</p>
<p>Gpt-4 technical report. J Achiam, 2023Preprint at</p>
<p>Health system-scale language models are all-purpose prediction engines. L Y Jiang, Nature. 6192023</p>
<p>Large language models encode clinical knowledge. K Singhal, Nature. 6202023</p>
<p>Are large language models superhuman chemists?. A Mirza, 2024</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, Chem. Sci. 92018</p>
<p>Galactica: a large language model for science. R Taylor, 2022</p>
<p>The Falcon series of open language models. E Almazrouei, 2023Preprint at</p>
<p>Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. C A Lipinski, F Lombardo, B W Dominy, P J Feeney, Adv. Drug Deliv. Rev. 642012</p>
<p>G Landrum, 10.5281/zenodo.14779836rdkit/rdkit: 2024_09_5 (Q3 2024) Release. Release_2024_09_5. 2025</p>
<p>10.1038/s42256-025-00994-zArticle. </p>
<p>Strategies for pre-training graph neural networks. W Hu, Proc. International Conference on Learning Representations. International Conference on Learning Representations2020</p>
<p>Graph contrastive learning with augmentations. Y You, Adv. Neural Inf. Process. Sys. 332020</p>
<p>Molecular contrastive learning of representations via graph neural networks. Y Wang, J Wang, Z Cao, A B Farimani, Nat. Mach. Intell. 42022</p>
<p>3D Infomax improves GNNs for molecular property prediction. H Stärk, Proc. International Conference on Machine Learning. K Chaudhuri, International Conference on Machine LearningPMLR2022</p>
<p>Pre-training molecular graph representation with 3D geometry. S Liu, Proc. 10th International Conference on Learning Representations. 10th International Conference on Learning Representations2022</p>
<p>Mole-bert: rethinking pre-training graph neural networks for molecules. J Xia, Proc. 11th International Conference on Learning Representations. 11th International Conference on Learning Representations2023</p>
<p>Self-supervised graph transformer on large-scale molecular data. Y Rong, Adv. Neural Inf. Process. Sys. 332020</p>
<p>Uni-mol: a universal 3D molecular representation learning framework. G Zhou, Proc. 11th International Conference on Learning Representations. 11th International Conference on Learning Representations2023</p>
<p>Extended-connectivity fingerprints. D Rogers, M Hahn, J. Chem. Inf. Model. 502010</p>
<p>A deep learning approach to antibiotic discovery. J M Stokes, Cell. 1802020</p>
<p>Discovery of a structural class of antibiotics with explainable deep learning. F Wong, Nature. 6262024</p>
<p>Chemllm: a chemical large language model. D Zhang, 2024Preprint at</p>
<p>ChemDFM: a large language foundation model for chemistry. Z Zhao, 38th Conference on Neural Information Processing Systems, Foundation Models for Science: Progress, Opportunities, and Challenges. NeurIPS2024</p>
<p>. Z Cai, 2024Internlm2 technical report. Preprint at</p>
<p>Llama: open and efficient foundation language models. H Touvron, 2023Preprint at</p>
<p>Exploring ChatGPT and its impact on society. M Haque, S Li, 10.1007/s43681-024-00435-4AI Ethics. 2024</p>
<p>Emergent abilities of large language models. J Wei, Transact. Mach. Learn. Res. 2022</p>
<p>P E Mcknight, J Najab, The Corsini Encyclopedia of Psychology. I B Weiner, W E Craighead, Wiley2010</p>
<p>Computational modeling of β-secretase 1 (BACE-1) inhibitors using ligand based approaches. G Subramanian, B Ramsundar, V Pande, R A Denny, J. Chem. Inf. Model. 562016</p>
<p>Defining desirable central nervous system drug space through the alignment of molecular properties, in vitro adme, and safety attributes. T Wager, ACS Chem. Neurosci. 12010</p>
<p>Moving beyond rules: the development of a central nervous system multiparameter optimization (cns mpo) approach to enable alignment of druglike properties. T Wager, X Hou, P Verhoest, A Villalobos, ACS Chem. Neurosci. 12010</p>
<p>Molecular determinants of blood-brain barrier permeation. W Geldenhuys, A Mohammad, C Adkins, P Lockman, Ther. Deliv. 62015</p>
<p>Lost in the middle: how language models use long contexts. N F Liu, Trans. Assoc. Comput. Linguist. 122024</p>
<p>The NLP task effectiveness of long-range transformers. G Qin, Y Feng, B Van Durme, Proc. 17th Conference of the European Chapter. A Vlachos, I Augenstein, 17th Conference of the European ChapterACL2023</p>
<p>UniProt: a worldwide hub of protein knowledge. The UniProt Consortium. 201947</p>
<p>. D A Benson, Nucleic Acids Res. 412013</p>
<p>Random forests. L Breiman, Mach. Learn. 452001</p>
<p>Can chatgpt be used to generate scientific hypotheses?. Y J Park, J. Materiomics. 102024</p>
<p>Smiles transformer: pre-trained molecular fingerprint for low data drug discovery. S Honda, S Shi, H R Ueda, 2019Preprint at</p>
<p>Code repository LLM4SD: release v. J Ju, 10.5281/zenodo.139869212024</p>
<p>The probable error of a mean. Student, Biometrika. 61908</p>            </div>
        </div>

    </div>
</body>
</html>
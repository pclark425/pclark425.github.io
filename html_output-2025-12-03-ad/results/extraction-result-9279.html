<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9279 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9279</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9279</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-273350728</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.11123v1.pdf" target="_blank">A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have the potential to enhance K-12 STEM education by improving both teaching and learning processes. While previous studies have shown promising results, there is still a lack of comprehensive understanding regarding how LLMs are effectively applied, specifically through prompt engineering-the process of designing prompts to generate desired outputs. To address this gap, our study investigates empirical research published between 2021 and 2024 that explores the use of LLMs combined with prompt engineering in K-12 STEM education. Following the PRISMA protocol, we screened 2,654 papers and selected 30 studies for analysis. Our review identifies the prompting strategies employed, the types of LLMs used, methods of evaluating effectiveness, and limitations in prior work. Results indicate that while simple and zero-shot prompting are commonly used, more advanced techniques like few-shot and chain-of-thought prompting have demonstrated positive outcomes for various educational tasks. GPT-series models are predominantly used, but smaller and fine-tuned models (e.g., Blender 7B) paired with effective prompt engineering outperform prompting larger models (e.g., GPT-3) in specific contexts. Evaluation methods vary significantly, with limited empirical validation in real-world settings.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9279.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9279.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot vs Zero-shot (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting compared to Zero-shot prompting (aggregated finding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across multiple reviewed K-12 STEM studies, providing a small number of examples (few-shot) generally improved LLM outputs' accuracy, contextual appropriateness, and stylistic stability compared to zero-shot prompting, though results vary by model and task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-series (GPT-3, GPT-3.5, GPT-4), LLaMA variants, BLOOM, Blender (various studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (e.g., 7B, 13B, 70B, 175B) or null if not specified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Problem solving, content generation, grading-related tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math and physics problem solving, generation of lesson materials, rubric-based evaluation and other STEM tasks where examples guide expected format/solution style.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting: include a small number of solved examples (one- to few-shot) preceding the test item to guide response format; compared against zero-shot prompts that provide the task only.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot prompting (task description only) and simple/open-ended prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing exemplars helps the LLM infer task structure, expected reasoning style and output formatting, thereby reducing ambiguity and stabilizing outputs; efficacy depends on model capacity and prompt length.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used in 13 reviewed articles; one-shot included; some papers reported qualitative or task-specific improvements. Note: smaller models sometimes did not benefit (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9279.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot negative on small models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Degraded Few-shot performance with small models (LLAMA-7B example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Some studies found that small LLMs (e.g., LLaMA-7B) performed worse with multi-example (five-shot) prompts than with zero-shot prompts, likely due to context-length or model-capacity limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General prompting tasks (evaluated in reviewed study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where longer prompts with several examples were provided (e.g., few-shot tasks requiring longer context windows).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Five-shot prompting (five examples included) compared to zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reported qualitatively as worse performance for LLaMA-7B with five-shot vs zero-shot in one reviewed study</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Small model capacity and limited ability to handle long prompts likely cause degradation when many examples inflate prompt length; larger models do not show this issue.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Reported in [53] analog: five-shot prompting degraded performance for LLaMA-7B but not observed for larger LLaMA-70B or GPT-3.5; attributed to prompt length and model size constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9279.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT prompts that request step-by-step reasoning often improve LLM performance on complex STEM problems by eliciting intermediate reasoning steps rather than only final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-series (notably GPT-4, GPT-3.5) and other models in some studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (e.g., GPT-4 unspecified parameters, GPT-3.5) or null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex reasoning/problem solving (physics, math), content analysis, automated grading</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems requiring multi-step logical reasoning (e.g., physics problems, multi-step math reasoning) and tasks where explanation chains aid scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Chain-of-Thought prompting: request or demonstrate intermediate steps/reasoning; applied in zero-shot or few-shot contexts and sometimes implemented iteratively (self-review).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct answer prompting (no CoT), few-shot/no-CoT, and combinations with rubrics or explicit scoring instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitatively reported improvements in reasoning coherence and accuracy across several studies; limited/no gains in some automated grading tasks unless combined with explicit rubric/task descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT guides the model to reveal intermediate steps, making reasoning explicit and reducing shortcut/shortcut-like hallucinations; effectiveness increases when combined with domain-specific guidance or rubrics and for models with sufficient reasoning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used in 8 articles; variations include few-shot CoT, iterative CoT (model revises its own output), and CoT integrated in training data. GPT-4 sometimes exhibits CoT-like behavior without explicit CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9279.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative CoT / Self-review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Chain-of-Thought / model self-review prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies that ask the model to generate a solution, then review and correct its own output across rounds can improve solution quality and error correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-series (examples: GPT-3, GPT-3.5, GPT-4 in cited studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various or null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Problem solving, code generation for problem solutions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step STEM problems where the model first produces an answer or code and is then prompted to output answer plus explanation after self-review.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Iterative CoT: multiple rounds where model produces an initial solution (sometimes code), then is prompted to produce an answer and explanation, or to review/correct previous outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single-pass CoT or direct-answer prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reported qualitative improvements in some studies (e.g., [27], [54]) when using iterative review versus single-pass outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Iterative review enables detection and correction of earlier reasoning errors and encourages more robust reasoning chains, especially when code or intermediate artifacts are generated first.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Examples: one study ran a review loop where LLM generated Python code solution first, then asked to output an answer and explanation; other studies had the LLM review prior outputs to improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9279.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Role-assigned Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Persona / Role-assigned prompting (teacher/student simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instructing LLMs to adopt roles (e.g., 'middle school math teacher') affects behavior — generating more pedagogically-aligned responses but sometimes avoiding direct answers unless explicitly instructed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-series, Blender-9B (in role-simulation comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Blender-9B (9B), GPT-3 (175B) where specified, others null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lesson planning, tutoring simulation, dialogic interactions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generating teacher-like guidance, feedback, or simulated student interactions for training or dataset generation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Persona-based prompt e.g., 'Suppose you are a middle school math teacher...' possibly with extra descriptive elements to tune behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Simple prompts without role assignment; fine-tuned smaller models versus larger pre-trained models in role tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: role prompts produce more contextually relevant and personalized responses; one study reports Blender-9B (fine-tuned) outperformed GPT-3 in teacher simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Assigning a role provides a framing that biases the model's response style toward pedagogical behaviors, but additional descriptors are often needed to prevent models from withholding direct answers (e.g., being overly Socratic).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Role-assigned prompting used in 7 reviewed articles; observed behaviors include guiding students rather than giving answers unless prompts explicitly request answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9279.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Output Formatting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Output Formatting (e.g., JSON, labeled outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting LLMs to follow strict output formats (JSON, labeled fields, code blocks) aids downstream parsing, programmatic analysis, and automated scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-series and other LLMs across several studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various or null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Content generation, data extraction, automated analysis/labeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generation of lesson plans, question-answer pairs, labeled annotations, or code outputs intended for programmatic ingestion.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Output-formatting prompts that explicitly instruct format (e.g., 'Output a JSON object with keys: question, answer, difficulty').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Unstructured natural language output</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Structured prompts constrain the output space, reduce variability, and ease automated evaluation/parsing; they improve reproducibility of downstream processing even if not improving raw 'accuracy'.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used in 10 articles; examples include instructing outputs in JSON for parsing and explicit labeling for readability or scoring models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9279.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combining LLM prompting with external retrieval modules (RAG) can reduce hallucinations and provide up-to-date or specialized domain knowledge, improving response depth and accuracy for knowledge-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA variants (in at least one reviewed study), other LLM backends possible</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>example: LLaMA-13B fine-tuned for EduChat in referenced work; otherwise varied</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question answering, tutoring, content generation requiring up-to-date or specialized knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where external documents or current data are needed to ensure factual correctness and mitigate training-data cutoff issues.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>RAG: retrieval of documents or knowledge snippets combined with a prompt that conditions the LLM on retrieved context before generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting without retrieval (pure pre-trained LLM responses)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reported qualitatively improved quality and depth of responses in one reviewed study that used RAG for STEM topics.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>RAG grounds model outputs in external evidence, reducing hallucination and enabling up-to-date answers; effectiveness relies on retrieval quality and integration method.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Only one reviewed study explicitly used RAG in the K-12 STEM context and reported improvements (cited as [16] in review).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9279.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature & Randomness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling Temperature and Output Variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Temperature setting affects diversity of outputs: higher temperature increases diversity (useful for data augmentation), while lower temperature reduces randomness but does not eliminate non-determinism; multiple runs are commonly used to assess stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (example), other GPT models referenced</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various or null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Data augmentation, simulated student responses, repeated evaluation of problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generating diverse student-like responses for augmentation; stability testing of model correctness over repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt unchanged; sampling temperature varied (e.g., set to 0 vs higher values) as an experimental factor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different temperature settings (e.g., 0 vs higher)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>One study found higher temperature produced more diverse simulated student responses but comparable augmentation utility to lower temperature; two studies explicitly set temperature to 0 for evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Higher temperature yields diversity useful for augmentation and reducing overfitting; even temperature=0 yields some variability per OpenAI docs, so repeated sampling is recommended over relying solely on low temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Two studies explicitly reported temperature=0 during evaluation ([13], [53]); one study ran ChatGPT 60 times on same problem to measure variability ([21]); 17 reviewed articles used repeated interactions to assess output stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9279.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-determinism & Reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-determinism of LLM outputs and reproducibility issues</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM outputs vary across runs due to stochastic sampling and other factors; this necessitates repeated runs and detailed prompt sharing to ensure reproducibility, but many reviewed studies lacked full prompt disclosure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-series and other evaluated LLMs across studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various or null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General evaluation tasks across reviewed papers</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assessments of model correctness, grading, and content generation where reproducibility matters for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Any prompt format — reproducibility affected by lack of prompt sharing, temperature, and stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because outputs are non-deterministic, single-run results can be misleading; multiple independent runs offer more reliable assessments; lack of prompt transparency hampers replication.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>17 articles used repeated independent interactions; 12 articles fully shared prompts, 11 partially, some not at all; reviewers recommend multiple runs and prompt sharing for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9279.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Small fine-tuned models outperforming larger models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance gains from smaller, fine-tuned models vs larger pre-trained LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Certain fine-tuned smaller models (e.g., Blender 9B, fine-tuned LLaMA variants) outperformed larger general-purpose LLMs (e.g., GPT-3) on specific tasks like teacher simulation and equitable tutoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Blender-9B, LLaMA-13B (fine-tuned EduChat), compared to GPT-3, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B (Blender), 13B (LLaMA), 175B (GPT-3) where noted</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Teacher simulation, question answering in Chinese K-12 suite, tutoring equity tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Simulating pedagogical interactions, question-answering on evaluation suite, and providing accurate/equitable tutoring responses.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Fine-tuning on domain/task-specific data combined with task-specific prompts / role prompts</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Larger pre-trained models used with prompting but not fine-tuned for the specific task</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reported qualitative or task-specific superiority of fine-tuned smaller models in certain contexts (e.g., Blender-9B > GPT-3 for teacher simulation; EduChat better than base LLaMA-13B but below GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Fine-tuning on domain-specific data yields models better aligned to task distributions and desired behavior; combined with careful prompt engineering, smaller models can be more effective and efficient for targeted tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Examples: [73] Blender-9B fine-tuned for teacher simulation; [16] LLaMA-13B fine-tuned for EduChat; comparison to GPT-4 and other larger models reported in review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9279.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual-content limitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations of text-only prompting for visual STEM problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies (including CoT) often fail to overcome LLM limitations on tasks involving visual content (e.g., kinematic graphs, geometric diagrams); GPT-4 performs well on algebra but struggles with visual geometry questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3 series, other LLMs evaluated in cited studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various or null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Physics kinematics graphs, geometric problems with diagrams, visual programming tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems requiring interpretation of graphs, diagrams, or visual inputs in STEM contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Text-only prompts possibly augmented with CoT; some multimodal evaluations where available</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Text-only vs multimodal-capable approaches (multimodal largely absent in reviewed studies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs trained primarily on text lack robust mechanisms to process and reason over visual representations; CoT and advanced prompting help limitedly but do not remedy fundamental multimodal shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Cited observations: CoT did not help sufficiently with kinematic graphs [21]; GPT-4 struggles on geometry involving visual elements [84]; visual programming tasks showed different prompting success compared to text-based programming [70].</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9279.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Combination: CoT + Task Descriptions/Rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combining Chain-of-Thought with explicit task descriptions or rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pairing CoT prompting with detailed zero-shot task descriptions (e.g., scoring rubrics) or additional constraints yields better results in content analysis and automated grading than CoT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-series (examples include GPT-3.5, GPT-4 in referenced studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various or null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Automated grading, content analysis, rubric-aligned scoring</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluating student responses against nuanced criteria where rubric-guided scoring is required.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>CoT prompting combined with explicit scoring rubrics or enriched zero-shot task descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>CoT alone, or CoT without explicit task/rubric guidance</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative improvements reported: CoT alone gave limited gains in automated grading whereas CoT + rubric/task description significantly enhanced accuracy and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Rubrics supply objective evaluation criteria that the model can follow when CoT reveals intermediate reasoning; combining both reduces ambiguity in scoring and improves alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Reported in review: [36] found significant enhancement when combining CoT with more detailed zero-shot task descriptions and scoring rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9279.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e9279.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Repeated sampling for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of repeated independent runs to assess prompt robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Because LLM outputs are non-deterministic, many studies run prompts multiple times (e.g., 3, 60) to obtain reliable performance estimates; single-run evaluations are unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various GPT models in reviewed studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various or null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Model performance evaluation across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assessing model accuracy, stability, and distribution of outputs on tasks like physics problem solving and simulated dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Same prompt repeated across multiple independent runs; sometimes multiple outputs per prompt are collected</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single-run evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Repeated sampling captures variability and gives more robust measures of typical performance and output distribution; studies reported narrow performance distributions and randomness across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>17 articles adopted repeated runs; example: one study ran ChatGPT 60 times on same problems finding only three questions were consistently answered correctly in every run ([21]); other studies ran 3 outputs per prompt for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners. <em>(Rating: 2)</em></li>
                <li>Educhat: A large-scale language model-based chatbot system for intelligent education. <em>(Rating: 2)</em></li>
                <li>Improving assessment of tutoring practices using retrieval-augmented generation <em>(Rating: 2)</em></li>
                <li>Towards a formative feedback generation agent: Leveraging a human-in-the-loop, chain-of-thought prompting approach with LLMs to evaluate formative assessment responses in K-12 science. <em>(Rating: 1)</em></li>
                <li>Evaluating ChatGPT and GPT-4 for Visual Programming. <em>(Rating: 1)</em></li>
                <li>Is GPT-4 a reliable rater? Evaluating consistency in GPT-4's text ratings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9279",
    "paper_id": "paper-273350728",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Few-shot vs Zero-shot (general)",
            "name_full": "Few-shot prompting compared to Zero-shot prompting (aggregated finding)",
            "brief_description": "Across multiple reviewed K-12 STEM studies, providing a small number of examples (few-shot) generally improved LLM outputs' accuracy, contextual appropriateness, and stylistic stability compared to zero-shot prompting, though results vary by model and task.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-series (GPT-3, GPT-3.5, GPT-4), LLaMA variants, BLOOM, Blender (various studies)",
            "model_size": "various (e.g., 7B, 13B, 70B, 175B) or null if not specified",
            "task_name": "Problem solving, content generation, grading-related tasks",
            "task_description": "Math and physics problem solving, generation of lesson materials, rubric-based evaluation and other STEM tasks where examples guide expected format/solution style.",
            "presentation_format": "Few-shot prompting: include a small number of solved examples (one- to few-shot) preceding the test item to guide response format; compared against zero-shot prompts that provide the task only.",
            "comparison_format": "Zero-shot prompting (task description only) and simple/open-ended prompts.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Providing exemplars helps the LLM infer task structure, expected reasoning style and output formatting, thereby reducing ambiguity and stabilizing outputs; efficacy depends on model capacity and prompt length.",
            "null_or_negative_result": null,
            "experimental_details": "Few-shot used in 13 reviewed articles; one-shot included; some papers reported qualitative or task-specific improvements. Note: smaller models sometimes did not benefit (see separate entry).",
            "uuid": "e9279.0",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Few-shot negative on small models",
            "name_full": "Degraded Few-shot performance with small models (LLAMA-7B example)",
            "brief_description": "Some studies found that small LLMs (e.g., LLaMA-7B) performed worse with multi-example (five-shot) prompts than with zero-shot prompts, likely due to context-length or model-capacity limitations.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "LLaMA-7B",
            "model_size": "7B",
            "task_name": "General prompting tasks (evaluated in reviewed study)",
            "task_description": "Tasks where longer prompts with several examples were provided (e.g., few-shot tasks requiring longer context windows).",
            "presentation_format": "Five-shot prompting (five examples included) compared to zero-shot prompting",
            "comparison_format": "Zero-shot prompting",
            "performance": null,
            "performance_comparison": "Reported qualitatively as worse performance for LLaMA-7B with five-shot vs zero-shot in one reviewed study",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Small model capacity and limited ability to handle long prompts likely cause degradation when many examples inflate prompt length; larger models do not show this issue.",
            "null_or_negative_result": true,
            "experimental_details": "Reported in [53] analog: five-shot prompting degraded performance for LLaMA-7B but not observed for larger LLaMA-70B or GPT-3.5; attributed to prompt length and model size constraints.",
            "uuid": "e9279.1",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "CoT prompts that request step-by-step reasoning often improve LLM performance on complex STEM problems by eliciting intermediate reasoning steps rather than only final answers.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-series (notably GPT-4, GPT-3.5) and other models in some studies",
            "model_size": "various (e.g., GPT-4 unspecified parameters, GPT-3.5) or null",
            "task_name": "Complex reasoning/problem solving (physics, math), content analysis, automated grading",
            "task_description": "Problems requiring multi-step logical reasoning (e.g., physics problems, multi-step math reasoning) and tasks where explanation chains aid scoring.",
            "presentation_format": "Chain-of-Thought prompting: request or demonstrate intermediate steps/reasoning; applied in zero-shot or few-shot contexts and sometimes implemented iteratively (self-review).",
            "comparison_format": "Direct answer prompting (no CoT), few-shot/no-CoT, and combinations with rubrics or explicit scoring instructions.",
            "performance": null,
            "performance_comparison": "Qualitatively reported improvements in reasoning coherence and accuracy across several studies; limited/no gains in some automated grading tasks unless combined with explicit rubric/task descriptions.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "CoT guides the model to reveal intermediate steps, making reasoning explicit and reducing shortcut/shortcut-like hallucinations; effectiveness increases when combined with domain-specific guidance or rubrics and for models with sufficient reasoning capacity.",
            "null_or_negative_result": null,
            "experimental_details": "Used in 8 articles; variations include few-shot CoT, iterative CoT (model revises its own output), and CoT integrated in training data. GPT-4 sometimes exhibits CoT-like behavior without explicit CoT prompts.",
            "uuid": "e9279.2",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Iterative CoT / Self-review",
            "name_full": "Iterative Chain-of-Thought / model self-review prompting",
            "brief_description": "Prompting strategies that ask the model to generate a solution, then review and correct its own output across rounds can improve solution quality and error correction.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-series (examples: GPT-3, GPT-3.5, GPT-4 in cited studies)",
            "model_size": "various or null",
            "task_name": "Problem solving, code generation for problem solutions",
            "task_description": "Multi-step STEM problems where the model first produces an answer or code and is then prompted to output answer plus explanation after self-review.",
            "presentation_format": "Iterative CoT: multiple rounds where model produces an initial solution (sometimes code), then is prompted to produce an answer and explanation, or to review/correct previous outputs.",
            "comparison_format": "Single-pass CoT or direct-answer prompts",
            "performance": null,
            "performance_comparison": "Reported qualitative improvements in some studies (e.g., [27], [54]) when using iterative review versus single-pass outputs.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Iterative review enables detection and correction of earlier reasoning errors and encourages more robust reasoning chains, especially when code or intermediate artifacts are generated first.",
            "null_or_negative_result": null,
            "experimental_details": "Examples: one study ran a review loop where LLM generated Python code solution first, then asked to output an answer and explanation; other studies had the LLM review prior outputs to improve accuracy.",
            "uuid": "e9279.3",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Role-assigned Prompting",
            "name_full": "Persona / Role-assigned prompting (teacher/student simulation)",
            "brief_description": "Instructing LLMs to adopt roles (e.g., 'middle school math teacher') affects behavior — generating more pedagogically-aligned responses but sometimes avoiding direct answers unless explicitly instructed.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-series, Blender-9B (in role-simulation comparisons)",
            "model_size": "Blender-9B (9B), GPT-3 (175B) where specified, others null",
            "task_name": "Lesson planning, tutoring simulation, dialogic interactions",
            "task_description": "Generating teacher-like guidance, feedback, or simulated student interactions for training or dataset generation.",
            "presentation_format": "Persona-based prompt e.g., 'Suppose you are a middle school math teacher...' possibly with extra descriptive elements to tune behavior.",
            "comparison_format": "Simple prompts without role assignment; fine-tuned smaller models versus larger pre-trained models in role tasks.",
            "performance": null,
            "performance_comparison": "Qualitative: role prompts produce more contextually relevant and personalized responses; one study reports Blender-9B (fine-tuned) outperformed GPT-3 in teacher simulation tasks.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Assigning a role provides a framing that biases the model's response style toward pedagogical behaviors, but additional descriptors are often needed to prevent models from withholding direct answers (e.g., being overly Socratic).",
            "null_or_negative_result": null,
            "experimental_details": "Role-assigned prompting used in 7 reviewed articles; observed behaviors include guiding students rather than giving answers unless prompts explicitly request answers.",
            "uuid": "e9279.4",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Output Formatting",
            "name_full": "Structured Output Formatting (e.g., JSON, labeled outputs)",
            "brief_description": "Prompting LLMs to follow strict output formats (JSON, labeled fields, code blocks) aids downstream parsing, programmatic analysis, and automated scoring.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-series and other LLMs across several studies",
            "model_size": "various or null",
            "task_name": "Content generation, data extraction, automated analysis/labeling",
            "task_description": "Generation of lesson plans, question-answer pairs, labeled annotations, or code outputs intended for programmatic ingestion.",
            "presentation_format": "Output-formatting prompts that explicitly instruct format (e.g., 'Output a JSON object with keys: question, answer, difficulty').",
            "comparison_format": "Unstructured natural language output",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Structured prompts constrain the output space, reduce variability, and ease automated evaluation/parsing; they improve reproducibility of downstream processing even if not improving raw 'accuracy'.",
            "null_or_negative_result": null,
            "experimental_details": "Used in 10 articles; examples include instructing outputs in JSON for parsing and explicit labeling for readability or scoring models.",
            "uuid": "e9279.5",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG)",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "Combining LLM prompting with external retrieval modules (RAG) can reduce hallucinations and provide up-to-date or specialized domain knowledge, improving response depth and accuracy for knowledge-intensive tasks.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "LLaMA variants (in at least one reviewed study), other LLM backends possible",
            "model_size": "example: LLaMA-13B fine-tuned for EduChat in referenced work; otherwise varied",
            "task_name": "Question answering, tutoring, content generation requiring up-to-date or specialized knowledge",
            "task_description": "Tasks where external documents or current data are needed to ensure factual correctness and mitigate training-data cutoff issues.",
            "presentation_format": "RAG: retrieval of documents or knowledge snippets combined with a prompt that conditions the LLM on retrieved context before generation.",
            "comparison_format": "Standard prompting without retrieval (pure pre-trained LLM responses)",
            "performance": null,
            "performance_comparison": "Reported qualitatively improved quality and depth of responses in one reviewed study that used RAG for STEM topics.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "RAG grounds model outputs in external evidence, reducing hallucination and enabling up-to-date answers; effectiveness relies on retrieval quality and integration method.",
            "null_or_negative_result": null,
            "experimental_details": "Only one reviewed study explicitly used RAG in the K-12 STEM context and reported improvements (cited as [16] in review).",
            "uuid": "e9279.6",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Temperature & Randomness",
            "name_full": "Sampling Temperature and Output Variability",
            "brief_description": "Temperature setting affects diversity of outputs: higher temperature increases diversity (useful for data augmentation), while lower temperature reduces randomness but does not eliminate non-determinism; multiple runs are commonly used to assess stability.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 (example), other GPT models referenced",
            "model_size": "various or null",
            "task_name": "Data augmentation, simulated student responses, repeated evaluation of problem solving",
            "task_description": "Generating diverse student-like responses for augmentation; stability testing of model correctness over repeated runs.",
            "presentation_format": "Prompt unchanged; sampling temperature varied (e.g., set to 0 vs higher values) as an experimental factor.",
            "comparison_format": "Different temperature settings (e.g., 0 vs higher)",
            "performance": null,
            "performance_comparison": "One study found higher temperature produced more diverse simulated student responses but comparable augmentation utility to lower temperature; two studies explicitly set temperature to 0 for evaluations.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Higher temperature yields diversity useful for augmentation and reducing overfitting; even temperature=0 yields some variability per OpenAI docs, so repeated sampling is recommended over relying solely on low temperature.",
            "null_or_negative_result": null,
            "experimental_details": "Two studies explicitly reported temperature=0 during evaluation ([13], [53]); one study ran ChatGPT 60 times on same problem to measure variability ([21]); 17 reviewed articles used repeated interactions to assess output stability.",
            "uuid": "e9279.7",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Non-determinism & Reproducibility",
            "name_full": "Non-determinism of LLM outputs and reproducibility issues",
            "brief_description": "LLM outputs vary across runs due to stochastic sampling and other factors; this necessitates repeated runs and detailed prompt sharing to ensure reproducibility, but many reviewed studies lacked full prompt disclosure.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-series and other evaluated LLMs across studies",
            "model_size": "various or null",
            "task_name": "General evaluation tasks across reviewed papers",
            "task_description": "Assessments of model correctness, grading, and content generation where reproducibility matters for evaluation.",
            "presentation_format": "Any prompt format — reproducibility affected by lack of prompt sharing, temperature, and stochasticity.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Because outputs are non-deterministic, single-run results can be misleading; multiple independent runs offer more reliable assessments; lack of prompt transparency hampers replication.",
            "null_or_negative_result": null,
            "experimental_details": "17 articles used repeated independent interactions; 12 articles fully shared prompts, 11 partially, some not at all; reviewers recommend multiple runs and prompt sharing for reproducibility.",
            "uuid": "e9279.8",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Small fine-tuned models outperforming larger models",
            "name_full": "Performance gains from smaller, fine-tuned models vs larger pre-trained LLMs",
            "brief_description": "Certain fine-tuned smaller models (e.g., Blender 9B, fine-tuned LLaMA variants) outperformed larger general-purpose LLMs (e.g., GPT-3) on specific tasks like teacher simulation and equitable tutoring.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Blender-9B, LLaMA-13B (fine-tuned EduChat), compared to GPT-3, GPT-4",
            "model_size": "9B (Blender), 13B (LLaMA), 175B (GPT-3) where noted",
            "task_name": "Teacher simulation, question answering in Chinese K-12 suite, tutoring equity tasks",
            "task_description": "Simulating pedagogical interactions, question-answering on evaluation suite, and providing accurate/equitable tutoring responses.",
            "presentation_format": "Fine-tuning on domain/task-specific data combined with task-specific prompts / role prompts",
            "comparison_format": "Larger pre-trained models used with prompting but not fine-tuned for the specific task",
            "performance": null,
            "performance_comparison": "Reported qualitative or task-specific superiority of fine-tuned smaller models in certain contexts (e.g., Blender-9B &gt; GPT-3 for teacher simulation; EduChat better than base LLaMA-13B but below GPT-4).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Fine-tuning on domain-specific data yields models better aligned to task distributions and desired behavior; combined with careful prompt engineering, smaller models can be more effective and efficient for targeted tasks.",
            "null_or_negative_result": null,
            "experimental_details": "Examples: [73] Blender-9B fine-tuned for teacher simulation; [16] LLaMA-13B fine-tuned for EduChat; comparison to GPT-4 and other larger models reported in review.",
            "uuid": "e9279.9",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Visual-content limitation",
            "name_full": "Limitations of text-only prompting for visual STEM problems",
            "brief_description": "Prompting strategies (including CoT) often fail to overcome LLM limitations on tasks involving visual content (e.g., kinematic graphs, geometric diagrams); GPT-4 performs well on algebra but struggles with visual geometry questions.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-4, GPT-3 series, other LLMs evaluated in cited studies",
            "model_size": "various or null",
            "task_name": "Physics kinematics graphs, geometric problems with diagrams, visual programming tasks",
            "task_description": "Problems requiring interpretation of graphs, diagrams, or visual inputs in STEM contexts.",
            "presentation_format": "Text-only prompts possibly augmented with CoT; some multimodal evaluations where available",
            "comparison_format": "Text-only vs multimodal-capable approaches (multimodal largely absent in reviewed studies)",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "LLMs trained primarily on text lack robust mechanisms to process and reason over visual representations; CoT and advanced prompting help limitedly but do not remedy fundamental multimodal shortcomings.",
            "null_or_negative_result": true,
            "experimental_details": "Cited observations: CoT did not help sufficiently with kinematic graphs [21]; GPT-4 struggles on geometry involving visual elements [84]; visual programming tasks showed different prompting success compared to text-based programming [70].",
            "uuid": "e9279.10",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Combination: CoT + Task Descriptions/Rubrics",
            "name_full": "Combining Chain-of-Thought with explicit task descriptions or rubrics",
            "brief_description": "Pairing CoT prompting with detailed zero-shot task descriptions (e.g., scoring rubrics) or additional constraints yields better results in content analysis and automated grading than CoT alone.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-series (examples include GPT-3.5, GPT-4 in referenced studies)",
            "model_size": "various or null",
            "task_name": "Automated grading, content analysis, rubric-aligned scoring",
            "task_description": "Evaluating student responses against nuanced criteria where rubric-guided scoring is required.",
            "presentation_format": "CoT prompting combined with explicit scoring rubrics or enriched zero-shot task descriptions",
            "comparison_format": "CoT alone, or CoT without explicit task/rubric guidance",
            "performance": null,
            "performance_comparison": "Qualitative improvements reported: CoT alone gave limited gains in automated grading whereas CoT + rubric/task description significantly enhanced accuracy and alignment.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Rubrics supply objective evaluation criteria that the model can follow when CoT reveals intermediate reasoning; combining both reduces ambiguity in scoring and improves alignment with human judgments.",
            "null_or_negative_result": null,
            "experimental_details": "Reported in review: [36] found significant enhancement when combining CoT with more detailed zero-shot task descriptions and scoring rubrics.",
            "uuid": "e9279.11",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Repeated sampling for evaluation",
            "name_full": "Use of repeated independent runs to assess prompt robustness",
            "brief_description": "Because LLM outputs are non-deterministic, many studies run prompts multiple times (e.g., 3, 60) to obtain reliable performance estimates; single-run evaluations are unreliable.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Various GPT models in reviewed studies",
            "model_size": "various or null",
            "task_name": "Model performance evaluation across tasks",
            "task_description": "Assessing model accuracy, stability, and distribution of outputs on tasks like physics problem solving and simulated dialogues.",
            "presentation_format": "Same prompt repeated across multiple independent runs; sometimes multiple outputs per prompt are collected",
            "comparison_format": "Single-run evaluation",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Repeated sampling captures variability and gives more robust measures of typical performance and output distribution; studies reported narrow performance distributions and randomness across runs.",
            "null_or_negative_result": null,
            "experimental_details": "17 articles adopted repeated runs; example: one study ran ChatGPT 60 times on same problems finding only three questions were consistently answered correctly in every run ([21]); other studies ran 3 outputs per prompt for comparisons.",
            "uuid": "e9279.12",
            "source_info": {
                "paper_title": "A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners.",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Educhat: A large-scale language model-based chatbot system for intelligent education.",
            "rating": 2,
            "sanitized_title": "educhat_a_largescale_language_modelbased_chatbot_system_for_intelligent_education"
        },
        {
            "paper_title": "Improving assessment of tutoring practices using retrieval-augmented generation",
            "rating": 2,
            "sanitized_title": "improving_assessment_of_tutoring_practices_using_retrievalaugmented_generation"
        },
        {
            "paper_title": "Towards a formative feedback generation agent: Leveraging a human-in-the-loop, chain-of-thought prompting approach with LLMs to evaluate formative assessment responses in K-12 science.",
            "rating": 1,
            "sanitized_title": "towards_a_formative_feedback_generation_agent_leveraging_a_humanintheloop_chainofthought_prompting_approach_with_llms_to_evaluate_formative_assessment_responses_in_k12_science"
        },
        {
            "paper_title": "Evaluating ChatGPT and GPT-4 for Visual Programming.",
            "rating": 1,
            "sanitized_title": "evaluating_chatgpt_and_gpt4_for_visual_programming"
        },
        {
            "paper_title": "Is GPT-4 a reliable rater? Evaluating consistency in GPT-4's text ratings",
            "rating": 1,
            "sanitized_title": "is_gpt4_a_reliable_rater_evaluating_consistency_in_gpt4s_text_ratings"
        }
    ],
    "cost": 0.0182335,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education</p>
<p>I-Sheng Chen 
Danyang Wang 
Jionghao Lin </p>
<p>Carnegie Mellon University
USA</p>
<p>University of Sheffield
UK</p>
<p>LUYI XU *
Harvard University
USA</p>
<p>CHEN CAO
University of Sheffield
UK</p>
<p>XIAO FANG
Massachusetts Institute of Technology
USA</p>
<p>Carnegie Mellon University
USA</p>
<p>A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education
A15FA1A7E3626248A0911F10D5B64B08Prompt EngineeringLarge Language ModelSystematic Literature Review
Large language models (LLMs) have the potential to enhance K-12 STEM education by improving both teaching and learning processes.While previous studies have shown promising results, there is still a lack of comprehensive understanding regarding how LLMs are effectively applied, specifically through prompt engineering-the process of designing prompts to generate desired outputs.To address this gap, our study investigates empirical research published between 2021 and 2024 that explores the use of LLMs combined with prompt engineering in K-12 STEM education.Following the PRISMA protocol, we screened 2,654 papers and selected 30 studies for analysis.Our review identifies the prompting strategies employed, the types of LLMs used, methods of evaluating effectiveness, and limitations in prior work.Results indicate that while simple and zero-shot prompting are commonly used, more advanced techniques like few-shot and chain-ofthought prompting have demonstrated positive outcomes for various educational tasks.GPT-series models are predominantly used, but smaller and fine-tuned models (e.g., Blender 7B) paired with effective prompt engineering outperform prompting larger models (e.g., GPT-3) in specific contexts.Evaluation methods vary significantly, with limited empirical validation in real-world settings.CCS Concepts: • Computing methodologies → Natural language processing; • Applied computing → Education.</p>
<p>INTRODUCTION</p>
<p>K-12 education refers to the publicly supported school grades prior to college in the United States and several other countries.The term "K-12" stands for "Kindergarten through 12th grade" and represents the full range of primary and secondary education.Within this system, a strong emphasis has been placed on STEM (Science, Technology, Engineering, and Mathematics) education as a means to prepare students for a technology-driven future.STEM education at the K-12 level focuses on building foundational knowledge in scientific inquiry, technological literacy, engineering principles, and mathematical reasoning [10,29,64].The K-12 STEM education emphasizes interdisciplinary learning, where students apply concepts from multiple domains to solve real-world challenges, such as integrating mathematics with science to tackle engineering problems [29].The importance of K-12 STEM education lies in its ability to prepare students for a rapidly evolving, technology-driven world by fostering critical thinking, creativity, and problem-solving skills from an early age [10].Students who engage in well-structured STEM curricula are more likely to pursue further education and careers in high-demand fields like information technology and engineering which are essential for technological innovation.Additionally, K-12 STEM education equips students with competencies such as analytical thinking, which prepare them for a wide range of career paths while enabling them to tackle complex problems [64].Recognizing the importance of STEM education at the K-12 level, it is essential to deliver K-12 STEM education at scale to ensure equitable access to individual students.Achieving this scalability requires the integration of advanced technologies that can enhance learning experiences, increase accessibility, and adapt to the diverse needs of students across different educational contexts.</p>
<p>Many previous works have focused on developing computer systems and artificial intelligence technologies to support K-12 STEM education [59].These technological advancements have demonstrated significant potential and efficacy in enhancing student learning.For example, Intelligent Tutoring Systems (ITS) are computer-based systems that provide personalized instruction and feedback to students based on their individual learning needs [12,61].In K-12 STEM education, ITS has been widely adopted across various K-12 STEM subjects, such as math [22,33], physics [12,60], and programming [41].Despite these promising developments, there remains a need for continued technological improvement in K-12 STEM education to fully meet the diverse needs of learners and ensure equitable access to high-quality STEM education.Recent advancements in artificial intelligence (AI), like Large Language Models (LLMs), offer the potential to further enhance how K-12 STEM education is delivered and supported, enabling personalized and accessible learning experiences at scale [32,87].</p>
<p>LLMs are AI models trained on vast amounts of text data to generate human-like text, comprehend context, and perform a wide range of tasks across various domains [31].The LLM models, such as GPT-4 [1] and Claude 3.5 [4], are capable of generating coherent and contextually relevant responses, making them valuable tools in STEM education [8,74].To interact with LLMs, users need to provide a segment of guiding text, known as a "prompt."A prompt serves as the input that instructs the model on how to respond, shaping the model's output based on the specific task at hand [9].Effective prompts are crucial for eliciting accurate, relevant, and contextually appropriate responses from the model [4,34].Achieving an effective prompt often requires a process known as prompt engineering, which involves carefully designing, refining, and iterating prompts to optimize the LLM's performance on specific tasks [45,77].Prompt engineering entails understanding how the model interprets the input, testing different phrasing, adjusting for specificity, and even incorporating additional steps, such as reasoning or formatting constraints, to improve results [45,77].Despite the growing interest in LLMs in education [11,40,78], prompt engineering is still in its early stages.There is limited understanding of how LLMs interpret prompts and the underlying mechanisms that affect task performance, making it a developing field in need of further exploration.Recognizing this gap, our study aims to systematically examine how existing research in K-12 STEM education has utilized prompt engineering, the methods they employed to evaluate these prompts, and the challenges or limitations encountered in the field.Additionally, we seek to identify key insights that can inspire future research directions in education and learning analytics.</p>
<p>BACKGROUND</p>
<p>Large Language Models in K12 STEM Education</p>
<p>Large Language Models (LLMs) are advanced artificial intelligence (AI) models designed to comprehend and generate human-like language by being trained on vast datasets of text [31].These models are termed "large" due to their extensive number of parameters-often reaching billions-which enable them to capture and replicate complex language patterns with high fidelity [55].As a result, LLMs have shown potential across many educational tasks [32], such as personalized tutoring [72] and automatic grading [20].The integration of LLMs into K-12 STEM education holds great promise for improving how students engage with challenging subjects such as mathematics, science, and technology [14,35].One of the key benefits of LLMs in this context is their ability to provide real-time feedback.Unlike traditional classroom settings, where feedback from teachers may be delayed, LLMs can offer immediate clarification and guidance, helping students to better understand complex material as they encounter difficulties.This instant feedback loop is particularly valuable in subjects like mathematics and physics, where students often need immediate assistance to grasp abstract concepts and solve problems efficiently [48,72].Despite the promise LLMs hold for improving K-12 STEM education, there are also limitations and challenges to consider.For instance, while LLMs are capable of generating coherent and accurate responses, they are not immune to errors, and their outputs may sometimes lack depth or accuracy, especially when it comes to highly specialized or domain-specific knowledge [25].Therefore, the process of designing prompts to guide LLMs for content generation is important, and human oversight remains critical to ensure the effective use of LLMs in educational contexts.</p>
<p>Prompt Engineering</p>
<p>Prompting refers to the specific input or instructions provided to LLMs to guide their output generation and shape their responses to various tasks and queries [9,47,65,67].Effective prompt engineering is crucial, as it directly influences the relevance and accuracy of the LLM's outputs [47,65,67].Prompt engineering involves the systematic crafting and refining of prompts to optimize the model's performance and generate desired outputs [47,65,67].According to various studies [65], prompting strategies can be categorized based on their purpose, such as prompting LLMs without extensive model training (e.g., Zero-shot [63] and Few-shot [9]), prompting LLMs to demonstrate reasoning and logic (e.g., Chain-of-Thoughts [80]), and prompting LLMs to mitigate hallucinations (e.g., Retrieval-Augmented Generation [37]).As the goal of this study is to examine the role of prompting strategies in STEM education, we primarily focus on the prompting strategies applied to previous works in K-12 STEM education.Simple Prompting (Direct Instruction Prompting): Users explicitly instruct LLMs to generate desired outputs without additional context or examples [65,67].For example, prompts like "Solve this algebraic equation" or "Explain Newton's third law" direct the model to respond directly to the query.Role-assigned Prompting (Persona-Based Prompting): Instructing the LLM to adopt a specific role or persona, such as a teacher, student, or domain expert, to generate responses aligned with that role [67,79,88].For example, "Act as a high school physics teacher" prompts the model to simulate how a teacher might explain complex topics, guide students, or provide feedback.Zero-Shot Prompting: The model is given a task and expected to generate a response based on its general knowledge, without prior examples or specific training for that task [63].Few-Shot Prompting: Providing the model with a small number of examples to guide its responses, enhancing its understanding of the task and leading to more accurate outputs [9].For instance, include solved examples of a problem before presenting a new, unsolved one.Chain of Thought (CoT) Prompting: Encouraging the model to reason through problems stepby-step, improving logical coherence and accuracy [80].This can be applied in both zero-shot and few-shot contexts and is valuable for tasks requiring complex reasoning.Retrieval-Augmented Generation (RAG): LLMs can have limitations, such as hallucinations, contradictory responses, incorrect reasoning, or outdated training data [6,7,15,17,23].Combining LLM prompting with external information retrieval mechanisms allows the model to access external knowledge bases to retrieve and output relevant and up-to-date information [37] and further improve accuracy [39].Output Formatting: Designing prompts that guide LLMs to generate responses in a structured, organized format, facilitating programmatic processing or quantitative analysis [67,83].</p>
<p>Existing works on systematic literature review of LLM in education</p>
<p>Prior systematic literature reviews have primarily focused on potentials of LLM applications [11,40,78,85].For example, [85] summarized the pracitcial applications of LLMs to differnt types of educational tasks including grading, teaching support, and feedback.However, the study [85] mainly focuse on the applications of LLMs prior to the advanced LLM models (e.g., GPT-4 and Claude) and did not in detail how the use of prompting strategies in delivering desired generated output.Similar to other systematic literature reviews on the use of LLMs in education [5,19,58,85], rare of them have specifically focused on the usage and impact of prompt engineering strategies in K-12 STEM educarion.To this end, our study seeks to address this gap by systematically reviewing the role of prompt engineering in K-12 STEM education.</p>
<p>While many previous studies [49,81,86] prompted LLMs to operate certain tasks related to K-12 STEM education, the question about how prompt engineering can best be leveraged to enhance learning outcomes still remains largely unknown as the the applications of LLM in K-12 STEM education remains in its early stages.Recognizing the significance of prompt engineering for educational purposes, our study aims to conduct a systematic literature review (SLR) that delves into the comprehensive effectiveness, strengths, and limitations of different prompt strategies in K-12 education.This includes evaluating their impact on learning outcomes, how they enhance interactions between students, instructors, and LLM-based systems, and the practical challenges faced when implementing these strategies in educational settings.By thoroughly examining existing research and current trends, our study aims to provide a detailed understanding of how prompt engineering can be further refined and applied to optimize its potential in K-12 STEM education.Thus, our research seeks to address the following Research Questions (RQ):</p>
<p>METHODS</p>
<p>Review Procedures</p>
<p>We followed the PRISMA [56] protocol to conduct our current systematic review on the use of prompt engineering with large language models in STEM education.A comprehensive search was conducted across several reputable databases, including Scopus, ACM Digital Library, IEEE Xplore, Web of Science (WoS), and Education Resources Information Center (ERIC) from ProQuest to ensure the inclusion of high-quality peer-reviewed publications.In addition, we performed supplementary searches through Arxiv and SSRN from Socpus (preprint) to capture publications that may not yet published.Our initial search query used was: TITLE-ABS-KEY ( ( chatgpt OR large AND language AND</p>
<p>model ) AND prompt AND education AND ( science OR technology OR engineering OR math OR steam ) ) AND PUBYEAR &gt; 2021 AND PUBYEAR &lt; 2025</p>
<p>Given the significant growth and development of prompt engineering and GPT [51] after 2021, we review papers from 2021 to January 25, 2024.To expand the range of potential sources and consider papers that have not yet been accepted or published, we included both peer-reviewed papers and preprints found through Scopus preprint searches.The initial search yielded 3,095 publications.After removing 441 duplicates, 2,654 papers remained for title and abstract screening.Our reviewing process is shown at Figure 1.</p>
<p>Notably, our study only considers papers written in English.A set of inclusion and exclusion criteria were used to select the relevant studies from the initial inquiry.First, studies that do not mention prompt engineering or use LLMs were excluded.Additionally, research unrelated to Generative AI was excluded, such as only using BERT for regression or classification.Third, papers that do not focus on STEM education were also excluded.Six reviewers with experience in education and computer science reviewed the titles and abstracts of the studies based on inclusion and exclusion criteria.After screening, 296 papers were included for the next step with a with an absolute percentage agreement score of 76.8%, indicating a moderate to good agreement between the reviewers [68] .The mild moderate agreement stems from differing views on the scope of STEM education, such as whether fields like statistics, medicine, and finance should be included in STEM.Ultimately, we refined the definition of STEM for this paper based on the inclusion criteria, ensuring clarity on the fields covered.Moreover, this issue was resolved later once we narrowed our scope to K-12 education.Following the full-text review, 146 papers were initially included for synthesis.However, due to the large number of papers remaining, we decided to focus exclusively on K12 education.As a result, 116 papers were excluded for non-K12 education.This left a final total of 30 studies, all of which met the inclusion criteria.</p>
<p>Data Anylysis</p>
<p>We conducted an in-depth qualitative analysis of the selected studies from three aspects to address the RQs described in Section 2.3.For RQ1, we gather information about the models and the prompt engineering strategies used in the study while also checking if the prompts are provided in the paper.Specifically, we explore the role of prompting LLMs.For RQ2, we check how these papers evaluate the effectiveness of their prompts with LLMs.As for RQ3, we focus on identifying the limitations discussed in these papers.</p>
<p>RESULTS</p>
<p>Table 1 provides an overview of references that apply LLMs to K-12 STEM education across various subjects, categorized by educational levels.The Table 1 shows that math has the most extensive research focus, particularly at the elementary and high school levels, with numerous studies applying LLMs to support learning tasks in this subject.Subjects like physics, chemistry, biology, and computer sciences are also represented, though to a lesser extent, with more studies occurring at the high school level as students engage with more specialized content.Additionally, interdisciplinary subjects, including psychology, environmental science, and general sciences, are present but show fewer applications of LLMs across all educational levels.Overall, the table highlights that while the application of LLMs has primarily focused on the math domain, there is growing interest in extending these models to support a wider range of STEM subjects.</p>
<p>Commonly used prompting strategies in K-12 STEM education contexts (RQ 1.1)</p>
<p>In our study, we identified various prompting strategies employed in previous studies under the K-12 STEM education context, as shown in Table 2.These strategies include simple prompting, role-assigned prompting, zero-shot prompting, few-shot prompting, chain of thought prompting, retrieval-augmented generation, and output formatting, which were introduced in Section 2.2.These strategies are often compatible and can be combined to use together.4.1.1Simple Prompting.Simple prompting, or direct instruction prompting, was used in 12 of the reviewed articles.This straightforward approach involves explicitly asking LLMs to generate desired Note: Other subjects involved are Psychology [13,16]; General Sciences [2]; Natural Science and Social Science [42]; Physical Sciences [36]; Environmental Science [13].outputs without additional context or examples.For instance, [13] used GPT-3.5 for similar problem creation by prompting the model to "paraphrase this sentence."Similarly, [26] asked ChatGPT to "design a lesson plan for an eighth-grade class focusing on square roots and cube roots."Most studies employing simple prompting explored how LLMs could support educators by generating educational materials, such as lesson planning [26,71], or by addressing subject-specific inquiries in STEM fields [7,15].Our result indicated that while simple prompting is intuitive and accessible, it may not always produce outputs that fully meet user expectations or standards of accuracy.To be specific, several studies noted that LLM outputs generated via simple prompting could be too narrow and cannot consider different aspects [53] or require significant post-generation review, editing, and iterating by human expert to reach acceptable quality [7,15,23,26,71].</p>
<p>4.1.2Role-assigned Prompting.Role-assigned prompting, or persona-based prompting, was applied in 7 of the reviewed articles.This strategy involves instructing LLMs to assume specific roles to generate responses aligned with those roles.Common roles assigned were teachers and students.For example, [53] prompted the LLM with "Suppose you are a middle school math teacher, " resulting in the model providing guidance and encouragement akin to a teacher.Studies used teacher roles for lesson planning [71] and student guidance [2,16,53,73], while student roles were used to simulate student interactions for teacher training or dataset construction [7,43].</p>
<p>Role-assigned prompting allows LLMs to generate more contextually relevant and personalized responses but requires a careful prompt design to avoid unintended behaviors.Take "Suppose you are a middle school math teacher."[53] as an example; the research found that even when the prompt also instructs LLMs to share answers, LLMs with role-assigned prompts tend to guide students and avoid giving answers away.This potential issue may be alleviated by multiple descriptive elements to facilitate LLMs to generate more personalized and contextually relevant responses [2].</p>
<p>4.1.3Zero-Shot Prompting.Zero-shot prompting was utilized in 18 of the reviewed articles, primarily in solving math and physics problems.The difference between the Zero-shot and simple prompting techniques is that the zero-shot method asks LLMs to finish a task, while the simple prompt's output is more open-ended.To be specific, the model is tasked with generating solutions based on its pre-trained knowledge without prior examples.For instance, [21,27,38,44] evaluated the efficacy of zero-shot prompting in STEM subjects.Studies found that providing more detailed task descriptions or additional information could enhance the model's performance [28,36,38,53,70].</p>
<p>4.1.4Few-Shot Prompting.Few-shot prompting was employed in 13 articles.In this article, we integrated the one-shot prompting in this category.By providing the model with a few examples, researchers observed improved performance compared to zero-shot prompting [18,27,28,36,54,57,62,76]. For instance, [28] found that few-shot prompting led to more accurate and contextually appropriate outputs by helping the model better understand task nuances.Few-shot prompting also helped stabilize output style, producing more standardized responses [36].Nevertheless, [53] found that smaller models like LLAMA-7B perform worse with five-shot prompting compared to zero-shot, likely due to the model size being too small to handle long prompts, since this issue isn't seen in larger models like LLAMA 70B or GPT-3.5.</p>
<p>Chain of Thought</p>
<p>Prompting.Chain of Thought (CoT) prompting was used in 8 articles to encourage LLMs to generate intermediate reasoning steps rather than direct answers.This strategy is particularly effective in STEM education, where step-by-step reasoning and comprehensive explanations are essential.Studies applied CoT prompting for content analysis [27,28,36], problemsolving [18,21,36], and content generation [43].</p>
<p>CoT strategies can be integrated within training data or used in few-shot examples to guide models implicitly towards CoT reasoning (i.e., few-shot CoT) [18].As training datasets expand, advanced models like GPT-4 exhibit CoT-like reasoning without explicit CoT prompts.For instance, [21] observed GPT-4 naturally breaking down complex physics problems into smaller steps, structuring solutions without explicit CoT instructions.CoT also works effectively across multiple rounds of interaction.[27] improved LLM's performance by having the LLM review and correct its prior output.While [54] applied an iterative CoT approach that asked LLMs to generate Python code solution for the problem first, then ask it to output the answer and explanation.CoT's effectiveness increases when combined with other prompting strategies.For example, [36] found that while CoT alone led to limited improvements in automated grading tasks, combining CoT with more zero-shot task descriptions (e.g., scoring rubrics) significantly enhanced content analysis and output accuracy.</p>
<p>Retrieval-Augmented Generation (RAG)</p>
<p>. Only one study [16] employed RAG to enhance LLM performance in STEM education.The study [16] demonstrated that RAG improved the quality and depth of responses in subjects like math and literature by providing up-to-date or specialized knowledge not included in the LLM's original training data.</p>
<p>4.1.7Output Formatting.Formatted outputs were utilized in 10 articles to guide LLMs in generating responses in structured formats, facilitating further programmatic processing or analysis.For example, some studies instructed the LLM to output data in JSON format for easy parsing [3,27,42], while others directed the model to produce code outputs or explicitly label specific types of information [28,36,53].</p>
<p>Types of LLMs used with these prompting strategies (RQ 1.2)</p>
<p>The majority of reviewed articles (n = 28) employed models from the GPT series, including GPT-4, GPT-3.5, GPT-3, and GPT-2 [9,55,63].However, a smaller subset of articles (n = 4) utilized alternative models.[16] fine-tuned the LLaMA 13B model [75] to create the EduChat model for questionanswering tasks in the Chinese K-12 evaluation suite [30].Although EduChat outperformed other LLaMA 13B models, it was less effective than GPT-4 and other larger LLMs.[52] used LLaMA-2-7B, LLaMA-2-70B, and BLOOMZ-7.1B-MTalongside GPT series models to assess performance on multiple-choice math questions, concluding that GPT-4 demonstrated the highest performance.</p>
<p>[76] compared BLOOM and YouChat against ChatGPT, GPT-3, and traditional machine learning methods in evaluating responses to open-ended math questions.They found that while GPT-3 outperformed BLOOM and YouChat, all tested LLMs performed worse than traditional machine learning methods.</p>
<p>Interestingly, some studies revealed that smaller, fine-tuned models could outperform larger GPT-series models in specific tasks.[73] showed that a fine-tuned smaller model, Blender 9B, outperformed GPT-3 (175B parameters) in simulating the role of a teacher.Similarly, [16] found that smaller fine-tuned models exhibited superior performance in terms of accuracy and equitable tutoring compared to larger models like ChatGPT, despite the latter being pre-trained on a significantly larger dataset with a higher parameter count.These findings suggest that while GPT-series models generally exhibit strong performance, smaller fine-tuned models or even traditional machine learning (ML) methods can sometimes achieve better results on certain tasks.Therefore, combining these approaches-such as fine-tuning GPT models or integrating outputs from traditional ML methods-may yield improved performance over relying on a single method.</p>
<p>Lastly, two research exclusively utilized other LLMs.[50] fine-tuned the RoBERTa-Longformer model to extend the input context window for classification tasks aimed at providing formative feedback in intelligent textbooks.[3] fine-tuned the LLaMA2-7B model, expanding a set of physics problems from an initial count of 766 to 7,983, thereby enriching educational resources.</p>
<p>Implementation of LLM-based systems in K-12 STEM education (RQ 1.3)</p>
<p>In our study, we identified 9 articles that specifically investigated interactions between LLMs and human participants.[73] developed a web interface enabling teachers on Prolific to interact with AIsimulated students, effectively generating teacher-student interaction data.[69] implemented a web platform for Amazon Mechanical Turk participants to engage with Socratic sub-questions generated by GPT-3, which improved the success rates of their study.[7] utilized ChatGPT to facilitate student discussions on quantum physics, resulting in improved student perceptions and a more cautious approach to using ChatGPT after the intervention.[82] conducted a quasi-experiment where GPT-3 was integrated with iPads via LINE, significantly enhancing students' intrinsic motivation and self-regulation in mathematics learning.Five additional studies developed demonstration interfaces but did not conduct user studies or evaluate effectiveness.For example, [84] and [54] integrated GPT-4 for personalized tutoring.Meanwhile, [2,16,53] developed demo interfaces based on their research findings.</p>
<p>Evaluation methods to assess the performance of prompting strategies (RQ 2.1)</p>
<p>There are 8 papers [15,21,23,26,38,57,71,84] that offer no comparison; they either present a prompt or an application and simply demonstrate its potential.Additionally, 13 studies compare a single prompt with either a dataset or human performance to assess the extent to which large language models (LLMs) achieve high performance [18, 42-44, 53, 54], as well as which LLM performs better in various contexts [27,42,50,52,70,73,76].Only 9 articles [16,27,42,50,52,69,70,73,76] establish prompting strategies and conduct comparisons to evaluate which prompting methods yield better results.</p>
<p>Evaluation of prompt engineering strategies.</p>
<p>A total of 18 articles evaluate the LLM's performance across various prompting strategies [2, 7, 15, 16, 18, 26-28, 36, 38, 42, 44, 57, 70, 71, 73, 76, 84].In addition, because of the non-determinism of LLM outputs, repeated independent interactions were adopted in 17 articles to obtain a more reliable assessment of the outputs [2, 13, 18, 21, 27, 36, 42-44, 50, 52-54, 57, 62, 73, 76].For example, [2] developed an interface allowing users to interact with the chatbot and collected data for independent evaluation, with each prompt running three outputs to compare the effectiveness of different custom-designed prompt personalities.Similarly, [21] had ChatGPT solve the same problem 60 times and found that it consistently answered only three questions correctly every time, with a degree of randomness in accuracy for the remaining cases.</p>
<p>4.4.2Temperature.Some articles mention the issue of temperature, with only two studies [13,53] explicitly stating that they set the temperature to zero during evaluation.[13] found that a higher temperature for GPT-3.5 for data augmentation on stimulated students yields comparable results to a lower temperature.The benefit was generating more diverse student responses, reducing overfitting even with more augmentation data points.In fact, according to OpenAI's documentation1 , even when the temperature is set to 0, there will still be some variability in the results, though with a lower probability.Therefore, thoroughly evaluating multiple times may be more effective than simply lowering the temperature.</p>
<p>Reproducibility.</p>
<p>Regarding the reproducibility of the findings in reviewed articles, we observed that 12 articles provided prompts that are accessible, replicable, and reproducible [16, 18, 27, 28, 42-44, 54, 57, 62, 76, 84].Then, 11 articles partially shared their prompts [2,15,21,23,26,36,38,53,70,73,82]. 2 articles [3,13] provided prompt templates, offering an outline of the prompt's structure.Additionally, 1 article included the prompt in an image format, which may limit its accessibility for replication purposes [7].Another study did not make its prompt available [69], and in one case, the prompt was intended to be accessible via GitHub, but the link was unfortunately inactive at the time of review [52].By reviewing 30 articles in K-12 STEM education, roughly 40% (12 out of 30 articles) demonstrated prompts that are accessible and replicable, while the others lack detailed prompts and transparent methodologies, which hinder the reproducibility of studies.We acknowledge that there may be various reasons behind the limited availability of prompts.However, without access to the exact prompts used, it is challenging for educational practitioners to effectively apply LLMs to the educational task and other researchers to replicate the results or build upon the work.This issue is compounded by the non-deterministic nature of LLM outputs, as minor variations in prompts or model parameters can lead to different outcomes.Thus, we encourage future studies to consider sharing their materials in a fully accessible manner to promote greater reproducibility in this important area of research.</p>
<p>Metrics to measure the performance of prompting strategies with LLMs (RQ 2.2)</p>
<p>Our findings highlight the diverse evaluation methods employed by various researchers.14 articles utilize custom indicators [2,15,16,18,21,23,36,38,42,54,57,62,69,73] such as determining whether a given lesson plan aligns with the 5Es instructional model [26].Some of these custom indicators are evaluated post-hoc by human coders, who calculate consensus scores.The resulting Cohen's  values range between 0.72 and 0.89, as demonstrated in [2,21,46,62].In other cases, machine learning models or predefined rules are used to compute the indicators, followed by a direct comparison of numerical values.For instance, [54] employs a BERT model along with several readability index models in regression analysis to assess the readability of the generated outputs.Moreover, there are 15 that evaluate the prompting strategy and LLM performance based on traditional machine learning metrics, such as accuracy, F1 Score, and precision [3,13,18,21,28,36,43,44,50,52,53,57,62,73,76].Then, some studies incorporate statistical metrics such as mean and standard deviation in their evaluations to analyze the distribution of GPT's performance [24,66].For example, [21] found that although GPT's accuracy in solving physics problems was comparable to that of students, the actual performance distribution was much narrower than that of students.Moreover, GPT exhibited particularly strong performance on some questions while performing poorly on others, in contrast to students, whose errors were more evenly distributed across different problems.</p>
<p>4.6 Effectiveness of LLM-based system in real-world settings (RQ 2.3) Only 2 articles have conducted real-world experiments with students to validate LLM-based prompting strategies in educational settings [7,82].The study by [7] involved a pilot study in a secondary school physics classroom, where 53 students participated in a two-lesson intervention using ChatGPT.The study found that the integration of ChatGPT had a positive impact on students' perceptions of the tool, with increased agreement on the benefits of using ChatGPT in daily life and education.Students also appreciated its potential for supporting critical thinking, especially in abstract subjects like quantum physics.The study by [82] focused on using a ChatGPT-based Intelligent Learning Aid (CILA) system in a blended learning environment to promote self-regulation and knowledge construction in a mathematics class.This study involved 70 students and compared the effectiveness of CILA system with traditional search engines like Google.Findings indicated that CILA significantly enhanced students' self-regulation and knowledge construction, particularly by providing real-time, convergent information that minimized learning interruptions.Other papers have not directly engaged with real users; instead, they rely on expert reviews, individual researcher judgments, theoretical frameworks, or machine learning models as the basis for their findings.</p>
<p>Limitations of using LLMs with prompting strategies in previous studies (RQ 3)</p>
<p>Despite the promising advancements in leveraging LLMs with prompting strategies for K-12 STEM education, the research we reviewed indicated several limitations related to prompt engineering.</p>
<p>Complexity in Designing Effective</p>
<p>Prompts.Designing prompts that effectively elicit the desired responses from LLMs is a complex task that often requires iterative refinement and domain expertise.[2] emphasized the need for clear and precise prompts to maximize the benefits of using LLMs as teaching assistants.However, the process of crafting such prompts can be time-consuming and may require specialized knowledge in both the subject matter and prompt engineering techniques.This complexity can be a barrier for educators who may not have the resources or expertise to develop and evaluate optimal prompts for their specific educational needs.4.7.2Sensitivity to Prompt Variations.One key limitation is the sensitivity of LLMs to minor changes in prompts, which can lead to very different outputs and affect the reliability of results.For instance, [21] found that ChatGPT's performance on physics problems was highly sensitive to subtle modifications in the prompt wording, resulting in varying levels of accuracy.This sensitivity complicates the design of effective prompts, as even minor adjustments can significantly impact the model's responses, making it challenging to achieve consistent performance across different tasks or educational contexts.4.7.3Lack of Prompt Generalizability and Scalability.The effectiveness of specific prompting strategies may not be generalized across different subjects or tasks.As observed by [70], prompting techniques worked well for text-based programming did not yield similar success in visual programming tasks.Similarly, [28] noted that simple few-shot prompts with limited examples were insufficient for capturing nuanced criteria in evaluating the tutor's praise, suggesting that more elaborate and domain-specific prompts are necessary.[36] also noted that CoT without domainspecific information leads to limited improvements, as LLMs struggle to break down tasks effectively.This limitation indicates that prompt engineering often requires significant customization for each specific application, limiting the scalability of these strategies.4.7.4Dependence on Model-Specific Features.Prompting strategies sometimes rely on features specific to certain LLMs, limiting their applicability to other models.For example, some advanced prompting techniques may leverage the large context window of GPT-4, which is not available in smaller or open-source models [27,52].This dependence on proprietary models raises concerns about accessibility and equity, as not all educators or researchers have the resources to access the latest or most capable LLMs.Additionally, fine-tuned smaller models sometimes outperform larger models when properly fine-tuned and prompted [73], suggesting that prompt engineering must be tailored not only to the task but also to the specific model being used.4.7.5 Limitations in Handling Visual Educational Content.Prompting strategies may not sufficiently overcome LLM limitations in handling visual educational content.For instance, [21] observed that despite using chain-of-thought prompting, LLMs struggled with interpreting kinematic graphs in physics problems.Similarly, [84] found that GPT-4 effectively solves algebra problems but struggles with geometric questions involving visual elements like graphs and diagrams, highlighting limitations in visual comprehension.These findings suggest that while prompting strategies can enhance LLM performance, they may not be sufficient to address fundamental limitations in the models' ability to handle certain educational content.4.7.6 Non-determinism and Reproducibility Issues.The inherent non-determinism of LLM outputs poses challenges for reproducibility.As highlighted by [54], LLM's responses can vary due to its random nature, making it difficult to replicate results consistently.This stochastic nature requires multiple runs to obtain reliable assessments, as single outputs may not be representative of the model's typical performance [70].Moreover, the lack of detailed documentation of prompts in some studies further hinders reproducibility, as other researchers cannot accurately replicate the prompting strategies used.4.7.7 Evaluation Challenges Specific to Prompting Strategies.The methods used to evaluate the effectiveness of LLMs with prompting strategies in education often have limitations.Many studies rely on subjective assessments or custom indicators without standardized benchmarks, which hinders comparability across studies.For example, [26] used qualitative analysis and subjective evaluation of ChatGPT-generated lesson plans, which may vary greatly due to diverse teaching standards.Similarly, [13,57] noted the need for publicly available benchmark datasets to compare and evaluate AI-generated outputs with different prompting strategies systematically.</p>
<p>DISCUSSION AND CONCLUSION</p>
<p>Summary of Key Findings</p>
<p>Our study investigated the application of prompt engineering strategies with Large Language Models (LLMs) in K-12 STEM education.Our analysis focused on three primary research questions: first, identifying the prompting strategies employed in various educational tasks across different educational levels within K-12 settings; second, exploring the types of LLMs utilized as the backend model for these prompting strategies; and third, evaluating the methods researchers have implemented to assess the effectiveness LLM-facilitated applications.Through this investigation, we aim to shed light on how prompt engineering can enhance educational outcomes and offer insights into the practical use of LLMs in real-world K-12 STEM education.</p>
<p>Firstly, we identified a variety of prompting strategies utilized in the literature, including simple prompting (or direct instruction prompting), zero-shot prompting, few-shot prompting, and chainof-thought prompting.Simple prompting and zero-shot prompting were the most commonly used strategies due to their simplicity and ease of implementation.However, more sophisticated strategies like few-shot prompting and chain-of-thought were found to enhance the performance of LLMs by providing additional context or examples, thereby improving their ability to handle complex educational tasks (e.g., Problem Solving [2, 3, 15, 18, 21, 23, 27, 38, 42-44, 52, 53, 62, 70, 82] and Analyzing and Labeling [28,42,53,76].For more, please refer to Table 2).Secondly, the majority of studies employed models from the GPT series, such as GPT-3, GPT-3.5, and GPT-4, which demonstrated strong performance across various educational tasks.However, other studies have also explored the use of different LLMs beyond the GPT series, including fine-tuned versions of LLaMA and open-source models like Blender, BLOOM, and YouChat.In certain cases, smaller, fine-tuned models outperformed larger pre-trained ones, suggesting that model selection should be tailored to the specific educational task and context.Lastly, researchers used a range of evaluation methods to assess the effectiveness of prompting strategies, including comparisons with human performance, the use of custom indicators and metrics, machine learning evaluation metrics (e.g., accuracy, F1 score), and statistical analyses of LLM outputs.However, only a limited number of studies conducted real-world experiments involving interactions with actual students or educators, revealing a gap in empirical validation and practical implementation that future research should aim to address.</p>
<p>Discussion</p>
<p>The findings of our study underscore the critical role of prompt engineering in enhancing the effectiveness of LLMs in K-12 STEM education.Advanced prompting strategies such as few-shot prompting and chain-of-thought prompting provide LLMs with additional guidance, enabling them to better understand complex tasks and generate more accurate and coherent responses.</p>
<p>The predominance of GPT-series models in the reviewed studies reflects their accessibility and robust performance across a range of tasks.However, the success of smaller, fine-tuned models in certain contexts suggests that customization and fine-tuning can be effective strategies, particularly when resources are limited or specific domain expertise is required.This highlights the importance of selecting LLMs to align with the educational objectives and constraints of specific settings.</p>
<p>The evaluation methods employed in the studies varied widely, with many relying on custom indicators or comparisons with existing datasets.The lack of standardized benchmarks and limited use of empirical validation present challenges for assessing the true effectiveness of LLMs in educational contexts.The few studies that involved direct interactions with students or educators provided valuable insights into the practical implications of using LLMs in classrooms but also pointed to the need for more rigorous and comprehensive evaluations.</p>
<p>Several limitations were identified in the current research.Many studies focused on narrow domains or specific datasets, limiting the generalizability of their findings.Methodological constraints, such as small sample sizes, lack of control groups, and absence of pre-tests, further hinder the robustness of conclusions.Additionally, inherent limitations of LLMs, such as variability in outputs and difficulty handling visual information, pose challenges for their integration into STEM education.The sensitivity of LLMs to prompt changes and the non-deterministic nature of their outputs necessitate careful design and testing of prompts to ensure consistency and reliability.</p>
<p>for Future Works</p>
<p>Empirical Validation in Real-world Settings: There is a pressing need for more empirical studies that implement LLMs with advanced prompting strategies in actual classroom environments.Such studies should involve different prompts, diverse educational contexts, subjects, and student populations to assess the best practice, generalizability, and practical impact of LLM with prompt engineering on learning outcomes.Development of Dataset and Evaluation Metrics for Prompt Engineering: Creating standardized datasets, benchmarks, and metrics for evaluating LLM performance with various prompts in educational tasks would ensure consistent, comparable results.This could involve building datasets across STEM subjects and educational levels, allowing researchers to rigorously assess different prompting strategies and models.Customization and Fine-tuning of LLMs with augmented data by LLM with prompt engineering: Exploring the customization and fine-tuning of LLMs for specific educational purposes could enhance their effectiveness and accessibility.Investigating the potential of smaller, opensource models that can be adapted to meet the needs of different educational environments is a promising avenue for making AI tools more widely available and relevant to educators and students.Addressing Methodological and Ethical Challenges with Prompt Engineering: Future research should aim to overcome the methodological limitations identified, such as enhancing study designs with larger sample sizes, control groups, and pre-and post-assessments.Additionally, ethical considerations, including issues related to bias, data privacy, and the evolving role of educators in the context of AI integration, should be thoroughly examined to ensure responsible and equitable use of LLMs in education.Enhancing Multimodal Capabilities of LLMs with Prompt Engineering: Given the importance of visual information in STEM education, improving the ability of LLMs to process and generate content that includes graphs, diagrams, and other visual elements is essential.Research into multimodal AI models that can handle both textual and visual data could significantly expand the applicability of LLMs in STEM subjects.Longitudinal Studies on Learning Outcomes with LLMs with different prompts: Conducting longitudinal studies to evaluate the sustained impact of LLMs with different prompts on student learning, engagement, and motivation would provide deeper insights into the long-term benefits and potential drawbacks of integrating AI technologies into educational practices.</p>
<p>By addressing these areas, future work can contribute to the use of LLMs and Prompt Engineering in the development of effective, ethical, and accessible AI-driven tools that enhance STEM education and support educators in fostering deeper understanding and engagement among students.</p>
<p>Fig. 1 .
1
Fig. 1.Systematic scoping review process following the PRISMA protocol.</p>
<p>Table 1 .
1
Overview of research studies across different subject areas and educational levels.
Educational LevelsMathPhysicsChemistry BiologyComputer SciencesOther*Elementary School (K -K5, 6 -10 years old)[16, 43, 53, 54, 57, 73, 76][16][16][16][2, 71, 73][2, 16, 42]Middle School (K6 -K8, 11 -14 years old)[16, 26, 43, 53, 54, 69, 84][13, 16, 18, 27][16, 26][16][2, 70][2, 13, 16, 36, 42]High School (K9 -K12, 14 -18 years old)[16, 43, 53, 54, 69, 82][3, 7, 16, 21, 27, 50][16][16, 50][2, 50][2, 16, 42]</p>
<p>Table 2 .
2
Types of prompts used in LLMs for different educational tasks in K-12 STEM education research.
EducationalSimpleRole-assignedZero ShotFew ShotCoTRAGOutputTasksPromptPromptFormattingProblem Solving[2, 15, 18, 21, 23, 38, 43, 53, 62, 82][2, 53]
https://platform.openai.com/docs/api-reference</p>
<p>50, 53] [53] [36, 50, 53] [36] [36Problem Creation. 1336, 53. 3, 13, 16, 18, 26, 54, 57, 69, 84. 3, 18, 54. 15, 16, 18, 84</p>
<p>. Chatbot, 2, 7, 16, 23, 43, 73, 82. 2, 7, 73] [2, 7, 23, 43, 73, 82. 16, 23. 7, 16, 43</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. </p>
<p>Supporting self-directed learning and self-assessment using TeacherGAIA, a generative AI chatbot application: Learning approaches and prompt engineering. Farhan Ali, Doris Choy, Shanti Divaharan, Hui Yong Tay, Wenli Chen, Learning: Research and Practice. 92023. 2023</p>
<p>Revolutionizing high school physics education: A novel dataset. Avinash Anand, Krishnasai Addala, Kabir Baghel, Arnav Goel, Medha Hira, Rushali Gupta, Rajiv Ratn Shah, ICBDA. Springer2023</p>
<p>Enhancing Software Code Vulnerability Detection Using GPT-4o and Claude-3.5 Sonnet: A Study on Prompt Engineering Techniques. Jaehyeon Bae, Seoryeong Kwon, Seunghwan Myeong, Electronics. 1326572024. 2024</p>
<p>Applications of Large Language Models in Education: Literature Review and Case Study. John Duchateau, Baierl , 2023Los AngelesUniversity of California</p>
<p>Artificial hallucination: GPT on LSD?. Gernot Beutel, Eline Geerits, Jan T Kielstein, Critical Care. 271482023. 2023</p>
<p>ChatGPT in physics education: A pilot study on easy-to-implement activities. Philipp Bitzenbauer, Contemporary Educational Technology. 154302023. 2023</p>
<p>Innovative approaches to high school physics competitions: Harnessing the power of AI and open science. Dominik Borovskỳ, Jozef Hanč, Martina Hančová, In JPCS. 2715120112024IOP Publishing</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020</p>
<p>The Case for STEM Education: Challenges and Opportunities. Rodger W Bybee, 2013NSTA Press</p>
<p>Use of AI-driven Code Generation Models in Teaching and Learning Programming: a Systematic Literature Review. Doga Cambaz, Xiaoling Zhang, Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. the 55th ACM Technical Symposium on Computer Science Education V20241</p>
<p>Navigating the Future of Learning: A Systematic Review of AI-Driven Intelligent Tutoring Systems (ITS) in K-12 Education. Patrick Charland, Angélique Létourneau, Marion Deslandes Martineau, Jared Boasen, Pierre-Majorique Léger, 2024. 2024</p>
<p>Improving automated evaluation of student text responses using gpt-3.5 for text data augmentation. Keith Cochran, Clayton Cohn, Jean Francois Rouet, Peter Hastings, AIED. Springer2023</p>
<p>Towards a formative feedback generation agent: Leveraging a human-in-the-loop, chain-of-thought prompting approach with LLMs to evaluate formative assessment responses in K-12 science. Clayton Cohn, Nicole Hutchins, Gautam Biswas, 2023. 2023</p>
<p>Examining science education in ChatGPT: An exploratory study of generative artificial intelligence. Grant Cooper, Journal of Science Education and Technology. 322023. 2023</p>
<p>Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tie, Yougen Zhou, Yilei Wang, arXiv:2308.02773Educhat: A large-scale language model-based chatbot system for intelligent education. 2023</p>
<p>Chat Overflow: Artificially Intelligent Models for Computing Education-renAIssance or apocAIypse. Paul Denny, Brett A Becker, Juho Leinonen, James Prather, Proceedings of the ITiCSE. the ITiCSE2023. 2023</p>
<p>Jingzhe Ding, Yan Cen, Xinyuan Wei, arXiv:2309.08182Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. 2023</p>
<p>Large Language Models in Education: A Systematic Review. Bingyu Dong, Jie Bai, Tao Xu, Yun Zhou, CSTE. IEEE. 2024. 2024</p>
<p>Beyond traditional assessment: Exploring the impact of large language models on grading practices. O Fagbohun, Np Iduwe, Abdullahi, Ifaturoti, Nwanna, Journal of Artifical Intelligence and Machine Learning &amp; Data Science. 22024. 2024</p>
<p>Performance of a Large Multimodal Model-based chatbot on the Test of Understanding Graphs in Kinematics. Polverini Giulia, Gregorcic Bor, arXiv:2311.069462023</p>
<p>ElectronixTutor: an intelligent tutoring system with multiple learning resources for electronics. Xiangen Arthur C Graesser, Benjamin D Hu, Kurt Nye, Rohit Vanlehn, Cristina Kumar, Neil Heffernan, Beverly Heffernan, Andrew M Woolf, Olney, Vasile Rus, International journal of STEM education. 52018. 2018</p>
<p>ChatGPT and the frustrated Socrates. Bor Gregorcic, Ann-Marie Pendrill, Physics Education. 58350212023. 2023</p>
<p>Is GPT-4 a reliable rater? Evaluating consistency in GPT-4's text ratings. Veronika Hackl, Alexandra Elena Müller, Michael Granitzer, Maximilian Sailer, Frontiers in Education. 202381272229</p>
<p>Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects. Qasem Muhammad Usman Hadi, Abbas Al Tashi, Rizwan Shah, Amgad Qureshi, Muhammad Muneer, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Wu, Authorea Preprints. 2024. 2024</p>
<p>AI to the rescue: Exploring the potential of ChatGPT as a teacher ally for workload relief and burnout prevention. Reem Hashem, Nagla Ali, Farah El Zein, Patricia Fidalgo, Othman Abu, Khurma , Research &amp; Practice in Technology Enhanced Learning. 192024. 2024</p>
<p>Assessing and Enhancing LLMs: A Physics and History Dataset and One-More-Check Pipeline Method. Chaofan He, Chunhui Li, Tianyuan Han, Liping Shen, ICONIP. Springer2023</p>
<p>Dollaya Hirunyasiri, Danielle R Thomas, Jionghao Lin, Kenneth R Koedinger, Vincent Aleven, arXiv:2307.02018Comparative analysis of gpt-4 and human graders in evaluating praise given to students in synthetic dialogues. 2023</p>
<p>Margaret Honey, Greg Pearson, Heidi Schweingruber, STEM Integration in K-12 Education: Status, Prospects, and an Agenda for Research. 2014</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Large Language Models: An Introduction. Uday Kamath, Kevin Keenan, Garrett Somers, Sarah Sorenson, Large Language Models: A Deep Dive: Bridging Theory and Practice. Springer2024</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. Kathrin Kasneci, Stefan Seßler, Maria Küchemann, Daryna Bannert, Frank Dementieva, Urs Fischer, Georg Gasser, Stephan Groh, Eyke Günnemann, Hüllermeier, Learning and individual differences. 1031022742023. 2023</p>
<p>Impact of intelligent tutoring systems on mathematics achievement of underachieving students. Rashmi Khazanchi, Daniele Di Mitri, Hendrik Drachsler, SITE. AACE. 2022</p>
<p>Prompt Engineering for Large Language Models. Nimrita Koul, 2023Nimrita Koul</p>
<p>Analyzing the Impact of a Structured LLM Workshop in Different Education Levels. Boyana Vasil Kozov, Kamelia Ivanova, Magdalena Shoylekova, Andreeva, Applied Sciences. 14142024. 2024</p>
<p>Applying large language models and chain-of-thought for automatic scoring. Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, Xiaoming Zhai, Computers and Education: Artificial Intelligence. 61002132024. 2024</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>Exploring the potential of using ChatGPT in physics education. Yicong Liang, Di Zou, Haoran Xie, Fu Lee, Wang , Smart Learning Environments. 10522023. 2023</p>
<p>Jionghao Lin, Ashish Gurung, Danielle R Thomas, Eason Chen, Conrad Borchers, Shivang Gupta, Kenneth R Koedinger, arXiv:2402.14594Improving assessment of tutoring practices using retrieval-augmented generation. 2024</p>
<p>Preliminary Systematic Review of Open-Source Large Language Models in Education. Daniel Michael Pin-Chuan Lin, Sarah Chang, Gaganpreet Hall, Jhajj, ITS. Springer2024</p>
<p>Gamifying programming education in K-12: A review of programming curricula in seven countries and programming games. Teemu H Renny Sn Lindberg, Lassi Laine, Haaranen, British journal of educational technology. 502019. 2019</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Mathdial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems. Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan, arXiv:2305.145362023</p>
<p>No more pencils no more books: Capabilities of generative AI on Irish and UK computer science school leaving examinations. Joyce Mahon, Brian Mac Namee, Brett A Becker, Proceedings of the UKICER '23. the UKICER '232023</p>
<p>Prompt engineering in large language models. Ggaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, Joyce Nakatumba-Nabende, ICDICI. Springer2023</p>
<p>Interrater reliability: the kappa statistic. Mary L Mchugh, Biochemia medica. 222012. 2012</p>
<p>Prompt engineering as an important emerging skill for medical professionals: tutorial. Bertalan Meskó, Journal of medical Internet research. 25e506382023. 2023</p>
<p>Using LLMs to bring evidence-based feedback into the classroom: AI-generated feedback increases secondary students' text revision, motivation, and positive emotions. Jennifer Meyer, Thorben Jansen, Ronja Schiller, Lucas W Liebenow, Marlene Steinbach, Andrea Horbach, Johanna Fleckenstein, Computers and Education: Artificial Intelligence. 61001992024. 2024</p>
<p>Promptaid: Prompt exploration, perturbation, testing and iteration using visual analytics for large language models. Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul Kwon, Chris Bryan, arXiv:2304.019642023</p>
<p>Using Large Language Models to Provide Formative Feedback in Intelligent Textbooks. Wesley Morris, Scott Crossley, Langdon Holmes, Chaohua Ou, Danielle Mcnamara, Mihai Dascalu, AIED. Springer2023</p>
<p>Golam Md, Muktadir , arXiv:2310.04438A Brief History of Prompt: Leveraging Language Models. 2023</p>
<p>Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. Duc-Vu Nguyen, Quoc-Nam Nguyen, SOICT '23. 2023</p>
<p>Evaluating chatgpt's decimal skills and feedback generation in a digital learning game. Hayden Huy A Nguyen, Xinying Stec, Sarah Hou, Bruce M Di, Mclaren, ECTEL. Springer2023</p>
<p>Rewriting Math Word Problems with Large Language Models. Kole Norberg, Husni Almoubayyed, Stephen E Fancsali, Logan De Ley, Kyle Weldon, April Murphy, Steve Ritter, 2023. 2023Grantee Submission</p>
<p>The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. Joanne E Matthew J Page, Patrick M Mckenzie, Isabelle Bossuyt, Tammy C Boutron, Cynthia D Hoffmann, Larissa Mulrow, Jennifer M Shamseer, Elie A Tetzlaff, Sue E Akl, Brennan, bmj. 3722021. 2021</p>
<p>Improving mathematics assessment readability: Do large language models help. Nirmal Patel, Pooja Nagpal, Tirth Shah, Aditya Sharma, Shrey Malvi, Derek Lomas, Journal of Computer Assisted Learning. 392023. 2023</p>
<p>The impact of large language models on education: exploring the connection between AI and Education 4.0. Iris Cristina Peláez-Sánchez, Davis Velarde-Camaqui, Leonardo David Glasserman-Morales, Frontiers in Education. 2024. 13920919</p>
<p>Rediscovering the use of chatbots in education: A systematic literature review. José Quiroga Pérez, Thanasis Daradoumis, Joan Manuel, Marquès Puig, Computer Applications in Engineering Education. 282020. 2020</p>
<p>Lessons learned from intelligent tutoring research for simulation. Anna Ray S Perez, Paul Skinner, Chatelier, Using games and simulations for teaching and assessment. Routledge2016</p>
<p>Anna Ray S Perez, Robert A Skinner, Sottilare, A REVIEW OF INTELLIGENT TUTORING SYSTEMS FOR SCIENCE TECHNOLOGY ENGINEERING AND MATHEMATICS (STEM). Assessment of intelligent tutoring systems technologies and opportunities. 2018. 20181</p>
<p>Generating high-precision feedback for programming syntax errors using large language models. Tung Phung, José Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares, arXiv:2302.046622023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 192019. 2019</p>
<p>STEM and Technology Education: International State-of-the-art. John M Ritz, Mark J Fan, International Journal of Technology and Design Education. 252015. 2015</p>
<p>Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.07927A systematic survey of prompt engineering in large language models: Techniques and applications. 2024</p>
<p>arXiv:2308.01530DeepWrite Project: Evaluating GPT Consistency in Education. 2023</p>
<p>Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, Hyojung Han, Sevien Schulhoff, arXiv:2406.06608The Prompt Report: A Systematic Survey of Prompting Techniques. 2024</p>
<p>The percentage of consonants correct (PCC) metric: Extensions and reliability data. Diane Lawrence D Shriberg, Barbara A Austin, Jane L Lewis, David L Mcsweeny, Wilson, Journal of Speech, Language, and Hearing Research. 401997. 1997</p>
<p>Automatic generation of socratic subquestions for teaching math word problems. Kumar Shridhar, Jakub Macina, Mennatallah El-Assady, Tanmay Sinha, Manu Kapur, Mrinmaya Sachan, arXiv:2211.128352022</p>
<p>Evaluating ChatGPT and GPT-4 for Visual Programming. Adish Singla, Proceedings of the 2023 ACM ICER. the 2023 ACM ICER2023</p>
<p>Using ChatGPT standard prompt engineering techniques in lesson preparation: role, instructions and seed-word prompts. J Aleksandar, Spasić, Dragan, Janković, 2023 ICEST. IEEE2023</p>
<p>Enhancing llm-based feedback: Insights from intelligent tutoring systems and the learning sciences. John Stamper, Ruiwei Xiao, Xinying Hou, AIED. Springer2024</p>
<p>Anaïs Tack, Chris Piech, arXiv:2205.07540The AI teacher test: Measuring the pedagogical ability of blender and GPT-3 in educational dialogues. 2022</p>
<p>Jie Tian, Jixin Hou, Zihao Wu, Peng Shu, Zhengliang Liu, Yujie Xiang, Beikang Gu, Nicholas Filla, Yiwei Li, Ning Liu, arXiv:2401.12983Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding. 2024</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023</p>
<p>Who's the Best Detective? Large Language Models vs. Traditional Machine Learning in Detecting Incoherent Fourth Grade Math Answers. Felipe Urrutia, Roberto Araya, Journal of Educational Computing Research. 612024</p>
<p>Prompt Engineering: a methodology for optimizing interactions with AI-Language Models in the field of engineering. Juan David, Velásquez- Henao, Carlos Jaime Franco-Cardona, Lorena Cadavid-Higuita, Dyna. 902023. 2023</p>
<p>Large language models for education: A survey and outlook. Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S Yu, Qingsong Wen, arXiv:2403.181052024</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, arXiv:2310.00746Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 352022. 2022</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, arXiv:2302.11382A prompt pattern catalog to enhance prompt engineering with chatgpt. 2023</p>
<p>Promoting self-regulation progress and knowledge construction in blended learning via ChatGPT-based learning aid. Hsin-Yu Wu, Pin-Hui Lee, Chia-Nan Li, Yueh-Min Huang, Huang, Journal of Educational Computing Research. 612024. 2024</p>
<p>Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, Caiming Xiong, arXiv:2402.18667FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability. 2024</p>
<p>Gautam Yadav, Ying-Jui Tseng, Xiaolin Ni, arXiv:2306.00190Contextualizing problems to student interests at scale in intelligent tutoring system using large language models. 2023</p>
<p>Practical and ethical challenges of large language models in education: A systematic scoping review. Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, Dragan Gašević, British Journal of Educational Technology. 552024. 2024</p>
<p>Why Johnny can't prompt: how non-AI experts try (and fail) to design LLM prompts. Richmond Y Jd Zamfirescu-Pereira, Bjoern Wong, Qian Hartmann, Yang, Proceedings of the 2023 CHI Conference. the 2023 CHI Conference2023</p>
<p>Systematic review of research on artificial intelligence applications in higher education-where are the educators?. Olaf Zawacki-Richter, Victoria I Marín, Melissa Bond, Franziska Gouverneur, International Journal of Educational Technology in Higher Education. 162019. 2019</p>
<p>Is" a helpful assistant" the best role for large language models? a systematic evaluation of social roles in system prompts. Mingqian Zheng, Jiaxin Pei, David Jurgens, arXiv:2311.100542023</p>            </div>
        </div>

    </div>
</body>
</html>
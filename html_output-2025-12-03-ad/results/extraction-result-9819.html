<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9819 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9819</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9819</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-ab2948a2feae953abbed67021ece999e8e29f488</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ab2948a2feae953abbed67021ece999e8e29f488" target="_blank">Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Cureus</p>
                <p><strong>Paper Abstract:</strong> This editorial explores the innovative application of large language Models (LLMs) in conducting systematic reviews, specifically focusing on quality assessment and risk-of-bias evaluation. As integral components of systematic reviews, these tasks traditionally require extensive human effort, subjectivity, and time. Integrating LLMs can revolutionize this process, providing an objective, consistent, and rapid methodology for quality assessment and risk-of-bias evaluation. With their ability to comprehend context, predict semantic relationships, and extract relevant information, LLMs can effectively appraise study quality and risk of bias. However, careful consideration must be given to potential risks and limitations associated with over-reliance on machine learning models and inherent biases in training data. An optimal balance and combination between human expertise and automated LLM evaluation might offer the most effective approach to advance and streamline the field of evidence synthesis.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9819.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9819.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for QA/ROB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for quality assessment and risk-of-bias evaluation in systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An editorial-proposed use of large language models to automate quality assessment (QA) and risk-of-bias (ROB) appraisal in systematic reviews by extracting study features, identifying bias-related phrases/patterns, and assigning scores or risk labels to included studies to improve consistency and speed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generic LLMs (examples cited: GPT-4, LLaMA, LaMDA) and related multimodal models (DALL-E, SAM, Vision Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as advanced AI models trained on extensive text (and in some cases image) data capable of understanding context, predicting semantic relationships, and extracting relevant information; GPT-4 specifically noted as a multimodal foundation model incorporated into ChatGPT that can process text and images.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; general statement that LLMs are trained on extensive text corpora, and that in application they would analyze the corpus of studies included in a systematic review (i.e., the set of included primary papers) to extract QA/ROB-relevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Automated assessment of methodological quality and risk-of-bias across the included studies of a systematic review (identifying selection, performance, detection, attrition, reporting biases and assigning QA scores or ROB categories).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>High-level conceptual description only: LLMs would be trained or prompted to identify common sources of bias by recognizing phrases, language patterns, or missing information in study reports and then assign a risk level for each bias domain; no specific prompting strategies, retrieval-augmentation, chain-of-thought, or pipeline details are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated QA scores (numerical counts of safeguards) and categorical ROB assessments (e.g., low/high risk labels), or structured appraisals per standard tools (e.g., JBI, Cochrane ROB tools) as narrative/structured outputs to streamline evidence synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No empirical evaluation described in this paper; the authors discuss the need for combining machine outputs with human expertise and acknowledge potential validation via human review but do not report concrete evaluation protocols or metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Reported potential advantages include increased efficiency, improved consistency and objectivity across assessors, the ability to capture nuanced signals in text, and faster evidence synthesis useful for time-sensitive contexts (e.g., guideline development, pandemics).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors note risks including overreliance on models weakening researchers' critical thinking, and inherent biases in the training data of LLMs that may influence assessments; no technical mitigations or robustness analyses are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific empirical failure cases are reported; the paper warns in general about biased assessments stemming from training-data bias and the possibility of missed nuances without human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9819.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9819.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an example of a state-of-the-art LLM/multimodal foundation model (incorporated in ChatGPT) that could be harnessed to perform QA and ROB appraisals for systematic reviews by processing text and image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as the latest GPT model used in ChatGPT; characterized as a multimodal foundation model that generates human-like text and can process text and image inputs. No architectural or training dataset details beyond 'trained on extensive text data' are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified here; the paper generically states LLMs (including GPT-4) are trained on extensive text data and could be applied to the body of studies included in systematic reviews to extract relevant QA/ROB signals.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Suggested application: automate quality assessment and risk-of-bias appraisal across studies included in systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified; the paper broadly suggests LLM capability to recognize phrases/patterns and assign bias levels but gives no concrete prompting, fine-tuning, or pipeline details for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Proposed outputs are automated QA assessments (numerical safeguards counts) and ROB categorizations (rankings of studies by bias risk), delivered as structured or narrative appraisals.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No formal evaluation of GPT-4 for this task is reported in the paper; the authors advocate human oversight and combination with expert review but give no empirical evaluation method.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Implied strengths from the paper: strong contextual understanding, multimodal input processing, ability to speed up appraisal and reduce inter-assessor variability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Noted concerns include potential bias from training data, risk of overreliance reducing human critical appraisal skills, and absence of reported validation for the proposed application.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No concrete failure cases with GPT-4 are provided in the article; only general concerns about biased outputs and the need for human verification are discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models in medical education: opportunities, challenges, and future directions. <em>(Rating: 2)</em></li>
                <li>Embracing large language models for medical applications: opportunities and challenges. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9819",
    "paper_id": "paper-ab2948a2feae953abbed67021ece999e8e29f488",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LLMs for QA/ROB",
            "name_full": "Large language models for quality assessment and risk-of-bias evaluation in systematic reviews",
            "brief_description": "An editorial-proposed use of large language models to automate quality assessment (QA) and risk-of-bias (ROB) appraisal in systematic reviews by extracting study features, identifying bias-related phrases/patterns, and assigning scores or risk labels to included studies to improve consistency and speed.",
            "citation_title": "Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation",
            "mention_or_use": "mention",
            "model_name": "Generic LLMs (examples cited: GPT-4, LLaMA, LaMDA) and related multimodal models (DALL-E, SAM, Vision Transformer)",
            "model_description": "Described in the paper as advanced AI models trained on extensive text (and in some cases image) data capable of understanding context, predicting semantic relationships, and extracting relevant information; GPT-4 specifically noted as a multimodal foundation model incorporated into ChatGPT that can process text and images.",
            "model_size": null,
            "input_corpus_description": "Not specified in this paper; general statement that LLMs are trained on extensive text corpora, and that in application they would analyze the corpus of studies included in a systematic review (i.e., the set of included primary papers) to extract QA/ROB-relevant information.",
            "input_corpus_size": null,
            "topic_query_description": "Automated assessment of methodological quality and risk-of-bias across the included studies of a systematic review (identifying selection, performance, detection, attrition, reporting biases and assigning QA scores or ROB categories).",
            "distillation_method": "High-level conceptual description only: LLMs would be trained or prompted to identify common sources of bias by recognizing phrases, language patterns, or missing information in study reports and then assign a risk level for each bias domain; no specific prompting strategies, retrieval-augmentation, chain-of-thought, or pipeline details are provided.",
            "output_type": "Automated QA scores (numerical counts of safeguards) and categorical ROB assessments (e.g., low/high risk labels), or structured appraisals per standard tools (e.g., JBI, Cochrane ROB tools) as narrative/structured outputs to streamline evidence synthesis.",
            "output_example": null,
            "evaluation_method": "No empirical evaluation described in this paper; the authors discuss the need for combining machine outputs with human expertise and acknowledge potential validation via human review but do not report concrete evaluation protocols or metrics.",
            "evaluation_results": null,
            "strengths": "Reported potential advantages include increased efficiency, improved consistency and objectivity across assessors, the ability to capture nuanced signals in text, and faster evidence synthesis useful for time-sensitive contexts (e.g., guideline development, pandemics).",
            "limitations": "Authors note risks including overreliance on models weakening researchers' critical thinking, and inherent biases in the training data of LLMs that may influence assessments; no technical mitigations or robustness analyses are provided.",
            "failure_cases": "No specific empirical failure cases are reported; the paper warns in general about biased assessments stemming from training-data bias and the possibility of missed nuances without human oversight.",
            "uuid": "e9819.0",
            "source_info": {
                "paper_title": "Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "Mentioned as an example of a state-of-the-art LLM/multimodal foundation model (incorporated in ChatGPT) that could be harnessed to perform QA and ROB appraisals for systematic reviews by processing text and image inputs.",
            "citation_title": "Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Described in the paper as the latest GPT model used in ChatGPT; characterized as a multimodal foundation model that generates human-like text and can process text and image inputs. No architectural or training dataset details beyond 'trained on extensive text data' are provided.",
            "model_size": null,
            "input_corpus_description": "Not specified here; the paper generically states LLMs (including GPT-4) are trained on extensive text data and could be applied to the body of studies included in systematic reviews to extract relevant QA/ROB signals.",
            "input_corpus_size": null,
            "topic_query_description": "Suggested application: automate quality assessment and risk-of-bias appraisal across studies included in systematic reviews.",
            "distillation_method": "Not specified; the paper broadly suggests LLM capability to recognize phrases/patterns and assign bias levels but gives no concrete prompting, fine-tuning, or pipeline details for GPT-4.",
            "output_type": "Proposed outputs are automated QA assessments (numerical safeguards counts) and ROB categorizations (rankings of studies by bias risk), delivered as structured or narrative appraisals.",
            "output_example": null,
            "evaluation_method": "No formal evaluation of GPT-4 for this task is reported in the paper; the authors advocate human oversight and combination with expert review but give no empirical evaluation method.",
            "evaluation_results": null,
            "strengths": "Implied strengths from the paper: strong contextual understanding, multimodal input processing, ability to speed up appraisal and reduce inter-assessor variability.",
            "limitations": "Noted concerns include potential bias from training data, risk of overreliance reducing human critical appraisal skills, and absence of reported validation for the proposed application.",
            "failure_cases": "No concrete failure cases with GPT-4 are provided in the article; only general concerns about biased outputs and the need for human verification are discussed.",
            "uuid": "e9819.1",
            "source_info": {
                "paper_title": "Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models in medical education: opportunities, challenges, and future directions.",
            "rating": 2
        },
        {
            "paper_title": "Embracing large language models for medical applications: opportunities and challenges.",
            "rating": 2
        }
    ],
    "cost": 0.00730575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Review began 06/01/2023
Review ended 06/04/2023
Published 08/06/2023
(c) Copyright 2023</p>
<p>Nashwan et al. This is an open access article distributed under the terms of the Creative Commons Attribution License CCBY 4.0, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<h2>Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation</h2>
<p>Abdulqadir J. Nashwan ${ }^{1}$, Jaber H. Jaradat ${ }^{2}$<br>1. Nursing Department, Hamad Medical Corporation, Doha, QAT 2. School of Medicine, Mutah University, Al Karak, JOR</p>
<p>Corresponding author: Abdulqadir J. Nashwan, anashwan@hamad.qa</p>
<h4>Abstract</h4>
<p>This editorial explores the innovative application of large language Models (LLMs) in conducting systematic reviews, specifically focusing on quality assessment and risk-of-bias evaluation. As integral components of systematic reviews, these tasks traditionally require extensive human effort, subjectivity, and time. Integrating LLMs can revolutionize this process, providing an objective, consistent, and rapid methodology for quality assessment and risk-of-bias evaluation. With their ability to comprehend context, predict semantic relationships, and extract relevant information, LLMs can effectively appraise study quality and risk of bias. However, careful consideration must be given to potential risks and limitations associated with over-reliance on machine learning models and inherent biases in training data. An optimal balance and combination between human expertise and automated LLM evaluation might offer the most effective approach to advance and streamline the field of evidence synthesis.</p>
<p>Categories: Healthcare Technology, Epidemiology/Public Health
Keywords: evidence synthesis, machine learning, artificial intelligence, large language models, risk of bias, quality assessment, systematic reviews</p>
<h2>Editorial</h2>
<p>Large language models (LLMs) present a groundbreaking opportunity to revolutionize the process of conducting systematic reviews, particularly in quality assessment (QA) and risk-of-bias (ROB) appraisal. This lies in the potential of LLMs to automate the inherently labor-intensive and often subjective process of these tasks that may lead to inconsistencies among different assessors.</p>
<p>Systematic reviews conduct rigorous and comprehensive assessments of existing literature on a particular medical topic [1]. They are an essential part of evidence-based medicine and involve several stages, of which the QA of included studies or the appraisal of ROB is a critical stage to ensure the credibility of the review. These reviews critically analyzed the available evidence, identified potential biases or shortcomings in individual studies, and synthesized the findings [1].</p>
<p>LLMs are advanced artificial intelligence (AI) models, such as Generative Pre-trained Transformer 4 (GPT-4), DALL-E, Segment Anything Model (SAM), Large Language Model Meta AI (LLaMA), Language Model for Dialogue Applications (LaMDA), Vision Transformer, etc., capable of automating various tasks in natural language processing, computer vision, etc. GPT-4 is the latest GPT model incorporated in ChatGPT, which is a multimodal foundation model that excels in generating human-like text and can process text and image inputs and beyond [2]. LLMs are trained on extensive text data, enabling them to understand context, predict semantic relationships, and extract relevant information from diverse datasets [2]. Consequently, LLMs can be effectively harnessed to automatically assess the quality of the included studies in a systematic review, streamlining the process and reducing the subjectivity associated with human independent assessors.</p>
<p>Although QA and ROB assessments are considered similar safeguard elements and are sometimes used interchangeably by researchers, their applications and interpretations differ. QA evaluates methodological safeguards within a study to ensure it is well-designed, conducted, analyzed, interpreted, and reported, thereby minimizing systematic errors or bias. Conversely, the ROB assessment, also known as critical appraisal, aims to understand how these safeguards may influence the study results, involving judgments about the level of bias and delving deeper into the implications of methodological safeguards on the study's outcomes. QA entails counting the safeguards present as numerical assessment, whereas a ROB assessment employs an approach to rank studies based on potential bias into low or high-risk studies [3,4]. There are numerous tools to assess QA and ROB; they differ according to the study design, such as the Joanna Briggs Institute (JBI), Assessing the Methodological Quality of Systematic Reviews (AMSTAR), Critical Appraisal Skills Program (CASP), Cochrane Risk of Bias (ROB 1 and 2) tool, Risk Of Bias In Non-randomized Studies of Interventions (ROBINS-I) tool, the Newcastle-Ottawa Scale (NOS), etc. [4].</p>
<p>LLMs can be trained to identify common sources of bias, such as selection, performance, detection, attrition, and reporting, by recognizing specific phrases, language patterns, or missing information; the LLM can assign a risk level of bias to each study. This objective, automated assessment can minimize human assessors' subjectivity, enhance the systematic review's reliability and capture nuances that may otherwise have been overlooked [5]. The use of LLMs not only improves the efficiency and consistency of QA and ROB evaluations but also expedites the overall systematic review process. By eliminating the need for manual appraisal, LLMs enable faster evidence synthesis and knowledge dissemination, crucial in developing clinical guidelines and public health emergencies (e.g., pandemics), where timely access to highly-quality synthesized evidence is crucial. However, using LLMs holds many potential risks, limitations, and challenges. Overreliance on LLMs may weaken critical thinking skills in researchers. Risks associated with inherent biases in the training data of LLMs might influence the assessment outcomes [2,5]. Therefore, combining human expertise and automated LLM evaluation might offer the best approach, ensuring both the efficiency of automation and the nuanced understanding of human assessors.</p>
<p>In conclusion, harnessing LLMs in conducting quality assessments and the risk of bias in systematic reviews can be transformative. With proper caution to ensure appropriate use and mitigate potential risks, efficiency, consistency, and objectivity benefits can significantly advance the evidence synthesis field.</p>
<h2>Additional Information</h2>
<p>Disclosures</p>
<p>Conflicts of interest: In compliance with the ICMJE uniform disclosure form, all authors declare the following: Payment/services info: All authors have declared that no financial support was received from any organization for the submitted work. Financial relationships: All authors have declared that they have no financial relationships at present or within the previous three years with any organizations that might have an interest in the submitted work. Other relationships: All authors have declared that there are no other relationships or activities that could appear to have influenced the submitted work.</p>
<h2>References</h2>
<ol>
<li>Burns PB, Rohrich RJ, Chung KC: The levels of evidence and their role in evidence-based medicine . Plast Reconstr Surg. 2011, 128:305-10. 10.1097/PRS.0b013e318219c171</li>
<li>Abd-Alrazaq A, Alliaad R, Alhuwall D, et al.: Large language models in medical education: opportunities, challenges, and future directions. JMIR Med Educ. 2023, 9:e48291. 10.2196/48291</li>
<li>Furuya-Kanamori L, Xu C, Hasan SS, Doi SA: Quality versus risk-of-bias assessment in clinical research . J Clin Epidemiol. 2021, 129:172-5. 10.1016/j.jclinepi.2020.09.044</li>
<li>Barker TH, Stone JC, Sears K, et al.: Revising the JBI quantitative critical appraisal tools to improve their applicability: an overview of methods and the development process. JBI Evid Synth. 2023, 21:478-93. 10.11124/JBIES-22-00125</li>
<li>Karabacak M, Margetis K: Embracing large language models for medical applications: opportunities and challenges. Cureus. 2023, 15:e39305. 10.7759/cureus.39305</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4718 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4718</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4718</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-37588705a2af7d5b24d901dd33ade1ff293aabdd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/37588705a2af7d5b24d901dd33ade1ff293aabdd" target="_blank">Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work investigates the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to learn numeracy, to struggle considerably in the extrapolation setting across all four tasks.</p>
                <p><strong>Paper Abstract:</strong> The transformer-based pre-trained language models have been tremendously successful in most of the conventional NLP tasks. But they often struggle in those tasks where numerical understanding is required. Some possible reasons can be the tokenizers and pre-training objectives which are not specifically designed to learn and preserve numeracy. Here we investigate the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to learn numeracy. We consider four numeracy tasks: numeration, magnitude order prediction, finding minimum and maximum in a series, and sorting. We find that, although T5 models perform reasonably well in the interpolation setting, they struggle considerably in the extrapolation setting across all four tasks.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4718.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4718.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-to-Text Transfer Transformer (T5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer encoder-decoder family (T5-small / T5-base / T5-large) evaluated on four numeracy tasks (numeration, magnitude-order prediction, list-min/max, sorting); shows strong interpolation performance but large interpolation→extrapolation gaps, and sensitivity to number representation and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (small / base / large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based text-to-text model (encoder-decoder). In this paper: T5-small (~60M parameters), T5-base (~220M), T5-large (~770M). Pretrained on natural-language corpora and fine-tuned on synthetic numeracy tasks described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeration (word→integer), Magnitude-order prediction (masked number → magnitude class), List-Min/Max (find min and max in a list), Sorting (ascending/descending reorder). Multi-digit integer tasks; no multi-step symbolic arithmetic like multiplication/division evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>T5 appears to perform numeracy primarily via learned pattern memorization and contextual association rather than explicit algorithmic arithmetic; tokenization / representation (digit-splitting) and model capacity influence its ability to generate correct digit sequences; larger parameter counts allow better coverage of numeric patterns seen in training.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large empirical interpolation vs extrapolation gaps indicate models memorize patterns and number-range statistics from training data. Ablation-style comparisons of representations (split-digit vs full integer) show split-digit improves generation (suggesting token-level generation of digits is easier). Larger models (T5-large) achieve much higher interpolation EM and F1 than smaller variants, indicating capacity helps memorization/generalization over seen ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite capacity and split-digit improvements, models still fail systematically in extrapolation and zero-shot settings (very low exact-match on unseen number ranges), which contradicts a hypothesis of learned algorithmic numeral reasoning; manual error analysis shows structured errors (e.g., ignoring magnitude words or dropping zeros) rather than random noise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Numeration (exact-match): e.g., on 4.9K train data, full-int (FL) T5-small IN 45.31% / EX 0.08%; T5-base IN 92.16% / EX 1.03%; T5-large IN 98.06% / EX 1.91%. Split-digit (SP) representation improves: T5-small IN 69.67% / EX 39.35%; T5-base IN 99.50% / EX 11.31%; T5-large IN 100.00% / EX 10.05%. Magnitude-order prediction: T5-large μF1 = 81.40 (Article Titles), μF1 = 80.29 (Market Comments); cross-domain (extrapolation) T5-large μF1 ≈ 50.18 when trained on AT and tested on MC (or vice versa). List-Min/Max: T5-large often >95% interpolation EM across small ranges and short lists; extrapolation varies widely (List-Min up to ~81%, List-Max up to ~84.9% in some settings). Sorting: T5-large interpolation often 90–100% for small ranges / short lists, but extrapolation drops to single/double digits for larger ranges and longer lists (examples: ascending sort, range<9999 and 10 elements EX ≈ 12.6%). Zero-shot: T5-large produced only 553 and 8783 exact-matches out of 100k (two zero-shot tests reported).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Interventions included changing numeric token representation (split-digit vs full-integer) and expanding training numeric-range. Split-digit representation produced consistent improvements (notably for smaller models) in numeration EM and extrapolation. Cross-domain (train on one dataset, test on another) used as extrapolation probe: T5 variants beat BiGRU baseline by up to ~25% in some cross-domain magnitude-order tests. NT5 variant (see another entry) was tested as fine-tuned variant to probe effects of numerical pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Consistent failure to extrapolate to numbers outside training ranges; partial decoding of numbers with multiple zeros (omitting zeros, producing shorter numbers), ignoring magnitude words ('hundred'), predicting nearby feasible magnitudes instead of exact values, predicting second-min/max instead of true min/max, missing or repeating elements in sorting, off-by-digit errors, and severe degradation for small parameter models (T5-small often fails for long lists or larger ranges). Zero-shot arithmetic performance is very poor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to sequence models and baselines on magnitude-order tasks (CNN, GRU, BiGRU), T5-large outperforms prior baselines (e.g., BiGRU) on in-domain magnitude-order metrics. Cross-domain/extrapolation tests showed T5 variants beat BiGRU by up to ~25%. However, specialized arithmetic architectures (NALU/NAU/NMU), numeracy-preserving embeddings (DICE, numBERT), and NT5 are discussed/benchmarked — specialized methods sometimes yield improvements on specific numeracy tasks in prior work or in variants (see NT5 entry).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4718.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4718.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NT5 (numeracy-trained T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5 variant trained with objectives or data intended to improve numerical reasoning; evaluated here as a comparative model versus T5-small on the same numeracy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nt5?! training t5 to perform numerical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A numeracy-focused variant of T5 (per Yang et al., 2021). In this paper NT5 is evaluated in the same experimental suite and compared primarily to T5-small; exact parameterization in this evaluation is consistent with the NT5 paper's published variant.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same four tasks: numeration, magnitude-order prediction, list-min/max, sorting.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Specialized pretraining/fine-tuning on numeracy-related objectives increases contextual exposure to numeric patterns, enabling better generalization for some numeracy tasks (especially numeration, and cross-domain magnitude-order prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>When tested here, NT5 with split-digit representation achieved 73.07% numeration accuracy (a ~4% absolute improvement over T5-small in the split setting). Cross-domain (extrapolation) μF1 for magnitude-order prediction increased by ~5–7% compared to T5-small, suggesting better generalization to unseen contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>NT5 did not improve performance for List-MinMax and Sorting tasks; on 3-element sorting NT5 performance dropped by 10–20% relative to T5-small. In-domain magnitude-order sometimes decreased (by 3–6%) despite cross-domain gains, indicating gains are task/domain-dependent and not uniformly beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Numeration (split): NT5 accuracy = 73.07% (≈ +4% vs T5-small). Magnitude-order cross-domain μF1: improved by ≈5–7%; in-domain μF1 decreased by ≈3–6%. List-MinMax & Sorting: no improvement or performance drop (3-element sorting dropped 10–20%). Exact numeric breakdowns for every cell not provided in text beyond these summary deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>NT5 was tested as an intervention (specialized numeracy training). Results indicate that targeted numeracy-pretraining can improve certain tasks (numeration, cross-domain magnitude-order) but may harm or not affect others (sorting, minmax), showing that pretraining objectives produce task-specific effects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Improvements are not consistent across tasks; NT5's numeration gains did not translate to better list-based comparison or sorting, and may reduce in-domain magnitude-order performance. NT5 still suffers from many same failure modes as T5 (extrapolation difficulty, digit-omission errors).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared directly to T5-small in this paper; NT5 outperforms T5-small on numeration split representation and cross-domain magnitude-order, but underperforms or matches on list/sort tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4718.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4718.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Split-digit representation (SP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Split-digit number representation (tokenize numbers into digit tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A numeric representation strategy where integers are output/generated as sequences of individual digit tokens rather than a single full-integer token; used as an experimental condition to test whether producing digits is easier for generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Data-formatting/representation choice: numbers either represented as full integer tokens (FL) or split into digits/subtokens (SP) before model generation/consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Affects all generation-style numeracy tasks in the paper (numeration, magnitude-order generation, list-min/max outputs, sorting outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Generating numbers as digit sequences reduces the difficulty of producing exact integers because the model composes digits rather than memorizing full integer tokens; this mitigates tokenizer-related loss of numeric semantics and allows the generative decoder to learn digit-level patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical gains in numeration EM and extrapolation when using SP vs FL: e.g., with 4.9K training data numeration, T5-small FL IN 45.31%→SP IN 69.67% and FL EX 0.08%→SP EX 39.35%; T5-base and T5-large also show improvements in many settings. Authors explicitly report that all T5 versions benefit when trained with split representation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Even with split-digit representation, models (especially larger ones) still fail to extrapolate reliably to unseen numerical ranges (extrapolation performance remains low in many settings); SP does not solve list comparison/sorting failures completely.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative improvement: Numeration on 4.9K train: T5-small FL IN 45.31% / EX 0.08% → SP IN 69.67% / EX 39.35%; T5-base FL IN 92.16% / EX 1.03% → SP IN 99.50% / EX 11.31%; T5-large FL IN 98.06% / EX 1.91% → SP IN 100.00% / EX 10.05%. These illustrate substantial SP gains, especially for smaller models and extrapolation in T5-small.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Direct ablation: the paper compares FL vs SP representations across models and training sizes, demonstrating SP as an effective intervention for numeration and some extrapolation cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>SP improves digit-generation but does not endow models with algorithmic numeric rules; many structured failures persist (zero-omission, ignoring magnitude words, ordering mistakes). SP helps more for some models/tasks than others.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating the limitations of the transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Nt5?! training t5 to perform numerical reasoning <em>(Rating: 2)</em></li>
                <li>Neural arithmetic logic units <em>(Rating: 2)</em></li>
                <li>Methods for numeracy-preserving word embeddings <em>(Rating: 2)</em></li>
                <li>Learning numeral embeddings <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Representing numbers in nlp: a survey and a vision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4718",
    "paper_id": "paper-37588705a2af7d5b24d901dd33ade1ff293aabdd",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "T5",
            "name_full": "Text-to-Text Transfer Transformer (T5 family)",
            "brief_description": "Transformer encoder-decoder family (T5-small / T5-base / T5-large) evaluated on four numeracy tasks (numeration, magnitude-order prediction, list-min/max, sorting); shows strong interpolation performance but large interpolation→extrapolation gaps, and sensitivity to number representation and model capacity.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5 (small / base / large)",
            "model_description": "Transformer-based text-to-text model (encoder-decoder). In this paper: T5-small (~60M parameters), T5-base (~220M), T5-large (~770M). Pretrained on natural-language corpora and fine-tuned on synthetic numeracy tasks described in the paper.",
            "arithmetic_task_type": "Numeration (word→integer), Magnitude-order prediction (masked number → magnitude class), List-Min/Max (find min and max in a list), Sorting (ascending/descending reorder). Multi-digit integer tasks; no multi-step symbolic arithmetic like multiplication/division evaluated here.",
            "mechanism_hypothesis": "T5 appears to perform numeracy primarily via learned pattern memorization and contextual association rather than explicit algorithmic arithmetic; tokenization / representation (digit-splitting) and model capacity influence its ability to generate correct digit sequences; larger parameter counts allow better coverage of numeric patterns seen in training.",
            "evidence_for_mechanism": "Large empirical interpolation vs extrapolation gaps indicate models memorize patterns and number-range statistics from training data. Ablation-style comparisons of representations (split-digit vs full integer) show split-digit improves generation (suggesting token-level generation of digits is easier). Larger models (T5-large) achieve much higher interpolation EM and F1 than smaller variants, indicating capacity helps memorization/generalization over seen ranges.",
            "evidence_against_mechanism": "Despite capacity and split-digit improvements, models still fail systematically in extrapolation and zero-shot settings (very low exact-match on unseen number ranges), which contradicts a hypothesis of learned algorithmic numeral reasoning; manual error analysis shows structured errors (e.g., ignoring magnitude words or dropping zeros) rather than random noise.",
            "performance_metrics": "Numeration (exact-match): e.g., on 4.9K train data, full-int (FL) T5-small IN 45.31% / EX 0.08%; T5-base IN 92.16% / EX 1.03%; T5-large IN 98.06% / EX 1.91%. Split-digit (SP) representation improves: T5-small IN 69.67% / EX 39.35%; T5-base IN 99.50% / EX 11.31%; T5-large IN 100.00% / EX 10.05%. Magnitude-order prediction: T5-large μF1 = 81.40 (Article Titles), μF1 = 80.29 (Market Comments); cross-domain (extrapolation) T5-large μF1 ≈ 50.18 when trained on AT and tested on MC (or vice versa). List-Min/Max: T5-large often &gt;95% interpolation EM across small ranges and short lists; extrapolation varies widely (List-Min up to ~81%, List-Max up to ~84.9% in some settings). Sorting: T5-large interpolation often 90–100% for small ranges / short lists, but extrapolation drops to single/double digits for larger ranges and longer lists (examples: ascending sort, range&lt;9999 and 10 elements EX ≈ 12.6%). Zero-shot: T5-large produced only 553 and 8783 exact-matches out of 100k (two zero-shot tests reported).",
            "probing_or_intervention_results": "Interventions included changing numeric token representation (split-digit vs full-integer) and expanding training numeric-range. Split-digit representation produced consistent improvements (notably for smaller models) in numeration EM and extrapolation. Cross-domain (train on one dataset, test on another) used as extrapolation probe: T5 variants beat BiGRU baseline by up to ~25% in some cross-domain magnitude-order tests. NT5 variant (see another entry) was tested as fine-tuned variant to probe effects of numerical pretraining.",
            "limitations_and_failure_modes": "Consistent failure to extrapolate to numbers outside training ranges; partial decoding of numbers with multiple zeros (omitting zeros, producing shorter numbers), ignoring magnitude words ('hundred'), predicting nearby feasible magnitudes instead of exact values, predicting second-min/max instead of true min/max, missing or repeating elements in sorting, off-by-digit errors, and severe degradation for small parameter models (T5-small often fails for long lists or larger ranges). Zero-shot arithmetic performance is very poor.",
            "comparison_to_other_models": "Compared to sequence models and baselines on magnitude-order tasks (CNN, GRU, BiGRU), T5-large outperforms prior baselines (e.g., BiGRU) on in-domain magnitude-order metrics. Cross-domain/extrapolation tests showed T5 variants beat BiGRU by up to ~25%. However, specialized arithmetic architectures (NALU/NAU/NMU), numeracy-preserving embeddings (DICE, numBERT), and NT5 are discussed/benchmarked — specialized methods sometimes yield improvements on specific numeracy tasks in prior work or in variants (see NT5 entry).",
            "uuid": "e4718.0",
            "source_info": {
                "paper_title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "NT5",
            "name_full": "NT5 (numeracy-trained T5)",
            "brief_description": "A T5 variant trained with objectives or data intended to improve numerical reasoning; evaluated here as a comparative model versus T5-small on the same numeracy tasks.",
            "citation_title": "Nt5?! training t5 to perform numerical reasoning",
            "mention_or_use": "use",
            "model_name": "NT5",
            "model_description": "A numeracy-focused variant of T5 (per Yang et al., 2021). In this paper NT5 is evaluated in the same experimental suite and compared primarily to T5-small; exact parameterization in this evaluation is consistent with the NT5 paper's published variant.",
            "arithmetic_task_type": "Same four tasks: numeration, magnitude-order prediction, list-min/max, sorting.",
            "mechanism_hypothesis": "Specialized pretraining/fine-tuning on numeracy-related objectives increases contextual exposure to numeric patterns, enabling better generalization for some numeracy tasks (especially numeration, and cross-domain magnitude-order prediction).",
            "evidence_for_mechanism": "When tested here, NT5 with split-digit representation achieved 73.07% numeration accuracy (a ~4% absolute improvement over T5-small in the split setting). Cross-domain (extrapolation) μF1 for magnitude-order prediction increased by ~5–7% compared to T5-small, suggesting better generalization to unseen contexts.",
            "evidence_against_mechanism": "NT5 did not improve performance for List-MinMax and Sorting tasks; on 3-element sorting NT5 performance dropped by 10–20% relative to T5-small. In-domain magnitude-order sometimes decreased (by 3–6%) despite cross-domain gains, indicating gains are task/domain-dependent and not uniformly beneficial.",
            "performance_metrics": "Numeration (split): NT5 accuracy = 73.07% (≈ +4% vs T5-small). Magnitude-order cross-domain μF1: improved by ≈5–7%; in-domain μF1 decreased by ≈3–6%. List-MinMax & Sorting: no improvement or performance drop (3-element sorting dropped 10–20%). Exact numeric breakdowns for every cell not provided in text beyond these summary deltas.",
            "probing_or_intervention_results": "NT5 was tested as an intervention (specialized numeracy training). Results indicate that targeted numeracy-pretraining can improve certain tasks (numeration, cross-domain magnitude-order) but may harm or not affect others (sorting, minmax), showing that pretraining objectives produce task-specific effects.",
            "limitations_and_failure_modes": "Improvements are not consistent across tasks; NT5's numeration gains did not translate to better list-based comparison or sorting, and may reduce in-domain magnitude-order performance. NT5 still suffers from many same failure modes as T5 (extrapolation difficulty, digit-omission errors).",
            "comparison_to_other_models": "Compared directly to T5-small in this paper; NT5 outperforms T5-small on numeration split representation and cross-domain magnitude-order, but underperforms or matches on list/sort tasks.",
            "uuid": "e4718.1",
            "source_info": {
                "paper_title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Split-digit representation (SP)",
            "name_full": "Split-digit number representation (tokenize numbers into digit tokens)",
            "brief_description": "A numeric representation strategy where integers are output/generated as sequences of individual digit tokens rather than a single full-integer token; used as an experimental condition to test whether producing digits is easier for generative models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Data-formatting/representation choice: numbers either represented as full integer tokens (FL) or split into digits/subtokens (SP) before model generation/consumption.",
            "arithmetic_task_type": "Affects all generation-style numeracy tasks in the paper (numeration, magnitude-order generation, list-min/max outputs, sorting outputs).",
            "mechanism_hypothesis": "Generating numbers as digit sequences reduces the difficulty of producing exact integers because the model composes digits rather than memorizing full integer tokens; this mitigates tokenizer-related loss of numeric semantics and allows the generative decoder to learn digit-level patterns.",
            "evidence_for_mechanism": "Empirical gains in numeration EM and extrapolation when using SP vs FL: e.g., with 4.9K training data numeration, T5-small FL IN 45.31%→SP IN 69.67% and FL EX 0.08%→SP EX 39.35%; T5-base and T5-large also show improvements in many settings. Authors explicitly report that all T5 versions benefit when trained with split representation.",
            "evidence_against_mechanism": "Even with split-digit representation, models (especially larger ones) still fail to extrapolate reliably to unseen numerical ranges (extrapolation performance remains low in many settings); SP does not solve list comparison/sorting failures completely.",
            "performance_metrics": "Representative improvement: Numeration on 4.9K train: T5-small FL IN 45.31% / EX 0.08% → SP IN 69.67% / EX 39.35%; T5-base FL IN 92.16% / EX 1.03% → SP IN 99.50% / EX 11.31%; T5-large FL IN 98.06% / EX 1.91% → SP IN 100.00% / EX 10.05%. These illustrate substantial SP gains, especially for smaller models and extrapolation in T5-small.",
            "probing_or_intervention_results": "Direct ablation: the paper compares FL vs SP representations across models and training sizes, demonstrating SP as an effective intervention for numeration and some extrapolation cases.",
            "limitations_and_failure_modes": "SP improves digit-generation but does not endow models with algorithmic numeric rules; many structured failures persist (zero-omission, ignoring magnitude words, ordering mistakes). SP helps more for some models/tasks than others.",
            "uuid": "e4718.2",
            "source_info": {
                "paper_title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating the limitations of the transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2
        },
        {
            "paper_title": "Nt5?! training t5 to perform numerical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Neural arithmetic logic units",
            "rating": 2
        },
        {
            "paper_title": "Methods for numeracy-preserving word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Learning numeral embeddings",
            "rating": 2
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Representing numbers in nlp: a survey and a vision",
            "rating": 1
        }
    ],
    "cost": 0.014301249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model</h1>
<p>Kuntal Kumar Pal, Chitta Baral<br>Department of Computer Science<br>Arizona State University, Tempe, Arizona, USA,<br>kkpal@asu.edu, chitta@asu.edu</p>
<h4>Abstract</h4>
<p>The transformer-based pre-trained language models have been tremendously successful in most of the conventional NLP tasks. But they often struggle in those tasks where numerical understanding is required. Some possible reasons can be the tokenizers and pre-training objectives which are not specifically designed to learn and preserve numeracy. Here we investigate the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to learn numeracy. We consider four numeracy tasks : numeration, magnitude order prediction, finding minimum and maximum in a series, and sorting. We find that, although T5 models perform reasonably well in the interpolation setting, they struggle considerably in the extrapolation setting across all four tasks.</p>
<h2>1 Introduction</h2>
<p>Recent advances in transfer learning in NLP have led to the emergence of pre-trained models which show a much stronger contextual representation of words than earlier static word embeddings. They have all performed extremely well in conventional NLP tasks. Yet, they fail to capture a better understanding of numbers. Numbers are integral part of natural language texts which can change the meaning of a sentence. So there is a need for NLP models which can identify numbers represented in any surface forms like words, floats or strings ( Numeration), understand its values in various context (Magnitude Order Prediction), compare their values with others (List-MinMax) or able to rearrange a series of numbers based on its values (Sorting).</p>
<p>The transfer-learned models are pre-trained on huge amount of natural language texts with specially designed tasks and tokenizers to create stronger word-embeddings. This causes the numbers embedded in the texts to lose their meaning and inherent rules of numeracy guiding them
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of Numeracy Tests
(Thawani et al., 2021; Nogueira et al., 2021). This is possibly the reason they perform worse in numerical reasoning tasks on numbers absent in training data (Nogueira et al., 2021; Wallace et al., 2019).</p>
<p>In this paper, we test this numeracy learning ability of a text-to-text transfer learning generative model, T5 (Raffel et al., 2020) which has outperformed its predecessors in conventional NLP tasks. The text-to-text format of input and output helps the model to generalize all the NLP tasks as a unified model. We use four numeracy tests both in interpolation (training and testing on same range of data) and extrapolation settings (training on lower and testing on higher range of data) and study how much numeracy skill it can acquire. Figure 1 shows some examples of each of the numeracy tests.</p>
<p>Our contributions in this paper are: (1) Extensive study on three versions of T5 models (small, base, large) on four numeracy tests in interpolation and extrapolation settings. (2) Reporting interesting observations in the behavior of each model version across multiple experimental settings through detailed manual error analysis. The synthetically generated data and codes are publicly available ${ }^{1}$ for future numeracy analysis in similar settings.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>2 Numeracy Tests</h2>
<p>We perform four essential numeracy tests to explore model's ability to understand numerical values.
Motivation: These four elementary tasks are simple and easy for the models, since they do not need to generate a completely new number in a different numerical range (like in mathematical tests : multiplication, division, exponentiation). Here we evaluate whether the models learn the numeracy tasks or they simply learn bias from the number range seen in training data.</p>
<h3>2.1 Numeration</h3>
<p>The probability of a number represented in multiple surface forms (word, scientific, float, integer) increases with the increase in the volume of pre-training corpus of the language models. It is impractical for an end-to-end NLP model to semantically parse these numbers accurately and convert into a single representation to retain its value or reason with. This task tests the model's ability to understand word representation of a number and to decode into integer form.</p>
<h3>2.2 Magnitude Order Prediction</h3>
<p>The task is to identify the order of magnitude of a missing (masked) number which fits the context of a natural language text. This task is important in numerical commonsense reasoning (Lin et al., 2020) and prompt-based methods (Liu et al., 2021). Here, we do not expect the model to predict the exact number that fits the context as this may vary in different domains. Instead, this task tests the model's ability to understand a missing number's context and predict its appropriate range.</p>
<h3>2.3 List-MinMax</h3>
<p>We test the model's ability to understand numerical values and compare among them. Given a series of $n$ positive numbers, the task is to find the minimum and the maximum number. This is the basis of many question answering and commonsense numerical reasoning dataset like SQuAD (Rajpurkar et al., 2016), DROP (Dua et al., 2019) and NUMBERGAME (Mishra et al., 2020). We simplify the task by generating templates so that the models can concentrate on understanding the task rather than getting confused by the language complexities.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># TRAIN $\rightarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">4.9K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">1.3K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">0.9K</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TP</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
</tr>
<tr>
<td style="text-align: center;">FL</td>
<td style="text-align: center;">T5-SM</td>
<td style="text-align: center;">45.31</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-BS</td>
<td style="text-align: center;">92.16</td>
<td style="text-align: center;">1.03</td>
<td style="text-align: center;">66.47</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">37.20</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-LG</td>
<td style="text-align: center;">98.06</td>
<td style="text-align: center;">1.91</td>
<td style="text-align: center;">89.49</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">79.48</td>
<td style="text-align: center;">1.58</td>
</tr>
<tr>
<td style="text-align: center;">SP</td>
<td style="text-align: center;">T5-SM</td>
<td style="text-align: center;">69.67</td>
<td style="text-align: center;">39.35</td>
<td style="text-align: center;">26.89</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-BS</td>
<td style="text-align: center;">99.50</td>
<td style="text-align: center;">11.31</td>
<td style="text-align: center;">81.21</td>
<td style="text-align: center;">22.44</td>
<td style="text-align: center;">73.61</td>
<td style="text-align: center;">31.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-LG</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">10.05</td>
<td style="text-align: center;">99.97</td>
<td style="text-align: center;">7.35</td>
<td style="text-align: center;">91.59</td>
<td style="text-align: center;">12.92</td>
</tr>
</tbody>
</table>
<p>Table 1: Numeration EM scores w/ split (SP) and w/o split (FL) representation on $4.9 \mathrm{~K}, 1.3 \mathrm{~K}, 0.9 \mathrm{~K}$ train-data in Interpolation (IN) and Extrapolation (EX) settings.</p>
<h3>2.4 Sorting</h3>
<p>In addition to understanding the values of each number in a series, the model will have to rearrange them in the correct order through this task, making it even harder than List-MinMax. Even if a model is successful in the previous test, it is necessary to identify whether it has actually compared among all the numbers in the series. Hence, sorting a list of $n$ numbers in ascending and descending orders ensures that the model compares all the numbers and rearrange them into two different sequences.</p>
<h2>3 Experiments</h2>
<h3>3.1 Experimental Setup:</h3>
<p>We use T5-SM (small, 60M parameters), T5-BS (base, 220M), T5-LG (large, 770M) and positive integers for the experiments. The results are average of three random seeds. We perform experiments in two settings: interpolation (training and testing on same numerical range) and extrapolation (training on lower and testing on higher numerical range). The latter helps us to analyze whether a model has learnt the task, or it has exploited bias in the numerical range of the training data.</p>
<h3>3.2 Data Preparation:</h3>
<p>Numeration: We create a dataset keeping in mind that at least few examples of all unique words needed to represent each number, are present in the training data (Trask et al., 2018). In Table 1, interpolation samples are from $[0,10 \mathrm{~K})$ and 99 K extrapolation samples are from $[10 \mathrm{~K}, 1000 \mathrm{~K})$. We use num2words ${ }^{2}$ for generating word-form of each integer. To simulate fewer shot setting, we carefully craft two smaller training sets taking only $20 \%$ and $10 \%$ data. We show two number representation schemes with split-digits (SP) and without</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">LIST MINIMUM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LIST MAXIMUM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># ELEMENTS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Range</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-SM</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;99$</td>
<td style="text-align: center;">T5-BS</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-LG</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-SM</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">5.2</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;999$</td>
<td style="text-align: center;">T5-BS</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">25.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-LG</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-SM</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;9999$</td>
<td style="text-align: center;">T5-BS</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-LG</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">59.1</td>
</tr>
</tbody>
</table>
<p>Table 2: List-MinMax (series length: 3, 5, 10) in three different number ranges evaluated as Interpolation (IN) and Extrapolation (EX) exact-match scores on 1 K test data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Datasets $\rightarrow$</th>
<th style="text-align: center;">AT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Models $\downarrow$</td>
<td style="text-align: center;">$\mu \mathbf{F 1}$</td>
<td style="text-align: center;">$m \mathbf{F 1}$</td>
<td style="text-align: center;">$\mu \mathbf{F 1}$</td>
<td style="text-align: center;">$m \mathbf{F 1}$</td>
</tr>
<tr>
<td style="text-align: left;">LR</td>
<td style="text-align: center;">62.49</td>
<td style="text-align: center;">30.81</td>
<td style="text-align: center;">71.25</td>
<td style="text-align: center;">60.80</td>
</tr>
<tr>
<td style="text-align: left;">CNN</td>
<td style="text-align: center;">69.27</td>
<td style="text-align: center;">35.96</td>
<td style="text-align: center;">77.17</td>
<td style="text-align: center;">58.49</td>
</tr>
<tr>
<td style="text-align: left;">GRU</td>
<td style="text-align: center;">70.92</td>
<td style="text-align: center;">38.43</td>
<td style="text-align: center;">78.25</td>
<td style="text-align: center;">58.08</td>
</tr>
<tr>
<td style="text-align: left;">BiGRU</td>
<td style="text-align: center;">71.49</td>
<td style="text-align: center;">39.94</td>
<td style="text-align: center;">$\underline{80.16}$</td>
<td style="text-align: center;">62.74</td>
</tr>
<tr>
<td style="text-align: left;">CRNN</td>
<td style="text-align: center;">69.50</td>
<td style="text-align: center;">36.15</td>
<td style="text-align: center;">78.00</td>
<td style="text-align: center;">$\underline{64.62}$</td>
</tr>
<tr>
<td style="text-align: left;">CNN-capsule</td>
<td style="text-align: center;">63.11</td>
<td style="text-align: center;">29.41</td>
<td style="text-align: center;">75.89</td>
<td style="text-align: center;">59.22</td>
</tr>
<tr>
<td style="text-align: left;">GRU-capsule</td>
<td style="text-align: center;">70.73</td>
<td style="text-align: center;">33.57</td>
<td style="text-align: center;">77.36</td>
<td style="text-align: center;">$\mathbf{6 4 . 7 1}$</td>
</tr>
<tr>
<td style="text-align: left;">BiGRU-capsule</td>
<td style="text-align: center;">71.49</td>
<td style="text-align: center;">34.18</td>
<td style="text-align: center;">77.97</td>
<td style="text-align: center;">64.34</td>
</tr>
<tr>
<td style="text-align: left;">BiLSTM-DICE</td>
<td style="text-align: center;">75.56</td>
<td style="text-align: center;">$\mathbf{4 6 . 8 0}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">T5-SM</td>
<td style="text-align: center;">69.87</td>
<td style="text-align: center;">31.36</td>
<td style="text-align: center;">66.11</td>
<td style="text-align: center;">34.68</td>
</tr>
<tr>
<td style="text-align: left;">T5-BS</td>
<td style="text-align: center;">$\underline{78.06}$</td>
<td style="text-align: center;">40.04</td>
<td style="text-align: center;">72.22</td>
<td style="text-align: center;">47.44</td>
</tr>
<tr>
<td style="text-align: left;">T5-LG</td>
<td style="text-align: center;">$\mathbf{8 1 . 4 0}$</td>
<td style="text-align: center;">$\underline{44.64}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 2 9}$</td>
<td style="text-align: center;">59.16</td>
</tr>
</tbody>
</table>
<p>Table 3: Magnitude Order Prediction for Market Comments (MC) and Article Titles (AT) datasets of numeracy600K in micro-F1 ( $\mu \mathrm{F} 1$ ) and macro-F1 ( $m \mathrm{~F} 1$ ). Best score is in bold and second-best is underlined.
split (FL) hypothesizing that for a generative model it would be easier to correctly generate individual digits instead of full integer at once.
Magnitude Order Prediction: For this task we work on Numeracy600K (Chen et al., 2019) dataset. We consider this as a mask prediction task. We train models to find the exact number that fits the mask. Then, we map the predicted numbers into its magnitude order, save the model based on best magnitude order and calculate the evaluation metrics on test data. Since this is a generation task we reject those answers which are not valid floating point numbers. The baseline results in Table 3 are from (Chen et al., 2019; Sundararaman et al., 2020). We also consider extrapolation setting by showing the cross-domain performance (train on market comments and test on article title and vice-versa) in Table 4.
List Min-Max \&amp; Sort: We experiment on three different number ranges: $[0,100),[0,1 \mathrm{~K}),[0,10 \mathrm{~K})$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Train on $\rightarrow$</th>
<th style="text-align: center;">AT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Models $\downarrow$</td>
<td style="text-align: center;">$\mu \mathbf{F 1}$</td>
<td style="text-align: center;">$m \mathbf{F 1}$</td>
<td style="text-align: center;">$\mu \mathbf{F 1}$</td>
<td style="text-align: center;">$m \mathbf{F 1}$</td>
</tr>
<tr>
<td style="text-align: left;">BiGRU</td>
<td style="text-align: center;">25.59</td>
<td style="text-align: center;">10.58</td>
<td style="text-align: center;">31.38</td>
<td style="text-align: center;">11.08</td>
</tr>
<tr>
<td style="text-align: left;">T5-SM</td>
<td style="text-align: center;">28.88</td>
<td style="text-align: center;">12.04</td>
<td style="text-align: center;">37.35</td>
<td style="text-align: center;">10.81</td>
</tr>
<tr>
<td style="text-align: left;">T5-BS</td>
<td style="text-align: center;">35.53</td>
<td style="text-align: center;">14.48</td>
<td style="text-align: center;">31.51</td>
<td style="text-align: center;">12.25</td>
</tr>
<tr>
<td style="text-align: left;">T5-LG</td>
<td style="text-align: center;">$\mathbf{5 0 . 1 8}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 4 3}$</td>
<td style="text-align: center;">$\mathbf{1 2 . 3 2}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Cross Domain (Extrapolation) Tests of Order Prediction. Train on MC, test on AT and vice-versa.</p>
<p>For interpolation tests, the numbers in the test data are from the same ranges. The extrapolation numbers are from the maximum of respective ranges to 100 K . To prevent the model's bias on number lengths, we bring them closer following prior work (Wallace et al., 2019). We extend the experiment on a series of 3,5 and 10 numbers (for each range) to study how each of the models behave with increasing series length. We consider the same data for sorting experiments as well. The results are in Table 2 for List-MinMax and Table 5 for List-Sort.</p>
<h2>4 Results and Error Analysis</h2>
<p>Table 1 shows, all versions of T5 benefit when they are trained with split representation. When trained with 4.9 K data, T5-SM gains $24 \%$ points in interpolation evaluation where T5-LG gains only $2 \%$. None of the models perform well on unseen number data ranges. In fewer shot interpolation settings however, only the T5-LG model maintains its performance beyond $90 \%$ which is not surprising because of its large parameter-space. We noticed that the best model could only partially decode numbers having multiple zeros (Figure 2). In the first example, the model predicts an extra seven and in the second (extrapolation), it ignored the key word 'hundred' as it attempts to fit this unseen</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LIST-SORT ASCENDING</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LIST-SORT DESCENDING</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># ELEMENTS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Range</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
<td style="text-align: center;">IN</td>
<td style="text-align: center;">EX</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-SM</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;99$</td>
<td style="text-align: center;">T5-BS</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-LG</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-SM</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;999$</td>
<td style="text-align: center;">T5-BS</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-LG</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">1.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-SM</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;9999$</td>
<td style="text-align: center;">T5-BS</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-LG</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">11.9</td>
</tr>
</tbody>
</table>
<p>Table 5: List-Sort (Ascending \&amp; Descending) on series lengths: 3, 5, 10 in three different integer ranges evaluated as Interpolation (IN) and Extrapolation (EX) exact-match scores on 1K test data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">What is one hundred seventy ?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Label : 170 Predicted : 170</td>
</tr>
<tr>
<td style="text-align: left;">What is five hundred thousand three ?</td>
</tr>
<tr>
<td style="text-align: left;">Label : 600000 Predicted : 6000</td>
</tr>
<tr>
<td style="text-align: left;">The Dominican Republic beats Puerto Rico 3- -AMGKv for the WBC 2013 championship</td>
</tr>
<tr>
<td style="text-align: left;">Label : 0 (0) Predicted : 1 (1)</td>
</tr>
<tr>
<td style="text-align: left;">Indian Light 140 (LUPRI300v says Sept-Qtr Net Sales -AMGKv Bin Rupees</td>
</tr>
<tr>
<td style="text-align: left;">Label : 22.38 (2) Predicted : 1.09 (1)</td>
</tr>
<tr>
<td style="text-align: left;">Which is minimum in value among 805786704763750801795799799793 ?</td>
</tr>
<tr>
<td style="text-align: left;">Label : 750 Predicted : 750</td>
</tr>
<tr>
<td style="text-align: left;">Which is maximum in value among 8804868486588760869187558679870187578649 ?</td>
</tr>
<tr>
<td style="text-align: left;">Label : 8760 Predicted: 8760</td>
</tr>
<tr>
<td style="text-align: left;">Sort in descending order : 805786704763750801795799799793</td>
</tr>
<tr>
<td style="text-align: left;">Label : 805801799795786763750792704795</td>
</tr>
<tr>
<td style="text-align: left;">Predicted : 805801799795786763750792702704</td>
</tr>
<tr>
<td style="text-align: left;">Sort in descending order : 92473528235274668801693895492996584813165734592317</td>
</tr>
<tr>
<td style="text-align: left;">Label : 90584924739231781316693896880157345549295292352746</td>
</tr>
<tr>
<td style="text-align: left;">Predicted : 9058490584813166880157345549295292392317</td>
</tr>
</tbody>
</table>
<p>Figure 2: Two incorrect predictions for each task.
data into a similar seen number range (4 digits).
In magnitude order prediction (Table 3), T5LG's performance improves by $5 \mu \mathrm{~F} 1$ in article title. For extrapolation (Table 4), all T5 versions beats previous estimates (BiGRU) by at most $25 \%$. This shows that T5 can learn robust numeric representations based on contexts. Both the samples in Fig 2 are hard as they need prior explicit knowledge. Yet they are able to predict numbers in similar feasible ranges. This shows that the model is not randomly assigning magnitude but has learnt based on the domain and context. We found that, the best T5 model predicted an order of 1 instead of 2 for market and article data making a maximum error of $39.07 \%$ and $33.59 \%$ respectively.</p>
<p>Table 2 shows List-MinMax results. Both T5-BS and T5-LG perform over $80 \%$ across all ranges and series lengths. T5-SM however, degrades in performance as the range increases along with the list size. As the model learns more variations in numbers, the extrapolation performance increases to a max of $81 \%$ (List-Min) and $84.9 \%$ (List-Max). But the performance drops as series length increases. The best model predicted second minimum and maximum element in the examples of Fig 2.</p>
<p>From the sorting results (Table 5), we see T5SM performance drops ( $18-22 \%$ from 2-3 digits, $8-9 \%$ from 3-4) as number ranges increase across series length of 3 . T5-SM fails to generate a single correct order for a series of 10 elements and achieves less than $10 \%$ success in 5-element series across all ranges. This degrading performance can be attributed to its mere 60 M parameter space. As the number of parameters keep increasing the models performs consistently across each of 3,5 , 10 elements in series, both for interpolation and extrapolation settings. With the increasing range of training data, the models become more robust to extrapolated numbers across all series lengths with $8-30 \%$ change in ascending order and $7-20 \%$ change in descending order. Finally, for sorting, we find a variety of incorrect predictions: missing order of one element, omission of one and two elements or repeating a particular element.</p>
<p>Overall, none of the models were able to perform well on extrapolation samples showing the inherent rules of numeracy is difficult for these models to learn. But, it also shows, more variations in numbers (increasing the range) help them perform better in extrapolation setting. The smaller model's limited parameter-space affects its performance in all four tasks whereas larger models are able to pick up some numeracy skills through training. We show more predictions in Figure 3, 4, 5, 6.</p>
<p>Analysis of NT5: We test with the NT5 (Yang et al., 2021) model on all our experiments and compared the results with T5-small. For the Numeration task with the split number representation NT5 performed 73.07 (accuracy), a 4\% improvement over T5. The performance however did not improve for the MinMax and Sorting tasks. For 3-element sorting it dropped by 10-20\%. In the Magnitude</p>
<table>
<thead>
<tr>
<th style="text-align: left;">What is one thousand nine hundred ninety ?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Label : 1990 Predicted : 1919</td>
</tr>
<tr>
<td style="text-align: left;">What is eight hundred fifty five thousand five hundred fifty seven ?</td>
</tr>
<tr>
<td style="text-align: left;">Label : 855557 Predicted : 8557</td>
</tr>
<tr>
<td style="text-align: left;">What is four thousand ninety nine ?</td>
</tr>
<tr>
<td style="text-align: left;">Label : 4099 Predicted : 4099</td>
</tr>
<tr>
<td style="text-align: left;">What is fifteen thousand nine hundred three ?</td>
</tr>
<tr>
<td style="text-align: left;">Label : 15903 Predicted : 15903</td>
</tr>
</tbody>
</table>
<p>Figure 3: Some predictions for Numeration task.</p>
<p>2012 Chick-Fil-A Bowl preview: No. 8 LSU vs. No. <MASK> Clemson Label : 14 (2) Predicted : 6 (1)
$&lt;$ MASK $&gt;$ days to go before wind tax credit expires.
Label : 5 (1) Predicted : 63 (2)
Nonprofit Homefront America Receives \$ &lt; MASK &gt; from Walmart Foundation Label : 10000 (5) Predicted : 100000 (6)
NYSE indication BRKa.N last 130150.0 bid 128000.0 ask &lt; MASK &gt; Label : 131000 (6) Predicted : 138000 (8)</p>
<p>Figure 4: Magnitude Order Prediction Examples.</p>
<p>Order Prediction, we find the cross-domain (extrapolation) $\mu \mathrm{F} 1$ score increases by $5-7 \%$ while in-domain decreases by $3-6 \%$. This might be because NT5 has seen more variety of contexts of numbers and can generalize well on this task.</p>
<h2>5 Related Works</h2>
<p>Numeracy Tests: Multiple numeracy tests have been proposed to evaluate the static word embeddings (Naik et al., 2019) like GloVe, Word2Vec, FastText and contextual embeddings (Wallace et al., 2019) like BERT through probing tasks like numeration, magnitude comparison, addition, listmaximum. Multilingual numeration (Johnson et al., 2020) tests have been performed by probing models like DistilBERT, XLM, and BERT. CNN, BiGRU models have been shown to perform well in magnitude order prediction (Chen et al., 2019) and T5 on addition and subtraction tasks (Nogueira et al., 2021) through training on similar texts. We, however focus on studying how much text-to-text transfer models (T5) can learn across four fundamental numeracy tasks in samples containing both in-domain and out-of-domain numerical ranges.
Specially Designed Models: NALU (Trask et al.,
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: Some predictions for List-MinMax task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sort in descending order : 4873488048274871487748464865484048794836</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Label : 4880487948774873487148654846484048364827</td>
</tr>
<tr>
<td style="text-align: center;">Predicted : 4880487948734871486548464840483648274877</td>
</tr>
<tr>
<td style="text-align: center;">Sort in descending order : 632642652634651638621649633630</td>
</tr>
<tr>
<td style="text-align: center;">Label : 652651649642638634633632630621</td>
</tr>
<tr>
<td style="text-align: center;">Predicted : 652651649642638634633632621630</td>
</tr>
<tr>
<td style="text-align: center;">Sort in descending order : 594598632600633630560574634599</td>
</tr>
<tr>
<td style="text-align: center;">Label : 634633632630600599598594574560</td>
</tr>
<tr>
<td style="text-align: center;">Predicted : 634633632630599598594574560600</td>
</tr>
</tbody>
</table>
<p>Figure 6: Some predictions for List-Sort task.</p>
<p>2018), NAU and NMU (Madsen and Johansen, 2020), numBERT (Zhang et al., 2020), GenBERT (Geva et al., 2020), NT5 (Yang et al., 2021) have emerged in the last few years to incorporate arithmetic skills into models through specially designed architecture or fine-tuning tasks which improves the performance in synthetic arithmetic or crowdsourced numerical reasoning tasks like DROP.
Numerical Embeddings: There are limited prior works in numeracy aware embeddings which show good performance in extrapolation setting. One approach (Jiang et al., 2019) represents numerals as a weighted average of prototype numeral embeddings obtained using either self organizing map or Gaussian Mixture models. DICE (Sundararaman et al., 2020) is a deterministic numeral embedding approach, independent of corpus, which preserves the relative magnitude between two numerals and their embeddings.</p>
<h2>6 Conclusion \&amp; Future Works</h2>
<p>We show that text-to-text models are able to learn numeracy quite well in an interpolation setting. Our extensive experiments show that T5 models struggle to learn with numbers outside training data ranges. We believe that, to make further progress in transfer learning, models need to achieve such elementary numeracy skills and this gap between interpolation and extrapolation performance needs to be reduced. We are of the opinion that, adding more data would not bridge this gap since domain of numbers is open. However, special pre-training objectives for digits rather than whole numbers can be designed to teach the inherent numeracy to models. In future, we intend to explore these objectives centered around preserving numeracy rules in transfer-learned models to generalize between in-domain and out-of-domain numbers.</p>
<h2>Acknowledgement</h2>
<p>The authors acknowledge support from DARPA grant number FA875019C0003 for this project.</p>
<h2>Ethical Considerations</h2>
<p>In this paper, we analyze performance of three publicly available T5 models on four numeracy tasks. For Magnitude Order Prediction task we use publicly available dataset, Numeracy600K. We synthetically create the data for rest of the tasks.</p>
<h2>References</h2>
<p>Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, and Hsin-Hsi Chen. 2019. Numeracy-600k: learning numeracy for detecting exaggerated information in market comments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6307-6313.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958, Online. Association for Computational Linguistics.</p>
<p>Chengyue Jiang, Zhonglin Nian, Kaihao Guo, Shanbo Chu, Yinggong Zhao, Libin Shen, and Kewei Tu. 2019. Learning numeral embeddings. arXiv preprint arXiv:2001.00003.</p>
<p>Devin Johnson, Denise Mak, Andrew Barker, and Lexi Loessberg-Zahl. 2020. Probing for multilingual numerical understanding in transformer-based language models. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 184-192, Online. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020. Birds have four legs?! numersense: Probing numerical commonsense knowledge of pretrained language models. In Proceedings of EMNLP. To appear.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing.</p>
<p>Andreas Madsen and Alexander Rosenberg Johansen. 2020. Neural arithmetic units. arXiv preprint arXiv:2001.05016.</p>
<p>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, and Chitta Baral. 2020. Towards question format independent numerical reasoning: A set of prerequisite tasks. arXiv preprint arXiv:2005.08516.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, and Eduard Hovy. 2019. Exploring numeracy in word embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374-3380.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Li. 2021. Investigating the limitations of the transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Dhanasekar Sundararaman, Shijing Si, Vivek Subramanian, Guoyin Wang, Devamanyu Hazarika, and Lawrence Carin. 2020. Methods for numeracypreserving word embeddings. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4742-4753.</p>
<p>Avijit Thawani, Jay Pujara, Pedro A Szekely, and Filip Ilievski. 2021. Representing numbers in nlp: a survey and a vision. arXiv preprint arXiv:2103.13136.</p>
<p>Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. 2018. Neural arithmetic logic units. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 53075315, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Peng-Jian Yang, Ying Ting Chen, Yuechan Chen, and Daniel Cer. 2021. Nt5?! training t5 to perform numerical reasoning. arXiv preprint arXiv:2104.07307.</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4889-4896, Online. Association for Computational Linguistics.</p>
<h2>A Appendix</h2>
<h2>A. 1 Data Statistics Experimental Setup</h2>
<p>Numeration: We have 4906, 2097, 2997 in train, dev and test respectively. We make sure that all numbers within 10 K are present in any of train, dev or test. For extrapolation we select 1 K integers randomly from every 10 K range from $[10 \mathrm{~K}, 1000 \mathrm{~K})$ making it a total of 99 K .
Magnitude Order Prediction: For this data we consider $450 \mathrm{~K}, 50 \mathrm{~K}$ and 100 K samples for train, dev and test data respectively from each of market comments and article titles data.
List-Sort: We consider both the task of arranging in ascending and descending orders since if a series is already sorted in ascending order the model can directly predict by copying it from the given input.</p>
<div class="codehilite"><pre><span></span><code>What is nine thousand one hundred sixty two ?
Label : 9162 Predicted : 9172
What is eight hundred twenty thousand six ?
Label : 820006 Predicted : 826
What is one thousand nine hundred sixty ?
Label : 1960 Predicted : 1959
What is three hundred thousand four hundred fifteen ?
Label : 300415 Predicted : 3415
</code></pre></div>

<p>Figure 7: More predictions for Numeration task.</p>
<h2>A. 2 Hyperparameters</h2>
<p>For all the experiments we use maximum sequence length of 128 and 256 for question context. The maximum sequence length of the answers is kept as $[5,10,20,25]$ for different tasks. We ran for 20 epochs and save a model based on validation EM performance. Our training and validation batch size varies between $[2,4,8,16,32]$ based on the experiment. We work on 4 Tesla V100 GPUs. We use AdamW optimizer and StepLR scheduler with step size of 2 , learning rate of $5 \mathrm{e}-5$ and gamma of 0.1 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">2012 Chick-Fil-A Bowl preview: No. 8 LSU vs. No. <MASK> Clemson</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Label : 14 (2) Predicted : 6 (1)</td>
</tr>
<tr>
<td style="text-align: left;">&lt; MASK &gt; days to go before wind tax credit expires.</td>
</tr>
<tr>
<td style="text-align: left;">Label : 5 (1) Predicted : 63 (2)</td>
</tr>
<tr>
<td style="text-align: left;">Nonprofit Homefront America Receives \$ &lt; MASK &gt; from Walmart Foundation</td>
</tr>
<tr>
<td style="text-align: left;">Label : 10000 (5) Predicted : 100000 (6)</td>
</tr>
<tr>
<td style="text-align: left;">Gun ban advocates must decide if they're willing--and able--to kill &lt; MASK &gt;</td>
</tr>
<tr>
<td style="text-align: left;">Label : 50000000 (7) Predicted : 10000 (5)</td>
</tr>
<tr>
<td style="text-align: left;">NYSE indication BRKa.N last 130150.0 bid 128000.0 ask &lt; MASK &gt;</td>
</tr>
<tr>
<td style="text-align: left;">Label : 131000 (6) Predicted : 138000 (6)</td>
</tr>
<tr>
<td style="text-align: left;">NCR CORP - Updating its full year 2016 guidance for non-Gaap diluted eps to</td>
</tr>
<tr>
<td style="text-align: left;">$\$ 2.85$ from its previous guidance of $\$ 2.72$ to $\$&lt;$ MASK &gt;</td>
</tr>
<tr>
<td style="text-align: left;">Label : 2.82 (1) Predicted : 2.85 (1)</td>
</tr>
</tbody>
</table>
<p>Figure 8: More Magnitude Order Prediction Examples.</p>
<div class="codehilite"><pre><span></span><code>Which is minimum in value among 1625652 ?
Label : 52 Predicted : 56
Which is minimum in value among 630628627 ?
Label : 627 Predicted : 630
Which is minimum in value among 15379 32373 42492 ?
Label : 15379 Predicted : 32373
Which is minimum in value among 9682962196209707974797909665
    970197699762 ?
Label : 9620 Predicted : 9621
Which is minimum in value among 924735282352746688016938954929
96584813165734592317 ?
Label : 52746 Predicted : 52723
</code></pre></div>

<p>Figure 9: More predictions for List-MinMax task.</p>
<div class="codehilite"><pre><span></span><code>Sort in descending order : 4873488048274871487748464865484048794836
Label : 4880487948774873487148654846484048364827
Predicted : 4880487948734871486548464840483648274877
Sort in descending order : 632642652634651638621649633630
Label : 652651649642638634633632630621
Predicted : 652651649642638634633632621630
Sort in descending order : 594598632600633630560574634599
Label : 634633632630600599598594574560
Predicted : 634633632630599598594574560600
Sort in descending order : 60090220120123714921732913028
Label : 14921237902600291173120302820
Predicted : 149212379026002912911731203020
Sort in descending order : 4004922125387212251344180433131992317563
    1838538121
Label : 44180433134004938721381212251322125199231836517563
Predicted : 44180400494331338121387212251322125199231836517563
</code></pre></div>

<p>Figure 10: More predictions for List-Sort task.</p>
<h2>A. 3 Results and Error analysis</h2>
<p>Magnitude Order Prediction: We also experimented with zero-shot magnitude order predictions. We found 553 and 8783 exact-matches out of 100 K test data using T5-large which shows that the performance is very poor without proper fine-tuning. We show some more predictions of the best performing T5 model in Figure 7, 8, 9, 10.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/savoirfairelinux/num2words&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
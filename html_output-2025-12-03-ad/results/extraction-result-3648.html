<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3648 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3648</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3648</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-90.html">extraction-schema-90</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <p><strong>Paper ID:</strong> paper-269330043</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.15458v1.pdf" target="_blank">Learning Electromagnetic Metamaterial Physics With ChatGPT</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) such as ChatGPT, Gemini, LlaMa, and Claude are trained on massive quantities of text parsed from the internet and have shown a remarkable ability to respond to complex prompts in a manner often indistinguishable from humans. For all-dielectric metamaterials consisting of unit cells with four elliptical resonators, we present a LLM fine-tuned on up to 40,000 data that can predict the absorptivity spectrum given a text prompt that only specifies the metasurface geometry. Results are compared to conventional machine learning approaches including feed-forward neural networks, random forest, linear regression, and K-nearest neighbor (KNN). Remarkably, the fine-tuned LLM (FT-LLM) achieves a comparable performance across large dataset sizes with a deep neural network. We also explore inverse problems by asking the LLM to predict the geometry necessary to achieve a desired spectrum. LLMs possess several advantages over humans that may give them benefits for research, including the ability to process enormous amounts of data, find hidden patterns in data, and operate in higher-dimensional spaces. This suggests they may be able to leverage their general knowledge of the world to learn faster from training data than traditional models, making them valuable tools for research and analysis.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3648.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3648.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FT-LLM (ChatGPT 3.5 fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned Large Language Model based on ChatGPT 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ChatGPT 3.5 model fine-tuned on textual encodings of geometry→spectrum simulation pairs (up to 40,000 examples) to perform forward spectral prediction, inverse design attempts, and produce textual explanations for all-dielectric metasurfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT 3.5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative Pre-trained Transformer (GPT) 3.5 conversational LLM; authors used OpenAI's API to fine-tune the off-the-shelf model (no architecture changes reported), training on text-encoded numerical geometry vectors and text-encoded spectra (50 frequency points). Fine-tuning performed via OpenAI cloud; temperature used during inference = 0.5 (experiments with 0–1 range reported).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>predictive regression (spectral surrogate), inverse design generation, and textual explanation (knowledge summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>materials science / photonics / metamaterials (all-dielectric metasurfaces operating 150–500 THz; evaluations focused on 150–350 THz range, 50-point spectra)</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>simulation dataset: numerical geometry vectors (14-d) converted to structured text prompts; ground-truth absorptivity spectra downsampled to 50 numeric values and encoded as text (3-decimal precision)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Numerical encoding of tabular simulation data into text prompts (e.g., 'The Alldielectric metasurface is: <h,p,...> Get the absorptivity'), then supervised fine-tuning of ChatGPT 3.5 via OpenAI API to map geometry-text prompts → spectrum-text completions; inference outputs are parsed back to numeric arrays. Evaluated effects of prompt template and generation temperature; managed output length/formatting by regeneration and truncation heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual numeric spectra (lists of 50 decimal values), and natural-language explanations / proposed geometry lists for inverse tasks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Open-access metasurface simulation dataset used in prior work (CST-generated ground truth; geometry vectors of 14 parameters; spectra downsampled from 2000→100→select 50 points in 150–350 THz). Baselines: feed-forward neural network (NN), Random Forest (RF), K-Nearest Neighbors (KNN), Linear Regression (LR).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Mean Squared Error (MSE) and Mean Absolute Relative Error (MARE) over test set (n samples, f=50 frequency points); small epsilon added (1e-4) to denominator for MARE to avoid division by zero. Also qualitative checks for output format validity and inverse-design resimulation error.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FT-LLM achieved best reported MSE = 0.00639 and best MARE = 0.165 on the task; MARE performance became comparable to a domain-optimized deep neural network once training size exceeded ~1,000 samples. FT-LLM had worse MSE in low-data regimes (<10k) but improved faster than most baselines with increasing data up to 40k. Inverse-design attempts were frequently invalid for models fine-tuned on large (>10k) datasets; small-data fine-tuned models sometimes produced valid but inaccurate designs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM training objective (cross-entropy) mismatched to regression loss (MSE) leading to metric bias; LLM struggled to preserve high numeric precision beyond a few decimals; generated variable-length or malformed outputs (handled by regeneration/truncation); poor interpretability — fine-tuning did not yield superior physical explanations over the base GPT; inverse-design outputs often invalid/hallucinated (especially when trained on >10k examples); high computational/monetary cost for fine-tuning on OpenAI platform (~$4,000 for experiments reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly against a specialized feed-forward NN and classical ML (RF, KNN, LR). FT-LLM matched NN in MARE at modest data sizes (>1k) but typically underperformed NN on MSE in low-data regimes; FT-LLM improved more quickly with increased data. For inverse design, traditional inverse-design methods (neural-adjoint, tandem, conventional NN approaches cited) outperformed FT-LLM; FT-LLM produced more invalid/hallucinated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>Inverse-design prompts: FT-LLM fine-tuned on >10k samples frequently returned invalid formats (repeated tokens, fragmented words, numeric lists that violate geometric constraints). Explanatory prompts: FT-LLM did not provide substantially better physical insight than the original un-fine-tuned GPT 3.5, suggesting memorization/statistical pattern-matching rather than learned causal laws. Numeric-conversion errors and non-numeric outputs occurred (non-numeric <0.1% reported) and length mismatches occurred more frequently at some training sizes (e.g., 85% correct length at 1,000 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Electromagnetic Metamaterial Physics With ChatGPT', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3648.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3648.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based literature knowledge extraction (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for extraction/summarization of knowledge from scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper mentions (in discussion/future work and citations) that LLMs can extract knowledge from papers and summarize voluminous/high-dimensional scientific data, potentially aiding discovery and insight across domains, but provides no implemented pipeline for distilling quantitative laws from corpora of scholarly papers within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning Electromagnetic Metamaterial Physics With ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (general mention: ChatGPT, Gemini, LLaMA, Claude and related foundation models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General pretrained transformer-based LLMs (no size/architecture details provided in paper for these mentions); discussed as possessing broad world knowledge from large-scale pretraining and as potential building blocks for agents that can read/search/analyze papers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge extraction / summarization / potential literature-scale information retrieval (not executed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific literature (suggested applicability across chemistry, optics, mechanics, materials science, and metamaterials research)</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>scholarly papers / textbooks / simulation datasets (proposed future data sources; not used in experiments here)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Stated as potential directions: create more diverse training corpora including research papers and textbooks, build LLM agent systems that autonomously interact with datasets and search literature, or transition to reinforcement-learning fine-tuning to gain physics insight; no concrete pipeline (e.g., retrieval-augmentation, symbolic regression, or equation-fitting) is implemented or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual summaries, extracted facts/parameters, and (hypothetically) distilled insights; the paper does not report LLM-generated mathematical laws/equations from literature in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>no benchmark dataset for law discovery is used in this work; paper cites external works (e.g., predictive chemistry, OptoGPT, MeLM, retrieval-of-synthesis-parameters) as related but does not evaluate them here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>none reported for literature-scale law extraction in this paper (only MSE/MARE used for the simulation surrogate tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper only asserts potential: 'LLMs have proven effective in the extraction of knowledge from papers' (citing prior work) and proposes LLM agents as feasible future directions; the authors did not perform or demonstrate distillation of quantitative laws/equations from large scholarly corpora within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors explicitly caution that fine-tuning on geometry→spectrum pairs alone is insufficient for physical insight, and that dataset diversity and training objective mismatch are likely obstacles to using LLMs to discover causal quantitative laws; hallucination, format fragility, and lack of numeric precision are additional challenges highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>No empirical comparison in this paper; the text suggests combining foundational models with diverse literature corpora and possibly reinforcement learning or agent frameworks, and references other domain-specific LLM applications (predictive chemistry, OptoGPT, MeLM) as related approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>Within this study, attempts to elicit physical explanations from the FT-LLM did not outperform un-fine-tuned GPT 3.5, serving as a negative example that fine-tuning on simulation pairs alone does not yield robust law-discovery or superior physical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Electromagnetic Metamaterial Physics With ChatGPT', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Leveraging large language models for predictive chemistry <em>(Rating: 2)</em></li>
                <li>OptoGPT: A foundation model for inverse design in optical multilayer thin film structures <em>(Rating: 2)</em></li>
                <li>MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems <em>(Rating: 2)</em></li>
                <li>Retrieval of synthesis parameters of polymer nanocomposites using LLMs <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Language-controllable programmable metasurface empowered by large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3648",
    "paper_id": "paper-269330043",
    "extraction_schema_id": "extraction-schema-90",
    "extracted_data": [
        {
            "name_short": "FT-LLM (ChatGPT 3.5 fine-tuned)",
            "name_full": "Fine-tuned Large Language Model based on ChatGPT 3.5",
            "brief_description": "A ChatGPT 3.5 model fine-tuned on textual encodings of geometry→spectrum simulation pairs (up to 40,000 examples) to perform forward spectral prediction, inverse design attempts, and produce textual explanations for all-dielectric metasurfaces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT 3.5 (fine-tuned)",
            "model_description": "Generative Pre-trained Transformer (GPT) 3.5 conversational LLM; authors used OpenAI's API to fine-tune the off-the-shelf model (no architecture changes reported), training on text-encoded numerical geometry vectors and text-encoded spectra (50 frequency points). Fine-tuning performed via OpenAI cloud; temperature used during inference = 0.5 (experiments with 0–1 range reported).",
            "task_type": "predictive regression (spectral surrogate), inverse design generation, and textual explanation (knowledge summarization)",
            "domain": "materials science / photonics / metamaterials (all-dielectric metasurfaces operating 150–500 THz; evaluations focused on 150–350 THz range, 50-point spectra)",
            "input_data_type": "simulation dataset: numerical geometry vectors (14-d) converted to structured text prompts; ground-truth absorptivity spectra downsampled to 50 numeric values and encoded as text (3-decimal precision)",
            "method_description": "Numerical encoding of tabular simulation data into text prompts (e.g., 'The Alldielectric metasurface is: &lt;h,p,...&gt; Get the absorptivity'), then supervised fine-tuning of ChatGPT 3.5 via OpenAI API to map geometry-text prompts → spectrum-text completions; inference outputs are parsed back to numeric arrays. Evaluated effects of prompt template and generation temperature; managed output length/formatting by regeneration and truncation heuristics.",
            "output_type": "textual numeric spectra (lists of 50 decimal values), and natural-language explanations / proposed geometry lists for inverse tasks",
            "benchmark_or_dataset": "Open-access metasurface simulation dataset used in prior work (CST-generated ground truth; geometry vectors of 14 parameters; spectra downsampled from 2000→100→select 50 points in 150–350 THz). Baselines: feed-forward neural network (NN), Random Forest (RF), K-Nearest Neighbors (KNN), Linear Regression (LR).",
            "evaluation_metrics": "Mean Squared Error (MSE) and Mean Absolute Relative Error (MARE) over test set (n samples, f=50 frequency points); small epsilon added (1e-4) to denominator for MARE to avoid division by zero. Also qualitative checks for output format validity and inverse-design resimulation error.",
            "results_summary": "FT-LLM achieved best reported MSE = 0.00639 and best MARE = 0.165 on the task; MARE performance became comparable to a domain-optimized deep neural network once training size exceeded ~1,000 samples. FT-LLM had worse MSE in low-data regimes (&lt;10k) but improved faster than most baselines with increasing data up to 40k. Inverse-design attempts were frequently invalid for models fine-tuned on large (&gt;10k) datasets; small-data fine-tuned models sometimes produced valid but inaccurate designs.",
            "limitations_or_challenges": "LLM training objective (cross-entropy) mismatched to regression loss (MSE) leading to metric bias; LLM struggled to preserve high numeric precision beyond a few decimals; generated variable-length or malformed outputs (handled by regeneration/truncation); poor interpretability — fine-tuning did not yield superior physical explanations over the base GPT; inverse-design outputs often invalid/hallucinated (especially when trained on &gt;10k examples); high computational/monetary cost for fine-tuning on OpenAI platform (~$4,000 for experiments reported).",
            "comparison_to_other_methods": "Compared directly against a specialized feed-forward NN and classical ML (RF, KNN, LR). FT-LLM matched NN in MARE at modest data sizes (&gt;1k) but typically underperformed NN on MSE in low-data regimes; FT-LLM improved more quickly with increased data. For inverse design, traditional inverse-design methods (neural-adjoint, tandem, conventional NN approaches cited) outperformed FT-LLM; FT-LLM produced more invalid/hallucinated outputs.",
            "notable_counterexamples": "Inverse-design prompts: FT-LLM fine-tuned on &gt;10k samples frequently returned invalid formats (repeated tokens, fragmented words, numeric lists that violate geometric constraints). Explanatory prompts: FT-LLM did not provide substantially better physical insight than the original un-fine-tuned GPT 3.5, suggesting memorization/statistical pattern-matching rather than learned causal laws. Numeric-conversion errors and non-numeric outputs occurred (non-numeric &lt;0.1% reported) and length mismatches occurred more frequently at some training sizes (e.g., 85% correct length at 1,000 samples).",
            "uuid": "e3648.0",
            "source_info": {
                "paper_title": "Learning Electromagnetic Metamaterial Physics With ChatGPT",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-based literature knowledge extraction (mentioned)",
            "name_full": "Large Language Models for extraction/summarization of knowledge from scientific papers",
            "brief_description": "The paper mentions (in discussion/future work and citations) that LLMs can extract knowledge from papers and summarize voluminous/high-dimensional scientific data, potentially aiding discovery and insight across domains, but provides no implemented pipeline for distilling quantitative laws from corpora of scholarly papers within this work.",
            "citation_title": "Learning Electromagnetic Metamaterial Physics With ChatGPT",
            "mention_or_use": "mention",
            "model_name": "various LLMs (general mention: ChatGPT, Gemini, LLaMA, Claude and related foundation models)",
            "model_description": "General pretrained transformer-based LLMs (no size/architecture details provided in paper for these mentions); discussed as possessing broad world knowledge from large-scale pretraining and as potential building blocks for agents that can read/search/analyze papers.",
            "task_type": "knowledge extraction / summarization / potential literature-scale information retrieval (not executed in this paper)",
            "domain": "general scientific literature (suggested applicability across chemistry, optics, mechanics, materials science, and metamaterials research)",
            "input_data_type": "scholarly papers / textbooks / simulation datasets (proposed future data sources; not used in experiments here)",
            "method_description": "Stated as potential directions: create more diverse training corpora including research papers and textbooks, build LLM agent systems that autonomously interact with datasets and search literature, or transition to reinforcement-learning fine-tuning to gain physics insight; no concrete pipeline (e.g., retrieval-augmentation, symbolic regression, or equation-fitting) is implemented or evaluated in this paper.",
            "output_type": "textual summaries, extracted facts/parameters, and (hypothetically) distilled insights; the paper does not report LLM-generated mathematical laws/equations from literature in experiments.",
            "benchmark_or_dataset": "no benchmark dataset for law discovery is used in this work; paper cites external works (e.g., predictive chemistry, OptoGPT, MeLM, retrieval-of-synthesis-parameters) as related but does not evaluate them here.",
            "evaluation_metrics": "none reported for literature-scale law extraction in this paper (only MSE/MARE used for the simulation surrogate tasks).",
            "results_summary": "The paper only asserts potential: 'LLMs have proven effective in the extraction of knowledge from papers' (citing prior work) and proposes LLM agents as feasible future directions; the authors did not perform or demonstrate distillation of quantitative laws/equations from large scholarly corpora within this study.",
            "limitations_or_challenges": "Authors explicitly caution that fine-tuning on geometry→spectrum pairs alone is insufficient for physical insight, and that dataset diversity and training objective mismatch are likely obstacles to using LLMs to discover causal quantitative laws; hallucination, format fragility, and lack of numeric precision are additional challenges highlighted.",
            "comparison_to_other_methods": "No empirical comparison in this paper; the text suggests combining foundational models with diverse literature corpora and possibly reinforcement learning or agent frameworks, and references other domain-specific LLM applications (predictive chemistry, OptoGPT, MeLM) as related approaches.",
            "notable_counterexamples": "Within this study, attempts to elicit physical explanations from the FT-LLM did not outperform un-fine-tuned GPT 3.5, serving as a negative example that fine-tuning on simulation pairs alone does not yield robust law-discovery or superior physical reasoning.",
            "uuid": "e3648.1",
            "source_info": {
                "paper_title": "Learning Electromagnetic Metamaterial Physics With ChatGPT",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Leveraging large language models for predictive chemistry",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        },
        {
            "paper_title": "OptoGPT: A foundation model for inverse design in optical multilayer thin film structures",
            "rating": 2,
            "sanitized_title": "optogpt_a_foundation_model_for_inverse_design_in_optical_multilayer_thin_film_structures"
        },
        {
            "paper_title": "MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems",
            "rating": 2,
            "sanitized_title": "melm_a_generative_pretrained_language_modeling_framework_that_solves_forward_and_inverse_mechanics_problems"
        },
        {
            "paper_title": "Retrieval of synthesis parameters of polymer nanocomposites using LLMs",
            "rating": 2,
            "sanitized_title": "retrieval_of_synthesis_parameters_of_polymer_nanocomposites_using_llms"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Language-controllable programmable metasurface empowered by large language models",
            "rating": 1,
            "sanitized_title": "languagecontrollable_programmable_metasurface_empowered_by_large_language_models"
        }
    ],
    "cost": 0.013006499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Electromagnetic Metamaterial Physics With ChatGPT</p>
<p>Darui Lu 
Department of Electrical and Computer Engineering
Duke University
27705DurhamNCUSA</p>
<p>Yang Deng 
Department of Electrical and Computer Engineering
Duke University
27705DurhamNCUSA</p>
<p>Jordan M Malof 0000-0002-7851-4920
Department of Electrical Engineering and Computer Science
University of Missouri
65211ColumbiaMOUSA</p>
<p>Willie J Padilla willie.padilla@duke.edu 0000-0001-7734-8847
Department of Electrical and Computer Engineering
Duke University
27705DurhamNCUSA</p>
<p>Learning Electromagnetic Metamaterial Physics With ChatGPT
FE3DBBA4211F19D59301F18DA7F3A02010.1109/ACCESS.2025.3552418Received 11 February 2025, accepted 13 March 2025, date of publication 18 March 2025, date of current version 28 March 2025.
Large language models (LLMs) such as ChatGPT, Gemini, LlaMa, and Claude are trained on massive quantities of text parsed from the internet and have shown a remarkable ability to respond to complex prompts in a manner often indistinguishable from humans.For all-dielectric metamaterials consisting of unit cells with four elliptical resonators, we present a LLM fine-tuned on up to 40,000 data that can predict the absorptivity spectrum given a text prompt that only specifies the metasurface geometry.Results are compared to conventional machine learning approaches including feed-forward neural networks, random forest, linear regression, and K-nearest neighbor (KNN).Remarkably, the fine-tuned LLM (FT-LLM) achieves a comparable performance across large dataset sizes with a deep neural network.We also explore inverse problems by asking the LLM to predict the geometry necessary to achieve a desired spectrum.LLMs possess several advantages over humans that may give them benefits for research, including the ability to process enormous amounts of data, find hidden patterns in data, and operate in higher-dimensional spaces.This suggests they may be able to leverage their general knowledge of the world to learn faster from training data than traditional models, making them valuable tools for research and analysis.INDEX TERMS Metamaterial, large language model, deep learning.Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.VOLUME 13, 2025Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>I. INTRODUCTION</p>
<p>Deep learning, and particularly deep neural networks (DNNs), have recently emerged as a valuable tool in the field of metamaterials research and have produced many novel results [1], [2], [3].This data-driven approach to metamaterial design has profound capabilities for both forward [4] and inverse processes [5], [6].Once trained, DNNs can accelerate the simulation of material systems by orders of magnitude compared to traditional numerical simulations [7], [8], [9], [10], enabling faster prototyping and exploration.Similarly, in inverse design, these models have successfully discovered state-of-the-art solutions that push the boundaries of what is achievable with metamaterials [11], [12], [13], [14].</p>
<p>Despite these advances, the implementation of DNNs still faces several challenges [1], [2], [3].As a data-driven method, DNNs necessitate large datasets for training to</p>
<p>The associate editor coordinating the review of this manuscript and approving it for publication was Venkateshkumar M .achieve high accuracy and generalizability [15].This socalled ''data bottleneck'' issue is compounded by questions of interpretability and understanding, and therefore explaining model predictions remains a significant hurdle [16], [17].This has led to the pursuit of models that are capable of learning effectively from smaller datasets, and to the development of techniques such as transfer learning, [18], [19], [20], [21] and physics-informed/driven DNNs [9], [22], [23], [24], [25], [26], [27].</p>
<p>In parallel, large language models (LLMs), such as the widely-used generative pre-trained transformer (GPT) models, have made remarkable strides in natural language processing tasks [28].These models are trained to perform a simple task: given some textual input (e.g., a sentence fragment), predict the most probable next word of the input.Even though this training task is relatively simple, the models are trained on massive volumes of textual data, and they have been found to acquire an ability to construct coherent and contextually relevant text that encapsulates broad knowledge across diverse domains.There is also some evidence that LLMs can utilize this apparent knowledge to perform basic reasoning tasks [29].For instance, LLMs can engage in conversations, translate languages, summarize texts, and even generate content that mimics human writing styles [28], [30] The multifaceted capabilities of LLMs are rooted in their extensive training on diverse datasets [31].First, they integrate a vast range of information from their training sets, making them repositories of wide-reaching knowledge about much the world.This allows them to recall and leverage facts, concepts, and relationships when generating responses.Secondly, LLMs can perform reasoning tasks based on the information they have been trained on, enabling them to handle queries that require logical deductions, problemsolving, or creative generation.This can range from solving mathematical problems to crafting detailed narratives or technical explanations.Lastly, the ability of LLMs to explain their reasoning process adds a layer of interpretability, often allowing users to understand the steps the model took to arrive at a conclusion or response, thus providing insights into the model's thought process.This development raises an exciting question: can LLMs be used to model and design complex electromagnetic metamaterials?</p>
<p>Our study investigates the feasibility of LLMs to address metamaterial design challenges.Building on recent findings that suggest LLM proficiency in scientific regression and classification tasks [32], we explore their potential in predicting the electromagnetic spectra of metamaterials -a problem expressible in textual terms.This adaptability of LLMs has already been demonstrated in chemistry [33], [34], optics [35], and mechanics [36], signaling their versatility across various scientific fields.</p>
<p>Most studies within the field of metamaterials have relied on traditional deep neural network architectures.For example, often a one-dimensional (1D) vector which represents the metamaterial geometry and material properties is used as input to a conventional feed-forward DNN, [37] which has been explored in various studies [7], [13], [38].Other studies have used two-dimensional (2D) arrays to quantify the cross-sectional shape of metamaterials, where binary values (0 or 1) represent different materials such as metal or air.These 2D arrays are then used as inputs to the network, and therefore align well with the architecture of convolutional neural networks (CNNs).[39], [40], [41] Other notable approaches applied to metamaterials include reinforcement learning [42] and graph neural networks [43].While all of the conventional deep learning methods mentioned above can provide accurate prediction of metamaterial properties, LLMs offer a fundamentally different approach.That is, since large language models can converse in a manner similar to humans, they posses the ability accept textual input and are not restricted to a vector input of a certain dimension.In this work, we introduce an encoding scheme that converts both geometric and spectral data into textual form, thereby allowing LLMs to handle metamaterial design as a language-processing task.Because LLMs are trained on massive, diverse datasets, they appear to have substantial internal physics knowledge, which they could leverage to perform basic reasoning about such systems.Moreover, LLMs may be able to explain the reasoning that leads to their predictions using natural language.If successful, LLM-based methods could greatly accelerate the simulation and design cycle in metamaterial research.</p>
<p>Here, we focus on all-dielectric metasurfaces, where we use a LLM to both predict absorptivity spectrum when prompted with the geometry, as well as to perform inverse design.We compare the capabilities of LLMs with established machine learning and deep learning models.We find that our fine-tuned model, FT-LLM (based on ChatGPT 3.5), achieves a competitive performance with deep neural networks on large datasets for prediction of the frequency dependent absorptance.Additionally, we assess the capabilities of the FT-LLM to perform inverse design by evaluating whether it can generate metamaterial geometries that achieve specified absorptivity spectra.Moreover, one of the most powerful capabilities of LLMs is their ability to summarize high-dimensional or voluminous data with interpretable natural language, such as generating short captions that succinctly describe image content [44].Motivated by this, we investigate whether LLMs might be able to summarize insights from large datasets of simulation data.In particular, we explore whether our FT-LLM model can explain what it has learned about all-dielectric metasurfaces (ADMs), and whether these explanations are superior to those obtained by an out-of-the-box LLM that has not been trained on our data.Through this work, we highlight the potential of LLMs as powerful tools in metamaterial research.</p>
<p>II. METHODOLOGY</p>
<p>This work adapts LLMs to predict the spectral response of metamaterials.Specifically, we focus on predicting the spectra of an all-dielectric metamaterial, consisting of a unit cell with four elliptical resonators characterized by fourteen geometric parameters: height (h), periodicity (p), semimajor axes (r ma ), semiminor axes (r mi ), and rotational angles (θ ).Let D = {(g (i) , s (i) ) | i = 1, . . ., n} be our simulation dataset [13].Here, simulation data are generated through use of a computational electromagnetic solver, CST Studio Suite (CST) [45].Thus the simulated data constitute the ''ground truth'' data.Given a raw geometry vector g (i) ∈ R 14 , our objective is to predict its corresponding spectrum s (i) ∈ R 50 .However, LLMs, represented here as f φ with trainable parameter φ, are designed to process text data.With an text input, the model produces an output text completion.</p>
<p>LLMs currently do not possess the ability to directly process numerical data g (i) and s (i) .To address this issue, we transform the tabular data into text descriptions T g(i) = En g (g (i) ) and T s(i) = En s (s (i) ), where En g and En s are transformation functions that converts numerical data into FIGURE 1. Schematic depicting the workflow.The process begins with simulations (orange rectangle) to acquire the metamaterial geometry-spectrum dataset.Subsequently, these numerical geometric parameters are transformed into textual descriptions -shown by the numerical encoding path.We then fine-tune ChatGPT 3.5 using OpenAI's API with the built-in loss shown as the blue box.</p>
<p>a structured text format.Based upon the textual input T g(i) , the LLMs can then produce a textual completion, Ts(i) = f φ (T g(i) ).We can then fine-tune a LLM to make more accurate predictions by comparing its output to T s(i) .As a result, we enable the model to handle the spectral response prediction as a language processing task without any modifications to the model structure and training process [32].The flowchart in Fig. 1 illustrates the steps of our method.</p>
<p>A. NUMERICAL ENCODING</p>
<p>Our first step is to transform the numerical data into a suitable format for input to the LLM.For the numerical vector describing the geometry of the metasurface, we embed it within a text prompt template [36].Given a raw geometry vector g (i) = (h, p, r ma1 , r mi1 , r ma2 , r mi2 , r ma3 ,r mi3 ,r ma4 , r mi4 , θ 1 , θ 2 , θ 3 , θ 4 ), the text encoding T g(i) is as follows: ''The Alldielectric metasurface suspended in free space is: <h, p, r ma1 , r mi1 , r ma2 , r mi2 , r ma3 , r mi3 , r ma4 , r mi4 , θ 1 , θ 2 , θ 3 , θ 4 > Get the absorptivity''.An example is provided in Table 2.</p>
<p>In terms of spectra, we directly convert the ground truth s (i) into strings T s(i) , such as translating number 0.1 to '0.1'.An issue that arises is that the spectrum value can possess many digits, such as 0.010290466248989103.Using a LLM to deal with a number such as this would be inefficient and -more importantly -challenging.LLMs are generative models and it is difficult for them to maintain prediction precision beyond a few decimal points [33].Furthermore, error measurements are often insensitive to such precision, indicating that maintaining this level of significant digits is unnecessary.Therefore, we use a precision of three decimal points, generating T s(i) such as '[0.001, • • • , 0.875]'.</p>
<p>B. MODEL FINE-TUNING</p>
<p>Once the geometry-spectrum data has been translated into text, it can be used to fine-tune the LLM.In this work, we train the Generative Pre-rained Transformer 3.5 (ChatGPT 3.5) utilizing the OpenAI API.During the training process, the model receives T g(i) and then produces a text response Ts(i) .By comparing prediction Ts(i) and ground truth T s(i) , we iteratively adjust the model parameters φ to improve its spectral prediction capabilities.We call the resulting trained GPT model the Fine-Tuned LLM (FT-LLM).</p>
<p>C. INFERENCE</p>
<p>The temperature setting in LLMs influences the level of randomness in the generated results [46].The formula is given by:
P(x i ) = e 1 T logit(x i ) j e 1 T logit(x j )(1)
where P(x i ) is the probability of the i-th token, and logit(x i ) = ln(x i /(1 − x i )) for x i ∈ (0, 1) is the logit (log probability) of the i-th token from the output layer and T is the temperature.We set the temperature at a value of 0.5.Let D t = {(g t(i) , s t(i) ) | i = 1, . . ., n} be our test dataset.During inference (i.e., making predictions), FT-LLM predicts a spectral response Tst(i) for geometry input T gt(i) .We then convert Tst(i) back into numerical form ŝt(i) , which is then benchmarked against spectra generated by CST s t(i) to measure the accuracy.Since the FT-LLM can give undesired outputs, we further discuss error management in Appendix A.</p>
<p>III. EXPERIMENTAL DESIGN AND RESOURCES</p>
<p>Dataset Our study harnesses a dataset introduced and benchmarked in previous work, [13], [37] and is chosen for its relevance to the understanding of the capability of LLMs in designing metasurfaces.This dataset is open-access, and the structured vector format of its geometrical inputs and spectral outputs are advantages for prompt engineering in LLMs.All simulation data are generated using the CST solver.That is we find that the vector description provides a compact representation of the complex metasurface, with each parameter encoding its physical impact on absorption.Such datasets are helpful for physically data manipulation and model training, enabling a higher degree of interpretability of feature importance.Below, we detail the metasurface geometry and spectra in the dataset.The all-dielectric metasurface is fashioned from silicon carbide and operates over a range in the THz from 150-500 THz.This complex structure is defined by a supercell comprising four elliptical resonators, each positioned at the center of a subdivided quadrant within a square supercell.The metasurface geometry is given as a 14-dimensional vector: [h, p, r ma1 , r mi1 , r ma2 , r mi2 , r ma3 ,r mi3 , r ma4 , r mi4 , θ 1 , θ 2 , θ 3 , θ 4 ].The periodicity p parameter specifies the side length of the supercell, and is proportional to the operating range of the resonator array.The height h parameter is set to be identical for all resonators, and all geometrical parameters are in units of µm.The in-plane dimensions of each elliptical resonator are proportionally scaled to the supercell periodicity through the x-axis radius ratio r ma,i and y-axis radius ratio r mi,i for each resonator, respectively.Additionally, the orientation of each elliptical resonator is adjusted through a rotational angle θ i , measured in radians, with respect to the x-axis.Given the challenges associated with processing high-dimensional data by LLMs, the spectrum output was manipulated by first downsampling from 2000 frequency points to 100 frequency points.Then, we only select 50 points from the 150 − 350 THz frequency range, aiming to refine the LLM's predictive accuracy and computational efficiency within the expansive operational bandwidth.</p>
<p>Data Handling To ensure experimental integrity, we divided the dataset into three distinct, independently sampled sets: training, validation, and test.The training set was randomly selected for each dataset size's training session, whereas the validation and test sets comprising 1,000 samples each were specified prior to the experiments.The test set was constructed to evaluate the model's generalization performance, and it contains metamaterial geometries and absorptance values that the model has not seen during training or validation.Optimal model selection was based on validation set performance, with the test set used for final evaluation.This approach guarantees the reliability and reproducibility of our comparison between the baseline models and large language models applied to the metamaterials problem.</p>
<p>Prior to model training, our dataset undergoes preprocessing steps to ensure optimal model performance.Scoring Metrics This section outlines the error metrics employed for evaluating the performance of both baseline and LLM models.Our models predict the electromagnetic spectra at a set of specific frequency values, and we wish to measure the error between these predictions and the CST-generated ground truth.In artificial electromagnetic material (AEM) research, both absolute and relative error metrics are commonly utilized [7], [10].To comprehensively evaluate accuracy of the FT-LLM, we employ two complementary metrics: mean squared error (MSE) and mean absolute relative error (MARE).The MSE and MARE over the testing dataset are given mathematically by
MSE = 1 n n i=1 1 f f j=1 (s i,j − ŝi,j ) 2 ),(2)MARE = 1 n n i=1 1 f f j=1 |s i,j − ŝi,j | |s i,j | + ϵ ), (3)
where n is the number of samples, f is the number of frequency points in the spectrum, S i,j represents the absorptivity value for ith sample at the jth frequency point, and Ŝi,j denotes the model predicted value.For MARE, we add a small value ϵ = 1e −4 to mitigate the effect of tiny values which can lead to extremely large percentage errors, thereby skewing the metric -see Appendix C.</p>
<p>Error Metric Bias It is important to note that MSE is favorably biased towards all of the baseline models, since they are all trained using this metric.Therefore, the MSE gives a unfavorable bias for the FT-LLM giving it a disadvantage.However, the MARE provides a more unbiased metic for all models, since none of them are trained on this metric.Therefore, the baseline models will tend to perform better on MSE than MARE, as MARE is more sensitive to errors in low-value regions.Additionally, LLMs are most often trained using cross-entropy loss (CEL) [28], which treats all errors equally, regardless of their magnitude, and therefore is not typically used to evaluate the accuracy of regression models.Because of this, and because CEL has not been utilized to evaluate the accuracy of such models in AEM studies, we do not include it in our evaluations.</p>
<p>Baseline Models To benchmark the performance of LLMs, we incorporate four other machine learning algorithms: of baseline models presented are the average of three models.Error bars indicate the standard deviation of the three trails.However, due to computational resource limitations, we only conducted single trial for the GPT model at large dataset sizes (10,000, 20,000, and 40,000 samples).While this may introduce some variances in the results, the observed trends are consistent with expectations.A temperature of 0.5 was used for the GPT model.</p>
<p>Feed-forward Neural Networks (NN), Random Forests (RF), K Nearest Neighbors (KNN), and Linear Regression (LR).The selection of the NN was motivated by its published efficacy in various practical applications, particularly within the domain of metamaterials research, where its ability to model complex, nonlinear relationships is highly valued [1], [2], [3].Conversely, RF, KNN, and LR represent classical machine learning algorithms designed to address regression challenges.Given that the design of metamaterials is predominantly formed as a regression problem, these algorithms were deemed especially suitable for benchmarking against the metasurface design challenges alongside LLMs.Our choice of algorithms aims to encompass both the cutting-edge capabilities of neural networks and the robust, well-established methodologies of classical machine learning, ensuring a comprehensive and fair evaluation of FT-LLM applicability and performance in the all-dielectric metasurface design.</p>
<p>Experimental Resources and Accessibility For the training and inference of NNs, we utilize NVIDIA GTX 3090 GPUs, employing the PyTorch library to facilitate our computations.The execution of RF, KNN, and LR models is conducted on an Intel R ⃝ Xeon Gold R ⃝ 6226 CPU.For the GPT, the entire process-from training to inference-is executed through the OpenAI API, with all computation handled by OpenAI's cloud computing.</p>
<p>IV. RESULTS AND DISCUSSION</p>
<p>In this section, our main focus is investigating the efficiency of FT-LLM (i.e., the fine-tuned ChatGPT 3.5 model) and comparing it with other baseline models.Additionally, we explore model performance on interpretability (Sec.IV-B) and inverse design (Sec.IV-C).</p>
<p>A. HOW EFFICIENTLY DOES CHATGPT LEARN?</p>
<p>Here we investigate how efficiently LLMs can learn to predict the physical properties of materials, in terms of the quantity of training data required to achieve a certain level of prediction accuracy.On one hand, there is evidence that LLMs possess significant knowledge about natural systems, including materials, and therefore they may require less training data to make accurate predictions.On the other hand, LLMs are pre-trained on language tasks rather than numerical prediction tasks, and they also possess a massive number of free parameters; both of these properties may imply that LLMs require relatively larger training datasets to make accurate predictions.</p>
<p>To evaluate the learning efficiency of LLMs, we evaluate the prediction accuracy of FT-LLM against all the baseline models, as we vary the size of their training data.In Fig. 2 we present the main results of these experiments which reveal the dependence of the two error metrics, MARE (a) and MSE (b), on the size of the training dataset.The FT-LLM achieves a best MSE of 0.00639 and a best MARE of 0.165, indicating that the FT-LLM's predictions are close to the CST-generated ground truths.This highlights the reliability of the LLMbased approach.</p>
<p>In Fig. 2(a) we find that the FT-LLM (red curve) performs similar to the linear and KNN machine learning models for the smallest dataset sizes, but shows significant improvement as the dataset size grows.At dataset sizes of 1,000 and larger, the FT-LLM outperforms all traditional machine learning models, and achieves similar accuracy to the deep neural network for all sizes thereafter.The results therefore indicate that, within the training set sizes investigated, the LLM model achieves similar learning efficiency to the neural network.This result is surprising since our neural network architecture is adopted from prior work, and highly optimized for this task, and it was also trained specifically for regression, while the FT-LLM was fine-tuned using cross-entropy, which is not well-suited for regression (see Sec. III).Despite these advantages, the FT-LLM model achieves comparable performance with the neural network after just 1,000 training samples.</p>
<p>In terms of MSE, in Fig. 2(b) we observe that the FT-LLM performs the worst among all models in the low-data scenario (&lt; 10,000 samples).However, when larger datasets are employed, the FT-LLM shows faster improvement in its accuracy, surpassing all baseline models except for the neural network once 40,000 training samples are used.We note that even with 40,000 training samples, the rate of improvement in the FT-LLM is greater than all other models, suggesting that it may achieve or surpass the accuracy of the neural network if more data were added.This result is surprising since, as noted in Sec.III, all of our models were trained to minimize MSE, while the FT-LLM model was trained to minimize a different criterion.Despite this disadvantage, the FT-LMM outperforms most models, and is improving faster than the neural network.</p>
<p>The differences between the MARE and MSE evaluations can be attributed to how these two different error metrics are calculated.The spectra in our training dataset possess low numerical values (e.g.0.001) at some frequencies.In such cases, if a prediction is 0.002 and the actual value is 0.001, the MSE is very low because it depends on the absolute error.However, MARE becomes very high in this case (100%) as it measures the relative error.LLMs that are fine-tuned using cross-entropy loss tend to maintain uniform performance across all spectral magnitudes.However, a neural network trained on MSE loss tends to achieve better accuracy in high-value regions while having lower accuracy in low-value regions because MSE does not heavily penalize small absolute errors.This explains why the FT-LLM performs competitively with neural networks in terms of MARE after 1,000 samples but requires larger dataset sizes to reach similar performance levels in terms of MSE.</p>
<p>We also explored the influence of the prompt template, and the LLM temperature hyperparameter, on FT-LLM's prediction accuracy.Specifically, the choice of prompt template affects how input information is structured and presented to the model, while the temperature setting controls the variability of model output.In addition, in Appendix D, we discuss the hardware efficiency of the model.</p>
<p>1) IMPACT OF PROMPT</p>
<p>Studies on prompt engineering indicate that the structure and detail of the prompt may influence the performance of the LLM [47].It may be conjectured that a more descriptive prompt may improve predictive accuracy.To investigate the template effect, we compared two prompts: one with relatively more detail than the other.Examples of both templates used are provided in Table 1.We conducted a fine-tuning experiment on the FT-LLM using both prompt templates with varying dataset sizes.The MSE comparison is shown in Fig. 3.</p>
<p>Our results indicate that both prompt designs yield nearly the same performance in all sizes of training samples.This consistency suggests that the accuracy of the model is FIGURE 3. Evaluations of model performance using two different prompt templates.These templates are provided in Table 1.All the results presented are averages from three models.However, the results at the 10,000 data points are exceptions, as computational constraints limited these to single trials.</p>
<p>not significantly influenced by whether the input data is presented in a concise vector form or as a detailed description.</p>
<p>2) INFLUENCE OF TEMPERATURE</p>
<p>The temperature settings of LLMs affect the variability of their output.In general LLM's produce different responses each time it is prompted, even if the prompts are identical.When the temperature is set lower, the LLM will tend to select a less diverse set of prompts that it estimates are most probable (i.e., the most likely to be accurate spectral predictions).By contrast, when the temperature is higher the model will tend to provide more diverse responses, at the cost of sometimes selecting responses that it estimates are less probable.We explored the effects of varying temperature settings, specifically [0, 0.25, 0.5, 0.75, 1], on model performance.The results are presented in Fig. 4. For each data size, we set different temperatures on the same models.The plot indicates that as the quantity of training data increases, the optimal temperature setting tends to be lower.We find that for small training set sizes (100 and 1000), the FT-LLM is relatively insensitive to temperature changes.Conversely, as the size of the training set increases, lower temperature settings tend to minimize the MSE.</p>
<p>The results also indicate that FT-LLM produces reliable, consistent outputs.As shown in Fig. 4, in most cases, the standard deviation (std) remains at a low level across the majority of temperature settings, indicating that when presented with the same prompts multiple times, FT-LLM will consistently achieve the same level of accuracy.</p>
<p>B. CAN FT-LLM EXPLAIN WHAT IT HAS LEARNED?</p>
<p>Understanding the behavior of conventional data-driven models requires additional methods, such as adding interpretability loss [48] during training.By contrast, the FT-LLM is able to generate text-based responses to prompts, giving FT-LLM the potential to provide textual explanations for its predictions [32], [49], [50].This raises an interesting 51518 VOLUME 13, 2025 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.question: can the FT-LLM describe in words the physical principles underlying the metamaterial response from observation of the geometry-spectrum pairs?To answer this question, we evaluated the FT-LLM's comprehension on electromagnetic metamaterials by asking it what the impact would be of altering various geometrical features.As an experimental control, we also provided the same responses to the un-trained ChatGPT 3.5 model, to ensure that any informative responses provided by FT-LLM were the result of training it on our data.</p>
<p>Appendix F presents the questions posed and the answers generated by FT-LLM.The results indicate that the FT-LLM did not demonstrate a significantly better understanding than the original unmodified GPT model.Rather than learning the physical relationships in our dataset, it may be matching statistical patterns in the dataset.This is a known limitation of many machine learning algorithms, i.e. high prediction accuracy does not necessarily equate to true causal understanding [51], [52].Although LLMs show a competitive level of forward prediction performance as that of a neural network, it appears that fine-tuning on geometryspectra pairs alone is insufficient to provide physical insight.</p>
<p>Furthermore, a stylistic difference was observed.The FT-LLM tends to produce responses comprising a single paragraph, in contrast to the original GPT's piecewise interpretation.This discrepancy in the output style can be attributed to the format of the training data.We have found that all completions are a continuous list of numbers, i.e. a single paragraph.It seems reasonable that this fact influenced the FT-LLM to follow this output style.</p>
<p>C. IS FT-LLM A GOOD DESIGNER?</p>
<p>An important goal of deep learning in metamaterials is to utilize a model for spectral design through the determination of the geometry, which is called the inverse design.In comparison to regression tasks, this is a more challenging problem, which has been termed in the literature as one-to-manyness [37].Given that our FT-LLM model has demonstrated comparable performance to a deep neural network, it is of interest to evaluate its inverse performance.To do this, we gave the FT-LLM the expected spectrum and asked the model to generate a possible design.Appendix E illustrates the example prompt and outputs.</p>
<p>We found that models trained on datasets that exceeded 10,000 samples were prone to producing invalid output.In the majority of instances, despite the imposition of strict constraints on the output format, our model disregarded instructions and provided merely a list of numbers.In some cases, the generated responses contained numerous repetitions and fragmented words.We tried different system prompts to address these issues, such as formatting the output structure, but the FT-LLM kept outputting meaningless completions.This may be attributed to the lack of diversity in our training dataset.As all completions are a list of numbers, this dominance in training data appears to have skewed the learning process [53].However, models fine-tuned on small data sets with strict constraints successfully generate valid geometries.We compared the inverse FT-LLM resimulation error with various inverse models including the neural adjoint [13], tandem [54], [55], and a conventional neural network.The results are shown in Table 2.We find that the FT-LLM trained on a dataset size of 1, 000 achieves poor results, indicating that its designs are not accurate.</p>
<p>V. CONCLUSION</p>
<p>Can large-language models learn the physics of metamaterials?Our empirical study of using ChatGPT to predict the absorptivity spectrum of all-dielectric metamaterial indicates that this is indeed possible.We fine-tuned ChatGPT 3.5 with simulation data consisting of metamaterial geometry and spectra, resulting in a model termed FT-LLM.Most of the predicted spectra from the FT-LLM fine-tuned on the large dataset closely match the results from CST, an electromagnetic solver, generated ground truth, demonstrating the reliability of our approach.We also compared the FT-LLM's performance with traditional ML models, including a deep neural network that has been highly optimized for the given task.The results demonstrated that the FT-LLM achieved a comparable mean absolute relative error (MARE) to the deep neural network, once the training dataset exceeded a size of 1,000.In terms of MSE, the FT-LLM had poor performance for small training dataset sizes.However, its performance increased substantially and faster than other ML models with increasing training dataset size.This capacity to process and analyze large datasets demonstrates that LLMs can serve as surrogate models in the electromagnetics research, exhibiting significant potential for predictive metamaterial physics.</p>
<p>On the other hand, interpretability and inverse design remain challenges.Although the FT-LLM model could generate reasonable answers related to physics, it did not show any better understanding of our particular problem, even after fine-tuning on a large dataset of related data.Additionally, using the FT-LLM for inverse design proved ineffective, as traditional inverse design methods achieved better The shortcomings exhibited by the FT-LLM studied here indicates the need for further research.</p>
<p>The results this study have broader implications for the application of AI in the physical sciences and engineering disciplines.By integrating different types of data, foundational models can provide a more holistic understanding of scientific phenomena, uncovering hidden patterns, and providing physical insights.This adaptability makes LLMs valuable tools in fields such as optics, photonics, and materials science, where they can accelerate progress and faster interdisciplinary research.</p>
<p>Future work: Our work tested two different geometry representation methods, but both of them are specific to the dataset.Inspired by the success of SMILES [56] in chemistry (a technique that uses linear text to describe the structure of chemical species), we propose that the development of a standardized encoding scheme designed for representing the geometry of all metamaterials could accelerate LLMs research in this field.</p>
<p>In addition, we encountered problems with poor interpretability and inverse design capabilities.This may be because our dataset only includes the forward prediction question-answer sets.In this case, creating a more diverse dataset, including related research papers and textbooks, may help address these issues.It may also be that transitioning from supervised fine-tuning to reinforcement learning [57] may help models to gain physics insights.</p>
<p>Another promising direction is building LLM agent systems.LLMs have proven effective in the extraction of knowledge from papers [58], and automating experimental procedures [59], [60], while our work highlights their potential in predicting metamaterial physics.Building an agent system that could autonomously interact with complex datasets, search for information in the paper, and analyze the results is feasible.In metamaterial research, such LLM agents could enhance productivity and innovation.</p>
<p>APPENDIX A MANAGING OUTPUT LENGTH AND CONVERSION ERRORS</p>
<p>One challenge with evaluating our LLMs is that, by design, they do not produce fixed-length sequences, and they can add extra unnecessary characters to their predictions (e.g., commas, white space, punctuation marks).The length of the ground truth spectra Len(S t ) is 50.However, Len(S p ) generated by FT-LLM, may differ from 50.We have found that for training sizes of 10, 000 and 1, 000 FT-LLM provides outputs of length Len(S p ) = 50, 98% and 85% of the time, respectively.For smaller and larger training data sets, the output began to have more occurrences of Len(S p ) = 49 or Len(S p ) = 51, respectively.Our process for dealing with these instances is that we regenerate the output up to 3 times to attempt to achieve L t = L p , otherwise we will only compare S t [0 : L min ] and S p [0 : L min ], where L min = min(Len(S t ), Len(S p )).</p>
<p>Another issue that may arise is the conversion of the text output from the LLM into numerical values.For example, a text output of ''0.0.13'' cannot be converted to a numeric value.In the case that, the output is regenerated and the process is repeated up to 3 times.If all attempts fail, we then skip this data point while evaluating the base-line models and FT-LLM.We have found that our FT-LLM outputs nonnumerical data less than 0.1% of the time.Additionally, even if the numerical transformation is successful, errors can still occur.For example, the completion output ''0 103'' could be transferred to numbers 0 and 13, but it should represent ''0.103,'' which misses a decimal point.Given that our data falls within the range of 0 to 1, any completions that do not comply with this format are regarded as errors.51520 VOLUME 13, 2025 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>APPENDIX B HYPERPAPRAMETER TUNING</p>
<p>To optimize hyperparameters, we used Bayesian optimization via the Scikit-Optimize package [61].We employed the default settings in Scikit-Optimize, conducting 30 Bayesian trials for each traditional model.We used this approach to efficiently explore the parameter space to identify optimized configurations based on the validation MSE [62].For the neural network, we used the same setting in the previous research [13] and only changed the output dimensions.We focus on fine-tuning the following models:</p>
<p>• Random Forest (RF):</p>
<p>-Number of trees: 33 -Minimum samples to split an internal node: 4 -Minimum samples to be at a leaf node: 2 -Maximum depth: 14 • Linear Regression (LR):</p>
<p>-We used the default setting of the Sklearn [63].</p>
<p>APPENDIX C HANDLING DIVISION BY ZERO IN MARE CALCULATION</p>
<p>In the test dataset, one spectrum has zero value data.In the MARE calculation, we need to divide the ground truth, but dividing by zero is invalid.To deal with this, a common strategy is to add a small constant to the denominator [64].</p>
<p>Given that the smallest non-zero value in the dataset is 0.001, we chose to add 0.0001 in our study.This effectively handles the division without significantly affecting the overall results.</p>
<p>APPENDIX D MODEL HARDWARE EFFICIENCY</p>
<p>computation usage is an important aspect while evaluating model performance [65].How efficiently does FT-LLM utilize computational resources?In this study, we evaluate training time and memory usage in various baseline models and the FT-LLM.Table 3 shows the results.Fine-tuning ChatGPT requires significantly more computational time due to its larger model size.As the finetuning process was run on the OpenAI platform, we were limited by platform constraints that were beyond our direct control.Future work focusing on reducing this computational time could include using quantized models which lower the numerical precision of model parameters while preserving high prediction accuracy [66] and exploring more efficient fine-tuning techniques.</p>
<p>In terms of memory, the fine-tuning of ChatGPT was conducted on OpenAI's platform so that we did not need to do the local computation, which is good when the resources are limited.However, the total price for doing all the FT-LLM experiments in the paper on OpenAI's platform was approximately $4,000.This indicates the extremely high computational resources required by LLMs.</p>
<p>APPENDIX E EXAMPLES OF INVERSE DESIGN A. EXAMPLE OUTPUTS</p>
<p>The following demonstrates the outputs of a ChatGPT model fine-tuned with different numbers of training samples.The prompts represent an inverse design task for specifying a metamaterial with a desired spectrum.The completion of the model fine-tuned with 1,000 samples yields a physically implausible design, since the axes of resonators are greater than half of the periodicity.In addition, the outputs of the model fine-tuned with 10,000 samples are invalid.Temperature is set as 0.5.</p>
<p>Input Prompt:</p>
<p>You are an electromagnetic expert.You can design a metamaterial with a given desired spectrum.Your output should fill the [value] in the following: An all-dielectric metasurface under the following specifications:</p>
<ul>
<li>The desired absorptivity values should have a peak value (greater than 0.8) in the 40th values.</li>
</ul>
<p>Completion from model fine-tuned in 1000 training samples:</p>
<p>an all-dielectric metasurface under the following specifications:</p>
<p>Height: 500, Periodicity: 800, Elliptical resonators' dimensions and rotational angles: 1. Semi-major axis: 300, Semi-minor axis: 150, Rotation: 0, Infeasible.No such solution.@@Nosuch design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@No such design@@.@@23@@18@@16@@18@@21@@26@@32@@41@@ 52@@63@@65@@67@@69@@74@@79@@84@@86@ @88@@81@@8.2@@78@@73@@74@@75@@81@@78@@68@@62@@62@@69@@79@@72@@66@@66@</p>
<p>B. EXAMPLE PROMPT</p>
<p>As demonstrated in the Appendix E-A, when the model is trained on a dataset exceeding 10,000 instances, it produces invalid outputs.We tried to use different prompts to address this issue, but the model remained unable to generate the desired output.The prompts used are listed below.</p>
<p>• You are a metamaterial expert.Please design an all-dielectric metasurface suspended in free space whose absorptivity values have a peak value (greater than 0.8) of 400 THZ.It would help if you could give me the common height, periodicity, semi-major and semi-major axis of four elliptical resonators, and the angles of rotation with respect to a vertical axis.</p>
<p>• You are a ChatGPT model fine-tuned on metamaterial simulation data, enabling you to perform inverse design.Your task is to design an all-dielectric metasurface suspended in free space that achieves a peak absorptivity greater than 0.8 at 400 THz.Please provide the following parameters in the order given: [height, periodicity, semimajor axis and semi-major axis of the four elliptical resonators, Rotational angles for each resonator].</p>
<p>• You are a ChatGPT model trained for inverse design.Design an all-dielectric metasurface suspended in free space, achieving a peak absorptivity greater than 0.8 at 400 THz.Important: Your output must only contain the values: Height,periodicity, semi-major and semi-minor axes of four elliptical resonators, rotational angles of four elliptical resonators.Random or irrelevant outputs such as lists of decimals or strings are not acceptable.</p>
<p>APPENDIX F EXAMPLES OF MODEL COMPLETIONS FOR EXPLAINING FEATURE IMPORTANCE</p>
<p>This section demonstrates examples of model completions for explaining feature importance.original LLM refers to the GPT 3.5-1106 without fine-tuning.On the other hand, the fine-tuned version of GPT 3.5 has been trained on a dataset of 10,000 data points, using a detailed description template.The temperature is set to 1.</p>
<p>A. HEIGHTS INFLUENCE</p>
<p>Input Prompt:</p>
<p>You are a physics expert.What is the influence of changing heights on the absorptivity of an all-dielectric metasurface?</p>
<p>Completion of FT-LLM: Changing the height of the dielectric metasurface affects the resonant modes and the overall absorption.As the height increases,the resonant modes shift to longer wavelengths, and the absorption at those wavelengths increases.This is because the increased height allows for more interaction between the incident electromagnetic wave and the metasurface, leading to stronger scattering and absorption.At the same time, the absorption at non-resonant wavelengths may decrease as the height increases, due to reduced interaction with the metasurface.However, the overall influence of changing heights on the absorptivity depends on the specific design and parameters of the metasurface.</p>
<p>Completion of Original LLM:</p>
<p>Changing the height of an all-dielectric metasurface can have a significant influence on its absorptivity.The absorptivity of a metasurface refers to its ability to absorb incident electromagnetic radiation.</p>
<p>By changing the height of the metasurface, the resonant modes of the structure can be tuned, which in turn affects its absorptivity.For example, changing the height can shift the resonant frequency of the metasurface, leading to changes in the absorption properties at specific frequencies.Additionally, changing the height can also affect the coupling between the metasurface and the incident electromagnetic radiation, which can further influence the overall absorptivity of the structure.</p>
<p>In general, tuning the height of an all-dielectric metasurface can provide a degree of control over its absorptivity, allowing for custom design and optimization for specific applications such as solar cells, sensors, and stealth technology.</p>
<p>B. PERIODICITY INFLUENCE</p>
<p>Input Prompt:</p>
<p>You are a physics expert.What is the influence of changing periodicity on the absorptivity of an all-dielectric metasurface?51522 VOLUME 13, 2025 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Completion of FT-LLM:</p>
<p>Varying the periodicity of an all-dielectric metasurface can significantly influence its absorptivity.As you change the periodicity, the effective refractive index can be altered, leading to different Mie resonances and interference effects.This can result in modified absorption spectra and different dispersion characteristics.In particular, as the periodicity is modified, the coupling between the metasurface and the incident radiation can change, affecting the intensity of the fields around and within the metasurface, consequently influencing the absorption.Additionally, the characteristic Mie resonances in the absorptivity can change with the periodicity due to the modified phase matching conditions, leading to different absorption features.Therefore, it's important to carefully consider the impact of periodicity on the design of all-dielectric metasurfaces in order to achieve the desired optical properties.</p>
<p>Completion of Original LLM:</p>
<p>The absorptivity of an all-dielectric metasurface can be influenced by changing the periodicity of the structure in several ways:</p>
<ol>
<li>
<p>Bandwidth of absorption: Changing the periodicity can affect the resonance of the metasurface and thereby change the range of wavelengths over which absorption occurs.By adjusting the periodicity, one can tune the absorptivity peak to different wavelengths, allowing for control over the bandwidth of absorption.</p>
</li>
<li>
<p>Absorption efficiency: The efficiency of absorption can also be affected by changing the periodicity.Optimizing the periodicity can enhance the absorption efficiency by matching the resonance condition to the incident light wavelength more closely.</p>
</li>
<li>
<p>Polarization sensitivity: The absorptivity of a metasurface can also be influenced by the polarization of the incident light.Changing the periodicity can lead to different responses for different polarizations, allowing for polarization-selective absorption.</p>
</li>
<li>
<p>Angular dependence: The absorptivity of a metasurface can exhibit angular dependency, meaning that the absorption properties change with the angle of incidence of the light.By tuning the periodicity, one can control the angular dependence of absorption.</p>
</li>
</ol>
<p>Overall, changing the periodicity of an all-dielectric metasurface provides a versatile way to tailor its absorptivity properties for specific applications, allowing for optimization of absorption efficiency, bandwidth, and other characteristics.</p>
<p>C. ELLIPTICAL RESONATORS INFLUENCE</p>
<p>Input Prompt:</p>
<p>You are a physics expert.What is the influence of changing the angle of the elliptical resonators on the absorptivity of an all-dielectric metasurface?</p>
<p>Completion of FT-LLM By changing the angle of elliptical resonators in an alldielectric metasurface, you change their orientation w.r.t incident electric field.This, in turn, will change the effective refractive index and the support of different Mie-type electric resonances.This will influence the energy distribution and the total amount of absorbed energy at the different resonant modes, and thus effectively change the absorptivity of the metasurface.You can expect changes in the resonant amplitudes and redshift or blueshift of resonance due to the rotation, all contributing to the alteration of the absorptive properties.It is similar to rotate a polarizer, the absorptive maxima dip and flip depending on the geometry and orientation of the polarization w.r.t the axes of the film.For example, the periodic array of elliptical resonators will have periodicity, orientation, and asymmetry, thus the absorption is dictated by the resonant response which in turn is affected by the orientation effects.The sensitive dependency of the rotation on the response means careful consideration of the material orthorhombic nature will have to be made for precise and accurate characterization.The resulting informatics will tell engineers and researchers how much the geometry must be rotated to extract the best performance or to keep the performance consistent to create an array of the particular geometry with lessening reliance on the variation at the different polarization angles.For different resonant orientations the metasurfaces whose symmetries will dictate absorption pattern into forward and backward scattering, thus understanding the orientations can aid in possible secondary uses or in reducing backscattering losses or optimizing forward scattering collimating emission.</p>
<p>Completion of Original LLM:</p>
<p>Changing the angle of elliptical resonators in an all-dielectric metasurface can have a significant impact on the absorptivity of the metasurface.The absorptivity of a metasurface is determined by its ability to efficiently absorb incident electromagnetic energy.This ability is closely related to the resonant behavior of the individual resonators within the metasurface.</p>
<p>Elliptical resonators exhibit different electromagnetic responses compared to circular or rectangular resonators, due to their asymmetric geometry.By changing the angle of the elliptical resonators, you can tune the resonant frequency of the individual resonators within the metasurface.This tuning can lead to enhanced absorption properties at specific frequencies or polarization states.</p>
<p>In general, adjusting the angle of elliptical resonators can affect the overall response of the metasurface by influencing the coupling between adjacent resonators, altering the effective refractive index of the metasurface, and modifying the scattering properties of the structure.These changes can result in a tailored absorptivity spectrum for specific applications such as sensing, energy harvesting, or stealth technology.</p>
<p>Therefore, careful design and optimization of the geometry and orientation of elliptical resonators in an all-dielectric metasurface can provide control over the absorptivity of the metasurface and enable customization of its electromagnetic response for desired functionalities.</p>
<p>D. RADIUS INFLUENCE</p>
<p>Input Prompt:</p>
<p>You are a physics expert.What is the influence of changing the radius of the elliptical resonators on the absorptivity of an all-dielectric metasurface?</p>
<p>Completion of FT-LLM:</p>
<p>As the radius of the resonator is changed, the absorptivity of the metasurface varies accordingly.For smaller radii, the absorptivity can increase significantly in the shorter wavelengths due to the presence of electric dipolar resonances.However, at larger radii, the electric dipolar resonance shifts to longer wavelengths and diminishes, while magnetic dipolar resonances arise leading to high absorption in the longer wavelengths.Overall, the absorptivity of the metasurface can be tuned by changing the radius, making it possible to design devices with specific absorption spectra.</p>
<p>Completion of Original LLM:</p>
<p>Changing the radius of elliptical resonators in an all-dielectric metasurface can have a significant impact on the absorptivity of the metasurface.The absorptivity of a metasurface is determined by its ability to effectively absorb incident electromagnetic radiation.</p>
<p>In the case of elliptical resonators, the resonant behavior is highly dependent on the size and shape of the resonators.By changing the radius of the elliptical resonators, the resonance frequency of the resonators can be tuned.This allows for control over the absorption properties of the metasurface.</p>
<p>Increasing the radius of the elliptical resonators can lead to a redshift in the resonance frequency, resulting in enhanced absorption at longer wavelengths.Conversely, decreasing the radius can lead to a blueshift in the resonance frequency, enhancing absorption at shorter wavelengths.</p>
<p>Therefore, adjusting the radius of the elliptical resonators in an all-dielectric metasurface can be a powerful way to tailor its absorptivity and optimize its performance for specific applications such as solar energy harvesting, thermal emission control, or sensing.</p>
<p>APPENDIX G</p>
<p>The geometry inputs are normalized to a range of [−1, 1], promoting convergence in the NN training process.The absorptivity spectra are already within the range of [0, 1], and therefore require no additional preprocessing.Details regarding hyperparameter tuning are shown in Appendix B.</p>
<p>FIGURE 2 .
2
FIGURE 2. Evaluations of the models for predicting metamaterial spectra, given a geometry, as a function of dataset size.(a) MARE and (b) MSE trends for baseline models and the FT-LLM as dataset size increases.All results of baseline models presented are the average of three models.Error bars indicate the standard deviation of the three trails.However, due to computational resource limitations, we only conducted single trial for the GPT model at large dataset sizes (10,000, 20,000, and 40,000 samples).While this may introduce some variances in the results, the observed trends are consistent with expectations.A temperature of 0.5 was used for the GPT model.</p>
<p>FIGURE 4 .
4
FIGURE 4. MSE trends for the model fine-tuned on different numbers of training samples as the temperature increases.The MSE is in log 10 scale.</p>
<p>Height: [value] -Periodicity: [value] -Elliptical resonators' dimensions and rotational angles: -1.Semi-major axis: [value], Semi-minor axis: [value], Rotation: [value] 2. Semi-major axis: [value], Semi-minor axis: [value], Rotation: [value] -3.Semi-major axis: [value], Semi-minor axis: [value], Rotation: [value] -4.Semi-major axis: [value], Semi-minor axis: [value], Rotation: [value]</p>
<p>2 .
2
Semi-major axis: 200, Semi-minor axis: 100, Rotation: 0, 3. Semi-major axis: 150, Semi-minor axis: 75, Rotation: 0, 4. Semi-major axis: 150, Semi-minor axis: 75, Rotation: 0 Completion 1 from model fine-tuned in 10,000 training samples: [0.006 0.008 0.011 0.014 0.017 0.021 0.026 0.032 0.04 0.059 0.12 0.141 0.092 0.1 0.126 0.154 0.197 0.252 0.264 0.233 0.247 0.294 0.354 0.409 0.466 0.499 0.502 0.484 0.484 0.513 0.556 0.647 0.621 0.553 0.519 0.515 0.535 0.563 0.585 0.595 0.587 0.574 0.563 0.551 0.536 0.521 0.509 0.5 0.497 0.499] Completion 2 from model fine-tuned in 10,000 training samples:</p>
<p>TABLE 1 .
1
Example Prompts.These prompts are used to convert the tabular data into text sentences.</p>
<p>TABLE 2 .
2
Resimulation evaluations of inverse design algorithms with 1,000 training data.For the simulation, we use a fully connected network with 2 × 10 −3 test MSE as a surrogate model.</p>
<p>TABLE 3 .
3
Training time and memory usage for each model on a dataset of 1,000 samples.Memory usage for FT-LLM is not available.</p>
<p>The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
   VOLUME 13, 2025    Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
VOLUME 13, 2025 <br />
   VOLUME 13, 2025    Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
CODE AND DATA SHARINGThe datasets and code are available in the repository, https://github.com/DukeMLM/LLM_ADM.This work was supported by the Department of Energy through U.S. Department of Energy (DOE) under Grant DESC0014372.Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.
Deep learning for the design of photonic structures. W Ma, Z Liu, Z A Kudyshev, A Boltasseva, W Cai, Y Liu, 10.1038/s41566-020-0685-yNature Photon. 152Oct. 2020</p>
<p>Deep neural networks for the evaluation and design of photonic devices. J Jiang, M Chen, J A Fan, 10.1038/s41578-020-00260-1Nature Rev. Mater. 68Dec. 2020</p>
<p>Deep learning the electromagnetic properties of metamaterials-A comprehensive review. O Khatib, S Ren, J Malof, W J Padilla, 10.1002/adfm.202101748Adv. Funct. Mater. 3131May 2021. 2101748</p>
<p>Intelligent nanophotonics: Merging photonics and artificial intelligence at the nanoscale. K Yao, R Unni, Y Zheng, 10.1515/nanoph-2018-0183Nanophotonics. 83Jan. 2019</p>
<p>Deep learning enabled inverse design in nanophotonics. S So, T Badloe, J Noh, J Bravo-Abad, J Rho, 10.1515/nanoph-2019-0474Nanophotonics. 95Feb. 2020</p>
<p>Deep learning in nano-photonics: Inverse design and beyond. P R Wiecha, A Arbouet, C Girard, O L Muskens, 10.1364/prj.415960Photon. Res. 95182Apr. 2021</p>
<p>Nanophotonic particle simulation and inverse design using artificial neural networks. J Peurifoy, Y Shen, L Jing, Y Yang, F Cano-Renteria, B G Delacy, J D Joannopoulos, M Tegmark, M Soljačić, 10.1126/sciadv.aar4206Sci. Adv. 46Jun. 2018</p>
<p>Deep-learning-enabled on-demand design of chiral metamaterials. W Ma, F Cheng, Y Liu, 10.1021/acsnano.8b03569ACS Nano. 126Jun. 2018</p>
<p>Global optimization of dielectric metasurfaces using a physics-driven neural network. J Jiang, J A Fan, 10.1021/acs.nanolett.9b01857Nano Lett. 198Jul. 2019</p>
<p>Deep learning for accelerated all-dielectric metasurface design. C C Nadell, B Huang, J M Malof, W J Padilla, 10.1364/oe.27.027523Sep. 20192727523</p>
<p>Generative model for the inverse design of metasurfaces. Z Liu, D Zhu, S P Rodrigues, K.-T Lee, W Cai, 10.1021/acs.nanolett.8b03171Nano Lett. 1810Sep. 2018</p>
<p>Freeform diffractive metagrating design based on generative adversarial networks. J Jiang, D Sell, S Hoyer, J Hickey, J Yang, J A Fan, 10.1021/acsnano.9b02371ACS Nano. 138Jul. 2019</p>
<p>Neural-adjoint method for the inverse design of all-dielectric metasurfaces. Y Deng, S Ren, K Fan, J M Malof, W J Padilla, 10.1364/oe.419138Feb. 2021297526</p>
<p>Inverse deep learning methods and benchmarks for artificial electromagnetic material design. S Ren, A Mahendra, O Khatib, Y Deng, W J Padilla, J M Malof, 10.1039/d1nr08346eNanoscale. 14102022</p>
<p>Y Lecun, Y Bengio, G Hinton, 10.1038/nature14539Deep learning. May 2015521</p>
<p>Understanding deep learning (still) requires rethinking generalization. C Zhang, S Bengio, M Hardt, B Recht, O Vinyals, 10.1145/3446776Commun. ACM. 643Feb. 2021</p>
<p>Interpretable machine learning: Fundamental principles and 10 grand challenges. C Rudin, C Chen, Z Chen, H Huang, L Semenova, C Zhong, 10.1214/21-ss133Statist. Surv.. 16Jan. 2022</p>
<p>Efficient design of a dielectric metasurface with transfer learning and genetic algorithm. D Xu, Y Luo, J Luo, M Pu, Y Zhang, Y Ha, X Luo, 10.1364/ome.427426Opt. Mater. Exp. 1171852Jun. 2021</p>
<p>Phase-to-pattern inverse design paradigm for fast realization of functional metasurfaces via transfer learning. R Zhu, T Qiu, J Wang, S Sui, C Hao, T Liu, Y Li, M Feng, A Zhang, C.-W Qiu, S Qu, 10.1038/s41467-021-23087-yNature Commun. 1212974May 2021</p>
<p>Heterogeneous transfer-learning-enabled diverse metasurface design. J Zhang, C Qian, Z Fan, J Chen, E Li, J Jin, H Chen, 10.1002/adom.202200748Adv. Opt. Mater. 1017Jun. 2022. 2200748</p>
<p>Transfer-learning-assisted inverse metasurface design for 30% data savings. Z Fan, C Qian, Y Jia, M Chen, J Zhang, X Cui, E Li, B Zheng, T Cai, H Chen, Phys. Rev. Appl. 182Aug. 2022. 024022</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, J. Comput. Phys. 378Nov. 2018</p>
<p>Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Y Chen, L Lu, G E Karniadakis, L D Negro, Opt. Exp. 288Mar. 2020</p>
<p>High speed simulation and freeform optimization of nanophotonic devices with physics-augmented deep learning. M Chen, ACS PhotonR Lupoiu, ACS PhotonC Mao, ACS PhotonD.-H Huang, ACS PhotonJ Jiang, ACS PhotonP Lalanne, ACS PhotonJ A Fan, ACS Photon10.1021/acsphotonics.2c00876Aug. 20229</p>
<p>Learning the physics of all-dielectric metamaterials with deep Lorentz neural networks. O Khatib, S Ren, J Malof, W J Padilla, 10.1002/adom.202200097Adv. Opt. Mater. 10132025Jul. 2022, Art. no. 2200097</p>
<p>Recent advances in metasurface design and quantum optics applications with machine learning. W Ji, J Chang, H.-X Xu, J R Gao, S Gröblacher, H P Urbach, A J L Adam, 10.1038/s41377-023-01218-yphysics-informed neural networks, and topology optimization methods. Light, Sci. Appl.. Jul. 202312169</p>
<p>Neural operator-based surrogate solver for free-form electromagnetic inverse design. Y Augenstein, T Repän, C Rockstuhl, 10.1021/acsphotonics.3c00156ACS Photon. 105Mar. 2023</p>
<p>Language models are few-shot learners,'' in Proc. T B Brown, Adv. Neural Inf. Process. Syst., H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. LinJan. 2020Curran Associates</p>
<p>Attention is all you need,'' in Proc. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin ; I. Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S , Adv. Neural Inf. Process. Syst. Vishwanathan, and R. Garnett30Jun. 2017</p>
<p>''Llama 2: Open foundation and fine-tuned chat models. H Touvron, arXiv:2307.092882023</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, W Ye, Y Zhang, Y Chang, P S Yu, Q Yang, X Xie, ACM Trans. Intell. Syst. Technol. 153Jan. 2024</p>
<p>LIFT: Language-interfaced fine-tuning for non-language machine learning tasks,'' in Proc. T Dinh, Y Zeng, R Zhang, Z Lin, M Gira, S Rajput, J.-Y Sohn, D Papailiopoulos, K Lee, Adv. Neural Inf. Process. Syst., S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. OhJan. 2022Curran Associates</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nature Mach. Intell. 62Feb. 2024</p>
<p>14 examples of how LLMs can transform materials science and chemistry: A reflection on a large language model hackathon. K M Jablonka, Digit. Discov. 25Jan. 2023</p>
<p>OptoGPT: A foundation model for inverse design in optical multilayer thin film structures. T Ma, H Wang, L Jay Guo, arXiv:2304.102942023</p>
<p>MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems. M J Buehler, J. Mech. Phys. Solids. 181Oct. 2023. 105454</p>
<p>Benchmarking data-driven surrogate simulators for artificial electromagnetic materials. Y Deng, J Dong, S Ren, O Khatib, M Soltani, V Tarokh, W Padilla, J Malof, Proc. 34th Conf. Neural Inf. Process. Syst. Datasets Benchmarks Track. 34th Conf. Neural Inf. ess. Syst. Datasets Benchmarks Track2021</p>
<p>Prediction network of metamaterial with split ring resonator based on deep learning. Z Hou, T Tang, J Shen, C Li, F Li, Nanosc. Res. Lett. 151Apr. 2020</p>
<p>Metamaterial design with nested-CNN and prediction improvement with imputation. E Kiymik, E Erçelebi, Appl. Sci. 1273436Mar. 2022</p>
<p>Dispersion relation prediction and structure inverse design of elastic metamaterials via deep learning. W Jiang, Y Zhu, G Yin, H Lu, L Xie, M Yin, 10.1016/j.mtphys.2022.100616Mater. Today Phys.. 22Jan. 2022. 100616</p>
<p>Deep learning-based design of broadband GHz complex and random metasurfaces,'' APL Photon. T Zhang, C Y Kee, Y S Ang, L K Ang, 10.1063/5.0061571Oct. 20216Art</p>
<p>Deep reinforcement learning for the design of mechanical metamaterials with tunable deformation and hysteretic characteristics. N K Brown, A Deshpande, A Garland, S A Pradeep, G Fadel, S Pilla, G Li, 10.1016/j.matdes.2023.112428Mater. Des. 235Nov. 2023. 112428</p>
<p>Graph-based metamaterials: Deep learning of structure-property relations. P P Meyer, C Bonatti, T Tancogne-Dejean, D Mohr, Mater. Des. 223Sep. 2022. 111175</p>
<p>Visual instruction tuning,'' in Proc. H Liu, C Li, Q Wu, Y J Lee, Adv. Neural Inf. Process. Syst. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine36Jan. 2023Curran Associates</p>
<p>. CST Microw. Studio, Dassault Systèmes. 2021CST Studio Suite</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, T Liu, arXiv:2311.052322023</p>
<p>Unleashing the potential of prompt engineering in large language models: A comprehensive review. B Chen, Z Zhang, N Langrené, S Zhu, arXiv:2310.147352023</p>
<p>Beyond sparsity: Tree regularization of deep models for interpretability. M Wu, M Hughes, S Parbhoo, M Zazzi, V Röth, F Doshi-Velez, 10.1609/aaai.v32i1.11501Proc. AAAI Conf. AAAI ConfApr. 201832</p>
<p>Chain-of-thought prompting elicits reasoning in large language models,'' in Proc. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q V Le, D Zhou, Adv. Neural Inf. Process. Syst. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh35Jan. 2022Curran Associates</p>
<p>Large language models are zero-shot reasoners,'' in Proc. T Kojima, S Gu, M Reid, Y Matsuo, Y Iwasawa, Adv. Neural Inf. Process. Syst., S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. OhJan. 2022Curran Associates</p>
<p>LLMs' classification performance is overclaimed. H Xu, R Lou, J Du, V Mahzoon, E Talebianaraki, Z Zhou, E Garrison, S Vucetic, W Yin, arXiv:2406.162032024</p>
<p>An analysis of explainability methods for convolutional neural networks. L V Haar, T Elvira, O Ochoa, 10.1016/j.engappai.2022.105606Eng. Appl. Artif. Intell. 117Jan. 2023. 105606</p>
<p>An empirical study of catastrophic forgetting in large language models during continual fine-tuning. Y Luo, Z Yang, F Meng, Y Li, J Zhou, Y Zhang, arXiv:2308.087472023</p>
<p>Training deep neural networks for the inverse design of nanophotonic structures. D Liu, ACS PhotonY Tan, ACS PhotonE Khoram, ACS PhotonZ Yu, ACS Photon10.1021/acsphotonics.7b01377Feb. 20185</p>
<p>Forward models: Supervised learning with a distal teacher. M I Jordan, D E Rumelhart, Backpropagation. East Sussex. 2013Psychology Press</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J. Chem. Inf. Comput. Sci. 281Feb. 1988</p>
<p>DeepSeek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning. D Guo, arXiv:2501.129482025</p>
<p>Retrieval of synthesis parameters of polymer nanocomposites using LLMs. D Circi, G Khalighinejad, S Badhwar, B Dhingra, L Brinson, Proc. NeurIPS, 2023. NeurIPS, 2023</p>
<p>Language-controllable programmable metasurface empowered by large language models. S Hu, J Xu, M Li, T J Cui, L Li, 10.1515/nanoph-2023-0646Nanophotonics. 1312Jan. 2024</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 6247992Dec. 2023</p>
<p>Scikit-optimize/scikit-optimize: V0.5.2. T Head, v0.5.22018Paris, FranceProbabilistic Mach. Learn. GroupTech. Rep.</p>
<p>Automatic tuning of hyperparameters using Bayesian optimization. A H Victoria, G Maragatham, 10.1007/s12530-020-09345-2Evolving Syst.. 121May 2020</p>
<p>API design for machine learning software: Experiences from the scikit-learn project. L Buitinck, G Louppe, M Blondel, F Pedregosa, A Mueller, O Grisel, V Niculae, P Prettenhofer, A Gramfort, J Grobler, R Layton, J Vanderplas, A Joly, B Holt, G Varoquaux, Proc. ECML PKDD Workshop. ECML PKDD WorkshopSep. 2013</p>
<p>A new metric of absolute percentage error for intermittent demand forecasts. S Kim, H Kim, 10.1016/j.ijforecast.2015.12.003Int. J. Forecasting. 323Jul. 2016</p>
<p>A deep neural network modeling methodology for efficient EMC assessment of shielding enclosures using MECA-generated RCS training data. R Choupanzadeh, A Zadehgol, IEEE Trans. Electromagn. Compat. 656Dec. 2023</p>
<p>where his research focuses on the development of advanced machine learning, deep learning, and computer vision to solve problems in fields, such as remote sensing, materials science, energy, and climate. WILLIE J. PADILLA (Fellow, IEEE) received the B.S. degree in physics from San Diego State University, USA, and the. Y Zhao, C Lin, K Zhu, Z Ye, L Chen, S Zheng, L Ceze, A Krishnamurthy, T Chen, B Kasikci, 2021, and the M.S. degree in electrical and computer engineer. M S , C Phil, Ph D , Chongqing, China; Durham, NC, USA; Rochester, NY, USA; Durham, NC, USA; Louisville, KY, USA; California San Diego, USA; Durham, NC, USAJan. 2023. 2020. 2015. 2018 and 20192024from Chongqing University ; ing from Duke University ; Duke University ; ECE) from the University of Louisville ; Electrical Engineering and Computer Science, University of Missouri ; Paul Wang Distinguished Professor of Duke University, has more than 240 peer-reviewed journal articles, one book, three book chapters, and 12 issued patents. His research interests include artificial electromagnetic materials, including metamaterials with a focus on machine learning, computational imaging, spectroscopy, and energy. He is a fellow of the American Physical Society, the Kavli Frontiers of Science, and the Optical Society of America, and an Awardee of the Young Investigator Award from the Office of Naval Research and the Presidential Early Career Award for Scientists and Engineers</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8363 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8363</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8363</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-f5df0667365764a970fc6abfa0a68b7d1d0ae413</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f5df0667365764a970fc6abfa0a68b7d1d0ae413" target="_blank">Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a framework called Patchscopes and shows how it can be used to answer a wide range of questions about an LLM's computation and opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.</p>
                <p><strong>Paper Abstract:</strong> Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8363.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8363.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hanna_et_al_2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited study that investigates GPT-2's mathematical ability for the 'greater-than' comparison task; referenced in this paper's related work but no experimental details are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>greater-than comparison (inequality comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>This paper only cites Hanna et al. (2023) as prior work on interpreting mathematical abilities of GPT-2; no internal evidence or results from that study are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8363.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8363.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stolfo_et_al_2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that applies causal mediation analysis to interpret arithmetic reasoning in language models; mentioned in related work but the current paper does not reproduce or detail its findings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>arithmetic reasoning (general)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Referenced as using causal mediation analysis to study arithmetic reasoning in LMs; this paper does not provide the experimental outcomes or quantitative evidence from Stolfo et al.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>The expressive power of transformers with chain of thought <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8363",
    "paper_id": "paper-f5df0667365764a970fc6abfa0a68b7d1d0ae413",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Hanna_et_al_2023",
            "name_full": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "brief_description": "Cited study that investigates GPT-2's mathematical ability for the 'greater-than' comparison task; referenced in this paper's related work but no experimental details are provided here.",
            "citation_title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "mention_or_use": "mention",
            "model_name": "GPT-2",
            "model_description": null,
            "arithmetic_task_type": "greater-than comparison (inequality comparison)",
            "mechanism_or_representation": null,
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "This paper only cites Hanna et al. (2023) as prior work on interpreting mathematical abilities of GPT-2; no internal evidence or results from that study are reported here.",
            "counterexamples_or_challenges": null,
            "uuid": "e8363.0",
            "source_info": {
                "paper_title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Stolfo_et_al_2023",
            "name_full": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "brief_description": "Cited work that applies causal mediation analysis to interpret arithmetic reasoning in language models; mentioned in related work but the current paper does not reproduce or detail its findings.",
            "citation_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "arithmetic_task_type": "arithmetic reasoning (general)",
            "mechanism_or_representation": null,
            "probing_or_intervention_method": "causal mediation analysis",
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Referenced as using causal mediation analysis to study arithmetic reasoning in LMs; this paper does not provide the experimental outcomes or quantitative evidence from Stolfo et al.",
            "counterexamples_or_challenges": null,
            "uuid": "e8363.1",
            "source_info": {
                "paper_title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "rating": 2
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2
        },
        {
            "paper_title": "The expressive power of transformers with chain of thought",
            "rating": 1
        }
    ],
    "cost": 0.0125005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models</h1>
<p>Asma Ghandeharioun<em> ${ }^{</em> 1}$ Avi Caciularu ${ }^{* 1}$ Adam Pearce ${ }^{1}$ Lucas Dixon ${ }^{1}$ Mor Geva ${ }^{12}$</p>
<h4>Abstract</h4>
<p>Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction ${ }^{1}$.</p>
<h2>1. Introduction</h2>
<p>The question of what information is captured in the hidden representations of large language models (LLMs) is of key importance in control and understanding of modern generative AI, and has drawn substantial attention recently (Casper et al., 2022; Madsen et al., 2022; Patel \&amp; Pavlick, 2021; Nanda et al., 2023). To tackle this question, prior work has introduced a diverse array of interpretability methods, which largely rely on three prominent approaches: training linear</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Illustration of our framework, showing a Patchscope for decoding the hidden representation of "CEO" in the source prompt (left). We use a target prompt (right) comprised of fewshot demonstrations of string repetitions to encourage the LLM to explain its internal representation. Step 1: Run the forward computation on the source prompt in the source model. Step 2: Optionally transform the hidden state at the source layer. Step 3: Run the forward computation on the target prompt up to the target layer in the target model. Step 4: Patch the target representation of the token "?" at the target layer with the transformed representation (from step 2), then continue the forward computation from that layer onward. The modularity of Patchscopes allows designing a variety of methods by configuring the target prompt, model and transformation.
classifiers, called probes, on top of hidden representations (Belinkov \&amp; Glass, 2019; Belinkov, 2022; Alain \&amp; Bengio, 2017), projecting representations to the model's vocabulary space (nostalgebraist, 2020; Din et al., 2023; Belrose et al., 2023), and intervening on the computation to identify if a representation is critical for certain predictions (Meng et al., 2022a; Wallat et al., 2020; Wang et al., 2022; Conmy et al., 2023; Geva et al., 2023).</p>
<p>Despite the wide success of these methods, they each exhibit practical shortcomings. First, probing relies on supervised training for pre-defined classes, which is hard to scale when there is a large number of classes or when all the categories are not known a priori. Second, the accuracy of vocabulary</p>
<p>Table 1. Many prior inspection methods with various objectives can be viewed as Patchscopes. The rows highlighted in green show Patchscope configurations that overcome several limitations of prior methods through more expressive inspection that is training-data free and is more robust across layers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Inspection Objective</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Expressive</th>
<th style="text-align: center;">Training Data Free</th>
<th style="text-align: center;">Robust Across Layers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Inspecting <br> Output <br> Distribution</td>
<td style="text-align: center;">Fine-Shot Token Identity Patchscope (§4.1)</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Logit Lens (nostalgebraist, 2020),</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{R}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Embedding Space Analysis (Dar et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tuned Lens (Belrose et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Future Lens (Pal et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Feature <br> Extraction</td>
<td style="text-align: center;">Zero-Shot Feat. Ext. Patchscope (§4.2)</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LRf. Attribute Lens (Hernandez et al., 2023b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For linear relation approx.</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Probing (e.g., Belinkov \&amp; Glass, 2019; Belinkov, 2022; Alain \&amp; Bengio, 2017; Wang et al., 2023)</td>
<td style="text-align: center;">$\mathbf{R}$</td>
<td style="text-align: center;">For training probe</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entity Description Patchscope (§4.3)</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">X-Model Entity Data. Patchscope (§4.4)</td>
<td style="text-align: center;">$\checkmark \checkmark \checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Causal Tracing (Meng et al., 2022a)</td>
<td style="text-align: center;">$\mathbf{R}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Attention Knockout (Wang et al., 2022; Conney et al., 2023; Gray et al., 2021)</td>
<td style="text-align: center;">$\mathbf{R}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Inspection <br> Application</td>
<td style="text-align: center;">Early Exiting, e.g., Linear Shortcuts (Din et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Caption Generation, e.g., Linear Mapping (Metullo et al., 2022)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>projections substantially decreases in early layers and the outputs are often hard to interpret. Last, all the above methods are not as expressive as one might like: they provide class probabilities or most likely tokens, as opposed to a high-quality explanation in natural language.</p>
<p>In this work, we argue that the advanced capabilities of LLMs in generating human-like text can be leveraged for "translating" the information in their representations for humans. We introduce a modular framework, called Patchscopes (§3), that can be configured to query various kinds of information from LLM representations. Patchscopes decode specific information from a representation within an LLM by "patching" it into the inference pass on a different prompt that has been designed to encourage the extraction of that information. ${ }^{2}$ Such configuration (a Patchscope) can be viewed as an inspection tool geared towards a particular objective, as illustrated in Fig. 1.</p>
<p>We show that many existing methods, including those that rely on vocabulary projections and computation interventions, can be cast as Patchscopes. Moreover, new configurations of our framework introduce more effective tools in addressing the same questions, while mitigating several limitations of prior approaches. Also, Patchscopes enables addressing underexplored questions, such as finegrained analysis of the input contextualization process and the extent to which a more expressive model can be used to inspect hidden representations of a smaller model.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We conduct a series of experiments to evaluate the benefits and opportunities introduced by Patchscopes, focusing on auto-regressive LLMs. First, we consider the problem of estimating the model's next-token prediction from its intermediate representations (see §4.1). Across multiple LLMs, we show that using a few-shot token identity prompt, a prompt in the form of "tok $1 \rightarrow$ tok $1_{1} ;$ tok $2_{2} \rightarrow$ $\operatorname{tok}<em k="k">{2} ; \ldots ;$ tok $</em>$ refers to a random token, leads to substantial gains over vocabulary projection methods. Next, we evaluate how well Patchscopes can decode specific attributes of an entity from its LLM representations, when these are detached from the original context (see §4.2). We observe that, despite using no training data, Patchscopes significantly outperforms probing in six out of twelve commonsense and factual reasoning tasks, and works comparably well in all but one of the remaining six.}$ " where tok $_{i</p>
<p>Beyond output estimation and attribute decoding, Patchscopes can address questions that are hard to answer with existing methods. In $\S 4.3$, we apply Patchscopes to study how LLMs contextualize input entity names in early layers, where vocabulary projections mostly fail and other methods, at best, provide only a binary signal of whether the entity has been resolved (Youssef et al., 2023; Tenney et al., 2019). With a new Patchscope, we are able to verbalize the gradual entity resolution process. For example, we show that, as the model processes the final token of "Alexander the Great" throughout the layers, it reflects different entities starting from "Great Britain", to "the Great Depression", to finally resolving "Alexander the Great". Then, in $\S 4.4$ we show how one can further improve Patchscope expressivity by using a stronger target model, e.g., Vicuna 13B instead of Vicuna 7B.</p>
<p>Lastly, we showcase the utility of Patchscopes for fixing latent multi-hop reasoning errors, particularly when the model is capable of conducting each reasoning step correctly, but fails when they need to be composed incontext (§5). Building on top of the data provided by Hernandez et al. (2023b), we introduce a more complex task that requires two steps of factual reasoning. Patchscope achieves $50 \%$ accuracy on this task, outperforming chain-ofthought (Wei et al., 2022) ( $35.71 \%$ ) and vanilla generations $(19.57 \%)$.</p>
<p>To conclude, our work makes the following contributions: We propose Patchscopes, a general modular framework for decoding information from the hidden representations in LLMs. We show that prominent interpretability methods can be viewed as instances of Patchscopes, and new configurations result in more expressive, robust across layers, and training-data free alternatives that mitigate their shortcomings. In addition, novel configurations introduce unexplored possibilities of stronger inspection techniques, as well as practical benefits, such as correcting multi-hop</p>
<p>reasoning errors.</p>
<h2>2. Related Work</h2>
<p>Activation patching is a causal intervention, commonly used as a tool for studying if certain activations play a key role in a model's computation (Geiger et al., 2021; Vig et al., 2020). Patching has been used largely for localizing specific information to specific layers and token positions (GoldowskyDill et al., 2023; Meng et al., 2022a;b; Stolfo et al., 2023; Merullo et al., 2023), and for finding information propagation paths in the computation (Wang et al., 2022; Geva et al., 2023; Hendel et al., 2023; Hanna et al., 2023; Lieberum et al., 2023). Prior works have also used specific forms of cross-model patching called stitching, in non-transformer architectures, mostly to analyze representational similarity (e.g., Bansal et al., 2021; Csiszárik et al., 2021; Lenc \&amp; Vedaldi, 2015). Despite certain limitations (Hase et al., 2023; Zhang \&amp; Nanda, 2023), patching remains a principal tool for mechanistic interpretability (Conmy et al., 2023).</p>
<p>Given promising results from emerging interpretability efforts that employ LLMs to generate human-like text for inspection (e.g., Mousi et al., 2023; Slobodkin et al., 2023; Bills et al., 2023), we argue that using patching only for localization purposes is myopic, and propose to use it for "translating" LLM representations into natural language. Very recently, patching has been used to study new problem setups (e.g., Pal et al., 2023; Hernandez et al., 2023b), all of which can be seen as different configurations of our proposed framework (see §3.2).</p>
<p>Among the growing research efforts in inspecting hidden representations of neural networks, probing classifiers are perhaps the most common (e.g., Alain \&amp; Bengio, 2017; Belinkov \&amp; Glass, 2019; Belinkov, 2022; Wang et al., 2023), and methods using projections into the vocabulary space or their extensions to other domains are another key category (e.g., Merullo et al., 2023; Geva et al., 2022b; nostalgebraist, 2020; Belrose et al., 2023; Dar et al., 2023; Din et al., 2023; Langedijk et al., 2023; Vilas et al., 2023). While various other latent inspection methods exist (e.g., Zhou et al., 2018; Strobelt et al., 2017; Ghandeharioun et al., 2021; Kim et al., 2018), the above are the most relevant to this work.</p>
<h2>3. Patchscopes</h2>
<p>In this section, we introduce Patchscopes and show how it extends prior interpretability methods with new capabilities. While not limited to particular LLM architectures, this work focuses on auto-regressive transformer-based LLMs.</p>
<h3>3.1. Framework Description</h3>
<p>The key idea in Patchscopes is to leverage the advanced capabilities of LLMs to generate human-like text for "translating" the information encoded in their own hidden representations. Concretely, given a hidden representation obtained from an LLM inference pass, we propose to decode specific information from it by patching it into a different inference pass (of the same or a different LLM) that encourages the translation of that specific information.</p>
<p>Notably, the rest of the forward computation after patching can augment the representation with additional information, hence, this approach does not guarantee that the patched representation itself stores all that information. However, dispatching the representation from its original context (the source prompt) stops contextualization and guarantees that no further information from the source prompt is incorporated in the post-patching computation. Thus, our framework reveals if specific information can be decoded from the patched representation via the post-patching computation.</p>
<p>Given an input sequence of $n$ tokens $S=\left\langle s_{1}, \ldots, s_{n}\right\rangle$ and a model $\mathcal{M}$ with $L$ layers, $\boldsymbol{h}<em i="i">{i}^{\ell}$ denotes the hidden representation obtained at layer $\ell \in[1, \ldots, L]$ and position $i \in[1, \ldots, n]$, when running $\mathcal{M}$ on $S$. To inspect $\boldsymbol{h}</em>^{}^{\ell}$, we consider a separate inference pass of a model $\mathcal{M<em>}$ with $L^{</em>}$ layers on a target sequence $T=\left\langle t_{1}, \ldots, t_{m}\right\rangle$ of $m$ tokens. Specifically, we choose a hidden representation $\hat{\boldsymbol{h}}<em i="i">{i^{<em>}}^{\ell^{</em>}}$ at layer $\ell^{<em>} \in\left[1, \ldots, L^{</em>}\right]$ and position $i^{<em>} \in[1, \ldots, m]$ in the execution of $\mathcal{M}^{</em>}$ on $T$. Moreover, we define a mapping function $f(\boldsymbol{h} ; \boldsymbol{\theta}): \mathbb{R}^{d} \mapsto \mathbb{R}^{d^{<em>}}$ parameterized by $\boldsymbol{\theta}$ that operates on hidden representations of $\mathcal{M}$, where $d$ and $d^{</em>}$ denote the hidden dimension of representations in $\mathcal{M}$ and $\mathcal{M}^{<em>}$, respectively. This function can be the identity function, a linear or affine function learned on task-specific pairs of representations, or even more complex functions that incorporate other sources of data. The patching operation refers to dynamically replacing the representation $\hat{\boldsymbol{h}}_{i^{</em>}}^{\ell^{<em>}}$ during the inference of $\mathcal{M}^{</em>}$ on $T$ with $f\left(\boldsymbol{h}</em>}^{\ell}\right)$. Namely, by applying $\hat{\boldsymbol{h}<em i="i">{i^{<em>}}^{\ell^{</em>}} \leftarrow f\left(\boldsymbol{h}</em>$.}^{\ell}\right)$, we intervene on the generation process and modify the computation after layer $\ell^{*</p>
<p>Overall, a Patchscope intervention applied to a representation determined by $(S, i, \mathcal{M}, \ell)$, is defined by a quintuplet $\left(T, i^{<em>}, f, \mathcal{M}^{</em>}, \ell^{<em>}\right)$ of a target prompt $T$, a target position $i^{</em>}$ in this prompt, a mapping function $f$, a target model $\mathcal{M}^{<em>}$, and a target layer $\ell^{</em>}$ of this model. It is possible that $\mathcal{M}$ and $\mathcal{M}^{*}$ are the same model, $S$ and $T$ are the same prompt, and $f$ is the identity function $\mathbb{I}$ (i.e., $\mathbb{I}(\boldsymbol{h})=\boldsymbol{h}$ ). Next, we show how this formulation covers prior interpretability methods and further extends them with new capabilities.</p>
<h3>3.2. Patchscopes Encompasses Prior Methods</h3>
<p>Many recent methods inspect LLM representations by projecting them to the output vocabulary space (nostalgebraist, 2020; Din et al., 2023; Belrose et al., 2023). Formally, an estimation of the output distribution is obtained from the representation $\boldsymbol{h}_{i}^{\ell}$ at position $i$ and layer $\ell$ by:</p>
<p>$$
\boldsymbol{p}<em U="U">{i}^{\ell}=\operatorname{softmax}\left(W</em>
$$} f\left(\boldsymbol{h}_{i}^{\ell}\right)\right) \in \mathbb{R}^{|V|</p>
<p>where $W_{U} \in \mathbb{R}^{|V| \times d}$ is the model's unembedding matrix and $f$ is a simple mapping function, such as the identity function or an affine mapping. We note that the operation applied to $f\left(\boldsymbol{h}_{i}^{\ell}\right)$ is the same computation applied by the model to the last-layer representation for obtaining the nexttoken prediction. Therefore, prior methods that inspect representations in the vocabulary space can be viewed as a class of Patchscopes that maps representations from any source layer $\ell$ to the last target layer $L^{*}$. Differences between these methods lie in the choice of $f$; logit lens (nostalgebraist, 2020; Dar et al., 2023) applies the identity function, linear shortcuts (Din et al., 2023) uses a linear mapping function, and tuned lens (Belrose et al., 2023) trains an affine mapping. Recently, Hernandez et al. (2023b) introduced LRE Attribute Lens that builds $f$ based on a relation linearity assumption, and showed its effectiveness in attribute extraction.</p>
<p>This class of methods has proven to be effective for different applications, for example, in improving inference efficiency via early exiting (Din et al., 2023). While the majority of methods and applications in this category use a single model $\left(\mathcal{M}^{<em>}=\mathcal{M}\right)$, Merullo et al. (2022) had demonstrated successful caption generation with a generative image model as $\mathcal{M}$ and a language model as $\mathcal{M}^{</em>}$.</p>
<p>Another category of inspection methods intervene on the LLM computation. Contemporary to our work, Pal et al. (2023) have investigated whether it is possible to anticipate multiple generated tokens ahead from a given hidden representation, rather than estimating just the next-token prediction. Their method (Future Lens) uses a target prompt different from the original prompt (i.e., $T \neq S$ ) and is designed to decode subsequent tokens from information encoded in a hidden representation $\boldsymbol{h}_{i}^{\ell}$. Example target prompts are "The multi-tokens present here are " and "Hello! Could you please tell me more about ". Future Lens can be cast as another Patchscope with $\mathcal{M}^{<em>}=\mathcal{M}$ and $\ell^{</em>}=\ell$.</p>
<p>More broadly, Patchscopes also cover recent mechanistic interpretability methods that analyze internal processes in LLMs with inference computation interventions. Specifically, causal tracing (Meng et al., 2022a) uses a source prompt augmented with Gaussian noise as the target prompt. Other previous methods have intervened on one or more target layers during inference by patching zero vectors to the computation (Wang et al., 2022; Conmy et al., 2023; Geva
et al., 2023), namely, setting $f(\boldsymbol{h})=\mathbf{0}$. For a configuration summary of how these interpretability methods can be cast as Patchscope instances, see §A, Tab. 4.</p>
<h3>3.3. Patchscopes Enables Novel Inspection Methods</h3>
<p>Prior work has utilized specific patching configurations for interpretability, largely patching the same model while using the same prompt (i.e., $\mathcal{M}^{*}=\mathcal{M}, T=S$ ). The framing of Patchscopes introduces a wide range of unexplored setups potentially unlocking new inspection capabilities.</p>
<p>Specifically, we observe that modifying the target prompt enables an expressive decoding of a wide range of features, detached from the source prompt computation. For instance, we can use the prompt "The capital of $X$ is" to check if the capital city of a given country is extractable from its (last token) hidden representation at a specific layer. Similarly, a prompt like "Tell me facts about $X$ " can be leveraged to assess whether the model has resolved the entity name in a specific layer. Contrary to probing, this approach is not restricted by the number of classes of the chosen feature.</p>
<p>Moreover, when the inspected model is not expressive enough to answer certain queries, patching representations into a more capable model could be useful (Hernandez et al., 2022; Singh et al., 2023; Schwettmann et al., 2023).</p>
<h2>4. Experiments</h2>
<p>In this section, we evaluate our framework on decoding next-token predictions (§4.1), extracting attributes (§4.2), analyzing the contextualization of entity names (§4.3), and leveraging stronger models for inspection via cross-model patching (§4.4). See a summary in Tab. 1.</p>
<h3>4.1. Decoding of Next-Token Predictions</h3>
<p>As introduced in $\S 3.2$, let $\boldsymbol{p}^{L}$ be the output probability distribution for some input, obtained by multiplying the finallayer last-position hidden representation $\boldsymbol{h}^{L}$ by the unembedding matrix $W_{U} \in \mathbb{R}^{|V| \times d}$. We wish to estimate $\boldsymbol{p}^{L}$ from intermediate representations $\mathbf{h}^{\ell}$ s.t. $\ell&lt;L$. Particularly, we ask how early in the computation the model has concluded its final prediction from the given context. In our experiments, we consider multiple LLMs - LLaMA2 (13B) (Touvron et al., 2023b), Vicuna (13B) (Chiang et al., 2023), GPT-J (6B) (Wang \&amp; Komatsuzaki, 2021), and Pythia (12B) (Biderman et al., 2023) (see more details in §B.1).</p>
<p>Methods We compare vocabulary projection methods (§3.2) with a new Patchscope. Each method yields an estimated output probability $\hat{\boldsymbol{p}}^{\ell}$ by patching an intermediate representation $\boldsymbol{h}^{\ell}$ to the model's final layer. Here, we focus on the common setting where $\mathcal{M}=\mathcal{M}^{<em>}$, and discuss extensions to $\mathcal{M} \neq \mathcal{M}^{</em>}$ in $\S 4.4$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Precision@1 ( $\uparrow$ is better) and Surprisal ( $\downarrow$ is better) of next-token prediction estimation in multiple models. From layer 10 and upwards, the token identity method (ours) consistently outperforms the rest of the baselines across all the models.</p>
<ul>
<li>Logit Lens: Following prior work (nostalgebraist, 2020; Geva et al., 2022a), we define $f$ as the identity function, meaning no change is applied to the patched representation. That is, $f(\boldsymbol{h}):=\mathbb{I}(\boldsymbol{h})$.</li>
<li>Tuned Lens: Motivated by Belrose et al. (2023); Din et al. (2023), we employ an affine mapping function between representations at layer $\ell$ and the final layer $L$. Specifically, we feed the model examples from a training set $\mathcal{T}$ and for each example $s \in \mathcal{T}$ obtain a pair $\left(\boldsymbol{h}<em s="s">{s}^{\ell}, \boldsymbol{h}</em>}^{L}\right)$ of hidden representations. Then, we fit a linear regression model to find the matrix $A^{\ell} \in \mathbb{R}^{d \times d}$ and the bias vector $\boldsymbol{b}^{\ell} \in \mathbb{R}^{d}$ that are numerical minimizers for $\sum_{s \in \mathcal{T}}\left|A \boldsymbol{h<em s="s">{s}^{\ell}-\boldsymbol{h}</em>$.}^{L}+\boldsymbol{b}\right|^{2}$. We define $f$ as: $f\left(\boldsymbol{h}^{\ell}\right):=A^{\ell} \boldsymbol{h}^{\ell}+\boldsymbol{b}^{\ell</li>
<li>Token Identity Patchscope: Unlike the previous methods, here we use a target prompt that is different from the source prompt $(T \neq S)$ and is meant to encourage decoding the token identity of the hidden representation. Also, while the above methods skip the computation between layers $l$ and $L$, here we modify it such that all the information from the source prompt computation is discarded, except for the patched representation. We craft a prompt with $k$ demonstrations representing an identity-like function, formatted as "tok $<em 1="1">{1} \rightarrow$ tok $</em> ;$ tok $<em 2="2">{2} \rightarrow$ tok $</em>$ ". See $\S$ B. 3 for further details and an experiment showing this method's robustness to different demonstrations. Note that this Patchscope does not require any training.} ; \ldots ;$ tok $_{k</li>
</ul>
<p>Evaluation Following Din et al. (2023), we use Pile evaluation set and the following metrics (see details in §B.2):</p>
<ul>
<li>Precision@1 ( $\uparrow$ is better): The portion of examples for which the highest-probability token $t$ in the estimated probability distribution matches the highest-probability token in the original output distribution. That is, if $\arg \max <em t="t">{t}\left(\hat{\boldsymbol{p}}</em>\right)=\arg \max }^{\ell<em t="t">{t}\left(\boldsymbol{p}</em>\right)$.}^{L</li>
<li>Surprisal ( $\downarrow$ is better): The minus log-probability of the highest-probability token in the predicted distribution $\hat{\boldsymbol{p}}^{\ell}$ according to $\boldsymbol{p}^{L}$, i.e., $-\log \boldsymbol{p}<em t="t">{\hat{t}}^{L}$, where $\hat{t}=\arg \max </em>\right)$.}\left(\hat{\boldsymbol{p}}_{t}^{\ell</li>
</ul>
<p>Results Across all the models, from layer 10 and upwards, the token identity Patchscope consistently outperforms the other baselines, obtaining a gain of up to $98 \%$ in layers 18-22 (see Fig. 2). This demonstrates the utility of leveraging the model's decoding procedure for inspecting representations of different source prompts, and shows that in most cases hidden representations in early layers carry the prediction information regardless of their context.</p>
<p>In the first 10 layers, performance of all methods is worsened, with the token identity prompt performing on-par with logit lens, and tuned lens performing slightly better, which could be due to the additional training of its mappings. Low performance in these layers is expected, as it is where the input contextualization happens. In $\S 4.3$, we introduce a Patchscope geared towards unraveling this process.</p>
<h3>4.2. Extraction of Specific Attributes</h3>
<p>Classification probes are arguably the most commonly used method for checking if certain attributes are encoded in hidden representations (Belinkov, 2022; Belinkov \&amp; Glass, 2019). However, they need to be trained, and the range of attribute classes needs to be known a priori. Here we show that repurposing Patchscopes for attribute extraction overcomes these limitations. First, it does not require training. Second, it is not limited by a predefined set of labels, but rather benefits from an open vocabulary. In addition, by taking advantage of the model's nonlinearities, it can capture more complex relations compared to linear probes.</p>
<p>Experimental Setup Consider factual and commonsense knowledge represented as triplets $(\sigma, \rho, \omega)$ of a subject (e.g., "United States"), a relation (e.g., "largest city of"), and an object (e.g., "New York City"). We investigate to what extent the object $\omega$ can be extracted from the last token representation of the subject $\sigma$ in an arbitrary input context. To this end, we conduct experiments on 8 commonsense and 25 factual knowledge tasks curated by Hernandez et al. (2023b). This dataset includes $(\sigma, \rho, \omega)$ triplets for different relations, along with prompt templates that verbalize them in natural language. We conduct experiments with GPT-J (6B) (Wang \&amp; Komatsuzaki, 2021), filtering the data to keep only the examples where $\omega$ appears in the the model's continuation of the prompt up to 20 tokens. The choice of 20 balances computation cost with accommodating the open-ended nature of Patchscopes, as the ground truth token does not necessarily appear in the next immediate token in a fluent</p>
<p>Table 2. Feature extraction accuracy (mean $\pm$ std). Comparing zeroshot feature extraction Patchscope to a logistic regression probe shows that despite using no training data, it has a significantly higher accuracy than baseline in 6 out of 12 tasks. We use pairwise t-test with Bonferroni correction for comparing the two methods. ${ }^{<em> </em>}$ and * indicate $p&lt;1 e-5$ and $p&lt;1 e-4$, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Probe</th>
<th style="text-align: center;">Patchscope</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fruit inside color</td>
<td style="text-align: center;">$37.4 \pm 6.6$</td>
<td style="text-align: center;">$38.0 \pm 18.7$</td>
</tr>
<tr>
<td style="text-align: center;">Fruit outside color</td>
<td style="text-align: center;">$35.5 \pm 3.1$</td>
<td style="text-align: center;">$\mathbf{7 1 . 0} \pm \mathbf{1 3 . 3}^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">Object superclass</td>
<td style="text-align: center;">$\mathbf{6 5 . 6} \pm \mathbf{1 0 . 5}^{*}$</td>
<td style="text-align: center;">$54.8 \pm 11.3$</td>
</tr>
<tr>
<td style="text-align: center;">Substance phase</td>
<td style="text-align: center;">$73.8 \pm 3.7$</td>
<td style="text-align: center;">$\mathbf{9 1 . 9} \pm \mathbf{1 . 7}^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">Task done by tool</td>
<td style="text-align: center;">$10.1 \pm 3.2$</td>
<td style="text-align: center;">$\mathbf{4 8 . 1} \pm \mathbf{1 3 . 2}^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">Company CEO</td>
<td style="text-align: center;">$5.0 \pm 2.6$</td>
<td style="text-align: center;">$\mathbf{4 7 . 8} \pm \mathbf{1 3 . 9}^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">Country currency</td>
<td style="text-align: center;">$17.7 \pm 2.2$</td>
<td style="text-align: center;">$\mathbf{5 1 . 0} \pm \mathbf{8 . 9}^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">Food from country</td>
<td style="text-align: center;">$5.1 \pm 3.7$</td>
<td style="text-align: center;">$\mathbf{6 3 . 8} \pm \mathbf{1 1 . 3}^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">Plays pos. in sport</td>
<td style="text-align: center;">$75.9 \pm 9.1$</td>
<td style="text-align: center;">$72.2 \pm 7.2$</td>
</tr>
<tr>
<td style="text-align: center;">Plays pro sport</td>
<td style="text-align: center;">$53.8 \pm 10.3$</td>
<td style="text-align: center;">$46.3 \pm 14.2$</td>
</tr>
<tr>
<td style="text-align: center;">Product by co.</td>
<td style="text-align: center;">$58.9 \pm 7.2$</td>
<td style="text-align: center;">$63.2 \pm 10.7$</td>
</tr>
<tr>
<td style="text-align: center;">Star constellation</td>
<td style="text-align: center;">$17.5 \pm 5.3$</td>
<td style="text-align: center;">$18.4 \pm 5.1$</td>
</tr>
</tbody>
</table>
<p>response. For each example, we sample 5 utterances from the WikiText-103 dataset (Merity et al., 2016) that include $\sigma$ and use them as $S$. Lastly, we keep tasks with at least 15 samples, which results in 5 commonsense and 7 factual tasks with a total of 1,453 datapoints. See details in §C.</p>
<p>Methods We compare our proposed Patchscope against linear probing (Köhn, 2015; Gupta et al., 2015).</p>
<ul>
<li>Zero-shot Feature Extraction Patchscope: We craft $T$ as a general verbalization of $\rho$ followed by a placeholder for $\sigma$, such that $i^{<em>}=m$. For example, we use $T \leftarrow$ "The largest city in x" with " x " as a placeholder for the subject. To extract the object from the entity representation in $S$, we patch the representation of token " x " at layer $\ell^{</em>}$ with the representation of "States" from layer $\ell$, and consider if the generated text includes $\omega$. The remaining configurations of this Patchscope are $f \leftarrow \mathbb{I}, \mathcal{M}^{<em>} \leftarrow \mathcal{M}, i \leftarrow$ the last token of $\sigma$ in $S$. We consider all combinations of $\ell \in[1, \ldots, L] \times \ell^{</em>} \in\left[1, \ldots, L^{*}\right]$. Later in this section, we discuss the role of $\ell$ pertaining to attribute extraction.</li>
<li>Logistic Regression Probe: Let $\Omega$ represent the range of possible objects for a given relation. We use the set of unique values of $\omega$ in the training set as a proxy for $\Omega$. We train a logistic regression probe (Köhn, 2015; Gupta et al., 2015) for each layer that predicts $\omega \in \Omega$ from the last token representation of $\sigma$. Given that 6 out of 12 tasks have fewer than 40 datapoints, we use three-fold cross-validation for training and evaluation of this baseline. Note that we have excluded tasks where
the probe fails completely due to insufficient number of training examples (fewer than 15 datapoints).</li>
</ul>
<p>Evaluation We measure the average attribute extraction accuracy. For a given sample, the Patchscope is considered correct if $\exists \ell^{<em>} \in\left[1, \ldots, L^{</em>}\right]$ where the generated text up to 20 tokens includes $\omega$. For the probe, a prediction is correct if the highest probability is assigned to $\omega$.</p>
<p>Results Tab. 2 summarizes the results, averaged over $\ell \in[1, \ldots, L]$. We conduct a T-test with Bonferroni correction to compare the two methods. Despite using no training data and having no restrictions on the output, the Patchscope achieves a significantly higher accuracy than the probe on six out of twelve tasks $(p&lt;1 e-5)$, and works comparably well in all but one of the remaining six. These results suggest that, in the majority of cases, the source representation without its original context carries enough information about many attributes that a targeted Patchscope can extract. We also study how the accuracy changes across the source layers, and observe that Patchscope consistently outperforms the baseline in early layers, outperforms or works on par with the baseline in mid layers, and almost all cases where it performs worse than the baseline occur in later layers. Our interpretation is that given the language modeling training objective, the representations shift toward next-token prediction in the later layers. Therefore, the attribute of interest would not be as readily accessible via the model's computation in these layers. This interpretation is also aligned with recent findings that show no decline in using linear relational embedding in predicting $\omega$ only when the next token also happens to be $\omega$ (Hernandez et al., 2023b). Note that this pattern explains the higher standard deviation of Patchscope accuracy observed in Tab. 2. We discuss this phenomenon in more detail in §C (see Fig. 6).</p>
<h3>4.3. Analyzing Entity Resolution in Early Layers</h3>
<p>The previous sections focused on analyzing the information encoded in a single hidden state. Here we turn to consider a more global question of how LLMs resolve entity mentions across multiple layers. Concretely, given a subject entity name, such as "the summer Olympics of 1996", how does the model contextualize the input tokens of the entity and at which layer is it fully resolved?</p>
<p>Answering these questions is hard with existing methods; vocabulary projections focus on the output prediction and fail to show clear patterns in early layers, and probing is restricted to outputs from a fixed number of classes, which may not be expressive enough to describe this process. Alternative approaches have studied this process indirectly via interventions (Meng et al., 2022a), showing that the model constructs a subject representation at the last token of the entity name. However, it is still unclear how this</p>
<p>Table 3: Illustrating entity resolution via a qualitative example. The expressive generations show that as we go through the layers, more tokens from the context get integrated into the current representation. The “Generation” column shows the automatically generated text. The “Explanation” column shows our own manually coded interpretation, aiming to specify what entity the generation refers to and how that relates to the tokens processed. $\mathcal{M}^{<em>}=\mathcal{M}\leftarrow$ Vicuna (13B), $\ell^{</em>}=\ell$, $S\leftarrow$ "Diana, Princess of Wales".</p>
<table>
<thead>
<tr>
<th>$\ell$</th>
<th>Generation</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-2</td>
<td>: Country in the United Kingdom</td>
<td>Wales</td>
</tr>
<tr>
<td>3</td>
<td>: Country in Europe</td>
<td>Wales</td>
</tr>
<tr>
<td>4</td>
<td>: Title held by female sovereigns in their own right or by queens consort</td>
<td>Princess of Wales (unspecific)</td>
</tr>
<tr>
<td>5</td>
<td>: Title given to the wife of the Prince of Wales (and later King)</td>
<td>Princess of Wales (unspecific)</td>
</tr>
<tr>
<td>6</td>
<td>: Diana, Princess of Wales (1961-1997), the first wife of Prince Charles, Prince of Wales, who was famous for her beauty and humanitarian work</td>
<td>Diana, Princess of Wales</td>
</tr>
</tbody>
</table>
<p>contextualization is performed.
We analyze how LLMs contextualize input entity names by leveraging Patchscopes. Particularly, we craft a target prompt for generating a description of a given subject, and apply it to the hidden representation at the last subject position in the source prompt - where the model forms the subject representation (Geva et al., 2023; Hernandez et al., 2023a) - across the early layers. This will allow us to see how the model describes the subject in each layer.</p>
<p>Analysis Setting We use a few-shot target prompt template for decoding an entity description: "subject $<em 1="1">{1}$ : description $</em>, \ldots$, subject $<em k="k">{k}$ : description $</em>, x$ ", while patching the last position corresponding to $x$. We take the 200 most popular and 200 least popular subject entities from the PopQA dataset (Mallen et al., 2023). The popular entities should appear frequently in LLMs’ pre-training data, and are thus likely to be captured by the model, while resolving the rare entities is expected to be more challenging (Kandpal et al., 2023; Mallen et al., 2023). Then, for the source prompt we use the entity name, and for the target prompt we sample $k=3$ random subject entities. We obtain a short (up to one sentence) description of every subject entity from Wikipedia. Our target prompt and more technical details are provided in §D.1. We patch the last position representations from the first 10 layers of Vicuna 13B to the target prompt and evaluate the generated subject name and description. Specifically, the generated descriptions are evaluated against the descriptions from Wikipedia using RougeL (Lin, 2004). Evaluation with Rouge1 (Lin, 2004) and Sentence-Bert (Reimers \&amp; Gurevych, 2019) shows
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. RougeL scores of the generated descriptions against descriptions from Wikipedia, using Vicuna models.
similar trends (see §D.2).</p>
<p>Results Tab. 3 illustrates the generations by Vicuna 13B for a sample subject entity, when patching its representation at different layers to the target prompt (see more examples in §D.3). For most entities, the contextualization process is spread over the first layers, with the last subject token gradually encompassing more distant positions across layers.</p>
<p>This trend can be quantitatively observed by the similarity between the generated descriptions and the descriptions from Wikipedia, as measured by RougeL. See Fig. 3 where $\mathcal{M}=\mathcal{M}^{*}$. For both models, similarity increases in the first 5 layers and then slowly decreases. This decrease could potentially be attributed to contamination caused by the representation of the placeholder token " $x$ " remaining in the early layers, when patching is applied to a later layer. Note that this potential issue is only applicable to multi-token generation scenarios as future positions can still attend to the placeholder position in early layers, potentially interfering with the model’s ability to accurately generate descriptions for the patched token. See §D. 3 for qualitative examples corroborating this interpretation. As expected, the scores for rare, long-tail entities are significantly lower than those of popular entities. See §D. 2 for additional results with Pythia where the smaller model seems to outperform the larger model, possibly because the larger model is biased toward output generation at the expense of input contextualization. To summarize, this analysis shows Patchscopes’ utility for inspecting the contextualization process in early layers.</p>
<h3>4.4. Expressiveness from Cross-Model Patching</h3>
<p>A possible avenue for improving inspection capabilities is to explain a given model with a model that is more expressive (Bills et al., 2023). In Patchscopes, this means to patch a representation of $\mathcal{M}$ into a more expressive model $\mathcal{M}^{*}$. However, it is not clear if such an intervention would yield plausible results, due to possible discrepancies between the two models resulting from different architectures, optimization processes, and so on. Here, we present experiments on next-token prediction and entity resolution that exemplify</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. An illustration of CoT Patchscope on a single example. In this example, $\pi_{1} \leftarrow$ "the company that created Visual Basic Script", $\pi_{2} \leftarrow$ "The current CEO of", $S=T \leftarrow\left[\pi_{2}\right]\left[\pi_{1}\right]=$ "The current CEO of the company that created Visual Basic Script". Note that $\mathcal{M}=\mathcal{M}^{*}$ and $f \leftarrow \mathbb{I}$.
not only the feasibility, but also the opportunities unlocked by patching across models of the same family.</p>
<p>Next-Token Prediction We repeat the experiment in $\S 4.1$, using the token identity Patchscope. To overcome discrepancies between the models, we learn affine mappings between their layers (similarly to Tuned Lens). Our results show that source and target layers on the diagonal exhibit the highest precision values, and that patching representations to an early layer of the larger model are the most effective. Overall, suggesting that when $\mathcal{M}^{<em>}$ and $\mathcal{M}$ are from the same model family, it is possible to leverage $\mathcal{M}^{</em>}$ for decoding information from the representations of $\mathcal{M}$. For detailed results on Precision@1 and Surprisal, see §E.</p>
<p>Entity Resolution in Early Layers We now show that using a large model as $\mathcal{M}^{<em>}$ can enhance the output expressivity. To this end, we repeat our entity resolution experiment in $\S 4.3$ with Vicuna model family, setting $\mathcal{M} \leftarrow 7 \mathrm{~B}, \mathcal{M}^{</em>} \leftarrow 13 \mathrm{~B}$. Fig. 3 shows the cross-model patching results (green lines) compared to the same-model patching $\mathcal{M} \leftarrow 7 \mathrm{~B}, \mathcal{M}^{*} \leftarrow 7 \mathrm{~B}$ (blue line). The results show that cross-model patching from a smaller model to its larger version generally improves the ability to inspect the input contextualization, both for popular and rare entities. For Pythia, since the smaller model outperforms the larger one, cross-model patching is not as effective (see §D.2).</p>
<h2>5. Application: Correcting Multi-Hop Errors</h2>
<p>Multi-hop reasoning is a challenging problem (Zhong et al., 2023). While a language model may be capable of correctly answering each step independently, it could still fail at processing the connection between different steps, resulting in an incorrect prediction. Recent attempts to improve multihop reasoning rely on prompting the model to generate a step-by-step answer autoregressively (e.g., Wei et al., 2022; Yao et al., 2023; Besta et al., 2024), some with an iterative process of self-refinement (e.g., Madaan et al., 2023). However, achieving similar benefits might be possible via directly rewiring the model's intermediate computation.</p>
<p>Here, we show that Patchscopes can improve multi-hop reasoning performance without generating the reasoning steps, particularly in cases where the model fails at completing a multi-hop query despite being successful in each reasoning step independently. Via Patchscopes, one can surgically operate on the model representations, reroute its intermediate answer to one reasoning step, simplify the consequent step, and ultimately correct the final prediction.</p>
<p>Data Building on Hernandez et al. (2023b), we systematically generate all valid multi-hop factual and commonsense reasoning queries where $\omega_{1}=\sigma_{2}$. We conduct experiments on Vicuna (13B), focusing on samples where $\mathcal{M}$ accurately represents both $\tau_{1}$ and $\tau_{2}$ independently, that is, $\omega$ appears in the next 20 tokens $\mathcal{M}$ generates conditioned on the prompt $\pi$ that verbalizes $\sigma$ and $\rho$. This process yields 1,104 multi-</p>
<p>hop reasoning samples, out of which 46 satisfy the above criteria and are used for evaluation. See more details in §F.</p>
<p>Experimental Setup Following the notation in $\S 4.2$, let $\tau_{1}=\left(\sigma_{1}, \rho_{1}, \omega_{1}\right)$ represent the relation $\rho_{1}$ between a subject entity $\sigma_{1}$ and an object entity $\omega_{1}$. Let $\tau_{2}=\left(\sigma_{2}, \rho_{2}, \omega_{2}\right)$ represent another tuple such that $\sigma_{2}=\omega_{1}$. A multi-hop reasoning query pertaining to $\tau_{1}$ and $\tau_{2}$ is a prompt composed of two parts: $\pi_{1}$ is a verbalization of $\sigma_{1}$ and $\rho_{1}$ from which $\omega_{1}$ can be inferred; $\pi_{2}$ is a verbalization of $\rho_{2}$, from which $\omega_{2}$ can be inferred after its concatenation with $\pi_{1}$. For example, Let $\tau_{1} \leftarrow$ ("Visual Basic", "product of", "Microsoft") and $\tau_{2} \leftarrow$ ("Microsoft", "company CEO", "Satya Nadella"). An example verbalization of these tuples is $\pi_{1} \leftarrow$ "the company that created Visual Basic", $\pi_{2} \leftarrow$ "The current CEO of", leading to the multi-hop query $\left[\pi_{2}\right]\left[\pi_{1}\right]=$ "The current CEO of the company that created Visual Basic".</p>
<p>Method We introduce a Chain-of-Thought (CoT) Patchscope to fix multi-hop reasoning via intervening on the computation graph and rerouting representation likely to capture $\omega_{1}$ in place of $\sigma_{2}$. Concretely, $S$ refers to the formed query discussed above, and we use the following configuration: $T \leftarrow S, \mathcal{M}^{<em>} \leftarrow \mathcal{M}, i \leftarrow n, i^{</em>} \leftarrow$ the token preceding $\pi_{1}$. We evaluate the outputs in terms of accuracy, similarly to $\S 4.2$. For a sample $S$, the Patchscope is considered accurate if $\exists\left(\ell, \ell^{<em>}\right): \ell \in[1, \ldots, L], \ell^{</em>} \in\left[1, \ldots, L^{<em>}\right]$ where the autoregressive generation up to 20 tokens includes $\omega_{2}$. Fig. 4 illustrates the CoT Patchscope with an example. We use the following configuration for CoT Patchscope: $S \leftarrow \pi_{1}, T \leftarrow \pi_{2}, i \leftarrow n, i^{</em>} \leftarrow m$, which is equivalent to $S=T \leftarrow\left[\pi_{2}\right]\left[\pi_{1}\right]$ and adjusting the attention mask such that no token in $S$ has visibility to $\pi_{2}$ and no token in $T$ has visibility to $\pi_{1}$. In addition, we consider two baselines.</p>
<p>Vanilla Baseline For this baseline, we set $S \leftarrow\left[\pi_{1}\right]\left[\pi_{2}\right]$, we let the model autoregressively generate up to 20 tokens and check whether $\omega_{2}$ appears in the generation.</p>
<p>Chain-of-Thought Baseline Here, the setup and evaluation is similar to the vanilla baseline, except that we prepend "Let's think step by step." to $S$, following (Wei et al., 2022). We then let the model generate up to 20 tokens and check whether $\omega_{2}$ appears in the generation. Note that this experiment uses Vicuna (13B). Vicuna is based on LLaMA, with supervised finetuning on additional instruction data, which makes it amenable to chain-of-thought prompting.</p>
<p>Results While the vanilla baseline accuracy is only $19.57 \%$, and CoT baseline accuracy is $35.71 \%$, our proposed Patchscope achieves $50 \%$ accuracy. For more
details about the interaction between $\ell$ and $\ell^{*}$, and how it affects the success rate, see Fig. 12 in $\S \mathrm{F}$.</p>
<p>We emphasize that our primary goal in this section is not to devise a new method to solve multi-hop queries that is necessarily a competitor to CoT , but rather to make a proof-of-concept while comparing with CoT as a common reference. We highlight that we have taken advantage of the extra structural information about the queries given the prior knowledge about how they were synthesized. One could potentially automate this by learning the right places to patch. Li et al. (2024) recently proposed an optimization-based approach for directly patching multi-head self-attention in mid-layers, which can be viewed as soft position-selection, and works effectively in multihop error correction, suggesting that inferring the right positions to patch could be effective. However, even if optimal source and target position are automatically decided upfront, Patchscope and CoT may not be directly comparable. CoT generates multiple steps which fundamentally extend the computational power of the LLM (Merrill \&amp; Sabharwal, 2024), but a Patchscope with pre-identified $\left(l, l^{*}\right)$ only makes $O(1)$ inference passes.</p>
<h2>6. Conclusion</h2>
<p>We present Patchscopes, a simple and effective framework that leverages the ability of LLMs to generate humanlike text for decoding information from intermediate LLM representations. We show that many existing interpretability methods can be cast as specific Patchscope instances, and even these only cover a small portion of the framework's possible configurations. Moreover, new underexplored Patchscopes substantially improve our ability to decode various types of information from the model's internal computation, such as the output prediction and knowledge attributes, typically outperforming prominent methods that rely on projection to the vocabulary and probing. In addition, our framework enables new capabilities, such as analyzing the contextualization process of input tokens in early LLM layers, and can correct multi-hop reasoning.</p>
<p>There are multiple future research directions to consider. An important factor in the effectiveness of a chosen target prompt is how the information from the patched position propagates during inference to other positions and across layers. Understanding how to best use a given target prompt, perhaps automatically, is an important factor for using Patchscopes. Another avenue for future work is investigating the effectiveness of few-shot target prompts compared to zero-shot/instruction-based prompts. More expressive instruction-based target prompts, for example, could enable extracting more complex information. Additionally, while our cross-model patching focused on models from the same family, it will be valuable to explore which mapping functions would enable patching across models</p>
<p>from different families and perhaps with different architectures. Other directions for future work include applications across different domains and modalities, investigating variants with simultaneous multi-token patching or multi-layer patching to mitigate the risks of placeholder contamination, and presenting recipes for task-specific and task-agnostic Patchscopes.</p>
<h2>Acknowledgements</h2>
<p>We thank Amir Globerson for feedback on writing and presentation of results. We thank Ardavan Saeedi, Martin Wattenberg, Ellie Pavlick, the AI Explorables team ${ }^{3}$ at Google Research, and Jasmijn Bastings for their helpful comments.</p>
<h2>Impact Statement</h2>
<p>Societal Impact This paper presents a new framework for interpreting hidden representations of large language models. Interpretability methods in general can be used to investigate models' reliability and safety prior to deployment. We hope that Patchscopes framework facilitates progress in this area with the introduction of inspection tools that are more expressive, robust across layers, and do not require training data.</p>
<p>Limitations While our proposed framework is not limited to any particular architecture or domain, the experimental evidence provided in this paper focuses on autoregressive Transformer-based language models, and future work is needed to verify its effectiveness in other setups.</p>
<h2>References</h2>
<p>Alain, G. and Bengio, Y. Understanding intermediate layers using linear classifier probes. 5th International Conference on Learning Representations, Workshop Track Proceedings, 2017.</p>
<p>Bansal, Y., Nakkiran, P., and Barak, B. Revisiting model stitching to compare neural representations. Advances in neural information processing systems, 34:225-236, 2021.</p>
<p>Belinkov, Y. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207-219, 2022.</p>
<p>Belinkov, Y. and Glass, J. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72, 2019.</p>
<p>Belrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I., McKinney, L., Biderman, S., and Steinhardt, J. Eliciting</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.</p>
<p>Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the 38th AAAI Conference on Artificial Intelligence, 2024.</p>
<p>Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning (ICML), 2023.</p>
<p>Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, J., and Saunders, W. Language models can explain neurons in language models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023), 2023.</p>
<p>Casper, S., Rauker, T., Ho, A., and Hadfield-Menell, D. Sok: Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. In First IEEE Conference on Secure and Trustworthy Machine Learning, 2022.</p>
<p>Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing GPT-4 with 90\%* ChatGPT quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/.</p>
<p>Conmy, A., Mavor-Parker, A. N., Lynch, A., Heimersheim, S., and Garriga-Alonso, A. Towards automated circuit discovery for mechanistic interpretability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</p>
<p>Csiszárik, A., Kőrösi-Szabó, P., Matszangosz, Á. K., Papp, G., and Varga, D. Similarity and matching of neural network representations. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=aedFIIRRfXr.</p>
<p>Dar, G., Geva, M., Gupta, A., and Berant, J. Analyzing transformers in embedding space. In Annual Meeting of the Association for Computational Linguistics, 2023.</p>
<p>Din, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump to conclusions: Short-cutting transformers with linear transformations. arXiv preprint arXiv:2303.09435, 2023.</p>
<p>Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Geiger, A., Lu, H., Icard, T., and Potts, C. Causal abstractions of neural networks. Advances in Neural Information Processing Systems, 34:9574-9586, 2021.</p>
<p>Geva, M., Caciularu, A., Dar, G., Roit, P., Sadde, S., Shlain, M., Tamir, B., and Goldberg, Y. LM-debugger: An interactive tool for inspection and intervention in transformerbased language models. In Che, W. and Shutova, E. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 12-21, Abu Dhabi, UAE, December 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-demos.2. URL https: //aclanthology.org/2022.emnlp-demos.2.</p>
<p>Geva, M., Caciularu, A., Wang, K., and Goldberg, Y. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 30-45, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main. 3. URL https://aclanthology.org/2022. emnlp-main. 3 .</p>
<p>Geva, M., Bastings, J., Filippova, K., and Globerson, A. Dissecting recall of factual associations in auto-regressive language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 12216-12235, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.751. URL https://aclanthology. org/2023.emnlp-main.751.</p>
<p>Ghandeharioun, A., Kim, B., Li, C.-L., Jou, B., Eoff, B., and Picard, R. W. Dissect: Disentangled simultaneous explanations via concept traversals. arXiv preprint arXiv:2105.15164, 2021.</p>
<p>Goldowsky-Dill, N., MacLeod, C., Sato, L., and Arora, A. Localizing model behavior with path patching. arXiv preprint arXiv:2304.05969, 2023.</p>
<p>Gupta, A., Boleda, G., Baroni, M., and Padó, S. Distributional vectors encode referential attributes. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 12-21, 2015.</p>
<p>Hanna, M., Liu, O., and Variengien, A. How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=p4PckNQR8k.</p>
<p>Hase, P., Bansal, M., Kim, B., and Ghandeharioun, A. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. arXiv preprint arXiv:2301.04213, 2023.</p>
<p>Hendel, R., Geva, M., and Globerson, A. In-context learning creates task vectors. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.</p>
<p>Hernandez, E., Schwettmann, S., Bau, D., Bagashvili, T., Torralba, A., and Andreas, J. Natural language descriptions of deep features. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=NudBMY-tzDr.</p>
<p>Hernandez, E., Li, B. Z., and Andreas, J. Measuring and manipulating knowledge representations in language models. arXiv preprint arXiv:2304.00740, 2023a.</p>
<p>Hernandez, E., Sharma, A. S., Haklay, T., Meng, K., Wattenberg, M., Andreas, J., Belinkov, Y., and Bau, D. Linearity of relation decoding in transformer language models. arXiv preprint arXiv:2308.09124, 2023b.</p>
<p>Kandpal, N., Deng, H., Roberts, A., Wallace, E., and Raffel, C. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pp. 15696-15707. PMLR, 2023.</p>
<p>Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pp. 2668-2677. PMLR, 2018.</p>
<p>Köhn, A. What's in an embedding? analyzing word embeddings through multilingual evaluation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 2067-2073, 2015.</p>
<p>Langedijk, A., Mohebbi, H., Sarti, G., Zuidema, W., and Jumelet, J. Decoderlens: Layerwise interpretation of encoder-decoder transformers. arXiv preprint arXiv:2310.03686, 2023.</p>
<p>Lenc, K. and Vedaldi, A. Understanding image representations by measuring their equivariance and equivalence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 991-999, 2015.</p>
<p>Li, Z., Jiang, G., Xie, H., Song, L., Lian, D., and Wei, Y. Understanding and patching compositional reasoning in llms. arXiv preprint arXiv:2402.14328, 2024.</p>
<p>Lieberum, T., Rahtz, M., Kramár, J., Irving, G., Shah, R., and Mikulik, V. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv preprint arXiv:2307.09458, 2023.</p>
<p>Lin, C.-Y. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013.</p>
<p>Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=S37hOerQLB.</p>
<p>Madsen, A., Reddy, S., and Chandar, S. Post-hoc interpretability for neural nlp: A survey. ACM Computing Surveys, 55(8):1-42, 2022.</p>
<p>Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. When not to trust language models: Investigating effectiveness of parametric and nonparametric memories. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9802-9822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. URL https: //aclanthology.org/2023.acl-long.546.</p>
<p>Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359-17372, 2022a.</p>
<p>Meng, K., Sharma, A. S., Andonian, A. J., Belinkov, Y., and Bau, D. Mass-editing memory in a transformer. In The Eleventh International Conference on Learning Representations, 2022b.</p>
<p>Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016.</p>
<p>Merrill, W. and Sabharwal, A. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Merullo, J., Castricato, L., Eickhoff, C., and Pavlick, E. Linearly mapping from image to text space. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Merullo, J., Eickhoff, C., and Pavlick, E. A mechanism for solving relational tasks in transformer language models. arXiv preprint arXiv:2305.16130, 2023.</p>
<p>Mousi, B., Durrani, N., and Dalvi, F. Can llms facilitate interpretation of pre-trained language models? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, December 2023. URL https: //browse.arxiv.org/pdf/2305.13386.pdf.</p>
<p>Nanda, N., Lee, A., and Wattenberg, M. Emergent linear representations in world models of self-supervised sequence models. In Belinkov, Y., Hao, S., Jumelet, J., Kim, N., McCarthy, A., and Mohebbi, H. (eds.), Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 16-30, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. blackboxnlp-1.2. URL https://aclanthology. org/2023.blackboxnlp-1.2.
nostalgebraist. interpreting gpt: the logit lens. LessWrong, 2020. URL https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens.</p>
<p>Pal, K., Sun, J., Yuan, A., Wallace, B. C., and Bau, D. Future lens: Anticipating subsequent tokens from a single hidden state. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pp. $548-560,2023$.</p>
<p>Patel, R. and Pavlick, E. Mapping language models to grounded conceptual spaces. In International Conference on Learning Representations, 2021.</p>
<p>Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs/1908.10084.</p>
<p>Schwettmann, S., Chowdhury, N., Klein, S., Bau, D., and Torralba, A. Multimodal neurons in pretrained text-only transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2862-2867, 2023.</p>
<p>Singh, C., Hsu, A., Antonello, R., Jain, S., Huth, A., Yu, B., and Gao, J. Explaining black box text modules in natural</p>
<p>language with language models. In XAI in Action: Past, Present, and Future Applications, 2023. URL https: //openreview.net/forum?id=3BX9tM03GT.</p>
<p>Slobodkin, A., Goldman, O., Caciularu, A., Dagan, I., and Ravfogel, S. The curious case of hallucinatory (un)answerability: Finding truths in the hidden states of over-confident large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 3607-3625, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.220. URL https:// aclanthology.org/2023.emnlp-main. 220.</p>
<p>Stolfo, A., Belinkov, Y., and Sachan, M. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 7035-7052, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.435. URL https:// aclanthology.org/2023.emnlp-main.435.</p>
<p>Strobelt, H., Gehrmann, S., Pfister, H., and Rush, A. M. Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks. IEEE transactions on visualization and computer graphics, 24(1):667-676, 2017.</p>
<p>Tenney, I., Das, D., and Pavlick, E. Bert rediscovers the classical nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593-4601, 2019.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33: $12388-12401,2020$.</p>
<p>Vilas, M. G., Schaumlöffel, T., and Roig, G. Analyzing vision transformers for image classification in class embedding space. arXiv preprint arXiv:2310.18969, 2023.</p>
<p>Wallat, J., Singh, J., and Anand, A. BERTnesia: Investigating the capture and forgetting of knowledge in BERT. In Alishahi, A., Belinkov, Y., Chrupała, G., Hupkes, D., Pinter, Y., and Sajjad, H. (eds.), Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 174183, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.blackboxnlp-1. 17. URL https://aclanthology.org/2020. blackboxnlp-1.17.</p>
<p>Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.</p>
<p>Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Wang, Z., Ku, A., Baldridge, J., Griffiths, T. L., and Kim, B. Gaussian Process Probes (GPP) for uncertainty-aware probing. arXiv preprint arXiv:2305.18213, 2023.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824-24837, 2022.</p>
<p>Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. R. Tree of thoughts: Deliberate problem solving with large language models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.</p>
<p>Youssef, P., Koraş, O., Li, M., Schlötterer, J., and Seifert, C. Give me the facts! a survey on factual knowledge probing in pre-trained language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 15588-15605, 2023.</p>
<p>Zhang, F. and Nanda, N. Towards best practices of activation patching in language models: Metrics and methods. arXiv preprint arXiv:2309.16042, 2023.</p>
<p>Zhong, Z., Wu, Z., Manning, C. D., Potts, C., and Chen, D. Mquake: Assessing knowledge editing in language models via multi-hop questions. arXiv preprint arXiv:2305.14795, 2023.</p>
<p>Zhou, B., Bau, D., Oliva, A., and Torralba, A. Interpreting deep visual representations via network dissection. IEEE transactions on pattern analysis and machine intelligence, 41(9):2131-2145, 2018.</p>
<h1>A. Detailed Configuration of Prior Methods as Patchscope Instances</h1>
<p>In this section, we first reiterate Patchscopes premise and distinguish it from prior work. We then detail Patchscope configurations that replicate prior methods. We highlight that clearly formulating Patchscopes in the form of source and target quintuplets, respectively $(S, i, M, l)$ and $\left(T, i^{<em>}, M^{</em>}, l^{*}\right)$, is not in itself a contribution of this work. However, it is what facilitates the main contributions that are 3-fold: 1) The framework ties methods that otherwise might seem unrelated, e.g., logit lens and attention knockout, 2) makes it easy to come up with new configurations that significantly improve them, 3) makes it easy to see the space that prior methods do not cover, and therefore open up new applications altogether.</p>
<p>We focus on a few inspection objectives. Note that some of the baselines considered have not been originally designed to address these objectives, but are the closest solutions in literature. Particularly, we ask, given a (set of) hidden representation(s) detached from their context, how well can we do next-token prediction, feature extraction, and analysis of model's contextualization process.</p>
<h2>A.1. Next-token Prediction</h2>
<p>Goal: Estimating output probability distribution $p^{L}$ given a hidden representation $h^{l}$, particularly to inspect when the model has enough information to make the final prediction.</p>
<p>Prior Work: Many recent methods to estimate output probability distribution given an inner representation use projection onto the vocabulary space (e.g., nostalgebraist, 2020; Din et al., 2023; Belrose et al., 2023). Considering $(S, i, M, l)$ and $\left(T, i^{<em>}, M^{</em>}, l^{<em>}\right)$ formulation above, all these methods can be seen as Patchscopes where $\mathbf{M}=\mathbf{M}^{</em>}, \mathbf{l}^{*}=\mathbf{L}$, where $\mathbf{L}$ is the number of layers.</p>
<p>Distinguishing Patchscope \&amp; its Improvements: We propose the token identity Patchscope that unlike (Belrose et al., 2023; Din et al., 2023) does not require a learned mapping, yet is more successful. Note that in this task, setting $\mathbf{l}^{<em>}=\mathbf{L}$ which all the prior methods discussed above have in common, means that the choice of $\mathbf{T}$ does not matter. However, in our proposed Patchscope, we show that setting $\mathbf{l}^{</em>}=\mathbf{l}$ where $l$ is arbitrary layer, and using a few-shot token identity prompt $\mathbf{T}$ that encourages repetition results in significant improvements, and by using the target model's computation from layer $l^{*}$ onward, there is no need for training a separate mapping function.</p>
<h2>A.2. Feature Extraction</h2>
<p>Goal: Extracting specific attributes of the subject given its hidden representation $h^{l}$ (e.g., extracting "largest city of" from the representation of "States" in "United States").</p>
<p>Prior Work: Classification probes (Alain \&amp; Bengio, 2017) are the most commonly used methods for this purpose. They work by training a classifier on a dataset of prompts and their corresponding labels (e.g., countries and their corresponding largest cities).</p>
<p>Distinguishing Patchscope \&amp; its Improvements: We show an intuitive Patchscope (i.e., $T=$ "The largest city in x") significantly outperforms probing across various tasks and layers. Note that this Patchscope 1) does not require supervised training, 2) is not limited by a predefined set of labels, and 3) is more accurate than a linear probe.</p>
<h2>A.3. Analysis of Model's Contextualization Process</h2>
<p>Goal: Identifying how entity tokens are processed, and at which layer the entity is fully resolved.
Prior Work: It is hard to answer this question with existing methods. First, methods that use vocabulary projection are focused on output prediction (rather than input processing) and fail particularly in early layers, precisely where entity resolution happens. Second, probing requires a predefined set of classes, which is not expressive enough to show the gradual entity resolution process, and nontrivial. For example, consider "Alexander the grea" input. We found that "Great Britain" and "Great depression" arose in the gradual resolution process. However, these are non-trivial choices a priori. We can train a multi-class classifier to choose one out of all possible entities (or binary classifiers for every possible entity), which is unlikely to work given the huge space of possible entities. Lastly, most prior work using activation patching aims to answer a different question: whether a certain representation plays a key role in the model's forward computation and the</p>
<p>final prediction (e.g., Meng et al., 2022a; Geva et al., 2023). However, they can be viewed as Patchscope instances, and provide indirect signals about the entity resolution process, namely, showing subject information accrues in the last token after a few layers.</p>
<p>Distinguishing Patchscope \&amp; its Improvements: To the best of our knowledge, there are not any methods that verbalize the gradual entity resolution process in coherent text, and the proposed Patchscope is the first to do that.</p>
<p>In summary, as discussed in 3.2, many prior methods can be seen as Patchscopes. Tab. 4 details the configurations of Patchscopes that yield various methods introduced in prior work.</p>
<p>Table 4. Patchscopes is a novel framework for inspection of hidden representations in language models. Many prior inspection methods with various objectives can be viewed as Patchscopes, as detailed in the "Configuration" column (see notation description in §3). The rows highlighted in green show our new configurations that overcome several limitations of prior methods through more expressive inspection that is training-data free and is more robust across layers. Additionally, the generality of this framework enables novel inspection possibilities that were unexplored before. When the target prompt $(T)$ is not specified, it means that the output would be invariant to the choice of $T$. When not specified, $f \leftarrow \mathbb{I}$ and $\mathcal{M}^{*}=\mathcal{M}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Inspection Objective</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Expressive</th>
<th style="text-align: center;">Training Data Free</th>
<th style="text-align: center;">Robust Across Layers</th>
<th style="text-align: center;">Configuration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Inspecting <br> Output <br> Distribution</td>
<td style="text-align: center;">Few-shot token identity Patchscope (§4.1)</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$t^{*}: \leftarrow t$, <br> $T \leftarrow$ "tok $<em 1="1">{1} \rightarrow$ tok $</em> j$ tok $<em 2="2">{2} \rightarrow$ tok $</em>$ "} \ldots$ tok $_{k</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Logit Lens (nostalgebraist, 2020), Embedding Space Analysis (Dar et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$t^{<em>}: \leftarrow L^{</em>}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tuned Lens (Belrose et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$t^{<em>}: \leftarrow L^{</em>}, f \leftarrow$ Affine</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Future Lens (Pal et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$t^{*}: \leftarrow t, f \leftarrow$ Linear, $T \leftarrow$ Fixed or learned soft prompt</td>
</tr>
<tr>
<td style="text-align: center;">Feature <br> Extraction</td>
<td style="text-align: center;">Zero-shot feature extraction Patchscope (§4.2)</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$t^{<em>}: \leftarrow f^{</em>} \in\left[1, \ldots, L^{<em>}\right], t^{</em>}: \leftarrow m$, <br> $T \leftarrow$ relation verbalization followed by $x$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LRE Attribute Lens (Hernandez et al., 2023b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For linear relation approx.</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$t^{<em>}: \leftarrow L^{</em>}, f \leftarrow$ Linear with additional variables, $T \leftarrow S$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Probing (e.g., Belinkov \&amp; Glass, 2019; Belinkov, 2022; Alain \&amp; Bengio, 2017; Wang et al., 2023)</td>
<td style="text-align: center;">$\boldsymbol{\aleph}$</td>
<td style="text-align: center;">For training probe</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">Entity <br> Resolution</td>
<td style="text-align: center;">Entity description Patchscope (§4.3)</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$t^{<em>}: \leftarrow t, t^{</em>}: \leftarrow m$, <br> $T \leftarrow$ "subject $<em 1="1">{1}:$ description $</em>, \ldots$, subject $<em k="k">{k}:$ description $</em>, x$ "</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">X-model entity description Patchscope (§4.4)</td>
<td style="text-align: center;">$\checkmark \checkmark \checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\mathcal{M}^{<em>}: \leftarrow$ a larger variant of $\mathcal{M}, t^{</em>}: \leftarrow t, t^{*}: \leftarrow m$, <br> $T \leftarrow$ "subject $<em 1="1">{1}:$ description $</em>, \ldots$, subject $<em k="k">{k}:$ description $</em>, x$ "</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Causal Tracing (Meng et al., 2022a) <br> Attention: Knockout (Wang et al., 2022; <br> Conery et al., 2023; Geva et al., 2023)</td>
<td style="text-align: center;">$\boldsymbol{\aleph}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$t^{*}: \leftarrow t, T \leftarrow S \circ \epsilon, \epsilon \sim \mathcal{N}(0, \sigma)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{\aleph}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$t^{*}: \leftarrow$ Multiple, $f \leftarrow 0, T \leftarrow S$</td>
</tr>
<tr>
<td style="text-align: center;">Inspection <br> Application</td>
<td style="text-align: center;">Early Exiting, e.g., Linear Shortcuts (Din et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$t^{<em>}: \leftarrow L^{</em>}, f \leftarrow$ Affine</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Caption Generation, e.g., Linear Mapping (Merullo et al., 2022)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">For learning mappings</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{M}^{<em>}: \leftarrow$ A language model of choice, $t^{</em>}: \leftarrow L^{*}, f \leftarrow$ Affine</td>
</tr>
</tbody>
</table>
<h1>B. Next-Token Prediction Additional Details and Experimental Results</h1>
<p>In this section, we provide more information about the models, data, and additional target prompts for few-shot token identity Patchscope configurations. It is worth highlighting that after patching, we continue computations from the target layer onward. Therefore, since in this Patchscope, source layer and target layer are the same, there are going to be more computations compared to baseline methods like LogitLens (nostalgebraist, 2020; Geva et al., 2022a) and Tuned Lens (Belrose et al., 2023; Din et al., 2023) where the target layer is fixed at the final layer.</p>
<h2>B.1. Models</h2>
<p>We use LLaMA2 (13B) ${ }^{4}$ (Touvron et al., 2023b), Vicuna (13B) ${ }^{5}$ (Chiang et al., 2023), GPT-J (6B) (Wang \&amp; Komatsuzaki, 2021) ${ }^{6}$, and Pythia (12B) ${ }^{7}$ (Biderman et al., 2023). LLaMA2 was pre-trained on 2T tokens from a mix of publicly available data. Vicuna is a LLaMA1 (Touvron et al., 2023a) model that was pre-trained on 1T tokens and fine-tuned on 70K</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Next-token prediction results for LLaMA2, using various token identity demonstrations (the token IDs appear in the legend). We report precision@1 ( $\uparrow$ is better), and surprisal ( $\downarrow$ is better).
user-shared conversations ${ }^{8}$. The primary architectural differences between LLaMA2 and Vicuna (LLaMA1) include a different context length and grouped-query attention. Pythia and GPT-J were pre-trained using a deduplicated version of the Pile corpus ${ }^{9}$ (Gao et al., 2020), and for about 300B and 402B tokens, respectively.</p>
<h1>B.2. Training and Evaluation Data</h1>
<p>We use 12,000 random samples from the Pile, partitioned into 10,000 examples for training the affine mappings, and 2,000 examples for evaluation. In our pre-processing strategy, we introduce randomness in the patching positions by trimming the input sequence length of each example.</p>
<h2>B.3. Additional Few-Shot Token Identity Prompts</h2>
<p>In this section, we provide additional details about the selection of the demonstrations for the token identity baseline, and further evaluate the robustness of LLaMA2 (13B) (Touvron et al., 2023b) to various token identity prompts.</p>
<p>Demonstrations Construction For the demonstrations used in this experiment, we sample a random set of $k$ tokens for all the models, where $k$ was also randomly sampled from the interval $[1, \ldots, 10]$.</p>
<p>Robustness to Additional token IDs' Demonstrations We randomly generate five realizations of token IDs series of varying lengths, formatted as "tok ${ }<em 1="1">{1} \rightarrow$ tok $</em>$; tok $<em 2="2">{2} \rightarrow$ tok $</em> ; \ldots$; tok $<em k="k">{k}$ ", similarly to the procedure from $\S 4.1$. We also include an ablation experiment, introducing the Single Token Prompt baseline. In this approach, the representation is transferred from the original multi-token prompt to a single-token prompt, while preserving the same position. This is crucial for maintaining consistent positional biases during the computation in the forward pass. The baseline is implemented by creating a target prompt in the format: "[PAD] [PAD] ... [PAD] tok $</em>$ " retains its original position from the source prompt.}$ ". Here, the number of [PAD] tokens is chosen such that "tok $_{k</p>
<p>The results are illustrated in Fig. 5, where a comprehensive overview of the evaluation metrics can be found in $\S 4.1$. The results indicate the stability of the token identity baseline across a range of token identity demonstrations, particularly notable in the upper layers of the model. The Single Token Prompt baseline overall seems to be less effective than using our token ID baseline.</p>
<p>Table 5. Comparison between the zero-shot feature extraction Patchscope and the logistic regression probe shows that despite using no training data, the Patchscope has a significantly higher accuracy than the baseline in most tasks $(p&lt;1 e-5)$. Pairwise t-statistics and the corresponding p-values are included in the table. The columns corresponding to each method show accuracy (mean $\pm$ std).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Probe</th>
<th style="text-align: center;">Patchscope</th>
<th style="text-align: center;">T-statistic</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fruit inside color</td>
<td style="text-align: center;">$37.41 \pm 6.58$</td>
<td style="text-align: center;">$37.99 \pm 18.67$</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.901</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fruit outside color</td>
<td style="text-align: center;">$35.50 \pm 3.09$</td>
<td style="text-align: center;">$71.00 \pm 13.26^{<em> </em>}$</td>
<td style="text-align: center;">12.426</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Object superclass</td>
<td style="text-align: center;">$68.92 \pm 10.69^{<em> </em>}$</td>
<td style="text-align: center;">$55.71 \pm 10.81$</td>
<td style="text-align: center;">$-5.25$</td>
<td style="text-align: center;">$&lt;1 e-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Substance phase</td>
<td style="text-align: center;">$73.77 \pm 3.74$</td>
<td style="text-align: center;">$91.92 \pm 1.73^{<em> </em>}$</td>
<td style="text-align: center;">25.647</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Task done by person</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$62.96 \pm 16.513^{<em> </em>}$</td>
<td style="text-align: center;">19.632</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Task done by tool</td>
<td style="text-align: center;">$10.14 \pm 3.23$</td>
<td style="text-align: center;">$48.12 \pm 13.23^{<em> </em>}$</td>
<td style="text-align: center;">18.231</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Work location</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$13.58 \pm 9.37^{<em> </em>}$</td>
<td style="text-align: center;">7.45990</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Company CEO</td>
<td style="text-align: center;">$4.99 \pm 2.56$</td>
<td style="text-align: center;">$47.82 \pm 13.89^{<em> </em>}$</td>
<td style="text-align: center;">16.700</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Country capital city</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$61.61 \pm 14.14^{<em> </em>}$</td>
<td style="text-align: center;">22.426</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Country currency</td>
<td style="text-align: center;">$17.70 \pm 2.20$</td>
<td style="text-align: center;">$50.95 \pm 8.85^{<em> </em>}$</td>
<td style="text-align: center;">20.293</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Country largest city</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$67.78 \pm 11.47^{<em> </em>}$</td>
<td style="text-align: center;">30.427</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Food from country</td>
<td style="text-align: center;">$5.13 \pm 3.66$</td>
<td style="text-align: center;">$63.80 \pm 11.34^{<em> </em>}$</td>
<td style="text-align: center;">26.710</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Person father</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$25.34 \pm 8.42^{<em> </em>}$</td>
<td style="text-align: center;">15.482</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Person plays position in sport</td>
<td style="text-align: center;">$75.89 \pm 9.14$</td>
<td style="text-align: center;">$72.20 \pm 7.21$</td>
<td style="text-align: center;">$-2.066$</td>
<td style="text-align: center;">0.049</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Person plays pro sport</td>
<td style="text-align: center;">$53.87 \pm 10.28$</td>
<td style="text-align: center;">$46.28 \pm 14.19$</td>
<td style="text-align: center;">$-2.020$</td>
<td style="text-align: center;">0.054</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Product by company</td>
<td style="text-align: center;">$58.91 \pm 7.15$</td>
<td style="text-align: center;">$63.24 \pm 10.74$</td>
<td style="text-align: center;">1.757</td>
<td style="text-align: center;">0.091</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Star constellation</td>
<td style="text-align: center;">$17.54 \pm 5.30$</td>
<td style="text-align: center;">$18.35 \pm 5.06$</td>
<td style="text-align: center;">$-2.98$</td>
<td style="text-align: center;">0.006</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Superhero archnemesis</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$41.73 \pm 18.72^{<em> </em>}$</td>
<td style="text-align: center;">11.47044</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Superhero person</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$28.32 \pm 14.05^{<em> </em>}$</td>
<td style="text-align: center;">10.37461</td>
<td style="text-align: center;">$&lt;1 e-5$</td>
</tr>
</tbody>
</table>
<h1>C. More Details on Attribute Extraction Experiments</h1>
<p>Dataset Details We start from the factual and commonsense reasoning subsets introduced by Hernandez et al. (2023b) ${ }^{10}$. This dataset includes 8 commonsense and 25 factual relations. Recall that each datapoint is representing a triplet $(\sigma, \rho, \omega)$, where $\sigma$ refers to the subject, $\rho$ represents the relation, and $\omega$ stands for the object (see 4.2 for notation details). For each data point representing the triplet $(\sigma, \rho, \omega)$, we sample 5 utterances from Wikitext-103 dataset (Merity et al., 2016) including $\sigma$. We then truncate the sampled text to a window of random length up to 20 tokens that contains $\sigma$. This constitutes our source prompt, $S$. Note that for each model, we filter the data to samples for which the underlying model correctly encodes the tuple. For experiments with zero-shot target prompt $T$ that includes $\rho$ followed by $\sigma$, we autoregressively generate the next 20 tokens, and only keep examples where $\omega$ appears in the generation. For experiments with few-shot demonstrations in $T$, after generating the next 20 tokens, we do an additional post-processing step. If the demonstration template of an example is identified in the generation, all the following tokens would be dropped. The example is used for evaluation only if $\omega$ appears in the post-processed truncated generation. To have a reasonable amount of data for training the classification probe baseline, tasks with fewer than 15 datapoints are dropped from the analysis. For GPT-J, 5 commonsense and 7 factual reasoning tasks remain after applying the above postprocessing steps.</p>
<p>Results on Additional Tasks We provide additional results on various factual and commonsense reasoning tasks. For a comparison of the zero-shot feature extraction Patchscope and the logistic regression probe averaged across layers, see t-statistic details in Tab. 5.</p>
<p>Performance Breakdown Across Source Layers Fig. 6 depicts how accuracy changes across source layers $\ell \in[1, \ldots, L]$. Considering the early layers, we observe that the Patchscope consistently outperforms the baseline. This further confirms our hypothesis that prior methods are particularly limited in surfacing information in the early layers, which often cannot be decoded via linear functions. However, Patchscope is able to extract such attributes significantly earlier. Note that this observation does not mean the attribute is explicitly encoded in a representation, but that there is enough information encoded such that the attribute can be extracted from the representation alone, without its original context, using the model's computation. In the middle layers, Patchscope works similarly or better than the baseline. Interestingly, we observe</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that almost all cases where Patchscope performs worse than the baseline occur in later layers. Our interpretation is that given the language modeling training objective, the representations shift toward next-token prediction in the later layers. Therefore, the attribute of interest would not be as readily accessible via the model's computation in these layers. This interpretation is also aligned with recent findings that show no decline in using linear relational embedding in predicting $\omega$ only when the next token also happens to be $\omega$ (Hernandez et al., 2023b). We postulate that when the immediate next token is not the attribute of interest, it does not necessarily mean the attribute information is lost, but rather it may not be as easily accessible on the surface. We hypothesize that using a Patchscope with a more expressive mapping $f$ could improve attribute extraction accuracy in the later layers, which we leave for future work to verify.</p>
<p>Source-Target Layer Interplay Fig. 7 visualizes the interaction between $\ell$ and $\ell^{<em>}$. These heatmaps show attribute extraction success rate for the zero-shot feature extraction Patchscope for a fixed $\left(\ell, \ell^{</em>}\right)$ combination. The lower left quadrants show setups where both $\ell$ and $\ell^{<em>}$ represent early to middle layers, and the success rate is maximal. The right half of the heatmaps represents late $\ell$, which achieves lower success rate due to token representations shifting toward next-token prediction as discussed earlier. In addition, we notice lower success rate in the top half of the heatmaps which represents late $\ell^{</em>}$. It is worth noting that in this task, the accuracy is not only based on the immediate next-token prediction, but rather whether $\omega$ appears in the next 20 autoregressively generated tokens. The placeholder token " $x$ " does still remain in the input, and its representation persists in the early layers in the target computation. This explains why a lower attribute extraction rate is observed in later $\ell^{<em>}$ values. We leave it to the future work to investigate adaptations to Patchscopes to control contamination from the placeholder tokens and make them more amenable to late $\ell^{</em>}$ choices.</p>
<p>Statistical Test Details The null hypothesis is: "There is no significant difference between probing accuracy and patchscope accuracy". We calculate attribute extraction accuracy per layer, in a 40-layer model, across various tasks. Since we run a test for each task independently (see Tab. 2), we use the conservative Bonferroni correction to address the multiple comparison problem. That is, for a desired overall $\alpha=0.05$, we consider $\alpha /$ number of tasks $=\frac{0.05}{12}=0.004$ for each individual task.</p>
<h1>D. Additional Information and Results on the Entity Resolution Experiment</h1>
<h2>D.1. Experimental Setup</h2>
<p>Recall that we use a few-shot target prompt template for decoding an entity description: "subject 1 : description $<em 2="2">{1}$, subject $</em>:$ description $<em k="k">{2}, \ldots$, subject $</em>$ python package for obtaining a description of every subject entity from Wikipedia.}:$ description $_{k}$, x", while patching the last position which corresponds to " $x$ ". Specifically, we use the following target prompt, obtained randomly: "Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, x" and task the model to generate the completion after the patched representation in " $x$ ". For the subject description, which is composed of $k=3$ random subject entities, we used the wptools ${ }^{11</p>
<h2>D.2. Additional Quantitative Results</h2>
<p>In this section, we present the Rouge1 (Lin, 2004) and SBERT score (Reimers \&amp; Gurevych, 2019) results, as well as the results for the Pythia models (Biderman et al., 2023). ${ }^{12}$ In Fig. 8 and Fig. 9, we present the Rouge1 and the SBERT results, respectively, complementary to the RougeL results in Fig. 3 from $\S 4.3$. Note that for Pythia, the smaller model (6.9B) outperforms the larger one (12B), and hence, our cross-model patching method would not be able to improve the inspection of the smaller model, unlike the trends we observed for Vicuna.</p>
<h2>D.3. Additional Qualitative Results</h2>
<p>In this section, we provide more examples and discuss our observations about the gradual process of entity resolution. As shown in Tab. 6 and Tab. 7, it is interesting to observe that the resolution process for the same input can look different across models, suggesting they assign different likelihoods to different entities, and weigh context differently. For example,</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Feature extraction accuracy with respect to source layer $(\ell)$ across various factual and commonsense reasoning tasks. The zero-shot feature extraction Patchscope works consistently better than the logistic regression probe in early layers, and mostly in mid layers. There is a decline in Patchscope accuracy in later $\ell$ as the source representations shift toward next-token prediction.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. The interaction between source and target layers in the zero-shot feature extraction Patchscope across various factual and commonsense reasoning tasks. Each cell $\left(\ell, \ell^{<em>}\right)$ in the heatmap shows the attribute extraction success rate where source and target layers are fixed to $\ell$ and $\ell^{</em>}$, respectively. Particularly, there is a higher success rate in the lower left quadrants, representing early to mid source and target layer combinations. The right half of the heatmaps shows late source layers, where the source representation has shifted toward next-token prediction, leading to a lower success rate in attribute extraction. The top half of the heatmaps shows late target layers. When the accuracy is a function of more than a single next-token, the placeholder token representation still remains in the early layers, leading to a lower attribute extraction rate.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Rouge1 scores of the generated descriptions against descriptions from Wikipedia.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. SBERT scores of the generated descriptions against descriptions from Wikipedia.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Precision@1 scores ( $\uparrow$ is better) on next-token prediction estimation in Vicuna and Pythia with cross-model P at chscopes
as Vicuna 13B processes "Will Smith", it goes from "Smithsonian Museum" to the "Smith rock band" to the actor and rapper, "Will Smith". However, Pythia 12B starts with "Smith \&amp; Wesson weapon manufacturing company" before it resolves the entity as the American actor, "Will Smith".</p>
<p>We also observe another phenomenon which we refer to as placeholder contamination. It is the case where the remaining representation of the placeholder entity " $x$ " in the early layers interferes with the model's capability in generating descriptions for the patched token. For example, see Vicuna 13B response to "Paris Hilton" entity in Tab. 6. First, we see the gradual process of going from "Rigatoni" pasta, to "Hilton Hotels", to the socialite "Paris Hilton" in layers 1-6. But in layers 7 and onward, the generation seems to describe the placeholder token " $x$ "rather than the "Paris Hilton" entity: "Placeholder for a variable or concept", "Variable representing any number of things or concepts" or " $x$ is a placeholder". For future, we would like to quantify these qualitative observations, study to what extend this contamination can be mitigated with a different placeholder choice, and why some models might be more susceptible to this contamination than others.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ https://github.com/siznax/wptools/
${ }^{12}$ We used the package from https://www.sbert.net/, with the sentence-transformers/all-MiniLM-L6-v2 model.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
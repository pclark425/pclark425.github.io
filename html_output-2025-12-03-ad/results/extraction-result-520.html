<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-520 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-520</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-520</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-6c858b48b42e234e743f8e75bcc9c29e8a9d8b14</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c858b48b42e234e743f8e75bcc9c29e8a9d8b14" target="_blank">A Neuro-Symbolic ASP Pipeline for Visual Question Answering</a></p>
                <p><strong>Paper Venue:</strong> Theory and Practice of Logic Programming</p>
                <p><strong>Paper TL;DR:</strong> The experiments show that the non-deterministic scene encoding achieves good results even if the neural networks are trained rather poorly in comparison with the deterministic approach, important for building robust VQA systems if network predictions are less-than perfect.</p>
                <p><strong>Paper Abstract:</strong> Abstract We present a neuro-symbolic visual question answering (VQA) pipeline for CLEVR, which is a well-known dataset that consists of pictures showing scenes with objects and questions related to them. Our pipeline covers (i) training neural networks for object classification and bounding-box prediction of the CLEVR scenes, (ii) statistical analysis on the distribution of prediction values of the neural networks to determine a threshold for high-confidence predictions, and (iii) a translation of CLEVR questions and network predictions that pass confidence thresholds into logic programmes so that we can compute the answers using an answer-set programming solver. By exploiting choice rules, we consider deterministic and non-deterministic scene encodings. Our experiments show that the non-deterministic scene encoding achieves good results even if the neural networks are trained rather poorly in comparison with the deterministic approach. This is important for building robust VQA systems if network predictions are less-than perfect. Furthermore, we show that restricting non-determinism to reasonable choices allows for more efficient implementations in comparison with related neuro-symbolic approaches without losing much accuracy.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e520.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e520.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLEVR functional programs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLEVR functional programs (symbolic question representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured, symbolic programs provided by the CLEVR dataset that encode questions as compositions of basic functions (scene, filter_*, count, union, relate_left/right/front/behind, query_*, etc.), representing procedural task structure independent of raw image data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a language model — procedural question structure is provided as explicit symbolic functional programs (no NLP component used in this work). These programs describe the sequence of symbolic operations required to answer a question (procedural knowledge encoded explicitly).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CLEVR Visual Question Answering (VQA) via symbolic execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a CLEVR image and an associated functional program (symbolic question), produce the correct answer by executing the program against an explicit object-level scene representation; environment: synthetic CLEVR scenes with annotated object attributes and coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / multi-step symbolic reasoning (procedural)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (procedural programs specify sequences of operations over sets of objects and their attributes)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit symbolic program templates provided by CLEVR (dataset), not learned</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>direct translation of provided functional program into ASP facts (symbolic translation / program-to-facts)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic procedures encoded as ASP facts referencing inputs/outputs of functions (indices), enabling symbolic execution by ASP solver</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>answer correctness rate on CLEVR (percentage of questions answered correctly), plus runtime of ASP solver</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Pipeline-level results reported in paper (examples): deterministic scene encoding correct rates range from 42.33% to 97.01% depending on detector training and bounding-box threshold; non-deterministic encoding (various alpha) improves correctness (e.g., alpha=2.0: up to 84.09% on poorly trained detector and up to 96.94% on well-trained detector in reported settings). See paper tables for full breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables exact, stepwise execution of multi-step questions (count, filter, union, intersection, uniqueness, attribute and integer comparisons) because the question semantics are explicit; procedural control flow is precise and transparent to the symbolic reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Depends on correctness/completeness of the symbolic program and of the scene facts; if scene objects are missing or mis-classified and deterministic scene encoding is used, the program can produce wrong or no answers; procedural program alone cannot correct perceptual errors without additional scene hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared implicitly to learning-based end-to-end VQA systems in related work; within this paper, pipeline variants differ only in scene encoding (deterministic vs non-deterministic) rather than alternate procedural encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not an ablation of the programs themselves, but experiments vary scene-encoding strategy (deterministic vs non-deterministic) and confidence-threshold parameter alpha; raising alpha (allowing more candidate object classes) increases correct answer rates but also impacts runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using explicit procedural representations (CLEVR functional programs) translated to ASP facts provides a clean way to encode multi-step procedures; when paired with symbolic reasoning, procedural knowledge can be used robustly even when perception is uncertain (if scene encoding allows alternative hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Neuro-Symbolic ASP Pipeline for Visual Question Answering', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e520.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e520.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASP scene encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answer-Set Programming scene encoding (deterministic and non-deterministic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ASP-based representation of detected scene objects as obj/7 facts (object id, class attributes, bounding box coords) combined with deterministic (single best class) or non-deterministic (choice rules over several class hypotheses plus weak constraints) encodings for scene interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a neural language model — the reasoning module is an ASP solver (clingo) that operates on symbolic object facts produced from perception; the ASP encoding is tailored to express object-relational facts, spatial relations, choice among hypotheses, and optimization over weak constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Symbolic VQA reasoning on CLEVR scenes</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compute answers to symbolic CLEVR programs by reasoning over ASP-encoded object facts (attributes and coordinates) and relations; environment: static scene graphs (symbolic facts) derived from object detector outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational reasoning + spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial (explicit object attributes and pairwise spatial relations), also procedural insofar as functional programs drive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit symbolic facts derived from neural detector outputs and dataset annotations; spatial coordinates come from bounding box predictions</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>translation of object-detector outputs (selected by confidence-threshold) into ASP facts; non-determinism encoded via choice rules and weak constraints</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic facts (obj/7), choice rules representing alternative object-class hypotheses, weak constraints encoding preference weights derived from class confidence scores, and numeric coordinate comparisons for spatial relations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>answer correctness rate (%) and total runtime (seconds) of VQA pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Deterministic scene encoding runtimes ~81–89s (for 15k questions) with correct-answer rates from ~42% to ~97% depending on detector quality and bbox threshold; non-deterministic encoding (with thresholding) yields higher correctness (e.g., alpha=2.0 correct up to 84.09% on poorly trained detectors) but runtime grows with more hypotheses (observed jumps when alpha allows many choices). See Tables 2 and 4.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Object-relational queries (attribute queries, counts, set operations) and many spatial queries succeed because objects and coordinates are explicitly represented; non-deterministic hypotheses let the reasoner recover from perception mistakes (e.g., misclassified color) by considering alternative class assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Unrestricted non-determinism (including all 96 classes per detection) causes combinatorial explosion and large runtimes; deterministic encoding fails (wrong/no-answer) when top-1 visual predictions are incorrect and no alternative hypotheses are considered.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to NeurASP and ProbLog: NeurASP (unrestricted) yields slightly higher correct rates but much larger runtimes (e.g., NeurASP runtimes thousands of seconds vs ~100s for thresholded non-deterministic ASP); ProbLog comparable to NeurASP in trend but also expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Varying the confidence-rate parameter alpha shows trade-offs: lower alpha (fewer hypotheses) reduces runtime but can reduce correctness; higher alpha increases correctness until a point where runtime sharply increases (alpha=2.5 caused big runtime jump when many classes included). Also varying bounding-box threshold and detector training epochs demonstrates sensitivity to perceptual quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoding object-relational and spatial information symbolically in ASP (with choice rules for uncertainty) enables robust reasoning without direct sensory processing in the reasoner; controlling non-determinism via a statistical confidence threshold (mean - alpha*std) is critical to balance robustness and tractable reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Neuro-Symbolic ASP Pipeline for Visual Question Answering', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e520.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e520.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Choice rules & weak constraints</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-deterministic choice-rule encoding with weak-constraint weighting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of ASP choice rules to represent multiple candidate object-class assignments per detection and weak constraints weighted by -ln(confidence) to prefer higher-confidence choices; enables the ASP solver to search over plausible scene interpretations and select most plausible answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A as a language model; this is a symbolic mechanism: choice rules ({obj(...); ...}=1 :- scene(O)) generate alternate scene hypotheses; weak constraints assign costs corresponding to detector confidences so answer-set optimisation ranks hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Robust symbolic inference under perceptual uncertainty (CLEVR VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given multiple candidate class labels per detected bounding box, the ASP solver uses choice rules to select exactly one class per object and weak constraints to rank hypotheses by total negative log-confidence, then executes the procedural program to obtain answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>uncertainty-aware object-relational reasoning / multi-hypothesis inference</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (class labels and attributes) and probabilistic/uncertainty encoding (preferences over hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>class-confidence scores from YOLOv3 detector (converted to weak-constraint weights), and selection thresholding based on validation distribution</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>generate choice rules from detector outputs filtered by a confidence threshold theta = max(mu - alpha*sigma, 0); add weak constraints with weights w_c = min(-1000 * ln(s), 5000) per candidate</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>symbolic alternative hypotheses represented as disjunctions/choice rules; uncertainty encoded via weak-constraints with weights derived from class confidence scores (preference encoding rather than probability calculus)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>correctness rate (%) and runtime (seconds) for VQA over sets of questions; number of answer sets and optimisation cost used to pick best hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Non-deterministic encoding increases correct-answer rates compared to deterministic encoding across detector qualities (e.g., on poorly trained detector, deterministic correct 64.39% vs non-deterministic alpha=1.5 correct 80.61% for bbox=0.25), while runtimes remain modest when candidate sets are restricted by the confidence threshold (total runtimes ~100–160s for 15k questions); unrestricted choice over all 96 classes (as in unconstrained NeurASP) yields much larger runtimes (thousands of seconds).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Recovers correct answers when top-1 detector label is wrong but the true label is among candidate hypotheses; weak constraints bias towards higher-confidence interpretations so the solver prefers plausible scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When many low-confidence alternatives pass the threshold (large alpha or low threshold), combinatorial search explodes, increasing runtime drastically; if the true label is not among candidates (k fall-back too small), correct answer cannot be derived.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to NeurASP (which includes all 96 classes in choices) and ProbLog (neural-annotated disjunctions interpreted probabilistically): unrestricted approaches slightly improve accuracy but at heavy runtime cost; restricting choices via confidence threshold yields a better efficiency-accuracy tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Vary alpha (confidence-rate) and fallback k: with small alpha (fewer classes) faster but lower robustness; increasing alpha increases correct rates but can sharply increase runtime (alpha=2.5 caused large runtime increase in some settings). Using deterministic (single best class) is fastest but less robust.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoding perceptual uncertainty symbolically as choice rules plus weighted weak constraints lets a symbolic reasoner perform hypothesis-driven inference without sensory processing, improving robustness to perception errors; however, controlling the number of hypotheses is essential to avoid intractable search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Neuro-Symbolic ASP Pipeline for Visual Question Answering', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e520.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e520.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial-relation ASP rules</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit spatial-relation encoding in ASP (relate_left/right/front/behind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spatial relations are encoded by ASP rules that compare numerical bounding-box coordinates (e.g., X1 < X1') to determine left/right/front/behind relations between objects, allowing the symbolic reasoner to answer spatial queries without raw sensory processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A (not a language model). Spatial knowledge is represented symbolically by coordinate comparisons in ASP predicates derived from detector bounding boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial relation reasoning in CLEVR (symbolic spatial queries)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer spatial queries (e.g., 'objects left of X') by evaluating ASP rules that inspect object bounding-box coordinates encoded as arguments in obj/7 facts; environment: symbolic scene graphs with numeric coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial reasoning / object-relational queries</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (calculated relative positions using coordinates, plus attributes of objects)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>bounding-box coordinates from YOLOv3 object detector (numerical values used in ASP comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>coordinates embedded into obj(...) facts; spatial-relate rules use arithmetic comparisons in ASP bodies (e.g., X1 < X1') to derive relational obj(...) facts</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit numeric coordinates in symbolic facts and relational rules that compute left/right/front/behind via coordinate inequalities</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>correctness on spatial queries within CLEVR functional programs (contributes to overall question-answer correctness) and runtime</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Spatial-relation queries are handled accurately when object detections (including bounding-box coordinates) are correct; overall pipeline correctness improves when non-deterministic encoding is used to handle class uncertainty, but spatial correctness still depends on detection recall and bbox threshold (see Tables 1–2).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Precise relative-position queries succeed because of direct coordinate comparisons; combining spatial-relational rules with attribute filters supports complex queries like 'big brown object of same shape as green thing' via composition.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If bounding boxes are missing (false negative) or coordinates are noisy such that comparisons flip relational order, spatial queries can be wrong; the ASP reasoner cannot correct for missing/incorrect detections unless alternative hypotheses include additional objects.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Implicitly compared to neural end-to-end spatial reasoning approaches in related work; within this paper, success depends on detector quality and scene-encoding strategy (deterministic vs non-deterministic).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not a direct ablation of spatial rules, but experiments show that looser bounding-box thresholds (more detections) increase recall and improve success rates for spatial queries at the cost of more false positives that must be handled by ASP constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit numeric-coordinate based rules in ASP allow symbolic reasoning over spatial relations without direct sensory processing in the reasoner; correctness of such symbolic spatial knowledge depends critically on the upstream perception (detection recall and bbox accuracy) and benefits from multi-hypothesis scene encodings to tolerate perceptual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Neuro-Symbolic ASP Pipeline for Visual Question Answering', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>NeurASP: Embracing neural networks into answer set programming <em>(Rating: 2)</em></li>
                <li>DeepProbLog: Neural probabilistic logic programming <em>(Rating: 2)</em></li>
                <li>Neural-symbolic VQA: Disentangling reasoning from vision and language understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-520",
    "paper_id": "paper-6c858b48b42e234e743f8e75bcc9c29e8a9d8b14",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "CLEVR functional programs",
            "name_full": "CLEVR functional programs (symbolic question representation)",
            "brief_description": "Structured, symbolic programs provided by the CLEVR dataset that encode questions as compositions of basic functions (scene, filter_*, count, union, relate_left/right/front/behind, query_*, etc.), representing procedural task structure independent of raw image data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_description": "Not a language model — procedural question structure is provided as explicit symbolic functional programs (no NLP component used in this work). These programs describe the sequence of symbolic operations required to answer a question (procedural knowledge encoded explicitly).",
            "task_name": "CLEVR Visual Question Answering (VQA) via symbolic execution",
            "task_description": "Given a CLEVR image and an associated functional program (symbolic question), produce the correct answer by executing the program against an explicit object-level scene representation; environment: synthetic CLEVR scenes with annotated object attributes and coordinates.",
            "task_type": "instruction following / multi-step symbolic reasoning (procedural)",
            "knowledge_type": "procedural + object-relational (procedural programs specify sequences of operations over sets of objects and their attributes)",
            "knowledge_source": "explicit symbolic program templates provided by CLEVR (dataset), not learned",
            "has_direct_sensory_input": false,
            "elicitation_method": "direct translation of provided functional program into ASP facts (symbolic translation / program-to-facts)",
            "knowledge_representation": "explicit symbolic procedures encoded as ASP facts referencing inputs/outputs of functions (indices), enabling symbolic execution by ASP solver",
            "performance_metric": "answer correctness rate on CLEVR (percentage of questions answered correctly), plus runtime of ASP solver",
            "performance_result": "Pipeline-level results reported in paper (examples): deterministic scene encoding correct rates range from 42.33% to 97.01% depending on detector training and bounding-box threshold; non-deterministic encoding (various alpha) improves correctness (e.g., alpha=2.0: up to 84.09% on poorly trained detector and up to 96.94% on well-trained detector in reported settings). See paper tables for full breakdown.",
            "success_patterns": "Enables exact, stepwise execution of multi-step questions (count, filter, union, intersection, uniqueness, attribute and integer comparisons) because the question semantics are explicit; procedural control flow is precise and transparent to the symbolic reasoner.",
            "failure_patterns": "Depends on correctness/completeness of the symbolic program and of the scene facts; if scene objects are missing or mis-classified and deterministic scene encoding is used, the program can produce wrong or no answers; procedural program alone cannot correct perceptual errors without additional scene hypotheses.",
            "baseline_comparison": "Compared implicitly to learning-based end-to-end VQA systems in related work; within this paper, pipeline variants differ only in scene encoding (deterministic vs non-deterministic) rather than alternate procedural encodings.",
            "ablation_results": "Not an ablation of the programs themselves, but experiments vary scene-encoding strategy (deterministic vs non-deterministic) and confidence-threshold parameter alpha; raising alpha (allowing more candidate object classes) increases correct answer rates but also impacts runtime.",
            "key_findings": "Using explicit procedural representations (CLEVR functional programs) translated to ASP facts provides a clean way to encode multi-step procedures; when paired with symbolic reasoning, procedural knowledge can be used robustly even when perception is uncertain (if scene encoding allows alternative hypotheses).",
            "uuid": "e520.0",
            "source_info": {
                "paper_title": "A Neuro-Symbolic ASP Pipeline for Visual Question Answering",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "ASP scene encoding",
            "name_full": "Answer-Set Programming scene encoding (deterministic and non-deterministic)",
            "brief_description": "An ASP-based representation of detected scene objects as obj/7 facts (object id, class attributes, bounding box coords) combined with deterministic (single best class) or non-deterministic (choice rules over several class hypotheses plus weak constraints) encodings for scene interpretation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_description": "Not a neural language model — the reasoning module is an ASP solver (clingo) that operates on symbolic object facts produced from perception; the ASP encoding is tailored to express object-relational facts, spatial relations, choice among hypotheses, and optimization over weak constraints.",
            "task_name": "Symbolic VQA reasoning on CLEVR scenes",
            "task_description": "Compute answers to symbolic CLEVR programs by reasoning over ASP-encoded object facts (attributes and coordinates) and relations; environment: static scene graphs (symbolic facts) derived from object detector outputs.",
            "task_type": "object-relational reasoning + spatial reasoning",
            "knowledge_type": "object-relational + spatial (explicit object attributes and pairwise spatial relations), also procedural insofar as functional programs drive reasoning",
            "knowledge_source": "explicit symbolic facts derived from neural detector outputs and dataset annotations; spatial coordinates come from bounding box predictions",
            "has_direct_sensory_input": false,
            "elicitation_method": "translation of object-detector outputs (selected by confidence-threshold) into ASP facts; non-determinism encoded via choice rules and weak constraints",
            "knowledge_representation": "explicit symbolic facts (obj/7), choice rules representing alternative object-class hypotheses, weak constraints encoding preference weights derived from class confidence scores, and numeric coordinate comparisons for spatial relations",
            "performance_metric": "answer correctness rate (%) and total runtime (seconds) of VQA pipeline",
            "performance_result": "Deterministic scene encoding runtimes ~81–89s (for 15k questions) with correct-answer rates from ~42% to ~97% depending on detector quality and bbox threshold; non-deterministic encoding (with thresholding) yields higher correctness (e.g., alpha=2.0 correct up to 84.09% on poorly trained detectors) but runtime grows with more hypotheses (observed jumps when alpha allows many choices). See Tables 2 and 4.",
            "success_patterns": "Object-relational queries (attribute queries, counts, set operations) and many spatial queries succeed because objects and coordinates are explicitly represented; non-deterministic hypotheses let the reasoner recover from perception mistakes (e.g., misclassified color) by considering alternative class assignments.",
            "failure_patterns": "Unrestricted non-determinism (including all 96 classes per detection) causes combinatorial explosion and large runtimes; deterministic encoding fails (wrong/no-answer) when top-1 visual predictions are incorrect and no alternative hypotheses are considered.",
            "baseline_comparison": "Compared to NeurASP and ProbLog: NeurASP (unrestricted) yields slightly higher correct rates but much larger runtimes (e.g., NeurASP runtimes thousands of seconds vs ~100s for thresholded non-deterministic ASP); ProbLog comparable to NeurASP in trend but also expensive.",
            "ablation_results": "Varying the confidence-rate parameter alpha shows trade-offs: lower alpha (fewer hypotheses) reduces runtime but can reduce correctness; higher alpha increases correctness until a point where runtime sharply increases (alpha=2.5 caused big runtime jump when many classes included). Also varying bounding-box threshold and detector training epochs demonstrates sensitivity to perceptual quality.",
            "key_findings": "Encoding object-relational and spatial information symbolically in ASP (with choice rules for uncertainty) enables robust reasoning without direct sensory processing in the reasoner; controlling non-determinism via a statistical confidence threshold (mean - alpha*std) is critical to balance robustness and tractable reasoning.",
            "uuid": "e520.1",
            "source_info": {
                "paper_title": "A Neuro-Symbolic ASP Pipeline for Visual Question Answering",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Choice rules & weak constraints",
            "name_full": "Non-deterministic choice-rule encoding with weak-constraint weighting",
            "brief_description": "Use of ASP choice rules to represent multiple candidate object-class assignments per detection and weak constraints weighted by -ln(confidence) to prefer higher-confidence choices; enables the ASP solver to search over plausible scene interpretations and select most plausible answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_description": "N/A as a language model; this is a symbolic mechanism: choice rules ({obj(...); ...}=1 :- scene(O)) generate alternate scene hypotheses; weak constraints assign costs corresponding to detector confidences so answer-set optimisation ranks hypotheses.",
            "task_name": "Robust symbolic inference under perceptual uncertainty (CLEVR VQA)",
            "task_description": "Given multiple candidate class labels per detected bounding box, the ASP solver uses choice rules to select exactly one class per object and weak constraints to rank hypotheses by total negative log-confidence, then executes the procedural program to obtain answers.",
            "task_type": "uncertainty-aware object-relational reasoning / multi-hypothesis inference",
            "knowledge_type": "object-relational (class labels and attributes) and probabilistic/uncertainty encoding (preferences over hypotheses)",
            "knowledge_source": "class-confidence scores from YOLOv3 detector (converted to weak-constraint weights), and selection thresholding based on validation distribution",
            "has_direct_sensory_input": false,
            "elicitation_method": "generate choice rules from detector outputs filtered by a confidence threshold theta = max(mu - alpha*sigma, 0); add weak constraints with weights w_c = min(-1000 * ln(s), 5000) per candidate",
            "knowledge_representation": "symbolic alternative hypotheses represented as disjunctions/choice rules; uncertainty encoded via weak-constraints with weights derived from class confidence scores (preference encoding rather than probability calculus)",
            "performance_metric": "correctness rate (%) and runtime (seconds) for VQA over sets of questions; number of answer sets and optimisation cost used to pick best hypothesis",
            "performance_result": "Non-deterministic encoding increases correct-answer rates compared to deterministic encoding across detector qualities (e.g., on poorly trained detector, deterministic correct 64.39% vs non-deterministic alpha=1.5 correct 80.61% for bbox=0.25), while runtimes remain modest when candidate sets are restricted by the confidence threshold (total runtimes ~100–160s for 15k questions); unrestricted choice over all 96 classes (as in unconstrained NeurASP) yields much larger runtimes (thousands of seconds).",
            "success_patterns": "Recovers correct answers when top-1 detector label is wrong but the true label is among candidate hypotheses; weak constraints bias towards higher-confidence interpretations so the solver prefers plausible scenes.",
            "failure_patterns": "When many low-confidence alternatives pass the threshold (large alpha or low threshold), combinatorial search explodes, increasing runtime drastically; if the true label is not among candidates (k fall-back too small), correct answer cannot be derived.",
            "baseline_comparison": "Compared to NeurASP (which includes all 96 classes in choices) and ProbLog (neural-annotated disjunctions interpreted probabilistically): unrestricted approaches slightly improve accuracy but at heavy runtime cost; restricting choices via confidence threshold yields a better efficiency-accuracy tradeoff.",
            "ablation_results": "Vary alpha (confidence-rate) and fallback k: with small alpha (fewer classes) faster but lower robustness; increasing alpha increases correct rates but can sharply increase runtime (alpha=2.5 caused large runtime increase in some settings). Using deterministic (single best class) is fastest but less robust.",
            "key_findings": "Encoding perceptual uncertainty symbolically as choice rules plus weighted weak constraints lets a symbolic reasoner perform hypothesis-driven inference without sensory processing, improving robustness to perception errors; however, controlling the number of hypotheses is essential to avoid intractable search.",
            "uuid": "e520.2",
            "source_info": {
                "paper_title": "A Neuro-Symbolic ASP Pipeline for Visual Question Answering",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Spatial-relation ASP rules",
            "name_full": "Explicit spatial-relation encoding in ASP (relate_left/right/front/behind)",
            "brief_description": "Spatial relations are encoded by ASP rules that compare numerical bounding-box coordinates (e.g., X1 &lt; X1') to determine left/right/front/behind relations between objects, allowing the symbolic reasoner to answer spatial queries without raw sensory processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_description": "N/A (not a language model). Spatial knowledge is represented symbolically by coordinate comparisons in ASP predicates derived from detector bounding boxes.",
            "task_name": "Spatial relation reasoning in CLEVR (symbolic spatial queries)",
            "task_description": "Answer spatial queries (e.g., 'objects left of X') by evaluating ASP rules that inspect object bounding-box coordinates encoded as arguments in obj/7 facts; environment: symbolic scene graphs with numeric coordinates.",
            "task_type": "spatial reasoning / object-relational queries",
            "knowledge_type": "spatial + object-relational (calculated relative positions using coordinates, plus attributes of objects)",
            "knowledge_source": "bounding-box coordinates from YOLOv3 object detector (numerical values used in ASP comparisons)",
            "has_direct_sensory_input": false,
            "elicitation_method": "coordinates embedded into obj(...) facts; spatial-relate rules use arithmetic comparisons in ASP bodies (e.g., X1 &lt; X1') to derive relational obj(...) facts",
            "knowledge_representation": "explicit numeric coordinates in symbolic facts and relational rules that compute left/right/front/behind via coordinate inequalities",
            "performance_metric": "correctness on spatial queries within CLEVR functional programs (contributes to overall question-answer correctness) and runtime",
            "performance_result": "Spatial-relation queries are handled accurately when object detections (including bounding-box coordinates) are correct; overall pipeline correctness improves when non-deterministic encoding is used to handle class uncertainty, but spatial correctness still depends on detection recall and bbox threshold (see Tables 1–2).",
            "success_patterns": "Precise relative-position queries succeed because of direct coordinate comparisons; combining spatial-relational rules with attribute filters supports complex queries like 'big brown object of same shape as green thing' via composition.",
            "failure_patterns": "If bounding boxes are missing (false negative) or coordinates are noisy such that comparisons flip relational order, spatial queries can be wrong; the ASP reasoner cannot correct for missing/incorrect detections unless alternative hypotheses include additional objects.",
            "baseline_comparison": "Implicitly compared to neural end-to-end spatial reasoning approaches in related work; within this paper, success depends on detector quality and scene-encoding strategy (deterministic vs non-deterministic).",
            "ablation_results": "Not a direct ablation of spatial rules, but experiments show that looser bounding-box thresholds (more detections) increase recall and improve success rates for spatial queries at the cost of more false positives that must be handled by ASP constraints.",
            "key_findings": "Explicit numeric-coordinate based rules in ASP allow symbolic reasoning over spatial relations without direct sensory processing in the reasoner; correctness of such symbolic spatial knowledge depends critically on the upstream perception (detection recall and bbox accuracy) and benefits from multi-hypothesis scene encodings to tolerate perceptual errors.",
            "uuid": "e520.3",
            "source_info": {
                "paper_title": "A Neuro-Symbolic ASP Pipeline for Visual Question Answering",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "NeurASP: Embracing neural networks into answer set programming",
            "rating": 2
        },
        {
            "paper_title": "DeepProbLog: Neural probabilistic logic programming",
            "rating": 2
        },
        {
            "paper_title": "Neural-symbolic VQA: Disentangling reasoning from vision and language understanding",
            "rating": 1
        }
    ],
    "cost": 0.012858249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Neuro-Symbolic ASP Pipeline for Visual Question Answering*</h1>
<p>THOMAS EITER, NELSON HIGUERA, JOHANNES OETSCH, MICHAEL PRITZ<br>Institute of Logic and Computation, Vienna University of Technology (TU Wien), Austria<br>{eiter,higuera,oetsch,pritz}@kr.tuwien.ac.at</p>
<h4>Abstract</h4>
<p>We present a neuro-symbolic visual question answering (VQA) pipeline for CLEVR, which is a wellknown dataset that consists of pictures showing scenes with objects and questions related to them. Our pipeline covers (i) training neural networks for object classification and bounding-box prediction of the CLEVR scenes, (ii) statistical analysis on the distribution of prediction values of the neural networks to determine a threshold for high-confidence predictions, and (iii) a translation of CLEVR questions and network predictions that pass confidence thresholds into logic programs so that we can compute the answers using an ASP solver. By exploiting choice rules, we consider deterministic and non-deterministic scene encodings. Our experiments show that the non-deterministic scene encoding achieves good results even if the neural networks are trained rather poorly in comparison with the deterministic approach. This is important for building robust VQA systems if network predictions are less-than perfect. Furthermore, we show that restricting non-determinism to reasonable choices allows for more efficient implementations in comparison with related neuro-symbolic approaches without loosing much accuracy. This work is under consideration for acceptance in TPLP.</p>
<p>KEYWORDS: answer-set programming, visual question answering, neuro-symbolic computation</p>
<h2>1 Introduction</h2>
<p>The goal in visual question answering (VQA) (Antol et al. 2015) is to find the answer to a question using information from a scene. A system must understand the question, extract the relevant information from the corresponding scene, and perform some kind of reasoning. Neuro-symbolic approaches are useful in this regard as they combine deep learning, which can be used for perception (e.g., object detection or natural language processing), with symbolic reasoning ( Xu et al. 2018; Manhaeve et al. 2018; Yi et al. 2018; Yang et al. 2020; Basu et al. 2020; Mao et al. 2019). As the semantics of the employed reasoning formalism is known, the way in which an answer is reached is transparent.</p>
<p>We present a neuro-symbolic VQA pipeline for the CLEVR dataset (Johnson et al. 2017) that combines deep neural networks for perception and answer-set programming (ASP) (Brewka et al. 2011) to implement the reasoning part. The system is publicly available at</p>
<div class="codehilite"><pre><span></span><code>https://github.com/Macehil/nesy-asp-vqa-pipeline
</code></pre></div>

<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ASP offers a simple yet expressive modelling language and efficient solver technology. It is in particular attractive for this task as it allows to easily express non-determinism, preferences, and defaults. The scene encoding in the ASP program makes use of non-deterministic choice rules for the objects predicted with high confidence by the network. This means that we do not only consider the prediction with the highest score, but also reasonable alternatives with lower ones. This allows our approach to make up for mistakes made in object classification in the reasoning component as the constraints in the program exclude choices that do not lead to an answer.</p>
<p>For illustration, assume a scene with one red cylinder and the question "What shape is the red object?". Furthermore, assume that the neural network wrongly gives the cylinder a higher score for being blue than red. The ASP constraints enforce that an answer is derived. This entails that the right choice for the colour of the object must be red, and the correct answer "cylinder" is produced in the end.</p>
<p>While non-determinism improves the robustness of the VQA system, the downside is that, in case there are many object classes and the system is used with no restriction, it can negatively impact the reasoning performance in terms of run time. The objective of this paper is to introduce a new method for restricting non-determinism that is sensitive to how well networks have been trained such that efficient reasoning is facilitated.</p>
<p>While several datasets have been published to examine the strengths and weaknesses of VQA systems (Malinowski and Fritz 2014; Antol et al. 2015; Ren et al. 2015; Zhu et al. 2016; Johnson et al. 2017; Sampat et al. 2021), CLEVR is an ideal test bed for the purposes of this paper, since it is simple, well known, has detailed annotations describing the kind of reasoning each question requires, and focuses on basic object detection.</p>
<p>We in fact omit natural language processing for the VQA tasks because this would add a further variable and is not the focus of this work which is reasoning on top of object detection. Instead, we use functional programs which are structured representations of the natural language questions already provided by CLEVR.</p>
<p>Our VQA pipeline for consists of the following stages:
(1) Training neural networks for object classification and bounding-box prediction of the CLEVR scenes using the object detection framework YOLOv3 (Redmon and Farhadi 2018).
(2) Statistical analysis on the distribution of prediction values of the neural networks to determine a threshold for high-confidence predictions defined as a function of mean and standard deviation of the distribution.
(3) Translating CLEVR questions and network predictions that pass confidence thresholds into logic programs so that we can compute the answers using an ASP solver.</p>
<p>To determine what we regard as predictions of high confidence, we use statistical analysis on the distribution of network prediction values and disregard predictions that are below a threshold in Stage (2).</p>
<p>Our non-deterministic scene encoding approach is inspired by the neuro-symbolic systems NeurASP (Yang et al. 2020) and DeepProbLog (Manhaeve et al. 2018). DeepProbLog uses ProbLog, a probabilistic extension of Prolog, for reasoning. There, the output of the neural network is passed to the logic program using neural-annotated disjunctions. NeurASP uses the same bridging idea, now called neural atoms, but uses ASP instead of ProbLog. Similar to our approach, neural atoms are translated into choice rules. NeurASP and DeepProbLog take multiple network predictions into account, in fact they are designed to consider all network predictions</p>
<p>which are treated as a probability distribution. This can, as our experiments confirm, become a performance bottleneck if there are many object classes. Both systems feature closed-loop reasoning, i.e., the outcome of the reasoning system can be back-propagated into the neural networks to facilitate better learning. Our pipeline is however uni-directional, as our goal is to explore the interplay between non-determinism and confidence thresholds regarding efficiency and robustness of the reasoning component. We leave the learning component for future work as a scalable implementation is not trivial in our setting and thus outside the scope of this paper.</p>
<p>We compare NeurASP and the reasoning component of DeepProbLog with our approach on the CLEVR data. Indeed, limiting non-determinism of neural network outputs in ASP programs to reasonable choices leads to a drastic performance improvement in terms of run time with only little loss in accuracy and is thus important for efficient reasoning. Furthermore, our experiments show that our system performs well even if the neural networks are trained rather poorly and predictions by the network are less-than perfect. This is important for robust reasoning as even well-trained networks can be negatively affected by noise or if settings like illumination change.</p>
<p>The remainder of this paper is organised as follows. We first review ASP and CLEVR in Section 2 Our VQA pipeline using ASP and confidence-thresholds is detailed in Section 3. Afterwards, we present an experimental evaluation of our approach in Section 4, discuss further relevant related work in Section 5, and conclude in Section 6.</p>
<h1>2 Background</h1>
<p>We next provide preliminaries on ASP and background on the CLEVR dataset.</p>
<h3>2.1 Answer-Set Programming</h3>
<p>Answer-set programming (ASP) is a declarative problem solving paradigm, where a problem is encoded as a logic program such that its answer sets (which are special models) correspond to the solutions of the problem and are computable using ASP solvers, e.g., from potassco.org or www.dlvsystem.com. We just briefly recall important ASP concepts; and refer for more details to the literature Brewka et al. 2011, Gebser et al. 2012.</p>
<p>An ASP program is a finite set of rules $r$ of the form</p>
<p>$$
a_{1}|\ldots| a_{k}:-b_{1}, \ldots, b_{m}, \text { not } c_{1}, \ldots, \text { not } c_{n}, \quad k, m, n \geq 0
$$</p>
<p>where all $a_{i}, b_{j}, c_{l}$ are first-order atoms and not is default negation; we denote by $H(r)=$ $\left{a_{1}, \ldots, a_{k}\right}$ and $B(r)=B^{+}(r) \cup\left{\right.$ not $\left.c_{j} \mid c_{j} \in B^{-}(r)\right}$ the head and body of $r$, respectively, where $B^{+}(r)=\left{b_{1}, \ldots, b_{m}\right}$ and $B^{-}(r)=\left{c_{1}, \ldots, c_{n}\right}$. Intuitively, $r$ says that if all atoms in $B^{+}(r)$ are true and there is no evidence that some atom in $B^{-}(r)$ is true, then some atom in $H(r)$ must be true. If $m=n=0$ and $k=1$, then $r$ is a fact (with :- omitted); if $k=0, r$ is a constraint.</p>
<p>An interpretation $I$ is a set of ground (i.e., variable-free) atoms. It satisfies a ground rule $r$ if $H(r) \cap I \neq \emptyset$ whenever $I \subseteq B^{+}(r)$ and $I \cap B^{-}(r)=\emptyset ; I$ is a model of a ground program $P$ if $I$ satisfies each $r \in P$, and $I$ is an answer-set of $P$ if in addition no $J \subset I$ is a model of the Gelfond-Lifschitz reduct of $P$ w.r.t. $I$ (Gelfond and Lifschitz 1991).</p>
<p>Models and answer sets of a program $P$ with variables are defined in terms of the grounding of $P$ (replace each rule by its possible instances over the Herbrand universe).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
$\mathbf{Q}$ : Is there a big brown object of the same shape as the green thing?
A: Yes
<img alt="img-1.jpeg" src="img-1.jpeg" />
$\mathbf{Q}$ : How many large things are either cyan metallic cylinders or yellow blocks?
A: 0
<img alt="img-2.jpeg" src="img-2.jpeg" />
$\mathbf{Q}$ : The tiny shiny cylinder has what color?
A: Brown</p>
<p>Fig. 1: Three scenes and question-answer pairs from the CLEVR validation set.</p>
<p>We will also use choice rules and weak constraints, which are of the respective forms</p>
<dl>
<dt>$$</dt>
<dt>\begin{aligned}</dt>
<dt>i\left{a_{1} ; \ldots ; a_{n}\right} j: &amp; -b_{1}, \ldots, b_{m}, \text { not } c_{1}, \ldots, \text { not } c_{n} \</dt>
<dd>\sim b_{1}, \ldots, b_{m}, \text { not } c_{1}, \ldots, \text { not } c_{n} . &amp; {[w, t]}
\end{aligned}
$$</dd>
</dl>
<p>Informally, (2) says that when the body is satisfied, at least $i$ and at most $j$ atoms from $\left{a_{1}, \ldots, a_{n}\right}$ must be true in an answer set $I$, while (3) contributes tuple $t$ with costs $w$, which is an integer number, to a cost function, when the body is satisfied in $I$, rather than to eliminate $I$; the answer set $I$ is optimal, if the total cost of all such tuples is minimal.</p>
<h1>2.2 The CLEVR Dataset</h1>
<p>CLEVR (Johnson et al. 2017) is a dataset designed to test and diagnose the reasoning capabilities of VQA systems. ${ }^{1}$ It consists of pictures showing scenes with objects and questions related to them; there are about ten questions per image. The dataset was created with the goal of minimising biases in the data, since some VQA systems are suspected to exploit them to find answers instead of actually reasoning about the question and scene information (Johnson et al. 2017).</p>
<p>Each CLEVR image depicts a scene with objects in it. The objects differ by the values of their attributes, which are size (big, small), color (brown, blue, cyan, gray, green, purple, red, yellow), material (metal, rubber), and shape (cube, cylinder, sphere). Every image comes with a ground truth scene graph describing the scene depicted in it. Figure 1 contains three images from the CLEVR validation dataset with corresponding questions.</p>
<p>In CLEVR, questions are constructed using functional programs that represent the questions in a structured format. These programs are symbolic templates for a question which are instantiated with the corresponding values. For each such question template, there are one or more natural language sentences to which they are mapped. For illustration, the question "How many large things are either cyan metallic cylinders or yellow blocks?" from Fig. 1 can be represented by the functional program shown in Fig. 2. There, function scene() returns the set of objects of the scene, the filter_* functions restrict a set of objects to subsets with respective properties, union() yields the union of two sets, and count() finally returns the number of elements of a set. A detailed description of functional programs in CLEVR can be found in the dataset documentation (Johnson et al. 2017).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 2: CLEVR functional program representing the question: "How many large things are either cyan metallic cylinders or yellow blocks?".
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 3: Overview of our neuro-symbolic VQA pipeline.</p>
<h1>3 The VQA Pipeline</h1>
<p>The architecture of our neuro-symbolic VQA pipeline that builds on object detection and ASP solving is depicted in Fig. 3. A particular VQA task, which consists of a CLEVR scene and question, is translated to an ASP program given predictions from a neural network for object detection, a functional program, and a confidence threshold; by running an ASP solver, the answer to the CLEVR task is then figured out.</p>
<p>Before going into details of our VQA pipeline, we recapitulate the necessary stages of establishing it for the CLEVR dataset:</p>
<ol>
<li>Object detection: we train neural networks for bounding-box prediction and object classification of the CLEVR scenes;</li>
<li>Confidence thresholds: we determine a threshold for network predictions that we consider to be of high confidence by statistical analysis on the distribution of prediction values of the neural networks;</li>
<li>ASP encoding: we translate CLEVR functional programs that represent questions as well as network predictions that pass confidence thresholds into ASP programs and use an ASP solver to compute the answers.</li>
</ol>
<p>While the VQA tasks are designed to always have a unique answer, the ASP solver may give multiple results that correspond to alternative interpretations of the scene through the object detection network.</p>
<h1>3.1 Object Detection</h1>
<p>We use YOLOv3 (Redmon and Farhadi 2018) for bounding-box prediction and object classification, adopting that the object detector's output is a matrix whose rows correspond to the bounding-box predictions in the input picture. Each bounding-box prediction is a vector of the form $\left(c_{1}, \ldots, c_{n}, x_{1}, y_{1}, x_{2}, y_{2}\right)$, where the pairs $\left(x_{1}, y_{1}\right)$ and $\left(x_{2}, y_{2}\right)$ give the top-left and bottom-right corner point of the bounding box, respectively, and $c_{1}, \ldots, c_{n}$ are class confidence scores with $c_{i} \in[0,1]$ for $1 \leq i \leq n$; as customary, higher confidence scores represent higher confidence of a correct prediction. Each $c_{i}$ represents the score for a specific combination of object attributes size, color, material and shape and their respective values; we call this combination the object class of position $i$. For any object class $c$, let $\bar{c}$ be the list size, shape, material, color of its attribute values. For example, assume $c$ is the object class "large red metallic cylinder", then $\bar{c}=$ large, cylinder, metallic, red. In total, there are $n=96$ object classes in CLEVR.</p>
<p>Every row of the prediction matrix has also its own bounding-box confidence score. The number of bounding-box predictions of the object detection system depends on the bounding-box threshold, which is a hyper-parameter used to filter out rows with a low confidence score. For example, setting this threshold to 0.5 discards all predictions with confidence score below 0.5 .</p>
<h3>3.2 Confidence Thresholds</h3>
<p>Given class confidence scores $c_{1}, \ldots, c_{n}$ from an object detection prediction, we would like to focus on classifications that have reasonable high confidence and discard others with low confidence for the subsequent reasoning process. Using a fixed threshold hardly achieves this, since it does not take the distribution of confidence scores in the application area (or validation data for experiments) into account. Our approach solves this problem by fixing the threshold based on the mean and the standard deviation of prediction scores.</p>
<p>More formally, given a list of prediction matrices $\boldsymbol{X}^{1}, \ldots, \boldsymbol{X}^{m}$, where any $\boldsymbol{X}^{\boldsymbol{i}}$ is of dimension $N^{i} \times M, N^{i}$ is the number of bounding box predictions in the input image $i$, each of which is described by $M$ features. We compute the mean $\mu$ and standard deviation $\sigma$ for the maximum class confidence scores:</p>
<p>$$
\begin{aligned}
&amp; \mu=\frac{1}{\sum_{k=1}^{m} N^{k}} \sum_{k=1}^{m} \sum_{i=1}^{N^{k}} \max <em i_="i," j="j">{1 \leq j \leq n}\left(\boldsymbol{X}</em>\right) \
&amp; \sigma=\sqrt{\frac{1}{\sum_{k=1}^{m} N^{k}} \sum_{k=1}^{m} \sum_{i=1}^{N^{k}}}\left(\max }^{k<em i_="i," j="j">{1 \leq j \leq n}\left(\boldsymbol{X}</em>
\end{aligned}
$$}^{k}\right)-\mu\right)^{2</p>
<p>We suggest computing these values on the validation dataset used in training the object detector. Then, we define the confidence threshold $\theta$ that determines what is considered a confident class prediction as follows:</p>
<p>$$
\theta=\max (\mu-\alpha \cdot \sigma, 0)
$$</p>
<p>We consider class predictions as sufficiently confident if their confidence score is not lower than the mean minus $\alpha$ many standard deviations. The value for $\alpha$ in Eqn. (6) is a parameter we call the confidence rate. It must be provided and should depend on how well the network is trained: for a fixed $\alpha$, the number of class predictions that pass the threshold decreases if the network gets better trained as the standard deviation becomes smaller. For a fixed network, the number of class predictions that pass the threshold decreases if $\alpha$ decreases and increases otherwise.</p>
<h1>3.3 ASP Encoding</h1>
<p>To solve VQA tasks, we rely on ASP to infer the right answer given the neural network output and a confidence threshold. We outline the details in the following.</p>
<h3>3.3.1 Question encoding</h3>
<p>The first step of our approach is to translate the functional program representing a natural language question into an ASP fact representation. We illustrate this for the question "How many large things are either cyan metallic cylinders or yellow blocks?" in Section 2.2. The respective functional program in Fig. 2 is encoded by the following ASP facts:</p>
<p>$$
\begin{gathered}
\operatorname{end}(8) \
\operatorname{count}(8,7) \
\text { filter_large }(7,6) \
\operatorname{union}(6,3,5) \
\text { filter_cylinder }(3,2) \
\text { filter_cyan }(2,1) \
\text { filter_metal }(1,0)
\end{gathered}
$$</p>
<p>filter_cube $(5,4)$.
filter_yellow $(4,0)$.
scene $(0)$</p>
<p>The structure of the functional program is encoded using indices that refer to output (first arguments) and input (remaining arguments) of the respective basic functions.</p>
<h3>3.3.2 Scene encoding</h3>
<p>Let $\boldsymbol{X}$ be a prediction matrix and $\theta$ be a confidence threshold as described in Section 3.2. Recall that the output of the basic CLEVR function scene() corresponds to the objects detected in the scene which in turn correspond to the individual rows of $\boldsymbol{X}$.</p>
<p>For a row $\boldsymbol{X}<em 1="1">{i}$ with confidence class scores $c</em>$ contains the $k$ classes with highest class confidence scores, where $k \in{1, \ldots, 96}$, is a fixed integer; intuitively, $k$ is a fall-back parameter to ensure that some classes are selected if all scores are low.}, \ldots, c_{n}$, set $C_{i}$ contains every object class $c_{j}$ with score greater or equal than $\theta$. If no such $c_{j}$ exists, then $C_{i</p>
<p>For every row $\boldsymbol{X}<em 1="1">{i}$ with bounding-box corners $\left(x</em>\right}$, we construct a choice rule of form}, y_{1}\right)$ and $\left(x_{2}, y_{2}\right)$, as well as $C_{i}=\left{c_{1}, \ldots c_{l</p>
<p>$$
\left{\operatorname{obj}\left(O, i, \bar{c}<em 1="1">{1}, x</em>}, y_{1}, x_{2}, y_{2}\right) ; \ldots ; \operatorname{obj}\left(O, i, \bar{c<em 1="1">{l}, x</em>(O)
$$}, y_{1}, x_{2}, y_{2}\right)\right}=1:-\operatorname{scene</p>
<p>Every object with sufficiently high confidence score will thus be considered for computing the final answer in a non-deterministic way. For every $c \in C_{i}$, we add the weak constraint</p>
<dl>
<dt>$$</dt>
<dd>\sim \operatorname{obj}\left(O, i, \bar{c}, x_{1}, y_{1}, x_{2}, y_{2}\right)\left[w_{c}, i\right]
$$</dd>
</dl>
<p>where the weight $w_{c}$ is defined as $\min (-1000 \cdot \ln (s), 5000)$, and $s$ is the class confidence score for $c$ in $\boldsymbol{X}<em i="i">{i}$. This approach, which comes from the NeurASP implementation, achieves that object selections are penalised by a weight which corresponds to the object's class confidence score. Resulting answer sets can thus be ordered according to the total confidence of the involved object predictions. We refer to this encoding as non-deterministic scene encoding, but we also consider the special case of a deterministic scene encoding where each $C</em>$ holds only the single object class with the highest confidence score.</p>
<h1>3.3.3 Encoding of the basic CLEVR functions</h1>
<p>We next present encodings for the remaining CLEVR functions.
Filter rules: The CLEVR filter rules restrict sets of objects. We only present the rule for filter_color(yellow); the rules for the other colours, materials, and shapes are analogous:</p>
<p>$$
\operatorname{obj}(T, I, \ldots, \text { yellow }, \ldots):-\text { filter_yellow }\left(T, T_{1}\right), \operatorname{obj}\left(T_{1}, I, \ldots, \text { yellow }, \ldots\right)
$$</p>
<p>The variables $T$ and $T_{1}$ are used to indicate output resp. input references; $I$ represents the object identifier. We omit arguments irrelevant for the particular filter functions.
Count rule: Function count() returns the number of elements of a given set. We encode it as follows:</p>
<p>$$
\operatorname{int}(T, V):-\operatorname{count}\left(T, T_{1}\right), # \operatorname{count}\left{I: \operatorname{obj}\left(T_{1}, I, \ldots\right)\right}=V
$$</p>
<p>Here, #count is an ASP aggregate function that computes the numbers of object identifiers referenced by variable $T_{1}$.
Rules for set operations: The two set operation functions in CLEVR are intersection and union. We present respective ASP rules for each of them:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{obj}(T, I, \ldots):-\operatorname{and}\left(T, T_{1}, T_{2}\right), \operatorname{obj}\left(T_{1}, I, \ldots\right), \operatorname{obj}\left(T_{2}, I, \ldots\right) \
&amp; \operatorname{obj}(T, I, \ldots):-\operatorname{or}\left(T, T_{1}, T_{2}\right), \operatorname{obj}\left(T_{1}, I, \ldots\right) \
&amp; \operatorname{obj}(T, I, \ldots):-\operatorname{or}\left(T, T_{1}, T_{2}\right), \operatorname{obj}\left(T_{2}, I, \ldots\right)
\end{aligned}
$$</p>
<p>Uniqueness constraint: The CLEVR function unique() is used to assert that there is exactly one input object, which is then propagated to the output. We encode this in ASP using a constraint to eliminate answer sets violating uniqueness and a rule for propagation:</p>
<p>$$
\begin{aligned}
&amp; :-\text { unique }\left(T, T_{1}\right), \operatorname{obj}\left(T_{1}, I, \ldots\right), \operatorname{obj}\left(T_{1}, I^{\prime}, \ldots\right), I \neq I^{\prime} \
&amp; \operatorname{obj}(T, \ldots):-\operatorname{unique}\left(T, T_{1}\right), \operatorname{obj}\left(T_{1}, \ldots\right)
\end{aligned}
$$</p>
<p>Spatial-relation rules: Several CLEVR functions allow to determine objects in a certain spatial relation with another object. We present the rule for identifying all objects that are left relative to a given reference; the rules for right, front, and behind, are analogous:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{obj}(T, I, \ldots):- \text { relate_left }\left(T, T_{1}, T_{2}\right), I \neq I^{\prime}, X_{1}&lt;X_{1}^{\prime}, \
&amp; \operatorname{obj}\left(T_{1}, I, \ldots, X_{1}, \ldots\right), \operatorname{obj}\left(T_{2}, I^{\prime}, \ldots, X_{1}^{\prime}, \ldots\right) .
\end{aligned}
$$</p>
<p>Exist rule: The exist() rule in CLEVR returns true if the referenced set of objects is not empty, and it returns false otherwise using default negation. Respective ASP rules look as follows:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{bool}(T, \text { true }):-\operatorname{exist}\left(T, T_{1}\right), \operatorname{obj}\left(T_{1}, \ldots\right) \
&amp; \operatorname{bool}(T, \text { false }):-\operatorname{exist}\left(T, T_{1}\right), \text { not } \operatorname{bool}(T, \text { true })
\end{aligned}
$$</p>
<p>Query rules: Query functions allow to return an attribute value of a referenced object. We present a rule to query for the size of an object; the rules for colour, material, and shape look similar:</p>
<p>$$
\operatorname{size}(T, \text { Size }):-\text { query_size }\left(T, T_{1}\right), \operatorname{obj}\left(T_{1}, \ldots, \text { Size }, \ldots\right)
$$</p>
<p>Same-attribute-relation rules: Similar to the spatial-relation functions, same-attribute-relation rules allow to select sets of objects if they agree on a specified attribute with a specified reference object. We illustrate the ASP encoding for the size attribute, the ones for colour, material and shape are defined with the necessary changes:</p>
<p>$$
\begin{gathered}
o b j(T, I, \ldots):-\text { same_size }\left(T, T_{1}, T_{2}\right), \text { obj }\left(T_{1}, I, \ldots, \text { Size }, \ldots\right) \
o b j\left(T_{2}, I^{\prime}, \ldots, \text { Size }, \ldots\right), I \neq I^{\prime}
\end{gathered}
$$</p>
<p>Integer-comparison rules: CLEVR supports the common relations for comparing integers like "equals", "less-than", and "greater-than". We present the ASP encoding for "equals":</p>
<p>$$
\begin{aligned}
&amp; \text { bool }(T, \text { true }):=\text { equal_integer }\left(T, T_{1}, T_{2}\right), \text { int }\left(T_{1}, V\right), \text { int }\left(T_{2}, V\right) \
&amp; \text { bool }(T, \text { false }):=\text { equal_integer }\left(T, T_{1}, T_{2}\right), \text { not bool }(T, \text { true })
\end{aligned}
$$</p>
<p>Attribute-comparison rules: To check whether two objects have the same attributes, like size, colour, material, or shape, CLEVR provides attribute-comparisons rules. The one for size can be represented in ASP as follows, the others are defined analogously:</p>
<p>$$
\begin{aligned}
&amp; \text { bool }(T, \text { true }):=\text { equal_size }\left(T, T_{1}, T_{2}\right), \operatorname{size}\left(T_{1}, V\right), \operatorname{size}\left(T_{2}, V\right) \
&amp; \text { bool }(T, \text { false }):=\text { equal_size }\left(T, T_{1}, T_{2}\right), \text { not bool }(V, \text { true })
\end{aligned}
$$</p>
<p>In addition to the rules above, we also use rules to derive the ans/1 atom that extracts the final answer for the encoded CLEVR question from the output of the basic function at the root of the computation:</p>
<p>$$
\begin{array}{ll}
\operatorname{ans}(V):-\operatorname{end}(T), \operatorname{size}(T, V) . &amp; \operatorname{ans}(V):-\operatorname{end}(T), \operatorname{shape}(T, V) \
\operatorname{ans}(V):-\operatorname{end}(T), \operatorname{color}(T, V) . &amp; \operatorname{ans}(V):-\operatorname{end}(T), \operatorname{bool}(T, V) \
\operatorname{ans}(V):-\operatorname{end}(T), \operatorname{material}(T, V) . &amp; \operatorname{ans}(V):-\operatorname{end}(T), \operatorname{int}(T, V) \
:-\operatorname{not} \operatorname{ans}(-)
\end{array}
$$</p>
<p>The last constraint enforces that at least one answer is derived.
Putting all together, to find an answer to a CLEVR question, we translate the corresponding functional program into its fact representation and join it with the rules from above. Each answer set then corresponds to a CLEVR answer founded in a particular choice for scene objects. For</p>
<p>Table 1: Precision and recall for object detection on CLEVR scenes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Epochs/Threshold</th>
<th style="text-align: center;">25/0.25</th>
<th style="text-align: center;">50/0.25</th>
<th style="text-align: center;">200/0.25</th>
<th style="text-align: center;">25/0.50</th>
<th style="text-align: center;">50/0.50</th>
<th style="text-align: center;">200/0.50</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">recall</td>
<td style="text-align: center;">$71.23 \%$</td>
<td style="text-align: center;">$96.39 \%$</td>
<td style="text-align: center;">$99.20 \%$</td>
<td style="text-align: center;">$44.41 \%$</td>
<td style="text-align: center;">$89.06 \%$</td>
<td style="text-align: center;">$98.03 \%$</td>
</tr>
<tr>
<td style="text-align: left;">precision</td>
<td style="text-align: center;">$83.77 \%$</td>
<td style="text-align: center;">$98.27 \%$</td>
<td style="text-align: center;">$99.58 \%$</td>
<td style="text-align: center;">$97.17 \%$</td>
<td style="text-align: center;">$99.28 \%$</td>
<td style="text-align: center;">$99.82 \%$</td>
</tr>
</tbody>
</table>
<p>the deterministic, resp., non-deterministic encoding, at most one, resp., multiple answer sets are possible; no answer set means imperfect object recognition. In case of multiple answer sets, we use answer-set optimisation over the weak constraints to determine the most plausible solution.</p>
<h1>4 Experiments on the CLEVR Dataset</h1>
<p>Recall that the parameters of our approach are (i) the bounding-box threshold for object detection, (ii) the confidence rate $\alpha$ for computing the confidence threshold as distance from the mean in terms of standard deviations, and (iii) $k$ as a fall-back parameter for object-class selection. We experimentally evaluated our approach on the CLEVR dataset to study the effects of different parameter settings. In particular, we study</p>
<ul>
<li>different bounding-box thresholds and training epochs for object detection,</li>
<li>how the deterministic scene encoding compares to the non-deterministic one, and</li>
<li>runtime performance of our approach in comparison with NeurASP and ProbLog.</li>
</ul>
<p>For the non-deterministic scene encoding, we consider different settings for $\alpha$ and set $k=1$.
We restricted our experiments to a sample of 15000 CLEVR questions as the systems NeurASP and ProbLog would exceed the memory limits on the unrestricted dataset.</p>
<p>All experiments were carried out on an Ubuntu (20.04.3 LTS) system with a 3.60 GHz Intel CPU, 16GiB of RAM, and an NVIDIA GeForce GTX 1080 GPU with 8GB of memory installed.</p>
<h3>4.1 Object-Detection Evaluation</h3>
<p>For object detection, we used an open-source implementation of YOLOv3 (Redmon and Farhadi 2018). ${ }^{2}$ The system was trained on 4000 CLEVR images with bounding box annotations, as suggested in related work (Yi et al. 2018). We used models trained for 25, 50 and 200 epochs, resp., to obtain different levels of training for the neural networks. For the bounding-box thresholds, we considered two settings, namely 0.25 and 0.50 . Table 1 summarises the results of the evaluation of how the networks of different training quality perform for detecting the objects in the CLEVR scenes. We report on precision and recall, which are defined as usual in terms of true positives (TP), false positives (FP), and false negatives (FN). A $T P$ (resp., $F P$ ) is a prediction that is correct (resp., incorrect) w.r.t. the scene annotations in CLEVR. An $F N$ is an object that exists according to the scene annotations, but there is no corresponding prediction.</p>
<p>As expected, our results show that the total number of FP and FN decreases for the better trained YOLOv3 models. Naturally, a low bounding-box threshold yields more FP detections,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Results for CLEVR question answering.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Epochs/Threshold</th>
<th style="text-align: left;">$25 / 0.25$</th>
<th style="text-align: left;">$50 / 0.25$</th>
<th style="text-align: left;">$200 / 0.25$</th>
<th style="text-align: left;">$25 / 0.50$</th>
<th style="text-align: left;">$50 / 0.50$</th>
<th style="text-align: left;">$200 / 0.50$</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1>Deterministic Scene Encoding</h1>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">correct</th>
<th style="text-align: right;">$64.39 \%$</th>
<th style="text-align: right;">$92.93 \%$</th>
<th style="text-align: right;">$97.01 \%$</th>
<th style="text-align: right;">$42.33 \%$</th>
<th style="text-align: right;">$82.79 \%$</th>
<th style="text-align: right;">$95.05 \%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">wrong</td>
<td style="text-align: right;">$17.60 \%$</td>
<td style="text-align: right;">$3.06 \%$</td>
<td style="text-align: right;">$1.11 \%$</td>
<td style="text-align: right;">$29.55 \%$</td>
<td style="text-align: right;">$8.65 \%$</td>
<td style="text-align: right;">$2.22 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">no answer</td>
<td style="text-align: right;">$18.01 \%$</td>
<td style="text-align: right;">$4.01 \%$</td>
<td style="text-align: right;">$1.88 \%$</td>
<td style="text-align: right;">$28.12 \%$</td>
<td style="text-align: right;">$8.56 \%$</td>
<td style="text-align: right;">$2.73 \%$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Non-Deterministic Scene Encoding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\alpha=0.5$</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">$74.12 \%$</td>
<td style="text-align: center;">$93.13 \%$</td>
<td style="text-align: center;">$96.94 \%$</td>
<td style="text-align: center;">$72.36 \%$</td>
<td style="text-align: center;">$92.54 \%$</td>
<td style="text-align: center;">$96.71 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">wrong</td>
<td style="text-align: center;">$12.29 \%$</td>
<td style="text-align: center;">$2.60 \%$</td>
<td style="text-align: center;">$1.01 \%$</td>
<td style="text-align: center;">$13.21 \%$</td>
<td style="text-align: center;">$2.98 \%$</td>
<td style="text-align: center;">$1.27 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">no answer</td>
<td style="text-align: center;">$13.59 \%$</td>
<td style="text-align: center;">$4.27 \%$</td>
<td style="text-align: center;">$2.05 \%$</td>
<td style="text-align: center;">$14.43 \%$</td>
<td style="text-align: center;">$4.48 \%$</td>
<td style="text-align: center;">$2.02 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha=1.0$</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">$76.17 \%$</td>
<td style="text-align: center;">$93.13 \%$</td>
<td style="text-align: center;">$96.94 \%$</td>
<td style="text-align: center;">$74.29 \%$</td>
<td style="text-align: center;">$92.54 \%$</td>
<td style="text-align: center;">$96.71 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">wrong</td>
<td style="text-align: center;">$12.53 \%$</td>
<td style="text-align: center;">$2.60 \%$</td>
<td style="text-align: center;">$1.01 \%$</td>
<td style="text-align: center;">$13.47 \%$</td>
<td style="text-align: center;">$2.98 \%$</td>
<td style="text-align: center;">$1.27 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">no answer</td>
<td style="text-align: center;">$11.30 \%$</td>
<td style="text-align: center;">$4.27 \%$</td>
<td style="text-align: center;">$2.05 \%$</td>
<td style="text-align: center;">$12.24 \%$</td>
<td style="text-align: center;">$4.48 \%$</td>
<td style="text-align: center;">$2.02 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha=1.5$</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">$80.61 \%$</td>
<td style="text-align: center;">$93.13 \%$</td>
<td style="text-align: center;">$96.94 \%$</td>
<td style="text-align: center;">$78.83 \%$</td>
<td style="text-align: center;">$92.55 \%$</td>
<td style="text-align: center;">$96.71 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">wrong</td>
<td style="text-align: center;">$13.02 \%$</td>
<td style="text-align: center;">$2.60 \%$</td>
<td style="text-align: center;">$1.01 \%$</td>
<td style="text-align: center;">$14.00 \%$</td>
<td style="text-align: center;">$2.98 \%$</td>
<td style="text-align: center;">$1.27 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">no answer</td>
<td style="text-align: center;">$6.37 \%$</td>
<td style="text-align: center;">$4.27 \%$</td>
<td style="text-align: center;">$2.05 \%$</td>
<td style="text-align: center;">$7.17 \%$</td>
<td style="text-align: center;">$4.47 \%$</td>
<td style="text-align: center;">$2.02 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha=2.0$</td>
<td style="text-align: center;">correct</td>
<td style="text-align: center;">$84.09 \%$</td>
<td style="text-align: center;">$93.17 \%$</td>
<td style="text-align: center;">$96.94 \%$</td>
<td style="text-align: center;">$82.65 \%$</td>
<td style="text-align: center;">$92.56 \%$</td>
<td style="text-align: center;">$96.71 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">wrong</td>
<td style="text-align: center;">$13.52 \%$</td>
<td style="text-align: center;">$2.60 \%$</td>
<td style="text-align: center;">$1.01 \%$</td>
<td style="text-align: center;">$14.53 \%$</td>
<td style="text-align: center;">$2.98 \%$</td>
<td style="text-align: center;">$1.27 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">no answer</td>
<td style="text-align: center;">$2.39 \%$</td>
<td style="text-align: center;">$4.23 \%$</td>
<td style="text-align: center;">$2.05 \%$</td>
<td style="text-align: center;">$2.82 \%$</td>
<td style="text-align: center;">$4.46 \%$</td>
<td style="text-align: center;">$2.02 \%$</td>
</tr>
</tbody>
</table>
<p>while the number of FN decreases. Setting the bounding-box threshold to a higher value usually leads to fewer FP but also more FN.</p>
<h3>4.2 Question-Answering Evaluation</h3>
<p>We used the ASP solver clingo (v. 5.5.1) to compute answer sets. ${ }^{3}$ Table 2 sheds light on the impact of the training level and bounding-box thresholds of the models on question answering for the deterministic and non-deterministic scene encodings. Our system yields either correct, incorrect, or no answers to the CLEVR questions, and we report respective rates. The nondeterministic scene encoding outperformed the deterministic approach for all settings of training epochs and bounding-box thresholds, and the rate of correct answers increases with larger $\alpha$. The differences are considerable if the networks are trained rather poorly and and become small or even disappear for well trained ones. It thus seems beneficial to consider more than one prediction of the object detection system if network predictions are less than perfect. Also, lower bounding-box thresholds lead to more correct results in all cases, especially for the deterministic encoding. Hence, being too selective in the object detection is counterproductive.</p>
<p>Table 3: Comparisons with NeurASP and ProbLog.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Epochs/Threshold</th>
<th style="text-align: right;">$25 / 0.25$</th>
<th style="text-align: right;">$50 / 0.25$</th>
<th style="text-align: right;">$200 / 0.25$</th>
<th style="text-align: right;">$25 / 0.50$</th>
<th style="text-align: right;">$50 / 0.50$</th>
<th style="text-align: right;">$200 / 0.50$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NeurASP</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">correct</td>
<td style="text-align: right;">$86.09 \%$</td>
<td style="text-align: right;">$96.74 \%$</td>
<td style="text-align: right;">$98.53 \%$</td>
<td style="text-align: right;">$84.87 \%$</td>
<td style="text-align: right;">$96.17 \%$</td>
<td style="text-align: right;">$98.20 \%$</td>
</tr>
<tr>
<td style="text-align: left;">wrong</td>
<td style="text-align: right;">$13.88 \%$</td>
<td style="text-align: right;">$3.21 \%$</td>
<td style="text-align: right;">$1.45 \%$</td>
<td style="text-align: right;">$15.09 \%$</td>
<td style="text-align: right;">$3.77 \%$</td>
<td style="text-align: right;">$1.77 \%$</td>
</tr>
<tr>
<td style="text-align: left;">no answer</td>
<td style="text-align: right;">$0.03 \%$</td>
<td style="text-align: right;">$0.05 \%$</td>
<td style="text-align: right;">$0.03 \%$</td>
<td style="text-align: right;">$0.04 \%$</td>
<td style="text-align: right;">$0.06 \%$</td>
<td style="text-align: right;">$0.03 \%$</td>
</tr>
<tr>
<td style="text-align: left;">NeurASP (best prediction)</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">correct</td>
<td style="text-align: right;">$75.63 \%$</td>
<td style="text-align: right;">$95.78 \%$</td>
<td style="text-align: right;">$98.33 \%$</td>
<td style="text-align: right;">$53.12 \%$</td>
<td style="text-align: right;">$88.21 \%$</td>
<td style="text-align: right;">$96.87 \%$</td>
</tr>
<tr>
<td style="text-align: left;">wrong</td>
<td style="text-align: right;">$23.51 \%$</td>
<td style="text-align: right;">$4.14 \%$</td>
<td style="text-align: right;">$1.63 \%$</td>
<td style="text-align: right;">$38.17 \%$</td>
<td style="text-align: right;">$11.30 \%$</td>
<td style="text-align: right;">$3.06 \%$</td>
</tr>
<tr>
<td style="text-align: left;">no answer</td>
<td style="text-align: right;">$0.87 \%$</td>
<td style="text-align: right;">$0.08 \%$</td>
<td style="text-align: right;">$0.03 \%$</td>
<td style="text-align: right;">$8.71 \%$</td>
<td style="text-align: right;">$0.49 \%$</td>
<td style="text-align: right;">$0.07 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ProbLog</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">correct</td>
<td style="text-align: right;">$83.84 \%$</td>
<td style="text-align: right;">$96.25 \%$</td>
<td style="text-align: right;">$98.33 \%$</td>
<td style="text-align: right;">$81.97 \%$</td>
<td style="text-align: right;">$95.27 \%$</td>
<td style="text-align: right;">$97.85 \%$</td>
</tr>
<tr>
<td style="text-align: left;">wrong</td>
<td style="text-align: right;">$14.46 \%$</td>
<td style="text-align: right;">$2.66 \%$</td>
<td style="text-align: right;">$1.14 \%$</td>
<td style="text-align: right;">$15.45 \%$</td>
<td style="text-align: right;">$3.13 \%$</td>
<td style="text-align: right;">$1.34 \%$</td>
</tr>
<tr>
<td style="text-align: left;">no answer</td>
<td style="text-align: right;">$1.70 \%$</td>
<td style="text-align: right;">$1.09 \%$</td>
<td style="text-align: right;">$0.53 \%$</td>
<td style="text-align: right;">$2.57 \%$</td>
<td style="text-align: right;">$1.59 \%$</td>
<td style="text-align: right;">$0.81 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of total VQA runtimes (in seconds).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Epochs/Threshold</th>
<th style="text-align: right;">$25 / 0.25$</th>
<th style="text-align: right;">$50 / 0.25$</th>
<th style="text-align: right;">$200 / 0.25$</th>
<th style="text-align: right;">$25 / 0.50$</th>
<th style="text-align: right;">$50 / 0.50$</th>
<th style="text-align: right;">$200 / 0.50$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NeurASP</td>
<td style="text-align: right;">9149</td>
<td style="text-align: right;">4375</td>
<td style="text-align: right;">4364</td>
<td style="text-align: right;">8750</td>
<td style="text-align: right;">4234</td>
<td style="text-align: right;">4694</td>
</tr>
<tr>
<td style="text-align: left;">NeurASP (best prediction)</td>
<td style="text-align: right;">3114</td>
<td style="text-align: right;">3628</td>
<td style="text-align: right;">3575</td>
<td style="text-align: right;">1245</td>
<td style="text-align: right;">3174</td>
<td style="text-align: right;">3577</td>
</tr>
<tr>
<td style="text-align: left;">ProbLog</td>
<td style="text-align: right;">6235</td>
<td style="text-align: right;">6894</td>
<td style="text-align: right;">6463</td>
<td style="text-align: right;">5311</td>
<td style="text-align: right;">5883</td>
<td style="text-align: right;">6138</td>
</tr>
<tr>
<td style="text-align: left;">Deterministic Scene Encoding</td>
<td style="text-align: right;">84</td>
<td style="text-align: right;">89</td>
<td style="text-align: right;">87</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">89</td>
<td style="text-align: right;">85</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">$\alpha=0.5$</td>
<td style="text-align: right;">112</td>
<td style="text-align: right;">123</td>
<td style="text-align: right;">130</td>
<td style="text-align: right;">109</td>
<td style="text-align: right;">124</td>
</tr>
<tr>
<td style="text-align: left;">Non-Det. Scene Encoding with</td>
<td style="text-align: right;">$\alpha=1.5$</td>
<td style="text-align: right;">141</td>
<td style="text-align: right;">125</td>
<td style="text-align: right;">126</td>
<td style="text-align: right;">140</td>
<td style="text-align: right;">125</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">$\alpha=2.0$</td>
<td style="text-align: right;">154</td>
<td style="text-align: right;">126</td>
<td style="text-align: right;">130</td>
<td style="text-align: right;">165</td>
<td style="text-align: right;">125</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">$\alpha=2.5$</td>
<td style="text-align: right;">3656</td>
<td style="text-align: right;">127</td>
<td style="text-align: right;">128</td>
<td style="text-align: right;">3541</td>
<td style="text-align: right;">123</td>
</tr>
</tbody>
</table>
<h1>4.3 Comparison with NeurASP and ProbLog</h1>
<p>The related systems NeurASP and DeepProbLog also embody the idea of non-determinism for object classifications, but they do not incorporate a mechanism to restrict object classes to ones with high confidence like in our approach; this can drag down performance considerably. We conducted different experiments to investigate the impact of limiting the number of object classes as in our approach on runtimes and accuracy of the questions answering task in comparison with the aforementioned related systems.</p>
<p>The choice rules in NeurASP always contain the 96 CLEVR object classes, whose scores come from the YOLOv3 network. In addition, we also used a setup for NeurASP where only the highest prediction is considered while the probabilities of all other atoms are set to 0 . While the former setting is more similar to our approach, the latter is the one used by Yang et al. (2020) in</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>their object detection example. Table 3 summarises the results on question answering accuracy, and Table 4 shows the total runtime for the different systems under consideration on the 15000 questions. NeurASP outperforms our approach in terms of correct answers as it does not restrict the number of atoms for the choice rules. However, this comes at a price as runtimes are much longer, which can be explained by the inflation of the search space due to the unrestricted choice rules. Also, the rate of incorrect answers is higher for NeurASP while our approach will more often remain agnostic when in doubt. For the non-deterministic encoding of our approach, we observe a similar jump in runtimes for $\alpha=2.5$ as more object classes are included in the choice rules.</p>
<p>We could not use DeepProbLog for CLEVR directly as annotated disjunctions that depend on a variable number of objects in a scene are not supported and would require some extensions. Instead, we evaluated a translation to ProbLog, as DeepProblog's inference component is essentially the one of Problog (Manhaeve et al. 2018). Recall that for NeurASP and DeepProbLog, neural network outputs are interpreted as probability distributions. While NeurASP does not strictly require this and works also in our setting, ProbLog is less lenient and network outputs need to be normalised so that the sum of their scores does not exceed 1. This does however not change results as only the relative order of the object-class scores is relevant for determining the most plausible answer. For the case of the unrestricted 96 object classes, computing results on our hardware was infeasible. We thus considered only the three object classes with highest confidence scores for every bounding-box prediction in our experiments. Results on runtimes and accuracy are therefore lower bounds for the unrestricted case. The picture for ProbLog is quite similar to that of NeurASP: While additional predictions help for question answering to some extent, this is at the cost of a considerable increase in runtime.</p>
<p>Overall, the experiments further support our belief that non-determinism is useful for neurosymbolic VQA systems and suitable mechanisms to restrict it do reasonable choices allows for more efficient implementations.</p>
<h1>5 Further Related Work</h1>
<p>Purely deep-learning-based approaches (Yang et al. 2016; Lu et al. 2016; Jabri et al. 2016) led to significant advances in VQA. Some systems rely on attention mechanisms to focus on certain features of the image and question to retrieve the answer (Yang et al. 2016; Lu et al. 2016). Jabri et al. (2016)) achieved good results by framing a VQA task as a classification problem. Some VQA systems are however suspected to not learn to reason but to exploit dataset biases to find answers, as described by Johnson et al. (2017).</p>
<p>Besides these purely data driven attempts, there are also systems which incorporate symbolic reasoning in combination with neural-based methods for VQA (Yi et al. 2018; Basu et al. 2020; Mao et al. 2019). The system proposed by Yi et al. (2018) consists of a scene parser, which retrieves object level scene information from images, a question parser, which creates symbolic programs from natural language questions, and a program executor that runs these programs on the abstract scene representation. Our system is akin to this system, but we use ASP for scene representation as well as question encoding, and our program executor is an ASP solver. A similar system architecture appears in the approach by Mao et al. (2019) with the difference that scene and question parsing is jointly learned from image and question-answer pairs, whereas the components of Yi et al.'s system are trained separately. This means that annotated images are not necessary for training, which makes the system more versatile. The approach of Basu et al. (2020) builds like</p>
<p>ours on ASP. They use object-level scene representations and parse natural language questions to obtain rules which represent the question. The answer to a question is given by the answer set for the image-question encoding which is combined with commonsense knowledge. However, their approach is not amenable to non-determinism for the scene encoding in order to deal with competing object classifications as we do.</p>
<p>Riley and Sridharan (2019) present an integrated approach for deep learning and ASP for representing incomplete commonsense knowledge and non-monotonic reasoning that also involves learning and program induction. They apply their approach to VQA tasks with explanatory questions and are able to achieve better accuracy on small data sets than end-to-end architectures. As in our work, they use neural networks to extract features of an image. We focus however more narrowly on the interface between the neural network outputs and the logical rules and turn network outputs into choice rules to further improve robustness, which has not been considered by Riley and Sridharan.</p>
<h1>6 Conclusion</h1>
<p>We have introduced a neuro-symbolic VQA pipeline for the CLEVR dataset based on a translation from CLEVR questions in the form of functional programs to ASP rules, where we proposed a non-deterministic and a deterministic approach to encode object-level scene information. Notably, non-determinism is restricted to network predictions that pass a confidence threshold determined by statistical analysis. It takes the variance of predication quality into account and can be adjusted by the novel confidence rate parameter $\alpha$, which supports control of non-determinism, resp. disjunctive information.</p>
<p>Our experiments confirm that, on the one hand, non-determinism is important for robustness of the reasoning component especially if the neural networks for object classification are not welltrained or predictions are negatively affected by other causes. On the other hand, unrestricted nondeterminism as featured by related neuro-symbolic systems can pose a performance bottleneck. Our method of using a confidence threshold is a viable compromise between quality of question answering and efficiency of the reasoning component. The insight that it makes sense to deal with uncertainty at the level of the reasoning component is in fact not restricted to ASP and therefore also provides directions to further improve related approaches.</p>
<p>While CLEVR is well-suited for the purposes of this work, the scenes are quite simple and do not require advanced features of ASP like expressing common-sense knowledge. For future work, we intend to apply our approach also to other datasets, especially ones that do not use synthetic scenes and where object classification might be harder, resulting in increased uncertainty. There, using additional domain knowledge and representing it with ASP could come in handy. Furthermore, we plan to extend our pipeline to closed-loop reasoning, i.e., using the output of the ASP solver also in the learning process.</p>
<h2>References</h2>
<p>Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. VQA: Visual question answering. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) 2015, pp. 2425-2433. IEEE.
Basu, K., Shakerin, F., and Gupta, G. AQuA: ASP-based visual question answering. In Proceedings</p>
<p>of the 22nd International Symposium on Practical Aspects of Declarative Languages (PADL 2020) 2020, volume 12007 of Lecture Notes in Computer Science, pp. 57-72. Springer.
Brewka, G., Eiter, T., and Truszczyński, M. 2011. Answer set programming at a glance. Communications of the ACM, 54, 12, 92-103.
Gebser, M., Kaminski, R., Kaufmann, B., and Schaub, T. 2012. Answer Set Solving in Practice, volume 6 of Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan \&amp; Claypool Publishers.
Gelfond, M. and Lifschitz, V. 1991. Classical negation in logic programs and disjunctive databases. New Generation Computing, 9, 3-4, 365-385.
Jabri, A., Joulin, A., and Van Der Maaten, L. Revisiting visual question answering baselines. In Proceedings of the 14th European Conference on Computer Vision (ECCV 2016) 2016, volume 9912 of Lecture Notes in Computer Science, pp. 727-739. Springer.
Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C. L., and Girshick, R. B. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017, pp. 1988-1997. IEEE.
Lu, J., Yang, J., Batra, D., and Parikh, D. Hierarchical question-image co-attention for visual question answering. In Advances in Neural Information Processing Systems (NIPS 2016) 2016, volume 29, pp. 289-297. Curran Associates, Inc.
Malinowski, M. and Fritz, M. A multi-world approach to question answering about real-world scenes based on uncertain input. In Advances in Neural Information Processing Systems (NIPS 2014) 2014, volume 27, pp. 1682-1690. Curran Associates, Inc.
Manhaeve, R., Dumancic, S., Kimmig, A., Demeester, T., and Raedt, L. D. DeepProbLog: Neural probabilistic logic programming. In Advances in Neural Information Processing Systems (NeurIPS 2018) 2018, volume 31, 3753-3763.</p>
<p>Mao, J., Gan, C., Kohli, P., Tenenbaum, J. B., and Wu, J. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In Proceedings of the 7th International Conference on Learning Representations (ICLR 2019) 2019.
Redmon, J. and Farhadi, A. 2018. YOLOv3: An incremental improvement. arXiv preprint arXiv:1804.02767, abs/1804.02767.
Ren, M., Kiros, R., and Zemel, R. Exploring models and data for image question answering. In Advances in Neural Information Processing Systems (NIPS 2015) 2015, volume 28, pp. 2953-2961. Curran Associates, Inc.
Riley, H. and Sridharan, M. 2019. Integrating non-monotonic logical reasoning and inductive learning with deep learning for explainable visual question answering. Frontiers in Robotics and AI, 6:125.
Sampat, S. K., Kumar, A., Yang, Y., and Baral, C. CLEVR_HYP: A challenge dataset and baselines for visual question answering with hypothetical actions over images. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2021) 2021, pp. 3692-3709. Association for Computational Linguistics.
Xu, J., Zhang, Z., Friedman, T., Liang, Y., and Van den Broeck, G. A semantic loss function for deep learning with symbolic knowledge. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018) 2018, volume 80 of Proceedings of Machine Learning Research, pp. 5502-5511. PMLR.
Yang, Z., He, X., Gao, J., Deng, L., and Smola, A. Stacked attention networks for image question answering. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016, pp. 21-29.
Yang, Z., Ishay, A., and Lee, J. NeurASP: Embracing neural networks into answer set programming. In Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI 2020) 2020, pp. 1755-1762. International Joint Conferences on Artificial Intelligence Organization.
Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J. Neural-symbolic VQA:</p>
<p>Disentangling reasoning from vision and language understanding. In Advances in Neural Information Processing Systems (NeurIPS 2018) 2018, volume 39, pp. 1039-1050. Curran Associates, Inc.
Zhu, Y., Groth, O., Bernstein, M., and Fei-Fei, L. Visual7w: Grounded question answering in images. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016, pp. 4995-5004.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://potassco.org/clingo/.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
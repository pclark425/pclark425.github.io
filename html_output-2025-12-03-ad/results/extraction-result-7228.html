<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7228 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7228</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7228</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-271050717</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.05721v2.pdf" target="_blank">PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Mental health has attracted substantial attention in recent years and LLM can be an effective technology for alleviating this problem owing to its capability in text understanding and dialogue. However, existing research in this domain often suffers from limitations, such as training on datasets lacking crucial prior knowledge and evidence, and the absence of comprehensive evaluation methods. In this paper, we propose a specialized psychological large language model (LLM), named PsycoLLM, trained on a proposed high-quality psychological dataset, including single-turn QA, multi-turn dialogues and knowledge-based QA. Specifically, we construct multi-turn dialogues through a three-step pipeline comprising generation, evidence judgment, and refinement. We augment this process with real-world psychological case backgrounds extracted from online platforms, enhancing the relevance and applicability of the generated data. Additionally, to compare the performance of PsycoLLM with other LLMs, we develop a comprehensive psychological benchmark based on authoritative psychological counseling examinations in China, which includes assessments of professional ethics, theoretical proficiency, and case analysis. The experimental results on the benchmark illustrates the effectiveness of PsycoLLM, which demonstrates superior performance compared to other LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7228.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7228.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PsycoLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PsycoLLM (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A psychological-domain LLM obtained by supervised fine-tuning Qwen1.5-14B-Chat on a curated psychology dataset (single-turn QA, multi-turn dialogues, knowledge QA) to improve counseling and exam-style performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PsycoLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned instruction-style decoder-only LLM based on Qwen1.5-14B-Chat, optimized on a high-quality psychology dataset including single-/multi-turn and knowledge QA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark derived from Chinese psychological counselor exams)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice questions drawn from authoritative Chinese psychological counseling examinations to assess mastery of psychological theory (SMCQ = single correct option; MMCQ = multiple correct options).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard accuracy and elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 74.20% standard accuracy; MMCQ (Theory): 72.63% standard accuracy (elastic accuracy not separately reported here for this model in the table). Overall benchmark average reported: 61.71% (paper reports PsycoLLM as the only model with average >60% standard accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts per-question (task description + question). Answers extracted with regex; no special chain-of-thought reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper reports PsycoLLM was fine-tuned on the domain dataset and obtains the best standard-accuracy results among compared models. No human baseline or human exam pass rates are provided in the paper. The benchmark's theory section includes cognitive-psychology topics (example question on implicit memory appears in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7228.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's advanced general-purpose large language model (referenced in the paper as a comparative baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI multimodal/advanced GPT-family model (general-domain instruction-following LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See PsycoLLM entry; theoretical MCQs from Chinese counseling exams.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 54.79% standard accuracy; MMCQ (Theory): 74.65% standard accuracy. Overall average reported in paper: 60.45% (paper shows GPT-4o strong on SMCQ case and generally high elastic accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts; answers parsed via regex from model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper notes GPT-4o attains best SMCQ results on some sections and strong elastic accuracy; no human baseline or statistical tests vs humans reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7228.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen1.5-14B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen1.5-14B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 14B-parameter decoder-only LLM (Qwen family) optimized for chat; used both as a backbone and as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen1.5-14B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (Qwen family) with SwiGLU activations and RoPE, pre-trained on a large multilingual corpus and provided in a chat-optimized variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MCQs from Chinese psychological counseling exams testing theoretical knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 67.93% standard accuracy; MMCQ (Theory): 70.69% standard accuracy. Overall average reported: 58.57%.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompt per MCQ; regex-based answer extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Qwen1.5-14B-Chat is also the backbone for PsycoLLM; paper reports strong performance but no human baselines are given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7228.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A faster/optimized variant of Qwen models used as a comparative baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Optimized/serving-oriented variant of Qwen models (details not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MCQs drawn from authoritative psychological counseling exams.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 73.25% standard accuracy; MMCQ (Theory): 69.96% standard accuracy. Overall average reported: 60.03% (elastic accuracy for MMCQs also >60% reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts; regex extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper notes Qwen-turbo achieves >60% elastic accuracy; no human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7228.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-1.5-9B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-1.5-9B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 9B-parameter open-source LLM from the Yi model family used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-1.5-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation model trained on large corpus with extended capabilities for Chinese; decoder-only architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MCQ evaluation of theoretical psychology knowledge from Chinese counselor exams.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 66.40% standard accuracy; MMCQ (Theory): 70.51% standard accuracy. Overall average reported: 60.09%.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts; regex extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper highlights Yi-1.5-9B as performing well despite smaller size; no human baseline included.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7228.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5 family chat-optimized model used as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat-optimized GPT-3.5 model (instruction-following decoder transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice exam items assessing theoretical psychological knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 43.98% standard accuracy; MMCQ (Theory): 55.90% standard accuracy. Overall average reported: 49.01%.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts per MCQ; regex extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper observes GPT-3.5/GPT-4 family often output single option for MMCQs, affecting standard vs elastic accuracy; no human baseline data provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7228.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baichuan2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baichuan2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>13B-parameter open-source LLM from Baichuan family evaluated as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Baichuan2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source large language model (13B) trained on a large multilingual corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Authoritative-exam-derived multiple-choice questions for theory evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 50.41% standard accuracy; MMCQ (Theory): 52.49% standard accuracy. Overall average reported: 45.22%.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts; regex extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper notes models tend to perform better on ethics than on theoretical knowledge; human baselines absent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7228.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM3-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM3-6B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>6B-parameter ChatGLM v3 family model (Chinese-optimized) included as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM3-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chinese-oriented dialogue LLM from the ChatGLM family (6B parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MCQ theoretical knowledge questions from Chinese counselor exams.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 52.21% standard accuracy; MMCQ (Theory): 46.36% standard accuracy. Overall average reported: 37.84%.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts; regex extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Lower performance relative to larger models; no human baseline reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7228.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmoLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmoLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An emotion-focused LLM fine-tuned on conversational mental-health datasets; included for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EmoLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned model (paper states EmoLLM fine-tuned on LLaMA3-8B-instruct) optimized for empathetic conversation; performs relatively better on case-based generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Theoretical MCQs from Chinese counseling examinations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 33.06% standard accuracy; MMCQ (Theory): 38.54% standard accuracy. However, EmoLLM performed comparatively well on some case-based generation metrics (ROUGE/BLEU/BERTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts; models fine-tuned on conversational data used with standard prompts for MCQs.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper remarks EmoLLM and MindChat (both fine-tuned on conversational data) do better on case-based QA generation metrics despite weaker MCQ performance. No human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7228.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7228.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MindChat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MindChat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A psychology-focused LLM (fine-tuned on mental dialogue data) used as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MindChat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Psychological-domain model fine-tuned on Qwen-7B with mental dialogue datasets (optimized for empathetic conversations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SMCQ / MMCQ — Theoretical proficiency (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>psychological knowledge / cognitive psychology (theoretical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MCQs assessing theoretical knowledge from Chinese counseling exams.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SMCQ (Theory): 30.13% standard accuracy; MMCQ (Theory): 33.94% standard accuracy. Per paper, stronger on case generation metrics but low MCQ scores.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style prompts; regex extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper highlights the trade-off between conversational fine-tuning (better case responses) and knowledge-based exam performance. No human baseline or statistical comparisons to humans reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Psybench: a balanced and in-depth psychological chinese evaluation benchmark for foundation models <em>(Rating: 2)</em></li>
                <li>Psyeval: A comprehensive large language model evaluation benchmark for mental health <em>(Rating: 2)</em></li>
                <li>Psy-llm: Scaling up global mental health psychological services with ai-based large language models <em>(Rating: 1)</em></li>
                <li>PsyQA: A Chinese dataset for generating long counseling text for mental health support <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7228",
    "paper_id": "paper-271050717",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "PsycoLLM",
            "name_full": "PsycoLLM (this paper)",
            "brief_description": "A psychological-domain LLM obtained by supervised fine-tuning Qwen1.5-14B-Chat on a curated psychology dataset (single-turn QA, multi-turn dialogues, knowledge QA) to improve counseling and exam-style performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PsycoLLM",
            "model_description": "Fine-tuned instruction-style decoder-only LLM based on Qwen1.5-14B-Chat, optimized on a high-quality psychology dataset including single-/multi-turn and knowledge QA.",
            "model_size": "14B",
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark derived from Chinese psychological counselor exams)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "Multiple-choice questions drawn from authoritative Chinese psychological counseling examinations to assess mastery of psychological theory (SMCQ = single correct option; MMCQ = multiple correct options).",
            "evaluation_metric": "Standard accuracy for SMCQ; standard accuracy and elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 74.20% standard accuracy; MMCQ (Theory): 72.63% standard accuracy (elastic accuracy not separately reported here for this model in the table). Overall benchmark average reported: 61.71% (paper reports PsycoLLM as the only model with average &gt;60% standard accuracy).",
            "prompting_method": "Instruction-style prompts per-question (task description + question). Answers extracted with regex; no special chain-of-thought reported.",
            "fine_tuned": true,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The paper reports PsycoLLM was fine-tuned on the domain dataset and obtains the best standard-accuracy results among compared models. No human baseline or human exam pass rates are provided in the paper. The benchmark's theory section includes cognitive-psychology topics (example question on implicit memory appears in paper).",
            "uuid": "e7228.0",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "OpenAI's advanced general-purpose large language model (referenced in the paper as a comparative baseline).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary OpenAI multimodal/advanced GPT-family model (general-domain instruction-following LLM).",
            "model_size": null,
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "See PsycoLLM entry; theoretical MCQs from Chinese counseling exams.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 54.79% standard accuracy; MMCQ (Theory): 74.65% standard accuracy. Overall average reported in paper: 60.45% (paper shows GPT-4o strong on SMCQ case and generally high elastic accuracy).",
            "prompting_method": "Instruction-style prompts; answers parsed via regex from model outputs.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper notes GPT-4o attains best SMCQ results on some sections and strong elastic accuracy; no human baseline or statistical tests vs humans reported.",
            "uuid": "e7228.1",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Qwen1.5-14B-Chat",
            "name_full": "Qwen1.5-14B-Chat",
            "brief_description": "A 14B-parameter decoder-only LLM (Qwen family) optimized for chat; used both as a backbone and as a baseline in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen1.5-14B-Chat",
            "model_description": "Decoder-only transformer (Qwen family) with SwiGLU activations and RoPE, pre-trained on a large multilingual corpus and provided in a chat-optimized variant.",
            "model_size": "14B",
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "MCQs from Chinese psychological counseling exams testing theoretical knowledge.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 67.93% standard accuracy; MMCQ (Theory): 70.69% standard accuracy. Overall average reported: 58.57%.",
            "prompting_method": "Instruction-style prompt per MCQ; regex-based answer extraction.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Qwen1.5-14B-Chat is also the backbone for PsycoLLM; paper reports strong performance but no human baselines are given.",
            "uuid": "e7228.2",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Qwen-turbo",
            "name_full": "Qwen-turbo",
            "brief_description": "A faster/optimized variant of Qwen models used as a comparative baseline in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-turbo",
            "model_description": "Optimized/serving-oriented variant of Qwen models (details not specified in paper).",
            "model_size": null,
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "MCQs drawn from authoritative psychological counseling exams.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 73.25% standard accuracy; MMCQ (Theory): 69.96% standard accuracy. Overall average reported: 60.03% (elastic accuracy for MMCQs also &gt;60% reported in paper).",
            "prompting_method": "Instruction-style prompts; regex extraction.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper notes Qwen-turbo achieves &gt;60% elastic accuracy; no human baseline provided.",
            "uuid": "e7228.3",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Yi-1.5-9B",
            "name_full": "Yi-1.5-9B",
            "brief_description": "A 9B-parameter open-source LLM from the Yi model family used as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi-1.5-9B",
            "model_description": "Open foundation model trained on large corpus with extended capabilities for Chinese; decoder-only architecture.",
            "model_size": "9B",
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "MCQ evaluation of theoretical psychology knowledge from Chinese counselor exams.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 66.40% standard accuracy; MMCQ (Theory): 70.51% standard accuracy. Overall average reported: 60.09%.",
            "prompting_method": "Instruction-style prompts; regex extraction.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper highlights Yi-1.5-9B as performing well despite smaller size; no human baseline included.",
            "uuid": "e7228.4",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "OpenAI's GPT-3.5 family chat-optimized model used as a baseline in comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI chat-optimized GPT-3.5 model (instruction-following decoder transformer).",
            "model_size": null,
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "Multiple-choice exam items assessing theoretical psychological knowledge.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 43.98% standard accuracy; MMCQ (Theory): 55.90% standard accuracy. Overall average reported: 49.01%.",
            "prompting_method": "Instruction-style prompts per MCQ; regex extraction.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper observes GPT-3.5/GPT-4 family often output single option for MMCQs, affecting standard vs elastic accuracy; no human baseline data provided.",
            "uuid": "e7228.5",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Baichuan2-13B",
            "name_full": "Baichuan2-13B",
            "brief_description": "13B-parameter open-source LLM from Baichuan family evaluated as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Baichuan2-13B",
            "model_description": "Open-source large language model (13B) trained on a large multilingual corpus.",
            "model_size": "13B",
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "Authoritative-exam-derived multiple-choice questions for theory evaluation.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 50.41% standard accuracy; MMCQ (Theory): 52.49% standard accuracy. Overall average reported: 45.22%.",
            "prompting_method": "Instruction-style prompts; regex extraction.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper notes models tend to perform better on ethics than on theoretical knowledge; human baselines absent.",
            "uuid": "e7228.6",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ChatGLM3-6B",
            "name_full": "ChatGLM3-6B",
            "brief_description": "6B-parameter ChatGLM v3 family model (Chinese-optimized) included as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGLM3-6B",
            "model_description": "Chinese-oriented dialogue LLM from the ChatGLM family (6B parameters).",
            "model_size": "6B",
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "MCQ theoretical knowledge questions from Chinese counselor exams.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 52.21% standard accuracy; MMCQ (Theory): 46.36% standard accuracy. Overall average reported: 37.84%.",
            "prompting_method": "Instruction-style prompts; regex extraction.",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Lower performance relative to larger models; no human baseline reported in paper.",
            "uuid": "e7228.7",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "EmoLLM",
            "name_full": "EmoLLM",
            "brief_description": "An emotion-focused LLM fine-tuned on conversational mental-health datasets; included for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "EmoLLM",
            "model_description": "Fine-tuned model (paper states EmoLLM fine-tuned on LLaMA3-8B-instruct) optimized for empathetic conversation; performs relatively better on case-based generation metrics.",
            "model_size": "8B",
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "Theoretical MCQs from Chinese counseling examinations.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 33.06% standard accuracy; MMCQ (Theory): 38.54% standard accuracy. However, EmoLLM performed comparatively well on some case-based generation metrics (ROUGE/BLEU/BERTScore).",
            "prompting_method": "Instruction-style prompts; models fine-tuned on conversational data used with standard prompts for MCQs.",
            "fine_tuned": true,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper remarks EmoLLM and MindChat (both fine-tuned on conversational data) do better on case-based QA generation metrics despite weaker MCQ performance. No human baseline provided.",
            "uuid": "e7228.8",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MindChat",
            "name_full": "MindChat",
            "brief_description": "A psychology-focused LLM (fine-tuned on mental dialogue data) used as a comparative baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MindChat",
            "model_description": "Psychological-domain model fine-tuned on Qwen-7B with mental dialogue datasets (optimized for empathetic conversations).",
            "model_size": "7B",
            "test_name": "SMCQ / MMCQ — Theoretical proficiency (benchmark)",
            "test_category": "psychological knowledge / cognitive psychology (theoretical knowledge)",
            "test_description": "MCQs assessing theoretical knowledge from Chinese counseling exams.",
            "evaluation_metric": "Standard accuracy for SMCQ; standard/elastic accuracy for MMCQ",
            "human_performance": null,
            "llm_performance": "SMCQ (Theory): 30.13% standard accuracy; MMCQ (Theory): 33.94% standard accuracy. Per paper, stronger on case generation metrics but low MCQ scores.",
            "prompting_method": "Instruction-style prompts; regex extraction.",
            "fine_tuned": true,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Paper highlights the trade-off between conversational fine-tuning (better case responses) and knowledge-based exam performance. No human baseline or statistical comparisons to humans reported.",
            "uuid": "e7228.9",
            "source_info": {
                "paper_title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Psybench: a balanced and in-depth psychological chinese evaluation benchmark for foundation models",
            "rating": 2,
            "sanitized_title": "psybench_a_balanced_and_indepth_psychological_chinese_evaluation_benchmark_for_foundation_models"
        },
        {
            "paper_title": "Psyeval: A comprehensive large language model evaluation benchmark for mental health",
            "rating": 2,
            "sanitized_title": "psyeval_a_comprehensive_large_language_model_evaluation_benchmark_for_mental_health"
        },
        {
            "paper_title": "Psy-llm: Scaling up global mental health psychological services with ai-based large language models",
            "rating": 1,
            "sanitized_title": "psyllm_scaling_up_global_mental_health_psychological_services_with_aibased_large_language_models"
        },
        {
            "paper_title": "PsyQA: A Chinese dataset for generating long counseling text for mental health support",
            "rating": 1,
            "sanitized_title": "psyqa_a_chinese_dataset_for_generating_long_counseling_text_for_mental_health_support"
        }
    ],
    "cost": 0.01597925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation
7 Aug 2024</p>
<p>Jinpeng Hu jinpenghu@hfut.edu.cn 
Tengteng Dong 
Luo Gang gang@bit.edu.cn 
Hui Ma huima@hfut.edu.cn 
Peng Zou 
Xiao Sun sunx@hfut.edu.cn 
Dan Guo guodan@hfut.edu.cn 
Meng Wang 
Moonshot Api Generate→evidence→optimize 
Knowledge Qa </p>
<p>School of Com-puter Science and Information Engineering
Hefei University of Tech-nology (HFUT)
230601HefeiChina</p>
<p>Ministry of Education
Key Laboratory of Brain Health Intelligent Evaluation and Intervention
Beijing Institute of Technology)
BeijingP. R. China</p>
<p>School of Medical Technology
Beijing Institute of Technology
BeijingP. R. China</p>
<p>Institute of Artificial Intelligence
Hefei Comprehensive National Science Center
230088HefeiChina</p>
<p>School of Computer Science and Information Engineering
Hefei University of Technology (HFUT)
230601HefeiChina</p>
<p>Institute of Artificial Intelligence
Hefei Com-prehensive National Science Center
HefeiChina, China</p>
<p>Hefei Zhongjuyuan Intelligent Technology Co., Ltd
HefeiChina</p>
<p>School of Computer Science and Information Engineer-ing
Hefei University of Technology (HFUT)
Hefei</p>
<p>School of Computer Science and Information Engi-neering
Hefei University of Technology (HFUT)
230009HefeiChina</p>
<p>Institute of Artificial Intelligence
Hefei Comprehensive National Science Center
230088HefeiChina</p>
<p>PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation
7 Aug 2024B0A2A80C28361E121F20711B1DD60702arXiv:2407.05721v2[cs.CL]Large language modelpsychological understandingpsychological evaluationmental health Question description
Mental health has attracted substantial attention in recent years and LLM can be an effective technology for alleviating this problem owing to its capability in text understanding and dialogue.However, existing research in this domain often suffers from limitations, such as training on datasets lacking crucial prior knowledge and evidence, and the absence of comprehensive evaluation methods.In this paper, we propose a specialized psychological large language model (LLM), named PsycoLLM, trained on a proposed high-quality psychological dataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.Specifically, we construct multi-turn dialogues through a three-step pipeline comprising generation, evidence judgment, and refinement.We augment this process with real-world psychological case backgrounds extracted from online platforms, enhancing the relevance and applicability of the generated data.Additionally, to compare the performance of PsycoLLM with other LLMs, we develop a comprehensive psychological benchmark based on authoritative psychological counseling examinations in China, which includes assessments of professional ethics, theoretical proficiency, and case analysis.The experimental results on the benchmark illustrates the effectiveness of PsycoLLM, which demonstrates superior performance compared to other LLMs.</p>
<p>such as ChatGPT1 , KimiChat2 , PaLM [1] , BaiChuan [2] and LLama [3].These models have demonstrated significant advancements across a variety of tasks, including question answering (QA), summarization [4], and information extraction [5].Moreover, large language models (LLMs) have been effectively applied to diverse domains, such as the medical and legal domains [6]- [8].In recent years, the escalating pressures from personal, and occupational demands have emerged as significant challenges to the mental health status of individuals.Since LLMs are trained on massive data and contain huge number of parameters, many experiments have shown that these models have the strong ability to understand the natural language and provide resaonable feedback.Hence, leveraging LLM to aid individuals in addressing psychological issues may represent an efficacious strategy for mitigating the impact of adverse mental health conditions and making it easier for seekers to obtain timely help.</p>
<p>Many studies have been proposed in this area.For example, The work [9] combined pre-trained LLMs with real-world professional QA data, with the purpose of providing online consultation services.[10] and [11] constructed a multi-turn empathetic conversation dataset and used a base LLM for fine-tuning.Further, EmoLLM [12] follow a similar way and fine-tune the open base model on the mental health dataset.Although these existing efforts have contributed significantly to the advancement of the field, they still exhibit certain limitations.First, most existing psychological finetuning datasets are generated by even larger language model such as ChatGPT.However, the generation process lacks adequate prior knowledge and evidence, potentially leading to the creation of datasets that deviate from real-world psychological communication scenarios.Second, owing to the lack of a comprehensive psychological benchmark, the evaluation of most studies in this area often relies on expert assessment based on subjective judgment or overly depends on other large models, which may not fully encompass the subtle performance.For instance, PsyBench [13] utilized questions generated by GPT-4 to evaluate model performance.</p>
<p>Therefore, in this paper, we propose a psychological LLM (PsycoLLM) trained on a high-quality dataset tailored to the domain of psychology, encompassing single-turn QA, multiturn dialogues, and knowledge-based QA.Specifically, for single-turn QA, we collect question-answer pairs from open platforms.For multi-turn dialogues, we develop an innovative pipeline with generation, evidence judgment, and refinement for producing high-quality data.For knowledge-based QA, we extract after-school exercises from psychology textbooks and use LLMs enhanced with agents to extract QA pairs from psychology books.Additionally, to explore the performance of the proposed model and exiting LLMs, we introduce a benchmark based on authoritative psychological counseling examinations in China for comparative analysis.The benchmark consists of three distinct components: professional ethics, theoretical proficiency, and case analysis.Professional ethics and theoretical proficiency are assessed through multiple-choice questions (MCQs) to evaluate the mastery of psychological principles, while the case analysis component emphasizes practical utility by incorporating both MCQs and QA sections to explore the application of the model in real-world scenarios.</p>
<p>The results reveal several insightful observations: I) Some LLMs have achieved an accuracy rate surpassing 60% in MCQs, showing the potential of using LLM to provide psychological assistance, with the proposed PsycoLLM demonstrating superior performance among these models.II) LLMs display greater proficiency in professional ethics compared to their mastery of psychological theory.</p>
<p>To summarize, the contributions of this paper are four-fold:</p>
<p>• We propose a psychological LLM (PsycoLLM), finetuned on the high-quality psychological dataset.• We present a comprehensive framework for constructing high-quality mental health datasets suitable for model training, along with the introduction of a psychological counseling examination-based benchmark for robust performance evaluation.• We conduct a comparative performance analysis of various LLMs on the established benchmark.Our findings demonstrate that the proposed PsycoLLM achieves superior performance.</p>
<p>II. RELATED WORK A. Large Language Model</p>
<p>Owing to highly parallelizable architecture of self-attention mechanisms, Transformer-based models have been popular in recent years.BERT [14] was a first pre-trained Transformerbased language model, which trained a bidirectional language encoder with specially designed masked language modelling.GPT [15] trained a Transformer-based decoder by generative pre-training.Subsequently, GPT-2 [16] was introduced as an expanded version of GPT, featuring a similar architecture but with a significantly larger scale, encompassing up to 1.5 billion parameters.BART [17] used a sequence-to-sequence architecture and combined the strengths of BERT and GPT, which employed a bidirectional encoder similar to BERT and an autoregressive decoder similar to GPT.These models typically followed a pre-training and fine-tuning paradigm, which need to fine-tune these model for different downstream tasks.Owing the findings that the larger models typically yield better performance, LLM become a prominent topic of research and have achieved significant success.For example, GPT-3 [18] with 175B parameters surpassed its predecessor, GPT-2, and performed well in few-shot learning.Recently, ChatGPT based on GPT-3.5, trained with a sizable amount of internet-sourced text data, has drawn sustainable attention which is exceptional at producing human-like responses.Building upon this success, OpenAI introduced GPT-4 [19], which further enhanced the capabilities of its predecessors.In contrast to ChatGPT and GPT-4 were proprietary models, LLaMA [3] was a openresource language model, which attracted widespread attention from both academia and industry due to its accessible nature and has achieved noteworthy performance.Further, Touvron et al [20] proposed an updated version of LLamA, called LLaMA2, which was trained on a larger corpus and outperformed LLaMA1 in many benchmarks.Additionally, many other LLMs based on LLaMA architecture also have been proposed, including Alpaca [21] and Mistral-7B [22], where the former first employed instruct-following to train the LLaMA and the latter is a 7B-parameter and outperformed LLamA-2-13B in various evaluations.Baichuan [2] introduced a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch on a corpus of 2.6 trillion tokens.ChatGLM [23] was an evolving family of large language models, which included ChatGLM [24], [25], ChatGLM2 and ChatGLM3, repectively.Besides, the Yi series models [26] were also trained from scratch on 3T multilingual corpus and notable for their ability to extend context length up to 200K by continuing pretraining.Furthermore, Qwen [27] represented another open-resource LLMs and obtained superior performance across a multitude of downstream tasks, especially in Chinese, which was also the main reason we choose it as our backbone.</p>
<p>B. Benchmark for Mental Health Support</p>
<p>Many studies have been devoted to evaluating the performance of the model across various domains.For example, in the general domain, [28] proposed a Chinese evaluation benchmark to evaluate advanced knowledge and reasoning abilities of Chinese-based models.Zhong at al [29] designed a human-centric benchmark for assessing the performance of foundation models via 20 diverse tasks across a wide variety of subjects.The MMLU benchmark [30] provided a comprehensive evaluation benchmark encompassing various tasks to evaluate the performance of a text model in multitask contexts.In the medical domain, [31] proposed a localized medical benchmark across various clinical medical professions to evaluate the existing LLMs for medicine.In the legal domain, [32] constructed a Chinese legal benchmark based on the logic of legal practice, categorizing the legal capabilities of language models into different levels.In the psychological domain, several studies have introduced benchmarks to assess model performance in mental health contexts.Jin et al [33] introduced a meticulously crafted benchmark to evaluate the model performance in five mental health-related subtasks.Zhang et al [13] used GPT-4 to generate question and constructed a psychological benchmark aimed at evaluating model performance in psychology.Furthermore, Qiu et al [34] created a benchmark with fine-grained labels for each dialogue and focused on dialogue safety in mental health support.Despite the advances made with these benchmarks, there are several limitations.Some benchmarks narrow their scope to specific subfields or isolated tasks within mental health, neglecting to evaluate the models' overall psychological knowledge mastery.Furthermore, benchmarks generated by large models carry the risk of deviating from real-world contexts.Compared to these works, we propose a benchmark that incorporates different levels of official examinations in China, enabling a comprehensive assessment of both theoretical knowledge and practical proficiency.</p>
<p>C. Mental Application in LLM</p>
<p>The success of LLMs in general domains has sparked interest in their application across various vertical fields.Many LLMs tailored for specific domains have achieved remarkable success [35], [36].For instance, HuatuoGPT [37], an LLM designed for the medical domain, has demonstrated excellent performance in medical consultations.Based on HuatuoGPT, HuatuoGPT-II [38] introduced a unified domain adaptation protocol to combine continued pre-training and supervised fine-tuning into a single process, which achieved better performance than HuatuoGPT.In the legal domain, Yue et al [39] developed an LLM tailored for building intelligent legal systems with legal reasoning and knowledge retrieval capability.Mental health is another crucial domain that has garnered significant attention in many studies [40]- [46].For example, the work [47] developed an LLM-based expert-guided pipeline to analyze raw interview transcripts and extract insights.Zhu et al [48] investigated the use of LLM to perform inferring users' underlying goals and fundamental psychological needs, finding that LLM-based approaches can be comparable to human performance.Wu et al [49] introduced an anthropomorphic LLM-based conversational agent designed to offer personalized support and suggest practical actions based on positive psychology and social psychology research.</p>
<p>Several LLMs have also been proposed to aid help-seekers in accessing effective mental health support.Lai et al [9] proposed an AI-based assistive tool that utilized LLMs for QA, aiming to alleviate the demand on mental health professionals.PsyQA [50] introduced a high-quality Chinese dataset of psychological health support in the form of question and</p>
<p>Answer text</p>
<p>Hello, dear questioner!This issue is quite significant and worth contemplating.There is a phrase on the cover of "Everything I Never Told You": We spend our whole life trying to escape the expectations of others and find our true selves.Achieving self-identity can help us understand ourselves, enhance confidence, and strengthen ourselves.Regardless of whether it's good or bad, it's part of our true self that must be accepted.However, unconditional acceptance might lead to more problems.Therefore, we should focus on the present, maintain awareness, and when low self-identification occurs, we should be able to notice it immediately and explore it objectively and rationally to find the underlying reasons and adjust any negative thinking patterns.Finally, strengthen self-awareness.Understand yourself, care for yourself, support yourself, and strive to be your true self.Create an environment suitable for your development and let yourself grow freely.Loving and recognizing yourself is, after all, the most important thing.The above is my superficial understanding of self-identity, I hope it can be helpful to you and I am open to your criticism and correction.</p>
<p>Topic: Unable to provide even the most basic guidance, how can someone from a dysfunctional family save themselves?Keywords: Family Trauma, Family Relationships</p>
<p>Question description</p>
<p>How can one truly achieve self-identification and enhance self-efficacy?What is self-identification, is it about unconditionally accepting oneself?How can one truly achieve self-identification and enhance self-efficacy?Are there any specific methods?</p>
<p>The case of Single-turn Dialogue (Streamlined) Multi-turn Dialogue I feel like I can never truly accept myself.Sometimes I think I've done quite a few good things, but deep down, I still can't accept myself, which makes me feel very troubled.</p>
<p>Hello, I can understand how you feel.Self-identification is a complex process.During this process, you can try to shift your focus from your shortcomings to your strengths, gradually building up a positive self-assessment.</p>
<p>I have tried to focus on my strengths, but I always feel that those strengths are not enough to make me feel proud.It seems that I am always dissatisfied with my achievements; how should I adjust this mindset?</p>
<p>This mindset may stem from setting excessively high expectations for oneself, or an overemphasis on achievements.I suggest you try to adjust your expectations and set more realistic and achievable goals.I can understand your feelings.Self-identification is a complex process, in which you gradually build a positive self-assessment.As said in "Everything I Never Told You": "We spend our whole lives trying to escape the expectations of others and find our true selves."</p>
<p>I have tried to focus on my strengths, but I always feel that those strengths are not enough to make me feel proud.It seems that I am always dissatisfied with my achievements; how should I adjust this mindset?</p>
<p>This mindset may arise from setting excessively high expectations for oneself or an overemphasis on achievements.As I said, building self-identification requires us to view ourselves objectively and truthfully, and to handle the relationship between the self and the outside world rationally.</p>
<p>......I feel like I can never truly accept myself.Sometimes I think I've done quite a few good things, but deep down, I still can't accept myself, which makes me feel very troubled.</p>
<p>Hello, I can sense your struggle, and it must not be easy.Self-identification is indeed a complex and profound journey.In this process, I am here to support you, and together we can explore how to gradually build a positive self-assessment.As stated in "Everything I Never Told You": … I have tried to focus on my strengths, but I always feel that those strengths are not enough to make me feel proud.It seems that I am always dissatisfied with my achievements; how should I adjust this mindset?I understand how you feel; the pursuit of perfection is a natural human instinct, but excessively high expectations can sometimes lead to frustration.Let's take it step by step, you've already done very well.I suggest you try to adjust your expectations and set more realistic and achievable goals.Remember, establishing self-identity requires us to view ourselves objectively and truthfully, and to handle the relationship between the self and the outside world rationally.</p>
<p>...... answer pair.Based on PsyQA, Qiu et al [11] further used Chat-GPT to covert single-turn dialogues to multi-turn dialogues and subsequently fine-tuned ChatGLM2-6B on this enhanced dataset.SoulChat [10] constructed a multi-turn empathetic conversation dataset and fine-tuned ChatGLM with this data.Yang et al [44] formalized the interpretable mental health analysis task and built a multi-task and multi-source interpretable mental health instruction dataset.After that, based on the contructed dataset, they released MentaLLaMA, an opensource instruction-following LLM fine-tuned on LLaMA2.The work [51] fine-tuned the LLM across multiple datasets simultaneously, demonstrating its effectiveness in various mental health prediction tasks.Although these efforts have greatly advanced the field, they rely too heavily on distillations from other models during data generation, leading to the creation of data that may not accurately reflect real-world scenarios.In this paper, we constructed a high-quality psychological dataset, including single-turn, multi-turn dialogue data, and knowledge QA data.</p>
<p>III. PSYCOLLM</p>
<p>In this section, we present the details of dataset including, single-turn QA, multi-turn dialogue and knowledge-based QA, respectively.The overview of dataset are shown in Fig. 1.Besides, we give the training process of PsycoLLM.</p>
<p>A. Single-Turn QA Construction</p>
<p>There are several publicly accessible websites committed to establishing a psychology platform and offering online solutions for individuals seeking psychological assistance, such as Yixinli, Zhihu and so on.The data from these platforms can be regarded as real-world inquiries and the solutions provided by professionals in response to these inquiries.On the original website, users typically provide descriptions along with their questions, including background information.Subsequently, professional or experienced individuals offer detailed solutions or advice in response to these questions.We collect over 267,000 pairs of data from websites.Furthermore, several important data cleaning procedures are implemented to enhance the quality and relevance of the dataset, including:</p>
<p>• Removal of irrelevant content: This involves eliminating extraneous materials such as advertisements, which do not contribute to the core data and could potentially introduce noise.• Deletion of short data entries: Data entries with fewer than 100 characters are removed to ensure that the dataset</p>
<p>CATEGORY PROMPT TEMPLATE</p>
<p>Multi-Turn Generation</p>
<p>You are an expert who has studied countless conversations between patients with mental health problems and mental health doctors... Please construct a continuous multi round dialogue record between help-seekers and psychologist that is in line with the actual situation.You will see a Q&amp;A consisting of user questions and responses from a psychological assistant.The output format follows: User: statement from the patient.Psychological assistant: the doctor's comfort, advice, and guidance.User:.Psychological assistant:.The dialogue should have more conversation rounds.</p>
<p>Evidence Support</p>
<p>You are a professional psychologist who has provided numerous psychological consultations ... Your task is to identify the evidence source of each response from the psychologist in the multi-turn dialogue Q&amp;A.</p>
<p>[Important ]Please identify the evidence of each sentence in the multi-turn conversation from the original Q&amp;A .individual answers: This ensures the dataset reflects highquality professional advice to some extent.The respondents' level may serve as an indicator of their extensive experience in assisting help-seekers, which, in turn, can be indicative of their professional competence.This metric provides a basis for assessing the quality of the provided information.</p>
<p>After these data cleaning procedures, we obtain over 155k pairs for single-turn QA.We report detailed statistics of the processed data drawn from different aspects.We divide the processed data into 9 major topics and 127 subtopics, as illustrated by the distribution of major topics depicted in Fig. 2. It is observed that the number of reports related to emotional issues and regulation, as well as interpersonal relationships and socializing, is higher than those in other categories, each comprising more than 20% of the total.Following these topics, family and marriage, and personal growth and development are the next most significant areas, each accounting for more than 10% of the reports.Besides, the visualization of the collected data is depicted through word clouds, as shown in Fig. 3, which provides an intuitive depiction of the frequency and prominence of specific terms within the dataset.The analysis underscores the pervasive connection of the majority of these data to everyday psychological counseling, further emphasizing the relevance and reasonability of the collected data.Based on the extracted data in section III-A, we select a subset of the highest upvoted answers for each question to generate multi-turn dialogue data using KimiChat.This process aims to emulate the behavior of a psychological counselor who progressively asks probing questions to gain insight into the user's psychological state and requirements, thereby facilitating more effective assistance.The multi-turn dialogue mainly includes two roles: the person seeking help and psychological professionals.We employ a three-step pipeline to generate high-quality multi-turn dialogue data.First, we use an appropriate prompt to guide the KimiChat to construct multiturn dialogue data between pre-defined roles according to the selected pairs.The original pairs serve as prior knowledge to guide KimiChat in generating multi-turn conversations, thereby enhancing the fidelity of the generated data to realworld conversations.Second, we employ an additional prompt to assess whether the answers in the multi-turn dialogue dataset are derived from the original context.If the majority of responses in the multi-turn dialogue can be supported by evidence extracted from the original text data, it is considered to better reflect the conversational flow of realistic counselors.Conversely, if most responses are generated predominantly by the model without effectively leveraging the given context, such data entries require further processing.This can be achieved by using an additional prompt to enhance the in- tegration of factual evidence and incorporate content-relevant information from the original context without compromising fluency.Third, to enhance the quality of the multi-turn dialogue data further, we utilize a prompt to revise the data in terms of empathy, supportiveness, guidance, and safety.Ultimately, we perform manual proofreading to further ensure data quality and to derive the processed multi-turn dialogue data.The prompts used for multi-turn dialogue generation, evidence support identification, and dialogue evaluation are presented in Table I.</p>
<p>A example is shown in Fig. 4, with three steps to generate high-quality data.In fact, we have compared using a single complex prompt to achieve the same goal as the aforementioned three steps.However, the utilization of a single prompt often results in an abundance of model-generated templated responses, rather than the comprehensive integration of provided factual information.This is the primary reason we opt for a pipeline approach to generate data.The statistics of the data is reported in Table II.</p>
<p>C. knowledge-based QA</p>
<p>In addition to the daily QA interactions, we also introduce some more abstract psychological knowledge data, such as explanations of psychological terms.We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA from them.Specifically, we segment books into text spans using a predefined fixed length, identifying the nearest sentence or paragraph as segmentation indicators.These text spans serve as the fundamental units for subsequent QA generation through the utilization of LLMs.First, the LLM generates questions and their corresponding answers.These question-answer pairs are then input into two LLM-based student modules, one utilizing retrieval-augmented generation (RAG) and the other without RAG, to produce two new sets of answers.Subsequently, a teacher module, also based on an LLM, evaluates and selects best answer from those generated by the student modules.Furthermore, to ensure the quality and accuracy of the generated QA pairs, a manual validation process is implemented, wherein human evaluators assess and eliminate low-quality data.In addition, we extract after-school exercises from several books and convert them to QA format, with the corresponding answer analyses.Finally, we obtain 10K knowledge QA data.</p>
<p>D. Supervised Fine-Tuning</p>
<p>After obtaining the high-quality psychological dataset, we further conduct supervised fine-tuning (SFT) to enhance the capability of the LLM in psychology.We select Qwen1.5-14B-Chat as our backbone model, a decoder-only architecture demonstrating excellent performance in both English and Chinese.We then perform SFT on the top of Qwen1.5-14B-Chat to develop PsycoLLM.</p>
<p>At training stage, assuming the text input as a sequence, e.g., x = {x 1 , x 1 , ..., x L }, where each x i is a text token and L is the length of x.The core architecture of Qwen is based on Transformer-Decoder structure [52], which is a typical autoregressive framework designed to sequentially predict each subsequent word in a given sequence.This can be mathematically formulated as follows:
p(x) = L t=1 p (x t | x 1 , . . . , x t−1 )(1)
For training PsycoLLM, we utilize the cross-entropy loss function as the objective.The model is trained to maximize the negative conditional log-likelihood of the predicted sequence given the input data.
θ * = arg max θ L t=1 log p (x t | x 1 , ..., x t−1 ; θ) ,(2)
where θ is the trainable parameters of the model.The hyperparameters in this process are configured as follows: the learning rate is set to 1e-5, the batch size is set to 128, and the number of epochs is set to 3. We employ full parameter fine-tuning to train the model using eight A6000 GPUs, each equipped with 48GB memory.</p>
<p>IV. BENCHMARK</p>
<p>In this section, we present the psychological benchmark and provide comprehensive details, including data preparation and evaluation metrics.</p>
<p>A. Dataset Collection and Preprocessing</p>
<p>In order to accurately evaluate the model performance in the psychology domain, we introduce a benchmark that psychological professional individuals need to master.The Answer: ABD, 是一贯感到紧张、提心吊胆、不安全和自卑， 总是需要被人 喜欢和接纳， 对拒绝和批评过分敏感， 因习惯性地夸大日常处境中的潜在 危险， 所以有回避某些活动的倾向。(English version: ABD, It is characterized by consistently feeling tense, apprehensive, insecure, and inferior.Individuals with this disorder constantly need to be liked and accepted and are overly sensitive to rejection and criticism.Due to their habitual exaggeration of potential dangers in everyday situations, they tend to avoid certain activities.)
MMCQ SMCQ 求助者10余年来工作认真负责，积极努力… 1个多月前上级来了正式任 命，但任命的科长不是他。求助者很震惊，觉得特别没有面子，认为领导 是故意让自己难堪为此情绪较为低落，内心苦恼，烦躁不安，觉得无助。 近2周来，调动工作未果，经常失眠、食欲明显下降… (</p>
<p>English version :</p>
<p>The client has been diligent and responsible in their work for over 10 years, always putting in a lot of effort … Over a month ago, an official appointment was made by their superior, but the section chief position was not given to him.The client was shocked and felt deeply humiliated, believing that the leader intentionally made him look bad.As a result, the client has been feeling down, distressed, restless, and helpless.For the past two weeks, their efforts to transfer positions have been unsuccessful, leading to frequent insomnia and a significant decrease in appetite … data is sourced from publicly available examination questions.We follow a standardized data preprocessing procedure.For some data, we first need to perform optical character recognition (OCR) to convert images to text.After that, we invite several students to manually review the collected data to ensure its quality and consistency with the original document.This process involves rectifying formatting errors, eliminating duplicate questions, and rectifying any instances of garbled characters.Our proposed psychological benchmark draws inspiration from the format of the most authoritative psychological counseling examination in China, comprising two primary components: objective questions and subjective questions.Note that the objective questions mainly comprise two types of multiple-choice questions (MCQs): MCQ with only a single correct option (SMCQ) and MCQ with multiple correct options (MMCQ).The content of benchmark can be categorized into three sections: professional ethics, theoretical proficiency, and case analysis.The professional ethics section generally encompasses general principles such as honesty, integrity, professional responsibility, and social responsibility.Theoretical proficiency typically pertains to fundamental theories and concepts in psychology, including cognitive psychology, developmental psychology, and related fields.Both the professional ethics and theoretical proficiency sections contain only MCQs, designed to evaluate the mastery of knowledge within the psychological domain.For case analysis, it includes both MCQs and iterative questioning based on given case, aiming to better evaluate performance in real-world scenarios.The former focuses on assessing the grasp of professional knowledge related to the case, while the latter offers a more open-ended assessment of the model's capability in practical utility.Finally, we obtained a total of 3863 MCQs and 20 cases with 100 QAs.An example of MCQs is shown in Fig. 5, and an example of case-based QA is presented in Fig. 6.The statistics of the benchmark are shown in Table III, detailing the distribution of questions across various categories.The questions are classified into three main categories: Level 2 Psychological Counselor Exam, Level 3 Psychological Counselor Exam, and Others.Note that, according to the official definitions, the Level 2 exam is considered more challenging than the Level 3 exam.</p>
<p>Case</p>
<p>B. Evaluation Metric</p>
<p>There are three elements in each instance of the case analysis.B is the description of the given case, including the background, environment, etc. of the seeker, Q is the question according to the case, such as the diagnosis, reason and treatment for the case and A is the reference answer for each questions.For QA in case analysis, we use the models to generate the response O according to the given B and Q.We evaluate the response O from model with the corresponding references A. We follow existing text generation metric, including ROUGE-1 (R-1), ROUGE-L (R-L) [53], BLEU-4 (B-4) [54], BERTScore [55].For the ROUGE metric, we report the F 1 scores for both R-1 and R-L.R-1 measures unigram overlap, which serves as an indicator of informativeness, while R-L evaluates the longest common subsequence overlap, providing an assessment of fluency.BERTScore leverages embedding from pre-trained encoder to capture the semantic and contextual information of tokens through computing cosine similarities.This metric allows for a more nuanced evaluation of text generation quality by considering the underlying meaning and context rather than just surface-level token matching.</p>
<p>For SMCQ, we use the standard accuracy to evaluate the model performance.For MMCQ, we employ two types of accuracy metrics: standard accuracy and elastic accuracy.Standard accuracy is defined as the ratio of completely correct answers to the total number of questions, providing a clear measure of the model's precision by counting only fully accurate responses.Elastic accuracy, on the other hand, accounts for partially correct answers that contain no incorrect options but are not entirely complete.This metric offers a evaluation by recognizing the model's ability to produce answers that are mostly correct, even if they are not fully comprehensive.The elastic accuracy for a MMCQ is computed as follows:
elastic accuracy = |predicted answer| |correct answer| ,
where || indicates the count of choices in the answer.</p>
<p>V. EXPERIMENTAL SETTING A. Baseline</p>
<p>In our experiments, our experiment is implemented based on transformers 3 and LLaMA-Factory4 .</p>
<p>To explore the performance of different models, we also evaluate the following models on the proposed benchmark:</p>
<p>• BAICHUAN25 : This is a generation of open-source large language models launched by Baichuan Intelligent Technology, which is trained on a high-quality corpus with 2.6 trillion tokens.• CHATGLM3 [24]: This is a generation of pre-trained dialogue models jointly released by Zhipu AI and Tsinghua KEG.• LLAMA3-CHINESE [56] 6 : This model uses LLama3 as the base model and is fine-tuned with high-quality Chinese instruction datasets with the training methods of DORA and LORA+.• YI (e.g., Yi-1.5-6B,Yi-1.5-9B) 7 : These open-source large language models are trained from scratch by developers at 01.AI, which are trained with a high-quality corpus of 500B tokens.• QWEN (e.g., Qwen1.5-14B-Chat,Qwen-turbo): these are decoder-only transformer models with SwiGLU activation, RoPE, multi-head attention, developed by Alibaba Cloud and are pre-trained using over 3 trillion tokens of high-quality corpus.• GPT-3.5-turbo,GPT-4o: These models, developed by OpenAI and built upon the foundations of GPT-2 and GPT-3, excel in dialogue generation.Besides, we also compare with several psychological LLMs: • MINDCHAT [57] 8 : This model is fine-tuned on Qwen-7B using a mental dialogue dataset.</p>
<p>• EMOLLM [12] 9 : This model is fine-tuned on LLaMA3-8B-instruct.</p>
<p>B. Evaluation Details</p>
<p>Prompt in our experiments usually comprises following components: a well-defined task description and a pertinent question soliciting the model's response.For tasks involving MCQ, we extract answers from model outputs using an empirically designed regular expression.</p>
<p>VI. RESULTS AND ANALYSIS</p>
<p>A. Results on Objective Assessments</p>
<p>To explore the performance of different models in our benchmark, we report the results of objective questions in Table IV.There are several observations drawn from the results.First, when comparing performance between ethics and theoretical knowledge, the results indicate that these models exhibit superior performance in the ethics domain.This discrepancy may be attributed to the closer alignment of ethics aspects with general domain dataset.It is plausible that the training datasets for these general LLMs include a significant amount of morally relevant data, which enhances their performance in ethical evaluations.In contrast, theoretical knowledge may require more specialized data that is less prevalent in the general-domain training sets.Second, within the realm of MCQs, the accuracy for MMCQs is lower than that for SMCQs.This difference is likely due to the increased difficulty in correctly identifying all the correct options in MMCQs.The requirement to select multiple correct options, as opposed to a single correct option, introduces additional challenges in ensuring comprehensive and accurate responses, thereby affecting overall accuracy.Third, GPT-4o achieves the best results in SMCQ for both theoretical knowledge and case analysis.This demonstrates that this model, despite being trained for general domain, exhibits strong performance in the psychological domain as well.This suggests that its robust general training provides a strong foundation for effectively addressing domain-specific tasks, including those related to psychological concepts and case-based evaluations.Fourth, several models demonstrate the capability to pass the MCQs in the benchmark, primarily extracted from authoritative psychological counseling examinations.Notably, only PsycoLLM achieves an average exceeding 60% in standard accuracy.In terms of elastic accuracy, GPT-4o, Qwen-turbo, Yi-1.5-9B, and PsycoLLM all achieve averages exceeding 60%.This observation highlights the significant potential of using LLM for providing psychological assistance, demonstrating that these models possess the capability to pass relevant exams and deliver reliable support in this domain.Among these models, Yi-1.5-9B is the smallest, yet it still surpasses the 60% in elastic accuracy.FIfth, our proposed model, fine-tuned on the psychological dataset, demonstrates improvements across most evaluated aspects compared to other models, particularly in terms of standard accuracy, yielding more significant advancements.This underscores the effectiveness of our proposed dataset in enhancing performance within the psychological domain.</p>
<p>B. Results on Case-Based QA</p>
<p>We report the results of subjective questions in Table V.The results obtained from automatic evaluations using ROUGE and BLEU metrics for these models are relatively low.This may be due to the inherent limitations of these metrics in capturing the nuances and complexities of natural language generation, as they primarily focus on surface-level n-gram overlap rather than semantic equivalence and contextual appropriateness.Therefore, we also incorporate BERTScore into our evaluation framework.Most models consistently score between 63 and 65 on the BS indicator, indicating the consistency and stability of the BS evaluation.PSYCOLLM demonstrates superior performance on R-1, B-4, and BERTScore, respectively and EmoLLM achieves better results on R-L.Additionally, although EmoLLM and MindChat do not perform well on MCQs, they achieve relatively strong performance in R-1, R-L, and B-4.The main reason might be that these models are fine-tuned with psychological conversation data, enabling them to better grasp case information and generate higher-quality responses.We provide a comparative example of the results from PsycoLLM and GPT-4o in Fig. 6.Both models are proficient at identifying the majority of patient issues.A notable distinction 本案例最可能的诊断及诊断依据是什么？What is the most likely diagnosis for this case and what is the basis for this diagnosis?</p>
<p>GPT-4o</p>
<p>(1) 诊断是广泛性焦虑障碍(Generalized Anxiety Disorder, GAD)。
(2) 以下是诊断依据： 1. 持续的过度担忧：求助者对男友的态度变化、未来婚姻状况以及自己 可能成为"剩女"的担忧，表现出持续的过度担忧和焦虑。 2. 身体症状：求助者在与男友通话后出现了胸闷、心慌、气短、头晕、 两腿无力等症状，这些都是焦虑障碍的常见身体表现。尽管求助者多次 到医院检查，但没有查出明显的器质性病变，这进一步支持了心理因素 在症状中的作用。 3. 功能受损： 求助者的工作效率降低，精神痛苦难以摆脱，表明焦虑已 经影响到她的日常生活和工作功能。 4. 长期的心理压力：求助者从学生时代起就受到母亲的严格管教，母亲 对异性的负面评价和对婚前性行为的严厉警告，导致求助者在与异性交 往时始终保持距离，内心充满矛盾和冲突。父亲的离异行为和母亲的负 面言论加剧了求助者对男性的不信任和对婚姻的恐惧。 5. 情绪和认知的负面影响： 求助者对未来婚姻的担忧和对男友态度变化 的敏感，反映出她在情绪和认知上的负面影响。综上所述，求助者的症 状符合广泛性焦虑障碍的诊断标准。她的焦虑不仅表现在对具体事件的 过度担忧上，还伴随着明显的身体症状和功能受损，且这种焦虑状态已 经持续了一段时间。
The client grew up in a single-parent family after her parents divorced many years ago.She has always lived with her mother, who never remarried.From her student days, her mother strictly prohibited her from being alone with boys.After college, she started dating, but her mother warned her that men are unreliable and fickle, advising caution.Her mother also warned that if she had premarital sex, she would sever their mother-daughter relationship.The client feels she shouldn't go against her mother's wishes, believing her mother sacrificed remarriage for her sake.Remembering her father's abandonment, she views men as unreliable, leading to unsuccessful relationships.A year ago, she met a man who recently returned from studying abroad.She was very satisfied with his looks, build, and financial status.However, due to her mother's warnings, she kept her distance.After repeatedly refusing his advances, she felt he started to distance himself.Six months ago, after a phone call with him, she suddenly felt chest tightness, palpitations, shortness of breath, dizziness, and weakness in her legs.She often feels anxious, worrying her boyfriend no longer likes her… These symptoms have led her to seek medical help, but no significant physical issues were found…</p>
<p>PsycoLLM
(1)诊断：神经症。 (2) 诊断依据： 1.求助者的症状已经持续半年，严重影响了其正常的生活、工作，因此 可以诊断为神经症。 2.神经症的诊断要点：a.症状复杂多样，其典型体验是：患者感到不能控 制自身的思维、情感、意志，对症状感到无能为力，症状多与精神因素 有关，如对考试的焦虑、担心等。b.症状多变，时轻时重，波动与心理 社会因素有关，病程多迁延。c.可有一定的自制力。d.社会功能相对良好 但有下降。
(1) Diagnosis: Neurosis (2) Diagnostic Basis: 1.The client's symptoms have persisted for six months and severely impact their daily life and work, justifying a diagnosis of neurosis.2. Key points for diagnosing neurosis: a. Symptoms are complex and varied, with typical experiences including a sense of loss of control over thoughts, emotions, and will, and feeling powerless against symptoms, which are often related to psychological factors such as anxiety about exams.b.Symptoms are fluctuating and vary in intensity, influenced by psychosocial factors, with a prolonged course.c.There is a certain degree of self-control.d.Social functioning remains relatively good but has a decline.</p>
<p>(1) Generalized Anxiety Disorder (GAD).</p>
<p>(2) The diagnostic basis includes: 1. Persistent Excessive Worry: The client shows ongoing excessive worry about her boyfriend's changing attitude, future marital status, and fear of becoming a "leftover woman" 2. Physical Symptoms: After a call with her boyfriend, the client experienced chest tightness, palpitations, shortness of breath, dizziness, and leg weakness, common physical manifestations of anxiety.Multiple hospital visits revealed no significant physical abnormalities, supporting the role of psychological factors.3. Functional Impairment: The client's decreased work efficiency and persistent emotional distress indicate that anxiety is impacting her daily life and work functionality.4. Long-term Psychological Stress: Strict parental control and negative evaluations of men from her mother, along with warnings about premarital sex, have caused the client to keep a distance in relationships, creating internal conflict.Her father's abandonment and her mother's negative comments have intensified her mistrust of men and fear of marriage.5. Negative Emotional and Cognitive Impact: The client's worry about future marriage and sensitivity to her boyfriend's attitude changes reflect negative emotional and cognitive effects.In summary, the client's symptoms align with the diagnostic criteria for Generalized Anxiety Disorder, characterized by excessive worry, physical symptoms, functional impairment, and a prolonged anxiety state.emerges in their responses: GPT-4o tends to offer more detailed and specific answers, while PsycoLLM generally provides more concise responses.
求助者生活在一个单亲家庭，父母多年前离异，她一直跟着母亲生活，母亲没有再婚。从学生时代起，母亲就严格禁止她单独与异性来往；大学毕业后，她开始谈恋爱，母亲 时常告诫她：男人都不可靠，朝三暮四、喜新厌旧，交往时一定要慎重。并且郑重警告她，如果敢在婚前发生性行为，就和她断绝母女关系。求助者一直与母亲相依为命，觉 得母亲是为了自己而没有再婚，自己不该违背母亲的意愿；想到父亲当年抛弃她们母女的行为，感到男人确实不太可靠。因此与几任男友交往都无果而终。一年前认识了一位 从海外留学归来的男士，身材、相貌、经济条件都让她非常满意。但由于母亲的告诫，求助者与该男士交往时始终保持距离。因为多次拒绝了男士发生亲密关系的要求，她自 己感觉男士开始 她，跟她在一起时不再像刚认识时那样关心她。半年前的一天，她与男士通过电话之后，突然感觉胸闷、心慌、气短、头晕、两腿无力，此后便经常提心吊胆， 担心男友不再喜欢她……心慌气短等症状也时有出现，曾到多家医院检查，</p>
<p>C. Effect of Difficulty</p>
<p>We further conduct experiments to compare the results of PsycoLLM, Qwen1.5-14B and Qwen-turbo on the Level 2 and Level 3 Psychological Counselor Exams, as detailed in Table VI.Among these models, the performance on theoretical proficiency is generally superior in the Level 3 examination compared to the Level 2 examination across most indicators.This disparity may be attributed to the Level 2 examination's requirement for a more extensive understanding of psycho-logical concepts and a deeper comprehension of the relevant theoretical frameworks.However, it is noteworthy that these models exhibit better performance on the professional ethics and case analysis of the Level 2 exam compared to the Level 3 exam.These observation also suggests that under certain circumstances, LLMs may perceive difficulty differently compared to humans.Not all aspects considered complex by humans necessarily pose challenges for models.</p>
<p>D. Discussion</p>
<p>a) Model Size: On the one hand, within the same model families, models with larger parameters generally obtain better results in Table IV.For instance, models such as YI-1.5-9B and Qwen1.5-14B-Chatdemonstrate better performance compared to their smaller counterparts, YI-1.5-6B and Qwen1.5-7B-Chat,respectively.This performance enhancement can be attributed to the increased capacity of models with larger parameters to capture complex patterns and effectively learn text representations from the data.This observation aligns with findings from a broad spectrum of existing studies that emphasize the advantages of larger model sizes in terms of learning capacity and performance.On the other hand, when compared models from different model families, this findings have slight difference.For example, YI-1.5-9B outperform Baichuan2-13B and Qwen1.5-14B-Chat in the MCQ evaluation.This discrepancy shows that training corpus, and other factors such as training procedures also significantly influence performance.</p>
<p>b) Model preference in MMCQs: In our experiments, we observe distinct response patterns from different models when addressing MMCQs.For example, GPT-3.5-turbo and GPT-4o frequently provide a single option in MMCQs, whereas Qwen-turbo tends to select all available options.Therefore, GPT-3.5-turbo and GPT-4o have a large improvements when shifting from standard accuracy measure to elastic accuracy metric.The main reason might be that, although GPT-3.5turbo and GPT-4 are multilingual, they primarily focus on English.In English-speaking contexts, MMCQs are less prevalent compared to SMCQs.Consequently, the training data for these models may predominantly contains SMCQs and lack a substantial MMCQ dataset.This discrepancy leads the models to exhibit a tendency to select only one answer when presented with MMCQs.c) Fine-tuned Models vs. General Models: Among these models, EmoLLM, MindChat are fine-tuned with the psychology-related dataset and others are for general domain.Specifically, EmoLLM and MindChat leverage empathetic conversation data for fine-tuning, which contributes to their relatively higher performance in Case-Based QA than general LLMs.However, due to their limited incorporation of knowledge-based data, their performance in MCQ evaluations is average.</p>
<p>VII. CONCLUSION</p>
<p>In this paper, to enhance LLM in the psychological domain, we propose a high-quality psychological dataset.Specifically for multi-turn dialogue, we employ a comprehensive pipeline that includes stages of generation, evidence support, and refinement.For knowledge-based QA, we use a teacher-student LLM-based agent to enhance data generation.Additionally, we develop a benchmark based on psychological examinations.Fine-tuning LLMs with the proposed dataset results in the development of PsycoLLM, which demonstrates superior performance on the benchmark, excelling in both knowledge mastery and case analysis compared to other LLMs.</p>
<p>This study primarily focuses on the development and evaluation of a language model for psychological applications.However, it is important to note that multimodal LLMs, which incorporate both video and speech inputs, are potentially more effective for understanding user emotions due to their ability to process and integrate multiple modalities of data.The creation of multimodal datasets and benchmarks in the psychological domain warrants further investigation.We leave it for future work.</p>
<p>Fig. 1 .
1
Fig. 1.Overview of dataset preparation.</p>
<p>Fig. 2 .
2
Fig. 2. The topic distribution.</p>
<p>Fig. 3 .
3
Fig. 3. Word cloud map of psychological consultants' response in single-turn QA dataset.</p>
<p>You are an expert proficient in linguistics and psychology.Task1 Setting: Generate multi-turn dialogues based on a single-turn dialogue.Dialogue Constraints: … Output Format: … Please rewrite the following Q&amp;A content: {Single-turn QA} into the form of a multi-round dialogue.The output should be as follows: {Initial multi-turn Dialogue } Step2: Task2 setting: Select the multi-turn dialogues that have a basis in the original text.{Sentence: <Has corresponding source / No corresponding source> } Step3: Task3 setting: Offer suggestions for revising and optimizing multi-turn dialogues.{Suggestion: …; Optimized multi-turn dialogue} Prompts for Converting Single-turn Dialogue to Multi-turn Dialogue AnswerLevel: 10; LikeNum: 2 (At the time of initial data acquisition)</p>
<p>......I feel like I can never truly accept myself.Sometimes I think I've done quite a few good things, but deep down, I still can't accept myself, which makes me feel very troubled.</p>
<p>Fig. 4 .
4
Fig. 4. Examples of the generated multi-turn dialogue data: Step 1 involves data generation, Step 2 focuses on evidence judgment and integration, and Step 3 entails revision for aspects such as empathy.</p>
<p>内隐记忆是指()的记忆。 选项: A.受意识控制B.没有意识到C.对知识技能D.对情绪情感(English version : Implicit memory refers to memory that is ().Options: A. controlled by consciousness B. not consciously aware C. related to knowledge and skills D. related to emotions and feelings) Answer: B, 内隐记忆是个体并没有意识到，过去的经验却对当前的活动产 生了影响，又称自动的、无意识的记忆。(English version: B, implicit memory refers to memory that an individual is not consciously aware of, but past experiences influence current activities.It is also known as automatic or unconscious memory.)焦虑性人格障碍的特点是() 。 选项：A.总是感到不安B.对他人意见过分敏感 C.一贯猜疑偏执 D.习惯性夸 大潜在危险(English version : The characteristics of anxiety personality disorder are ( ).Options: A. Always feeling uneasy B. Overly sensitive to others' opinions C. Consistently suspicious and paranoid D. Habitually exaggerating potential dangers)</p>
<p>Fig. 5 .
5
Fig. 5. Examples of MCQs in the proposed benchmark.</p>
<p>但没有查出明显器质性病变…… QuestionFig. 6 .
6
Fig. 6.Examples of case-based QA, including corresponding references and responses generated by PsycoLLM and GPT-4o.</p>
<p>TABLE I PROMPT
I
USED TO GENERATE THE MULTI-TURN DIALOGUE.</p>
<p>You are a psychology professor and are proficient in psychology and linguistics.... From the perspective of a professional psychology professor, please make a strict judgment based on the given four indicators.1. Empathy, whether the psychologist can truly understand the emotions and needs of the visitor, express sincere care and understanding ... 2. Supportive, whether the psychologist expresses listening to the visitor through verbal feedback ... 3. Guiding, whether the psychologist guides the visitor through questioning or providing choices... 4. Safety, whether it may have a negative impact on visitors...
..If there is no corresponding source, simply output "No corresponding source"[Important ]Output format:response in multi-turn dialogue:<Original dialogue>Source:<From visitor's self description/From doctor's reply/No corresponding source>:<Corresponding original text>Dialogue Evaluation
contains sufficiently substantive content for meaningful analysis.•Exclusion of low-engagement answers: Answers with fewer than 5 likes are excluded to prioritize content that has been deemed valuable by users.The number of likes functions as a measure of public endorsement of the response.A high number of likes suggests a broad consensus among users regarding the validity and reliability of the answer.• Exclusion of responses from lower-level counselors or</p>
<p>TABLE II THE
II
STATISTICS OF MULTI-TURN DIALOGUE DATA.
CATEGORYREPORT## Context-Response Pairs11511# Avg. Turns per Dialogue5.90# Avg. Tokens per Turn136.28# Avg. Tokens per Question43.62# Avg. Tokens per Answer92.67
B. Multi-Turn Dialogue Construction</p>
<p>TABLE III THE
III
STATISTICS FOR THE BENCHMARK DATASET, WHERE MCQ1 INDICATES MCQ WITH A SINGLE CORRECT OPTION, AND MCQ2 DENOTES MCQ WITH MULTIPLE CORRECT OPTIONS.LEVELS 2 AND 3 CORRESPOND TO THE LEVEL 2 AND LEVEL 3 PSYCHOLOGICAL EXAMINATIONS, RESPECTIVELY, AS ADMINISTERED IN AUTHORITATIVE PSYCHOLOGICAL COUNSELING EXAMINATIONS IN CHINA.
Professional ethicsTheoretical knowledgeCase AnalysisCATEGORYSMCQMMCQSMCQMMCQSMCQMMCQQALEVEL 2484833722824521444LEVEL 3727256636333845540OTHERS323824119216520916TOTAL1521581144783748878100</p>
<p>TABLE IV COMPARISONS
IV
OF DIFFERENT MODEL ON THE PROPOSED BENCHMARK.ETHICS, THEORY AND CASE ARE PROFESSIONAL ETHICS, THEORETICAL PROFICIENCY AND CASE ANALYSIS, RESPECTIVELY.THE UNDERLINED NUMBER INDICATES THE ELASTIC ACCURACY FOR MMCQ.THE AVERAGE VALUE REPRESENTS THE OVERALL AVERAGE OF THE STANDARD ACCURACY RATES AND THE VALUE IN PARENTHESES DENOTES THE MEAN OF THE STANDARD ACCURACY FOR SMCQ AND THE ELASTIC ACCURACY FOR MMCQ.
MODELSMCQETHICS MMCQSMCQTHEORY MMCQSMCQCASE MMCQAVG.CHATGLM3-6B63.8137.97 52.2146.3625.08 36.9637.9315.94 29.1237.84 (44.39)YI-1.5-6B71.0540.50 54.7963.7226.27 41.6344.5118.90 27.9744.15 (50.61)QWEN1.5-7B-CHAT79.8153.16 64.5061.8827.33 43.2340.8020.12 39.5147.18 (54.95)LLAMA3-CHINESE42.1018.35 25.8429.5413.64 18.6821.3910.70 21.9222.62 (26.57)BAICHUAN2-13B64.4739.87 50.4152.4925.69 37.6137.138.9929.2438.10 (45.22)GPT-3.5-TURBO74.3422.78 43.9855.9015.01 36.5249.736.7133.6237.41 (49.01)GPT-4O88.1533.54 54.7974.6524.10 45.0765.5313.67 34.5349.94 (60.45)QWEN1.5-14B-CHAT80.2655.69 67.9370.6932.44 49.6841.0121.52 41.8850.26 (58.57)QWEN-TURBO82.2360.75 73.2569.9629.81 48.1041.5524.4845.1151.38 (60.03)YI-1.5-9B81.5750.63 66.4070.5133.67 50.3950.5321.86 41.1751.46 (60.09)MINDCHAT55.2613.29 30.1333.947.5523.359.654.7812.0920.74 (27.40)EMOLLM55.2620.25 33.0638.5412.75 24.4024.5911.88 22.2827.21 (33.02)PSYCOLLM88.8169.62 74.2072.6348.59 54.1255.5835.0742.8961.71 (64.70)</p>
<p>TABLE V COMPARISONS
V
OF DIFFERENT MODEL ON THE PROPOSED BENCHMARK, WHERE R-1, R-L, B-4 AND BS ARE ROUGE-1, ROUGE-L, BLEU-4 AND BERTSCORE.
MODELR-1CASE R-L B-4BSCHATGLM322.0014.12 1.82 64.44BAICHUAN2-13B24.0617.74 1.93 64.43LLAMA3-CHINESE23.1117.28 1.25 62.18GPT-3.5-TURBO22.9214.19 1.59 64.86GPT-4O22.0915.36 1.39 64.69QWEN1.5-14B22.5113.41 1.49 64.16QWEN-TURBO21.7713.21 1.34 63.97YI-1.5-9B21.9315.35 1.39 64.51MINDCHAT24.3417.49 1.71 63.59EMOLLM23.3417.981.9463.41PSYCOLLM24.4517.452.0465.29</p>
<p>TABLE VI COMPARISONS
VI
OF DIFFERENT MODEL ON THE LEVEL 2 AND LEVEL 3.
MODELCATE.SMCQETHICS MMCQSMCQTHEORY MMCQSMCQCASE MMCQPSYCOLLMLevel 2 Level 390.91 87.5072.92 77.08 68.06 72.1170.15 74.7347.14 51.91 51.10 56.9157.55 50.7433.64 36.7041.39 44.57QWEN1.5-14BLevel 2 Level 383.34 76.3962.50 70.65 51.39 62.2767.35 69.3830.26 48.83 27.82 47.0243.26 36.7922.89 19.1240.38 40.54QWEN-TURBOLevel 2 Level 387.50 83.3364.58 76.56 45.83 61.8063.79 69.0832.01 49.85 36.64 53.2843.26 39.9423.83 20.4343.14 42.01
https://chat.openai.com/
https://kimi.moonshot.cn/
https://github.com/huggingface/transformers
https://github.com/hiyouga/LLaMA-Factory
https://github.com/baichuan-inc/Baichuan2
https://github.com/seanzhang-zhichen/llama3-chinese
https://github.com/01-ai/Yi
https://github.com/X-D-Lab/MindChat
https://github.com/SmartFlowAI/EmoLLM</p>
<p>. 根据区分心理正常与异常的三原则，该求助者的主客观世界是统一的， 有自知力，并且能够主动求医，无幻觉妄想等精神病的症状，因此可 以排除精神病性问题。, </p>
<p>. 求助者目前情绪和躯体症状与现实因素无密切关系，没有明显的道德 色彩， 因此心理冲突属于变形冲突。, </p>
<p>) The diagnosis is likely Generalized Anxiety Disorder (GAD). 根据许又新神经症评定标准，该求助者：a．病程：半年左右。b．精 神痛苦程度：比较严重，依靠自己难以摆脱。c．社会功能：工作效率显 著下降， 需要减少工作量。, </p>
<p>Diagnostic Basis: 1. According to the three principles for distinguishing between normal and abnormal psychology, the client's subjective and objective worlds are consistent, with self-awareness, and they are actively seeking help without symptoms of psychosis such as hallucinations or delusions, thus excluding psychotic disorders. </p>
<p>The client's current emotional and physical symptoms are not closely related to real-life factors and lack significant moral overtones, indicating that the psychological conflict is a distorted one. </p>
<p>According to the criteria for neurosis, the client: a. Duration: approximately six months. b. Severity of emotional distress: relatively severe, difficult to overcome independently. c. Social Function: Significant decline in work efficiency. requiring a reduction in workload. REFERENCES</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Baichuan 2: Open large-scale language models. A Yang, B Xiao, B Wang, B Zhang, C Bian, C Yin, C Lv, D Pan, D Wang, D Yan, arXiv:2309.103052023arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Summit: Iterative text summarization via chatgpt. H Zhang, X Liu, J Zhang, Findings of the Association for Computational Linguistics: EMNLP 2023. 202310657</p>
<p>Zero-shot information extraction via chatting with chatgpt. X Wei, X Cui, N Cheng, X Wang, X Zhang, S Huang, P Xie, J Xu, Y Chen, M Zhang, arXiv:2302.102052023arXiv preprint</p>
<p>Doctorglm: Fine-tuning your chinese doctor is not a herculean task. H Xiong, S Wang, Y Zhu, Z Zhao, Y Liu, L Huang, Q Wang, D Shen, arXiv:2304.010972023arXiv preprint</p>
<p>Chatlaw: Open-source legal large language model with integrated external knowledge bases. J Cui, Z Li, Y Yan, B Chen, L Yuan, arXiv:2306.160922023arXiv preprint</p>
<p>Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt. Y Chen, Z Wang, X Xing, Z Xu, K Fang, J Wang, S Li, J Wu, Q Liu, X Xu, arXiv:2310.158962023arXiv preprint</p>
<p>Psy-llm: Scaling up global mental health psychological services with ai-based large language models. T Lai, Y Shi, Z Du, J Wu, K Fu, Y Dou, Z Wang, arXiv:2307.119912023arXiv preprint</p>
<p>Soulchat: Improving llms' empathy, listening, and comfort abilities through fine-tuning with multi-turn empathy conversations. Y Chen, X Xing, J Lin, H Zheng, Z Wang, Q Liu, X Xu, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support. H Qiu, H He, S Zhang, A Li, Z Lan, arXiv:2305.004502023arXiv preprint</p>
<p>Emollm. Emollm, 2024</p>
<p>Psybench: a balanced and in-depth psychological chinese evaluation benchmark for foundation models. J Zhang, H He, N Song, S He, H Qiu, A Li, L Ma, Z Lan, arXiv:2311.098612023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Alpaca: A strong, replicable instructionfollowing model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford Center for Research on Foundation Models. 32023</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>T Glm, A Zeng, B Xu, B Wang, C Zhang, D Yin, D Rojas, G Feng, H Zhao, H Lai, arXiv:2406.12793Chatglm: A family of large language models from glm-130b to glm-4 all tools. 2024arXiv preprint</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, The Eleventh International Conference on Learning Representations. </p>
<p>Yi: Open foundation models by 01. A Young, B Chen, C Li, C Huang, G Zhang, G Zhang, H Li, J Zhu, J Chen, J Chang, arXiv:2403.046522024arXiv preprint</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, Y Fu, Advances in Neural Information Processing Systems. 202436</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. </p>
<p>Cmb: A comprehensive medical benchmark in chinese. X Wang, G H Chen, D Song, Z Zhang, Z Chen, Q Xiao, F Jiang, J Li, X Wan, B Wang, arXiv:2308.088332023arXiv preprint</p>
<p>Y Dai, D Feng, J Huang, H Jia, Q Xie, Y Zhang, W Han, W Tian, H Wang, arXiv:2310.05620Laiw: A chinese legal large language models benchmark (a technical report). 2023arXiv preprint</p>
<p>Psyeval: A comprehensive large language model evaluation benchmark for mental health. H Jin, S Chen, M Wu, K Q Zhu, arXiv:2311.091892023arXiv preprint</p>
<p>A benchmark for understanding dialogue safety in mental health support. H Qiu, T Zhao, A Li, S Zhang, H He, Z Lan, CCF International Conference on Natural Language Processing and Chinese Computing. Springer2023</p>
<p>Q Huang, M Tao, C Zhang, Z An, C Jiang, Z Chen, Z Wu, Y Feng, arXiv:2305.15062Lawyer llama technical report. 2023arXiv preprint</p>
<p>Pmc-llama: toward building open-source language models for medicine. C Wu, W Lin, X Zhang, Y Zhang, W Xie, Y Wang, Journal of the American Medical Informatics Association. e0452024</p>
<p>Huatuogpt, towards taming language model to be a doctor. H Zhang, J Chen, F Jiang, F Yu, Z Chen, G Chen, J Li, X Wu, Z Zhiyi, Q Xiao, Findings of the Association for Computational Linguistics: EMNLP 2023. 202310885</p>
<p>Huatuogpt-ii, one-stage training for medical adaption of llms. J Chen, X Wang, A Gao, F Jiang, S Chen, H Zhang, D Song, W Xie, C Kong, J Li, arXiv:2311.097742023arXiv preprint</p>
<p>Disc-lawllm: Fine-tuning large language models for intelligent legal services. S Yue, W Chen, S Wang, B Li, C Shen, S Liu, Y Zhou, Y Xiao, S Yun, W Lin, arXiv:2309.113252023arXiv preprint</p>
<p>Calibration of transformerbased models for identifying stress and depression in social media. L Ilias, S Mouzakitis, D Askounis, IEEE Transactions on Computational Social Systems. 1122023</p>
<p>Factors influencing mental health among youth during the covid-19 lockdown: A cross-sectional study in bangladesh. A Hossain, IEEE Transactions on Computational Social Systems. 2024</p>
<p>Ensemble hybrid learning methods for automated depression detection. L Ansari, S Ji, Q Chen, E Cambria, IEEE transactions on computational social systems. 1012022</p>
<p>Towards interpretable mental health analysis with large language models. K Yang, S Ji, T Zhang, Q Xie, Z Kuang, S Ananiadou, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Mentallama: interpretable mental health analysis on social media with large language models. K Yang, T Zhang, Z Kuang, Q Xie, J Huang, S Ananiadou, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Behavioral information feedback with large language models for mental disorders: Perspectives and insights. M Yang, Y Tao, H Cai, B Hu, IEEE Transactions on Computational Social Systems. 1132024</p>
<p>Heterogeneous graph attention networks for depression identification by campus cyberactivity patterns. M Yang, Z Li, Y Gao, C He, F Huang, W Chen, IEEE Transactions on Computational Social Systems. 2024</p>
<p>Racer: An llm-powered methodology for scalable analysis of semi-structured mental health interviews. S H Singh, K Jiang, K Bhasin, A Sabharwal, N Moukaddam, A B Patel, arXiv:2402.026562024arXiv preprint</p>
<p>Reading users' minds from what they say: An investigation into llm-based empathic mental inference. Q Zhu, L Chong, M Yang, J Luo, arXiv:2403.133012024arXiv preprint</p>
<p>Sunnie: An anthropomorphic llm-based conversational agent for mental well-being activity recommendation. S Wu, F Han, B Yao, T Xie, X Zhao, D Wang, arXiv:2405.138032024arXiv preprint</p>
<p>Psyqa: A chinese dataset for generating long counseling text for mental health support. H Sun, Z Lin, C Zheng, S Liu, M Huang, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Mental-llm: Leveraging large language models for mental health prediction via online text data. X Xu, B Yao, Y Dong, S Gabriel, H Yu, J Hendler, M Ghassemi, A K Dey, D Wang, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies20248</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Rouge: A Rackage for Automatic Evaluation of Summaries. C.-Y Lin, Text summarization branches out. 2004</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Bertscore: Evaluating text generation with bert. T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, International Conference on Learning Representations. 2019</p>
<p>Llama3-chinese. L C Zhang, L U Xin, 2024</p>
<p>Mindchat: Psychological large language model. D X Xin Yan, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
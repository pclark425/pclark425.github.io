<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4788 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4788</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4788</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-bf442ab269074665a68e4dbbe19e4efc97862541</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bf442ab269074665a68e4dbbe19e4efc97862541" target="_blank">Large Memory Layers with Product Keys</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A structured memory which can be easily integrated into a neural network and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead is introduced.</p>
                <p><strong>Paper Abstract:</strong> This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4788.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4788.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PKM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Product Key Memory (PKM) augmented Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large trainable key-value memory layer using product-structured keys and sparse top-k reads, integrated into transformer models to drastically increase capacity with little added compute; applied to large-scale language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transformer + Product Key Memory (PKM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer language model where one or more feed-forward (FFN) blocks are replaced by a Product Key Memory layer: a learnable key-value store where keys are factorized as the Cartesian product of two sub-key codebooks (product keys), reads are multi-head and sparse (top-k per head), and values are trainable vectors updated sparsely.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value memory (trainable, retrieval-augmented, sparse read/write)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory implemented as a large trainable value table indexed by product keys: two sub-key codebooks define an implicit |C|x|C'| set of keys (|K| up to ~1M slots). For each input a query network produces queries (multiple heads), each head selects k nearest sub-keys in each sub-codebook, implicitly yielding k^2 candidate keys; the top-k keys (by inner product) are selected and aggregated with a softmax-weighted sum of their values. Only the selected value slots are updated (sparse updates). BatchNorm on the query, multi-head queries, and higher optimizer LR for sparse value updates are used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Large-scale language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-token prediction (language modeling) on a very large news corpus (Common Crawl / CC-News) where models are evaluated by test-set perplexity; experiments include large-scale training (28B words) and smaller ablations on a reduced setting.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Common Crawl News corpus (CC-News) — 28 billion-word training split (and separate validation/test of 5k articles)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Multiple reported results. Representative values (test perplexity): Dimension=1024: 12-layer no-memory baseline 17.7 → 12-layer + 1 PKM 15.6 → +2 PKM 14.8 → +3 PKM 14.5. Dimension=1600: 12-layer no-memory 15.0 → 12-layer +1 PKM 13.7. A 12-layer PKM model (1024 dim) outperforms a 24-layer memoryless transformer (1024 dim) which had perplexity 16.0. Ablations: small-model baseline without memory (6-layer setting) perplexity ~23.0; adding a 1M-slot PKM (with BatchNorm) reduces perplexity to 18.0 in that setting. Inference speed: PKM models (with product keys) maintain ~36k words/sec across memory sizes, while flat-key implementations slow dramatically (examples in table: flat 1M keys ~1.2k w/s vs product keys ~35.7k w/s).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>See above baselines: e.g., Dimension=1024, 12-layer transformer without memory: perplexity 17.7 (test); 24-layer memoryless baseline: 16.0 (1024 dim). Small-model no-memory baseline (6-layer ablation) perplexity ~23.0.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Adding PKM consistently improves perplexity more than simply increasing number of transformer layers: a 12-layer model with PKM outperforms a 24-layer memoryless model while being ~2x faster at inference. Larger memory sizes (up to 1M slots) reduce perplexity (e.g., in ablations from ~22.8 at 16k to ~18.0 at 1M with BatchNorm). Product keys substantially outperform flat (fully explicit) keys in memory usage, perplexity, parameter count, and inference speed; product-key lookup yields ~O(sqrt(|K|) * d_q) comparisons vs O(|K| * d_q) for flat keys. BatchNorm on queries and multi-head queries increase memory usage and downstream performance; memory is most beneficial when inserted at intermediate transformer layers (layers 4-5 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported limitations and challenges: (1) Large memories can be underutilized without query BatchNorm (example: 1M-slot memory used only ~25.8% of slots without BatchNorm, hurting performance). (2) Flat-key implementations are prohibitively slow and memory-inefficient at large |K|; product keys are needed for practical scaling. (3) Sparse updates require special optimization choices (authors used a higher Adam LR for value vectors). (4) Memory benefits depend on insertion position (intermediate layers best); inserting at input embedding layer gives worst results. (5) There is potential imbalance in key usage (measured by KL), so capacity can be wasted if access distribution is skewed. (6) Some trade-offs between number of heads, k, speed and performance: increasing heads/k improves performance but increases compute.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>A dedicated large key-value memory with product-structured keys and sparse top-k access can increase model capacity by orders of magnitude with negligible inference overhead when product keys are used; query BatchNorm and multi-head independent queries improve key coverage and utilization; inserting PKM at intermediate transformer layers is most effective; product keys deliver exact nearest-neighbor selection with much lower compute and parameter cost than flat keys; sparse updates require tuning (e.g., larger LR for values). Overall, memory augmentation is a more effective capacity/compute trade-off than simply increasing depth for large-scale language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Memory Layers with Product Keys', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4788.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4788.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse Access Memory (Rae et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Access Memory (Scaling memory-augmented neural networks with sparse reads and writes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that integrates a large key-value memory into neural networks using sparse reads/writes and an external approximate indexing structure to speed nearest-neighbor retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling memory-augmented neural networks with sparse reads and writes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sparse Access Memory (Rae et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory-augmented neural network that leverages sparse reads and writes into a large external key-value memory, using approximate nearest-neighbor indexing structures to enable scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value memory with approximate nearest-neighbor index (sparse read/write)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses an external approximate indexing structure (for nearest-neighbor search) to select a sparse subset of memory slots to read/write, enabling large memories; indexing is approximate and separate from the neural net parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General memory-augmented learning tasks (related work referenced for large-memory scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to scale memory-augmented neural networks for tasks that benefit from large external memories; specific tasks not enumerated in this paper but referenced as prior work applying sparse memory reads/writes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as prior work that also uses sparse reads/writes to scale memory size, but relies on an external approximate indexing structure which the PKM paper argues is a drawback because that index must be relearned periodically and is approximate in high dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>According to the paper, this approach depends on an approximate external index that needs to be re-learned periodically to avoid catastrophic drift during training; approximation may be inaccurate in high-dimensional spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>PKM is presented as an alternative that incorporates exact key selection (via product-structured keys) inside the network and avoids dependence on an external approximate index, addressing scalability and training stability concerns raised for sparse-access approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Memory Layers with Product Keys', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4788.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4788.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Cache / Continuous Cache (Grave et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous cache / Neural cache models for language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Memory-based approaches (cache-style) that augment language models with a dynamic cache of recent contexts/representations to improve language modeling; these approaches face scaling challenges that prior work addresses with approximate lookups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving neural language models with a continuous cache</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural cache / Continuous cache models</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Language model augmentations that maintain a cache of recent hidden states or representations and retrieve from this cache to improve prediction of recurring tokens and context-specific patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic cache memory (retrieval from recent contexts), typically using approximate lookups at test time for scalability</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Caches recent activations or contexts (a continuous cache) and retrieves similar entries to influence next-token predictions; scaling usually handled via approximate lookup methods at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling (improving next-token prediction via cache of recent contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Augment base language models with a cache of recent examples/activations to better model token repetition and local contexts; evaluated with perplexity on language modeling corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited as prior work that helps language models by using caches but suffers from scaling issues; such models often use approximate lookup at test time, which the PKM approach aims to avoid by offering exact and efficient search via product keys.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scaling caches to very large sizes is challenging; prior approaches circumvent this by approximate test-time lookups which can be inaccurate in high-dimensional spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>The PKM layer targets the same high-level goal — augmenting language models with large memories — but proposes a trainable, exact-search-friendly product-key design to scale to much larger capacities without relying on approximate external indexes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Memory Layers with Product Keys', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scaling memory-augmented neural networks with sparse reads and writes <em>(Rating: 2)</em></li>
                <li>Improving neural language models with a continuous cache <em>(Rating: 2)</em></li>
                <li>Unbounded cache model for online language modeling with open vocabulary <em>(Rating: 1)</em></li>
                <li>Product Quantization for Nearest Neighbor Search <em>(Rating: 1)</em></li>
                <li>Billion-scale similarity search with gpus <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4788",
    "paper_id": "paper-bf442ab269074665a68e4dbbe19e4efc97862541",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "PKM",
            "name_full": "Product Key Memory (PKM) augmented Transformer",
            "brief_description": "A large trainable key-value memory layer using product-structured keys and sparse top-k reads, integrated into transformer models to drastically increase capacity with little added compute; applied to large-scale language modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Transformer + Product Key Memory (PKM)",
            "agent_description": "A transformer language model where one or more feed-forward (FFN) blocks are replaced by a Product Key Memory layer: a learnable key-value store where keys are factorized as the Cartesian product of two sub-key codebooks (product keys), reads are multi-head and sparse (top-k per head), and values are trainable vectors updated sparsely.",
            "memory_type": "external key-value memory (trainable, retrieval-augmented, sparse read/write)",
            "memory_description": "Memory implemented as a large trainable value table indexed by product keys: two sub-key codebooks define an implicit |C|x|C'| set of keys (|K| up to ~1M slots). For each input a query network produces queries (multiple heads), each head selects k nearest sub-keys in each sub-codebook, implicitly yielding k^2 candidate keys; the top-k keys (by inner product) are selected and aggregated with a softmax-weighted sum of their values. Only the selected value slots are updated (sparse updates). BatchNorm on the query, multi-head queries, and higher optimizer LR for sparse value updates are used.",
            "task_name": "Large-scale language modeling",
            "task_description": "Next-token prediction (language modeling) on a very large news corpus (Common Crawl / CC-News) where models are evaluated by test-set perplexity; experiments include large-scale training (28B words) and smaller ablations on a reduced setting.",
            "benchmark_name": "Common Crawl News corpus (CC-News) — 28 billion-word training split (and separate validation/test of 5k articles)",
            "performance_with_memory": "Multiple reported results. Representative values (test perplexity): Dimension=1024: 12-layer no-memory baseline 17.7 → 12-layer + 1 PKM 15.6 → +2 PKM 14.8 → +3 PKM 14.5. Dimension=1600: 12-layer no-memory 15.0 → 12-layer +1 PKM 13.7. A 12-layer PKM model (1024 dim) outperforms a 24-layer memoryless transformer (1024 dim) which had perplexity 16.0. Ablations: small-model baseline without memory (6-layer setting) perplexity ~23.0; adding a 1M-slot PKM (with BatchNorm) reduces perplexity to 18.0 in that setting. Inference speed: PKM models (with product keys) maintain ~36k words/sec across memory sizes, while flat-key implementations slow dramatically (examples in table: flat 1M keys ~1.2k w/s vs product keys ~35.7k w/s).",
            "performance_without_memory": "See above baselines: e.g., Dimension=1024, 12-layer transformer without memory: perplexity 17.7 (test); 24-layer memoryless baseline: 16.0 (1024 dim). Small-model no-memory baseline (6-layer ablation) perplexity ~23.0.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Adding PKM consistently improves perplexity more than simply increasing number of transformer layers: a 12-layer model with PKM outperforms a 24-layer memoryless model while being ~2x faster at inference. Larger memory sizes (up to 1M slots) reduce perplexity (e.g., in ablations from ~22.8 at 16k to ~18.0 at 1M with BatchNorm). Product keys substantially outperform flat (fully explicit) keys in memory usage, perplexity, parameter count, and inference speed; product-key lookup yields ~O(sqrt(|K|) * d_q) comparisons vs O(|K| * d_q) for flat keys. BatchNorm on queries and multi-head queries increase memory usage and downstream performance; memory is most beneficial when inserted at intermediate transformer layers (layers 4-5 in experiments).",
            "limitations_or_challenges": "Reported limitations and challenges: (1) Large memories can be underutilized without query BatchNorm (example: 1M-slot memory used only ~25.8% of slots without BatchNorm, hurting performance). (2) Flat-key implementations are prohibitively slow and memory-inefficient at large |K|; product keys are needed for practical scaling. (3) Sparse updates require special optimization choices (authors used a higher Adam LR for value vectors). (4) Memory benefits depend on insertion position (intermediate layers best); inserting at input embedding layer gives worst results. (5) There is potential imbalance in key usage (measured by KL), so capacity can be wasted if access distribution is skewed. (6) Some trade-offs between number of heads, k, speed and performance: increasing heads/k improves performance but increases compute.",
            "key_insights": "A dedicated large key-value memory with product-structured keys and sparse top-k access can increase model capacity by orders of magnitude with negligible inference overhead when product keys are used; query BatchNorm and multi-head independent queries improve key coverage and utilization; inserting PKM at intermediate transformer layers is most effective; product keys deliver exact nearest-neighbor selection with much lower compute and parameter cost than flat keys; sparse updates require tuning (e.g., larger LR for values). Overall, memory augmentation is a more effective capacity/compute trade-off than simply increasing depth for large-scale language modeling.",
            "uuid": "e4788.0",
            "source_info": {
                "paper_title": "Large Memory Layers with Product Keys",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Sparse Access Memory (Rae et al.)",
            "name_full": "Sparse Access Memory (Scaling memory-augmented neural networks with sparse reads and writes)",
            "brief_description": "A prior approach that integrates a large key-value memory into neural networks using sparse reads/writes and an external approximate indexing structure to speed nearest-neighbor retrieval.",
            "citation_title": "Scaling memory-augmented neural networks with sparse reads and writes",
            "mention_or_use": "mention",
            "agent_name": "Sparse Access Memory (Rae et al.)",
            "agent_description": "Memory-augmented neural network that leverages sparse reads and writes into a large external key-value memory, using approximate nearest-neighbor indexing structures to enable scalability.",
            "memory_type": "external key-value memory with approximate nearest-neighbor index (sparse read/write)",
            "memory_description": "Uses an external approximate indexing structure (for nearest-neighbor search) to select a sparse subset of memory slots to read/write, enabling large memories; indexing is approximate and separate from the neural net parameters.",
            "task_name": "General memory-augmented learning tasks (related work referenced for large-memory scaling)",
            "task_description": "Used to scale memory-augmented neural networks for tasks that benefit from large external memories; specific tasks not enumerated in this paper but referenced as prior work applying sparse memory reads/writes.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Mentioned as prior work that also uses sparse reads/writes to scale memory size, but relies on an external approximate indexing structure which the PKM paper argues is a drawback because that index must be relearned periodically and is approximate in high dimensions.",
            "limitations_or_challenges": "According to the paper, this approach depends on an approximate external index that needs to be re-learned periodically to avoid catastrophic drift during training; approximation may be inaccurate in high-dimensional spaces.",
            "key_insights": "PKM is presented as an alternative that incorporates exact key selection (via product-structured keys) inside the network and avoids dependence on an external approximate index, addressing scalability and training stability concerns raised for sparse-access approaches.",
            "uuid": "e4788.1",
            "source_info": {
                "paper_title": "Large Memory Layers with Product Keys",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Neural Cache / Continuous Cache (Grave et al.)",
            "name_full": "Continuous cache / Neural cache models for language modeling",
            "brief_description": "Memory-based approaches (cache-style) that augment language models with a dynamic cache of recent contexts/representations to improve language modeling; these approaches face scaling challenges that prior work addresses with approximate lookups.",
            "citation_title": "Improving neural language models with a continuous cache",
            "mention_or_use": "mention",
            "agent_name": "Neural cache / Continuous cache models",
            "agent_description": "Language model augmentations that maintain a cache of recent hidden states or representations and retrieve from this cache to improve prediction of recurring tokens and context-specific patterns.",
            "memory_type": "dynamic cache memory (retrieval from recent contexts), typically using approximate lookups at test time for scalability",
            "memory_description": "Caches recent activations or contexts (a continuous cache) and retrieves similar entries to influence next-token predictions; scaling usually handled via approximate lookup methods at test time.",
            "task_name": "Language modeling (improving next-token prediction via cache of recent contexts)",
            "task_description": "Augment base language models with a cache of recent examples/activations to better model token repetition and local contexts; evaluated with perplexity on language modeling corpora.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Cited as prior work that helps language models by using caches but suffers from scaling issues; such models often use approximate lookup at test time, which the PKM approach aims to avoid by offering exact and efficient search via product keys.",
            "limitations_or_challenges": "Scaling caches to very large sizes is challenging; prior approaches circumvent this by approximate test-time lookups which can be inaccurate in high-dimensional spaces.",
            "key_insights": "The PKM layer targets the same high-level goal — augmenting language models with large memories — but proposes a trainable, exact-search-friendly product-key design to scale to much larger capacities without relying on approximate external indexes.",
            "uuid": "e4788.2",
            "source_info": {
                "paper_title": "Large Memory Layers with Product Keys",
                "publication_date_yy_mm": "2019-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scaling memory-augmented neural networks with sparse reads and writes",
            "rating": 2
        },
        {
            "paper_title": "Improving neural language models with a continuous cache",
            "rating": 2
        },
        {
            "paper_title": "Unbounded cache model for online language modeling with open vocabulary",
            "rating": 1
        },
        {
            "paper_title": "Product Quantization for Nearest Neighbor Search",
            "rating": 1
        },
        {
            "paper_title": "Billion-scale similarity search with gpus",
            "rating": 1
        }
    ],
    "cost": 0.011411,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Memory Layers with Product Keys</h1>
<p>Guillaume Lample ${ }^{\varnothing}$, Alexandre Sablayrolles<em>, Marc ${ }^{\star}$ Aurelio Ranzato</em>, Ludovic Denoyer ${ }^{\varnothing}$, Hervé Jégou*<br>{glample,asablayrolles,ranzato,denoyer,rvj}@fb.com</p>
<h4>Abstract</h4>
<p>This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes. ${ }^{2}$</p>
<h2>1 Introduction</h2>
<p>Neural networks are commonly employed to address many complex tasks such as machine translation [43], image classification [27] or speech recognition [16]. As more and more data becomes available for training, these networks are increasingly larger [19]. For instance, recent models both in vision [29] and in natural language processing [20, 36, 28] have more than a billion parameters. The higher-capacity enables better modeling of data like natural text or images, and it also improves generalization [41, 33]. Unfortunately, increasing capacity has led to a dramatic increase of computational complexity, both at training and inference time [20].
There is a growing interest in developing architectures with reasonable computational complexity. Recently, there has been some efforts to develop high capacity architectures that operate on a limited computational budget [40, 18]. This is well illustrated by the "On-device Visual Intelligence Challenge" [5], which specifically focuses on the complexity/accuracy trade-off for image classification.
Some researchers have attempted to increase the capacity of a network without increasing its computational complexity. Most notably, Rae et al. [37] incorporate fast nearest neighbor search within a neural network architecture to leverage large key-value layers with sparse reads and writes. Their approach relies on an external indexing structure [32], which is approximate and needs to be relearned regularly while training the neural network to avoid a catastrophic drift.
In this work, we propose a key-value memory layer that can scale to very large sizes while keeping exact search on the key space. This layer dramatically increases the capacity of the overall system for a negligible computational overhead. Unlike existing models based on key-value memories (see</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of a key-value memory layer: The input $x$ is processed through a query network that produces a query vector $q$, which is compared to all the keys. The output is the sparse weighted sum over the memories associated with the selected keys. For a large number of keys $|\mathcal{K}|$, the key selection procedure becomes too expensive in practice. Our product key method is exact and makes this search process very fast.</p>
<p>Figure 1), we define keys as the concatenation of two sub-keys, in the spirit of product quantization [21]. As shown in more details in Figure 2, this structure implicitly defines a very large set of keys, each being associated with a value memory slot. The set of value vectors introduces the bulk of the parameters, as it scales quadratically with the number of sub-keys. Despite the large number of memory slots, finding the exact closest keys to the input is very efficient, typically requiring $\mathcal{O}(\sqrt{|\mathcal{K}|})$ vector comparisons, where $|\mathcal{K}|$ is the total number of memory slots. All the memory parameters are trainable, yet only a handful of memory slots are updated for each input at training time. Sparsity of key selection and parameter updates make both training and inference very efficient.</p>
<p>Our layer allows us to tackle problems where current architectures underfit given the vast amount of available data, or when they are too slow to work in practice. We thus focus on the language modeling task, integrating our memory within the popular transformer architecture [44]. This choice is motivated by the success of BERT [11] and GPT-2 [36], which demonstrated that increasing the capacity of large models directly translates to large improvements in language modeling, which in turn translates to better performance in both language understanding tasks [11, 46] and text generation [36]. Overall, our paper makes the following contributions:</p>
<ul>
<li>We introduce a new layer that provides a large capacity to a neural network for only a slight computational overhead both at train and test time.</li>
<li>Our fast indexing strategy offers exact nearest neighbor search by construction, and avoids the pitfall of relying on an indexing structure that needs to be re-learned during training.</li>
<li>We demonstrate our method within a large state-of-the-art transformer, composed of 24 layers of dimension 1600. Our method with 1 memory and 12 layers outperforms a 24-layer transformer while being twice faster at inference time. We show that adding more memory layers to transformers of various complexities provides systematic and significant improvements on our target task.</li>
</ul>
<h2>2 Related work</h2>
<p>Different approaches have been proposed to increase the capacity of neural networks without increasing too much the computational complexity. For instance, conditional computation models aim at routing inputs into very large neural networks such that only a subset of connections and/or layers are used to process each input. Different methods have been developed like large mixture of experts [40], gating techniques [3, 12, 6] or even reinforcement learning-based approaches [10].</p>
<p>Another line of research is the development of memory augmented neural networks. For instance, memory-based neural layers [47, 42] are an efficient way to represent variable length inputs for complex problems such as question answering [48]. Such memories can also operate in feature space and have various reading and writing mechanisms [23, 17]. Unfortunately, these approaches scale linearly with the size of the memory which is prohibitive for very large memories. Neural cache models [15] suffer from the same scaling issues, which are circumvented by adopting approximate lookup techniques at test time [14].</p>
<p>Discretization techniques have been intensively studied for compressing network weights [8; 38] and/or activations [7; 38] or to accelerate inference. For instance, Gerald et al. [13] propose to map an input to a low-dimensional binary code, each code being associated with one category, thus reducing the complexity of inference by avoiding the use of a final large linear layer. Another model is proposed in [45], where the authors develop a fast locality-sensitive hashing technique to approximate the dot product between large matrices and vectors in neural networks. However, exploiting binary codes or approximate techniques at training time raises several challenges in terms of optimization, because approximate indexes are not accurate in high-dimensional spaces. In our paper, we borrow some ideas from product quantization (PQ) [21]. This is an approximate search technique that maps database vectors into compact codes. However, our goal is different: we do not build an approximate index, but rather we exploit the idea to represent a large set of key vectors by a drastically smaller number of vectors, that we update by regular back-propagation. As discussed later, the selection of the closest keys is exact and inherits from the fast neighbor search of PQ.</p>
<p>Our model is also related to sparsity models which have been mainly studied in the unsupervised learning setting [34; 24]. For instance, the k-sparse autoencoder [30] only keeps the k largest values in the latent representation of an auto-encoder, similar to our memory layer but without the product keys component. In <em>winner take all</em> autoencoders [31], sparsity is induced by using mini-batch statistics, while in the sparse access memory [37] reports some speed-up by both thresholding the memory to a sparse subset, and by using efficient data structures for content-based read operations. Unfortunately, the fast access to memories rely on an approximate external indexing structure [32] that has to be re-learned periodically. Our work solves this issue by fully incorporating the key selection mechanism as a network component.</p>
<p>The transformer network [44] is the current workhorse of Natural Language Processing (NLP): it is employed ubiquitously across a large variety of tasks. Transformers are built by stacking blocks composed of self-attention layers followed by fully connected layers (dubbed FFN), as shown in Figure 3. The components of the memory layer bear similarities to the query, key and value networks used in self-attention layers with two notable differences: the keys and values do not correspond to input tokens but are free embedding vectors, and the number of values (memory size) is very large.</p>
<h2>3 Learnable product key memories</h2>
<p>We consider the design of a function $m:^{d}\rightarrow^{n}$, that will act as a layer in a neural network. The purpose of $m$ is to offer a large capacity within a neural network.</p>
<h3>3.1 Memory design</h3>
<p>High-level structure. The overall structure of our memory is illustrated by Figures 1 and 2. The memory is composed of three components: a query network, a key selection module containing two sets of sub-keys, and a value lookup table. It first computes a query that is compared to the set of product keys. For each product key, it computes a score and selects the $k$ product keys with the highest scores. The scores are then used to produce an output $m(x)$ via a weighted sum over the values associated with the selected keys. All the parameters of the memory are trainable, yet only $k$ memory slots are updated for each input. The sparse selection and parameter update make both training and inference very efficient.</p>
<p>Query generation: pre-processing network. The function $q:x\mapsto q(x)\in^{d_{\mathrm{q}}}$, referred to as the query network, maps the $d$-dimensional input to a latent space of dimensionality $d_{\mathrm{q}}$. Typically, $q$ is a linear mapping or a multi-layer perceptron that reduces the dimensionality from $d$ to $d_{\mathrm{q}}=$ 512. As keys are randomly initialized, they occupy the space relatively uniformly. Adding a batch normalization layer on the top of the query network helps increasing key coverage during training. This insight is confirmed by our ablation experiments in Section 4.5.</p>
<p>Standard key assignment and weighting. Let $q(x)$ be a query and $\mathcal{T}<em 1="1">{k}$ denote the top-k operator. Given a set of keys $\mathcal{K}=\left{k</em>$-dimensional vectors, and an input $x$,},\ldots,k_{|\mathcal{K}|}\right}$ composed of $|\mathcal{K}|d_{\mathrm{q}</p>
<p>[table] 0412 If the permutation $(i_{1},\ldots,i_{n})$ sorts numbers $(t_{1},\ldots,t_{n})$ as $t_{i_{1}}\geq t_{i_{2}}\geq\cdots\geq t_{i_{n}}$, the top-k indices are $\mathcal{T}<em 1="1">{k}(t</em>\right}$},\ldots,t_{n})=\left{i_{1},\ldots,i_{k</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the product keys. We define two discrete subsets of keys (sub-key set 1 and sub-key set 2). They induce a much larger set of keys, which are never made explicit (product keys). Given a query, we split it into two sub-queries ( $q_{1}$ and $q_{2}$ ). Selecting the $k$ closest keys ( $k=2$ in the figure) in each subset implicitly selects $k \times k$ keys. The $k$ keys maximizing the inner product with the query are guaranteed to belong to this subset, on which the search can be done efficiently.
we select the top $k$ keys maximizing the inner product with the query $q(x)$ :</p>
<p>$$
\begin{aligned}
\mathcal{I} &amp; =\mathcal{T}<em i="i">{k}\left(q(x)^{T} k</em> \
w &amp; =\text { Softmax }\left(\left(q(x)^{T} k_{i}\right)}\right) &amp; &amp; \text { # Get k nearest neighbors <em _in="\in" _mathcal_I="\mathcal{I" i="i">{i \in \mathcal{I}}\right) &amp; &amp; \text { # Normalize top-k scores } \
m(x) &amp; =\sum</em>
\end{aligned}
$$}} w_{i} v_{i} &amp; &amp; \text { # Aggregate selected values </p>
<p>Here $\mathcal{I}$ denotes the indices of the $k$ most similar keys (where the similarity measure is the inner product), and $w$ is the vector that represents the normalized scores associated with the selected keys. All these operations can be implemented using auto-differentiation mechanisms, making our layer pluggable at any location in a neural network.
Operations (2), (3) only depend on the top-k indices and are therefore computationally efficient. In contrast, the exhaustive comparison of Equation (1) is not efficient for large memories since it involves computing $|\mathcal{K}|$ inner products. To circumvent this issue, we resort to a structured set of keys, that we refer to as product keys.</p>
<p>The product key set is defined as the outer product, with respect to the vector concatenation operator, of two vector codebooks $\mathcal{C}$ and $\mathcal{C}^{\prime}$ :</p>
<p>$$
\mathcal{K}=\left{\left(c, c^{\prime}\right) \mid c \in \mathcal{C}, c^{\prime} \in \mathcal{C}^{\prime}\right}
$$</p>
<p>The total number of keys induced by this Cartesian product construction is $|\mathcal{K}|=|\mathcal{C}| \times\left|\mathcal{C}^{\prime}\right|$. The sets $\mathcal{C}$ and $\mathcal{C}^{\prime}$ both comprise a set of sub-keys of dimension $d_{\mathrm{q}} / 2$. We exploit this structure to compute the closest keys $\mathcal{I} \in(1, \ldots, K)$ efficiently. First, we split the query $q(x)$ into two sub-queries $q_{1}$ and $q_{2}$. We then compute the $k$ sub-keys in $\mathcal{C}$ (resp. $\mathcal{C}^{\prime}$ ) closest to the sub-query $q_{1}$ (resp. $q_{2}$ ):</p>
<p>$$
\mathcal{I}<em k="k">{\mathcal{C}}=\mathcal{T}</em>\right)}\left(\left(q_{1}(x)^{T} c_{i<em _mathcal_C="\mathcal{C">{i \in{1 \ldots|\mathcal{C}|}}\right), \quad \mathcal{I}</em>}^{\prime}}=\mathcal{T<em 2="2">{k}\left(\left(q</em>\right)
$$}(x)^{T} c_{j}^{\prime}\right)_{j \in{1 \ldots\left|\mathcal{C}^{\prime}\right|}</p>
<p>We are guaranteed that the $k$ most similar keys in $\mathcal{K}$ are of the form $\left{\left(c_{i}, c_{j}^{\prime}\right) \mid i \in \mathcal{I}<em _mathcal_C="\mathcal{C">{\mathcal{C}}, j \in \mathcal{I}</em>\right}$. An example of product keys with the key selection process is shown in Figure 2.}^{\prime}</p>
<h1>3.2 Complexity</h1>
<p>Searching for the top-k most similar keys when the keys have a flat representation requires $|\mathcal{K}|$ comparisons of vectors of size $d_{\mathrm{q}}$, i.e. $\mathcal{O}\left(|\mathcal{K}| \times d_{\mathrm{q}}\right)$ operations.
For product keys, we consider the setup where $|\mathcal{C}|=\left|\mathcal{C}^{\prime}\right|$, i.e. the configuration that maximizes $|\mathcal{C}| \times\left|\mathcal{C}^{\prime}\right|$ for a fixed number of sub-keys $|\mathcal{C}|+\left|\mathcal{C}^{\prime}\right|$. Since $|\mathcal{K}|=|\mathcal{C}| \times\left|\mathcal{C}^{\prime}\right|$, we have $|\mathcal{C}|=\sqrt{|\mathcal{K}|}$. We only need to compare the two sub-queries with $|\mathcal{C}|$ and $\left|\mathcal{C}^{\prime}\right|$ sub-keys of size $d_{\mathrm{q}} / 2$, which amounts to $\mathcal{O}\left(|\mathcal{C}| \times d_{\mathrm{q}} / 2+\left|\mathcal{C}^{\prime}\right| \times d_{\mathrm{q}} / 2\right)=\mathcal{O}\left(|\mathcal{C}| \times d_{\mathrm{q}}\right)=\mathcal{O}\left(\sqrt{|\mathcal{K}|} \times d_{\mathrm{q}}\right)$ operations.
Then, we need to search for the top-k keys in $\left{\left(c_{i}, c_{j}^{\prime}\right) \mid i \in \mathcal{I}<em _mathcal_C="\mathcal{C">{\mathcal{C}}, j \in \mathcal{I}</em>\right)$ operations (in practice, this could be}}^{\prime}\right}$, which is a set composed of $k^{2}$ keys of dimension $d_{\mathrm{q}}$. This can be done in $\mathcal{O}\left(k^{2} \times d_{\mathrm{q}</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left: A typical transformer block is composed by a self-attention layer followed by an FFN layer (a two layer network). Right: In our system, we replace the FFN layer with a product key memory layer, which is analogous to a sparse FFN layer with a very large hidden state. In practice, we only replace the FFN layer in N layers, where typically N ∈ {0, 1, 2}.</p>
<p>done in O(k log k) scalar operations with a priority list [1], but this choice is less compliant with GPU architectures). As a result, the overall complexity is:</p>
<p>$$\mathcal{O}\left((\sqrt{|\mathcal{K}|} + k^2) \times d_q\right)$$</p>
<p>For small values of k, and a memory of size |K| = 1024², retrieving the nearest product keys requires about 10³ less operations than an exhaustive search. As shown later in our ablation study, product keys also lead to a better performance compared to a set composed of flat keys.</p>
<h3>3.3 Multi-head memory attention</h3>
<p>We make the model more expressive with a multi-head mechanism, where each head independently computes a query used to select k keys from the memory. The memory simply sums the output m_i(x) of each head i: m(x) = ∑_i=1^H m_i(x) where H is the number of heads.</p>
<p>Each head has its own query network and its own set of sub-keys, but all heads share the same values. This is similar to the multi-head attention used in transformers, except that we do not split the query into H heads, but instead create H queries. As the query networks are independent from each other and randomly initialized, they often map the same input to very different values of the memory. In practice, for the same input we observe very little overlap between the keys selected by two different heads. This method let us increase key usage and generally improves performance. The impact of the multi-head attention mechanism is discussed in Section 4.5.</p>
<h2>4 Experiments</h2>
<p>We report results on large-scale experiments for transformer models equipped with a memory, followed by an ablation study that shows the impact of different memory components on the model performance and memory usage. We propose to replace the FFN block of some transformer layers by a memory, as presented in Figure 3. In that setting, the memory is integrated with a residual connection in the network, and the input x to the memory layer becomes x ← x + PKM(x) instead of x ← x + FFN(x). In practice, we could also keep the FFN layer and simply interleave the memory between some transformer layers.</p>
<h3>4.1 Dataset</h3>
<p>We evaluate the impact of our memory in a large scale language modeling task, where traditional models are known to underfit. The largest publicly available language modeling dataset is the One Billion Word corpus [4]. As noted in prior work [2, 9, 36], obtaining a good performance on this dataset requires tedious regularization as it is now too small for standard architectures. In our experiments, we encountered the same issues, and observed that even a small model was enough to overfit: on this dataset, for a 16 layers model with a dimensionality of 1024, we obtain a test perplexity of 25.3 when the validation perplexity starts to increase. The train perplexity is then equal to 14.8 and keeps improving while the validation perplexity deteriorates.</p>
<p>We therefore evaluate the benefit of our approach on a corpus that is 30 times larger and extracted from the public Common Crawl. The training set is composed of 28 billion words (140 GB of data) extracted from about 40 million English news articles indexed by Common Crawl corpora. The validation and test sets are both composed of 5000 news articles removed from the training set.</p>
<p>Unlike in the One Billion Word corpus, we did not shuffle sentences, allowing the model to learn long range dependencies. On this dataset, we did not observe any overfitting, and increasing the</p>
<p>model capacity systematically led to a better performance on the validation set. We tokenized the data using the tokenizer provided by the Moses toolkit [26]. To reduce the vocabulary size, we use fastBPE to apply Byte Pair Encoding (BPE) [39], with 60k BPE splits.</p>
<h1>4.2 Evaluation metrics</h1>
<p>We measure the performance of our models by reporting the perplexity on the test set. For models with memories, we report two different metrics to evaluate the usage:</p>
<ul>
<li>The memory usage that represents the fraction of accessed values: $#\left{z_{i} \neq 0\right}$</li>
<li>The KL divergence between $z$ and the uniform distribution: $\log (|\mathcal{K}|)+\sum z_{i} \log \left(z_{i}\right)$
where $z=z^{\prime} /\left|z^{\prime}\right|<em i="i">{1}$, and $z^{\prime} \in \mathbb{R}^{|\mathcal{K}|}$ is defined as $z</em>$ where $w(x)$ represents the weights of the keys accessed in the memory when the network is fed with an input $x$ from the test set (i.e., the $w(x)$ are sparse with at most $H \times k$ non-zero elements).
At test time, we expect the model to access as many keys as possible, i.e. to have a usage near $100 \%$; a lower usage means that part of the capacity is not exploited at all. The KL divergence reflects imbalance in the access patterns to the memory: if the model attends the same key for every query (while giving a tiny weight to the remaining keys), it would give a perfect usage but a very high KL, showing that the same performance could be achieved with just one value.}^{\prime}=\sum_{x} w(x)_{i</li>
</ul>
<h3>4.3 Training details</h3>
<p>We use a transformer architecture with 16 attention heads and learned positional embeddings. We consider models with 12, 16 or 24 layers, with either 1024 or 1600 dimensions. We train our models with the Adam optimizer [25], with a learning rate of $2.5 \times 10^{-4}$, with $\beta_{1}=0.9, \beta_{2}=0.98$, following the learning rate schedule of Vaswani et al. [44]. In the memory, the keys and the query network are learned with the same optimizer and learning rate as the rest of the network. Since the memory values are learned with sparse updates, we found it beneficial to learn them with a higher Adam learning rate of $10^{-3}$. We implement our models with PyTorch [35], and train them on 32 Volta GPUs. We use float16 operations to speed up training and to reduce the GPU memory usage of our models. To retrieve key indices efficiently, we perform the search over sub-keys with a fast nearest neighbors implementation by Johnson et al. [22].
For a transformer model with $L$ layers and $N$ memories, we interspersed the memories at regular intervals. For instance, for $L=16$ and $N=2$, we replace the FFN of layers 6 and 12. This way, the network can leverage information at different levels of the architecture. The impact of the memory position within the network is studied in Section 4.5. In our main experiments, we use $H=4$ memory heads, we select $k=32$ keys per head, and use $|\mathcal{K}|=512^{2}$ memory slots.</p>
<h3>4.4 Results</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Dimension</th>
<th style="text-align: center;">1024</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">1600</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">N memories</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">12 layers</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">13.7</td>
</tr>
<tr>
<td style="text-align: center;">16 layers</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">13.2</td>
</tr>
<tr>
<td style="text-align: center;">24 layers</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: Test perplexity for models with and without memory. PKM models with 12 layers outperforms 24 layer models of same dimensionality. Bold refers to models optimizing performance for a given dimension.</p>
<p>Table 1 and Figure 4 show the perplexity of different models on the test set of the CC-News corpus. We observe that increasing either the dimensionality or the number of layers leads to significant perplexity improvements in all the models. However, adding a memory to the model is more beneficial than increasing the number of layers; for instance, a model with a single memory and 12 layers outperforms a memoryless model with the same hidden dimension and 24 layers, both when the number of hidden units is 1024 and 1600. Adding 2 or 3 memory layers further improves performance.</p>
<p>Figure 4 also shows speed as measured in words per second, for different model configurations. In particular, when the internal hidden states have 1024 dimensions, a model with 12 layers and a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Trade-off between speed and perplexity on the test set. Labels on the graph represent the number of layers. Adding memory layers significantly improves the performance and has a negligible impact on the inference speed. Models with 12 layers and a Product Key Memory (PKM) outperform 24-layer models of the same dimension, while being almost twice faster at inference. In particular, a 12-layer model of dimension 1024 with a memory outperforms a model of 24 layers of the same dimension (same configuration as BERT large).
memory obtains a better perplexity than a model with 24 layers (same configuration as BERT large), and it is almost twice faster. When adding memory to large models that have internal dimensionality equal to 1600 , inference time barely increases.</p>
<h1>4.5 Ablation Study</h1>
<p>In this section we study the impact of the different components on the memory layer, and measure how they affect the model performance and the memory usage. For all experiments, we consider a transformer network with 6 layers and 8 heads. Unless specified otherwise, we consider a memory of $512^{2}=262 \mathrm{k}$ slots, with 4 memory heads, $k=32$ selected keys, and we insert it at layer 5 .</p>
<p>Memory size. We train transformer models with memories of size $|\mathcal{K}|=|\mathcal{C}| \times\left|\mathcal{C}^{\prime}\right|$, with $\left|\mathcal{C}^{\prime}\right|=$ $|\mathcal{C}|$ and $|\mathcal{C}| \in{128,256,384,512,768,1024}$. Table 2 shows that test perplexity decreases as the memory becomes larger. A model with a memory size of 16 k obtains a perplexity of 22.8 . Increasing the size to 1 M decreases the perplexity down to 18.0 while leaving the inference time unchanged. The dominant factor for inference time is the number of accessed memory values, which is governed by the number of memory heads and the parameter k , but not the memory size.</p>
<p>Query Batch Normalization. Table 2 and Figure 5 present results with and without batch normalization in the query network. We observe that for small memories the usage is always close to $100 \%$, but for a memory of size 1 M , the batch normalization layer improves usage from $25.8 \%$ to $80.3 \%$, with a consequent perplexity decrease from 19.8 down to 18.0. For comparison, a model without memory obtains a perplexity of 23.0 , which is on par with a memory of size 16 k .</p>
<p>Finally, we observe a correlation between the number of used keys and the model performance. In particular, a model with a memory of size 1 M that does not use batch normalization uses about $25.8 \%$ of the memory values (i.e. roughly 250 k values), and obtains a perplexity of 19.8 , which is on par with the model using a memory of size 262 k that uses batch normalization, and that has a nearly optimal memory usage of $100 \%$.</p>
<p>Memory position. In this experiment we insert the memory at different levels in the transformer, to see where it is the most beneficial. In Table 3 we observe that the model benefits the most from the memory when it replaces the FFN of the layers 4 or 5 in the transformer. Putting memory at layer 1 (after the input token embeddings) gives the worst performance. When the memory is inserted in layer 6, it is located right before the softmax output, the model has only one linear layer to process</p>
<p>Table 2: Perplexity and memory usage for different memory sizes, with and without BatchNorm. Adding a batch normalization layer in the query network encourages the model to use more keys. This is not necessary for small memories of size 16k and 65k where the usage is already close to 100% without batch normalization, but for memories of size 147k of more, batch normalization improves the memory usage significantly, along with the perplexity.</p>
<table>
<thead>
<tr>
<th>Memory size</th>
<th>16k</th>
<th></th>
<th>65k</th>
<th></th>
<th>147k</th>
<th></th>
<th>262k</th>
<th></th>
<th>590k</th>
<th></th>
<th>1M</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>BatchNorm</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Perplexity</td>
<td>22.8</td>
<td>23.0</td>
<td>21.7</td>
<td>21.9</td>
<td>20.9</td>
<td>20.7</td>
<td>20.5</td>
<td>19.8</td>
<td>20.0</td>
<td>18.7</td>
<td>19.8</td>
<td>18.0</td>
</tr>
<tr>
<td>Usage (%)</td>
<td>100</td>
<td>100</td>
<td>99.0</td>
<td>100.0</td>
<td>83.8</td>
<td>99.6</td>
<td>64.4</td>
<td>97.9</td>
<td>38.0</td>
<td>90.3</td>
<td>25.8</td>
<td>80.3</td>
</tr>
<tr>
<td>KL</td>
<td>0.56</td>
<td>0.56</td>
<td>0.69</td>
<td>0.58</td>
<td>0.94</td>
<td>0.65</td>
<td>1.20</td>
<td>0.68</td>
<td>1.70</td>
<td>0.83</td>
<td>2.06</td>
<td>0.95</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Memory usage and perplexity with and without query batch normalization. Adding batch normalization increases both performance and the fraction of used memory slots.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Memory usage and perplexity for different number of heads, and number of k-NN. Increasing the number of heads or k-NN increases both performance and the fraction of used memory slots.</p>
<p>The information read from the memory. The best position to insert the memory is at an intermediate layer. We surmise that effective use of the memory requires operating in a more abstract feature space than the input and that it is important to have some layers on the top of the memory to further process and aggregate information from every location.</p>
<p>Number of heads / k-NN. Figure 6 shows that increasing the number of heads or the number of k-NN improves both the perplexity of the model, and the memory usage. We also note that models with identical $h \times k$ ( $h$ being the number of heads and $k$ the number of nearest neighbors) have a similar memory usage, i.e. models with $(h, k) \in{(1,64),(2,32),(4,16),(8,8)}$ all have a memory usage around 70%, and a perplexity around 20.5. Adding more heads overall improves the performance, but also increases the computation time. Overall, we found that using 4 heads and 32 k-NN strikes a good trade-off between speed and performance.</p>
<p>Table 3: Perplexity and memory usage for different memory positions in a transformer with 6 layers. Adding a memory in positions 4 or 5 maximizes the performance (layer 1 is the worst).</p>
<table>
<thead>
<tr>
<th>Position</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr>
<td>Perplexity</td>
<td>21.5</td>
<td>20.7</td>
<td>20.4</td>
<td>20.1</td>
<td>$\mathbf{1 9 . 8}$</td>
<td>20.3</td>
</tr>
<tr>
<td>Usage (\%)</td>
<td>$\mathbf{1 0 0 . 0}$</td>
<td>$\mathbf{1 0 0 . 0}$</td>
<td>98.3</td>
<td>97.1</td>
<td>97.9</td>
<td>96.9</td>
</tr>
<tr>
<td>KL</td>
<td>2.23</td>
<td>0.95</td>
<td>0.74</td>
<td>0.71</td>
<td>$\mathbf{0 . 6 8}$</td>
<td>1.08</td>
</tr>
</tbody>
</table>
<p>Table 4: Perplexity, memory usage and inference speed with product keys and regular keys. Models with product keys have a much better usage than models that represent keys by a flat matrix, and obtain a better perplexity. They also have significantly less parameters and are dramatically faster to run. The speed is measured at inference, in thousands of words per second (w/s). For models with more than 262 k memory slots, we only report the inference time. We observe that with product keys, the memory size do not impact the inference time.</p>
<table>
<thead>
<tr>
<th>Memory size</th>
<th>16k</th>
<th></th>
<th>65k</th>
<th></th>
<th>147k</th>
<th></th>
<th>262k</th>
<th></th>
<th>590k</th>
<th></th>
<th>1M</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Product Keys</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Perplexity</td>
<td>23.2</td>
<td>23.0</td>
<td>22.6</td>
<td>21.9</td>
<td>22.1</td>
<td>20.7</td>
<td>-</td>
<td>19.8</td>
<td>-</td>
<td>18.7</td>
<td>-</td>
<td>18.0</td>
</tr>
<tr>
<td>Usage (\%)</td>
<td>19.6</td>
<td>100</td>
<td>13.6</td>
<td>100.0</td>
<td>10.1</td>
<td>99.6</td>
<td>-</td>
<td>97.9</td>
<td>-</td>
<td>90.3</td>
<td>-</td>
<td>80.3</td>
</tr>
<tr>
<td>KL</td>
<td>2.04</td>
<td>0.56</td>
<td>2.48</td>
<td>0.58</td>
<td>2.77</td>
<td>0.65</td>
<td>-</td>
<td>0.68</td>
<td>-</td>
<td>0.83</td>
<td>-</td>
<td>0.95</td>
</tr>
<tr>
<td>Speed (w/s)</td>
<td>$\mathbf{3 5 . 0 k}$</td>
<td>$\mathbf{3 5 . 8 k}$</td>
<td>28.5 k</td>
<td>$\mathbf{3 6 . 7 k}$</td>
<td>13.9 k</td>
<td>$\mathbf{3 6 . 4 k}$</td>
<td>7.7 k</td>
<td>$\mathbf{3 6 . 3 k}$</td>
<td>4.7 k</td>
<td>$\mathbf{3 6 . 2 k}$</td>
<td>1.2 k</td>
<td>$\mathbf{3 5 . 7 k}$</td>
</tr>
</tbody>
</table>
<p>Product keys vs. flat keys. Product keys presented in Figure 2 enable finding the nearest neighbors in a matrix of size $\left(|C|^{2}, d_{k}\right)$ with the same time/compute complexity of a search over two matrices of size $\left(|C|, \frac{d_{k}}{2}\right)$. As a result, product keys contain $|C|$ times less parameters than keys represented by a full matrix. Table 4 and Figure 7 compare product keys to the default regular flat keys. In the second case, searching the nearest keys boils down to a liner index search at each iteration, which is computationally very expensive. As a result, we only report results for memories of size $16 \mathrm{k}, 65 \mathrm{k}, 147 \mathrm{k}$, as experiments with a flat index on larger memories takes an unreasonable amount of time to converge. We can see that models with product keys are not only faster but they have also a much better memory usage, and consequently obtain a better perplexity.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Speed over memory size. Speed (in thousands of words per second) for different memory sizes. For regular flat keys, increasing the number of keys significantly slows down the model, while with product keys, increasing the memory size barely impacts the inference speed.</p>
<h1>5 Conclusion</h1>
<p>This paper introduces a memory layer that allows to drastically improve the capacity of a neural network with a negligible computational overhead. The efficiency of our layer relies on two key ingredients: the factorization of keys as a product set, and the sparse read/write accesses to the memory values. Our layer is integrated into an existing neural network architecture. We show experimentally that it provides important gains on large-scale language modeling, reaching with 12 layers the performance of a 24-layer BERT-large model with half the running time.</p>
<h1>References</h1>
<p>[1] Artem Babenko and Victor Lempitsky. The inverted multi-index. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.
[2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Representation Learning, 2019.
[3] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013.
[4] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. Conference of the International Speech Communication Association, 2014.
[5] Bo Chen and Jeffrey M. Gilbert. The on-device visual intelligence challenge. https:// ai.googleblog.com/2018/04/introducing-cvpr-2018-on-device-visual.html, 2019. Accessed: 2019-05-20.
[6] Kyunghyun Cho and Yoshua Bengio. Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning. CoRR, abs/1406.7362, 2014.
[7] Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1 . CoRR, 2016.
[8] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. Advances in Neural Information Processing Systems, 2015.
[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Conference of the Association for Computational Linguistics, 2019.
[10] Ludovic Denoyer and Patrick Gallinari. Deep sequential neural network. CoRR, abs/1410.0510, 2014.
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistic, 2018.
[12] D. Eigen, I. Sutskever, and M. Ranzato. Learning factored representations in a deep mixture of experts. In Workshop at the International Conference on Learning Representations, 2014.
[13] Thomas Gerald, Nicolas Baskiotis, and Ludovic Denoyer. Binary stochastic representations for large multi-class classification. In International Conference on Neural Information Processing, 2017.
[14] Edouard Grave, Moustapha M Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. In Advances in Neural Information Processing Systems, 2017.
[15] Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In International Conference on Representation Learning, 2017.
[16] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In International Conference on Acoustics, Speech, and Signal Processing, 2013.
[17] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014.
[18] Sam Gross, Marc'Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale weakly supervised vision. In Conference on Computer Vision and Pattern Recognition, 2017.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, 2016.
[20] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. CoRR, abs/1811.06965, 2018.
[21] Hervé Jégou, Matthijs Douze, and Cordelia Schmid. Product Quantization for Nearest Neighbor Search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011.</p>
<p>[22] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 2017.
[23] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in Neural Information Processing Systems, 2015.
[24] Koray Kavukcuoglu, Marc'Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding algorithms with applications to object recognition. CoRR, abs/1010.3467, 2010. URL http: //arxiv.org/abs/1010.3467.
[25] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Representation Learning, 2015.
[26] Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source toolkit for statistical machine translation. In Conference of the Association for Computational Linguistics, 2007.
[27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.
[28] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems, 2019.
[29] Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In European Conference on Computer Vision, 2018.
[30] Alireza Makhzani and Brendan Frey. K-sparse autoencoders. In International Conference on Representation Learning, 2014.
[31] Alireza Makhzani and Brendan J Frey. Winner-take-all autoencoders. In Advances in Neural Information Processing Systems, 2015.
[32] Marius Muja and David G. Lowe. Scalable nearest neighbor algorithms for high dimensional data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.
[33] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. In International Conference on Representation Learning, 2019.
[34] Bruno A. Olshausen and David J. Field. Sparse coding with an overcomplete basis set, a strategy employed by v1? Vision Research, 1997.
[35] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In Neurips Autodiff Workshop, 2017.
[36] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.
[37] Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves, and Timothy Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. In Advances in Neural Information Processing Systems, 2016.
[38] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, 2016.
[39] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Conference of the Association for Computational Linguistics, 2015.
[40] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In International Conference on Representation Learning, 2017.
[41] Stefano Spigler, Mario Geiger, Stéphane d'Ascoli, Levent Sagun, Giulio Biroli, and Matthieu Wyart. A jamming transition from under- to over-parametrization affects loss landscape and generalization. CoRR, abs/1810.09665, 2018.
[42] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In Advances in Neural Information Processing Systems, 2015.</p>
<p>[43] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, 2014.
[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.
[45] Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with large output spaces. In Workshop at the International Conference on Learning Representations, 2015.
[46] Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Representation Learning, 2018.
[47] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In International Conference on Representation Learning, 2015.
[48] Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. In International Conference on Representation Learning, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/glample/fastBPE&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
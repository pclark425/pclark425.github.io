<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8761 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8761</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8761</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-234742446</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2021.acl-srw.2.pdf" target="_blank">Stage-wise Fine-tuning for Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel tree-level embedding method to capture the inter-dependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8761.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8761.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearized triple sequence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flattened linearized triple sequence with |S/|P/|O tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A serialization of an RDF graph into a flat token sequence by concatenating triples and prefixing each phrase with special markers for subject, predicate (relation), and object (|S, |P, |O), used as input to pretrained sequence-to-sequence PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization / linearized triple sequences</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF triple (s, r, o) is converted to a contiguous token sequence and triples are concatenated into a single sequence; special tokens |S, |P, |O are prepended to tokens belonging to subject, predicate, and object phrases to mark their roles.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graphs (WebNLG dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Simple serialization: for graph G = {(s1,r1,o1),...,(sn,rn,on)} form the flattened sequence: |S s1 |P r1 |O o1 ... |S sn |P rn |O on. No graph encoder; relies on PLM tokenization and linear order.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (text generation describing RDF KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported improvements across BLEU, METEOR, TER, PARENT and BERTScore when used with PLMs and additional embeddings (exact numeric table values referenced but not reproduced in text); authors report state-of-the-art on most metrics for WebNLG and paired t-test significance p ≤ 0.008 versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively/empirically against approaches using explicit graph encoders (GCN, graph transformer) and PLMs fine-tuned only on labeled data; linearized input + PLMs achieves strong results and is simpler than complex graph encoders, but benefits further from added positional embeddings and stage-wise fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simplicity and compatibility with large pretrained sequence models (BART, T5); leverages PLM's language knowledge and scales to existing text-to-text frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization can lose explicit graph structural signals and inter-dependencies unless augmented; sequence order ambiguity can introduce pronoun/reference errors without extra structure signals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>On KGs with multiple similar relations or unseen relations, linearization alone can lead to missing relations, subject/object mixing, or pronoun ambiguity (examples: confusing leader of country vs dean of university).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Stage-wise Fine-tuning for Graph-to-Text Generation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8761.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8761.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple role & tree-level embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple-role ID and tree-level position embeddings (structure-preserving embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two additional position-aware embeddings appended to the input layer of PLMs: triple-role IDs indicating whether tokens belong to subject/relation/object, and tree-level IDs encoding graph distance from a designated root (source vertex), intended to capture local triple roles and global tree structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>triple-role and tree-level positional embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Extend the PLM input embeddings with (1) Triple Role ID that takes values {1=subj,2=pred,3=obj} for tokens in each triple, and (2) Tree Level ID indicating number of relations (distance) from the graph root for the triple, in addition to the standard sequential position ID.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graphs (WebNLG); generally tree-like RDF subgraphs rooted at a source vertex</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>After linearizing triples, assign each token a triple-role ID based on its position inside a triple and compute a tree-level (distance from root) per triple; these IDs are embedded and added to the PLM input embedding to inject graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report that adding triple-role and tree-level embeddings to PLMs (BART/T5) significantly improves all automatic text-generation metrics (BLEU, METEOR, TER) on WebNLG and improves PARENT/BERTScore; specific numeric results referenced in tables but not reproduced in main text. Statistical significance reported (paired t-test p ≤ 0.008).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented as an alternative to explicit graph encoders (GCN, graph transformer): simpler to integrate into PLMs and shown to boost performance over PLMs with only token+position embeddings and over PLMs fine-tuned without these embeddings; qualitative comparisons show better handling of pronoun ambiguity and grouping of related triples into single sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures local triple role and coarse hierarchical/tree information without a separate graph encoder; reduces pronoun/reference errors and helps combine multiple related triples into fluent sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Tree-level is a coarse distance measure (number of relations from root) that may not fully encode complex graph topologies; requires selection of a graph root and may be less effective on non-tree or highly cyclic graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Still fails on relations lacking clear directionality or deep semantic connections (e.g., 'doctoralStudent' inverted), and can miss relations or mix subjects/objects in KGs with many triples or unseen relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Stage-wise Fine-tuning for Graph-to-Text Generation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8761.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8761.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Position ID (standard)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard sequential position embeddings (Position ID)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The original token position embeddings used by Transformer PLMs (e.g., BART) indicating token index in the flattened input sequence; retained and used alongside the new triple-role and tree-level embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sequential position embeddings (Position ID)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each token in the flattened linearized triple sequence receives its original PLM position index embedding to encode token order within the serialized input.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graphs serialized as sequences</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Position ID assigned based on index within the flattened |S s1 |P r1 |O o1 ... sequence</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as baseline embedding; improvements are reported when combined with triple-role and tree-level embeddings, but specific ablation numbers are in referenced tables (not shown in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Works in combination with the new embeddings; by itself can lead to pronoun ambiguity and confusion across multiple similar triples, which triple-role/tree-level helps mitigate.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Standard and well-supported by PLMs; preserves linear order information.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Insufficient to disambiguate graph roles or capture hierarchical/tree structure in the serialized representation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Pronoun/reference ambiguity when multiple triples contain same predicate names for different entities (e.g., leaderName for country vs university dean).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Stage-wise Fine-tuning for Graph-to-Text Generation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8761.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8761.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-step (stage-wise) fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stage-wise fine-tuning with Wikipedia RDF-article pairs then labeled training data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-adaptive fine-tuning approach that first fine-tunes PLMs on a large noisy corpus of Wikipedia-derived RDF graphs paired with related articles, then fine-tunes on the smaller, clean/labeled WebNLG training set to improve factual grounding and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>stage-wise fine-tuning (Wikipedia fine-tuning + supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>First adapt the PLM to the domain by fine-tuning on noisy but large-scale RDF graph ↔ article pairs crawled from Wikipedia (exposing more relations), then perform final supervised fine-tuning on the labeled WebNLG dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graphs (noisy Wikipedia-extracted triples; WebNLG RDF graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graphs in the Wikipedia corpus are linearized (as above) and paired with corresponding article text for the initial fine-tuning stage; followed by standard supervised fine-tuning on WebNLG linearized graphs and references.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (improving generation faithfulness and coverage of relation types)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report qualitative and quantitative improvements: better handling of unseen relations, reduction in fabricated facts, improved order/combination of relations; overall metrics (BLEU/METEOR/TER/PARENT/BERTScore) improved and many results reached state-of-the-art; p ≤ 0.008 significance vs baselines. Exact numbers are in tables but not reproduced in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to direct fine-tuning of PLMs solely on labeled WebNLG data (as in Ribeiro et al. and others), stage-wise Wikipedia fine-tuning yields better factual fidelity and handling of unseen relations; complements structural embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages large noisy data to expose model to more relation types and lexical contexts, reduces hallucination of extraneous facts, and improves generalization to unseen relations.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Noisy training data can still propagate errors; does not eliminate all hallucinations or relation-confusion errors; requires crawling/curation of large Wikipedia graph-text pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Even with Wikipedia fine-tuning, models can ignore particular relations, mix relation arguments, or produce incorrect relations (examples: still ignores 'sports Governing Body' in a sample and confuses geographic relations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Stage-wise Fine-tuning for Graph-to-Text Generation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8761.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8761.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Network (Duvenaud et al. reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph encoder that applies convolution-like operations over graph nodes to produce node/graph representations, used in prior structured graph-to-text models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Convolutional networks on graphs for learning molecular fingerprints.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph convolutional encoding (GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph structure by propagating and aggregating information across neighbors through stacked convolutional layers over nodes/edges.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (used in prior work for RDF/AMR graphs and molecular graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph-structured encoder consumes native graph (nodes and edges) rather than serializing to sequence; produces embeddings used by a decoder to generate text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (prior work), molecular fingerprinting (original Duvenaud et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced as a baseline approach in related work; specific comparative numbers are in cited works but not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Prior graph encoders like GCN aim to preserve graph dependencies but can be more complex; the paper notes PLMs with linearized input often outperform structured encoders unless structure is explicitly injected into PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models graph topology and local neighborhoods; suited for non-sequential graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds modeling complexity; may require task-specific design and more computation; prior work reported lower performance than PLMs with simple linearization on WebNLG in some settings (per paper discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not discussed in detail in this paper; generally can struggle with long-range dependencies or require tuning to capture global graph context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Stage-wise Fine-tuning for Graph-to-Text Generation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8761.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8761.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Transformer (Koncel-Kedziorski et al. reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based graph encoder that adapts self-attention to graph structures to encode nodes and edges for text generation from graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text Generation from Knowledge Graphs with Graph Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph transformer encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Extends Transformer self-attention to operate over graph nodes/edges, incorporating structural adjacency or relation-aware attention patterns to encode graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operate on graph node/edge representations, using transformer layers modified to respect graph adjacency or incorporate relation types; used as encoder in encoder-decoder generation models.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced as a competitive graph-encoding approach in related work; specific results are in the cited paper and not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Graph transformers explicitly model graph structure and can outperform naive linearization in some setups; paper positions its embedding-based augmentation as a simpler alternative that leverages PLMs without full graph encoder complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Models complex graph interactions and can capture global structure via attention modified for graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Architecturally more complex and computationally heavier than sequence PLMs with serializations; integration with large pre-trained PLMs is less straightforward.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed here; complexity and scalability to large graphs can be limiting factors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Stage-wise Fine-tuning for Graph-to-Text Generation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8761.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8761.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTR-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GTR-LSTM: A triple encoder for sentence generation from RDF data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-sequence model that encodes triples with a triple-centric encoder (GTR-LSTM) to produce text from RDF data; cited as a prior structured approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GTR-LSTM: A triple encoder for sentence generation from RDF data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>triple-centric graph-to-sequence encoding (GTR-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses a dedicated triple encoder (LSTM-based with triple structure awareness) to map RDF triples into representations fed to a decoder for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Encodes each triple's components with structured LSTM encoders and composes triple representations for sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mentioned among prior models; concrete metrics are available in the cited work but not included in this paper's main text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>An example of structured encoders contrasted with PLM-based linearization approaches; authors note PLMs with linearized input have recently achieved strong results relative to such structured encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Specifically models triple structure and relationships between s/p/o components.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Less scalable to large pre-trained text models and may underperform PLMs fine-tuned on serialized inputs unless combined with large-scale pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specified in this paper; general issues include increased modeling complexity and potential mismatch with decoder expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Stage-wise Fine-tuning for Graph-to-Text Generation', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>KGPT: Knowledge-grounded pretraining for data-to-text generation. <em>(Rating: 2)</em></li>
                <li>Don't stop pretraining: Adapt language models to domains and tasks <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Convolutional networks on graphs for learning molecular fingerprints. <em>(Rating: 1)</em></li>
                <li>TaPas: Weakly supervised table parsing via pre-training <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>GTR-LSTM: A triple encoder for sentence generation from RDF data. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8761",
    "paper_id": "paper-234742446",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Linearized triple sequence",
            "name_full": "Flattened linearized triple sequence with |S/|P/|O tokens",
            "brief_description": "A serialization of an RDF graph into a flat token sequence by concatenating triples and prefixing each phrase with special markers for subject, predicate (relation), and object (|S, |P, |O), used as input to pretrained sequence-to-sequence PLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "linearization / linearized triple sequences",
            "representation_description": "Each RDF triple (s, r, o) is converted to a contiguous token sequence and triples are concatenated into a single sequence; special tokens |S, |P, |O are prepended to tokens belonging to subject, predicate, and object phrases to mark their roles.",
            "graph_type": "RDF knowledge graphs (WebNLG dataset)",
            "conversion_method": "Simple serialization: for graph G = {(s1,r1,o1),...,(sn,rn,on)} form the flattened sequence: |S s1 |P r1 |O o1 ... |S sn |P rn |O on. No graph encoder; relies on PLM tokenization and linear order.",
            "downstream_task": "Graph-to-text generation (text generation describing RDF KGs)",
            "performance_metrics": "Reported improvements across BLEU, METEOR, TER, PARENT and BERTScore when used with PLMs and additional embeddings (exact numeric table values referenced but not reproduced in text); authors report state-of-the-art on most metrics for WebNLG and paired t-test significance p ≤ 0.008 versus baselines.",
            "comparison_to_others": "Compared qualitatively/empirically against approaches using explicit graph encoders (GCN, graph transformer) and PLMs fine-tuned only on labeled data; linearized input + PLMs achieves strong results and is simpler than complex graph encoders, but benefits further from added positional embeddings and stage-wise fine-tuning.",
            "advantages": "Simplicity and compatibility with large pretrained sequence models (BART, T5); leverages PLM's language knowledge and scales to existing text-to-text frameworks.",
            "disadvantages": "Linearization can lose explicit graph structural signals and inter-dependencies unless augmented; sequence order ambiguity can introduce pronoun/reference errors without extra structure signals.",
            "failure_cases": "On KGs with multiple similar relations or unseen relations, linearization alone can lead to missing relations, subject/object mixing, or pronoun ambiguity (examples: confusing leader of country vs dean of university).",
            "uuid": "e8761.0",
            "source_info": {
                "paper_title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Triple role & tree-level embeddings",
            "name_full": "Triple-role ID and tree-level position embeddings (structure-preserving embeddings)",
            "brief_description": "Two additional position-aware embeddings appended to the input layer of PLMs: triple-role IDs indicating whether tokens belong to subject/relation/object, and tree-level IDs encoding graph distance from a designated root (source vertex), intended to capture local triple roles and global tree structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "triple-role and tree-level positional embeddings",
            "representation_description": "Extend the PLM input embeddings with (1) Triple Role ID that takes values {1=subj,2=pred,3=obj} for tokens in each triple, and (2) Tree Level ID indicating number of relations (distance) from the graph root for the triple, in addition to the standard sequential position ID.",
            "graph_type": "RDF knowledge graphs (WebNLG); generally tree-like RDF subgraphs rooted at a source vertex",
            "conversion_method": "After linearizing triples, assign each token a triple-role ID based on its position inside a triple and compute a tree-level (distance from root) per triple; these IDs are embedded and added to the PLM input embedding to inject graph structure.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Authors report that adding triple-role and tree-level embeddings to PLMs (BART/T5) significantly improves all automatic text-generation metrics (BLEU, METEOR, TER) on WebNLG and improves PARENT/BERTScore; specific numeric results referenced in tables but not reproduced in main text. Statistical significance reported (paired t-test p ≤ 0.008).",
            "comparison_to_others": "Presented as an alternative to explicit graph encoders (GCN, graph transformer): simpler to integrate into PLMs and shown to boost performance over PLMs with only token+position embeddings and over PLMs fine-tuned without these embeddings; qualitative comparisons show better handling of pronoun ambiguity and grouping of related triples into single sentences.",
            "advantages": "Captures local triple role and coarse hierarchical/tree information without a separate graph encoder; reduces pronoun/reference errors and helps combine multiple related triples into fluent sentences.",
            "disadvantages": "Tree-level is a coarse distance measure (number of relations from root) that may not fully encode complex graph topologies; requires selection of a graph root and may be less effective on non-tree or highly cyclic graphs.",
            "failure_cases": "Still fails on relations lacking clear directionality or deep semantic connections (e.g., 'doctoralStudent' inverted), and can miss relations or mix subjects/objects in KGs with many triples or unseen relation types.",
            "uuid": "e8761.1",
            "source_info": {
                "paper_title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Position ID (standard)",
            "name_full": "Standard sequential position embeddings (Position ID)",
            "brief_description": "The original token position embeddings used by Transformer PLMs (e.g., BART) indicating token index in the flattened input sequence; retained and used alongside the new triple-role and tree-level embeddings.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "sequential position embeddings (Position ID)",
            "representation_description": "Each token in the flattened linearized triple sequence receives its original PLM position index embedding to encode token order within the serialized input.",
            "graph_type": "RDF knowledge graphs serialized as sequences",
            "conversion_method": "Position ID assigned based on index within the flattened |S s1 |P r1 |O o1 ... sequence",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Used as baseline embedding; improvements are reported when combined with triple-role and tree-level embeddings, but specific ablation numbers are in referenced tables (not shown in main text).",
            "comparison_to_others": "Works in combination with the new embeddings; by itself can lead to pronoun ambiguity and confusion across multiple similar triples, which triple-role/tree-level helps mitigate.",
            "advantages": "Standard and well-supported by PLMs; preserves linear order information.",
            "disadvantages": "Insufficient to disambiguate graph roles or capture hierarchical/tree structure in the serialized representation.",
            "failure_cases": "Pronoun/reference ambiguity when multiple triples contain same predicate names for different entities (e.g., leaderName for country vs university dean).",
            "uuid": "e8761.2",
            "source_info": {
                "paper_title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Two-step (stage-wise) fine-tuning",
            "name_full": "Stage-wise fine-tuning with Wikipedia RDF-article pairs then labeled training data",
            "brief_description": "A domain-adaptive fine-tuning approach that first fine-tunes PLMs on a large noisy corpus of Wikipedia-derived RDF graphs paired with related articles, then fine-tunes on the smaller, clean/labeled WebNLG training set to improve factual grounding and reduce hallucinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "stage-wise fine-tuning (Wikipedia fine-tuning + supervised fine-tuning)",
            "representation_description": "First adapt the PLM to the domain by fine-tuning on noisy but large-scale RDF graph ↔ article pairs crawled from Wikipedia (exposing more relations), then perform final supervised fine-tuning on the labeled WebNLG dataset.",
            "graph_type": "RDF knowledge graphs (noisy Wikipedia-extracted triples; WebNLG RDF graphs)",
            "conversion_method": "Graphs in the Wikipedia corpus are linearized (as above) and paired with corresponding article text for the initial fine-tuning stage; followed by standard supervised fine-tuning on WebNLG linearized graphs and references.",
            "downstream_task": "Graph-to-text generation (improving generation faithfulness and coverage of relation types)",
            "performance_metrics": "Authors report qualitative and quantitative improvements: better handling of unseen relations, reduction in fabricated facts, improved order/combination of relations; overall metrics (BLEU/METEOR/TER/PARENT/BERTScore) improved and many results reached state-of-the-art; p ≤ 0.008 significance vs baselines. Exact numbers are in tables but not reproduced in main text.",
            "comparison_to_others": "Compared to direct fine-tuning of PLMs solely on labeled WebNLG data (as in Ribeiro et al. and others), stage-wise Wikipedia fine-tuning yields better factual fidelity and handling of unseen relations; complements structural embeddings.",
            "advantages": "Leverages large noisy data to expose model to more relation types and lexical contexts, reduces hallucination of extraneous facts, and improves generalization to unseen relations.",
            "disadvantages": "Noisy training data can still propagate errors; does not eliminate all hallucinations or relation-confusion errors; requires crawling/curation of large Wikipedia graph-text pairs.",
            "failure_cases": "Even with Wikipedia fine-tuning, models can ignore particular relations, mix relation arguments, or produce incorrect relations (examples: still ignores 'sports Governing Body' in a sample and confuses geographic relations).",
            "uuid": "e8761.3",
            "source_info": {
                "paper_title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "GCN",
            "name_full": "Graph Convolutional Network (Duvenaud et al. reference)",
            "brief_description": "A graph encoder that applies convolution-like operations over graph nodes to produce node/graph representations, used in prior structured graph-to-text models.",
            "citation_title": "Convolutional networks on graphs for learning molecular fingerprints.",
            "mention_or_use": "mention",
            "representation_name": "graph convolutional encoding (GCN)",
            "representation_description": "Encodes graph structure by propagating and aggregating information across neighbors through stacked convolutional layers over nodes/edges.",
            "graph_type": "General graphs (used in prior work for RDF/AMR graphs and molecular graphs)",
            "conversion_method": "Graph-structured encoder consumes native graph (nodes and edges) rather than serializing to sequence; produces embeddings used by a decoder to generate text.",
            "downstream_task": "Graph-to-text generation (prior work), molecular fingerprinting (original Duvenaud et al.)",
            "performance_metrics": "Referenced as a baseline approach in related work; specific comparative numbers are in cited works but not reproduced here.",
            "comparison_to_others": "Prior graph encoders like GCN aim to preserve graph dependencies but can be more complex; the paper notes PLMs with linearized input often outperform structured encoders unless structure is explicitly injected into PLMs.",
            "advantages": "Directly models graph topology and local neighborhoods; suited for non-sequential graph structure.",
            "disadvantages": "Adds modeling complexity; may require task-specific design and more computation; prior work reported lower performance than PLMs with simple linearization on WebNLG in some settings (per paper discussion).",
            "failure_cases": "Not discussed in detail in this paper; generally can struggle with long-range dependencies or require tuning to capture global graph context.",
            "uuid": "e8761.4",
            "source_info": {
                "paper_title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Graph Transformer",
            "name_full": "Graph Transformer (Koncel-Kedziorski et al. reference)",
            "brief_description": "A transformer-based graph encoder that adapts self-attention to graph structures to encode nodes and edges for text generation from graphs.",
            "citation_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "mention_or_use": "mention",
            "representation_name": "graph transformer encoding",
            "representation_description": "Extends Transformer self-attention to operate over graph nodes/edges, incorporating structural adjacency or relation-aware attention patterns to encode graph information.",
            "graph_type": "Knowledge graphs / RDF graphs",
            "conversion_method": "Operate on graph node/edge representations, using transformer layers modified to respect graph adjacency or incorporate relation types; used as encoder in encoder-decoder generation models.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Referenced as a competitive graph-encoding approach in related work; specific results are in the cited paper and not reproduced here.",
            "comparison_to_others": "Graph transformers explicitly model graph structure and can outperform naive linearization in some setups; paper positions its embedding-based augmentation as a simpler alternative that leverages PLMs without full graph encoder complexity.",
            "advantages": "Models complex graph interactions and can capture global structure via attention modified for graphs.",
            "disadvantages": "Architecturally more complex and computationally heavier than sequence PLMs with serializations; integration with large pre-trained PLMs is less straightforward.",
            "failure_cases": "Not detailed here; complexity and scalability to large graphs can be limiting factors.",
            "uuid": "e8761.5",
            "source_info": {
                "paper_title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "GTR-LSTM",
            "name_full": "GTR-LSTM: A triple encoder for sentence generation from RDF data",
            "brief_description": "A graph-to-sequence model that encodes triples with a triple-centric encoder (GTR-LSTM) to produce text from RDF data; cited as a prior structured approach.",
            "citation_title": "GTR-LSTM: A triple encoder for sentence generation from RDF data.",
            "mention_or_use": "mention",
            "representation_name": "triple-centric graph-to-sequence encoding (GTR-LSTM)",
            "representation_description": "Uses a dedicated triple encoder (LSTM-based with triple structure awareness) to map RDF triples into representations fed to a decoder for generation.",
            "graph_type": "RDF knowledge graphs",
            "conversion_method": "Encodes each triple's components with structured LSTM encoders and composes triple representations for sequence generation.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Mentioned among prior models; concrete metrics are available in the cited work but not included in this paper's main text.",
            "comparison_to_others": "An example of structured encoders contrasted with PLM-based linearization approaches; authors note PLMs with linearized input have recently achieved strong results relative to such structured encoders.",
            "advantages": "Specifically models triple structure and relationships between s/p/o components.",
            "disadvantages": "Less scalable to large pre-trained text models and may underperform PLMs fine-tuned on serialized inputs unless combined with large-scale pretraining.",
            "failure_cases": "Not specified in this paper; general issues include increased modeling complexity and potential mismatch with decoder expectations.",
            "uuid": "e8761.6",
            "source_info": {
                "paper_title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "KGPT: Knowledge-grounded pretraining for data-to-text generation.",
            "rating": 2,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        },
        {
            "paper_title": "Don't stop pretraining: Adapt language models to domains and tasks",
            "rating": 2,
            "sanitized_title": "dont_stop_pretraining_adapt_language_models_to_domains_and_tasks"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Convolutional networks on graphs for learning molecular fingerprints.",
            "rating": 1,
            "sanitized_title": "convolutional_networks_on_graphs_for_learning_molecular_fingerprints"
        },
        {
            "paper_title": "TaPas: Weakly supervised table parsing via pre-training",
            "rating": 2,
            "sanitized_title": "tapas_weakly_supervised_table_parsing_via_pretraining"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "GTR-LSTM: A triple encoder for sentence generation from RDF data.",
            "rating": 2,
            "sanitized_title": "gtrlstm_a_triple_encoder_for_sentence_generation_from_rdf_data"
        }
    ],
    "cost": 0.012961749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Stage-wise Fine-tuning for Graph-to-Text Generation
August 5-6, 2021</p>
<p>Qingyun Wang qingyun4@illinois.edu 
University of Illinois at Urbana-Champaign</p>
<p>Semih Yavuz syavuz@salesforce.com 
Salesforce Research</p>
<p>Xi Victoria Lin 
Facebook AI</p>
<p>Heng Ji hengji@illinois.edu 
University of Illinois at Urbana-Champaign</p>
<p>Nazneen Fatema Rajani nazneen.rajani@salesforce.com 
Salesforce Research</p>
<p>Stage-wise Fine-tuning for Graph-to-Text Generation</p>
<p>Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop
the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research WorkshopAugust 5-6, 202116
Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes the model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel treelevel embedding method to capture the interdependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset. 1</p>
<p>Introduction</p>
<p>In the graph-to-text generation task (Gardent et al., 2017), the model takes in a complex KG (an example is in Figure 1) and generates a corresponding faithful natural language description (Table 1). Previous efforts for this task can be mainly divided into two categories: sequence-to-sequence models that directly solve the generation task with LSTMs (Gardent et al., 2017) or Transformer (Castro Ferreira et al., 2019); and graph-to-text models (Trisedya et al., 2018;Marcheggiani and Perez-Beltrachini, 2018) which use a graph encoder to capture the structure of the KGs. Recently, Transformer-based PLMs such as GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020), * This research was conducted during the author's internship at Salesforce Research. 1 The programs, data and resources are publicly available for research purpose at: https://github.com/ EagleW/Stage-wise-Fine-tuning and T5 (Raffel et al., 2020) have achieved stateof-the-art results on WebNLG dataset due to factual knowledge acquired in the pre-training phase (Harkous et al., 2020;Ribeiro et al., 2020b;Kale, 2020;Chen et al., 2020a). Despite such improvement, PLMs fine-tuned only on the clean (or labeled) data might be more prone to hallucinate factual knowledge (e.g., "Visvesvaraya Technological University" in Table  1). Inspired by the success of domain-adaptive pre-training (Gururangan et al., 2020), we propose a novel two-step fine-tuning mechanism graph-totext generation task. Unlike (Ribeiro et al., 2020b;Herzig et al., 2020;Chen et al., 2020a) which directly fine-tune the PLMs on the training set, we first fine-tune our model over noisy RDF graphs and related article pairs crawled from Wikipedia before final fine-tuning on the clean/labeled training set. The additional fine-tuning step benefits our model by leveraging triples not included in the training set and reducing the chances that the model fabricates facts based on the language model. Meanwhile, the PLMs might also fail to cover all relations in the KG by creating incorrect or missing facts. For example, in Table 1, although the T5-large with Wikipedia fine-tuning successfully removes the unwanted contents, it still ignores the "sports Governing Body" relation and incorrectly The Acharya Institute of Technology is located in the state of Karnataka . It was given the Technical Campus status by the All India Council for Technical Education which is located in Mumbai . The institute offers tennis and has Telangana to its northeast and the Arabian Sea to its west. [International Tennis Federation] T5-large + Position</p>
<p>The Acharya Institute of Technology is located in the state of Karnataka which has Telangana to its northeast and the Arabian Sea to its west. It was given the Technical Campus status by the All India Council for Technical Education in Mumbai . The Institute offers tennis which is governed by the International Tennis Federation . T5-large + Wiki + Position</p>
<p>The Acharya Institute of Technology in Karnataka was given the 'Technical Campus' status by the All India Council for Technical Education in Mumbai . Karnataka has Telangana to its northeast and the Arabian Sea to its west. One of the sports offered at the Institute is tennis which is governed by the International Tennis Federation .  Figure 1. We use the color box to frame each entity out with the same color as the corresponding entity in Figure 1. We highlight fabricated facts, [missed relations], and incorrect relations with different color. links the university to both "Telangana" and "Arabian Sea". To better capture the structure and interdependence of facts in the KG, instead of using a complex graph encoder, we leverage the power of Transformer-based PLMs with additional position embeddings which have been proved effective in various generation tasks (Herzig et al., 2020;Chen et al., 2020a,b). Here, we extend the embedding layer of Transfomer-based PLMs with two additional triple role and tree-level embeddings to capture graph structure.</p>
<p>We explore the proposed stage-wise fine-tuning and structure-preserving embedding strategies for graph-to-text generation task on WebNLG corpus (Gardent et al., 2017). Our experimental results clearly demonstrate the benefit of each strategy in achieving the state-of-the-art performance on most commonly reported automatic evaluation metrics.</p>
<p>Method</p>
<p>Given an RDF graph with multiple relations G = {(s 1 , r 1 , o 1 ), (s 2 , r 2 , o 2 ), ..., (s n , r n , o n )}, our goal is to generate a text faithfully describing the input graph. We represent each relation with a triple (s i , r i , o i ) ∈ G for i ∈ {1, ..., n}, where s i , r i , and o i are natural language phrases that represent the subject, type, and object of the relation, respectively. We augment our model with addi-  tional position embeddings to capture the structure of the KG. To feed the input for the large-scale Transformer-based PLM, we flatten the graph as a concatenation of linearized triple sequences: Ribeiro et al. (2020b), where |S, |P, |O are special tokens prepended to indicate whether the phrases in the relations are subjects, relations, or objects, respectively. Instead of directly finetuning the PLM on the WebNLG dataset, we first fine-tune our model on a noisy, but larger corpus crawled from Wikipedia, then we fine-tune the model on the training set. Positional embeddings Since the input of the WebNLG task is a small KG which describes properties of entities, we introduce additional positional  embeddings to enhance the flattened input of pretrained Transformer-based sequence-to-sequence models such as BART and TaPas (Herzig et al., 2020). We extend the input layer with two positionaware embeddings in addition to the original position embeddings 3 as shown in the Figure 2:
|S s 1 |P r 1 |O o 1 ... |S s n |P r n |O o n following
• Position ID, which is the same as the original position ID used in BART, is the index of the token in the flattened sequence |S s 1 |P r 1 |O o 1 ... |S s n |P r n |O o n .</p>
<p>• Triple Role ID takes 3 values for a specific triple (s i , r i , o i ): 1 for the subject s i , 2 for the relation r i , and 3 for the object o i .</p>
<p>• Tree level ID calculates the distance (the number of relations) from the root which is the source vertex of the RDF graph.   </p>
<p>Results and Analysis</p>
<p>We use the standard NLG evaluation metrics to report results: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and TER (Snover et al., 2006) , as shown in Table 2. Because Castro Ferreira et al. (2020) has found that BERTScore (Zhang* et al., 2020) correlates with human evaluation ratings better, we use BERTscore to evaluate system results 5 as shown in Table 4. When selecting the best models, we also evaluate each model with PARENT (Dhingra et al., 2019) metric which measures the overlap between predictions and both reference texts and graph contents. Dhingra et al. (2019) show PARENT metric has better human rating correlations. Table 3 shows the pre-trained models with 2-step fine-tuning and position embeddings achieve better results. 6 We conduct paired t-test between our proposed model and all the other baselines on 10 randomly sampled subsets. The differences are statistically significant with p ≤ 0.008 for all settings.</p>
<p>Results with Wikipedia fine-tuning. The Wikipedia fine-tuning helps the model handle unseen relations such as "inOfficeWhileVicePresident", and "activeYearsStartYear" by stating "His vice president is Atiku Abubakar." and "started playing in 1995" respectively. It also combines relations with the same type together with correct order, e.g., given two death places of a person, the model generates: "died in Sidcup, London" instead of generating two sentences or placing the city name ahead of the area name.</p>
<p>Results with positional embeddings. For the KG with multiple triples, additional positional embeddings help reduce the errors introduced by pro- 5 We only use BERTScore to evaluate baselines which have results available online. 6 For more examples, please check Appendix for reference. noun ambiguity. For instance, for a KG which has "leaderName" relation to both country's leader and university's dean, position embeddings can distinguish these two relations by stating "Denmark's leader is Lars Løkke Rasmussen" instead of "its leader is Lars Løkke Rasmussen". The tree-level embeddings also help the model arrange multiple triples into one sentence, such as combining the city, the country, the affiliation, and the affiliation's headquarter of a university into a single sentence: "The School of Business and Social Sciences at the Aarhus University in Aarhus, Denmark is affiliated to the European University Association in Brussels".</p>
<p>Remaining Challenges</p>
<p>However, pre-trained language models also generate some errors as shown in Table 5. Because the language model is heavily pre-trained, it is biased against the occurrence of patterns that would enable it to infer the right relation. For example, for the "activeYearsStartYear" relation, the model might confuse it with the birth year. For some relations that do not have a clear direction, the language model is not powerful enough to consider the deep connections between the subject and the object. For example, for the relation "doctoralStudent", the model mistakenly describes a professor as a Ph.D. student. Similarly, the model treats an asteroid as a person because it has an epoch date. For KGs with multiple triples, the generator still has a chance to miss relations or mixes the subject and the object of different relations, especially for the unseen category. For instance, for a soccer player with multiple clubs, the system might confuse the subject of one club's relation with another club.</p>
<p>Related Work</p>
<p>The WebNLG task is similar to Wikibio generation (Lebret et al., 2016;, AMRto-text generation (Song et al., 2018) and RO-TOWIRE (Wiseman et al., 2017;Puduppully et al., 2019). Previous methods usually treat the graphto-text generation as an end-to-end generation task. Those models (Trisedya et al., 2018;Gong et al., 2019;Shen et al., 2020) usually first lineralize the knowledge graph and then use attention mechanism to generate the description sentences. While the linearization of input graph may sacrifice the inter-dependency inside input graph, some papers (Ribeiro et al., 2019(Ribeiro et al., , 2020aZhao et al., 2020)   use graph encoder such as GCN (Duvenaud et al., 2015) and graph transformer (Wang et al., 2020a;Koncel-Kedziorski et al., 2019) to encode the input graphs. Others (Shen et al., 2020; try to carefully design loss functions to control the generation quality. With the development of computation resources, large scale PLMs such as GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) achieve state-ofthe-art results even with simple linearized graph input (Harkous et al., 2020;Chen et al., 2020a;Kale, 2020;Ribeiro et al., 2020b). Instead of directly fine-tuning the PLMs, we propose a two-step finetuning mechanism to get better domain adaptation ability. In addition, using positional embeddings as an extension for PLMs has shown its effectiveness in table-based question answering (Herzig et al., 2020), fact verification , and graph-to-text generation (Chen et al., 2020a). We capture the graph structure by enhancing the input layer with the triple role and tree-level embeddings.</p>
<p>Conclusions and Future Work</p>
<p>We propose a new two-step structured generation task for the graph-to-text generation task based on a two-step fine-tuning mechanism and novel treelevel position embeddings. In the future, we aim to address the remaining challenges and extend the framework for broader applications.</p>
<p>Acknowledgement</p>
<p>This work is partially supported by Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. </p>
<p>References</p>
<p>Figure 1 :
1Input RDF Knowledge Graph</p>
<p>Figure 2 :
2Position Embeddings for the KG inFigure 1</p>
<p>Reference The Acharya Institute of Technology in Karnataka state was given Technical Campus status by All India Council for Technical Education in Mumbai . The school offers tennis which is governed by the International Tennis Federation . Karnataka has the Arabian Sea to its west and in the northeast is Telangana . T5-large The state of Karnataka is located southwest of Telangana and east of the Arabian Sea . It is the location of the Acharya Institute of Technology which was granted the Technical Campus status by the All India Council for Technical Education in Mumbai . The Institute is affiliated with the Visvesvaraya Technological University and offers the sport of tennis . [International Tennis Federation] T5-large + Wiki</p>
<p>Table 1 :
1Human and System Generated Description in</p>
<p>Table 2 :
2System Results on WebNLG Test Set Evaluated by BLEU, METEOR, and TER with Official Scripts</p>
<p>Table 3 :
3description pairs for training, validation, and testing set respectfully. For this task, we investigate a variety of the BART and T5 models with our novel tree-level embeddings. The statistics and more details of those models are listed in Appendix A.Results with both Wikipedia Fine-tuning and 
Positional Embedding for Various Pre-trained Models 
over All Categories on Development Set Evaluated by 
average of PARENT 4 precision, recall, F1 and BLEU 
(%) </p>
<p>We use the original version of English 
WebNLG2017 (Gardent et al., 2017) dataset which 
contains 18,102/2,268/4,928 graph-</p>
<p>Table 4 :
4System Results on WebNLG Test Set Evaluated by BERTScore precision, recall, F1 (%)</p>
<p>Baade was born in the German Empire and graduated from the University of Gottingen. He was the doctoral student of Halton Arp and Allan Sandage and was the discoverer of 1036 Ganymed. S| Walter Baade P| doctoralStudent O| Halton Arp; S| Walter Baade P| doctoralStudent O| Allan Sandage T5-large +Wiki 11264 Claudiomaccone was born on the 26th of November, 2005. He has an orbital period of 1513.722 days, a periapsis of 296521000.0 kilometres and an apoapsis of 475426000.0 kilometres. S| 11264 Claudiomaccone P| epoch O| 2005-11-26; S| Aleksandr Prudnikov P| club O| FC Amkar Perm T5-large +Position The chairman of FC Spartak Moscow is Sergey Rodionov. Aleksandr Prudnikov plays for FC Spartak Moscow and manages FC Amkar Perm. [ S| FC Amkar Perm P| manager O| Gadzhi Gadzhiyev; S| Aleksandr Prudnikov P| club O| FC Amkar Perm ]T5-large 
Andrew White (born in 2003) is a musician who is associated with the band Kaiser Chiefs and Marry Banilow. 
He is also associated with the label Polydor Records and is signed to B-Unique Records. S| Aleksandra Kovač P| 
activeYearsStartYear O| 1990 
T5-large 
Walter </p>
<p>Table 5 :
5System Error Examples. We highlight fabricated facts, [missed relations], incorrect relations, and ground truth relations with different color.</p>
<p>Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 55-76, Dublin, Ireland (Virtual). Association for Computational Linguistics. Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 552-562, Hong Kong, China. Association for Computational Linguistics. Wenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. 2020a. KGPT: Knowledge-grounded pretraining for data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8635-8648, Online. Association for Computational Linguistics.
For this baseline, we use the results reported fromZhao et al. (2020) who also use official evaluation scripts.3 For T5 models, we only keep the Triple Role and Treelevel embeddings.
https://github.com/KaijuML/parent</p>
<p>Tabfact: A large-scale dataset for table-based fact verification. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang Wang, Proceedings of the 8th International Conference on Learning Representations. the 8th International Conference on Learning RepresentationsWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020b. Tabfact: A large-scale dataset for table-based fact verification. In Proceed- ings of the 8th International Conference on Learning Representations.</p>
<p>Handling divergent reference texts when evaluating table-to-text generation. Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, William Cohen, 10.18653/v1/P19-1483Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Co- hen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4884-4895, Flo- rence, Italy. Association for Computational Linguis- tics.</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Iparraguirre, Timothy Bombarell, Alan Hirzel, Ryan P Aspuru-Guzik, Adams, Advances in Neural Information Processing Systems. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. GarnettCurran Associates, Inc28David K Duvenaud, Dougal Maclaurin, Jorge Ipar- raguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. 2015. Convolu- tional networks on graphs for learning molecular fin- gerprints. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2224-2232. Curran Associates, Inc.</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational LinguisticsClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Pro- ceedings of the 10th International Conference on Natural Language Generation, pages 124-133, San- tiago de Compostela, Spain. Association for Compu- tational Linguistics.</p>
<p>Table-to-text generation with effective hierarchical encoder on three dimensions (row, column and time). Xiaocheng Heng Gong, Bing Feng, Ting Qin, Liu, 10.18653/v1/D19-1310Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsHeng Gong, Xiaocheng Feng, Bing Qin, and Ting Liu. 2019. Table-to-text generation with effective hier- archical encoder on three dimensions (row, column and time). In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process- ing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3143-3152, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Ana Suchin Gururangan, Swabha Marasović, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsSuchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.</p>
<p>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. Hamza Harkous, Isabel Groves, Amir Saffari, 10.18653/v1/2020.coling-main.218Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, SpainInternational Committee on Computational LinguisticsHamza Harkous, Isabel Groves, and Amir Saffari. 2020. Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2410-2424, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>TaPas: Weakly supervised table parsing via pre-training. Jonathan Herzig, Krzysztof Nowak, Thomas Müller, Francesco Piccinno, Julian Eisenschlos, 10.18653/v1/2020.acl-main.398Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsJonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pages 4320-4333, Online. Association for Computational Linguistics.</p>
<p>Text-to-text pre-training for data-totext tasks. Mihir Kale, arXiv:2005.10433Computation and Language Repository. 2Mihir Kale. 2020. Text-to-text pre-training for data-to- text tasks. Computation and Language Repository, arXiv:2005.10433. Version 2.</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. Alon Lavie, Abhaya Agarwal, Proceedings of the Second Workshop on Statistical Machine Translation. the Second Workshop on Statistical Machine TranslationPrague, Czech RepublicAssociation for Computational LinguisticsAlon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceed- ings of the Second Workshop on Statistical Machine Translation, pages 228-231, Prague, Czech Repub- lic. Association for Computational Linguistics.</p>
<p>Neural text generation from structured data with application to the biography domain. Rémi Lebret, David Grangier, Michael Auli, 10.18653/v1/D16-1128Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsRémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas. Association for Computational Lin- guistics.</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, 10.18653/v1/W18-6501Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational LinguisticsDiego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for struc- tured data to text generation. In Proceedings of the 11th International Conference on Natural Lan- guage Generation, pages 1-9, Tilburg University, The Netherlands. Association for Computational Linguistics.</p>
<p>Step-by-step: Separating planning from realization in neural data-to-text generation. Amit Moryossef, Yoav Goldberg, Ido Dagan, 10.18653/v1/N19-1236Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2267-2277, Minneapolis, Minnesota. Association for Computational Linguis- tics.</p>
<p>DART: Open-domain structured data record to text generation. Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational LinguisticsLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xian- gru Tang, Aadit Vyas, Neha Verma, Pranav Kr- ishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mu- tuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: Open-domain structured data record to text genera- tion. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, pages 432-447, Online. Association for Com- putational Linguistics.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Data-to-text generation with content selection and planning. Ratish Puduppully, Li Dong, Mirella Lapata, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6908-6915.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1-67.</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsLeonardo F. R. Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing AMR-to-text genera- tion with dual graph representations. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3183-3194, Hong Kong, China. Association for Computational Lin- guistics.</p>
<p>Claire Gardent, and Iryna Gurevych. 2020a. Modeling global and local node contexts for text generation from knowledge graphs. F R Leonardo, Yue Ribeiro, Zhang, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 8Leonardo F. R. Ribeiro, Yue Zhang, Claire Gardent, and Iryna Gurevych. 2020a. Modeling global and local node contexts for text generation from knowl- edge graphs. Transactions of the Association for Computational Linguistics, 8:589-604.</p>
<p>Hinrich Schütze, and Iryna Gurevych. 2020b. Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, arXiv:2007.08426arXiv preprintLeonardo FR Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020b. Investigating pre- trained language models for graph-to-text genera- tion. arXiv preprint arXiv:2007.08426.</p>
<p>Neural data-to-text generation via jointly learning the segmentation and correspondence. Xiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, Dietrich Klakow, 10.18653/v1/2020.acl-main.641Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsXiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, and Dietrich Klakow. 2020. Neural data-to-text genera- tion via jointly learning the segmentation and corre- spondence. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7155-7165, Online. Association for Computa- tional Linguistics.</p>
<p>A study of translation error rate with targeted human annotation. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, Ralph Weischedel, Proceedings of the Association for Machine Transaltion in the Americas. the Association for Machine Transaltion in the AmericasAMTAMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin- nea Micciulla, and Ralph Weischedel. 2006. A study of translation error rate with targeted human annota- tion. In In Proceedings of the Association for Ma- chine Transaltion in the Americas (AMTA 2006).</p>
<p>A graph-to-sequence model for AMRto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P18-1150Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaLong Papers1Association for Computational LinguisticsLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR- to-text generation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616- 1626, Melbourne, Australia. Association for Compu- tational Linguistics.</p>
<p>GTR-LSTM: A triple encoder for sentence generation from RDF data. Jianzhong Bayu Distiawan Trisedya, Rui Qi, Wei Zhang, Wang, 10.18653/v1/P18-1151Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia1Association for Computational LinguisticsBayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, and Wei Wang. 2018. GTR-LSTM: A triple encoder for sentence generation from RDF data. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1627-1637, Melbourne, Australia. As- sociation for Computational Linguistics.</p>
<p>Describing a knowledge base. Qingyun Wang, Xiaoman Pan, Lifu Huang, Boliang Zhang, Zhiying Jiang, Ji Heng, Kevin Knight, 10.18653/v1/W18-6502Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The Netherlands. Association for Computational LinguisticsQingyun Wang, Xiaoman Pan, Lifu Huang, Boliang Zhang, Zhiying Jiang, Heng Ji, and Kevin Knight. 2018. Describing a knowledge base. In Proceed- ings of the 11th International Conference on Natu- ral Language Generation, pages 10-21, Tilburg Uni- versity, The Netherlands. Association for Computa- tional Linguistics.</p>
<p>AMR-to-text generation with graph transformer. Tianming Wang, Xiaojun Wan, Hanqi Jin, 10.1162/tacl_a_00297Transactions of the Association for Computational Linguistics. 8Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020a. AMR-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics, 8:19-33.</p>
<p>Towards faithful neural table-to-text generation with content-matching constraints. Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, Changyou Chen, 10.18653/v1/2020.acl-main.101Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsZhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou Chen. 2020b. Towards faithful neural table-to-text generation with content-matching con- straints. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 1072-1086, Online. Association for Computa- tional Linguistics.</p>
<p>Challenges in data-to-document generation. Sam Wiseman, Stuart Shieber, Alexander Rush, 10.18653/v1/D17-1239Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval- uating text generation with bert. In International Conference on Learning Representations.</p>
<p>Bridging the structural gap between encoding and decoding for data-to-text generation. Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, 10.18653/v1/2020.acl-main.224Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsChao Zhao, Marilyn Walker, and Snigdha Chaturvedi. 2020. Bridging the structural gap between encod- ing and decoding for data-to-text generation. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 2481- 2491, Online. Association for Computational Lin- guistics.</p>            </div>
        </div>

    </div>
</body>
</html>
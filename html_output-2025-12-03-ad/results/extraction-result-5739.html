<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5739 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5739</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5739</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-9e8c7afd10560b5b339cd30f375ed428dcd6d018</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9e8c7afd10560b5b339cd30f375ed428dcd6d018" target="_blank">Identifying Semantically Deviating Outlier Documents</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A generative model to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents are developed.</p>
                <p><strong>Paper Abstract:</strong> A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many applications, such as screening health records for medical mistakes. In this paper, we study the problem of mining semantically deviating document outliers in a given corpus. We develop a generative model to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to 135% improvement over baselines in terms of recall at top-1% of the outlier ranking.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5739.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5739.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>word2vec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>word2vec (Mikolov et al., 2013)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural word embedding method that maps words/phrases to dense vectors preserving semantic proximity; used here to embed words/phrases (normalized) as inputs to a vMF generative model for semantic-region discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distributed representations of words and phrases and their compositionality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>word2vec</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shallow neural embedding model (CBOW / skip-gram family) that learns continuous vector representations of words/phrases by predicting context (or predicting target from context); in this paper trained on an external corpus D_e and used to produce 200-dimensional normalized vectors for words/phrases which are then modeled by a von Mises-Fisher mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Used as an embedding front-end: words/phrases are embedded (200-d), normalized, and then modeled by an Embedded von Mises-Fisher generative model; outlierness of documents is computed from inferred semantic regions and quantile-based scoring over 'orthodox' words.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured text sequences (documents represented as sequences of words/phrases)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Semantic outliers / semantically deviating documents (documents whose content deviates from the corpus semantic focuses)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>New York Times (NYT) and ArnetMiner abstracts (ARNET) (external corpora D_e used for training embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not an anomaly detector by itself; used as embedding input to downstream VMF-Q. Baseline methods that rely on embeddings (P2V-COS) reported MAP and recall@1%/2%/5% (P2V-COS results: NYT MAP=22.07, R@1%=23.45, R@2%=44.64, R@5%=66.18; ARNET MAP=7.39, R@1%=10.51, R@2%=14.78, R@5%=24.14).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>word2vec itself is an embedding; document-level detectors built on embeddings (e.g., paragraph2vec + cosine) are outperformed by the paper's VMF-Q method that uses word2vec embeddings with a vMF generative model and quantile scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Embedding-only baselines that use simple document-level vectors (e.g., paragraph2vec/TFIDF + cosine) can miss fine-grained semantic focuses and be misled by document-level summarization; embeddings require a sufficiently large external corpus D_e and out-of-vocabulary tokens were discarded; method operates on text sequences (documents), not on lists or tabular data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5739.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5739.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P2V-COS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>paragraph2vec (doc2vec) with cosine scoring (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline anomaly detection method that vectorizes each document using paragraph2vec (distributed document vectors) and scores outlierness by negative average cosine similarity between a document vector and the corpus vector.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distributed representations of sentences and documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>paragraph2vec (Doc2Vec)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Document embedding method (Le & Mikolov, 2014) that learns fixed-length vector representations for documents (variants include PV-DM and PV-DBOW); in this paper paragraph2vec vectors of length 100 were trained (jointly with external data) and used to vectorize documents for cosine-similarity based outlier scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Embedding + scoring baseline: compute a 100-d document vector via paragraph2vec and score each document's outlierness as the negative average cosine similarity between the document vector and the corpus vector (P2V-COS).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured text sequences (documents)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Semantic document outliers / novelty relative to a corpus</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>New York Times (NYT) and ArnetMiner (ARNET) (benchmarks generated by injecting documents from other corpora into a target corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MAP and recall at top-r percentages. Reported results: NYT MAP=22.07, Recall@1%=23.45, Recall@2%=44.64, Recall@5%=66.18; ARNET MAP=7.39, R@1%=10.51, R@2%=14.78, R@5%=24.14.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>P2V-COS is a competitive baseline but is substantially outperformed by the paper's VMF-Q method (e.g., on NYT VMF-Q R@1%=56.99 vs P2V-COS 23.45; VMF-Q MAP=41.88 vs P2V-COS 22.07). The authors cite examples where paragraph2vec ranks documents incorrectly because document-level vectors can be dominated by majority content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paragraph2vec summarization can overwhelm salient but small semantically orthodox fragments (leading to false positives/negatives); performs worse on shorter or highly technical documents (ARNET). The baseline is not designed for structured/tabular data or generic lists—applies to entire documents only.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5739.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5739.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VMF-Q</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedded von Mises-Fisher Allocation with Quantile outlierness (VMF-Q)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed method: learn semantic regions in normalized word/phrase embedding space via a vMF-mixture generative model, select concentrated 'semantic focuses', compute word-level 'orthodox' probabilities (penalizing lexically general words), and score document outlierness with a quantile-based measure robust to noisy content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Embedded von Mises-Fisher Allocation + quantile-based outlierness (VMF-Q)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative mixture model over unit-normalized word/phrase embeddings using von Mises-Fisher (vMF) components (T components), with priors on mean directions and log-normal priors on concentrations; parameters inferred via collapsed Gibbs sampling (with Metropolis-Hastings for kappa). Semantic focuses selected by thresholding component concentration (kappa). Document scoring uses posterior probabilities of words belonging to focuses, a background-corpus specificity penalty, and a Poisson-Binomial quantile-based outlierness (confidence θ).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Model-based detection: (1) embed words/phrases (word2vec), (2) fit vMF mixture to normalized embeddings to discover semantic regions and κ (concentration), (3) select semantic focuses by κ thresholding, (4) compute per-word orthodox probabilities combining focus membership and corpus-specificity from a background corpus, (5) compute quantile (Poisson-Binomial) based outlierness Ω_{θ·q} to emphasize highly-orthodox words and suppress noisy tail.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured text sequences (documents as sequences of words/phrases); method operates at word/phrase granularity aggregated to documents</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Semantic anomalies / semantically deviating documents (outlier documents relative to corpus semantic focuses)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>New York Times (NYT) (41,959 articles with section-labeled corpora) and ArnetMiner abstracts (ARNET); external corpora D_e used for embedding and phrase mining</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean Average Precision (MAP) and recall at top-r% (r = 1%, 2%, 5%). Reported VMF-Q results: NYT MAP=41.88, Recall@1%=56.99, Recall@2%=63.29, Recall@5%=79.23; ARNET MAP=19.74, Recall@1%=22.40, Recall@2%=34.40, Recall@5%=53.87.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>VMF-Q substantially outperforms traditional baselines (TFIDF-COS, P2V-COS, UNI-KL, TM-KL) and ablations VMF-SF and VMF-E. The paper reports VMF-Q achieves a 45%–135% increase over baselines in recall@1% depending on dataset; ablation shows quantile scoring and specificity penalty both contribute materially (quantile-based change alone improved R@1% by 50–75%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Designed for textual documents (sequences of words/phrases), not demonstrated on lists or structured/tabular data; relies on a sufficiently large external corpus for phrase mining and embedding (OOV tokens dropped); sensitive to short documents and very technical vocabulary (ARNET shows lower absolute performance); requires hyperparameters (T, β, θ) though authors report moderate robustness; sampling kappa requires Metropolis-Hastings (inference complexity). Failure modes include documents with small but important orthodox fragments being overwhelmed by noisy text if not for the quantile step; simple averaging outlier measures can fail (demonstrated).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Von mises-fisher clustering models <em>(Rating: 2)</em></li>
                <li>Nonparametric spherical topic modeling with word embeddings <em>(Rating: 2)</em></li>
                <li>Gaussian LDA for topic models with word embeddings <em>(Rating: 2)</em></li>
                <li>Unsupervised Detection of Anomalous Text <em>(Rating: 2)</em></li>
                <li>Outlier detection: A survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5739",
    "paper_id": "paper-9e8c7afd10560b5b339cd30f375ed428dcd6d018",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "word2vec",
            "name_full": "word2vec (Mikolov et al., 2013)",
            "brief_description": "A neural word embedding method that maps words/phrases to dense vectors preserving semantic proximity; used here to embed words/phrases (normalized) as inputs to a vMF generative model for semantic-region discovery.",
            "citation_title": "Distributed representations of words and phrases and their compositionality",
            "mention_or_use": "use",
            "model_name": "word2vec",
            "model_description": "Shallow neural embedding model (CBOW / skip-gram family) that learns continuous vector representations of words/phrases by predicting context (or predicting target from context); in this paper trained on an external corpus D_e and used to produce 200-dimensional normalized vectors for words/phrases which are then modeled by a von Mises-Fisher mixture.",
            "model_size": null,
            "anomaly_detection_method": "Used as an embedding front-end: words/phrases are embedded (200-d), normalized, and then modeled by an Embedded von Mises-Fisher generative model; outlierness of documents is computed from inferred semantic regions and quantile-based scoring over 'orthodox' words.",
            "data_type": "Unstructured text sequences (documents represented as sequences of words/phrases)",
            "anomaly_type": "Semantic outliers / semantically deviating documents (documents whose content deviates from the corpus semantic focuses)",
            "dataset_name": "New York Times (NYT) and ArnetMiner abstracts (ARNET) (external corpora D_e used for training embeddings)",
            "performance_metrics": "Not an anomaly detector by itself; used as embedding input to downstream VMF-Q. Baseline methods that rely on embeddings (P2V-COS) reported MAP and recall@1%/2%/5% (P2V-COS results: NYT MAP=22.07, R@1%=23.45, R@2%=44.64, R@5%=66.18; ARNET MAP=7.39, R@1%=10.51, R@2%=14.78, R@5%=24.14).",
            "baseline_comparison": "word2vec itself is an embedding; document-level detectors built on embeddings (e.g., paragraph2vec + cosine) are outperformed by the paper's VMF-Q method that uses word2vec embeddings with a vMF generative model and quantile scoring.",
            "limitations_or_failure_cases": "Embedding-only baselines that use simple document-level vectors (e.g., paragraph2vec/TFIDF + cosine) can miss fine-grained semantic focuses and be misled by document-level summarization; embeddings require a sufficiently large external corpus D_e and out-of-vocabulary tokens were discarded; method operates on text sequences (documents), not on lists or tabular data.",
            "uuid": "e5739.0"
        },
        {
            "name_short": "P2V-COS",
            "name_full": "paragraph2vec (doc2vec) with cosine scoring (baseline)",
            "brief_description": "Baseline anomaly detection method that vectorizes each document using paragraph2vec (distributed document vectors) and scores outlierness by negative average cosine similarity between a document vector and the corpus vector.",
            "citation_title": "Distributed representations of sentences and documents",
            "mention_or_use": "use",
            "model_name": "paragraph2vec (Doc2Vec)",
            "model_description": "Document embedding method (Le & Mikolov, 2014) that learns fixed-length vector representations for documents (variants include PV-DM and PV-DBOW); in this paper paragraph2vec vectors of length 100 were trained (jointly with external data) and used to vectorize documents for cosine-similarity based outlier scoring.",
            "model_size": null,
            "anomaly_detection_method": "Embedding + scoring baseline: compute a 100-d document vector via paragraph2vec and score each document's outlierness as the negative average cosine similarity between the document vector and the corpus vector (P2V-COS).",
            "data_type": "Unstructured text sequences (documents)",
            "anomaly_type": "Semantic document outliers / novelty relative to a corpus",
            "dataset_name": "New York Times (NYT) and ArnetMiner (ARNET) (benchmarks generated by injecting documents from other corpora into a target corpus)",
            "performance_metrics": "MAP and recall at top-r percentages. Reported results: NYT MAP=22.07, Recall@1%=23.45, Recall@2%=44.64, Recall@5%=66.18; ARNET MAP=7.39, R@1%=10.51, R@2%=14.78, R@5%=24.14.",
            "baseline_comparison": "P2V-COS is a competitive baseline but is substantially outperformed by the paper's VMF-Q method (e.g., on NYT VMF-Q R@1%=56.99 vs P2V-COS 23.45; VMF-Q MAP=41.88 vs P2V-COS 22.07). The authors cite examples where paragraph2vec ranks documents incorrectly because document-level vectors can be dominated by majority content.",
            "limitations_or_failure_cases": "Paragraph2vec summarization can overwhelm salient but small semantically orthodox fragments (leading to false positives/negatives); performs worse on shorter or highly technical documents (ARNET). The baseline is not designed for structured/tabular data or generic lists—applies to entire documents only.",
            "uuid": "e5739.1"
        },
        {
            "name_short": "VMF-Q",
            "name_full": "Embedded von Mises-Fisher Allocation with Quantile outlierness (VMF-Q)",
            "brief_description": "The paper's proposed method: learn semantic regions in normalized word/phrase embedding space via a vMF-mixture generative model, select concentrated 'semantic focuses', compute word-level 'orthodox' probabilities (penalizing lexically general words), and score document outlierness with a quantile-based measure robust to noisy content.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Embedded von Mises-Fisher Allocation + quantile-based outlierness (VMF-Q)",
            "model_description": "A generative mixture model over unit-normalized word/phrase embeddings using von Mises-Fisher (vMF) components (T components), with priors on mean directions and log-normal priors on concentrations; parameters inferred via collapsed Gibbs sampling (with Metropolis-Hastings for kappa). Semantic focuses selected by thresholding component concentration (kappa). Document scoring uses posterior probabilities of words belonging to focuses, a background-corpus specificity penalty, and a Poisson-Binomial quantile-based outlierness (confidence θ).",
            "model_size": null,
            "anomaly_detection_method": "Model-based detection: (1) embed words/phrases (word2vec), (2) fit vMF mixture to normalized embeddings to discover semantic regions and κ (concentration), (3) select semantic focuses by κ thresholding, (4) compute per-word orthodox probabilities combining focus membership and corpus-specificity from a background corpus, (5) compute quantile (Poisson-Binomial) based outlierness Ω_{θ·q} to emphasize highly-orthodox words and suppress noisy tail.",
            "data_type": "Unstructured text sequences (documents as sequences of words/phrases); method operates at word/phrase granularity aggregated to documents",
            "anomaly_type": "Semantic anomalies / semantically deviating documents (outlier documents relative to corpus semantic focuses)",
            "dataset_name": "New York Times (NYT) (41,959 articles with section-labeled corpora) and ArnetMiner abstracts (ARNET); external corpora D_e used for embedding and phrase mining",
            "performance_metrics": "Mean Average Precision (MAP) and recall at top-r% (r = 1%, 2%, 5%). Reported VMF-Q results: NYT MAP=41.88, Recall@1%=56.99, Recall@2%=63.29, Recall@5%=79.23; ARNET MAP=19.74, Recall@1%=22.40, Recall@2%=34.40, Recall@5%=53.87.",
            "baseline_comparison": "VMF-Q substantially outperforms traditional baselines (TFIDF-COS, P2V-COS, UNI-KL, TM-KL) and ablations VMF-SF and VMF-E. The paper reports VMF-Q achieves a 45%–135% increase over baselines in recall@1% depending on dataset; ablation shows quantile scoring and specificity penalty both contribute materially (quantile-based change alone improved R@1% by 50–75%).",
            "limitations_or_failure_cases": "Designed for textual documents (sequences of words/phrases), not demonstrated on lists or structured/tabular data; relies on a sufficiently large external corpus for phrase mining and embedding (OOV tokens dropped); sensitive to short documents and very technical vocabulary (ARNET shows lower absolute performance); requires hyperparameters (T, β, θ) though authors report moderate robustness; sampling kappa requires Metropolis-Hastings (inference complexity). Failure modes include documents with small but important orthodox fragments being overwhelmed by noisy text if not for the quantile step; simple averaging outlier measures can fail (demonstrated).",
            "uuid": "e5739.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Von mises-fisher clustering models",
            "rating": 2
        },
        {
            "paper_title": "Nonparametric spherical topic modeling with word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Gaussian LDA for topic models with word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised Detection of Anomalous Text",
            "rating": 2
        },
        {
            "paper_title": "Outlier detection: A survey",
            "rating": 1
        }
    ],
    "cost": 0.011348,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Identifying Semantically Deviating Outlier Documents*</h1>
<p>Honglei Zhuang ${ }^{1}$, Chi Wang ${ }^{2}$, Fangbo Tao ${ }^{1}$, Lance Kaplan ${ }^{3}$ and Jiawei Han ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, University of Illinois at Urbana-Champaign<br>${ }^{2}$ Microsoft Research, Redmond ${ }^{3}$ US Army Research Lab<br>{hzhuang3, ftao2, hanj}@illinois.edu,<br>wang.chi@microsoft.com, lance.m.kaplan.civ@mail.mil</p>
<h4>Abstract</h4>
<p>A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many applications, such as screening health records for medical mistakes. In this paper, we study the problem of mining semantically deviating document outliers in a given corpus. We develop a generative model to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to $135 \%$ improvement over baselines in terms of recall at top- $1 \%$ of the outlier ranking.</p>
<h2>1 Introduction</h2>
<p>The technology today has made it unprecedentedly easy to collect and store documents in an increasing number of domains. Automatic text analysis (e.g. document clustering, summarization, topic modeling) becomes more useful and demanded as the corpus size grows. Some trending</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>domains (e.g. health records) call for a new analytical task, mining outlier documents: given a corpus, identify a small number of documents which substantially deviate from the semantic focuses of the given corpus. Outlier documents can provide valuable insights or imply potential errors. For example, an outlier health record from records of the same disease could indicate a new variation of the disease if it has an abnormal symptom description, or a medical error if it has an abnormal treatment description. A previous study (Hauskrecht et al., 2013) uses structured data in health records to show the importance of this application, and points out that further improvement should be achieved by leveraging text data.</p>
<p>Existing work has studied a related albeit different task, novel document detection (Kasiviswanathan et al., 2012, 2013; Zhang et al., 2002, 2004), where one aims to identify from a document stream if a newly arriving document is novel or redundant. In other words, this task assumes all the previous documents are known to be "normal", and only checks if a new document is novel. In our task, no document is known to be normal, and there could be multiple outliers in the corpus. Outlier detection (Chandola et al., 2009; Hodge and Austin, 2004) is a popular topic in data mining but few focus on text data. A study (Guthrie, 2008) identifies anomalous text segments in a document, but mainly based on writing styles. We focus on studying semantically deviating documents.</p>
<p>The problem of detecting outlier documents has its unique challenges. First, different words or phrases may be used to indicate the same semantic meaning, which introduces lexical sparsity. Second, finding proper words or phrases to characterize the corpus is non-trivial. Semantically frequent words or phrases can still be too general or too vague. Third, a document can carry extremely</p>
<p>rich and noisy signals, most of which are not helpful to determine whether it is an outlier.</p>
<p>We tackle the problem of mining outlier documents in the following steps. We leverage word embedding (Mikolov et al., 2013) to capture the semantic proximities between words and/or phrases, in order to solve the sparsity issue. Then we propose a generative model to identify semantic regions in the embedded space frequently mentioned by documents in the corpus. The model represents each semantic region with a von MisesFisher distribution. We also learn a concentration parameter for each region with our model, and develop a selection method to identify semantically specific regions which can better represent the corpus, and filter regions with largely uninformative words.</p>
<p>As the final step, we design a robust outlierness measure emphasizing only the words or phrases in a document relatively close to the semantic focuses identified, and eliminating the noises and redundant information.</p>
<p>The remaining of the paper is organized as follows. Section 2 introduces the preprocessing of data sets and clarifies the notations. Section 3 proposes the methodology to mine outlier documents. Section 4 describes the experiment setup, Section 5 presents the results and Section 6 concludes the paper.</p>
<h2>2 Preliminaries</h2>
<p>In this section, we formalize the problem and then briefly describe the preprocessing step.</p>
<h3>2.1 Notations</h3>
<p>The notations used in this study are introduced here. A document is represented as a sequence $d_{i}=\left(w_{i 1}, w_{i 2}, \cdots, w_{i n_{i}}\right)$, where each $w_{i j} \in \mathcal{V}$ represents a word or phrase from a given vocabulary $\mathcal{V}$ and $n_{i}$ denotes the length of the $d_{i}$. We refer to a set of documents as a corpus, represented as $D=\left{d_{i}\right}_{i=1}^{|D|}$.</p>
<p>Notice that $w_{i j}$ may refer to a unigram word or a multi-gram phrase. Although it is nontrivial to appropriately segment a document into a mixed sequence of words and phrases, it is not the focus of our paper. A recently developed phrase mining technique (Liu et al., 2015) is used to extract quality phrases and segment the documents.</p>
<p>Word embedding provides vectorized representations of words and phrases to capture their se-
mantic proximity. We assume there is an effective word embedding technique (e.g. (Mikolov et al., 2013)), $f: \mathcal{V} \mapsto \mathbb{R}^{\nu}$, where $f$ is the transforming function that takes a word or a phrase as input and projects it into a $\nu$-dimensional vector as its distributed representation. The semantic proximity between two words or phrases $w$ and $w^{\prime}$ can be preserved by the cosine similarity between their embedded vectors:</p>
<p>$$
\operatorname{CosSim}\left(f(w), f\left(w^{\prime}\right)\right)=\frac{f(w) \cdot f\left(w^{\prime}\right)}{|f(w)| \times\left|f\left(w^{\prime}\right)\right|}
$$</p>
<p>Problem definition. This work studies how to effectively rank documents in a corpus based on how much they deviate from the semantic focuses of the corpus. Given a set of documents $D$, our objective is to design an outlierness measure $\Omega: D \mapsto \mathbb{R}$, such that documents with larger outlierness $\Omega(d)$ semantically deviate more from the majority of $D$.</p>
<h3>2.2 Preprocessing</h3>
<p>We perform several steps of preprocessing to derive the input representation of each document in a given corpus.
Phrase mining. SegPhrase, a recently developed phrase-mining method (Liu et al., 2015), is utilized to automatically identify quality phrases in a corpus. After being trained in one corpus, SegPhrase is also capable of segmenting unseen documents into chunks of phrases with mixed lengths. We train SegPhrase on an external corpus $D_{e}$ to obtain the list of quality phrases. Then for each corpus $D$ given for outlier detection, we employ the trained SegPhrase to chunk each document into a sequence of words and quality phrases.
Word embedding. We adopt word embedding as a preprocessing step to capture the semantic proximity between words/phrases. Instead of using the raw text, similar to (Liu et al., 2015), we use the sequence derived from SegPhrase as input to the word embedding algorithm. In particular, word2vec (Mikolov et al., 2013) is utilized in our experiments, but can be seamlessly replaced by any other embedding results.</p>
<p>We run the embedding algorithm based on the external corpus $D_{e}$, the same corpus used in phrase mining. As $D_{e}$ is sufficiently large, there are only few words or phrases in $D$ which never appear in $D_{e}$, and are simply discarded in the experiments.</p>
<p>Stop words removal. We remove stop words, as well as the words or phrases ranked high within a certain quantile in terms of document frequency ${ }^{1}$ (DF) in the external corpus $D_{e}$. Such words or phrases usually carry background noise, and obstruct outlier detection.</p>
<h2>3 Mining Outlier Documents</h2>
<p>Our framework consists of the following steps. First, we leverage a generative model to identify semantic "regions" in the word embedding space frequently mentioned by documents in the given corpus. Second, we develop a selection method to further remove semantics regions that are too general to properly characterize the given corpus, and only keep regions both frequent and semantically specific, denoted as "semantic focuses". Finally, we calculate the outlierness measure for each document based on the mined semantic focuses. We design a robust outlierness measure which is less sensitive to noisy words or phrases in documents.</p>
<h3>3.1 Embedded von Mises-Fisher Allocation</h3>
<p>We start with a generative model to identify the frequent semantic regions in the word embedding space.</p>
<p>Since we use cosine similarity to capture the semantic proximities between two words or phrases, the magnitude of the embedding vector of each word can be omitted in this part. We use $\mathbf{x}<em i="i" j="j">{i j}=$ $f\left(w</em>}\right) /\left|f\left(w_{i j}\right)\right|$ to represent the unit vector with the same direction as the embedded vector of $w_{i j}$, and use $\mathbf{X}$ to represent the collection of all $\mathbf{x<em i="i">{i j}$ where $1 \leq i \leq|D|$ and $1 \leq j \leq n</em>$.</p>
<p>In order to characterize a semantic region in the embedded space, we introduce von Mises-Fisher (vMF) distribution. The von Mises-Fisher (vMF) distribution is prevalently adopted in directional statistics, which studies the distribution of normalized vectors on a spherical space. The probability density function of the vMF distribution is explicitly instantiated by the cosine similarity. It is an ideal distribution for our task because we use cosine similarity to measure the semantic proximity. Moreover, as we will see later, it empowers us to characterize how specific each semantic region is, which is helpful in further identification of semantic focuses for outlier detection.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We first introduce the formalization of the von Mises-Fisher distribution.
Von Mises-Fisher (vMF) distribution. A $\nu$ dimensional unit random vector $\mathbf{x}$ (i.e. $\mathbf{x} \in \mathbb{R}^{\nu}$ and $|\mathbf{x}|=1$ ) follows a von Mises-Fisher distribution $\operatorname{vMF}(\cdot \mid \boldsymbol{\mu}, \kappa)$ if the probability density function follows:</p>
<p>$$
p(\mathbf{x})=C_{\nu}(\kappa) \exp \left(\kappa \boldsymbol{\mu}^{\top} \mathbf{x}\right)
$$</p>
<p>where $C_{\nu}(\kappa)=\kappa^{\nu / 2-1} /(2 \pi)^{\nu / 2} I_{\nu / 2-1}(\kappa)$; and $I_{\nu / 2-1}(\cdot)$ is the modified Bessel function of the first kind; $(\nu / 2-1)$ is the order.</p>
<p>The two parameters in the vMF distribution are the mean direction $\boldsymbol{\mu}$ and the concentration parameter $\kappa$ respectively, where $\boldsymbol{\mu} \in \mathbb{R}^{\nu},|\boldsymbol{\mu}|=1$ and $\kappa&gt;0$. The distribution concentrated around the mean direction $\boldsymbol{\mu}$, and is more concentrated if the concentration parameter $\kappa$ is larger.
Embedded von Mises-Fisher allocation. We propose a generative model by regarding each document as a bag of normalized embedded vectors, analogous to the bag-of-word representation of documents utilized in typical topic model (e.g., LDA (Blei et al., 2001)). The major difference is that the data to be generated is now a bag-of-normalized-embedded-vectors for each document, and should be generated from a mixed vMF distribution instead of a mixed multinomial distribution.</p>
<p>A formalized description of the model is summarized as follows:</p>
<p>$$
\begin{array}{ll}
\boldsymbol{\mu}<em 0="0">{t} \sim \operatorname{vMF}\left(\cdot \mid \boldsymbol{\mu}</em>\right), &amp; t=1,2, \cdots, T \
\kappa_{t} \sim \log \operatorname{Normal}\left(\cdot \mid m_{0}, \sigma_{0}^{2}\right), &amp; t=1,2, \cdots, T \
\boldsymbol{\pi}}, C_{0<em i="i" j="j">{i} \sim \operatorname{Dirichlet}(\cdot \mid \boldsymbol{\alpha}), &amp; i=1,2, \cdots,|D| \
z</em>\right| \
\mathbf{x}} \sim \operatorname{Categorical}\left(\cdot \mid \pi_{i}\right), &amp; j=1,2, \cdots,\left|d_{i<em j="j" z__i="z_{i">{i j} \sim \operatorname{vMF}\left(\cdot \mid \boldsymbol{\mu}</em>\right|
\end{array}
$$}}, \kappa_{z_{i j}}\right), &amp; j=1,2, \cdots,\left|d_{i</p>
<p>where $T&gt;0$ is an integer indicating the number of semantic regions, namely the number of vMF distributions in our mixture model.</p>
<p>We regularize the vMF parameters by the following prior distributions. We assume the mean direction $\boldsymbol{\mu}<em 0="0">{t}$ of each vMF distribution is generated from a prior vMF distribution $\operatorname{vMF}\left(\cdot \mid \boldsymbol{\mu}</em>\right)$. A similar design is also adopted in (Gopal and Yang, 2014).}, C_{0}\right)$, while the concentration parameter $\kappa_{t}$ is generated from a log-normal prior $\log \operatorname{Normal}\left(\cdot \mid m_{0}, \sigma_{0}^{2</p>
<p>Parameter inference. We infer the parameters by Gibbs sampling. Because both the von MisesFisher distribution and the Dirichlet distribution</p>
<p>have conjugate priors, we can integrate out parameters $\boldsymbol{\mu}<em i="i">{t}$ and $\boldsymbol{\pi}</em>$ :}$ and develop a collapsed Gibbs sampler of $z_{i j</p>
<p>$$
\begin{aligned}
&amp; P\left(z_{i j}=t \mid \mathbf{Z}^{-i j}, \mathbf{X}, \boldsymbol{\kappa} ; \boldsymbol{\alpha}, m_{0}, \sigma_{0}^{2}, \boldsymbol{\mu}<em 0="0">{0}, C</em>\right) \
&amp; \propto \frac{\left(n_{i t}^{-i j}+1+\boldsymbol{\alpha}^{(t)}\right) C_{\nu}\left(\kappa_{t}\right) C_{\nu}\left(\left|C_{0} \boldsymbol{\mu}<em t="t">{0}+\kappa</em>} \mathbf{x<em _nu="\nu">{. t}^{-i j}\right|\right)}{C</em>}\left(\left|C_{0} \boldsymbol{\mu<em t="t">{0}+\kappa</em>}\left(\mathbf{x<em i="i" j="j">{. t}^{-i j}+\mathbf{x}</em>
\end{aligned}
$$}\right)\right|\right)</p>
<p>where $n_{i t}^{-i j}=\sum_{j^{\prime}}^{\left|d_{i}\right|} \delta\left(z_{i j^{\prime}}=t\right)-\delta\left(z_{i j}=t\right)$ is the number of words in the $i$-th document being assigned to the $t$-th von Mises-Fisher distribution without taking $w_{i j}$ into account; $\mathbf{x}<em i_prime="i^{\prime">{. t}^{-i j}=$ $\sum</em>}}^{[D]} \sum_{j^{\prime}}^{\left|d_{i}\right|} \mathbf{x<em i_prime="i^{\prime">{i^{\prime} j^{\prime}} \delta\left(z</em>$. Here $\delta(\cdot)$ is the indicator function.} j^{\prime}}=t\right)-\delta\left(z_{i j}=t\right)$ is the sum of word vectors assigned to semantic region $t$ without counting $w_{i j</p>
<p>We can also derive a collapsed Gibbs sampler for concentration parameters $\kappa_{t}$ 's:</p>
<p>$$
\begin{aligned}
&amp; P\left(\kappa_{t} \mid \mathbf{Z}, \mathbf{X}, \boldsymbol{\kappa}^{-t} ; \boldsymbol{\alpha}, m_{0}, \sigma_{0}^{2}, \boldsymbol{\mu}<em 0="0">{0}, C</em>\right) \
\propto &amp; \frac{C_{\nu}^{n . t}\left(\kappa_{t}\right)}{C_{\nu}\left(\left|C_{0} \boldsymbol{\mu}<em t="t">{0}+\kappa</em>} \mathbf{x<em t="t">{. t}\right|\right)} \log \operatorname{Normal}\left(\kappa</em>\right)
\end{aligned}
$$} \mid m_{0}, \sigma_{0}^{2</p>
<p>where $n_{. t}$ is the number of words in semantic region $t$.</p>
<p>While sampling $z_{i j}$ is relatively trivial, sampling $\kappa_{t}$ is not straightforward. Similar difficulty is also mentioned in (Gopal and Yang, 2014). We employ a Metropolis-Hasting algorithm with another log-normal distribution centered at the current $\kappa_{t}$ value as the proposal distribution.</p>
<p>After obtaining a sample from the posterior distribution of $z_{i j}$ 's and $\kappa_{t}$ 's, we can easily obtain the MAP estimate of mean directions $\boldsymbol{\mu}<em i="i">{t}$ 's and the mixing distribution of each documents $\boldsymbol{\pi}</em>$ :</p>
<p>$$
\hat{\boldsymbol{\mu}}<em 0="0">{t}=\frac{C</em>} \boldsymbol{\mu<em t="t">{0}+\kappa</em>} \mathbf{x<em 0="0">{. t}}{\left|C</em>} \boldsymbol{\mu<em t="t">{0}+\kappa</em>} \mathbf{x<em i="i">{. t}\right|}, \quad \hat{\boldsymbol{\pi}}</em>
$$}=\frac{n_{i t}+\boldsymbol{\alpha}^{(t)}}{n_{i \cdot}+\sum_{t} \boldsymbol{\alpha}^{(t)}</p>
<p>Discussions. We notice that there are some topic models (Das et al., 2015; Batmanghelich et al., 2016) proposed for similar data, where words are represented as embedding vectors. Our model is proposed independently for the purpose of identifying semantic focuses, which serves the task of outlier detection. Existing models may lack signals for the following outlier detection steps and hence cannot be directly plugged in. However, it is possible to adapt certain models to the outlier detection task.</p>
<h3>3.2 Identifying Semantic Focuses</h3>
<p>The semantic regions learned from the Embedded vMF Allocation model provide a set of candidates frequently mentioned by documents in the corpus. However, not all of them are semantic focuses of the corpus - some are too general to distinguish outlier and normal document.</p>
<p>We notice that uninformative semantic regions (e.g. a semantic region containing {"percent", "average", "compare", ...}) tend to have more scattered distribution over embedded vectors, possibly because of the diverse context of their usage. In contrast, corpus-specific semantic regions are more concentrated, (e.g. a semantic region containing {"drugs", "antidepressant", "prescription", ...}). Modeling semantic regions by vMF distributions provides us with a parsimonious signal to characterize how concentrated a semantic region is, i.e. the concentration parameter $\kappa_{t}$. This allows us to simply filter unqualified semantic regions with too small concentration parameters and obtain high-quality semantic focuses. Let a binary variable $\phi_{t}(t=1,2, \cdots, T)$ indicate whether the $t$-th vMF distribution is a semantic focus. Suppose a user specifies a threshold parameter $0 \leq$ $\beta \leq 1$. We can determine $\phi_{t}$ by estimating the log-normal distribution that generates all $\kappa_{t}$ 's, $\log \operatorname{Normal}\left(\hat{m}, \hat{\sigma}^{2}\right)$, where
$\hat{m}=\frac{1}{T} \sum_{t} \log \left(\kappa_{t}\right), \hat{\sigma}^{2}=\frac{1}{T} \sum_{t}\left(\log \left(\kappa_{t}\right)-\hat{m}\right)^{2}$
Set $\hat{F}<em t="t">{\kappa}(\cdot)$ to be its cumulative distribution function. We assign $\phi</em>=0$.}=1$ for semantic regions with $\kappa_{t} \geq F_{\kappa}^{-1}(\beta)$, and filter all the other semantic regions as $\phi_{t</p>
<p>Although parameter $\beta$ needs to be set manually, our experiments suggest the performance is not quite sensitive to its value.</p>
<h3>3.3 Document Outlierness</h3>
<p>In this subsection, we start with a straightforward definition of outlierness based on the mined semantic focuses. Then we present several refinements to improve its robustness.
Baseline outlierness measure. A straightforward intuition is to assume outlier documents averagely have fewer words or phrases drawn from semantic focuses. To estimate this, we first need to calculate the probability of each word being drawn</p>
<p>from the semantic focuses.
$P\left(\phi_{z_{i j}}=1 \mid \mathbf{x}<em i="i">{i j}, \boldsymbol{\pi}</em>}\right)=\frac{\sum_{t} \phi_{t} \boldsymbol{\pi<em i="i" j="j">{i}^{(t)} \mathrm{vMF}\left(\mathbf{x}</em>} \mid \boldsymbol{\mu<em t="t">{t}, \kappa</em>}\right)}{\sum_{t} \boldsymbol{\pi<em i="i" j="j">{i}^{(t)} \mathrm{vMF}\left(\mathbf{x}</em>} \mid \boldsymbol{\mu<em t="t">{t}, \kappa</em>$
It is then possible to estimate the expected percentage of words not drawn from semantic focuses in each document as the outlierness:}\right)</p>
<p>$$
\Omega_{\mathrm{sf}}\left(d_{i}\right)=1-\frac{1}{\left|d_{i}\right|} \sum_{j=1}^{\left|d_{i}\right|} P\left(\phi_{z_{i j}}=1 \mid \mathbf{x}<em i="i">{i j}, \boldsymbol{\pi}</em>\right)
$$</p>
<p>However, due to the noisiness in text data, this assumption oversimplifies the characterization of outlier documents. In practice, we observe the following two issues: lexically general words/phrases, and noisy content in documents.</p>
<p>Penalizing lexically general words and phrases. Not all words or phrases close to semantic focuses are strong indicators of normal documents. General words (e.g. "science") can happen to be semantically close to a semantic focus, but are not as specific as most other words close to it (e.g. "medical research"). Therefore, we utilize a background corpus $D_{b g}$ to calculate the specificity of the word. Assuming the actual mention of the word can be chosen from either the general background, or a corpus-specific vocabulary, we write down the probability that a word is corpus-specific to be:</p>
<p>$$
P\left(\lambda_{i j} \mid w_{i j}\right)=\frac{n d\left(w_{i j}\right) /|D|}{n d\left(w_{i j}\right) /|D|+n d_{b g}\left(w_{i j}\right) /\left|D_{b g}\right|}
$$</p>
<p>where $n d(w)=\left|\left{d_{i} \mid w \in d_{i}, d_{i} \in D\right}\right|$ is the number of documents in $D$ containing word $w$; $n d_{b g}(w)=\left|\left{d_{i} \mid w \in d_{i}, d_{i} \in D_{b g}\right}\right|$ is the number of documents containing word $w$ in the background corpus $D_{b g} ; \lambda_{i j}$ is a binary random variable indicating whether $w_{i j}$ is specific enough.</p>
<p>For each word, we define the word is orthodox if the word is not only semantically close to a semantic focus of the corpus, but also sufficiently specific. We then define the probability that a word or phrase $w_{i j}$ in document $d_{i}$ is orthodox as:</p>
<p>$$
P\left(\varphi_{i j} \mid \mathbf{x}<em i="i">{i j}, \boldsymbol{\pi}</em>} w_{i j}\right)=P\left(\phi_{z_{i j}} \mid \mathbf{x<em i="i">{i j}, \boldsymbol{\pi}</em>\right)
$$}\right) P\left(\lambda_{i j} \mid w_{i j</p>
<p>where $\varphi_{i j}=1$ indicates that $w_{i j}$ (or equivalently $\mathbf{x}_{i j}$ ) is orthodox.</p>
<p>Now, we can define a second outlierness measure as the expected percentage of words that are not orthodox.</p>
<p>$$
\Omega_{\mathrm{e}}\left(d_{i}\right)=1-\frac{1}{\left|d_{i}\right|} \sum_{j=1}^{\left|d_{i}\right|} P\left(\varphi_{i j} \mid \mathbf{x}<em i="i">{i j}, \boldsymbol{\pi}</em>\right)
$$}, w_{i j</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Ranked orthodox probability $P\left(\varphi_{i j} \mid \mathbf{x}<em i="i">{i j}, \boldsymbol{\pi}</em>\right)$
(b) Probability distribution of random variable $n_{i}^{\varphi} /\left|d_{i}\right|$}, w_{i j</p>
<p>Figure 1: Comparison of a normal document and an outlier document in a news corpus ("Health" topic).</p>
<p>Noisy content in documents. We present the second issue of normal documents with an example. We compare a normal document in a corpus of New York Times news articles with tag "Health", to another document originally from another corpus, but with its outlierness calculated with regard to the semantic focuses of the "Health" corpus.</p>
<p>In Figure 1(a), we show the distribution of inferred orthodox probability $P\left(\varphi_{i j}=1 \mid \mathbf{x}<em i="i" j="j">{i j}, w</em>$ of two documents is insignificant. Therefore, the measure described in Equation (2) will be unable to tell the difference between these two documents.}\right)$ by ranking the words or phrases according to their probability value. We can observe that the outlier document barely has any words or phrases surely orthodox, while the normal document has $5 \%$ of words or phrases with a probability no less than 0.8 to be orthodox. However, if we simply take the average, these two documents become indistinguishable as the average is substantially dominated by the "tail" where most words or phrases in either documents are clearly not orthodox. Let $n_{i}^{\varphi}$ be a random variable indicating the true number of orthodox words or phrases in document $d_{i}$. Since $n_{i}^{\varphi}$ follows a Poisson-Binomial distribution, we can plot the probability distribution of $n_{i}^{\varphi}$ normalized by the length of the document, as shown in Figure 1(b). It can be observed that the difference between the normalized expectation $\mathbb{E}\left[n_{i}^{\varphi}\right] / d_{i</p>
<p>This example illustrates why the strategy of taking the average over the whole document can make mistakes, and also provides an important insight. As long as a document has a (potentially small) portion of words or phrases that are highly certain to be orthodox, it should not be considered as an outlier. Based on the above observation, we propose a third outlierness measure.</p>
<p>Orthodox quantile outlierness. We define a quantile-based outlierness definition to rank document outliers. Notice that the distribution of random variable $n_{i}^{\varphi}$ follows a Poisson-Binomial distribution, which is the total number of success trials when one tosses a coin for each word or phrase in the document to determine whether it is orthodox with probability $P\left(\varphi_{i j} \mid \mathbf{x}<em i="i" j="j">{i j}, w</em>\right)$.</p>
<p>Moreover, we define the first $\frac{1}{1-\theta}$-quantile of the Poisson-Binomial distribution of $n_{i}^{\varphi}$ as:</p>
<p>$$
q_{\theta}\left(n_{i}^{\varphi}\right)=\sup <em i="i">{q}\left{q: P\left(n</em> \geq q\right) \geq \theta\right}
$$}^{\varphi</p>
<p>where $0&lt;\theta&lt;1$ is a given parameter close to 1 . Intuitively, it measures the maximum lower bound of $n_{i}^{\varphi}$ we can guarantee with confidence $\theta$.</p>
<p>Based on Equation (3), we can give a formalized definition of our proposed outlierness:</p>
<p>$$
\Omega_{\theta \cdot q}\left(d_{i}\right)=1-\frac{q_{\theta}\left(n_{i}^{\varphi}\right)+1}{\left|d_{i}\right|+1}
$$</p>
<p>where the $\frac{1}{1-\theta}$-quantile is normalized by the document length with a smoothing constant. The cumulative probability distribution of a PoissonBinomial distribution can be efficiently calculated by dynamic programming (Chen and Liu, 1997).</p>
<p>The advantage of the last proposed outlierness measure is that it emphasizes more on the highly orthodox words or phrases and eliminates the noise from a number of relatively uncertain ones.</p>
<h2>4 Experiment Setup</h2>
<h3>4.1 Data Sets</h3>
<p>New York Times News (NYT). We collected 41,959 news article published in 2013 from The New York Times API ${ }^{2}$. Each article is assigned with a unique label indicating in which section the article is published, such as Arts, Travel, Sports, and Health. There are totally 9 section labels in our collected data set. We treat papers in each section as a corpus $D$. Thereby we have a set of corpora $\mathcal{D}=\left{D_{s}\right}$, without overlapping documents. We also have an external news data set $D_{e}$ crawled from Google news, with 51,114 news article published in 2015 without any label information.
ArnetMiner Paper Abstracts (ARNET). We employ abstracts of papers published in the field</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Data set statistics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data set</th>
<th style="text-align: center;">Corpus $D$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">External corpus $D_{e}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">$D$</td>
<td style="text-align: center;">Avg. $</td>
<td style="text-align: center;">d</td>
<td style="text-align: center;">$</td>
</tr>
<tr>
<td style="text-align: center;">NYT</td>
<td style="text-align: center;">4,662.11</td>
<td style="text-align: center;">592.66</td>
<td style="text-align: center;">52,114</td>
<td style="text-align: center;">471.63</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ARNET</td>
<td style="text-align: center;">2,930.60</td>
<td style="text-align: center;">137.21</td>
<td style="text-align: center;">11,463</td>
<td style="text-align: center;">152.17</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>of computer science up to 2013, collected by ArnetMiner (Tang et al., 2008), and assign each paper into a field, according to Wikipedia ${ }^{3}$. We use papers from a set of domains to serve as an external corpus $D_{e}$, while papers in other domains form different corpora $\mathcal{D}=\left{D_{s}\right}$. Each domain (e.g., data mining, computational biology, and computer graphics) forms a corpus $D_{s}$ respectively. Again, notice that the corpora do not have overlapping documents with each other.</p>
<p>A summary is presented in Table 1.
Benchmark generation. Since we do not have true labels for outliers in a corpus, we use injection method to generate outlier detection benchmark. For each data set, we randomly select a corpus $D_{s} \in \mathcal{D}$ and mark all of its document as "normal documents". We then randomly select another corpus $D_{s}^{\prime} \in \mathcal{D}, D_{s}^{\prime} \neq D_{s}$, to inject $\omega$ documents from $D_{s}^{\prime}$ into $D_{s}$ and mark them as outliers. We confine $\omega$ to be a small integer less than $1 \%$ of the size of $\left|D_{s}\right|$. More concretely, $\omega$ is an integer uniformly sampled from $\left(0,0.01\left|D_{s}\right|\right]$.</p>
<p>For each data set, we randomly generate 10 outlier detection benchmarks, and evaluate the overall performance by the average performance on all the benchmarks.</p>
<h3>4.2 Methods Evaluated</h3>
<p>We compare the performances of the following methods.
Cosine similarity based. We characterize each document as a vector, and use the negative average cosine similarity between each document and the corpus as outlierness. We use two different ways to vectorize documents: TF-IDF weighted, and paragraph2vec (Le and Mikolov, 2014). The two methods are denoted as TFIDF-COS and P2VCOS respectively.
KL divergence based. We represent each document as a probability distribution, and the entire corpus as another probability distribution. Then we use the KL-divergence between each document and the entire corpus as the outlierness. We also</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>use two different ways to calculate the probability distribution. The first is to estimate the unigram distribution for each document and the entire corpus respectively, denoted as UNI-KL. The other is to first perform LDA on the entire corpus with 10 topics, and then infer topical allocation distribution of each document and the entire corpus. This method is represented as TM-KL.</p>
<p>Our method Our quantile based method is denoted as VMF-Q. We also provide two baselines derived from our own method as an ablation analysis. One method abandons the quantile based outlierness but use the expected orthodox percentage as Equation (2), denoted as VMF-E. The other method further removes the penalty on lexical general words and phrases, using Equation (1), denoted as VMF-SF.</p>
<h3>4.3 Evaluation Measures</h3>
<p>In most outlier detection applications, people are more concerned with recall. We measure the performance by recall at a certain percentage. More specifically, we compute the recall of outlier detection if the user checks a certain percentage $r$ of the top-ranked documents in the output results. Since in our benchmark generation, the percentage of outliers does not exceed $1 \%$. Therefore, the perfect results for any $r \geq 1 \%$ should be 1.0 .</p>
<p>We choose $r$ to be $1 \%, 2 \%$, and $5 \%$ respectively and evaluate different methods with recall at top- $r$ (percentage). We also report the performance in terms of mean average precision (MAP).</p>
<h3>4.4 Parameter Configurations</h3>
<p>All benchmark data sets are preprocessed as described in Section 2. In the NYT data set we remove words or phrases within top $20 \%$ with respect to document frequency, while in the ARNET data set we remove the top $10 \%$. The document frequency is calculated based on a background corpus $D_{b g}$, which is the same as the external corpus of NYT. Word embedding are trained on the external data set $D_{e}$ using code of Mikolov et al. (Mikolov et al., 2013) with default parameter configurations, where the embedded vector length is set to 200. For paragraph2vec, we learn the length- 100 vectors for each document along with the external data set to guarantee sufficient training data.</p>
<p>For the prior vMF distribution, we set $C_{0}=0.1$, a sufficiently small number so the prior distribu-</p>
<p>Table 2: Performance comparison of different outlier document detection methods. All results are shown as percents.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data set</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">MAP</th>
<th style="text-align: center;">Rcl@1\%</th>
<th style="text-align: center;">Rcl@2\%</th>
<th style="text-align: center;">Rcl@5\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NYT</td>
<td style="text-align: center;">TFIDF-COS</td>
<td style="text-align: center;">05.03</td>
<td style="text-align: center;">04.73</td>
<td style="text-align: center;">06.72</td>
<td style="text-align: center;">14.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P2V-COS</td>
<td style="text-align: center;">22.07</td>
<td style="text-align: center;">23.45</td>
<td style="text-align: center;">44.64</td>
<td style="text-align: center;">66.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UNI-KL</td>
<td style="text-align: center;">10.28</td>
<td style="text-align: center;">11.92</td>
<td style="text-align: center;">16.32</td>
<td style="text-align: center;">31.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TM-KL</td>
<td style="text-align: center;">14.51</td>
<td style="text-align: center;">16.50</td>
<td style="text-align: center;">16.50</td>
<td style="text-align: center;">24.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VMF-SF</td>
<td style="text-align: center;">33.70</td>
<td style="text-align: center;">31.03</td>
<td style="text-align: center;">44.45</td>
<td style="text-align: center;">62.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VMF-E</td>
<td style="text-align: center;">36.57</td>
<td style="text-align: center;">35.91</td>
<td style="text-align: center;">49.41</td>
<td style="text-align: center;">67.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VMF-Q</td>
<td style="text-align: center;">41.88</td>
<td style="text-align: center;">56.99</td>
<td style="text-align: center;">63.29</td>
<td style="text-align: center;">79.23</td>
</tr>
<tr>
<td style="text-align: center;">ARNET</td>
<td style="text-align: center;">TFIDF-COS</td>
<td style="text-align: center;">08.99</td>
<td style="text-align: center;">15.40</td>
<td style="text-align: center;">18.75</td>
<td style="text-align: center;">30.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P2V-COS</td>
<td style="text-align: center;">07.39</td>
<td style="text-align: center;">10.51</td>
<td style="text-align: center;">14.78</td>
<td style="text-align: center;">24.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UNI-KL</td>
<td style="text-align: center;">07.46</td>
<td style="text-align: center;">14.13</td>
<td style="text-align: center;">22.26</td>
<td style="text-align: center;">39.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TM-KL</td>
<td style="text-align: center;">10.09</td>
<td style="text-align: center;">12.04</td>
<td style="text-align: center;">15.37</td>
<td style="text-align: center;">20.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VMF-SF</td>
<td style="text-align: center;">10.69</td>
<td style="text-align: center;">12.05</td>
<td style="text-align: center;">22.58</td>
<td style="text-align: center;">44.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VMF-E</td>
<td style="text-align: center;">10.51</td>
<td style="text-align: center;">12.67</td>
<td style="text-align: center;">25.92</td>
<td style="text-align: center;">45.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VMF-Q</td>
<td style="text-align: center;">19.74</td>
<td style="text-align: center;">22.40</td>
<td style="text-align: center;">34.40</td>
<td style="text-align: center;">53.87</td>
</tr>
</tbody>
</table>
<p>tion is close to a uniform distribution. $\boldsymbol{\mu}<em 0="0">{0}$ is set as a normalized all-1 vector. We also set $m</em>$ 's (i.e. $\eta=50$ ). The number of vMF distributions $T$ is set to 20 in the NYT data set and 10 in the ARNET data set respectively, due to the smaller sizes of corpora in the ARNET data set.}=$ $\log (100)$, and $\sigma^{2}=0.01$. The total number for Gibbs sampling is set to be 50 times of the total count of $z_{i j</p>
<p>To determine semantic focuses, we set threshold parameter $\beta=0.55$ for both data sets. The confidence parameter $\theta$ in outlierness calculation is set to 0.95 in both data sets. Our experiments later will show the performance is relatively robust to different configurations of both parameters.</p>
<h2>5 Results</h2>
<p>We present the experimental results in this section.
Performance comparison. Table 2 shows performance of different outlier document detection methods. It can be observed that our method outperforms all the baselines in both data sets. In both data sets, VMF-Q can achieve a $45 \%$ to $135 \%$ increase from baselines in terms of recall by examining the top $1 \%$ outliers. Generally, performances of most methods are lower in the ARNET data set comparing to NYT, potentially because the relatively short document lengths and more technical terminologies in ARNET.</p>
<p>Ablation analysis. Both refinements of the outlierness measure benefits the performance. Specifically, by changing the average based outlierness to quantile based outlierness, the recall@ $1 \%$ can be improved by $50-75 \%$, and the recall@5\% can also be improved by more than $17 \%$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance of outlier document detection with different parameter configurations.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Crowd evaluation to compare different outlier detection methods on two corpora in NYT data set.</p>
<p>Sensitivity studies of parameters. We study if our proposed method is sensitive to the confidence parameter $\theta$ and filtering threshold parameter $\beta$. We compare the performance of VMF-Q by varying each parameter on both data sets. Figure 2(a) and 2(b) show that the performance is not very sensitive to different values of $\theta$, as long as $\theta$ is sufficiently large (close to 1). Figure 2(c) and 2(d) show that the performance is relatively stable when $\beta$ is between 0.5 and 0.7 , but drops a little when $\beta$ is set to larger value.</p>
<p>Human judgments. We compare VMF-Q to VMF-E and P2V-COS respectively by crowdsourcing, without artificially inserting "outliers". We conduct this experiments on two corpora in NYT data sets with topic "Health" and "Art" respectively. To compare two methods, we randomly select pairs of documents $d_{i}$ and $d_{j}$ such that both are ranked as top- $10 \%$ outliers by at least one method, but their orders in the two rankings
disagree. We conduct the experiments on CrowdFlower. Online crowd workers are given $d_{i}$ and $d_{j}$ as well as other documents in the corpus, and are asked to judge which one of $d_{i}$ and $d_{j}$ deviates more from the corpus. For each corpus, we select 200 pairs of documents.</p>
<p>Before taking the questions, each crowd worker needs to go through at least 10 "test questions" which we know the correct answer. These questions are constructed by taking one document from the corpus as $d_{i}$ and another document not from the corpus as $d_{j}$. Therefore, the one not from the corpus should be the answer. A crowd worker needs to achieve no less than $80 \%$ of accuracy to be eligible to work on actual questions, and the accuracy needs to be maintained over $80 \%$ during the work, which is measured by "test questions" hidden in actual questions. Each question is answered by 3 workers. The final answer is determined by majority voting.</p>
<p>Figure 3 presents the results. On both corpora, there are significantly more workers tend to agree with VMF-Q comparing to P2V-COS, with significance level $\alpha=0.05$. This further verifies that our method VMF-Q can achieve better performance than the P2V-COS baseline. On the other hand, on both data sets we can still observe more workers favoring VMF-Q than VMF-E, but the difference is not as large as the difference between VMF-Q and P2V-COS.</p>
<p>Case study. We also conduct a case study to show how our proposed method outperforms other baselines. Table 3 shows two pairs of documents in "Health" corpus of NYT data set. The left two columns show some comparing methods and their higher ranked outlier documents. The row of "Crowds" shows the outlier document chosen by human workers from the crowdsourcing platform, with a consensus of opinions from multiple workers.</p>
<p>In the first document pair, document A is about gun control policy and is substantially irrelevant to "Health" topic, while document B is about lung infection cases. Document A is a significant outlier, and VMF-Q and VMF-E also agree with our intuition. However, paragraph2vec (P2V) ranks document B higher, probably because it tries to summarize the entire document.</p>
<p>In the second document pair, document B is clearly not an outlier as the story is about a new book of AIDS. In comparison, document A dis-</p>
<p>Table 3: Case study of documents in "Health" corpus of NYT data set. We present several pairs of documents and how different methods rank the pair. The "Outlier" column indicates the document ranked higher in the outlier document ranking generated by the corresponding methods, and the row "Crowds" shows the ranking given by human evaluators.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Outlier</th>
<th style="text-align: center;">Document A</th>
<th style="text-align: center;">Document B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">P2V-COS</td>
<td style="text-align: center;">Doc B</td>
<td style="text-align: center;">CHICAGO (AP) States with the most gun con-</td>
<td style="text-align: center;">A prominent Scottish bagpiping school has</td>
</tr>
<tr>
<td style="text-align: center;">VMF-E</td>
<td style="text-align: center;">Doc A</td>
<td style="text-align: center;">trol laws have the fewest gun-related deaths, ac-</td>
<td style="text-align: center;">warned pipers around to world to clean their</td>
</tr>
<tr>
<td style="text-align: center;">VMF-Q</td>
<td style="text-align: center;">Doc A</td>
<td style="text-align: center;">cording to a study that suggests sheer quantity</td>
<td style="text-align: center;">instruments regularly after one of its longtime</td>
</tr>
<tr>
<td style="text-align: center;">Crowds</td>
<td style="text-align: center;">Doc A</td>
<td style="text-align: center;">of measures might make a difference ...</td>
<td style="text-align: center;">members nearly died of a lung infection ...</td>
</tr>
<tr>
<td style="text-align: center;">P2V-COS</td>
<td style="text-align: center;">Doc B</td>
<td style="text-align: center;">ATLANTA There's more evidence that U.S.</td>
<td style="text-align: center;">Young men in a state prison for juveniles and</td>
</tr>
<tr>
<td style="text-align: center;">VMF-E</td>
<td style="text-align: center;">Doc B</td>
<td style="text-align: center;">births may be leveling off after years of de-</td>
<td style="text-align: center;">professors of library science from the Univer-</td>
</tr>
<tr>
<td style="text-align: center;">VMF-Q</td>
<td style="text-align: center;">Doc A</td>
<td style="text-align: center;">cline. The number of babies born last year only</td>
<td style="text-align: center;">sity of South Carolina have joined forces to fight</td>
</tr>
<tr>
<td style="text-align: center;">Crowds</td>
<td style="text-align: center;">Doc A</td>
<td style="text-align: center;">slipped a little, ...</td>
<td style="text-align: center;">AIDS with a graphic novel ...</td>
</tr>
</tbody>
</table>
<p>cussing U.S. population is an outlier. However, a great part of document B is about the content of the book, which confuses baselines P2V and VMFE, as both methods tend to summarize the entire document and highly relevant words like "AIDS" are overwhelmed by the majority of the document. The only method that agrees with human annotators is VMF-Q.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we propose a novel task of detecting document outliers from a given corpus. We propose a generative model to identify semantic focuses of a corpus, each represented as a vMF distribution in the embedded space. We also design a document outlierness measure. We experimentally verify the effectiveness of our methods. We hope this work provides insights for further studies on outlier document texts in specific domains, and in more challenging settings such as detecting outliers from crowdsourced data.</p>
<h2>References</h2>
<p>Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, and Samuel Gershman. 2016. Nonparametric spherical topic modeling with word embeddings. In $A C L$.</p>
<p>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2001. Latent dirichlet allocation. In NIPS, pages 601-608.</p>
<p>Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. ACM Computing Surveys, 41(3):15:1-15:58.</p>
<p>Sean X Chen and Jun S Liu. 1997. Statistical applications of the poisson-binomial and conditional bernoulli distributions. Statistica Sinica, pages 875892.</p>
<p>Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015. Gaussian LDA for topic models with word embeddings. In $A C L$, pages 795-804.</p>
<p>Siddharth Gopal and Yiming Yang. 2014. Von misesfisher clustering models. In ICML, pages 154-162.</p>
<p>David Guthrie. 2008. Unsupervised Detection of Anomalous Text. Ph.D. thesis, University of Sheffield.</p>
<p>Milos Hauskrecht, Iyad Batal, Michal Valko, Shyam Visweswaran, Gregory F Cooper, and Gilles Clermont. 2013. Outlier detection for patient monitoring and alerting. Journal of Biomedical Informatics, 46(1):47-55.</p>
<p>Victoria J Hodge and Jim Austin. 2004. A survey of outlier detection methodologies. Artificial Intelligence Review, 22(2):85-126.
S. P. Kasiviswanathan, G. Cong, P. Melville, and R. D. Lawrence. 2013. Novel document detection for massive data streams using distributed dictionary learning. IBM J. Res. Dev., 57(3-4):1:9-1:9.</p>
<p>Shiva P Kasiviswanathan, Huahua Wang, Arindam Banerjee, and Prem Melville. 2012. Online 11dictionary learning with application to novel document detection. In NIPS, pages 2258-2266.</p>
<p>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In ICML, pages 1188-1196.</p>
<p>Jialu Liu, Jingbo Shang, Chi Wang, Xiang Ren, and Jiawei Han. 2015. Mining quality phrases from massive text corpora. In SIGMOD, pages 1729-1744. ACM.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111-3119.</p>
<p>Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In $K D D$, pages 990-998. ACM.</p>
<p>Jian Zhang, Zoubin Ghahramani, and Yiming Yang. 2004. A probabilistic model for online document clustering with application to novelty detection. In NIPS, pages 1617-1624.</p>
<p>Yi Zhang, Jamie Callan, and Thomas Minka. 2002. Novelty and redundancy detection in adaptive filtering. In SIGIR, pages 81-88. ACM.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ http://developer.nytimes.com/docs&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://en.wikipedia.org/wiki/List_ of_computer_science_conferences&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8311 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8311</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8311</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-266999728</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.06805v2.pdf" target="_blank">Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8311.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8311.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits multi-step intermediate reasoning in LLMs by asking the model to generate explicit reasoning chains before the final answer; used to improve complex multi-step problem solving via in-context exemplars or rationale generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs / MLLMs (e.g., GPT-3, GPT-4, other transformer LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language models that can be prompted with exemplars or rationale-style prompts to produce intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'least-to-most / stepwise decomposition (variants mentioned)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT: prompt the model to produce intermediate natural-language reasoning steps (rationales) that lead to the final answer; variants like least-to-most prompting order subproblems to simplify multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey notes CoT and its variants are widely used as a single-method prompting strategy and also combined with other methods (e.g., tool use, multi-model composition) but does not report direct ablations comparing CoT-only vs multi-method ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Applied broadly (math, commonsense, VQA, multimodal benchmarks) — referenced across language-only and multimodal reasoning tasks (e.g., GSM8K-like math tasks, visual QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No single-number ablation in this survey; cited as a key method that improves LLM reasoning in prior works; used in systems (e.g., VISPROG, TaskMatrix) but no direct numeric comparison of CoT-only vs diverse-methods reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT helps LLMs perform multi-step reasoning and is often used when decomposing complex tasks; it's widely adopted in tool-using pipelines and multimodal compositions to structure planning and reasoning chains, but the survey notes a lack of systematic ablations addressing whether diverse reasoning methods outperform CoT alone in MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-thought is an important and widely used prompting technique for eliciting multi-step reasoning in both LLMs and MLLMs, frequently used alone or as part of multi-component systems, but the paper finds no systematic comparison specifically testing single-method (CoT) vs diverse-method ensembles in MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8311.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8311.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search-based reasoning framework that expands multiple reasoning trajectories (branches) and performs deliberative search over thought trees to find better solutions than single linear chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general, e.g., GPT-family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs used as a generative engine to expand nodes (thoughts) and evaluate branches in a tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['tree search over generated intermediate thoughts']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate multiple candidate 'thought' continuations from an LLM and search/evaluate these branches (instead of a single chain) to improve solution quality for hard reasoning problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey mentions ToT as an example of using multiple parallel reasoning trajectories (diverse reasoning) compared to single-chain methods, but the survey itself does not run ablations comparing ToT vs single-chain approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General reasoning tasks that benefit from exploring multiple solution trajectories (cited as part of tool-use and planning paradigms).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No direct numeric results in this survey; ToT cited as a method used in literature to improve problem solving compared to single-chain CoT in referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Tree-based deliberation allows exploration of multiple hypotheses and can avoid failure modes of greedy single-chain generation; survey highlights ToT as part of the broader family of multi-trajectory methods but notes lack of multimodal-specific ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>ToT represents a clear example of using diverse reasoning trajectories (multiple parallel thought branches) and is recognized as promising for hard problems; however, the survey reports no direct multimodal experimental comparison between ToT and single-chain strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8311.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8311.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program of Thoughts (PoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting style that separates computation from reasoning by using program-like (executable) steps generated by LLMs to improve numerical and algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (e.g., code-capable models / LLMs that can generate programs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs prompted to output program fragments or verified computation steps that can be executed or checked to ensure correctness of numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['programmatic decomposition', 'hybrid generate-and-execute pipelines']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models generate program-like steps or code that perform computation; the output can be executed or used to verify intermediate arithmetic to reduce symbolic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey notes PoT as one approach among CoT/ToT families that disentangles computation and reasoning; no within-paper ablation comparing PoT vs other methods in multimodal context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Numerical reasoning and math-heavy tasks (language-only and visual-math datasets like MathVista referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey references prior works showing PoT improves numerical accuracy, but provides no new numeric ablation results in multimodal settings.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>PoT can reduce arithmetic failure by delegating computation to programmatic/executable steps; useful when combining visual extraction (from images) with numerical solving stages in multimodal pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>PoT is valuable for disentangling computation from reasoning and is often combined with other system components (tool execution, code interpreters) though the survey finds no systematic multimodal ablation comparing it to purely linguistic CoT approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8311.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8311.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that interleaves reasoning traces and external actions (tool calls) so the model both reasons in natural language and acts (invokes tools) to solve tasks requiring external computation or perception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs / MLLMs (e.g., GPT-family used in tool-augmented pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive LLMs augmented with the ability to emit structured 'action' tokens (tool calls) and natural-language reasoning steps in the same trace.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['interleaved reasoning & tool actions', 'planner-actor loops']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model produces alternating reasoning text (thoughts) and actions (tool calls); actions are executed and results are fed back, enabling iterative problem solving combining internal reasoning and external computation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey references MM-ReAct (multimodal variant) and Visual ChatGPT which use ReAct-style planning with multiple visual tools; no systematic ablation directly comparing ReAct vs single-method reasoning in MLLMs in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multimodal tasks requiring tool calls (image editing, OCR, visual math, multimodal planning) and benchmarks like MM-Vet where tool-augmented systems are competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey notes GPT-4 driven MM-ReAct performs very well on MM-Vet-type reasoning tasks (GPT-4 and GPT-4-driven systems top leaderboards), but gives no controlled numerical comparison isolating ReAct's contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Interleaving actions and reasoning enables handling tasks that require external modules (OCR, search, editing); survey emphasizes ReAct's utility for complex multimodal pipelines but notes scarcity of ablations isolating reasoning-diversity benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>ReAct-style interleaving of reasoning and action is an effective way to combine LLM reasoning with external tools/models in multimodal contexts, enabling diverse-method problem solving though dedicated comparisons versus single-method prompting are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8311.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8311.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool Use (Toolformer/TALM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-augmented reasoning (Toolformer, TALM, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that teach or prompt LLMs to call external tools (calculators, search, APIs) to augment internal reasoning and improve task accuracy, including self-supervised methods that generate tool-usage data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (e.g., GPT-family, Codex, LLaMA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained transformer LMs fine-tuned or prompted to produce API/tool calls as part of their outputs; tools include calculators, search engines, QA systems, and code interpreters.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['tool selection and sequential tool usage', 'self-supervised tool learning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models are provided descriptions/demonstrations of tools and learn to select and call them (zero/few-shot or via fine-tuning); some approaches (Toolformer, TALM) use self-supervision to generate tool-usage training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey notes decomposition of complex tasks into subtasks interacting with tools in sequential or tree structures (CoT, ToT, PoT architectures used alongside tools); no explicit ablation in this survey that compares tool-augmented diverse-method pipelines vs single-method internal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Math, QA, web-assisted tasks, multimodal problems needing external computation/search. Cited datasets: GSM8K style math, BigBench tasks, MM-Vet (tool-augmented systems used).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Referenced works report improved zero-shot and few-shot performance when using tools (Toolformer, TALM); survey includes leaderboard notes that tool-augmented GPT-4 / GPT-driven systems excel on MM-Vet and other benchmarks but does not provide controlled single-vs-multi-method numeric ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Tool use reduces hallucination and computation errors and enables solving tasks beyond model's internal capacity; survey highlights tool use as a primary way to diversify reasoning methods (external computation + internal reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Tool-augmented reasoning is a central approach to expanding LLM/MLLM problem-solving abilities; it inherently introduces diverse reasoning steps (internal chain-of-thought + external tools), but the survey finds limited direct comparisons isolating the benefit of reasoning-method diversity vs single-method setups in multimodal evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8311.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8311.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal Prompting Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal prompting by model interaction (Socratic Models / Chameleon / multimodal prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that combine distinct foundation models (vision, language, other experts) through language-level prompting and interaction to perform multimodal reasoning without joint fine-tuning of a single monolithic model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Socratic models: Composing zero-shot multimodal reasoning with language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Compositions of foundation models (VLMs + LLMs, e.g., CLIP, GPT-3/4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Separate pretrained modality-specific models are coordinated via text prompts and pipelines so the LLM mediates exchange of information (textual interface) between vision/audio/code modules.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['language-mediated model composition', 'multimodal prompting and planner-driven tool composition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Use an LLM as a coordinator/planner that issues queries to vision or other modules and integrates responses; the LLM's reasoning guides which models/tools to call and how to combine outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey describes systems (Socratic Models, Chameleon) that explicitly compose many modules to solve tasks; no systematic ablation in this survey comparing a single integrated MLLM vs a composed multi-model pipeline in terms of reasoning-method diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Embodied AI tasks, multimodal question answering, image-based problem solving, zero-shot multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey cites examples where composed-model pipelines achieve zero-shot capabilities and solve tasks without additional training; no controlled numeric comparison of diversity vs single-model approaches provided.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Composing models via language prompts enables leveraging specialized experts and achieves flexible multimodal reasoning; survey notes this as an effective form of diverse-method reasoning but highlights few formal ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Model composition via language (multimodal prompting by interaction) is a practical way to deploy diverse reasoning capabilities by leveraging specialized modules; the paper calls attention to the need for more systematic comparisons between composed (diverse) vs monolithic (similar) reasoning setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8311.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8311.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Tuning (IFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Fine-Tuning for MLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning pretrained MLLMs/LLMs on instruction-response datasets (possibly multimodal) to improve in-context learning (ICL), instruction following, and multimodal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training language models to follow instructions with human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-7B (example), general MLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained LLMs/MLLMs fine-tuned on curated instruction datasets (⟨instruction, response⟩ or ⟨X-modality, instruction, response⟩) to improve alignment and reasoning in multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['in-context learning (ICL)', 'instruction-following prompts', 'ICL exemplars (multimodal)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Instruction tuning optimizes response generation conditioned on instructions and often includes multimodal examples and in-context exemplars to strengthen analogical and stepwise reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey provides an explicit pre/post instruction-tuning comparison for Qwen-VL-7B (Table 6) showing performance changes on InfiMM-Eval across reasoning categories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>InfiMM-Eval (complex open-ended multimodal reasoning benchmark) — Table 6 comparison: Qwen-VL-7B (pre-IFT) vs Qwen-VL-7B-Chat (post-IFT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 6 (survey): Qwen-VL-7B (pre-IFT) overall: 21.32; Qwen-VL-7B-Chat (post-IFT) overall: 33.44. Category breakdowns (pre -> post): Deductive 24.06 -> 32.25; Abductive 24.39 -> 48.18; Analogical 9.03 -> 24.03; Moderate complexity 32.33 -> 46.90; High complexity 12.69 -> 22.78.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Instruction tuning substantially improves multimodal reasoning across categories on InfiMM-Eval, indicating that training on instruction-format multimodal data increases the model's capacity to use richer reasoning (both single-method CoT-style and more complex multi-step strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The paper presents direct evidence (Qwen-VL pre/post IFT) that instruction fine-tuning significantly enhances multimodal reasoning performance; while this is not a direct 'diverse vs similar reasoning methods' ablation, it shows that a change in training (which can enable broader reasoning behaviors and ICL) materially affects reasoning outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8311.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8311.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InfiMM-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InfiMM-Eval: Complex open-ended reasoning evaluation for multi-modal LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-ended multimodal benchmark designed to evaluate visual reasoning capability including intermediate reasoning steps, with GPT-based evaluation used for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple MLLMs (survey reports many evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmark dataset (279 high-quality samples with manual collection and GPT evaluation) intended to probe deductive, abductive, analogical, and varying reasoning complexity levels in MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['evaluation of CoT-style outputs, multimodal instruction-following, tool-augmented pipelines (as used by submitted systems)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>InfiMM-Eval requires open-ended answers often necessitating intermediate reasoning steps; models are evaluated on their full reasoning chain correctness using GPT-4 evaluation and QA accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (benchmark evaluates models that use either single or multiple reasoning techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey compiles model leaderboard (Table 5) showing many models and their category-level scores; not a controlled ablation but allows cross-model comparison of systems that adopt different reasoning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>InfiMM-Eval (open-ended multimodal reasoning with step annotations; categories: Deductive, Abductive, Analogical, Moderate/High complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 5 (survey) selected results (Overall score shown): GPT-4V overall 74.44; SPHINX-v2 overall 39.48; Qwen-VL-Chat overall 37.39; CogVLM-Chat overall 37.16; InfiMM-LLaMA-13B overall 40.7; LLaVA-1.5 overall 32.62; InstructBLIP overall 28.02. Category-level scores are listed per model in Table 5 (see survey).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4V substantially outperforms open-source models; top open-source models vary by reasoning category indicating architecture/data/training differences (e.g., Qwen-VL-Chat stronger on highly complex questions while CogVLM-Chat stronger on moderate complexity), suggesting that differing training/architectural choices (which enable different reasoning behaviors) affect performance across reasoning types.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>InfiMM-Eval reveals substantial gaps between GPT-4V and open-source MLLMs and shows that instruction tuning and multi-stage training recipes materially influence multimodal reasoning ability; however, the benchmark itself is not an ablation study specifically isolating diverse vs similar reasoning-method advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8311.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8311.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MM-ReAct / VISPROG / Chameleon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Examples of multimodal tool-composition systems (MM-ReAct, VISPROG, Chameleon)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative systems that use LLMs/MLLMs as planners to orchestrate multiple specialized vision/audio/code tools (diverse reasoning and action modules) for complex multimodal tasks like image editing, compositional VQA, and web/multi-tool tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual programming: Compositional visual reasoning without training.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Composed systems using LLMs (e.g., GPT-3/4) + multiple vision/audio modules</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipelines where an LLM generates a program or plan coordinating multiple models/tools (detection, segmentation, diffusion, OCR, search) to perform complex multimodal operations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['program synthesis via LLM planners', 'chain-of-thought planning combined with tool invocation (ReAct-style)', 'modular neuro-symbolic composition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM acts as a planner that either synthesizes a program (VISPROG) or interleaves reasoning and tool actions (MM-ReAct, Chameleon), coordinating multiple specialized modules to solve complex tasks that single models cannot handle directly.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey catalogs these systems as multi-tool diverse-method pipelines but does not present controlled experiments comparing them to single-method monolithic MLLMs; notes strong task performance when using diverse toolsets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Compositional visual reasoning, image editing, visual dialogue, multimodal QA; evaluated qualitatively and on task-specific benchmarks referenced by the individual systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey cites that tool-composition systems can handle complicated tasks (examples include image editing pipelines, compositional VQA) and that multimodal ReAct achieves strong performance on visual QA and planning tasks; no uniform numeric ablation comparing diversity vs similarity provided.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Composing diverse specialized tools via LLM planning yields flexible, interpretable, and modular solutions for compositional multimodal tasks; survey emphasizes practicality but calls for systematic evaluation of trade-offs (robustness, latency, hallucination sources).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The survey highlights multimodal tool-composition approaches as effective examples of diverse reasoning methods in practice, but notes the literature lacks systematic ablation studies that directly compare these diverse pipelines against single-method monolithic MLLMs on equal footing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 2)</em></li>
                <li>Tool augmented language models. <em>(Rating: 1)</em></li>
                <li>Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models. <em>(Rating: 2)</em></li>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8311",
    "paper_id": "paper-266999728",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that elicits multi-step intermediate reasoning in LLMs by asking the model to generate explicit reasoning chains before the final answer; used to improve complex multi-step problem solving via in-context exemplars or rationale generation.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs / MLLMs (e.g., GPT-3, GPT-4, other transformer LLMs)",
            "model_description": "Autoregressive transformer language models that can be prompted with exemplars or rationale-style prompts to produce intermediate reasoning steps.",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "least-to-most / stepwise decomposition (variants mentioned)"
            ],
            "reasoning_methods_description": "CoT: prompt the model to produce intermediate natural-language reasoning steps (rationales) that lead to the final answer; variants like least-to-most prompting order subproblems to simplify multi-step tasks.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey notes CoT and its variants are widely used as a single-method prompting strategy and also combined with other methods (e.g., tool use, multi-model composition) but does not report direct ablations comparing CoT-only vs multi-method ensembles.",
            "task_or_benchmark": "Applied broadly (math, commonsense, VQA, multimodal benchmarks) — referenced across language-only and multimodal reasoning tasks (e.g., GSM8K-like math tasks, visual QA).",
            "performance_results": "No single-number ablation in this survey; cited as a key method that improves LLM reasoning in prior works; used in systems (e.g., VISPROG, TaskMatrix) but no direct numeric comparison of CoT-only vs diverse-methods reported here.",
            "qualitative_findings": "CoT helps LLMs perform multi-step reasoning and is often used when decomposing complex tasks; it's widely adopted in tool-using pipelines and multimodal compositions to structure planning and reasoning chains, but the survey notes a lack of systematic ablations addressing whether diverse reasoning methods outperform CoT alone in MLLMs.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Chain-of-thought is an important and widely used prompting technique for eliciting multi-step reasoning in both LLMs and MLLMs, frequently used alone or as part of multi-component systems, but the paper finds no systematic comparison specifically testing single-method (CoT) vs diverse-method ensembles in MLLMs.",
            "uuid": "e8311.0",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree of Thoughts",
            "brief_description": "A search-based reasoning framework that expands multiple reasoning trajectories (branches) and performs deliberative search over thought trees to find better solutions than single linear chains.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (general, e.g., GPT-family)",
            "model_description": "Transformer LLMs used as a generative engine to expand nodes (thoughts) and evaluate branches in a tree search.",
            "reasoning_methods": [
                "tree search over generated intermediate thoughts"
            ],
            "reasoning_methods_description": "Generate multiple candidate 'thought' continuations from an LLM and search/evaluate these branches (instead of a single chain) to improve solution quality for hard reasoning problems.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey mentions ToT as an example of using multiple parallel reasoning trajectories (diverse reasoning) compared to single-chain methods, but the survey itself does not run ablations comparing ToT vs single-chain approaches.",
            "task_or_benchmark": "General reasoning tasks that benefit from exploring multiple solution trajectories (cited as part of tool-use and planning paradigms).",
            "performance_results": "No direct numeric results in this survey; ToT cited as a method used in literature to improve problem solving compared to single-chain CoT in referenced works.",
            "qualitative_findings": "Tree-based deliberation allows exploration of multiple hypotheses and can avoid failure modes of greedy single-chain generation; survey highlights ToT as part of the broader family of multi-trajectory methods but notes lack of multimodal-specific ablations.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "ToT represents a clear example of using diverse reasoning trajectories (multiple parallel thought branches) and is recognized as promising for hard problems; however, the survey reports no direct multimodal experimental comparison between ToT and single-chain strategies.",
            "uuid": "e8311.1",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PoT",
            "name_full": "Program of Thoughts (PoT)",
            "brief_description": "A prompting style that separates computation from reasoning by using program-like (executable) steps generated by LLMs to improve numerical and algorithmic reasoning.",
            "citation_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "mention_or_use": "mention",
            "model_name": "LLMs (e.g., code-capable models / LLMs that can generate programs)",
            "model_description": "Transformer LLMs prompted to output program fragments or verified computation steps that can be executed or checked to ensure correctness of numerical reasoning.",
            "reasoning_methods": [
                "programmatic decomposition",
                "hybrid generate-and-execute pipelines"
            ],
            "reasoning_methods_description": "Models generate program-like steps or code that perform computation; the output can be executed or used to verify intermediate arithmetic to reduce symbolic errors.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey notes PoT as one approach among CoT/ToT families that disentangles computation and reasoning; no within-paper ablation comparing PoT vs other methods in multimodal context.",
            "task_or_benchmark": "Numerical reasoning and math-heavy tasks (language-only and visual-math datasets like MathVista referenced).",
            "performance_results": "Survey references prior works showing PoT improves numerical accuracy, but provides no new numeric ablation results in multimodal settings.",
            "qualitative_findings": "PoT can reduce arithmetic failure by delegating computation to programmatic/executable steps; useful when combining visual extraction (from images) with numerical solving stages in multimodal pipelines.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "PoT is valuable for disentangling computation from reasoning and is often combined with other system components (tool execution, code interpreters) though the survey finds no systematic multimodal ablation comparing it to purely linguistic CoT approaches.",
            "uuid": "e8311.2",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting)",
            "brief_description": "A framework that interleaves reasoning traces and external actions (tool calls) so the model both reasons in natural language and acts (invokes tools) to solve tasks requiring external computation or perception.",
            "citation_title": "React: Synergizing reasoning and acting in language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs / MLLMs (e.g., GPT-family used in tool-augmented pipelines)",
            "model_description": "Autoregressive LLMs augmented with the ability to emit structured 'action' tokens (tool calls) and natural-language reasoning steps in the same trace.",
            "reasoning_methods": [
                "interleaved reasoning & tool actions",
                "planner-actor loops"
            ],
            "reasoning_methods_description": "The model produces alternating reasoning text (thoughts) and actions (tool calls); actions are executed and results are fed back, enabling iterative problem solving combining internal reasoning and external computation.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey references MM-ReAct (multimodal variant) and Visual ChatGPT which use ReAct-style planning with multiple visual tools; no systematic ablation directly comparing ReAct vs single-method reasoning in MLLMs in this survey.",
            "task_or_benchmark": "Multimodal tasks requiring tool calls (image editing, OCR, visual math, multimodal planning) and benchmarks like MM-Vet where tool-augmented systems are competitive.",
            "performance_results": "Survey notes GPT-4 driven MM-ReAct performs very well on MM-Vet-type reasoning tasks (GPT-4 and GPT-4-driven systems top leaderboards), but gives no controlled numerical comparison isolating ReAct's contribution.",
            "qualitative_findings": "Interleaving actions and reasoning enables handling tasks that require external modules (OCR, search, editing); survey emphasizes ReAct's utility for complex multimodal pipelines but notes scarcity of ablations isolating reasoning-diversity benefits.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "ReAct-style interleaving of reasoning and action is an effective way to combine LLM reasoning with external tools/models in multimodal contexts, enabling diverse-method problem solving though dedicated comparisons versus single-method prompting are not provided in the paper.",
            "uuid": "e8311.3",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Tool Use (Toolformer/TALM)",
            "name_full": "Tool-augmented reasoning (Toolformer, TALM, etc.)",
            "brief_description": "Approaches that teach or prompt LLMs to call external tools (calculators, search, APIs) to augment internal reasoning and improve task accuracy, including self-supervised methods that generate tool-usage data.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools.",
            "mention_or_use": "mention",
            "model_name": "LLMs (e.g., GPT-family, Codex, LLaMA variants)",
            "model_description": "Large pretrained transformer LMs fine-tuned or prompted to produce API/tool calls as part of their outputs; tools include calculators, search engines, QA systems, and code interpreters.",
            "reasoning_methods": [
                "tool selection and sequential tool usage",
                "self-supervised tool learning"
            ],
            "reasoning_methods_description": "Models are provided descriptions/demonstrations of tools and learn to select and call them (zero/few-shot or via fine-tuning); some approaches (Toolformer, TALM) use self-supervision to generate tool-usage training examples.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey notes decomposition of complex tasks into subtasks interacting with tools in sequential or tree structures (CoT, ToT, PoT architectures used alongside tools); no explicit ablation in this survey that compares tool-augmented diverse-method pipelines vs single-method internal reasoning.",
            "task_or_benchmark": "Math, QA, web-assisted tasks, multimodal problems needing external computation/search. Cited datasets: GSM8K style math, BigBench tasks, MM-Vet (tool-augmented systems used).",
            "performance_results": "Referenced works report improved zero-shot and few-shot performance when using tools (Toolformer, TALM); survey includes leaderboard notes that tool-augmented GPT-4 / GPT-driven systems excel on MM-Vet and other benchmarks but does not provide controlled single-vs-multi-method numeric ablations.",
            "qualitative_findings": "Tool use reduces hallucination and computation errors and enables solving tasks beyond model's internal capacity; survey highlights tool use as a primary way to diversify reasoning methods (external computation + internal reasoning).",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Tool-augmented reasoning is a central approach to expanding LLM/MLLM problem-solving abilities; it inherently introduces diverse reasoning steps (internal chain-of-thought + external tools), but the survey finds limited direct comparisons isolating the benefit of reasoning-method diversity vs single-method setups in multimodal evaluations.",
            "uuid": "e8311.4",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Multimodal Prompting Interaction",
            "name_full": "Multimodal prompting by model interaction (Socratic Models / Chameleon / multimodal prompting)",
            "brief_description": "Methods that combine distinct foundation models (vision, language, other experts) through language-level prompting and interaction to perform multimodal reasoning without joint fine-tuning of a single monolithic model.",
            "citation_title": "Socratic models: Composing zero-shot multimodal reasoning with language.",
            "mention_or_use": "mention",
            "model_name": "Compositions of foundation models (VLMs + LLMs, e.g., CLIP, GPT-3/4)",
            "model_description": "Separate pretrained modality-specific models are coordinated via text prompts and pipelines so the LLM mediates exchange of information (textual interface) between vision/audio/code modules.",
            "reasoning_methods": [
                "language-mediated model composition",
                "multimodal prompting and planner-driven tool composition"
            ],
            "reasoning_methods_description": "Use an LLM as a coordinator/planner that issues queries to vision or other modules and integrates responses; the LLM's reasoning guides which models/tools to call and how to combine outputs.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey describes systems (Socratic Models, Chameleon) that explicitly compose many modules to solve tasks; no systematic ablation in this survey comparing a single integrated MLLM vs a composed multi-model pipeline in terms of reasoning-method diversity.",
            "task_or_benchmark": "Embodied AI tasks, multimodal question answering, image-based problem solving, zero-shot multimodal tasks.",
            "performance_results": "Survey cites examples where composed-model pipelines achieve zero-shot capabilities and solve tasks without additional training; no controlled numeric comparison of diversity vs single-model approaches provided.",
            "qualitative_findings": "Composing models via language prompts enables leveraging specialized experts and achieves flexible multimodal reasoning; survey notes this as an effective form of diverse-method reasoning but highlights few formal ablation studies.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Model composition via language (multimodal prompting by interaction) is a practical way to deploy diverse reasoning capabilities by leveraging specialized modules; the paper calls attention to the need for more systematic comparisons between composed (diverse) vs monolithic (similar) reasoning setups.",
            "uuid": "e8311.5",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Instruction Tuning (IFT)",
            "name_full": "Instruction Fine-Tuning for MLLMs",
            "brief_description": "Fine-tuning pretrained MLLMs/LLMs on instruction-response datasets (possibly multimodal) to improve in-context learning (ICL), instruction following, and multimodal reasoning.",
            "citation_title": "Training language models to follow instructions with human feedback.",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-7B (example), general MLLMs",
            "model_description": "Pretrained LLMs/MLLMs fine-tuned on curated instruction datasets (⟨instruction, response⟩ or ⟨X-modality, instruction, response⟩) to improve alignment and reasoning in multi-step tasks.",
            "reasoning_methods": [
                "in-context learning (ICL)",
                "instruction-following prompts",
                "ICL exemplars (multimodal)"
            ],
            "reasoning_methods_description": "Instruction tuning optimizes response generation conditioned on instructions and often includes multimodal examples and in-context exemplars to strengthen analogical and stepwise reasoning capabilities.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey provides an explicit pre/post instruction-tuning comparison for Qwen-VL-7B (Table 6) showing performance changes on InfiMM-Eval across reasoning categories.",
            "task_or_benchmark": "InfiMM-Eval (complex open-ended multimodal reasoning benchmark) — Table 6 comparison: Qwen-VL-7B (pre-IFT) vs Qwen-VL-7B-Chat (post-IFT).",
            "performance_results": "Table 6 (survey): Qwen-VL-7B (pre-IFT) overall: 21.32; Qwen-VL-7B-Chat (post-IFT) overall: 33.44. Category breakdowns (pre -&gt; post): Deductive 24.06 -&gt; 32.25; Abductive 24.39 -&gt; 48.18; Analogical 9.03 -&gt; 24.03; Moderate complexity 32.33 -&gt; 46.90; High complexity 12.69 -&gt; 22.78.",
            "qualitative_findings": "Instruction tuning substantially improves multimodal reasoning across categories on InfiMM-Eval, indicating that training on instruction-format multimodal data increases the model's capacity to use richer reasoning (both single-method CoT-style and more complex multi-step strategies).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "The paper presents direct evidence (Qwen-VL pre/post IFT) that instruction fine-tuning significantly enhances multimodal reasoning performance; while this is not a direct 'diverse vs similar reasoning methods' ablation, it shows that a change in training (which can enable broader reasoning behaviors and ICL) materially affects reasoning outcomes.",
            "uuid": "e8311.6",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "InfiMM-Eval",
            "name_full": "InfiMM-Eval: Complex open-ended reasoning evaluation for multi-modal LLMs",
            "brief_description": "An open-ended multimodal benchmark designed to evaluate visual reasoning capability including intermediate reasoning steps, with GPT-based evaluation used for scoring.",
            "citation_title": "Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models.",
            "mention_or_use": "use",
            "model_name": "Multiple MLLMs (survey reports many evaluated models)",
            "model_description": "Benchmark dataset (279 high-quality samples with manual collection and GPT evaluation) intended to probe deductive, abductive, analogical, and varying reasoning complexity levels in MLLMs.",
            "reasoning_methods": [
                "evaluation of CoT-style outputs, multimodal instruction-following, tool-augmented pipelines (as used by submitted systems)"
            ],
            "reasoning_methods_description": "InfiMM-Eval requires open-ended answers often necessitating intermediate reasoning steps; models are evaluated on their full reasoning chain correctness using GPT-4 evaluation and QA accuracy.",
            "reasoning_diversity": "both (benchmark evaluates models that use either single or multiple reasoning techniques)",
            "reasoning_diversity_experimental_setup": "Survey compiles model leaderboard (Table 5) showing many models and their category-level scores; not a controlled ablation but allows cross-model comparison of systems that adopt different reasoning strategies.",
            "task_or_benchmark": "InfiMM-Eval (open-ended multimodal reasoning with step annotations; categories: Deductive, Abductive, Analogical, Moderate/High complexity).",
            "performance_results": "Table 5 (survey) selected results (Overall score shown): GPT-4V overall 74.44; SPHINX-v2 overall 39.48; Qwen-VL-Chat overall 37.39; CogVLM-Chat overall 37.16; InfiMM-LLaMA-13B overall 40.7; LLaVA-1.5 overall 32.62; InstructBLIP overall 28.02. Category-level scores are listed per model in Table 5 (see survey).",
            "qualitative_findings": "GPT-4V substantially outperforms open-source models; top open-source models vary by reasoning category indicating architecture/data/training differences (e.g., Qwen-VL-Chat stronger on highly complex questions while CogVLM-Chat stronger on moderate complexity), suggesting that differing training/architectural choices (which enable different reasoning behaviors) affect performance across reasoning types.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "InfiMM-Eval reveals substantial gaps between GPT-4V and open-source MLLMs and shows that instruction tuning and multi-stage training recipes materially influence multimodal reasoning ability; however, the benchmark itself is not an ablation study specifically isolating diverse vs similar reasoning-method advantages.",
            "uuid": "e8311.7",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "MM-ReAct / VISPROG / Chameleon",
            "name_full": "Examples of multimodal tool-composition systems (MM-ReAct, VISPROG, Chameleon)",
            "brief_description": "Representative systems that use LLMs/MLLMs as planners to orchestrate multiple specialized vision/audio/code tools (diverse reasoning and action modules) for complex multimodal tasks like image editing, compositional VQA, and web/multi-tool tasks.",
            "citation_title": "Visual programming: Compositional visual reasoning without training.",
            "mention_or_use": "mention",
            "model_name": "Composed systems using LLMs (e.g., GPT-3/4) + multiple vision/audio modules",
            "model_description": "Pipelines where an LLM generates a program or plan coordinating multiple models/tools (detection, segmentation, diffusion, OCR, search) to perform complex multimodal operations.",
            "reasoning_methods": [
                "program synthesis via LLM planners",
                "chain-of-thought planning combined with tool invocation (ReAct-style)",
                "modular neuro-symbolic composition"
            ],
            "reasoning_methods_description": "LLM acts as a planner that either synthesizes a program (VISPROG) or interleaves reasoning and tool actions (MM-ReAct, Chameleon), coordinating multiple specialized modules to solve complex tasks that single models cannot handle directly.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey catalogs these systems as multi-tool diverse-method pipelines but does not present controlled experiments comparing them to single-method monolithic MLLMs; notes strong task performance when using diverse toolsets.",
            "task_or_benchmark": "Compositional visual reasoning, image editing, visual dialogue, multimodal QA; evaluated qualitatively and on task-specific benchmarks referenced by the individual systems.",
            "performance_results": "Survey cites that tool-composition systems can handle complicated tasks (examples include image editing pipelines, compositional VQA) and that multimodal ReAct achieves strong performance on visual QA and planning tasks; no uniform numeric ablation comparing diversity vs similarity provided.",
            "qualitative_findings": "Composing diverse specialized tools via LLM planning yields flexible, interpretable, and modular solutions for compositional multimodal tasks; survey emphasizes practicality but calls for systematic evaluation of trade-offs (robustness, latency, hallucination sources).",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "The survey highlights multimodal tool-composition approaches as effective examples of diverse reasoning methods in practice, but notes the literature lacks systematic ablation studies that directly compare these diverse pipelines against single-method monolithic MLLMs on equal footing.",
            "uuid": "e8311.8",
            "source_info": {
                "paper_title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Tool augmented language models.",
            "rating": 1,
            "sanitized_title": "tool_augmented_language_models"
        },
        {
            "paper_title": "Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models.",
            "rating": 2,
            "sanitized_title": "infimmeval_complex_openended_reasoning_evaluation_for_multimodal_large_language_models"
        },
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language.",
            "rating": 2,
            "sanitized_title": "socratic_models_composing_zeroshot_multimodal_reasoning_with_language"
        }
    ],
    "cost": 0.0245185,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EXPLORING THE REASONING ABILITIES OF MULTIMODAL LARGE LANGUAGE MODELS (MLLMS): A COMPREHENSIVE SURVEY ON EMERGING TRENDS IN MULTIMODAL REASONING
18 Jan 2024</p>
<p>Yiqi Wang 
Wentao Chen 
Xiaotian Han 
Xudong Lin 
Haiteng Zhao 
Yongfei Liu 
Bohan Zhai 
Jianbo Yuan 
Quanzeng You quanzeng.you@bytedance.com 
Hongxia Yang 
Bytedance Inc 
EXPLORING THE REASONING ABILITIES OF MULTIMODAL LARGE LANGUAGE MODELS (MLLMS): A COMPREHENSIVE SURVEY ON EMERGING TRENDS IN MULTIMODAL REASONING
18 Jan 2024C3AC724489C8EF00F8B07B41E383A72FarXiv:2401.06805v2[cs.CL]Multimodal reasoningMultimodal Large Language ModelInstruction TuningIn-Context Learning
Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI.Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications.Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks.These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs.However, the reasoning abilities of MLLMs have not been systematically investigated.In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoningintensive tasks, and finally discuss current practices and future directions.We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.</p>
<p>Introduction</p>
<p>In the past decade, with the help of increasing computational power and expanded datasets, Multimodal Large Language Models (MLLMs) have achieved remarkable progress in many domains and applications.They are considered to be the most capable family towards the ultimate goal of Strong Artificial Intelligence (Strong AI) [1] or Artificial General Intelligence (AGI) [2].Strong AI is often thought to possess a mind, the question of whether Multimodal Large Language Models have minds, or how one might determine their existence, remains an open and complex topic.One does not need to have Sherlock Holmes' mind to reason over observations naturally in multiple modalities from the real world, such as vision, audio, text, smell, etc., and subsequently take action.In the Dual-system theory [3,4], a widespread theory of cognitive science, the second type of the human cognition systems, which enable abstract reasoning, is considered to be "evolutionarily recent and distinctively human".This characterization prompts an intriguing inquiry: are MLLMs capable of reasoning?Specifically, we are interested in the reasoning tasks that require the comprehension and integration of information from various modalities, including vision, text, audio, and others.MLLMs have demonstrated effectiveness in a variety of multimodal reasoning tasks.Notable examples include Visual Question Answering (VQA) [5,6,7,8], multimodal dialogue [9,10,11], among others.Recently, there has also been extensive research focusing on particularly improving the reasoning abilities of MLLMs, such as multimodal instruction tuning [12,13,14,11] and multimodal reasoning by prompting [15,16].The strong capabilities of MLLMs also intrigue research on embodying them as agents in real-world environment [17,18,19] or enabling them using tools [20,21].Despite the impressive performance on existing benchmark datasets [12,22,13,23,19,24,25], it is still too early to conclude that current MLLMs can truly reason over multiple input modalities.</p>
<p>Even in the text-only setting, Large Language Models (LLMs) are still known to lack proper reasoning abilities in some aspects, such as mathematical [26], and multi-hop reasoning [27,28].Moreover, both LLMs and MLLMs are also known to have hallucination issues [29], which prevent the models from reasoning properly.Motivated by the significant importance of reasoning abilities in MLLMs and the fast-paced development of related research, we believe it is necessary to comprehensively review the current status of the reasoning abilities of MLLMs and provide insightful discussion to illuminate future research.In the following sections, we will initially define the reasoning abilities of interest and illustrate existing evaluation protocols (Sec.2); subsequently, we will summarize the current status of MLLMs (Sec.3); this will be followed by an in-depth look at a key training phase for multimodal reasoning, namely, multimodal instruction tuning (Sec.4); then we will discuss the reasoning-intensive applications of MLLMs, including Embodied AI (Sec.5.1) and tool usage (Sec.5.2); afterward, we will analyze results on multimodal reasoning benchmarks (Sec.6).Finally, we will provide concentrated insights and discussions on the current status and future directions of MLLMs (Sec.7).</p>
<p>2 Reasoning: Definition and Evaluation Protocols</p>
<p>What is Reasoning?</p>
<p>Reasoning is one of the fundamental intelligent behaviors of human beings, which requires understanding and analyzing given conditions and background knowledge to derive a new conclusion logically and rationally [30,31,32].Reasoning has been extensively studied within the field of logic [33,34,35].To obtain a clear understanding of reasoning, we refer to the definitions established in the field of logic [36,37], which defines reasoning based on the concepts of premises, conclusions, and inferences.Reasoning is usually regarded as the integration of these concepts.To be more specific, premises and conclusions are true or false claims about a case.Inferences are the intermediate reasoning steps that select and interpret information from given premises, make connections, verify, and ultimately draw conclusions based on the provided and interpreted data.</p>
<p>Reasoning in the field of logic highly relies on mathematics [35], which is used to construct a set of basic logic rules [38].Accordingly, only reasoning that adheres to these logical rules is considered valid.Apart from logic rules, domain knowledge is also required to perform practical reasoning tasks [39].For instance, arithmetic reasoning necessitates mathematical knowledge, while commonsense knowledge is essential for reasoning in daily life tasks.The domain knowledge serves as additional premises besides the given inputs and is indispensable to obtaining valid conclusions in specific fields.</p>
<p>According to [36], reasoning can be divided into formal and informal reasoning, where conclusions of formal reasoning are guaranteed to be true as long as the premises are true, while informal reasoning does not guarantee the truth of conclusions especially when the available information is incomplete or ambiguous.In usual, informal reasoning is performed with natural language and is essential for daily-life tasks.Additionally, based on the direction of inference, reasoning can be divided into deductive, inductive, abductive, and analogical reasoning:</p>
<p>• Deductive reasoning [40] represents the most classical form of reasoning.Given a set of known knowledge (premises), it infers new knowledge step by step to obtain the conclusion.For example, given the premises that "cats are mammals" and "all mammals have four feet", deductive reasoning can infer a new conclusion that "cats have four feet".Note that deductive reasoning only concerns that the inference steps follow the logic rules and does not impose any restrictions on the truth of premises.Therefore, wrong premises may lead to wrong conclusions even if the reasoning steps are logical.• Inductive reasoning concentrates on inferring general rules from specific observations [41].For example, given the premises (observations) that "any mammal that I have seen so far has four feet", induction reasoning can infer that "all mammals have four feet".Inductive reasoning is an effective tool in scientific areas to discover new principles and laws.Note that since it is hard to collect complete observations, the conclusions of inductive reasoning may be incorrect for some unseen observations.• Abductive reasoning is to infer the best explanation for the given observations [42].It is perceived as the backward direction of deductive reasoning, where multiple reasons can lead to the results (observations) and the most likely reason should be inferred.Consider this scenario: a car is parked on the highway with its hazard lights flashing.Abductive reasoning could lead to the more plausible conclusion that the car is broken down, rather than the less likely explanation of someone playing a prank.Since the number of possible reasons is usually massive, abduction reasoning requires a lot of commonsense and domain knowledge to infer a credible reason.</p>
<p>• Analogical reasoning involves transferring knowledge from one or several instances to another, based on their similarities [43].Two forms of analogical reasoning were studied and applied in real-life activities [44].</p>
<p>The first form takes as inputs one or multiple similar cases, then arrives at a hidden proposition, and finally applies the proposition to a new case.For example, consider the two cases that "iron can conduct electricity" and "copper can conduct electricity".From this, one might infer a proposition that "any metal can conduct electricity", thereby inferring that "silver, being a metal, can also conduct electricity".The second form of analogical reasoning considers the similarity of two entities to infer a property in one based on the property of the other.For instance, given the premises that "exposing plants to adequate sunlight enhances their growth and health" and "humans and plants both require certain environmental factors to thrive, like water, air, and nutrients", one could use analogical reasoning to hypothesize that "humans may also benefit from regular exposure to sunlight for their health and well-being" [45].With analogical reasoning, the properties of a new object can be quickly inferred at a low cost.However, the premises of analogical reasoning can only support the conclusions that are likely rather than definitively true.</p>
<p>In this paper, we focus on the reasoning abilities of Multimodal Large Language Models.The reasoning methods employed by these models fall under the category of informal reasoning.This is primarily because they utilize natural language to articulate the steps and conclusions involved in the reasoning process and they allow a certain degree of inaccuracy in their reasoning mechanisms.This paper primarily focuses on three reasoning types: deductive reasoning, abductive reasoning, and analogical reasoning.These types are highlighted due to their prevalent application in real-world reasoning tasks, particularly within the scope of current MLLMs.</p>
<p>Language-only Reasoning Tasks</p>
<p>To gain a deeper insight into the reasoning abilities of Multimodal Large Language Models, it is crucial to understand the associated reasoning tasks.These tasks are widely regarded as requiring the models' reasoning capabilities for effective resolution.We can divide reasoning tasks into two categories based on the input data: language-only reasoning tasks and multimodal reasoning tasks, with the former requiring no images and the latter involving both images and text.Research on language-only tasks has a longer history, and the methodologies used for task categorization and the insights gained from these studies offer valuable guidance for the development of multimodal reasoning tasks.In this section, we will present four distinct types of language-only reasoning tasks: solving math problems, engaging in commonsense reasoning, tackling symbolic reasoning tasks, and interacting with various environments.</p>
<p>Solving Math Problems</p>
<p>Solving math problems usually requires one-step or multistep arithmetic reasoning.Based on the understanding of input question, the implicit arithmetic operations, and concept knowledge, a solver should infer a sequence of operation steps that can derive final answers.The range of implicit operations and conceptual knowledge can be categorized according to different school grade levels.For example, the benchmarks of GSM8K [46], SVAMP [47], ASDiv [48], and MAWPS [49] require mathematical knowledge typically acquired in primary schools.This includes fundamental operations such as addition, subtraction, multiplication, and division.The MathQA [50] benchmark and the AQuA [51] benchmark include mathematical problems sourced from standardized tests such as the GMAT (Graduate Management Admission Test) and the GRE (General Test).The MATH [52] benchmark features highly challenging mathematical problems, including areas such as permutation and combination problems, geometric series problems, solving high-order equations, among others.This benchmark requires solvers to have a lot of advanced mathematical knowledge and mathematical reasoning techniques, as well as the ability to follow a multi-step procedure, and thus remains a very challenging task.</p>
<p>Engaging in Commonsense Reasoning</p>
<p>Commonsense is a broadly encompassing yet somewhat loosely defined concept.While lacking exact boundaries, this generally refers to knowledge that goes beyond specialized expertise and is expected to be well-known to individuals who have completed basic education.Commonsense knowledge extends across various domains, including social common sense (e.g., understanding that people would feel embarrassed if publicly blamed), physical common sense (e.g., recognizing that a car is faster than a bicycle), biological common sense (e.g., knowing that penguins and koalas do not naturally encounter each other), and numerous other areas.It is widely recognized that commonsense knowledge plays a significant role in everyday decision-making and real-life scenarios, making commonsense reasoning a fundamental prerequisite for language models.Recent studies have introduced several datasets aimed at commonsense reasoning, such as HellaSwag [53], Winogrande [54], Socialiqa [55], Piqa [56], CommonGen [57], Cosmos QA [58],</p>
<p>and ART [59], which usually do not require a multi-step reasoning process.</p>
<p>Tackling Symbolic Reasoning</p>
<p>Symbolic reasoning can be characterized as a cognitive process carried out on abstract objects, guided by precisely defined rules like logical derivation.Besides coding and mathematical problem-solving, there exist various tasks that necessitate the application of symbolic reasoning.One such task is logistic reasoning, exemplified by datasets like PrOntoQA [60], SimpleLogic [61], FOLIO [62], and ProofWriter [63].In these tasks, a set of facts and logical rules are given in the context and the model is required to prove a formula based on logical operations.Other tasks involve the comprehension of virtual objects.For instance, in datasets such as Penguins, Date, and Colored Objects from BIG-Bench Hard [64], there is a demand for statistical analysis and manipulation of properties related to virtual objects.An illustrative question might be, "Which penguin is named after a famous jazz musician?".While language models demonstrates the ability to understand simple symbolic manipulations, they have been identified as less adept in complex symbolic reasoning tasks [60].</p>
<p>Interacting with Various Environments</p>
<p>Interacting with environments require reasoning skills that involve using commonsense knowledge to understand the current situation and plan future actions.Additionally, these environments demand the ability to process feedback and adjust subsequent actions based on that feedback.This domain has seen extensive exploration in recent studies focusing on language model-driven agents.Many of these environments primarily employ textual modalities as their basis.Examples include Textworld [65], Alfworld [66], and WebShop [67].These virtual scenarios rely on textual descriptions for state representation, feedback, and interaction.In these environments, agents are tasked with completing various objectives.For instance, they might be required to place fresh lettuce on a dining table within a living room setting or to choose a 3-ounce bottle of citrus-scented deodorant from a simulated online shopping platform.Beyond purely textual environments, some environments incorporate multiple modalities to challenge language model reasoning.</p>
<p>For instance, BabyAI [68] and MineDojo [69] present game-like scenarios.Within these environments, models are expected to engage in tasks such as crafting an iron sword in the Minecraft game.Furthermore, a subset of studies has delved into the realm of reasoning within real-world contexts.For example, Ahn et al. [70] tackled 101 robotic tasks within an actual kitchen, pushing the boundaries of language model reasoning beyond simulated scenarios.</p>
<p>Multimodal Reasoning Benchmarks</p>
<p>With the ever-evolving multi-modal large language models' reasoning capability, benchmarks play a pivotal role in assessing their performance, understanding their capabilities, and identifying areas that demand improvement.Based on the aforementioned definition and categorization of reasoning, an ideal multimodal reasoning benchmark is supposed to 1) truly require multimodal information; 2) follow the categorization of reasoning; and 3) have detailed annotation of reasoning steps to examine whether the reasoning process is correct.</p>
<p>Benchmark Datasets</p>
<p>Traditional datasets, such as COCO caption [71], Nocaps [72], and Flickr30K [73], are utilized to evaluate the MLLMs' comprehension of image content and captioning.Datasets focused on visual question answering, such as VQAv2 [5], OK-VQA [6], ScienceQA [7], and GQA [8], feature question-answer pairings.These datasets serve as an initial platform for evaluating a model's reasoning capabilities.Recently, more evaluation benchmarks have been introduced to holistically compare the various capabilities of MLLMs.We summarize existing evaluation benchmarks in Table 1, with an emphasis on their reasoning-related evaluation.</p>
<p>Surprisingly, except for the recently introduced Infi-MM-Eval [74], most existing reasoning-related multimodal benchmarks are not reasoning-focused and do not follow the aforementioned criteria of an ideal benchmark.For example, another manually labeled benchmark MM-Vet [75] does not follow the reasoning categorization to properly evaluate different reasoning types, nor contain reasoning steps to examine the reasoning process.Another example is MMMU [76], a dataset collected from textbooks.It does not specify the type of reasoning required and only includes partial annotations of the reasoning steps.For simplicity and to ensure our empirical findings are more focused on reasoning and robustness, our analysis will primarily concentrate on these three datasets in Sec. 6.</p>
<p>Evaluation Metrics</p>
<p>With these rich evaluation datasets shown in Table 1, it is important to evaluate using proper metrics.Therefore, it is necessary to categorize and review the evaluation metrics.The complexity of evaluating MLLMs arises from the multimodal nature of these tasks.The metrics must capture the synergistic relationship between language and vision, as well as other possibly involved modalities.This relationship demands assessments that not only estimate the accuracy of predictions but also the depth, nuance, and relevancy of multimodal associations.The existing evaluation metrics can be categorized from different aspects.Although we mainly focus on three datasets for results analysis, we categorize all the related datasets for a comprehensive summarization.From the perspective of evaluating the different capabilities of models, evaluation can be categorized into 3 types:</p>
<p>• For vision capability, traditional perception metrics are relevant, including tasks such as identifying, classifying, and locating objects within an image, as well as interpreting those objects in the context of the overall scene.</p>
<p>The common metrics used are image classification accuracy (ImageNet [93]), object detection mAP (COCO [94]), object segmentation mIoU (LVIS [95]), and others.</p>
<p>• For linguistic capability, commonsense understanding and coherence needed to be considered as metrics.</p>
<p>It goes beyond merely generating grammatically correct sentences; it's about producing responses that are contextually relevant, logical, and semantically coherent, ensuring alignment with the given visual input.The common metrics used are BLEU [96], CIDEr [97], ROUGE [98], among others.</p>
<p>• For reasoning capability, it covers spatial reasoning, knowledge reasoning (including areas like common sense, mathematics, text, code, and more), as well as hypothesis-based reasoning.Just as humans deduce information based on textual and visual cues, MLLMs should exhibit the ability to reason, draw inferences, and predict outcomes based on the multi-modal data they process.The common metrics are QA-Accuracy [76], Elo score [99], GPT-4 [100] evaluation, and others.</p>
<p>From the types of evaluation questions, existing multimodal benchmarks can be divided into two distinct categories:</p>
<p>• Closed-Set Evaluation Benchmarks are circumscribed by a predetermined set of categories or outcomes.When MLLMs are assessed under this paradigm, they're essentially provided a limited set of potential responses or classifications.Such evaluations gauge the model's aptitude to correctly categorize or respond within the given confines, making them particularly suited for tasks where boundaries are well-defined and the space of responses is finite.Certain visual evaluation tasks are of a closed-set nature, such as image classification and attribute classification.Conversely, some benchmarks with open-ended questions force to reformat answers into multiple-choice format, examples of which includes MMBench [82], MME [83] and ScienceQA [90].However, as studied in a recent work [101], authors found that popular MLLMs are vulnerable to adversarial permutation in answer sets for multiple-choice prompting.</p>
<p>• Open-Set Evaluation Benchmarks are more unbounded and exploratory.They allow models to generate responses without confining them to a predefined set.This evaluates the model's capability to venture into the unknown, generating or deducing outcomes that might not be part of the training set.In the vast and dynamic domain of multimodal interactions, where real-world scenarios often elude strict categorizations, open-set evaluations become crucial in assessing a model's generalization capabilities and adaptiveness to novel inputs.Two common approaches for evaluating open-set answers are human scoring such as LVLM-eHub [88] and Lynx [89], and automatic language model scoring, like TouchStone [9] and VisIT-Bench [86].</p>
<p>With the three datasets subject to our analysis, diverse reasoning evaluation metrics are employed, including GPT-4 evaluation (Infi-MM-Eval, MM-Vet) and QA-Accuracy (MMMU).Additionally, these datasets feature different evaluation question setups: Open-Set (Infi-MM-Eval, MM-Vet) and Closed-Set (MMMU).Therefore, a representative subset of benchmarks are covered in our analysis.</p>
<p>Improving Reasoning Abilities for LLMs</p>
<p>Since reasoning abilities are essential for solving complex tasks, a wealth of research has focused on improving reasoning abilities in LLMs.Using supervised data to pretrain or fine-tune LLMs is a straightforward method to enhance their reasoning capabilities.For instance, during the pretraining stage, Lewkowycz et al. [102] incorporate a variety of mathematical corpora into their training set to improve quantitative reasoning abilities.In the fine-tuning stage, Rajani et al. [103] utilize a small instruction-tuning dataset to fine-tune the pretrained GPT model.Beyond supervised training, in-context learning [104,105] and prompt engineering [106] have also shown great potential for improving reasoning abilities.such as the Chain-of-Thought approach [104] and its variants [107,108].Another line of research [109,110,111] has proposed interacting LLMs with external tools, such as a Python code interpreter [111], to enhance the accuracy of each reasoning step.Within the domain of MLLMs, these methods, namely instruction-tuning, prompt engineering, and tool usage have also been explored for multimodal reasoning, which we will later elaborate.</p>
<p>3 Multimodal Large Language Models V-L Interaction Module: As shown in Figure 1, some models incorporate visual tokens as if they were a foreign language, directly injecting them into the input layer.For instance, LLaVA [13] directly inputs the visual signal into the input layer of the large language model.In contrast, other models, such as Flamingo [12], employ cross-attention layers to facilitate interactions between visual and language features within transformer blocks.Connector Architecture: The specifics of the connector design also play a pivotal role in determining the capabilities of MLLMs.A key differentiating factor among these models is their choice of visual-language connectors.Models such as BLIP-2 [22], Flamingo [12], and QWen-VL [112] utilize query-based connectors like Q-former/perceiver resampler.Conversely, LLaVA [13] and MiniGPT4-v2 [113] employ a multilayer perceptron (MLP) as the connector.</p>
<p>Large Language Models</p>
<p>Recent advancements in MLLMs represent an evolution of LLMs, which makes reviewing of LLM development both relevant and beneficial.GPT-3 [114] pioneered this domain, showcasing a powerful emergent capability in zero-shot generation tasks.This was evident in its in-context learning and chain-of-thought processes, achieved using 175B parameters and pretraining on extensive web-text data.Subsequently, ChatGPT [115] further augmented these capabilities by incorporating instruction tuning and reinforcement learning from human feedback.GPT-4 is an advanced iteration of ChatGPT; however, its training specifics remain undisclosed due to the source code not being made public.</p>
<p>Other noteworthy LLMs, such as PaLM [116], BLOOM [117], Chinchilla [118], LLaMA [119], OPT [120], GLM [121], Alpaca [122] and Vicuna [123], have also been introduced, propelling the domain of LLMs forward.</p>
<p>Multimodal Large Language Models</p>
<p>We give a brief timeline in Figure 2, the efficacy of Multimodal Large Language Models is enhanced by incorporating vision encoders and connectors.This integration allows for training on diverse multimodal datasets.Table 2 highlights key distinctions among various MLLMs.The Flamingo architecture [12] pioneers the usage of query-based crossattention mechanism, termed the "perceiver resampler".This innovative approach constructs a robust vision-language interactive module, showcasing remarkable abilities in in-context learning.BLIP-2 [22], integrates the Q-Former.This query-based sampler, initialized from pretrained BERT, aims to bridge the gap between vision and language modalities.</p>
<p>These methods demonstrate their proficiency in generating text from images, a capability achieved through various stages of pretraining alignment.Later, InstructBLIP [124], enhances the pretrained BLIP-2 by training on instruction tuning datasets sourced from diverse public datasets.This augmentation results in improvements across the majority of zero-shot performance metrics, surpassing the performance of BLIP-2.LLaVA [13] further enhances the instructionfollowing ability of MLLMs by training on visual instruction tuning data.Its subsequent iteration, LLaVA-1.5 [125], expands upon this by integrating VQA datasets into the instruction tuning data, leading to significant performance improvements across a range of benchmarks.Notably, both InstructBLIP and LLaVA-1.5 follow a two-stage training process, which includes pre-training and subsequent instruction tuning.</p>
<p>In contrast, QWen-VL [112] introduces a more complex three-stage training process.This process includes pretraining, multi-task training, and instruction tuning.A particular emphasis is placed on the use of high-quality supervised data during the multi-task training stage.Additionally, QWen-VL attempts to fine-tune all modules of the model, including the vision encoder, connector, and the large language model.Furthermore, MiniGPT-v2 [113] also employs a three-stage framework.This framework emphasizes task-specific instruction templates for diverse tasks and involves fine-tuning both the projection layer and the Large Language Model, showcasing considerable improvements over MiniGPT4.LLaMA-Adapter [23] incorporates a lightweight adaptation method for fine-tuning.This method excels in instruction-following and multimodal reasoning tasks, particularly image-based question answering.Additionally, mPLUG-owl2 [25] leverages modality collaboration.This approach enhances performance in both text-only and multimodal tasks.Meanwhile, Otter [126] refines the OpenFlamingo model, which focuses on improving adherence to instructions and effectively utilizing in-context samples.Notably, the CogVLM [127] demonstrates superior performance by integrating an extra visual expert in the language model, achieving state-of-the-art performance on several benchmarks.</p>
<p>Input Layer</p>
<p>Multimodal Reasoning through Instruction Tuning</p>
<p>Pre-trained LLMs are capable of complex reasoning in a few-shot manner when prompted with task-related exemplars or rationales [128,104,106].One of the methods to achieve such ability of in-context learning (ICL) is through instruction tuning.With appropriate tuning, the model is expected to enhance its ICL capabilities, e.g. when prompted with examples of input-output pairs.However, there are several main challenges when the prompting modalities go beyond language.In this section, we first review current works on instruction tuning that facilitate ICL, and then the primary challenges and recent solutions in multimodal prompting.</p>
<p>Definition of Instruction Tuning</p>
<p>In recent years, instruction tuning has attracted significant attention due to the remarkable achievements of Large Language Models such as GPT-3 [128], LLAMA [119], Claude [129], PaLM [116].These models have demonstrated exceptional capabilities in various human-level understanding and reasoning tasks, including mathematics [52,46], coding [130], and most notably in generalizing to new tasks with zero or few-shot examples through instruction tuning.Pretrained LLMs often struggle with generalizing to new tasks and aligning with users' intents, leading to untruthful, unhelpful, and unsafe responses.To mitigate these limitations, instruction tuning [131,115] has been proposed.</p>
<p>Instruction tuning typically begins with a carefully collected dataset comprising thousands of ⟨instruction, response⟩ pairs.In this process, LLMs are fine-tuned using this dataset to better align human intent with model behavior.It is worth noting that the tuning of model parameters is supervised by the loss function L it , which is computed based solely on the response tokens.The loss function is defined as:
L it = − xi∈response log p(x i |x 0:i−1 , instruction).(1)
The pursuit of high-quality and diverse instruction-tuning datasets has been a key goal.A well-crafted instruction usually includes clearly described question that represents human intent, along with any necessary background question context.In addition, the response must be accurate and detailed.In the context of multi-modal instruction tuning, each example in the dataset consists of ⟨X-modality, instruction, response⟩, where X-modality typically represents images, videos, or audio.Table 2 shows current literature and their training dataset, with some of these involving instruction tuning stage.</p>
<p>Instruction Tuning for In-Context Learning</p>
<p>In-context learning on Large Language Models has gained significant attention as a crucial indicator of models' reasoning capabilities.Models excelling in ICL should inherently possess strong analogical reasoning ability.LLMs with robust ICL capability can adapt to new tasks based on just a few examples, without the need for updating their weights.Recently, Multimodal Large Language Models built upon LLMs have also demonstrated such ICL capability.Frozen [15] is the first MLLM to showcase ICL ability using a frozen LLM.Flamingo [12] demonstrated strong ICL capabilities with a more powerful LLM and web-scale images interwoven with text for pretraining.OpenFlamingo [157] and IDEFICS [158] are open-source reproductions of Flamingo.While they may not match Flamingo in every performance metric, they have demonstrated similar In-Context Learning (ICL) capabilities.</p>
<p>All ICL-capable MLLMs mentioned above are pretrained models.During the instruction tuning stage, most MLLMs primarily focus on image-text pairs, which can gradually lead to a diminishing of ICL capabilities.This is attributed to the challenges in constructing ICL-focused instruction tuning datasets.Recently, Otter [126] has developed the image-text-interleaved instruction tuning dataset MIMIC-IT [159], which was then fine-tuned on OpenFlamingo.Another notable instruction fine-tuning dataset is MIC [160], which includes in-context examples and is composed of 16 public datasets.In MIC, samples and in-context examples are constructed by randomly sampling from pre-defined templates.While these advancements have equipped MLLMs with ICL abilities, challenges continue to exist in the area of multimodal prompting.[140], GQA [8], OK-VQA [6], AOK-VQA [141], OCRVQA [142], TextCaps [143], LLaVA150k [13]</p>
<p>Multimodal Prompting by Representation Learning</p>
<p>Multimodal prompting, while opening up avenues to leverage rich information from non-linguistic modalities, often leads to catastrophic forgetting [161] due to disparities in information densities and a lack of high-quality paired data.</p>
<p>A key challenge in this domain is to derive LLM-compatible representations from non-linguistic inputs without altering the base LLM.This challenge has spurred research in multimodal representation learning, aiming to develop a unified prompt embedding space for various modalities.As an attempt to accommodate a secondary prompting modality and to harness the knowledge from the pre-trained LLM, Tsimpoukelli et al. [15] train a vision encoder alongside a frozen LLM to transform vision prompts into an embedding sequence compatible with language embeddings.Additionally, Koh et al. [162] proposed a method for learning linear projections to convert embeddings between vision and language.This approach enables their contextual-retrieval-based model FROMAGe to uniformly handle multimodal prompts.</p>
<p>Multimodal Prompting by Exemplar Generation</p>
<p>Prompting with in-context examples is a paradigm that unlocks the analogical reasoning ability of the pretrained LLMs for downstream tasks [163].Although this approach is widely used in single-modality contexts [128,164,165,166], there are relatively few studies on prompting with multimodal examples.A significant challenge in multimodal in-context learning is that multimodal LLMs often require contextual information to tackle new tasks, but acquiring incontext examples for every new task is not always feasible.Addressing this issue, Guo et al. [167] presented Img2LLM, a system capable of automatically generating LLM-agnostic exemplar prompts for visual question answering based on question images.This is achieved by first extracting answer candidates from generated captions and then formulating questions that correspond to these answers, thus creating a set of question-answer pairs suitable for new tasks.</p>
<p>Multimodal Prompting by Model Interaction</p>
<p>Combining the distinct capabilities of foundation models across different modalities for general multimodal tasks remains challenging, often requiring computationally expensive joint pretraining or fine-tuning.Zeng et al. [16] introduced the concept of multimodal prompting.This approach facilitates the exchange of information between different modalities through language prompts.They demonstrate that by thoughtfully crafting interactive processes and prompt templates for specific multimodal tasks, these foundation models can perform new tasks in a zero-shot manner, eliminating the need for additional training.Wang et al. [168] similarly uses text as the interface to realize multimodal in-context learning without any training.</p>
<p>Applications</p>
<p>In this section, we mainly cover Embodied AI and Multimodal Agent to discuss the applications of MLLM models.</p>
<p>Embodied AI</p>
<p>Embodied AI has become increasingly popular with the advancement of powerful AI models.The Embodied AI workshop at the conference on Computer Vision and Pattern Recognition (CVPR) offers this definition:</p>
<p>Minds live in bodies, and bodies move through a changing world.The goal of embodied artificial intelligence is to create agents, such as robots, which learn to creatively solve challenging tasks requiring interaction with the environment.</p>
<p>https: // embodied-ai.org/ This field requires the integration of multiple technologies and knowledge areas, including Computer Vision, Speech, Natural Language Processing, and Control Systems.These are essential for enabling an agent to see, talk, listen, and interact with its environment.More importantly, it also demands that the agent possess reasoning abilities, allowing it to plan actions based on environmental cues and adapt its behavior in response to feedback from its surroundings.</p>
<p>Robotics stands as the foremost representation of embodiment AI agents.The recent advancements in MLLMs shed light on robotic manipulation by leveraging the commonsense knowledge from the visual world and the reasoning ability of LLMs. Figure 4 illustrates the interaction loop between an Embodied Agent and an MLLM.Specifically, this commonsense knowledge assists the Embodied agents in tasks like planning, complex reasoning, code generation, and the creation of control commands.These generated instructions, in various formats, are then provided to the control unit for the generation and execution of actions.This section provides a summary of the latest applications of MLLMs in embodied AI, with a focus on their reasoning capabilities.</p>
<p>Tasks and Challenges of Embodied AI Reasoning</p>
<p>Research tasks in Embodied AI are broadly classified into three categories: visual exploration, visual navigation, and embodied question answering [169,170].These tasks require the embodied agent to perceive its environment, plan and execute control strategies, and interact with humans or the environment.A primary challenge in this field is adapting agents to out-of-domain environments, which demands robust commonsense knowledge and reasoning abilities from the agent.This aligns with the recent advancements in pretrained large models, including both LLMs and MLLMs.The subsequent subsections will delve into how MLLMs can be applied to enhance the capabilities of Embodied Agents.</p>
<p>Direct Reasoning with MLLM Knowledge</p>
<p>Early methods of employing MLLMs primarily centered on the direct use of pre-trained LLM and MLLM models.These models typically incorporate commonsense knowledge acquired from extensive datasets.Observations gathered by the Embodied Agent are provided to these models, leveraging their pretrained reasoning abilities for task planning and problem-solving.The principal strategy involves decomposing the embodied tasks into smaller, more executable steps, allowing the pre-trained model to efficiently tackle each intermediate stage.Initially, these studies focused less on additional training of the model and more on the task decomposition and the efficient coordination of subtasks.</p>
<p>Reasoning with Separate Components.CLIPort [171] represents an early endeavor to harness the semantic knowledge of pretrained models trained on large-scale datasets.In particular, it utilizes CLIP [132] to assist broad semantic understanding.Featuring a two-stream architecture, their end-to-end framework is capable of solving various tasks without the need for explicit concept learning for each task.</p>
<p>Later, Socratic Models (SMs) are proposed in [16], where each pretrained model is assumed to contain commonsense knowledge from distinct domains.By combining these models, they can collectively address tasks across various domains.Specifically, different models can be composed by using language prompts as the bridge.The proposed Socratic Models employ a multimodal prompting strategy to combine different VLM, LM models.</p>
<p>In a related study, LM-Nav [172] leverages pretrained models for robotic navigation.The main idea is to employ language as the bridge to break down the instructions.The process involves employing GPT-3 for identifying textual landmarks for instructions, using CLIP for landmark grounding, and optimizing a probabilistic objective by inferring a joint likelihood over landmarks and nodes.Again, this method requires no further model fine-tuning.</p>
<p>More recently, Dasgupta et al. [173] explored the Embodied AI agent concept within a Planner-Actor-Reporter paradigm.The "Planner" interprets human instructions and directly translates them into sequences of simple steps to be executed by the "Actor", which can be implemented by a pretrained LLM.The "Reporter" observes the environment, gathers information, and reports it to the "Planner".The information will be appended to the previous instructions to form the new prompt.Their experiments highlight the advantages of this paradigm.They also suggest that the "Reporter" can be trained within a Reinforcement Learning (RL) framework.</p>
<p>Reasoning with External World Model.Currently, LLM has a limit of window context.To execute a task that involves the states or facts from the distant past, this work proposes to use a world state to represent the knowledge [174].</p>
<p>In particular, the proposed approach attempts to use memory or a world model to record the key states of the current environment.The mechanism includes a world state reader and world state writer, which utilizes the states as prompts for LLM to solve a complex task and update the world state memory according to new observations.Experimental results highlight the effectiveness of this model, surpassing existing methods in certain aspects.However, there are areas for improvement.The method still requires task-specific prompt definition, lacks visual feedback, and the world state may be updated inaccurately.</p>
<p>Reasoning with Feedback.Inner Monologue [175] attempts to imitate human thought processes in natural languages.</p>
<p>Inspired by human cognition, this work proposes a method for solving robotic tasks by using languages as a bridge.Robots are equipped with "short-horizon skills", enabling them to execute basic tasks or commands described in simple language.Additionally, a Large Language Model is employed as a planner.This LLM aims to fulfill high-level user commands by breaking them down into a sequence of skills that the robot can perform.Furthermore, the system can also incorporate environmental "textual" feedback into its prompts, a process described as an "inner monologue".This feedback includes success detection, object and scene description, Visual Question Answering (VQA), and human feedback.The authors highlight the emergent capabilities of their system.Most of the reasoning and re-planning are facilitated by the LLM.In particular, the authors attribute the success of this reasoning to the incorporation of environment feedback into the model's decision-making process.</p>
<p>Reasoning with Policy Generation.The research presented in [176] further studies the application of language model programming for robotic control.Moving beyond the direct use of the pretrained LLM's language capabilities, this study also involves prompting an LLM with coding ability to generate policy code.</p>
<p>This approach further expands the scope of reasoning tasks to include arithmetic and novel instructions.Specifically, this study uses OpenAI Codex code-davinci-002.It uses prompts, supplemented with examples, to guide the LLM in generating additional functions that adhere to user instructions.</p>
<p>Subsequently, the generated code is executed using the robot's control primitives.As discussed in the paper, this system effectively utilizes LLMs and code to enhance its reasoning capacity.However, it is constrained by the limitations of the available API and control primitives.Additionally, the system lacks a mechanism to assess the feasibility of a given instruction.</p>
<p>Improved Reasoning with Robotic data</p>
<p>Collecting robot trajectories with crowd-sourced annotations is challenging, often resulting in datasets of limited scale.The authors from [177] propose data driven instruction augmentation for language-conditioned control (DIAL).Their approach consists of several key steps: 1) Fine-tuning a Vision-Language Model (VLM) on a small, manually annotated dataset of robot manipulation trajectories; 2) Gathering a larger, unannotated dataset; 3) Using the fine-tuned VLM to annotate this newly collected dataset; 4) Retraining the VLM on both the original and the newly labeled dataset.</p>
<p>In the re-labeling step, the authors utilize GPT-3 to expand the range of human instruction candidates.They explore various strategies for selecting appropriate instructions for given robot trajectories.Overall, DIAL demonstrates more capabilities in solving novel tasks.It can also be used to distill Internet-scale vision language models into embodied agents [178].</p>
<p>Another approach involves integrating discrete text tokens directly into the training set as natural language tokens [17], further enriching the model's training data.</p>
<p>Embodied Foundation Model</p>
<p>PaLM-E [19]: the Largest Reported Embodied Multimodal Large Language Model.The primary goal is to transform inputs from various modalities into formats compatible with LLMs, thereby enabling embodied agents to interact with environments across different modalities.The paper evaluates various methods for converting these modalities into LLM inputs.In particular, the object scene representations [179] demonstrate its effectiveness even with limited training data in robotic applications.The largest PaLM-E-562B demonstrates emergent abilities, including zero-shot multimodal chain-of-thought reasoning.This is largely due to its ability to transfer knowledge from general visual-language understanding to tasks related to embodied AI.</p>
<p>However, the reasoning or planning skills of PaLM-E are integrated with separate low-level policies that generate robot control commands for execution.In this work, the authors employ RT-1 [180] for these low-level policies.RT-1 processes visual observations and language instructions, converting them into robot control commands.</p>
<p>RT2: Vision-Language-Action (VLA) Model.Subsequently, the following approach, RT-2 [17], directly utilizes a pretrained VLM model to generate robot control commands.To achieve this, they apply the action discretization pipeline.This pipeline transforms each dimension of the seven-dimensional continuous action space into 256 uniformly distributed discrete actions.</p>
<p>Following this transformation, the RT-2 model undergoes fine-tuning with both robotics data and original web data.This approach shows better generalization than models fine-tuned solely with robotic-only data.</p>
<p>For the base VLM, the authors employ two of the largest VLM models available: PaLI-X [18] and PaLM-E [179].Their results prove that after fine-tuning the VLA models with robot trajectory data, RT-2 demonstrates several emergent abilities.These include enhanced reasoning capabilities with novel objects and improved interpretation of human instructions.More interestingly, after only a few hundred gradient steps of fine-tuning, RT-2 acquires chain-of-thought reasoning abilities.The data augmentation process introduces an additional "Plan" step, wherein the model is required to plan its actions before executing them.This seemingly simple addition significantly enhances the model's ability to comprehend and respond to more complex commands.</p>
<p>MLLMs for Tool Usage</p>
<p>Tool Usage in LLM</p>
<p>Recent studies have explored the potential of using external tools to augment LLMs' capabilities for complex tasks [111].Research [181,182,183] has demonstrated that supplementing LLMs with tools, such as calculators, search engines, translation systems, calendars, or even API calls to other models can help solve tasks beyond their inherent capabilities.</p>
<p>The integration of tools into LLMs generally involves two primary phases: tool selection or creation, and processing the results obtained from these tools [184,185,186].Tool selection, a pivotal phase within tool learning, can involve allowing the LLM itself to make decisions based on input queries [187,188,189,111,19].Alternatively, it might include the use of predefined tools specifically designed for particular scenarios [182,190,191,192], such as retrieval systems.</p>
<p>When the model is required to autonomously determine which tools to employ, it necessitates the model to understand the characteristics of these tools and make accurate selections via their reasoning abilities.The majority of approaches tend to decompose a complex task into a series of subtasks interacting with tools in sequential or tree structures, such as Chain of Thoughts (CoT) [104,106], Tree of Thoughts (ToT) [193] and Program of Thoughts (PoT) [110].</p>
<p>A simple way to teach language models to use tools is by instruction [111,191,183].The description of tools is able to teach language models to use tools [191] in a zero-shot fashion.Language models can also learn to use tools from few-shot examples.This in-context tool learning can range from retrieval tasks to enhancing the model's ability to generalize to new challenges.For example, Art [183] demonstrates this by selecting examples of multistep reasoning and tool usage from a task library.This approach significantly improves upon traditional few-shot prompting and automated Chain of Thought (CoT) methods, particularly in unseen tasks within the BigBench and MMLU benchmarks.</p>
<p>Some studies have shown that LLMs can learn to utilize tools by fine-tuning on the tool-specific corpora [181,182,194].TALM [181] and Toolformer [182] both adopt a self-supervised method, starting with a few tool demonstrations and then enabling the language model to autonomously generate additional data involving tool usage.TALM is designed to use APIs for search and calculation, applying these functions to enhance language models' capabilities in question-answering tasks.Toolformer, on the other hand, utilizes a broader range of tools, including a calculator, a QA system, a search engine, a translation system, and a calendar, leading to improved zero-shot performance across various downstream tasks.</p>
<p>Gorilla [194] introduces APIBench, a comprehensive dataset comprising APIs from HuggingFace, TorchHub, and TensorHub.The study demonstrates that a fine-tuned LLaMA-based model, trained with this dataset, significantly surpasses GPT-4 in generating API calls, showcasing its enhanced performance.</p>
<p>Recent research [188,110] has expanded the capabilities of LLMs beyond just utilizing existing tools; it now includes enabling LLMs to create tools through programming.This method uses programming languages to tackle mathematics tasks, thereby enhancing the models' capacity for accurate mathematical computation and improving logical task planning.[196] ReAct image captioning, tagging, celebrity recognition, OCR, Web Modalities Integrating Chameleon [190] Program Modalities: image, web, knowledge, math, table TaskMatrix.AI [197] CoT Modalities: image, office, cloud, robot</p>
<p>MLLMs as Tool</p>
<p>MLLMs can function as tools to empower language models in handling various modalities and addressing challenging visual reasoning tasks.We briefly review these works from different application perspectives, as illustrated in Table 3.</p>
<p>Compositional Visual Reasoning.While current MLLM models demonstrate proficiency in visual tasks such as image classification and captioning, they encounter difficulties with more complex visual operations, like visual editing, due to inherent requirements for reasoning and planning.For instance, replacing a person's face with a cat's face in an image involves face detection, person recognition, cat face generation, and merging the new face into the original image.Critical reasoning is required to break down the tasks and develop a detailed execution plan.MLLMs can play a pivotal role in carrying out these decomposed steps.</p>
<p>To solve compositional visual reasoning tasks, VISPROG [195] utilizes MLLMs as a toolset within a modular and interpretable neuro-symbolic system.This system, designed for comprehensive visual understanding and manipulation, leverages GPT-3 to generate programs through in-context learning.These programs coordinate different modules, such as OWL-ViT [198] for open vocabulary localization, DSFD [199] for face detection, MaskFormer [200] for segmentation, CLIP [132] for image selection and classification, ViLT [201] for visual question answering, and Stable Diffusion [202] for object replacement.Additional tools such as Python Imaging Library 3 (Pillow) and OpenCV 4 are also used for image manipulation.VISPROG can handle various complex visual reasoning tasks, including compositional VQA, zero-shot reasoning on image pairs, factual knowledge object tagging, and image editing using natural language.</p>
<p>Multimodal Dialogue Reasoning.Following dialog instructions can become more complex than single-step instructions due to their context-dependent nature.Multi-turn dialogue understanding requires reasoning based on dialog history.MLLMs can serve as tools for LLMs to facilitate dialogs that incorporate visual inputs and outputs, drawing on the reasoning capabilities of LLMs to interpret dialog instructions.</p>
<p>Visual ChatGPT [20], incorporating various MLLM, enables users to interact with ChatGPT using both text and images.This system harnesses MLLMs such as BLIP [22], Stable Diffusion [202], and other visual foundation models like Pix2Pix [202], ControlNet [203].It is capable of performing tasks like image captioning, image question answering, and image editing during conversations.Chain-of-Thought (CoT) [104] reasoning method is used for case that requires collaboration between multiple visual models.</p>
<p>Additionally, MM-ReAct [196] utilizes the ReAct [111] planning method alongside a suite of image and video tools for a stronger capacity of visual tasks.These tools cover a wide range of functions, including image captioning, image tagging, dense captioning, celebrity recognition, receipt scanning, OCR, and Bing search.This enables MM-ReAct to handle difficult tasks such as Visual Math and Text Reasoning, Visual-Conditioned Jokes/Memes, and Visual Planning.</p>
<p>Integrating Modalities Beyond the Visual Domain.MLLMs are usually customized for specific modalities, such as images, which poses a challenge when trying to integrate their usage across varied domains like mathematics, the web, tables, and more.For instance, solving a math problem depicted in an image requires the visual model to first interpret the image and then invoke the math model for a solution.Addressing this challenge entails fostering reasoning and planning among various modality models.By conceptualizing all the modality models as tools and leveraging the reasoning capabilities of LLM, recent efforts aim to integrate additional modalities beyond the visual domain, such as web content, tables, office documents, and robotic data.This approach contributes to the development of a more powerful multimodal model.</p>
<p>Chameleon [190] exemplifies this approach.It can utilize a variety of tools, such as existing vision models, web search engines, Python functions, and heuristic-based modules.Capable of handling tasks across image, web, knowledge, math, and table domains, Chameleon synthesizes programs by combining these tools to coordinate their sequence through an LLM-driven planner for multistep tasks.</p>
<p>TaskMatrix.AI [197] provides a comprehensive set of APIs catering to a wide range of tasks.These include image editing, image-based question answering, image captioning, text-to-image conversion, and transformations like imageto-sketch/depth/hed/line.Furthermore, TaskMatrix.AI extends its utility to conversational robotics and IoT device control, offering APIs for tasks like robotics pick, move, and put, car air conditioner control, TV operation, and music playback.TaskMatrix.AI utilizes the CoT [104] reasoning method to device tool usage plans.In this section, we briefly summarize the performance of existing Multimodal Large Language Models on the evaluation benchmarks outlined in Sec.2.3.Our focus is primarily on accessing the reasoning capabilities of these models.InfiMM-Eval [74] is an open-ended question-answering benchmark designed to evaluate MLLMs' visual reasoning capability.In contrast to MMMU, which heavily relies on the knowledge capacity of the underlining LLM, InfiMM-Eval focuses more on the fundamental unified logic capabilities of MLLMs.The evaluation results on InfiMM-Eval are shown in Table 5.Similar to other benchmarks, GPT-4V has shown a significant lead over other MLLMs.Among the open-source models, SPHINXv2 [215], Qwen-VL-Chat [112], and CogVLM-Chat [127] rank as the top three.While Qwen-VL-Chat and CogVLM-Chat have achieved similar overall scores, their performance varies across different levels of reasoning complexity.Since CogVLM-Chat employs similar underline LLMs with other models, it is hypothesized that Qwen-VL-Chat's superior performance in highly complex questions may stem from the Qwen language model, whereas CogVLM-Chat's strengths in moderately complex questions are likely due to its enhanced detailed perception capability.</p>
<p>Multimodal Reasoning Benchmark Result Analysis</p>
<p>In reviewing the top-performing open-source models on reasoning-focused benchmarks such as SPHINX-v2, QWen-VL-Chat, CogVLM-Chat and InfiMM-LLaMA-13B, we summarize the following common recipes to facilitate future research: • Unfreezing the language model at a certain training stage is instrumental in enhancing multimodal reasoning abilities.For example, SPHINX-v2 and QWen-VL-Chat unfreeze their language models during the pretraining and instruction tuning stages, respectively.CogVLM, on the other hand, trains visual experts to modify the language model's response to visual input and also unfreeze the language model during fine-tuning.</p>
<p>While tuning the language model is expected to improve multimodal performance, careful control of training parameters such as the number of training steps and the learning rate is crucial to prevent catastrophic forgetting.• Improving visual representations is key to better multimodal performance.This can be achieved by increasing the resolution of images for finer details, as done in QWen-VL-Chat and CogVLM, or by employing stronger or multiple visual encoders, like in SPHINX-v2.Another effective method is fine-tuning the visual encoder during the MLLM training process, as demonstrated by QWen-VL-Chat.• The multi-task supervised learning stage significantly contributes to the superior performance of top models like QWen-VL-Chat, CogVLM, and InfiMM-LLaMA-13B.Each of these models employs a three-stage training process, demonstrating the value of this approach.Notably, InfiMM-LLaMA-13B, while using the same instruction dataset as LLaVA-1.5, showcases improved reasoning abilities across various benchmarks.The multi-task supervised learning stage can be achieved by utilizing datasets of higher quality than the pretraining stage, yet containing less conversational data compared to the instruction tuning stage.Additionally, CogVLM achieves impressive performance on the mm-vet, utilizing unknown in-house data during the instruction tuning stage.</p>
<p>Besides these general strategies, we also observed dataset-specific trends and variances:</p>
<p>• As shown in Table 6, instruction tuning significantly enhances performance on InfiMM-Eval, arguably the most representative dataset for evaluating multimodal reasoning.The importance of instruction tuning for MLLMs is also verified by the fact that MLLMs usually take instruction tuning as their final training stage.• There are significant differences in model rankings between InfiMM-Eval and MMMU.For example, BLIP-2, which performs relatively poorly on InfiMM-Eval, excels on MMMU, surpassing models such as CogVLM.</p>
<p>Due to a range of factors, including the types of evaluation datasets, language model architectures, and training methodologies, it is challenging to pinpoint the exact cause of these discrepancies.However, it is important to highlight the potential for performance gaps across various evaluation datasets.</p>
<p>Note that our analysis is more of a case-study approach, focusing on the findings from top-performed models, especially in the context of the reasoning-focused InfiMM-Eval dataset.Therefore, these insights may not be universally applicable across various training data source, language models, and their respective scales.</p>
<p>Conclusions and Future Directions</p>
<p>The concept of reasoning ability is pivotal in the quest for Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI), a goal that has been pursued across various scientific disciplines for several decades.With recent advancements in both models and evaluation benchmarks, there is a growing discourse on the reasoning abilities exhibited in current LLMs and MLLMs.In this work, we present the different types of reasoning and discuss the models, data, and evaluation methods used to measure and understand the reasoning abilities demonstrated in existing studies.Our survey aims to provide a better understanding of our current standing in this research direction and hopes to inspire further exploration into the reasoning abilities of future work.</p>
<p>In addition, we wish to outline some potential research directions to improve the reasoning capabilities of current MLLMs.The following discussions include our summaries regarding pretraining, alignment and evaluation benchmarks:</p>
<p>MLLM Architectures.Current MLLM architectures still have the following fundamental limitations in achieving human-level multimodal reasoning ability.The first limitation comes from the image resolution.Current Multimodal Large Language Models typically operate on images with a fixed, preset resolution.However, in real-world scenarios, images can come in a variety of resolutions.The hallucination problem of MLLM also possibly stems from the MLLM architecture design.With additional modalities as input, it is more complicated to diagnose the source of the hallucinations in MLLMs.It becomes challenging to determine whether these hallucinations arise from an incomplete perception of visual signals or from biases and spurious correlations ingrained in language models.Therefore, it is important to design MLLM architectures that can inherently minimize or avoid hallucinations caused by inaccurate perception.</p>
<p>Efficiency and Scalability of MLLM Training Recipes.As shown in Figure 3, different MLLMs utilize vastly varying amounts of data in each training stage.However, performance comparisons do not show proportional differences (Table 5 and Table 4).Understanding the relationship between data volume and model scale at each training stage is crucial.According to recent studies [216,125,217], the pretraining stage for aligning vision-text representation may not require extensive computational resources.</p>
<p>Long-context Support.Existing MLLMs often serve in short-context situations and lack the ability to handle diverse real-world, long-context scenarios.These scenarios range from comprehending lengthy documents and papers to more complex tasks like understanding videos or movies.While most MLLMs employ attention mechanisms, the challenge with long-context tasks is the need to minimize computational complexity and maintain focus as the prompt length increases.A significant gap exists in research on multimodal long-context tasks, largely attributable to the absence of appropriate datasets and evaluation benchmarks that are equipped to tackle these specific challenges.</p>
<p>Instruction Fine-tuning (IFT) Data.An enhanced approach to instruction fine-tuning data is necessary, given that most of the existing public MLLM instruction datasets are insufficient in developing reasoning abilities for multimodal tasks.Currently, there is a lack of ablation studies exploring the type of data that could boost multimodal reasoning capabilities.Therefore, the creation of future instruction datasets demands careful consideration to more effectively improve reasoning capabilities in multimodal contexts.</p>
<p>Reinforcement Learning for Multimodal Reasoning.Inspired by the success of InstructGPT [115], leveraging reinforcement learning algorithms [218] to align model predictions and human preference has been extensively studied [129,219].However, reinforcement learning with human feedback (RLHF) has been just applied to MLLM [220] and there are many exciting open research problems.For example, given the complexity of multiple modalities, it is important to collect preference data effectively and scalably.With sufficient data, it is also unclear whether existing learning algorithms can handle the multimodal tasks or not.Another example is how to develop multimodal agents capable of actively engaging and utilizing tools for complex, reasoning-intensive tasks.</p>
<p>Evaluation Benchmarks.Current evaluation benchmarks focus on single-round conversations, revealing a gap for multi-round, multi-image conversational benchmarks.Such benchmarks require models to respond to a variety of instructions within a single conversation, which could significantly advance research in enhancing models' generalization capabilities and reducing over-fitting to specific instructions.</p>
<ol>
<li>1
1
OverviewMultimodal Large Language Models typically comprise a visual encoder and a language model, linked by a connector.Both the visual encoder and the language model are commonly initialized from pretrained models.The distinctions among MLLMs can be analyzed based on the following key aspects:Training Data: Multimodal training data can be segmented into three parts.(1) Pretraining data is utilized for aligning other modalities, such as the visual modality, with the Large Language Model.(2) (Optional) High-quality multi-task supervised learning data comprises question-answer pairs used to inject knowledge into the model.(3) The instruction tuning data enhances Multimodal Large Language Models by improving their abilities to follow instructions and engage effectively in user interactions.</li>
</ol>
<p>Figure 1 :
1
Figure 1: Various architectures of Multimodal Large Language Models.</p>
<p>Figure 2 :
2
Figure 2: A brief timeline outlining recent developments in MLLMs.</p>
<p>Figure 3 :
3
Figure 3: The datasets used in different stages of various Multimodal Large Language Models are depicted.At the center of each circle, the total number of samples (in millions) for each stage is displayed.</p>
<p>Figure 4 :
4
Figure4: Illustration of the interaction between an Embodied Agent and MLLM/LLM models.Here, pretrained models generate commands and planning outcomes for the robot.In turn, the robot provides environmental observations to the MLLM models, aiding in refining their reasoning for future tasks.</p>
<p>Table 1 :
1
Evaluation Benchmark Summarization.Benchmarks are ordered by releasing date.
DatasetDescriptionStatsReasoning PartTagsMMMU[76]Designed to evaluate multimodal models11.5K questions fromManual Collect , Multiple-Choice Accuracyon massive multi-discipline tasks demand-college exams, quizzes,ing college-level subject knowledge andand textbooksdeliberate reasoningInfiMM-Eval[74]Designed for open-ended complex reason-279 high quality sam-Manual Collect , GPT eval , Full reasoning steping QA with intermediate stepsplesHallusionBench[77]benchmark for evaluating both visual and346 images with 1129Manual Collect , Public Source , GPT evallanguage hallucinationquestionsMathVista[78]Consolidated Mathematical reasoning6,141 samplesManual Collect , Public Source , Multiple-Choice Accuracybenchmark with visual contexts, covers 7types of reasoning, e.g. algebraic, arith-metic, geometry, logical, numeric com-mon sense and scientificLLMDoc[79]OCR-free document instruction under-100 samplesManual Collect , Public Source , Human evalstanding evaluation set, including table,chart, document, natural image, and web-pagePCA-Eval[80]Evaluating the decision-making ability of300 multi-choice ques-each question has a "rea-Manual Collect , Multiple-Choice Accuracyembodied agents from three perspectives:tionsson" field for evaluatingperception, cognition, and action. Imagesreasoning capabilityare from various embodies environmentsSparklesEval[11]GPT-assisted benchmark for assessing a150 dialogs, total 550GPT Generate , GPT evalmodel's conversational competence acrossimagesmultiple images and dialogue turnsM3Exam[81]Multilingual, multimodal, multilevel, mul-12137 questions, 23%Manual Collect , Multiple-Choice Accuracytiple choice questions from exam paperscontain imagesTouchStone[9]Comprehensive visual dialogue dataset,908 questionscomprehension ques-Manual Collect , GPT evalconsisting of open-world images and ques-tions at 29.6%, 3.6% fortions, covering 5 categories of abilities andmulti-image analysis27 subtasksMM-Vet[75]QAs for 6 VL capabilities with 187 online218 questions11.9% of math andManual Collect , GPT evalimages and 13 public dataset images34.4% of spatial aware-nessMMBench[82]Vision-language QAs in the format of mul-2,974 questions1114 Reasoning Ques-Manual Collect , Multiple-Choice Accuracytiple choice over 20 abilitiestionsMME[83]Yes and No choice questions on 14 sub-1097 images130 imagesManual Collect , Multiple-Choice Accuracytasks. Each image has 2 questionsLAMM-Benchmark[84]9 image tasks and 3 point cloud tasks62439 images andScanQA, SQAimageGPT Generate , Traditional Metrics , GPT eval12788 point cloud sam-and AI2Dples, 186k instruction-response pairsCCEval[85]benchmark for evaluating detailed caption100 VisualGenome im-Public Source , GPT evalhallucination with object coverageagesVisIT-Bench[86]Instruction following evaluation with592 queries with 1159Manual Collect , GPT eval , Human evalhuman-authored instruction-conditionedimagescaptionSEED-Bench[87]Multiple choice questions, spans 12 eval-19K questions331 visual reasoning, 97Manual Collect , GPT Generate , Multiple-Choice Accuracyuation dimensions in images and videos.instance interaction andQuestions generated by GPT, answered by657 spatial relationshumanI4[14]Evaluating instruction following ability on18k instructions withPublic Source , Multiple-Choice Accuracy , Caption Scoreinterleaved VL instructions, covers 19 sce-62k imagesnarios with 29 tasksLVLM-eHub[88]Combination of many public datasets and47 visual benchmarksDocVQA,OKVQA,Public Source ,Human eval ,Caption Score ,an online arena platformScienceQA, SNLI-VE etc.Multiple-Choice AccuracyOwlEval[25]50 collected images with questions, some82 questionsManual Collect , Human evalhas multi-turn conversationsOpen-VQA[89]Open-ended questions including images450 questions31 Reasoning QuestionsManual Collect , Human evaland videosScienceQA[90]Multimodal multiple-choice questions21208 questions, 10332Manual Collect , Multiple-Choice Accuracywith annotations and explanations coverscontain imagesnatural science, social science, and lan-guage science, collected from online edu-cation platformWinoground[91]Measuring visio-linguistic compositional1600 image-text pairs,reasoningcomprise 400 examples
[92]al Collect , Multiple-Choice Accuracy RAVEN[92]Multi-choices Raven's Progressive Matrices (RPM) questions 1,120,000 images and 70, 000 RPM problems Rule Generate , Multiple-Choice Accuracy</p>
<p>Table 2 :
2
Recent multimodal large language models,
MethodsVisual EncoderPre-training StageMulti-task learning StageInstruction TuningOpenFlamingosCLIP-ViT-LAION-2B [133], Multimodal--L/14 [132]C4 [134], Synthetic dataLLaMA-Adapter-v2CLIP Vit-L/14GPT-4-LLM [135], COCO--BLIP2CLIP ViT-L/14COCO [94], CC3M [136], CC12M--[137], LAION400M [138], VisualGenome [139],InstructBLIPViT-G/14COCO, Visual Genome CC3M [136],-26 publicly available vision-CC12M [137], LAION400M [138]language datasets, transformed intothe instruction tuning formatLLaVA-1.5CLIP ViT-L/14LLaVA [13]-LLaVA665k(VQAv2</p>
<p>Table 3 :
3
Comparison of Works Using MLLM as Tools.For Modalities Integrating Works, we list the tool domain instead of enumerating all individual tools.
ApplicationWorkReasoning methodToolsCompositional Visual ReasoningVISPROG [195]ProgramOWL-ViT, DSFD, MaskFormer, CLIP, ViLT, Stable DiffusionMultimodal DialogueVisual ChatGPT [20] MM-ReActCoTBLIP, Stable Diffusion, Pix2Pix, ControlNet</p>
<p>Table 4 :
4
Evaluation results for various MLLM on MMVet and MMMU (validation set).Open-source models best performances are indicated with underlines.
MLLMsMMVet MMMUMiniGPT-4 [113]24.426.8OpenFlamingo-v2 [157]24.828.7LLaMA-Adapter V2 [204]31.429.8CogVLM-Chat [127]52.832.1Otter [126]24.632.2mPLUG-Owl2 [205]36.332.7BLIP-2 [22]22.435.4InstructBLIP [124]26.235.7QWen-VL-Chat [112]-35.9LLaVA-1.5 [13]36.336.4InfiMM-LLaMA-13B [206]39.239.1GPT-4V [100]67.755.7</p>
<p>Table 5 :
5
Evaluation results for various MLLMs on InfiMM-Eval.The top performances achieved by open-source models are indicated with underlines for easy reference.
MLLMsReasoning Category Deductive Abductive Analogical Moderate Reasoning Complexity HighOverallOpenFlamingo-v2 [157]8.885.31.119.474.726.82MiniGPT-v2 [113]11.0213.285.6914.457.2710.43Fuyu-8B [210]16.4221.497.7823.069.9115.7BLIP-2 [22]22.7618.967.524.0514.1819.31InternLM-XComposer-VL [211]26.7735.9718.6139.1317.1826.84InstructBLIP [212]27.5637.7620.5640.6418.0928.02LLaMA-Adapter V2 [204]28.746.1222.0841.3321.9130.46Otter [126]22.4933.6413.3335.7912.3122.69mPLUG-Owl2 [205]23.4320.67.6428.7913.1820.05IDEFICS-9B-instruct [213]22.9934.6320.5634.4516.7324.53Emu [214]28.936.5718.1936.1822.028.24LLaVA-1.5 [13]30.9447.9124.3147.421.032.62CogVLM-Chat [127]36.7547.8828.7555.6722.537.16Qwen-VL-Chat [112]37.5544.3930.4246.6130.0937.39SPHINX-v2 [215]42.1749.8520.6954.8527.3139.48InfiMM-LLaMA-13B [206]39.953.1831.9455.8528.740.7GPT-4V [100]74.8677.8869.8693.9858.9874.44</p>
<p>Table 4
4
[209]nts a summary of the results from both MM-Vet and MMMU datasets.MM-Vet[75]is an open-ended QA benchmark designed for evaluating the comprehensive capabilities of MLLMs.According to their leaderboard, GPT-4V has achieved the best score among all MLLMs.The GPT-4 driven MM-ReAct[21]also performs exceptionally well, particularly in reasoning tasks like mathematics and spatial reasoning.This indicates that MLLMs' reasoning capabilities are heavily influenced by their underlying LLM.Among the open-source models, CogVLM-Chat has achieved the highest score.Compared with other public models, the response of IFT dataset used in CogVLM-Chat are mostly long format and go through further manual correction.Such data aligns better with open-ended evaluation, thus achieved highest score on MM-Vet.MMMU[76]is a benchmark designed to evaluate MLLMs on college-level subject questions.The leaderboard reveals intriguing results: proprietary MLLMs, like Gemini Ultra/Pro[207], GPT-4V[208], and Qwen-VL-PLUS[209]achieved much better scores than most open-source MLLMs.Notably, the scores of these open-source MLLMs are even lower than those of LLMs aided by OCR tools and image captions generated by LLaVA.MLLMs that reserve LLMs' capability, like Blip2/Instruct-Blip and InfiMM-LLaMA-13B achieved good performance, suggesting that MMMU requires model to retain knowledge obtained during LLM training and most other open-source MLLMs may experience catastrophic forgetting of language capability after visual IFT.</p>
<p>Table 6 :
6
Evaluation results for MLLMs before and after instruction tuning on InfiMM-Eval.
MLLMsReasoning Category Deductive Abductive Analogical Moderate Reasoning Complexity HighOverallQwen-VL-7B [112]24.0624.399.0332.3312.6921.32Qwen-VL-7B-Chat [112]32.2548.1824.0346.9022.7833.44
https://pypi.org/project/pillow/
https://opencv.org/</p>
<p>Minds, brains, and programs. John R Searle, Behavioral and brain sciences. 331980</p>
<p>Artificial general intelligence. Ben Goertzel, Cassio Pennachin, 2007Springer2</p>
<p>In two minds: dual-process accounts of reasoning. Jonathan St, B T Evans, Trends in cognitive sciences. 7102003</p>
<p>Dual-process theories of higher cognition: Advancing the debate. Jonathan St, B T Evans, Keith E Stanovich, Perspectives on psychological science. 832013</p>
<p>Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Conference on Computer Vision and Pattern Recognition (CVPR). 2017</p>
<p>Ok-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognition2019</p>
<p>Scienceqa: A novel resource for question answering on scholarly articles. Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, Pushpak Bhattacharyya, International Journal on Digital Libraries. 2332022</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Touchstone: Evaluating vision-language models by language models. Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, Jingren Zhou, 2023</p>
<p>Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs. Shengzhi Li, Nima Tajbakhsh, 2023</p>
<p>Sparkles: Unlocking chats across multiple images for multimodal instruction-following models. Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Collier Nigel, Yutong Lu, arXiv:2308.164632023arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan, 2022</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023</p>
<p>Empowering vision-language models to follow interleaved vision-language instructions. Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, arXiv:2308.04152Siliang Tang, and Yueting Zhuang. 2023arXiv preprint</p>
<p>Multimodal fewshot learning with frozen language models. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, Oriol Eslami, Felix Vinyals, Hill, Advances in Neural Information Processing Systems. 202134</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, arXiv:2204.005982022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Pali-x: On scaling up a multilingual vision and language model. Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, arXiv:2305.185652023arXiv preprint</p>
<p>. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, 2023Palm-e: An embodied multimodal language model</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, 2023</p>
<p>Mm-react: Prompting chatgpt for multimodal reasoning and action. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, 2023</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, 2023</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao, 2023</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, 2023</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang, mplug-owl: Modularization empowers large language models with multimodality. 2023</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, arXiv:2301.13867Mathematical capabilities of chatgpt. 2023arXiv preprint</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.040232023arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark, Gales, arXiv:2303.088962023arXiv preprint</p>
<p>Nature language reasoning, a survey. Fei Yu, Hongbo Zhang, Benyou Wang, arXiv:2303.147252023arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>What is reasoning? what is an argument?. Douglas N Walton, The journal of Philosophy. 8781990</p>
<p>A concise introduction to logic. Craig Delancey, Open SUNY Textbooks. 2017</p>
<p>Informal logic and the theory of reasoning. Maurice A Finocchiaro, Informal Logic. 621984</p>
<p>Reasoning and the logic of things: The cambridge conferences lectures of 1898. Edward H Madden, The Philosophical Review. 10321994</p>
<p>Logical reasoning in formal and everyday reasoning tasks. Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, Martin Goedhart, International Journal of Science and Mathematics Education. 182020</p>
<p>Bradley H Dowden, Logical reasoning. 2018</p>
<p>Admissibility of logical inference rules. Vladimir V Rybakov, 1997Elsevier</p>
<p>Belief rule-base inference methodology using the evidential reasoning approach-rimer. Jian-Bo Yang, Jun Liu, Jin Wang, How-Sing Sii, Hong-Wei Wang, IEEE Transactions on systems, Man, and Cybernetics-part A: Systems and Humans. 200636</p>
<p>. Johnson-Laird Philip, Deductive reasoning. Annual review of psychology. 5011999</p>
<p>Inductive reasoning and kolmogorov complexity. Ming Li, Paul Mb Vitanyi, Journal of Computer and System Sciences. 4421992</p>
<p>Abduction. Igor Douven, 2011</p>
<p>Analogical reasoning: What develops? a review of research and theory. Usha Goswami, Child development. 6211991</p>
<p>Analogy and analogical reasoning. Paul Bartha, 2013</p>
<p>Introduction to logic and critical thinking. Merrilee H Salmon, 2012Cengage Learning</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191Are nlp models really able to solve simple math word problems?. 2021arXiv preprint</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsJune 2016</p>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, Mathqa, arXiv:1905.13319Towards interpretable math word problem solving with operation-based formalisms. 2019arXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.041462017arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Social iqa: Commonsense reasoning about social interactions. Maarten Sap, Derek Hannah Rashkin, Ronan Chen, Yejin Le Bras, Choi, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Commongen: A constrained text generation challenge for generative commonsense reasoning. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih, Choi, International Conference on Learning Representations. 2019</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-ofthought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, arXiv:2205.11502On the paradox of learning to reason from data. 2022arXiv preprint</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018. Revised Selected Papers. Stockholm, SwedenSpringerJuly 13. 2018. 20197</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.037682020arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<p>Babyai: A platform to study the sample efficiency of grounded language learning. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, Yoshua Bengio, International Conference on Learning Representations. 2018</p>
<p>Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Minedojo, Advances in Neural Information Processing Systems. 202235</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Nocaps: Novel object captioning at scale. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. Liwei Bryan A Plummer, Chris M Wang, Juan C Cervantes, Julia Caicedo, Svetlana Hockenmaier, Lazebnik, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models. Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil Mrini, Xudong Lin, Yiqi Wang, Bohan Zhai, Jianbo Yuan, Heng Wang, Hongxia Yang, 2023</p>
<p>Mm-vet: Evaluating large multimodal models for integrated capabilities. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, 2023</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 2023</p>
<p>Hallusionbench: An advanced diagnostic suite for entangled language hallucination &amp; visual illusion in large vision-language models. Fuxiao Tianrui Guan, Xiyang Liu, Ruiqi Wu, Zongxia Xian, Xiaoyu Li, Xijun Liu, Lichang Wang, Furong Chen, Yaser Huang, Dinesh Yacoob, Tianyi Manocha, Zhou, 2023</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, Mathvista, Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint Link</p>
<p>. Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang, 2023Modularized multimodal large language model for document understandingmplug-docowl</p>
<p>Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond. Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Tianyu Liu, Baobao Chang, 2023ArXiv</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, 2023</p>
<p>Mmbench: Is your multi-modal model an all-around player?. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin, 2023</p>
<p>Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, arXiv:2306.13394A comprehensive evaluation benchmark for multimodal large language models. 2023arXiv preprint</p>
<p>Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. Jiong Zhenfei Yin, Jianjian Wang, Zhelun Cao, Dingning Shi, Mukai Liu, Lu Li, Lei Sheng, Xiaoshui Bai, Zhiyong Huang, Jing Wang, Wanli Shao, Ouyang, 2023</p>
<p>Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, Manling Li, Halle-switch: Controlling object hallucination in large vision language models. 2023</p>
<p>Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schimdt, 2023</p>
<p>Seed-bench: Benchmarking multimodal llms with generative comprehension. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, 2023</p>
<p>Tiny lvlm-ehub. Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao, Ping Luo, 2023Early multimodal experiments with bard</p>
<p>What matters in training a gpt4-style language model with multimodal inputs?. Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, Tao Kong, 2023</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Winoground: Probing vision and language models for visio-linguistic compositionality. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross, 2022</p>
<p>Raven: A dataset for relational and analogical visual reasoning. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, Song-Chun Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE Conference on Computer Vision and Pattern Recognition. 2009</p>
<p>. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C Lawrence Zitnick, Piotr Dollár, 2015Microsoft coco: Common objects in context</p>
<p>Lvis: A dataset for large vocabulary instance segmentation. Agrim Gupta, Piotr Dollár, Ross Girshick, 2019</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsJuly 2002</p>
<p>Cider: Consensus-based image description evaluation. Ramakrishna, C Lawrence Vedantam, Devi Zitnick, Parikh, 2015</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJuly 2004</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, 2023</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Fool your (vision and) language model with embarrassingly simple permutations. Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales, 2023</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, arXiv:1906.023612019arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.106252022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.007202022arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2022</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, 2023</p>
<p>Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, 2023</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, A 176b-parameter open-access multilingual language model. 2023</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>GLM: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv:2310.03744Improved baselines with visual instruction tuning. 2023arXiv preprint</p>
<p>Otter: A multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu, 2023</p>
<p>Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Laion-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Advances in Neural Information Processing Systems. 202235</p>
<p>Multimodal c4: An open, billion-scale corpus of images interleaved with text. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang, Wang , Yejin Choi, arXiv:2304.069392023arXiv preprint</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Conceptual 12m: Pushing web-scale imagetext pre-training to recognize long-tail visual concepts. Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, 2021</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1232017</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>A-okvqa: A benchmark for visual question answering using world knowledge. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, European Conference on Computer Vision. Springer2022</p>
<p>Ocr-vqa: Visual question answering by reading text in images. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, Anirban Chakraborty, 2019 international conference on document analysis and recognition (ICDAR). IEEE2019</p>
<p>Textcaps: a dataset for image captioning with reading comprehension. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part II 16</p>
<p>Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, Saehoon Kim, Coyo-700m: Image-text pair dataset. 2022</p>
<p>Yitzhak Samir, Gabriel Gadre, Alex Ilharco, Jonathan Fang, Georgios Hayase, Thao Smyrnis, Ryan Nguyen, Mitchell Marten, Dhruba Wortsman, Jieyu Ghosh, Zhang, arXiv:2304.14108search of the next generation of multimodal datasets. 2023arXiv preprint</p>
<p>Referitgame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014</p>
<p>Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, Teknium, Slimorca: An open dataset of gpt-4 augmented flan reasoning traces. 2023with verification</p>
<p>Im2text: Describing images using 1 million captioned photographs. Vicente Ordonez, Girish Kulkarni, Tamara Berg, Advances in neural information processing systems. 242011</p>
<p>Generation and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Llavar: Enhanced visual instruction tuning for text-rich image understanding. Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, Tong Sun, arXiv:2306.171072023arXiv preprint</p>
<p>Mitigating hallucination in large multi-modal models via robust instruction tuning. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang, arXiv:2306.1456520231arXiv preprint</p>
<p>Dvqa: Understanding data visualizations via question answering. Kushal Kafle, Brian Price, Scott Cohen, Christopher Kanan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Docvqa: A dataset for vqa on document images. Minesh Mathew, Dimosthenis Karatzas, Jawahar, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer vision2021</p>
<p>Chartqa: A benchmark for question answering about charts with visual and logical reasoning. Ahmed Masry, Xuan Do, Jia Long, Shafiq Qing Tan, Enamul Joty, Hoque, arXiv:2203.102442022arXiv preprint</p>
<p>Ai2d-rst: A multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation. Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, John A Bateman, 202155</p>
<p>Openflamingo: An open-source framework for training large autoregressive vision-language models. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Shiori Gadre, Jenia Sagawa, Simon Jitsev, Pang Wei Kornblith, Gabriel Koh, Mitchell Ilharco, Ludwig Wortsman, Schmidt, arXiv:2308.013902023arXiv preprint</p>
<p>Introducing idefics: An open reproduction of state-of-the-art visual language model. Hugo Laurencon, Stas Daniel Van Strien, Leo Bekman, Lucile Tronchon, Thomas Saulnier, Siddharth Wang, Amanpreet Karamcheti, Giada Singh, Yacine Pistilli, Victor Jernite, Sanh, August 2023</p>
<p>Mimic-it: Multi-modal in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, Ziwei Liu, 2023</p>
<p>Mmicl: Empowering vision-language model with multi-modal in-context learning. Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang, 2023</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, Neal J Cohen, Psychology of learning and motivation. Elsevier198924</p>
<p>Grounding language models to images for multimodal inputs and outputs. Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, 2023ICML</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational LinguisticsDeeLIO 2022. May 2022Proceedings of Deep Learning Inside Out</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>From images to textual prompts: Zero-shot visual question answering with frozen large language models. Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Boyang Li, Dacheng Tao, Steven Hoi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Language models with image descriptors are strong few-shot video-language learners. Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Advances in Neural Information Processing Systems. 202235</p>
<p>A survey of embodied ai: From simulators to research tasks. Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, Cheston Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 622022</p>
<p>Habitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on Robot Learning. PMLR2022</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. Dhruv Shah, Błażej Osiński, Sergey Levine, Conference on Robot Learning. PMLR2023</p>
<p>Collaborating with language models for embodied reasoning. Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, Rob Fergus, arXiv:2302.007632023arXiv preprint</p>
<p>Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, Matthew R Walter, arXiv:2306.17840Statler: State-maintaining language models for embodied reasoning. 2023arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Conference on Robot Learning. PMLR2023</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Robotic skill acquisition via instruction augmentation with vision-language models. Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, Jonathan Tompson, arXiv:2211.117362022arXiv preprint</p>
<p>Arun Ahuja, Rob Fergus, and Ishita Dasgupta. Distilling internet-scale vision-language models into embodied agents. Theodore Sumers, Kenneth Marino, arXiv:2301.125072023arXiv preprint</p>
<p>Object scene representation transformer. S M Mehdi, Daniel Sajjadi, Aravindh Duckworth, Mahendran, Filip Sjoerd Van Steenkiste, Mario Pavetic, Leonidas J Lucic, Klaus Guibas, Thomas Greff, Kipf, Advances in Neural Information Processing Systems. 202235</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , arXiv:2303.09014Automatic multi-step reasoning and tool-use for large language models. Art2023arXiv preprint</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, 2023Zhiyuan Liu, and Maosong Sun</p>
<p>. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 2023Augmented language models: a survey</p>
<p>Foundation models for decision making: Problems, methods, and opportunities. Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, Dale Schuurmans, 2023</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023</p>
<p>Creator: Disentangling abstract and concrete reasonings of large language models through tool creation. Chi Cheng Qian, Yi R Han, Yujia Fung, Zhiyuan Qin, Heng Liu, Ji, 2023</p>
<p>Webgpt: Browser-assisted questionanswering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2022</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, 2023</p>
<p>Tool documentation enables zero-shot tool-usage with large language models. Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister, 2023</p>
<p>. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Steven Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Vincent Bosma, Yanqi Zhao, Chung-Ching Zhou, Igor Chang, Will Krivokon, Marc Rusch, Pranesh Pickett, Laichee Srinivasan, Kathleen Man, Meier-Hellstern, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak2022Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben HutchinsonQuoc Le. Lamda: Language models for dialog applications</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, arXiv:2303.11381Mm-react: Prompting chatgpt for multimodal reasoning and action. 2023arXiv preprint</p>
<p>Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, arXiv:2303.164342023arXiv preprint</p>
<p>Simple open-vocabulary object detection with vision transformers. Minderer, Gritsenko, Stone, Neumann, Weissenborn, Dosovitskiy, Mahendran, Arnab, Dehghani, Shen, arXiv:2205.062302022arXiv preprint</p>
<p>Dsfd: dual shot face detector. Jian Li, Yabiao Wang, Changan Wang, Ying Tai, Jianjun Qian, Jian Yang, Chengjie Wang, Jilin Li, Feiyue Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Per-pixel classification is not all you need for semantic segmentation. Bowen Cheng, Alex Schwing, Alexander Kirillov, Advances in Neural Information Processing Systems. 202134</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, International Conference on Machine Learning. PMLR2021</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Adding conditional control to text-to-image diffusion models. Lvmin Zhang, Anyi Rao, Maneesh Agrawala, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, Yu Qiao, Llama-adapter v2: Parameter-efficient visual instruction model. 2023</p>
<p>Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou, mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. 2023</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Gpt-4v(ision) technical work and authors. 2023OpenAI</p>
<p>. Qwen-Vl Team, 2023</p>
<p>Introducing our multimodal models. Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sagnak Taşırlar, 2023</p>
<p>Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang, Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. 2023</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Scaling instruction-finetuned language models</p>
<p>Lucile Hugo Laurençon, Léo Saulnier, Stas Tronchon, Amanpreet Bekman, Anton Singh, Thomas Lozhkov, Siddharth Wang, Alexander M Karamcheti, Douwe Rush, Kiela, arXiv:2306.16527An open web-scale filtered dataset of interleaved image-text documents. 2023arXiv preprint</p>
<p>Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang, arXiv:2307.05222Generative pretraining in multimodality. 2023arXiv preprint</p>
<p>Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, arXiv:2311.07575The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. 2023arXiv preprint</p>
<p>Vx2text: End-toend learning of video-based text generation from multimodal inputs. Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, Lorenzo Torresani, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Towards fast adaptation of pretrained contrastive models for multi-channel video-language retrieval. Xudong Lin, Simran Tiwari, Shiyuan Huang, Manling Li, Mike Zheng Shou, Ji Heng, Shih-Fu Chang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, arXiv:2305.182902023arXiv preprint</p>
<p>Aligning large multimodal models with factually augmented rlhf. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, arXiv:2309.145252023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
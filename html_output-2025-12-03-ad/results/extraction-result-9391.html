<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9391 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9391</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9391</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-504c6f5db886b495103e4a04aaf91a9dd9e4ac60</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/504c6f5db886b495103e4a04aaf91a9dd9e4ac60" target="_blank">LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The practical details of prompting to elicit coherent predictive distributions, and the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions are investigated.</p>
                <p><strong>Paper Abstract:</strong> Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9391",
    "paper_id": "paper-504c6f5db886b495103e4a04aaf91a9dd9e4ac60",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00513925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</h1>
<p>James Requeima<em><br>University of Toronto<br>Vector Institute<br>requeima@cs.toronto.edu<br>John Bronskill ${ }^{</em>}$<br>University of Cambridge<br>jfb54@cam.ac.uk<br>Dami Choi<br>University of Toronto<br>choidami@cs.toronto.edu<br>Richard E. Turner<br>University of Cambridge<br>The Alan Turing Institute<br>ret26@cam.ac.uk<br>David Duvenaud<br>University of Toronto<br>Vector Institute<br>duvenaud@cs.toronto.edu</p>
<h4>Abstract</h4>
<p>Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.</p>
<h2>1 Introduction</h2>
<p>Incorporating prior knowledge into predictive models is highly challenging which can restrict the scope for detailed, context-sensitive analysis. In addition, the skill required to incorporate this prior knowledge into probabilistic modelling can restrict the use of these models to experts. In this work, our objective is to develop a probabilistic prediction model that facilitates user interaction through straightforward, natural language. For this purpose, we explore strategies for eliciting explicit, coherent numerical predictive distributions from LLMs.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Predictive distributions from an LLMP conditioned on both data and text information. The tenth-percentiles from 50 samples are visualized in faded blue and the median is presented in dark blue with five random samples shown in various colours.</p>
<p>Why go to so much effort to elicit predictions from a slow, expensive, and sometimes inconsistent model like an LLM? We expect their hypothesis class to be both rich, and grounded in exactly the kinds of high-level side information that we currently struggle to communicate to our numerical models. For instance, knowing that prices rarely go below zero, that certain kinds of sensors can saturate at particular values, or that trends almost always eventually level off, are easy to express in natural language, but not straightforward to incorporate into a model without getting lost in difficult-to-specify details about aspects of the domain that aren't well understood. To summarize, we want to develop such a model because it would allow users to 1) provide prior, potentially expert, information to the model about the problem setting in plain-language rather than attempting to capture this information in closed form priors (e.g. Gaussian Process kernels) and 2) it would allow users to access problem-relevant latent knowledge encoded in LLMs that users may not have themselves.</p>
<p>LLMs have recently been shown to be able to condition on the particular task being solved, leveraging contextual information to make better predictions or decisions [1]. They have also been shown to competitively predict time series based only on a text tokenization of numerical data [2]. In this work, we further push in both these directions; 1) using LLMs for numerical prediction tasks going beyond one-dimensional time series forecasting to multi-dimensional regression and density estimation and 2) exploring the ability of these models to condition on both numerical data and rich, unstructured text to improve these predictions. In this paper we make the following contributions:</p>
<ul>
<li>We define LLM Processes (LLMPs) using methods we develop for eliciting numerical predictive distributions from LLMs. ${ }^{2}$ LLMPs go beyond one-dimensional time series forecasting to multi-dimensional regression and density estimation. We propose two approaches for defining this joint predictive distribution over a collection of query points and evaluate their compatibility in principle with the consistency axioms necessary to specify a valid statistical process.</li>
<li>We develop effective prompting practices for eliciting joint numerical predictions. We investigate various methods for conditioning LLMs on numerical data, including prompt formatting, ordering, and scaling. We characterize which schemes perform best on a set of synthetic tasks.</li>
<li>We show that LLMPs are competitive and flexible regressors even on messy data. Through an extensive set of synthetic and real world experiments, including image reconstruction and black-box function optimization, we evaluate the zero-shot regression and forecasting performance of LLMPs. We demonstrate that LLMPs have well-calibrated uncertainty and are competitive with Gaussian Processes (GPs), LLMTime [2], and Optuna [3]. We show that LLMPs use in-context learning to automatically leverage information from related datasets, can easily handle missing datapoints, perform image reconstruction, and output multimodal predictive distributions.</li>
<li>Lastly, we demonstrate the ability to usefully incorporate problem-relevant information provided through unstructured text into numerical predictions, visualized in Figure 1, resulting in quantitative structure that reflects qualitative descriptions. Other additions such as labelling features using text and specifying units allow LLMPs to make use of usually-ignored side information.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Sampling from an LLM using either independent marginal or autoregressive sampling.</p>
<h2>2 LLM Processes: Defining a Stochastic Process That Can Condition on Text</h2>
<p>Our goal for this section is to use an LLM to elicit joint predictive distributions over arbitrary sized target sets that we can guide and modify using natural language. Formally, given a set of input and output observations $D_{\text{train}} = {(x_i, y_i)}<em j="1">{i=1}^{N}$ and some text, $T$, we would like to elicit the predictive distribution defined by an LLM at a collection of targets {(x^<em>, y^</em>)}</em>(y^}^N denoted $p_{\text{LLM}<em>, \ldots, y^</em>, | x^<em>, \ldots, x^</em>, D_{\text{train}}, T)$.</p>
<p>Rejection sampling from an LLM allows us to access what we may interpret as the LLM's predictive distribution and gain insights into the model's inductive biases; sampling from the LLM's categorical distribution over text tokens while ignoring non-numerical tokens yields numerical samples from the LLM. The process of sampling from an LLM is depicted in Figure 2 and Algorithm 1. Sample prompts are in Appendix C. Since an accurate sampling-based empirical distribution incurs a high computational cost, next we define an approach to elicit continuous likelihoods from an LLM.</p>
<h3>Continuous Marginal Likelihoods From an LLM</h3>
<p>We approximate a continuous density over our target values by discretizing the space using bins with arbitrarily fine precision, similar to the method used in Gruver et al. [2]. Crucially, this hierarchical approach allows us to compute the probability of a bin with width 10^-n. For example, if $n = 1$ then $\text{Pr}{y \in [1.0, 1.1)} = p(1)p(.|1)p(0|1.)$ because '1.0' is a prefix for all $y \in [1.0, 1.1)$. We can convert probability mass to probability density by assuming a uniform distribution within each bin, and dividing the mass by the bin width. A visualization of this construction is in Figures G.2 to G.4.</p>
<p>Unlike [2], we do not rescale the values to remove decimal places. We hypothesize that such scaling removes prior information communicated to the LLM via the scale of the problem. We examine the effect of scaling values in Section 3. We also differ from [2] by including a terminal token after every value in our prompt – for example, given a terminal token $\langle t \rangle$, we represent 12 as $12\langle t \rangle$. Including a terminal token prevents numbers of varying orders of magnitude to share the same prefix – i.e. $p(1)p(2|1)p(\langle t \rangle|12)$ no longer includes the probability of numbers in [120, 130), [1200, 1300), etc.</p>
<p>Note that this approach does not guarantee that $P(12\langle t \rangle)$ yields the mass assigned by the LLM to values in the bin $[12, 13)$ but we empirically observed that our predictive distribution closely matches the sampling distribution to our satisfaction. See Section G.1 for more details and comparison.</p>
<h3>Defining an LLM Process</h3>
<p>Thus far we have established a procedure defining the predictive distribution at a single target location, $p_{\text{LLM}}(y^<em>, | x^</em>, D_{\text{train}}, T)$. We now outline two methods which we call independent marginal (I-LLMP) and autoregressive (A-LLMP) predictions, for defining the joint predictive distribution over a collection of target points:</p>
<p>$$p_{\text{I-LLMP}}(y^<em>, \ldots, y^</em>, | x^<em>, \ldots, x^</em>, D_{\text{train}}, T) = \prod_{n=1}^{N} p_{\text{LLM}}(y^<em>, | x^</em>, D_{\text{train}}, T) \tag{1}$$</p>
<p>$$p_{\text{A-LLMP}}(y^<em>, \ldots, y^</em>, | x^<em>, \ldots, x^</em>, D_{\text{train}}, T) = \prod_{n=1}^{N} p_{\text{LLM}}(y^<em>, | y^</em>, \ldots, y^n_{n-1}, x^*, \ldots, x^n_{n}, D_{\text{train}}, T) \tag{2}$$</p>
<p>We note that Equation (1) satisfies the Kolmogorov Extension Theorem [4] therefore defining valid stochastic process (see Appendix A.3). However, it assumes conditional independence given the training set and model weights and the stochastistity represented by the model is via independent marginals. Equation (2) takes inspiration from the autoregressive structure of the LLMs predictive distribution and should yield much richer predictive distributions as we are now able to model</p>
<p>dependencies between output variables. However, this definition is no longer guaranteed to give us a valid stochastic process as the predictive distribution is now target order dependent and will likely fail the Kolmogorov exchangability condition. We investigate both of these questions in Section 3.</p>
<p>Connection to Neural processes Neural Processes (NPs) [5] are a class of meta-learning models parametrized by neural networks and trained to learn a map from training (context) sets to predictive distributions, $p_{\theta}(y_{1}^{<em>},\ldots,y_{N}^{</em>} \mid x_{1}^{<em>}, \ldots, x_{N}^{</em>}, D_{\text {train }})$. The definitions in Equations 1 and 2 take inspiration from the joint distributions defined by Conditional NPs [5] as independent marginals conditioned on the training/context set and Autoregressive NPs [6] utilizing the chain rule of probability, respectively. Through this lens, LLMPs can be viewed as examples of NPs. However, NPs are directly trained to output this predictive distribution where as LLMPs are repurposing pretrained LLMs.</p>
<p>Multi-dimensional Density Estimation and Handling Missing Data. We highlight that, through the flexibility of the LLM prompt, we do not have to draw a distinction between which variables, or variable dimensions are to be modelled or conditioned and can easily handle missing values. Suppose we have a collection of variables $\left{x_{1}, \ldots, x_{n}\right}$ and $\left{y_{1}, \ldots, y_{m}\right}$ (or more), some subset of which we would like to regress on (including $x$ and $y$-values) and the remainder we wish to condition on. To do so using an LLMP, we simply construct the training prompt such that the variables we would like to regress on occur at the end of the prompt and are blank (generated) when sampling from the LLMP. If any values are missing they can simply be removed from the prompt.</p>
<h1>3 LLMP Configuration</h1>
<p>Experiment Details. In all of the experiments in Sections 3 to 5, we use six different open source LLMs: Mixtral $8 \times 7$ B, Mixtral-8×7B-Instruct [7], Llama-2 7B, Llama-2 70B [8], Llama-3 8B, and Llama-3 70B [9]. Note that we never modify the LLM parameters via training or fine-tuning, we use only prompting. Our primary metrics are negative log probabilities (NLL) of the model evaluated at the true function values $f\left(x^{<em>}\right)$ averaged over the target locations and Mean Absolute Error (MAE) between the predictive median and the true function value. Unless otherwise stated, we use 50 samples from the LLM at each target location $x^{</em>}$ and compute the median and the $95 \%$ confidence interval of the sample distribution. Details of the datasets are given in Appendix D. Since the LLMs used in our experiments have undisclosed training sets, we address the steps taken to mitigate the issue of data-leakage in Appendix E. Additional implementation details and processing times are in Appendix F.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: NLL and MAE for various prompt formats ordered from the most to least token efficient (left), training data orderings (middle), and prompt $y$-scaling (right) using the Mixtral-8×7B LLM. The height of each bar is the mean of 10 random seeds that determine the training point locations. The vertical black lines indicate the standard error. In the Prompt Formatting legend (left), the two '_' characters indicate the positions of the $x$ and $y$ values and in represents a new line terminal token.</p>
<p>Prompt Engineering. We perform a set of experiments for determining the best LLMP prompt configuration. We use the Sigmoid, Quadratic, and Linear+Cosine functions with 10, 20 and 75 training points, respectively (see Appendix D.1) with I-LLMP using the Mixtral-8×7B LLM.</p>
<ul>
<li>Prompt Formatting Two separators are required to achieve the best performance. One to separate the $x$ and $y$ values within a pair and another to separate the $x, y$ pairs. Figure 3 (left) demonstrates that $__$n is the best option in terms of performance and token efficiency.</li>
<li>Prompt Ordering Figure 3 (middle) shows that ordering the training points by distance to the current target point is best, outperforming both random and sequential ordering. We posit that ordering by distance provides a hint to the LLM to weigh the contribution of closer training points to the current target point to a greater degree.</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Autoregressive Experiments. <em>Left:</em> NLL and MAE for A-LLMP and I-LLMP using different prompt orderings using the Mixtral-8x7B LLM. The height of each bar is the mean of 3 random seeds that determine the training point locations. The black lines indicate the standard error. <em>Center:</em> Log-likelihood results of using various test set orderings with Llama-2-7B, Llama-2-70B and Mixtral-8x7B A-LLMP. The orange X indicates I-LLMP, the purple circles used distance ordered test points, and the blue whiskers are the mean and standard error of 10 randomly sampled test orderings. The red dashed line shows the log-likelihood of the test set under the generative process. <em>Right:</em> Heatmap visualization of the Llama-3-70B A-LLMP predictive distribution conditioned on data from a bimodal generative process. Black dots are training points.</p>
<ul>
<li><em>Prompt y-Scaling</em> Figure 3 (<em>right</em>) shows that performance degrades as the range of the y components of the training points increases and when incorporating negative values. This is due to the fact that when the range is wider, the LLM must accurately generate more numerical digits and potentially a negative sign when predicting $$f(x^*)$$.</li>
<li><em>top-p and Temperature</em> Figure G.9 shows that performance is surprisingly insensitive to varying the LLM nucleus sampling parameter top-<em>p</em> [10] and LLM softmax temperature.</li>
</ul>
<p><strong>Autoregressive vs Independent Marginal Predictions.</strong> Here we examine two questions: first, does the autoregressive defininiton of the joint predictive likelihood (A-LLMP) in Equation (2) improve performance versus the independent marginal definition of Equation (1) (I-LLMP). Second, "how close" is A-LLMP to a stochastic process in terms of performance variability across query orderings.</p>
<p>We first look at log-likelihoods and MAE for A-LLMP and I-LLMP using the random and distance training point orderings discussed earlier. Results can be seen in Figure 4 (<em>left</em>). Similar to our findings earlier, ordering the training values according to distance to target has a large effect, improving performance for both I-LLMP and A-LLMP. Unsurprisingly, the richer joint distribution given by A-LLMP gives us better predictive performance.</p>
<p>We next examine the variability in performance of A-LLMP when different autoregressive target orderings are used to get a sense of how far our method is from a stochastic process (which would be permutation invariant in the target points). The results of using ten sets of randomly ordered target points compared to I-LLMP and the ground truth log-likelihood of the test sample under the generative distribution are presented in Figure 4 (<em>center</em>). Note that the training data is distance sorted in all cases. We also present the result when ordering target points according to distance to the closest training point, from smallest to largest. We make three key observations: first, log-likelihood performance of all A-LLMP orderings is better than I-LLMP. Second, the variance of random orderings is small on the scale of the log-likelihood of the generative model. And third, distance ordering the targets gives better or at least competitive performance with a random ordering. These results present practitioners a choice: do you care more about using a valid statistical process or obtaining good predictive performance? If it is the latter, you would be better served using A-LLMP.</p>
<h1>4 Evaluating LLMP Performance on Numerical Data</h1>
<p>In this section, we evaluate the performance of LLMPs on purely numerical data in a wide variety of settings. Additional details and results for experiments in this section can be found in Appendix H.</p>
<p><strong>1D Synthetic Data Experiments.</strong> To show that LLMPs are a viable regression model with well-calibrated uncertainties, we benchmark in Table 1 our A-LLMP method against a GP on the Function Dataset (Appendix D.1). The GP uses an RBF kernel with optimized length scale and noise. The Mixtral-8×7B A-LLMP achieves the lowest negative log-likelihoods averaged over 7 function sizes and 3 seeds on 10 out of 12 of the functions and equal or better MAE on 8 of the functions. Visualizations of the predictive distributions and plots of MAE and A-LLMP are shown in Appendix H.1.</p>
<p>Table 1: Mean and standard error of MAE and NLL averaged over over the seven training set sizes and 3 seeds of each function for Mixtral-8$\times$7B A-LLMP and a GP with an RBF kernel.</p>
<p>| Function | | | | | | | Function | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>Black-Box Function Optimization Black-box optimization involves minimizing or maximizing a function where there is only access to the output of a function for a specified input. We benchmark the ability of LLMPs to perform maximization on six commonly used multi-dimensional functions. We compare our results using Llama-2-7B to Optuna [3], a commercial hyperparameter optimization framework. Results and implementation details are in Appendix H.5. In all cases, LLMPs obtain as good or better approximation to the true maximum value in a fewer number of trials.
Simultaneous Temperature, Rainfall, and Wind Speed Regression To examine how well an LLMP can model multi-dimensional outputs, we compare LLMP regression to a multi-output GP on the weather dataset described in Appendix D.2. Figure 7 shows the results for the Llama-3-8B LLM (top) and a 3 output RBF kernel GP with trained hyperparameters (bottom). The LLM is similar to and in most cases better than the GP in terms of MAE and NLL.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Results for simultaneously predicting temperature, precipitation, and wind speed using the Llama-3-7B LLM (top) and a 3 output RBF kernel GP with trained hyperparameters (bottom).</p>
<p>In-context Learning Using Related Data Examples. In this experiment, we investigate LLMPs' ability to learn from similar examples in-context to predict average monthly precipitation across 13 Canadian locations [13], one from each province and territory. For each location, we use the Mixtral-8×7B A-LLMP to forecast 32 months of average precipitation values given the previous four month observations taken from a random historical three-year period between 1913-2017 (conditional on data availability). It is then provided with 1-12 examples of random three year periods of historical values from the same location in-context. Results shown in Figure 8 and experimental details in Appendix H.6. Conditioning the LLMP on historical examples improves performance saturating after 4 years, and degrading slightly thereafter. Generally, the LLMP is able to use the examples to pick up on seasonal trends from history. We note that some locations do not have obvious or strong seasonal patterns but examples still help performance in these cases (see Appendix H.6).</p>
<h1>5 Conditioning LLMPs on Textual Information</h1>
<p>One of the most exciting directions of LLMPs is the potential to incorporate prior information about problems via text. Now that we can examine functional predictive distributions of LLMs, we can begin to explore their rich prior over functions by conditioning on both text and numerical data. In this section we present two experiments with details and additional experiments presented in Appendix I.
Scenario-conditional Predictions. In this experiment, we examine the influence of text providing information about various synthetic problem settings on the predictive distribution of an LLMPs. In
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: (Left three plots) Visualizations of the predictions given by the Mixtral-8×7B LLMP for Ranfurly, Alberta. Blue and black circles are training and test points, respectively. Red circles are median predictions and shaded areas indicate tenth-percentiles over 30 samples. (Right) NLL vs number of examples. Error bars show standard error over 13 locations.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: a)-e) predictive distributions from an A-LLMP using Llama-3-70B under various scenario prompts. Black points are two training points given to the LLM process, the same values for each scenario. The tenth-percentiles from 50 samples are visualized in faded blue and the median is presented in dark blue with five random samples shown in various colours. Figure f) shows the actual average monthly rainfall for Singapore from 1991-2020 [14] and San Diego from 2000-2024 [15].</p>
<p>Figure 9: a)-e) predictive distributions from an A-LLMP using Llama-3-70B under various scenario prompts. Black points are two training points given to the LLM process, the same values for each scenario. The tenth-percentiles from 50 samples are visualized in faded blue and the median is presented in dark blue with five random samples shown in various colours. Figure f) shows the actual average monthly rainfall for Singapore from 1991-2020 [14] and San Diego from 2000-2024 [15].</p>
<p>all of the following examples, we provide the same two synthetic training points to the LLMP but change the prompting text that comes before the training data. We then use A-LLMP with Llama-3-70B to forecast trajectories 50 steps ahead. We begin by examining the predictive distribution with no prompt (Figure 9a). We prompt the LLMP to generate daily temperature measurements in degrees Celsius from Montreal in January (Figure 9b) and May (Figure 9c), and monthly precipitation values from San Diego, CA (Figure 9d) and Singapore (Figure 9e). Figure 1 Shows the results of prompting the LLMP to generate (left) a stock price financial time series (centre) for a company that eventually goes out of business and (right) for a company whose price goes to zero on day 30.</p>
<p>Indeed, the LLMP modifies the predictive distribution accordingly relative to the no prompt predictions. We highlight the following observations: first, for prompts b) and c), the model moves about half of its predictive mass below zero for temperatures beginning in January and above zero for the May temperatures. Second, the LLMP is able to recall actual historical trends for average monthly precipitation for Singapore and San Diego to condition on prompts d) and e). Despite getting the trend correct, we note that the median prediction in d) seems to be biased toward the training values and not reflective of the actual monthly median.</p>
<p>Last, for stock price simulations, the model places all of its density on positive numbers since it is modelling prices. It is able to produce realistic trajectories and decreases them in expectation when prompted that the company goes out of business. The model is able to condition on the fact that the price goes to zero on day 30 which correctly interprets the meaning of the $x$-values as days starting from 0, that the $y$-axis is the price and the phrase “price goes to zero” corresponds to a $y$-value of 0.</p>
<p>Labelling Features Using Text. In the following example, we examine the performance of a Mixtral-8x7B Instruct I-LLMP on predicting American housing prices. The dataset [16] contains 39980 housing prices and various variables around housing and demographics for the top 50 American cities by population. Note that this dataset was generated on 12/09/2023, however it contains data from the 2020 US Census and the 2022 American Community Survey (ACS) so we cannot guarantee that models did not see data within this dataset during training.</p>
<p>For each prediction task, we show the I-LLMP 10 randomly selected training examples from the dataset and predict on 20 randomly selected test examples. In the prompt, before the numerical value (price) we provide a string which encodes the datapoint index/features that the model can use. For our first experiment we examine the behaviour of the LLMP when more features are added to the prompt. We experiment with five ways of indexing the training and test points; For case (1), we provide latitude and longitude of the house as numerical values (eg. 32.74831, -97.21828) converted to strings similar to our method in previous experiments. For the remaining 4 cases, we provide additional labeled features, adding more features for each case with the prompt for case (5) containing</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: Results of a Mixtral-8x7B Instruct I-LLMP predicting US housing prices. Left: Predictions for 10 randomly selected houses using index style 1) and 5). Xs are mean predictions using 30 samples from the LLMP and error bars indicate 2 standard deviations. Centre and right: Average MAE and NLL performance of the LLMP over 10 experiments with error bars representing the standard error for experiments from Section 5.
all labelled features, illustrated with the following example: (2) Location: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, (3) Zip Code: 76112, Median Household Income: 71452.0, (4) Zip Code Population: 42404 people, Zip Code Density: 1445.0 people per square mile, (5) Living Space: 1620 square feet, Number of Bedrooms: 3, Number of Bathrooms: 2.
This procedure is repeated 10 times to compute statistics. Results are presented in Figure 10 (left, centre). Note that the LLMP is able to take advantage of the additional features provided to improve predictive performance. To see examine the effect of adding text labels to the features, we ran another set of experiments on 10 new random datasets providing the LLMP with either labeled or unlabelled numerical features. The following are example feature strings: (i) "30.45738, -97.75516" (ii) "Location: Austin, Texas, Latitude: 30.45738, Longitude: -97.75516" (iii) "30.45738, -97.75516, 78729, 107830.0, 30907, 1216.1, 1349, 3" (iv) "Location: Austin, Texas, Latitude: 30.45738, Longitude: -97.75516, Zip Code: 78729, Median Household Income: 107830.0, Zip Code Population: 30907 people, Zip Code Density: 1216.1 people per square mile, Living Space: 1349 square feet, Number of Bedrooms: 3, Number of Bathrooms: 2". Results of this experiment are presented in Figure 10 (right). Note that the LLMP is not able to use the raw feature values to improve performance from only 10 training examples, but is able to do so with labelled features suggesting that LLM is able to utilize the latent relationship between the feature and the price once the feature is identified. We found that the Mixtral-8×7B Instruct model had the best performance on this task and was able to utilize text information better (results for other models in Appendix I.2).</p>
<h1>6 Related Work</h1>
<p>In this section, we discuss work related to eliciting distributions from LLMs including forecasting, regression, in-context learning, and nearal processes among others.
LLM Forecasting The most closely related work to ours is LLMTime [2]. LLMTime is capable of zero-shot extrapolation of one-dimensional time series data at a level comparable to trained purposebuilt approaches. In addition, they develop a method for eliciting marginal probability distribution functions from LLM posteriors over functions, which we build on. They also begin to investigate the effect of conditioning on text. In contrast, we focus on (i) interpolation with multi-dimensional inputs and outputs; (ii) eliciting joint distributions over functions, not just marginals; and (iii) exploring the ability of models to condition simultaneously on both numerical data and text. More recently, TimesFM [17], a foundation model for one-dimensional zero-shot times series forecasting was introduced. However, TimesFM does not support interpolation or higher dimensional data and does not consider distributions. PromptCast [18] performs zero-shot time series forecasting by combining numerical data and text in a question answer format. Our approach for combining problem specific text along with numerical data differs in that it handles both interpolation and extrapolation and does not rely on a question-answer format. Hegselmann et al. [19] utilize LLMs to do zero-shot and few-shot classification on tabular data that compares favorably to standard ML approaches.
LLM Regression Pesut [20] do some initial investigations into the use of LLMs as regressors on 1D synthetic functions. Our work greatly expands on these early investigations. Vacareanu et al. [11] is concurrent work that shows that LLMs are capable linear and non-linear regressors. However, their work does not condition on any textual information, compute log probabilities, compare to Gaussian Processes, investigate the effect of prompt formatting, or employ auto-regressive sampling.</p>
<p>In-context learning (ICL) in LLMs Xie et al. [21] point out that ICL can be seen as being equivalent to Bayesian inference in a latent variable model. More recently, [22] explain in-context learning in LLMs as kernel regression. Garg et al. [23] train transformers to do in-context learning on various function classes including linear (up to 50 dimensions), decision trees, and two-layer ReLU networks. Coda-Forno et al. [24] demonstrate that LLMs are capable of meta-in-context learning and that performance on 1-D linear regression and two-armed bandit tasks improves with multiple examples. TabPFN [25] is a trained transformer that is able to do tabular classification given in-context examples.</p>
<p>LLM Hyperparameter Optimization Zhang et al. [26] and Liu et al. [27] use LLMs to perform hyperparameter optimization, showing that LLMs can condition on a mixture of textual data as numerical observations to effectively optimize hyperparameters in machine learning models.</p>
<p>Eliciting priors from LLMs Binz and Schulz [28] fine-tune LLMs on data from psychological experiments to achieve accurate representations of human behavior. Choi et al. [1] show how using an LLM to assess the importance of features or the causal relationship between variables that can improve performance on tasks. Lipkin et al. [29] find that LLMs can derive human-like distributions over the interpretations of complex pragmatic utterances.</p>
<p>Eliciting distributions from humans Schulz et al. [30] look at compositional inductive biases in function learning, showing humans have compositional structure in their priors on functions. [31] catalogue standard strategies for eliciting distributions from expert humans.</p>
<p>Neural processes Neural Processes are a class of meta-learning models trained to learn a map from training (context) sets to predictive distributions, $p_{\theta}\left(y_{1}^{<em>}, \ldots, y_{N}^{</em>} \mid x_{1}^{<em>}, \ldots, x_{N}^{</em>}, D_{\text {train }}\right)$. These models are parameterized using a neural network and there have been various proposals for different architectures using attention [32], transformers [33], Gaussian Process output layers [34], and diffusion models [35]. The definitions of the joint distributions in equations 1 and 2 take inspiration from the joint distributions defined by Conditional Neural Processes [5] as independent marginals conditioned on the training/context set and Autoregressive Neural Processes [6] utilizing the chain rule of probability, respectively. Through this lens, LLMPs can be viewed as examples of Neural Processes. LLMPs differ from standard NPs in two main ways: (i) Training objective: Neural Processes are meta-trained using maximum likelihood to optimize $p\left(y^{<em>}\left|x^{</em>}, D_{\text {train }}\right)\right.$ directly. LLMPs have a very indirect training procedure - they are trained to be language models i.e. autoregressive token predictors. One of the contributions of this paper is the demonstration that, despite this, they can perform zero-shot probabilistic regression. (ii) Architecture: NPs have an output layer that parametrizes the predictive distribution over targets directly. Since LLMPs are repurposing language models for regression, we need to define the mapping from distributions over language tokens to distributions over target variables. We note that LLMs themselves can be viewed as AR-CNPS [6] with a fixed, predefined target ordering.</p>
<h1>7 Discussion, Limitations, and Societal Impact</h1>
<p>Below we discuss our findings, the limitations and societal impact of the work presented. Further discussion on these issues can be found in Appendix J.</p>
<p>Discussion We defined LLMPs for eliciting numerical predictive distributions from LLMs and when used as a zero-shot muti-dimensional regression model are competitive with GPs. Excitingly, we demonstrated the ability to condition on text to improve predictions and probe the LLMs' hypothesis space. An interesting extension would be to condition on other modalities in addition to text.</p>
<p>Limitations Along with the flexibility of LLMs, LLMPs inherit their drawbacks. Maximum context sizes limit the size of tasks we can apply this method to and the amount of textual information we can condition on. LLMPs are also significantly more computationally expensive compared to Gaussian Processes and standard regression methods. All of experiments were performed on readily available open source LLMs that are smaller and generally less capable compared to proprietary LLMs.</p>
<p>Societal Impact Our work has demonstrated a new and useful zero-shot approach for generating probabilistic predictions using plain language to augment numerical data. It has the potential to allow practitioners from fields such as medical research and climate modelling to more easily access probabilistic modelling and machine learning. Like all machine learning technology, there is potential for abuse, and possible consequences from incorrect predictions made with LLMPs. Also, we do not know the biases in the underlying LLMs used and what effect they may have on LLMPs output.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>James Requeima and David Duvenaud acknowledge funding from the Data Sciences Institute at the University of Toronto and the Vector Institute. Dami Choi was supported by the Open Phil AI Fellowship. John Bronskill is supported by EPSRC grant EP/T005386/1. Richard E. Turner is supported by Google, Amazon, ARM, Improbable, EPSRC grant EP/T005386/1, and the EPSRC Probabilistic AI Hub (ProbAI, EP/Y028783/1).
We thank Anna Vaughan for help with the weather datasets and discussions. We also thank Will Tebbutt, Matthew Ashman, Stratis Markou, and Aristeidis Panos for helpful comments and suggestions.</p>
<h2>References</h2>
<p>[1] Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. Lmpriors: Pre-trained language models as task-specific priors. arXiv preprint arXiv:2210.12530, 2022.
[2] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. arXiv preprint arXiv:2310.07820, 2023.
[3] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pages 2623-2631, 2019 .
[4] Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer Science \&amp; Business Media, 2013.
[5] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International conference on machine learning, pages 1704-1713. PMLR, 2018.
[6] Wessel P Bruinsma, Stratis Markou, James Requiema, Andrew YK Foong, Tom R Andersson, Anna Vaughan, Anthony Buonomo, J Scott Hosking, and Richard E Turner. Autoregressive conditional neural processes. arXiv preprint arXiv:2303.14468, 2023.
[7] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
[8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[9] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md.
[10] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.
[11] Robert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, and Mihai Surdeanu. From words to numbers: Your large language model is secretly a capable regressor when given in-context examples. arXiv preprint arXiv:2404.07544, 2024.
[12] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
[13] Environment and Climate Change Canada. Monthly total of daily adjusted total precipitation. Online, 2024. URL https: //www.canada.ca/en/environment-climate-change/services/ climate-change/science-research-data/climate-trends-variability/ adjusted-homogenized-canadian-data/precipitation-access.html. Accessed: April 2024, Last updated: 2017-08-09.</p>
<p>[14] University of East Anglia Climatic Research Unit. Observed Historical Climate Data for Singapore. World Bank Climate Knowledge Portal, 2024. URL https://climateknowledgeportal.worldbank.org/country/singapore/ climate-data-historical. Accessed: 2024-05-06.
[15] Climate Data. National Weather Service, 2024. URL https://www.weather.gov/wrh/ Climate?wfo=sgx. Accessed: 2024-05-06.
[16] Jeremy Larcher. American house prices, 2023. URL https://www.kaggle.com/dsv/ 7162651 .
[17] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688, 2023.
[18] Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.
[19] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pages 5549-5581. PMLR, 2023.
[20] Lovre Pesut. Who models the models that model models? an exploration of gpt-3's in-context model fitting ability. URL https://www. alignmentforum. org/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of, 2022.
[21] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
[22] Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. Explaining emergent in-context learning as kernel regression. arXiv preprint arXiv:2305.12766, 2023.
[23] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022.
[24] Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, and Eric Schulz. Meta-in-context learning in large language models. arXiv preprint arXiv:2305.12907, 2023.
[25] Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer that solves small tabular classification problems in a second. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=cp5PvcI6w8_.
[26] Michael Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, and Jimmy Ba. Using large language models for hyperparameter optimization. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.
[27] Tennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela van der Schaar. Large language models to enhance bayesian optimization. arXiv preprint arXiv:2402.03921, 2024.
[28] Marcel Binz and Eric Schulz. Turning large language models into cognitive models. arXiv preprint arXiv:2306.03917, 2023.
[29] Benjamin Lipkin, Lionel Wong, Gabriel Grand, and Joshua B Tenenbaum. Evaluating statistical language models as pragmatic reasoners. arXiv preprint arXiv:2305.01020, 2023.
[30] Eric Schulz, Joshua B Tenenbaum, David Duvenaud, Maarten Speekenbrink, and Samuel J Gershman. Compositional inductive biases in function learning. Cognitive psychology, 99: $44-79,2017$.
[31] Bogdan Grigore, Jaime Peters, Christopher Hyde, and Ken Stein. Methods to elicit probability distributions from experts: a systematic review of reported practice in health technology assessment. Pharmacoeconomics, 31:991-1003, 2013.</p>
<p>[32] Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. Attentive neural processes. arXiv preprint arXiv:1901.05761, 2019.
[33] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. arXiv preprint arXiv:2207.04179, 2022.
[34] Stratis Markou, James Requeima, Wessel Bruinsma, and Richard Turner. Efficient gaussian neural processes for regression. arXiv preprint arXiv:2108.09676, 2021.
[35] Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. Neural diffusion processes. In International Conference on Machine Learning, pages 8990-9012. PMLR, 2023.
[36] OpenWeather. Weather API, 2024. URL https://openweathermap.org/api. Accessed: 2024-03-07.
[37] Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances in Neural Information Processing Systems, 2018.
[38] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285-294, 1933.
[39] William R Thompson. On the theory of apportionment. American Journal of Mathematics, 57 (2):450-456, 1935.
[40] Matthew W. Hoffman and Bobak Shahriari. benchfunk. https://github.com/mwhoffman/ benchfunk, 2015.
[41] Robert B Gramacy and Herbert KH Lee. Cases for the nugget in modeling computer experiments. Statistics and Computing, 22:713-722, 2012.</p>
<h1>A LLM Processes: Defining a Stochastic Process That Can Condition on Text</h1>
<p>In this section we elaborate on the explanations and definitions in Section 2 Our goal is to use an LLM to elicit joint predictive distribution over arbitrary sized target sets that we can guide and modify using plain language. Formally, given a set of observations $D_{\text {train }}=\left{\left(x_{i}, y_{i}\right)\right}<em j="j">{i=1}^{M}$ and some text, $T$, we would like to elicit the predictive distribution defined by an LLM at a collection of targets $\left{\left(x</em>^{<em>}, y_{j}^{</em>}\right)\right}<em _mathrm_LLM="\mathrm{LLM">{j=1}^{N}$ denoted $p</em>^{}}\left(y_{1<em>}, \ldots, y_{N}^{</em>} \mid x_{1}^{<em>}, \ldots, x_{N}^{</em>}, D_{\text {train }}, T\right)$. To achieve the goal, we can can keep in mind two interpretations of what we mean by a predictive distribution defined by an LLM. First, we can interpret the LLM as maintaining having a predictive distribution over numerical values, which we can probe by sampling from the LLM. This interpretation is beneficial if we believe that the LLM has learned useful prior information that we would like to access via its beliefs about these numerical values and for our goal of guiding the predictive distribution using text. The other interpretation is more empirical: we simply use the LLM as a tool to define a valid predictive distribution and evaluate how well this definition performs on test cases. Our approach is a combination of the two philosophies - we will propose a method defining a predictive distribution that is valid and performs well on test cases, but closely matches what we think of as the LLM's underlying distribution.</p>
<h2>A. 1 Continuous Marginal Likelihoods From an LLM</h2>
<p>As discussed in Section 2, we use a method similar to the one proposed by Gruver et al. [2]; we approximate the continuous density by discretizing the space using bins with arbitrarily fine precision. Let's assume a fixed number of decimal places $n$, and that LLMs generate one digit at a time ${ }^{3}$. The key idea is that each new digit can be viewed as being generated from a categorical distribution with the probabilities $p$ given by a softmax over numerical tokens. Crucially, this hierarchical approach allows us to compute the probability of a bin with width $10^{-n}$. For example, if $n=1$ then $\operatorname{Pr}{y \in[1.0,1.1)}=p(1) p(. \mid 1) p(0 \mid 1$.$) because ' 1.0$ ' is a prefix for all $y \in[1.0,1.1)$. We can convert probability mass to probability density by assuming a uniform distribution within each bin, and dividing the mass by the bin width. A visualization of this construction can be viewed in Appendix G.1.
The method in [2] has two main shortcomings for our purposes: first, the authors propose to scale all $y \in D_{\text {train }}$ to eliminate decimals from their numerical representation. For example, for a precision of 2 decimal places, the numbers $0.123,1.23,12.3$, and 123.0 will be transformed to $12,123,1230$, and 12300 respectively. Scaling removes prior information communicated to the LLM via the scale of the problem. For example, it is likely that the LLM has encountered financial data with decimal places. Potentially, it also makes it more difficult to communicate prior information about the problem to the LLM via text.</p>
<p>Second, probabilities of all sequences of integers given by an LLM contain the mass of all values that also start with that sequence. We can think of this as the problem of not knowing when the LLM intends to terminate a value. For example, if $y=12, \operatorname{Pr}{y \in[12,13)} \neq p(1) p(2 \mid 1)$ since $p(1) p(2 \mid 1)$ includes the probability of all numbers with ' 12 ' as a prefix - this includes [12, 13) but also $[120,130),[1200,1300)$ and so on.</p>
<h2>A. 2 The LLM Process Method</h2>
<p>We follow Gruver et al. [2] and discretize the continuous space with bins of width $10^{-n}$, computing the probabilities for each bin using the hierarchical softmax approach. However, different from their approach we 1) keep values at their original scale, and 2) include a terminal token after every value - for example, given a terminal token $\langle t\rangle$, we represent 12 as $12\langle t\rangle$ and 120 as $120\langle t\rangle$. Including a terminal token prevents numbers of varying orders of magnitude from sharing the same prefix - i.e. $p(1) p(2 \mid 1) p(\langle t\rangle \mid 12)$ no longer includes the probability of numbers in [120, 130), [1200, 1300), and so on. After we compute the mass of a bin via hierarchical softmax, we divide the mass by the bin width $10^{-n}$ to get an estimate of the density value. This procedure defines a valid predictive distribution over y-values, and we call this elicitation method 'logit-based' since we derive probabilities from the logits directly instead of sampling. Pseudocode can be found in Algorithm 2.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>It must be noted that this approach does not guarantee that $P(12\langle t\rangle)$ yields the mass assigned by the LLM to values in the bin $[12,13)$. However, we note that our method defines a valid predictive distribution and we empirically observed that our predictive distribution closely matches the sampling distribution to our satisfaction (see Appendix G.1).</p>
<h1>A. 3 Defining an LLM Process</h1>
<p>So far we have established a procedure for defining the predictive distribution at a single target location, $p_{\mathrm{LLM}}\left(y_{n}^{<em>} \mid x_{n}^{</em>}, D_{\text {train }}, T\right)$. We now discuss how to define the joint predictive distribution over a collection target points. In particular, we would like to define a stochastic process via its finitedimensional marginal distributions $\rho_{x_{1}, \ldots, x_{N}}$ defined over locations $x_{1}, \ldots, x_{N}$. The Kolmogorov Extension Theorem [4] states that such a collection defines a stochastic process if it satisfies</p>
<ol>
<li>Exchangeability: Given any permutation $\pi$ of the integers ${1, \ldots, N}$</li>
</ol>
<p>$$
\rho_{x_{1}, \ldots, x_{N}}\left(y_{1}, y_{N}\right)=\rho_{x_{\pi(1)}, \ldots, x_{\pi(N)}}\left(y_{\pi(1)}, y_{\pi(N)}\right)
$$</p>
<ol>
<li>Consistency: if $1 \leq M \leq N$ then</li>
</ol>
<p>$$
\rho_{x_{1}, \ldots, x_{M}}\left(y_{1}, \ldots, y_{M}\right)=\int \rho_{x_{\pi(1)}, \ldots, x_{\pi(N)}}\left(y_{\pi(1)}, y_{\pi(N)}\right) \mathrm{d} y_{M+1} \ldots \mathrm{~d} y_{N}
$$</p>
<p>In Equation (1) we define a collection of joint distributions by defining a factorized distribution over target locations $x_{1}^{<em>}, \ldots, x_{N}^{</em>}$ :</p>
<p>$$
p_{\mathrm{I}-\mathrm{LLMP}}\left(y_{1}^{<em>}, \ldots, y_{N}^{</em>} \mid x_{1}^{<em>}, \ldots, x_{N}^{</em>}, D_{\text {train }}, T\right)=\prod_{n=1}^{N} p_{\mathrm{LLM}}\left(y_{n}^{<em>}, \mid x_{n}^{</em>}, D_{\text {train }}, T\right)
$$</p>
<p>where $p_{\mathrm{LLM}}\left(y_{n}^{<em>}, \mid x_{n}^{</em>}, D_{\text {train }}, T\right)$ is defined above.
This definition satisfies the Kolmogorov Extension Theorem and so it defines a valid stochastic process. However, it assumes conditional independence given the training set and model weights and, conditional on these variables, the stochastistity represented by the model is via independent marginals. Taking inspiration from the autoregressive structure of the LLMs predictive distribution, we can write the joint distribution according to the product rule:</p>
<p>$$
p_{\mathrm{A}-\mathrm{LLMP}}\left(y_{1}^{<em>}, \ldots, y_{N}^{</em>} \mid x_{1}^{<em>}, \ldots, x_{N}^{</em>}, D_{\text {train }}, T\right)=\prod_{n=1}^{N} p_{\mathrm{LLM}}\left(y_{n}^{<em>} \mid y_{1}^{</em>}, \ldots, y_{n-1}^{<em>}, x_{1}^{</em>}, \ldots, x_{n}^{*}, D_{\text {train }}, T\right)
$$</p>
<p>Where, the previous target location is autoregressively added to the conditioning data via the LLM prompt. This should yield much richer predictive distributions as we are now able to model dependencies between output variables. However, this definition is no longer guaranteed to give us a valid stochastic process as the predictive distribution is now target order dependent and most likely will fail the Kolmogorov exchangability condition. We investigate these questions in Section 3.</p>
<h1>B LLM Processes Pseudocode</h1>
<div class="codehilite"><pre><span></span><code><span class="nv">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">Pseudocode</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">sampling</span><span class="w"> </span><span class="nv">numbers</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">LLM</span>
<span class="w">    </span>\<span class="ss">(</span><span class="nv">N</span><span class="w"> </span>\<span class="nv">leftarrow</span>\<span class="ss">)</span><span class="w"> </span><span class="nv">Number</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">desired</span><span class="w"> </span><span class="nv">samples</span>
<span class="w">    </span><span class="nv">samples</span><span class="w"> </span>\<span class="ss">(</span>\<span class="nv">leftarrow</span>[\<span class="ss">)</span><span class="w"> </span>]
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="nv">len</span><span class="ss">(</span><span class="nv">samples</span><span class="ss">)</span><span class="w"> </span>\<span class="ss">(</span><span class="o">&lt;</span><span class="nv">N</span>\<span class="ss">)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="nv">out</span><span class="w"> </span>\<span class="ss">(</span>\<span class="nv">leftarrow</span>\<span class="ss">)</span><span class="w"> </span><span class="nv">model</span>.<span class="nv">generate</span><span class="ss">(</span><span class="nv">prompt</span><span class="ss">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nv">out</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">number</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="nv">samples</span>.<span class="nv">append</span><span class="ss">(</span><span class="nv">out</span><span class="ss">)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<p>Algorithm 2 Pseudocode for computing the log pdf of $y$
$n \leftarrow$ number of digits after decimal point
nonnum_idxs $\leftarrow$ tokens $\notin$ tokenize([‘0', '1', ... , '9', '-', ' $\left.'^{\prime}, ' \langle t\rangle^{\prime}\right]$ )
full_text $\leftarrow$ prompt $+\operatorname{str}(y)$
y_idxs $\leftarrow$ indices of the tokens that correspond to y in full_text
logits $\leftarrow$ model(full_text)
y_logits $\leftarrow$ logits[y_idxs]
y_logits[nonnum_idxs] $\leftarrow-100$
y_logpmf $\leftarrow$ CrossEntropy $($ logits $=$ y_logits $[-1]$, targets $=\operatorname{str}(y)[1:]$.$) sum($ ) $\triangleright$ Mass of bin that
includes $y$
y_logpdf $\leftarrow$ y_logpmf $+n \log 10 \quad \triangleright$ Convert mass to continuous likelihood</p>
<h1>C Sample Prompts</h1>
<p>Figure C. 1 depicts three observed training points and four target locations. Below are sample prompts for various configurations discussed in the paper. $T$ refers to problem related text.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure C.1: Three observed training points and four target locations which serve as the basis for the example prompts.</p>
<h2>Independent Marginal Prompts</h2>
<p>Sequential:
$" T\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle C_{x}, C_{y}\langle t\rangle D_{x}^{<em> "}$
$" T\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle C_{x}, C_{y}\langle t\rangle E_{x}^{</em> "}$
$" T\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle C_{x}, C_{y}\langle t\rangle F_{x}^{<em> "}$
$" T\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle C_{x}, C_{y}\langle t\rangle G_{x}^{</em> "}$
Random:
$" T\langle t\rangle C_{x}, C_{y}\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle D_{x}^{<em> "}$
$" T\langle t\rangle C_{x}, C_{y}\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle E_{x}^{</em> "}$
$" T\langle t\rangle C_{x}, C_{y}\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle F_{x}^{<em> "}$
$" T\langle t\rangle C_{x}, C_{y}\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle G_{x}^{</em> "}$
Distance:
$" T\langle t\rangle C_{x}, C_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle A_{x}, A_{y}\langle t\rangle D_{x}^{<em> "}$
$" T\langle t\rangle C_{x}, C_{y}\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle E_{x}^{</em> "}$
$" T\langle t\rangle A_{x}, A_{y}\langle t\rangle C_{x}, C_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle F_{x}^{<em> "}$
$" T\langle t\rangle A_{x}, A_{y}\langle t\rangle B_{x}, B_{y}\langle t\rangle C_{x}, C_{y}\langle t\rangle G_{x}^{</em> "}$</p>
<h1>Autoregressive Prompts</h1>
<p>Sequential:
" $T(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) C_{x}, C_{y}(t) D_{x}^{<em>}$ "
" $T(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) C_{x}, C_{y}(t) D_{x}^{</em>}, D_{y}^{<em>}(t) E_{x}^{</em>}$ "
" $T(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) C_{x}, C_{y}(t) D_{x}^{<em>}, D_{y}^{</em>}(t) E_{x}^{<em>}, E_{y}^{</em>}(t) F_{x}^{<em>}$ "
" $T(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) C_{x}, C_{y}(t) D_{x}^{</em>}, D_{y}^{<em>}(t) E_{x}^{</em>}, E_{y}^{<em>}(t) F_{x}^{</em>}, F_{y}^{<em>}(t) G_{x}^{</em>}$ "
Random:
" $T(t) C_{x}, C_{y}(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) D_{x}^{<em>}$ "
" $T(t) C_{x}, C_{y}(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) D_{x}^{</em>}, D_{y}^{<em>}(t) E_{x}^{</em>}$ "
" $T(t) C_{x}, C_{y}(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) D_{x}^{<em>}, D_{y}^{</em>}(t) E_{x}^{<em>}, E_{y}^{</em>}(t) F_{x}^{<em>}$ "
" $T(t) C_{x}, C_{y}(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) D_{x}^{</em>}, D_{y}^{<em>}(t) E_{x}^{</em>}, E_{y}^{<em>}(t) F_{x}^{</em>}, F_{y}^{<em>}(t) G_{x}^{</em>}$ "
Distance:
" $T(t) C_{x}, C_{y}(t) B_{x}, B_{y}(t) A_{x}, A_{y}(t) D_{x}^{<em>}$ "
" $T(t) C_{x}, C_{y}(t) D_{x}^{</em>}, D_{y}^{<em>}(t) A_{x}, A_{y}(t) B_{x}, B_{y}(t) E_{x}^{</em>}$ "
" $T(t) D_{x}^{<em>}, D_{y}^{</em>}(t) A_{x}, A_{y}(t) E_{x}^{<em>}, E_{y}^{</em>}(t) C_{x}, C_{y}(t) B_{x}, B_{y}(t) F_{x}^{<em>}$ "
" $T(t) D_{x}^{</em>}, D_{y}^{<em>}(t) A_{x}, A_{y}(t) E_{x}^{</em>}, E_{y}^{<em>}(t) B_{x}, B_{y}(t) F_{x}^{</em>}, F_{y}^{<em>}(t) C_{x}, C_{y}(t) G_{x}^{</em>}$ "</p>
<h1>D Dataset Details</h1>
<p>This section provides details on the various datasets used in the experiments</p>
<h2>D. 1 Function Dataset</h2>
<p>We use the 12 synthetic function datasets (Linear, Exponential, Sigmoid, Log, Sine, Beat Inference, Linear + Cosine, Linear $\times$ Sine, Gaussian Wave. Sinc, Quadratic, $\mathrm{X} \times$ Sine) from Gruver et al. [2] each of which consists of 200 discrete points. We construct 7 datasets each with 10 random seeds for each function with a subset of $5,10,15,20,25,50$, and 75 randomly training points sampled from the original 200 points. We add Gaussian noise with $\mu=0$ and $\sigma=0.05$ to the training points and then round the values to 2 decimal places. Unless otherwise stated, we use 40 equally spaced target points to sample at.</p>
<h2>D. 2 Weather Dataset</h2>
<p>The dataset was queried from OpenWeather [36] and consists of daily high temperature, precipitation, and wind speed readings for 86 consecutive days from London, UK commencing on December 12, 2023. The data was recorded after the release dates of the Llama-2 and Mixtral-8x7B LLM release dates to avoid any data leakage into the LLM datasets.</p>
<p>For the "Comparison to LLMTime" experiment, We used the first 50 readings of the temperature data for training data and ask LLMTime and LLMPs to predict/forecast the final 36 values. The authors of LLMTime suggest the method can handle missing values by inputting NaN values in their place. Since LLMPs can work with irregularly spaced and missing data, we also compare the methods with a reduced number of randomly spaced training points.</p>
<p>For the "Simultaneous Temperature, Rainfall, and Wind Speed Regression" experiment we used 30 randomly chosen training points within the first 76 points, leaving the last 10 for extrapolation.</p>
<h2>E Data Leakage</h2>
<p>It is likely that LLMs used in our experiments have been exposed during training to some of the real-world data that we use in our experiments which would give it an advantage against other models. However, we feel confident that the LLMs tested were not simply recalling memorized data - note that in all cases the LLMPs produces a full distribution and not just a deterministic value - and we have taken steps in our experiments to mitigate this issue. When synthetic functions or Fashion MNIST data [12] is used, we have altered the original data via subsampling, rescaling and in some cases adding noise to the datapoints. Any data used from the internet was altered from its original form when given to the model. Some datasets (in particular the Weather Dataset described in Appendix D.2), were explicitly chosen to be recorded after the release dates of the LLMs that they were evaluated on.</p>
<h1>F Additional Implementation Details</h1>
<p>PyTorch is used as the basis for all of the experiments, with the exception of the Gaussian Processes baselines that are implemented using the GPyTorch package [37].
The experiments using the Mixtral $8 \times 7$ B, Mixtral-8×7B-Instruct [7], Llama-2 70B [8], and Llama-3 70B [9] LLMs were run on two NVidia A100 GPUs with 80 GB of memory. The experiments using the Llama-2 7B [8] and Llama-3 8B [9] LLMs were run on one NVidia 3090 GPU with 24 GB of memory. The total compute used in the paper exceeded 600 GPU hours.
No training was done in our LLM experiments, we simply input the prompt to the LLM and ran it forward to get a prediction for a particular target point.</p>
<h2>F. 1 Processing Times</h2>
<p>Processing times vary as a function of:</p>
<ul>
<li>The GPU used.</li>
<li>The length of the prompt.</li>
<li>The number of target points queried.</li>
<li>The number of tokens required to be generated for a particular target point.</li>
<li>The number of samples taken at each target point.</li>
<li>Whether independent or autoregressive sampling is used.</li>
</ul>
<p>Example experiment processing times:
Basic Scenario: Table F. 1 indicates that the longer the prompt, the longer the computation time for each target point. For independent sampling (I-LLMP), the prompt length is constant and is only a function of the number of training points as each target point is processed independently. For autoregressive sampling (A-LLMP), the prompt length is a function of both the number of training points and the number of target points since each target point is appended to the prompt as it is sampled.</p>
<p>Table F.1: Times to load the LLM into GPU memory, for the LLM to generate all samples at all target points, and to compute the probability distribution over the true target points. All runs used the Llama-2-7B LLM and were executed on an NVIDIA 3090 GPU with 24GB of memory with a batch size of 10 . All times are in seconds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Load (s)</th>
<th style="text-align: center;">Sample (s)</th>
<th style="text-align: center;">Compute Likelihood (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Quadratic - 10 Training Points, 40 Target Points</td>
<td style="text-align: left;">I-LLMP</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Quadratic - 10 Training Points, 40 Target Points</td>
<td style="text-align: left;">A-LLMP</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">Quadratic - 50 Training Points, 40 Target Points</td>
<td style="text-align: left;">I-LLMP</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">259</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Quadratic - 50 Training Points, 40 Target Points</td>
<td style="text-align: left;">A-LLMP</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">354</td>
<td style="text-align: center;">7</td>
</tr>
</tbody>
</table>
<p>1D Synthetic Data Experiments:</p>
<ul>
<li>LLM: Mixtral-8×-7B</li>
<li>GPU: $2 \times$ Nvidia A100, 80 GB</li>
<li>Parameters: A-LLMP, 40 target points, 50 samples, log probabilities</li>
<li>Tasks: 12 functions x 3 seeds x 4 sizes</li>
<li>Approximate Time: 19.6 hours</li>
</ul>
<p>Black Box Optimization:</p>
<ul>
<li>LLM: Llama-2 7B</li>
<li>GPU: $1 \times$ Nvidia A100, 80 GB</li>
<li>Parameters: I-LLMP, 500 target points, 1 sample</li>
<li>Tasks: 6 functions, 100 trials</li>
<li>Approximate Time: 20 hours</li>
</ul>
<p>Fashion MNIST Image Reconstruction:</p>
<ul>
<li>LLM: Mixtral-8×-7B</li>
<li>GPU: $2 \times$ Nvidia A100, 80 GB</li>
<li>Parameters: I-LLMP, 400 target points, 50 samples</li>
<li>Tasks: 6 images x 2 sizes</li>
<li>Approximate Time: 15 hours</li>
</ul>
<p>Simultaneous Temperature, Rainfall, and Wind Speed Regression</p>
<ul>
<li>LLM: Llama-3 8B</li>
<li>GPU: $1 \times$ Nvidia 3090, 24 GB</li>
<li>Parameters: A-LLMP, 40 target points, 50 samples</li>
<li>Tasks: 6 functions, 100 trials</li>
<li>Approximate Time: 31 minutes</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The models we evaluate are trained with tokenization schemes that tokenize each digit in a number separately. Gruver et al. [2] include a space between each digit for tokenizers that do not tokenize each digit separately.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9538 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9538</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9538</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-259095527</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.04610v1.pdf" target="_blank">The Two Word Test: A Semantic Benchmark for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or 'true' understanding of language, and even Artificial General Intelligence (AGI). Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental aspect of human language and intelligence. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or not meaningful (e.g., goat sky). by 150 human raters. We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments. We conducted a series of experiments using the TWT on GPT-4, GPT-3.5, and Bard, with both versions. Results demonstrated that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the limitations and weaknesses of current LLMs, and potentially improve them. The test also reminds us that caution is warranted in attributing 'true understanding' or AGI to LLMs. TWT is available at: https://github.com/NickRiccardi/two-word-test</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9538",
    "paper_id": "paper-259095527",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00252575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Two Word Test: A Semantic Benchmark for Large Language Models</p>
<p>Nicholas Riccardi 
Department of Psychology
University of South Carolina</p>
<p>Rutvik H Desai 
Department of Psychology
University of South Carolina</p>
<p>The Two Word Test: A Semantic Benchmark for Large Language Models</p>
<p>Large Language Models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or "true" understanding of language, and even Artificial General Intelligence (AGI). Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental aspect of human language and intelligence. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or not meaningful (e.g., goat sky) by 150 human participants. We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments. We conducted a series of experiments using the TWT on GPT-4, GPT-3.5, and Bard, with both versions. Results demonstrate that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases, with both models consistently judging highly nonsensical phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the limitations and weaknesses of current LLMs, and potentially improve them. The test also reminds us that caution is warranted in attributing "true understanding" or AGI to LLMs. TWT is available at: https://github.com/NickRiccardi/two-word-test</p>
<p>Introduction</p>
<p>Large Language Models (LLMs; also called Large Pre-Trained Models or Foundation Models) (Bommasani et al. 2021) are deep neural networks with billions or trillions or parameters that are trained on massive natural language corpora. They have shown remarkable and surprising abilities spanning many different tasks. Some examples include the ability to pass examinations required for advanced degrees, such as those in law , business (Terwiesch 2023), and medicine (Kung et al. 2023). Strong performance on benchmarks such as General Language Understanding Evaluation (GLUE) and its successor (SuperGLUE) have also been obtained (Brown et al. 2020, Chowdhery et al. 2022. Bubeck et al. (2023) investigated an early version of , and reported that it can solve difficult tasks in mathematics, coding, vision, medicine, law, and psychology, music, and exhibited "mastery of language." With such breadth of human-level (or better) performance, they suggested that it shows "sparks" of Artificial General Intelligence (AGI).</p>
<p>Such achievements have led many researchers to conclude that LLMs have achieved or are close to achieving real or humanlike understanding of language. Others remain skeptical. A recent survey (Michael et al. 2022) asked active researchers whether such models, trained only on text, could in principle understand natural language someday. About half (51%) agreed, while other half (49%) disagreed. This stark divide is closely tied to the question of what constitutes true understanding and is subject of intense debate (Michell and Karkauer 2023).</p>
<p>The skeptics have pointed out examples where LLMs produce less-than-satisfactory performance. Hallucinations (Lee et al. 2018, Raunak et al. 2021, inaccurate number comparisons, and reasoning errors are commonly cited problems, and failures in individual cases are frequently reported (e.g., https://github.com/giuven95/chatgptfailures). It is argued that while LLMs exhibit formal linguistic competence, they lack functional linguistic competence, which is the ability to robustly understand and use language in the real world (Mahowald et al. 2023). However, this claim still runs into the problem of how robust understanding is to be measured beyond subjective assessments of the quality of answers in response to prompts. Objective benchmarks are essential here, but as successes and failures of LLMs show, benchmarks that are suitable for measuring human understanding might not be appropriate for assessing LLMs (Choudhury et al. 2022;Gardner et al. 2021;Linzen 2020).</p>
<p>There are philosophical arguments as to why LLMs do not have true or humanlike understanding. For example, LLMs learn words-to-words mappings, but not words-toworld mappings, and hence cannot understand the objects or events that words refer to (Browning and LeCun, 2022). Such arguments aside, formal tests are critical, as that's where "rubber meets the road." If a system can match or surpass human performance in any task thrown at it, the argument that it does not possess real understanding, for whatever reason, rings hollow. If an LLM indeed lacks humanlike understanding, one ought to be able to design tests where it performs worse than humans. With such tests, the nebulous definition of "understanding" becomes less of a problem.</p>
<p>Here, we propose and evaluate one such novel benchmark, the Two Word Test (TWT). The test is based on a basic human psycholinguistic ability to understand combinations of two words. The test uses noun-noun combinations (e.g., beach ball) and requires discrimination between meaningful and nonsense (e.g., ball beach) combinations. Compared to other types of linguistic compositions, , such as adjective-noun (big ball) or verb-noun (throw ball),.noun-noun combinations do not offer grammatical assistance in determining meaningfulness. One can determine that ball red is not a meaningful phrase, because noun-adjective is not a valid word order in English. The same strategy cannot be used to determine that ball beach has low meaningfulness. Some phrases are learned as single units that combine unrelated words (sea lion), while others are 'built from the ground up'. Baby boy makes sense, and many other words could follow baby and the phrase would still be sensible (clothes, girl, sister, etc.). Simply reversing word order of some of these (clothes baby) can result in a low-meaningfulness phrase. These unique memory-dependent, semantic, and compositional elements make this test a valuable semantic benchmark for LLMs. A previous study (Graves et al 2013) obtained meaningfulness ratings on these phrases from 150 human participants, which we use here. We report results from three current LLMs (OpenAI's GPT-4 and GPT-3.5turbo and Google's Bard). Our main contributions are as follows:</p>
<p>• Two Word Test (TWT), a novel open-source benchmark for measuring LLM comprehension using simple two-word phrases. Unlike existing benchmarks, the test does not rely on the ability to do logical reasoning, planning, infer implied content, disambiguate ambiguous words, or solve puzzles or other problems, but relies on combinatorial semantics.</p>
<p>• The TWT measures the ability of LLMs to judge meaningfulness using a Likert scale. We provide a second version, binary TWT (bTWT), which measures binary 'makes sense' or 'nonsense' judgments for each phrase, which is expected to be easier for LLMs.</p>
<p>• A comprehensive statistical comparison using Signal Detection Theory (SDT) metrics and permutation testing, of the performance of GPT-4, 3.5-turbo, and Bard to human data.</p>
<p>• Identification of limitations of current LLMs in language comprehension ability, as a weakness distinct from those in tasks that rely on executive control, such as logical reasoning or puzzle solving.</p>
<p>Materials and Methods</p>
<p>Two Word Test Phrase Generation and Human Rating Collection</p>
<p>The TWT consists of noun-noun combinations and human meaningfulness ratings collected as part of behavioral and neuroimaging experiments conducted by Graves and colleagues (2013), whose methods we will now briefly summarize. They chose 500 common nouns, and all possible noun-noun combinations were generated. The occurrence of these combinations as two-word phrases was cross-referenced with a large corpus of human-generated text. Phrases with meaningful interchangeable word orders or that were taboo were removed. 'Nonsense' or low-meaningfulness phrases were generated by reversing the word order of meaningful phrases, resulting in 2,160 phrases.</p>
<p>Participants (N=150) rated subsets of the total phrase pool with the following instructions:</p>
<p>Please read each phrase, then judge how meaningful it is as a single concept, using a scale from 0 to 4 as follows: If the phrase makes no sense, the appropriate rating is 0. If the phrase makes some sense, the appropriate rating is 2. If the phrase makes complete sense, the appropriate rating is a 4. Please consider the full range of the scale when making your ratings.</p>
<p>Examples: the goat sky, 0 (makes no sense), the fox mask, 2 (makes some sense), and the computer programmer, 4 (makes complete sense).</p>
<p>For each phrase, the mean and standard deviation of participant responses were calculated. Here, 392 phrases with mean ratings between 1.5 and 2.5 were removed from the set due to being ambiguous to human raters, and resulted in 977 nonsense and 761 meaningful phrases used in the TWT presented here.</p>
<p>The Two Word Test: Assessment of Combinatorial Semantic Understanding in LLMs</p>
<p>We conducted a series of experiments comparing GPT-4, GPT-3.5-turbo, and Bard performance (each model as available in April 2023) to the human data. First, we gave the LLMs the same prompt used by Graves et al., followed by an enhanced version of the prompt. Then, we tested the LLMs on a binary version of the test (i.e., 'makes sense' / 'nonsense' judgment instead of numerical ratings).</p>
<p>TWT: Numerical Meaningfulness Judgments</p>
<p>For each LLM, we submitted the instructions and examples originally provided by Graves et al in subsets of randomized order. We repeated the instructions each time we submitted a subset (due to token restrictions) to ensure that errors were not due to memory limitations. Using Graves' original prompt resulted in the LLMs largely neglecting to use the 1 and 3 ratings, the two ratings not used as example cases in Graves' original prompt. To encourage the LLMs to use the full rating scale, we provided two additional examples in the instructions for scores of 1 and 3 (the knife army, 1 (makes very little sense) and the soap bubble, 3 (makes a lot of sense)). Compared to the human distribution, which reflects 'makes sense' and 'nonsense' phrases in the bimodal peaks, LLMs show a bias towards rating most phrases as a 2 or 3 (makes some sense, makes a lot of sense; Fig. 1).</p>
<p>Figure 1</p>
<p>Frequency of continuous meaningfulness ratings for humans and LLMs. Human mean responses reflect a bimodal distribution of meaningful and nonsense phrases, while that is lacking in all three LLMs.</p>
<p>However, it is more informative to take LLM ratings of each individual phrase and test the probability that its rating came from the same distribution as the human responses to that phrase. We conducted a series of phrase-wise statistical tests to compare each LLM to human meaningfulness ratings.</p>
<p>First, we used the human phrase-wise means and standard deviations to generate a gaussian distribution of 10,000 simulated human responses to each phrase, respecting the lower and upper limits of the 0-to-4 scale and rounded to the nearest integer to match the LLM response scale (Fig. 2). Then, for each phrase, we conducted a Crawford &amp; Howell t-test for case-control comparisons with the LLM as the case and the human distribution as the control. This modified t-test is designed for comparison of a single-case observation to a control group and returns the probability that the case comes from the same distribution as the group. We hereby define a 'TWT failure' as when the LLM meaningfulness rating has less than a 5% probability of coming from the human distribution (i.e., the LLM rating significantly different from that of humans, p &lt; .05).</p>
<p>Figure 2:</p>
<p>Simulated human rating distributions (blue) and LLM ratings (GPT-4 = olive, GPT-3.5 = red, Bard = yellow) for low-and high-meaningfulness phrases (the cake apple, the dog sled). For the cake apple, GPT-3.5 rated it as more meaningful than &gt; 95% of humans would be expected to, while GPT-4 and Bard responded within normal limits. For the dog sled, Bard rated it as less meaningful than &gt; 95% of humans would be expected to, while the other LLMs responded within normal limits.</p>
<p>We tested the LLMs with all 1,768 phrases, then limited it further to phrases most agreed-upon by human raters (determined by 95% confidence intervals around the human mean ratings). For 95% CI, there were 499 meaningful and 369 nonsense phrases.  Table 1 TWT failure percentages To understand where LLM responses fall between human ratings and chance or random ratings, we generated two rating distributions. (1) 'Human': 1000 simulated participants whose phrase-wise responses were generated from the underlying probability distribution of responses to each phrase in the Graves et al. (2013) study.</p>
<p>(2) 'Chance': 1000 permuted participants whose phrase-wise responses were selected based only on the frequency of 0-4 ratings from the original study. The 'Human' distribution approximates what would be expected from human raters if the study was run on a large number of human participants. The 'Chance' distribution is what would be generated by a system with no knowledge of word meaning. We then generated failure counts for the distributions and for each of the models. Table 1 and Figure 3 show that Bard and GPT-3.5 failure counts are closer to chance than to the simulated human distribution. GPT-4 is significantly better than the other LLMs, but still fails far more than what would be expected from a human rater. Taken together, these results show that the three LLMs fail at the TWT, but that there are significant differences between their abilities.</p>
<p>Experiment 1 Results:</p>
<p>Figure 3:</p>
<p>Number of LLM failures in TWT compared to simulated human (blue) and permuted-chance (orange) failure count distributions.</p>
<p>bTWT: Binary Meaningfulness Judgments</p>
<p>LLMs are often reported to make errors on numerical tasks. It is possible that the poor performance on the TWT was due to a difficulty in dealing with the numerical scale required for the task, rather than a lack of understanding of phrase meaning. In order to eliminate numerical ratings, we modified the TWT instructions to prompt binary responses:</p>
<p>Please read each phrase, then judge how meaningful it is as a single concept. If the phrase makes no sense or makes very little sense, the appropriate response is 'nonsense'. If the phrase makes a lot of sense or complete sense, the appropriate rating is 'makes sense'.</p>
<p>Examples: 'the goat sky' is 'nonsense', 'the knife army' is 'nonsense', 'the soap bubble' is 'makes sense', 'the computer programmer' is 'makes sense'</p>
<p>We then calculated the following to measure LLM performance: Chi-squared (χ 2 ) test, signal detection theory (SDT) metrics, and receiver operating characteristic (ROC) curve. Table 2 and Figure 4 show SDT results. SDT measures how well an actor (LLMs) can detect true signal (meaningful phrases) while correctly rejecting noise (nonsense phrases). It uses ratios of hits (true positives), correct rejections (true negatives), false alarms (false positives), and misses (false negatives). d' is a measure of overall ability to discriminate, with 0 being chance-level and &gt;4 being close to perfect discrimination. β measures response tendency, or whether an actor prefers to say that a signal is present (liberal) or absent (conservative). Base 10 logarithm of β, reported here, is interpreted as &lt; 0 being liberal and &gt; 0 being conservative. We also display the ROC curve ( Figure 5) and report area under the curve (AUC).  Table 2: d', β, and AUC for LLM 'makes sense'/'nonsense' discrimination  We also conducted the χ 2 test. Briefly, χ 2 test is used with categorical data and can test for statistical independence of observed frequencies to what is expected. Here, observed frequencies are the counts of LLM 'makes sense' and 'nonsense' responses and the expected response frequencies are those provided by the human data (e.g., 977 nonsense and 761 meaningful). Table 3 shows that the LLM frequency of responses are significantly different from the human response frequencies, and supports SDT and ROC results.</p>
<p>Experiment 2 Results:</p>
<p>LLM</p>
<p>χ 2 : all 1,768 phrases χ 2 : 868 most agreed upon phrases (95% CI) Bard χ 2 = 84.3 p &lt; .001 χ 2 = 27.9 p &lt; .001 GPT-3.5 χ 2 = 538.1 p &lt; .001 χ 2 = 217.9 p &lt; .001 GPT-4 χ 2 = 10.6 p = .001 χ 2 = 7.9 p = .004 Table 3: χ 2 test results for observed (LLM) compared to expected (human) frequency of 'makes sense' and 'nonsense' responses. P &lt; 0.05 indicates significantly different performance relative to humans. Table 2 and Figures 4 and 5 show that Bard and GPT-3.5 display poor-to-modest discrimination. GPT-3.5 is overly liberal, tending to say that nonsense phrases make sense. Bard is more conservative and tends to say that sensible phrases are nonsense. GPT-4 is substantially better than the other models and displays moderate-to-high discrimination abilities. Almost the entirety of its improvement at the TWT over Bard and GPT-3.5 is by being able to correctly identify nonsense phrases. However, it is still significantly different from human performance.</p>
<p>Results Summary:</p>
<p>Conclusions and Future Work</p>
<p>We presented a new benchmark for testing language understanding in LLMs. The task, essentially trivial for humans, requires rating meaningfulness of two-word phrases. Three current LLMs fail on this task. While GPT-4 performed better than GPT-3.5 and Bard, its performance still fell well short of humans. When test items were restricted to phrases that had the highest agreement among human raters, GPT-4 still provided statistically anomalous ratings on ~20% of phrases A binary version of the test, bTWT, was created to test whether the poor performance of LLMs was the result of a failure to deal with the numerical scale required for TWT. The bTWT revealed that GPT-3.5 and Bard fail to distinguish meaningfulness of phrases binarily, achieving poor discrimination. GPT-3.5 was excessively liberal, tending to rate everything as 'making sense'. Bard was more conservative, often labelling sensible phrases as 'nonsense'. GPT-4, however, takes a significant step forward on the bTWT. While there is still room for improvement when tasked with judging all phrases, it displayed high discrimination abilities when probed on the phrases with low variability in human ratings.</p>
<p>Several investigations have begun to examine and reveal limitations of LLMs. For example, Dziri et al. (2023) tested LLMs on three compositional tasks (multi-digit multiplication, logic grid puzzles, and dynamic programming). They found that LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching. They suggest that in multi-step reasoning problems, LLM performance will rapidly decay with increasing complexity. Failures have been demonstrated in other problems as well, such as those involving logical and commonsense reasoning (Koralus et al.2023;Bian et al. 2023) was well as sequence tagging (Qin et al. 2023).</p>
<p>The TWT differs from these cases in that it does not directly require inference or reasoning. A limitation in breaking down a complex chain of reasoning into smaller problems should not affect performance on the TWT. Understanding these phrases requires understanding the constituent concepts, and then using world knowledge to determine whether the combination makes sense in some manner. A 'mountain stream' is a stream located on a mountain, but a 'stream mountain' is not a thing at all. An 'army knife' is not necessarily a knife located in the army but a type of knife useful in certain situations. TWT may exploit the fact that the text corpora that LLMs are trained on, no matter how large, almost entirely contain sensible text. However, this is the case for humans as well. Almost all text that people are exposed to is also sensible, but if the task requires, they are easily able to determine that certain word combinations don't make much sense. Current LLMs may lack the depth of real-world knowledge that is required for this task.</p>
<p>Many of the limitations of LLMs identified previously can be associated with a lack of 'executive control' that presents difficulties in complex symbolic or rule-based reasoning. Because of this, many have proposed combining deep neural networks with symbolic reasoning systems that can exert executive control when required (e.g., in three-digit multiplication). The weakness identified by TWT is qualitatively distinct, in that it is not directly related to the ability for executive control or systematic application of rules. It appears to be a limitation related to underlying semantic knowledge itself, rather than to reason using that knowledge.</p>
<p>Figure 4 :
4SDT metrics for all 1,768 phrases. Hittrue positive; Missfalse negative; CRcorrect rejection (true negative); FAfalse alarm (false positive).</p>
<p>Figure 5 :
5ROC curve for all 1,768 phrases</p>
<p>Table 1 provides
1TWT failure counts for the LLMs in the three subsets of phrases.LLM 
% of failures: 
1,768 phrases </p>
<p>% of failures, 868 
most agreed upon 
phrases (95% CI) 
Bard 
42.7% 
57.9% 
GPT-3.5 
49.3% 
62.4% 
GPT-4 
23.4% 
23.2% </p>
<p>These results also urge for caution in attribution AGI or similar abilities to LLMs, based on testing on tasks that are difficult for humans. The mounting understanding of the impressive abilities as well as limitations of LLMs will be essential in improving these models, and in identifying appropriate use cases.
Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. N Bian, X Han, L Sun, H Lin, Y Lu, B He, arXiv:2303.16421arXiv preprintBian, N., Han, X., Sun, L., Lin, H., Lu, Y., &amp; He, B. (2023). Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. arXiv preprint arXiv:2303.16421.</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, . . Liang, P , arXiv:2108.07258On the opportunities and risks of foundation models. arXiv preprintBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... &amp; Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.</p>
<p>. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, . . Amodei, D , Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &amp; Amodei, D. (2020).</p>
<p>Language models are few-shot learners. Advances in neural information processing systems. 33Language models are few-shot learners. Advances in neural information processing systems, 33, 1877- 1901.</p>
<p>AI and the limits of language, 2022. Noema. J Browning, Y Lecun, Browning, J. and LeCun, Y. AI and the limits of language, 2022. Noema, August 23, https://www.noemamag.com/ai-and-the-limits-of-language.</p>
<p>Chatgpt goes to law school. J H Choi, K E Hickman, A Monahan, D Schwarcz, 10.2139/ssrn.4335905Minnesota Legal Studies Research Paper. 23-03Choi, J. H., Hickman, K. E., Monahan, A., &amp; Schwarcz, D. (2023). Chatgpt goes to law school. Minnesota Legal Studies Research Paper No. 23-03, Available at SSRN: https://ssrn.com/abstract=4335905 or http://dx.doi.org/10.2139/ssrn.4335905</p>
<p>S R Choudhury, A Rogers, I Augenstein, arXiv:2209.07430Machine Reading, Fast and Slow: When Do Models" Understand" Language?. arXiv preprint. Choudhury, S. R., Rogers, A., &amp; Augenstein, I. (2022). Machine Reading, Fast and Slow: When Do Models" Understand" Language?. arXiv preprint arXiv:2209.07430.</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, . . Fiedel, N , arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... &amp; Fiedel, N. (2022). Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>N Dziri, X Lu, M Sclar, X L Li, L Jian, B Y Lin, . . Choi, Y , arXiv:2305.18654Faith and Fate: Limits of Transformers on Compositionality. arXiv preprintDziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., ... &amp; Choi, Y. (2023). Faith and Fate: Limits of Transformers on Compositionality. arXiv preprint arXiv:2305.18654.</p>
<p>M Gardner, W Merrill, J Dodge, M E Peters, A Ross, S Singh, N A Smith, arXiv:2104.08646Competency problems: On finding and removing artifacts in language data. arXiv preprintGardner, M., Merrill, W., Dodge, J., Peters, M. E., Ross, A., Singh, S., &amp; Smith, N. A. (2021). Competency problems: On finding and removing artifacts in language data. arXiv preprint arXiv:2104.08646.</p>
<p>Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C Elepaño, . . Tseng, V , PLoS digital health. 22198Kung, T. H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepaño, C., ... &amp; Tseng, V. (2023). Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLoS digital health, 2(2), e0000198.</p>
<p>Hallucinations in neural machine translation. K Lee, O Firat, A Agarwal, C Fannjiang, D Sussillo, Lee, K., Firat, O., Agarwal, A., Fannjiang, C., &amp; Sussillo, D. (2018). Hallucinations in neural machine translation.</p>
<p>How can we accelerate progress towards human-like linguistic generalization. T Linzen, arXiv:2005.00955arXiv preprintLinzen, T. (2020). How can we accelerate progress towards human-like linguistic generalization?. arXiv preprint arXiv:2005.00955.</p>
<p>K Mahowald, A A Ivanova, I A Blank, N Kanwisher, J B Tenenbaum, E Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models: a cognitive perspective. arXiv preprintMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., &amp; Fedorenko, E. (2023). Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627.</p>
<p>J Michael, A Holtzman, A Parrish, A Mueller, A Wang, A Chen, . . Bowman, S R , arXiv:2208.12852What do NLP researchers believe? Results of the NLP community metasurvey. arXiv preprintMichael, J., Holtzman, A., Parrish, A., Mueller, A., Wang, A., Chen, A., ... &amp; Bowman, S. R. (2022). What do NLP researchers believe? Results of the NLP community metasurvey. arXiv preprint arXiv:2208.12852.</p>
<p>The debate over understanding in AI's large language models. M Mitchell, D C Krakauer, Proceedings of the National Academy of Sciences. 120132215907120Mitchell, M., &amp; Krakauer, D. C. (2023). The debate over understanding in AI's large language models. Proceedings of the National Academy of Sciences, 120(13), e2215907120.</p>
<p>The curious case of hallucinations in neural machine translation. V Raunak, A Menezes, M Junczys-Dowmunt, arXiv:2104.06683arXiv preprintRaunak, V., Menezes, A., &amp; Junczys-Dowmunt, M. (2021). The curious case of hallucinations in neural machine translation. arXiv preprint arXiv:2104.06683.</p>
<p>Would Chat GPT get a Wharton MBA? A prediction based on its performance in the operations management course. Mack Institute for Innovation Management at the Wharton School. C Terwiesch, University of PennsylvaniaTerwiesch, C. (2023). Would Chat GPT get a Wharton MBA? A prediction based on its performance in the operations management course. Mack Institute for Innovation Management at the Wharton School, University of Pennsylvania.</p>
<p>Is ChatGPT a general-purpose natural language processing task solver. C Qin, A Zhang, Z Zhang, J Chen, M Yasunaga, D Yang, arXiv:2302.06476Philipp E. Koralus and Vincent Wang-Mascianica. Humans in humans out: On GPT converging toward common sense in both success and failure. CoRR, abs/2303.17276. arXiv preprintQin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., &amp; Yang, D. (2023). Is ChatGPT a general-purpose natural language processing task solver?. arXiv preprint arXiv:2302.06476.Philipp E. Koralus and Vincent Wang-Mascianica. Humans in humans out: On GPT converging toward common sense in both success and failure. CoRR, abs/2303.17276, 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
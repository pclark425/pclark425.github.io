<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8343 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8343</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8343</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276580691</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.18431v1.pdf" target="_blank">TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Reasoning is a fundamental capability of large language models (LLMs), enabling them to comprehend, analyze, and solve complex problems. In this paper, we introduce TextGames, an innovative benchmark specifically crafted to assess LLMs through demanding text-based games that require advanced skills in pattern recognition, spatial awareness, arithmetic, and logical reasoning. Our analysis probes LLMs' performance in both single-turn and multi-turn reasoning, and their abilities in leveraging feedback to correct subsequent answers through self-reflection. Our findings reveal that, although LLMs exhibit proficiency in addressing most easy and medium-level problems, they face significant challenges with more difficult tasks. In contrast, humans are capable of solving all tasks when given sufficient time. Moreover, we observe that LLMs show improved performance in multi-turn predictions through self-reflection, yet they still struggle with sequencing, counting, and following complex rules consistently. Additionally, models optimized for reasoning outperform pre-trained LLMs that prioritize instruction following, highlighting the crucial role of reasoning skills in addressing highly complex problems.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8343.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8343.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-o3 Mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-o3 Mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary reasoning-optimized LLM evaluated in this work; configured to prioritize shortest reasoning generations and used as a high-performing baseline on TEXTGAMES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-o3 Mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary closed-model described by the authors as optimized for reasoning; evaluated with greedy decoding and configured to prefer shorter reasoning traces for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (mini)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based logic puzzles requiring spatial arrangement and sub-grid / adjacency constraints</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Text-only prompt P = (T, C, E, I) where T = prompt template, C = constraints, E = few-shot example(s) (one-shot in some experiments), I = interaction history for multi-turn; grader G verifies exact correctness; experiments run zero-shot and one-shot with up to N=3 interactive turns (multi-turn feedback). Greedy decoding (do_sample=False).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>In-context few-shot examples (one-shot in reported results), multi-turn self-reflection using explicit grader feedback (up to 3 turns), deterministic greedy decoding; no explicit internal chain-of-thought was disclosed by model, but authors constrained GPT-o3 Mini to produce shorter reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate multi-turn solve rates on 2D TEXTGAMES (reported as percentage solved): Easy: turn1=87.8%, turn2=94.9%, turn3=98.8%; Medium: turn1=66.5%, turn2=80.7%, turn3=86.9%; Hard: turn1=20.6%, turn2=40.5%, turn3=48.6% (solve rate / percent). Also reported as substantially higher than other evaluated models on the same 2D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Empirical: GPT-o3 Mini achieves much higher solve rates on the 2D puzzles than instruction-tuned LLMs, indicating some capability to handle grid/spatial constraints; however authors note inverse scaling of longer reasoning traces in some hard games (e.g., Islands) suggesting fragile or brittle spatial reasoning when reasoning traces grow.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to a range of open-source models (Gemma-2 9B/27B Instruct, Llama-3.1 8B, Llama-3.3 70B Instruct, Qwen-2.5 family, GPT-4o Mini). GPT-o3 Mini outperforms all listed instruction-tuned and open models on 2D puzzles across difficulty levels; humans outperform models on hard tasks (humans solve all tasks given sufficient time).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite overall strong performance, GPT-o3 Mini shows inverse-scaling with longer internal reasoning traces (producing more errors when forced into longer reasoning, notably on Bracket, Islands, Ordering Text). Still struggles on the hardest-level 2D instances (hard-turn3 solve rate <50%), indicating failures on complex sequencing/counting and rigorous enforcement of multiple spatial constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8343.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8343.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2 27B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2 27B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-tuned LLM evaluated on TEXTGAMES; included to measure scaling effects within its model family.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2 27B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned LLM (Gemma-2 family); used with greedy decoding and one-shot/zero-shot prompts in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>27B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based logic puzzles requiring spatial arrangement and adherence to row/column/sub-grid/island constraints</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same TEXTGAMES setup: prompt template with constraints, optional one-shot example(s), grader verifies exact correctness, multi-turn feedback up to 3 rounds; deterministic greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>In-context one-shot examples and multi-turn self-reflection using grader feedback; no specialized symbolic solver or external search; purely LLM generation verified by grader.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate multi-turn solve rates on 2D puzzles (percent solved): Easy: turn1=21.4%, turn2=36.0%, turn3=44.4%. (Medium and Hard solve rates are much lower; authors report markedly worse performance on 2D tasks versus easy 1D tasks.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited: poor success rates on 2D tasks compared to 1D tasks suggest Gemma-2 27B has weak abilities to represent or enforce spatial/grid constraints in these puzzles; no explicit probing or ablation performed to isolate spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Scaled-up Gemma-2 27B outperforms Gemma-2 9B within the family but is substantially worse than GPT-o3 Mini and other reasoning-optimized models on 2D spatial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails frequently on medium/hard 2D instances; authors highlight inability to handle 2D spatial reasoning and follow complex multi-constraint rules consistently; many solutions rejected by grader due to shape, invalid characters, incorrect island counts, or sub-grid violations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8343.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8343.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.3 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-following LLM (70B) evaluated on TEXTGAMES; included to test whether larger instruction models scale to spatial puzzle reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.3 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variant of the Llama-3 family (70B parameters); evaluated zero/one-shot with greedy decoding in TEXTGAMES.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based logic puzzles requiring spatial arrangement and sub-grid/island constraints</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>TEXTGAMES prompt format with constraints, optional one-shot example(s), grader-based verification, multi-turn feedback up to 3 rounds; deterministic greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>In-context one-shot examples; multi-turn self-reflection receiving grader feedback; no external symbolic solver used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate multi-turn solve rates on 2D puzzles (percent solved): Easy: turn1=23.2%, turn2=38.0%, turn3=48.4%; Medium and Hard rates drop to single-digit percentages in many cases (e.g., Hard turn3 ≈3.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Authors report Llama-3.3 performs better than smaller variants on easy 2D problems but still fails on medium/hard 2D tasks, indicating limited spatial reasoning for complex grids; no targeted probing experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs better than smaller instruction models (e.g., Llama-3.1 8B) but far worse than GPT-o3 Mini; model scaling improves easy-task performance but does not close gap on hard 2D puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very low solve rates on medium/hard 2D puzzles; failures include incorrect grid shape, violations of sub-grid rules, and inability to enforce multi-constraint layouts consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8343.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8343.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5 72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5 72B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned Qwen family model evaluated on TEXTGAMES to study scale and instruction-following impact on puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5 72B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned member of the Qwen-2.5 family (72B parameters) used in zero/one-shot experiments; inference performed with greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based logic puzzles requiring spatial arrangement and constraint satisfaction</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>TEXTGAMES framework with prompt template, constraints, optional few-shot examples, grader verification, and multi-turn feedback (up to 3 turns); deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>In-context one-shot examples and iterative refinement via grader feedback; no symbolic search or external solver hooking described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate multi-turn solve rates on 2D puzzles (percent solved): Easy: turn1=28.2%, turn2=43.1%, turn3=51.1%; Medium and Hard performance are substantially lower (medium ~10-19% across turns; hard single-digit percent).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Moderate evidence: larger Qwen shows improved easy-level 2D performance versus smaller sizes, but performance still degrades rapidly on medium/hard 2D tasks, implying limited robust spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms smaller Qwen variants and some other instruction models at easy difficulty, but is outperformed by GPT-o3 Mini and lags behind human performance on medium/hard puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles to satisfy multiple spatial constraints simultaneously; frequent grader feedback indicates common failures such as wrong grid shape, wrong island counts, and invalid placement of elements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8343.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8343.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o Mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o Mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary closed instruction-following model evaluated as a baseline; included to compare instruction-following vs reasoning-optimized models on TEXTGAMES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o Mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed, proprietary instruction-tuned model used in this study as a higher-end instruction-following baseline; evaluated with greedy decoding in zero/one-shot and multi-turn settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (mini)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based logic puzzles (crossword placement, Sudoku sub-grid constraints, island-count/grid construction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard TEXTGAMES prompt construction (P=(T,C,E,I)), grader-based single/multi-turn evaluation (N up to 3), greedy decoding; one-shot and zero-shot configurations tested.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>In-context one-shot prompting, multi-turn iterative refinement informed by grader feedback; no external symbolic solver integration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate multi-turn solve rates on 2D puzzles (percent solved): Easy: turn1=20.7%, turn2=34.7%, turn3=40.9%; Medium and Hard were substantially lower (medium ~5–12% across turns; hard often <3% across turns).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited: GPT-4o Mini shows modest improvement with multi-turn feedback on easy tasks but fails on medium/hard 2D puzzles, indicating constrained spatial reasoning compared to reasoning-focused models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed by reasoning-optimized GPT-o3 Mini, roughly comparable or slightly better than some open-source instruction models at easy levels, but underperforms humans on nearly all hard 2D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails frequently on medium/hard spatial puzzles; common failure modes are inability to enforce sub-grid uniqueness (Sudoku), miscount islands/coconut constraints (Islands), and place words in a connected crossword satisfying all rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Langbirds: An agent for angry birds using a large language model <em>(Rating: 2)</em></li>
                <li>Language models are crossword solvers <em>(Rating: 2)</em></li>
                <li>Solving olympiad geometry without human demonstrations <em>(Rating: 1)</em></li>
                <li>Do you even "word game bench" bro? (Word Game Bench) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8343",
    "paper_id": "paper-276580691",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-o3 Mini",
            "name_full": "GPT-o3 Mini",
            "brief_description": "A proprietary reasoning-optimized LLM evaluated in this work; configured to prioritize shortest reasoning generations and used as a high-performing baseline on TEXTGAMES.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-o3 Mini",
            "model_description": "Proprietary closed-model described by the authors as optimized for reasoning; evaluated with greedy decoding and configured to prefer shorter reasoning traces for inference.",
            "model_size": "not specified (mini)",
            "puzzle_name": "Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)",
            "puzzle_type": "2D grid-based logic puzzles requiring spatial arrangement and sub-grid / adjacency constraints",
            "task_setup": "Text-only prompt P = (T, C, E, I) where T = prompt template, C = constraints, E = few-shot example(s) (one-shot in some experiments), I = interaction history for multi-turn; grader G verifies exact correctness; experiments run zero-shot and one-shot with up to N=3 interactive turns (multi-turn feedback). Greedy decoding (do_sample=False).",
            "mechanisms_or_strategies": "In-context few-shot examples (one-shot in reported results), multi-turn self-reflection using explicit grader feedback (up to 3 turns), deterministic greedy decoding; no explicit internal chain-of-thought was disclosed by model, but authors constrained GPT-o3 Mini to produce shorter reasoning traces.",
            "performance_metrics": "Aggregate multi-turn solve rates on 2D TEXTGAMES (reported as percentage solved): Easy: turn1=87.8%, turn2=94.9%, turn3=98.8%; Medium: turn1=66.5%, turn2=80.7%, turn3=86.9%; Hard: turn1=20.6%, turn2=40.5%, turn3=48.6% (solve rate / percent). Also reported as substantially higher than other evaluated models on the same 2D tasks.",
            "evidence_of_spatial_reasoning": "Empirical: GPT-o3 Mini achieves much higher solve rates on the 2D puzzles than instruction-tuned LLMs, indicating some capability to handle grid/spatial constraints; however authors note inverse scaling of longer reasoning traces in some hard games (e.g., Islands) suggesting fragile or brittle spatial reasoning when reasoning traces grow.",
            "comparisons": "Directly compared to a range of open-source models (Gemma-2 9B/27B Instruct, Llama-3.1 8B, Llama-3.3 70B Instruct, Qwen-2.5 family, GPT-4o Mini). GPT-o3 Mini outperforms all listed instruction-tuned and open models on 2D puzzles across difficulty levels; humans outperform models on hard tasks (humans solve all tasks given sufficient time).",
            "limitations_or_failure_cases": "Despite overall strong performance, GPT-o3 Mini shows inverse-scaling with longer internal reasoning traces (producing more errors when forced into longer reasoning, notably on Bracket, Islands, Ordering Text). Still struggles on the hardest-level 2D instances (hard-turn3 solve rate &lt;50%), indicating failures on complex sequencing/counting and rigorous enforcement of multiple spatial constraints.",
            "uuid": "e8343.0",
            "source_info": {
                "paper_title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Gemma-2 27B",
            "name_full": "Gemma-2 27B Instruct",
            "brief_description": "An open-source instruction-tuned LLM evaluated on TEXTGAMES; included to measure scaling effects within its model family.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma-2 27B Instruct",
            "model_description": "Open-source instruction-tuned LLM (Gemma-2 family); used with greedy decoding and one-shot/zero-shot prompts in evaluations.",
            "model_size": "27B",
            "puzzle_name": "Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)",
            "puzzle_type": "2D grid-based logic puzzles requiring spatial arrangement and adherence to row/column/sub-grid/island constraints",
            "task_setup": "Same TEXTGAMES setup: prompt template with constraints, optional one-shot example(s), grader verifies exact correctness, multi-turn feedback up to 3 rounds; deterministic greedy decoding.",
            "mechanisms_or_strategies": "In-context one-shot examples and multi-turn self-reflection using grader feedback; no specialized symbolic solver or external search; purely LLM generation verified by grader.",
            "performance_metrics": "Aggregate multi-turn solve rates on 2D puzzles (percent solved): Easy: turn1=21.4%, turn2=36.0%, turn3=44.4%. (Medium and Hard solve rates are much lower; authors report markedly worse performance on 2D tasks versus easy 1D tasks.)",
            "evidence_of_spatial_reasoning": "Limited: poor success rates on 2D tasks compared to 1D tasks suggest Gemma-2 27B has weak abilities to represent or enforce spatial/grid constraints in these puzzles; no explicit probing or ablation performed to isolate spatial reasoning.",
            "comparisons": "Scaled-up Gemma-2 27B outperforms Gemma-2 9B within the family but is substantially worse than GPT-o3 Mini and other reasoning-optimized models on 2D spatial puzzles.",
            "limitations_or_failure_cases": "Fails frequently on medium/hard 2D instances; authors highlight inability to handle 2D spatial reasoning and follow complex multi-constraint rules consistently; many solutions rejected by grader due to shape, invalid characters, incorrect island counts, or sub-grid violations.",
            "uuid": "e8343.1",
            "source_info": {
                "paper_title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3.3 70B",
            "name_full": "Llama-3.3 70B Instruct",
            "brief_description": "A large instruction-following LLM (70B) evaluated on TEXTGAMES; included to test whether larger instruction models scale to spatial puzzle reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.3 70B Instruct",
            "model_description": "Instruction-tuned variant of the Llama-3 family (70B parameters); evaluated zero/one-shot with greedy decoding in TEXTGAMES.",
            "model_size": "70B",
            "puzzle_name": "Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)",
            "puzzle_type": "2D grid-based logic puzzles requiring spatial arrangement and sub-grid/island constraints",
            "task_setup": "TEXTGAMES prompt format with constraints, optional one-shot example(s), grader-based verification, multi-turn feedback up to 3 rounds; deterministic greedy decoding.",
            "mechanisms_or_strategies": "In-context one-shot examples; multi-turn self-reflection receiving grader feedback; no external symbolic solver used.",
            "performance_metrics": "Aggregate multi-turn solve rates on 2D puzzles (percent solved): Easy: turn1=23.2%, turn2=38.0%, turn3=48.4%; Medium and Hard rates drop to single-digit percentages in many cases (e.g., Hard turn3 ≈3.4%).",
            "evidence_of_spatial_reasoning": "Authors report Llama-3.3 performs better than smaller variants on easy 2D problems but still fails on medium/hard 2D tasks, indicating limited spatial reasoning for complex grids; no targeted probing experiments reported.",
            "comparisons": "Performs better than smaller instruction models (e.g., Llama-3.1 8B) but far worse than GPT-o3 Mini; model scaling improves easy-task performance but does not close gap on hard 2D puzzles.",
            "limitations_or_failure_cases": "Very low solve rates on medium/hard 2D puzzles; failures include incorrect grid shape, violations of sub-grid rules, and inability to enforce multi-constraint layouts consistently.",
            "uuid": "e8343.2",
            "source_info": {
                "paper_title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Qwen-2.5 72B",
            "name_full": "Qwen-2.5 72B Instruct",
            "brief_description": "A large instruction-tuned Qwen family model evaluated on TEXTGAMES to study scale and instruction-following impact on puzzle solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5 72B Instruct",
            "model_description": "Instruction-tuned member of the Qwen-2.5 family (72B parameters) used in zero/one-shot experiments; inference performed with greedy decoding.",
            "model_size": "72B",
            "puzzle_name": "Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)",
            "puzzle_type": "2D grid-based logic puzzles requiring spatial arrangement and constraint satisfaction",
            "task_setup": "TEXTGAMES framework with prompt template, constraints, optional few-shot examples, grader verification, and multi-turn feedback (up to 3 turns); deterministic decoding.",
            "mechanisms_or_strategies": "In-context one-shot examples and iterative refinement via grader feedback; no symbolic search or external solver hooking described.",
            "performance_metrics": "Aggregate multi-turn solve rates on 2D puzzles (percent solved): Easy: turn1=28.2%, turn2=43.1%, turn3=51.1%; Medium and Hard performance are substantially lower (medium ~10-19% across turns; hard single-digit percent).",
            "evidence_of_spatial_reasoning": "Moderate evidence: larger Qwen shows improved easy-level 2D performance versus smaller sizes, but performance still degrades rapidly on medium/hard 2D tasks, implying limited robust spatial reasoning.",
            "comparisons": "Outperforms smaller Qwen variants and some other instruction models at easy difficulty, but is outperformed by GPT-o3 Mini and lags behind human performance on medium/hard puzzles.",
            "limitations_or_failure_cases": "Struggles to satisfy multiple spatial constraints simultaneously; frequent grader feedback indicates common failures such as wrong grid shape, wrong island counts, and invalid placement of elements.",
            "uuid": "e8343.3",
            "source_info": {
                "paper_title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o Mini",
            "name_full": "GPT-4o Mini",
            "brief_description": "A proprietary closed instruction-following model evaluated as a baseline; included to compare instruction-following vs reasoning-optimized models on TEXTGAMES.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o Mini",
            "model_description": "Closed, proprietary instruction-tuned model used in this study as a higher-end instruction-following baseline; evaluated with greedy decoding in zero/one-shot and multi-turn settings.",
            "model_size": "not specified (mini)",
            "puzzle_name": "Crossword Arranger; Text Sudoku; Islands (TEXTGAMES 2D puzzles)",
            "puzzle_type": "2D grid-based logic puzzles (crossword placement, Sudoku sub-grid constraints, island-count/grid construction)",
            "task_setup": "Standard TEXTGAMES prompt construction (P=(T,C,E,I)), grader-based single/multi-turn evaluation (N up to 3), greedy decoding; one-shot and zero-shot configurations tested.",
            "mechanisms_or_strategies": "In-context one-shot prompting, multi-turn iterative refinement informed by grader feedback; no external symbolic solver integration.",
            "performance_metrics": "Aggregate multi-turn solve rates on 2D puzzles (percent solved): Easy: turn1=20.7%, turn2=34.7%, turn3=40.9%; Medium and Hard were substantially lower (medium ~5–12% across turns; hard often &lt;3% across turns).",
            "evidence_of_spatial_reasoning": "Limited: GPT-4o Mini shows modest improvement with multi-turn feedback on easy tasks but fails on medium/hard 2D puzzles, indicating constrained spatial reasoning compared to reasoning-focused models.",
            "comparisons": "Outperformed by reasoning-optimized GPT-o3 Mini, roughly comparable or slightly better than some open-source instruction models at easy levels, but underperforms humans on nearly all hard 2D tasks.",
            "limitations_or_failure_cases": "Fails frequently on medium/hard spatial puzzles; common failure modes are inability to enforce sub-grid uniqueness (Sudoku), miscount islands/coconut constraints (Islands), and place words in a connected crossword satisfying all rules.",
            "uuid": "e8343.4",
            "source_info": {
                "paper_title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Langbirds: An agent for angry birds using a large language model",
            "rating": 2,
            "sanitized_title": "langbirds_an_agent_for_angry_birds_using_a_large_language_model"
        },
        {
            "paper_title": "Language models are crossword solvers",
            "rating": 2,
            "sanitized_title": "language_models_are_crossword_solvers"
        },
        {
            "paper_title": "Solving olympiad geometry without human demonstrations",
            "rating": 1,
            "sanitized_title": "solving_olympiad_geometry_without_human_demonstrations"
        },
        {
            "paper_title": "Do you even \"word game bench\" bro? (Word Game Bench)",
            "rating": 1,
            "sanitized_title": "do_you_even_word_game_bench_bro_word_game_bench"
        }
    ],
    "cost": 0.01757325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TEXTGAMES: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning
25 Feb 2025</p>
<p>Frederikus Hudi frederikus.hudi.fe7@naist.ac.jp 
Brown
University 4 MBZUAI</p>
<p>Genta Indra Winata genta.winata@capitalone.com 
Brown
University 4 MBZUAI</p>
<p>Ruochen Zhang ruochen_zhang@brown.edu 
Brown
University 4 MBZUAI</p>
<p>Alham Fikri alham.fikri@mbzuai.ac.ae 
Brown
University 4 MBZUAI</p>
<p>Naist 
Brown
University 4 MBZUAI</p>
<p>TEXTGAMES: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning
25 Feb 2025C4485DC2FB24B61C03FC9F5707D35422arXiv:2502.18431v1[cs.CL]
Reasoning is a fundamental capability of large language models (LLMs), enabling them to comprehend, analyze, and solve complex problems.In this paper, we introduce TEXTGAMES, an innovative benchmark specifically crafted to assess LLMs through demanding text-based games that require advanced skills in pattern recognition, spatial awareness, arithmetic, and logical reasoning.Our analysis probes LLMs' performance in both single-turn and multi-turn reasoning, and their abilities in leveraging feedback to correct subsequent answers through self-reflection.Our findings reveal that, although LLMs exhibit proficiency in addressing most easy and medium-level problems, they face significant challenges with more difficult tasks.In contrast, humans are capable of solving all tasks when given sufficient time.Moreover, we observe that LLMs show improved performance in multi-turn predictions through self-reflection, yet they still struggle with sequencing, counting, and following complex rules consistently.Additionally, models optimized for reasoning outperform pretrained LLMs that prioritize instruction following, highlighting the crucial role of reasoning skills in addressing highly complex problems.* Equal contributions.† The work was done outside Capital One.</p>
<p>Introduction</p>
<p>Reasoning is a fundamental skill essential for logical thinking and development, enabling large language models (LLMs) to tackle complex problems (Wei et al., 2022;Longpre et al., 2023;Srivastava et al., 2023).This skill emphasizes the need for creating LLMs capable of handling tasks such as mathematical (Hendrycks et al., 2021;Shao et al., 2024;Trinh et al., 2024), commonsense (Talmor et al., 2019;Geva et al., 2021;Brohan et al., 2023), and symbolic reasoning (Nye et al., 2021;Sprague  et al., 2024).In general, reasoning is a multifaceted ability that involves understanding the context and effectively applying inference to solve problems.Research on LLMs has examined their reasoning capabilities across various dimensions, including their capacity to follow instructions for multi-hop reasoning (Yang et al., 2024b), comprehend psychological concepts (Almeida et al., 2024), and use context in classification tasks (Winata et al., 2024), and constrained logical tasks (Zhou et al., 2023).LLMs have also demonstrated remarkable skills in game reasoning, such as solving crossword puzzles (Berruti et al., 2024;Saha et al., 2024;Zugarini et al., 2024), physics-based puzzle games (Oh et al., 2024), and turn-based games (Feng et al., 2024;Guo et al., 2024).A longstanding issue in reasoning with LLMs is their tendency to hallucinate and inconsistency during inference (Maynez et al., 2020;Ji et al., 2023a;Huang et al., 2024b).Recently, self-reflection techniques have been employed to mitigate these hallucinations and improve the performance of LLMs through multiple rounds of follow-up interactions (Ji et al., 2023b).Additionally, selfevaluation has been applied to question-answering tasks (Ren et al., 2023), offering feedback that enables models to correct themselves.Consequently, LLMs have demonstrated the ability to rectify errors across various domains, gradually producing correct answers over successive iterations (Shinn et al., 2024).Despite these advancements, we aim to further challenge LLMs by engaging them with puzzles that require a combination of skills, including pattern recognition, spatial awareness, arithmetic, and logical thinking.</p>
<p>In our work, we introduce TEXTGAMES, a new benchmark designed to assess the proficiency of LLMs in solving text-based logical puzzle games and performing complex, constraint-based reasoning.The intricate rules of these puzzles allow us to evaluate the LLMs' capacity to follow detailed instructions.Additionally, we investigate whether LLMs can self-reflect on their previous generations when given feedback, correcting their errors by responding to specific error messages and refining their outputs.We also provide performance comparison between reasoning-specialized LLMs, with models that emphasize instruction-following.Our analysis indicates that even the recent advanced LLMs, such as the Llama 70B (Dubey et al., 2024) and Qwen2 72B Instruct (Yang et al., 2024a) models, perform adequately on Easy and Medium levels but struggle at the Hard level.In contrast, models specifically optimized for reasoning, like GPT-o3 Mini, exhibit strong performance on these more difficult tasks, as illustrated in Figure 1.We hypothesize that this disparity arises because TEXTGAMES demands a high level of reasoning ability to comprehend the rules and apply a combination of reasoning skills to solve the problems that Instruct models may not fully possess.</p>
<p>In summary, our contributions are threefold:</p>
<p>• We introduce TEXTGAMES,1 a text-based game benchmark that assesses LLMs' various logical reasoning skills.The benchmark features eight puzzle games across three difficulty levels.Figure 2 offers an overview of the game visualizations.</p>
<p>• We perform a thorough evaluation across a range of LLMs, including both off-the-shelf and proprietary models, in zero-shot and oneshot scenarios.We additionally compare their performance with that of human participants.</p>
<p>• We demonstrate that LLMs improve when given feedback in multi-turn interaction, enabling them to self-reflect on previous generations.Our observation on reasoning-focused models' performance also reveals that there can be diminishing returns on test-time scaling in some difficult games.</p>
<p>TEXTGAMES BENCHMARK</p>
<p>We introduce our benchmark TEXTGAMES, which comprises eight text-based puzzle games, each featuring three distinct levels of difficulty, aimed at evaluating the reasoning abilities of LLMs.These games are meticulously designed to assess a wide array of reasoning skills, encompassing both abductive and deductive reasoning.Additionally, we differentiate various skills through diverse output formats, as described in Table 1.</p>
<p>List of Games</p>
<p>We provide a detailed definition of the games as follows:</p>
<p>Anagram Scribble</p>
<p>Given a list of Latin characters, the player's objective is to arrange them into a valid N -character English word, without regard to case sensitivity.We explore two scenarios: one where characters can be used multiple times and another where each character can only be used once.</p>
<p>Password Game</p>
<p>Given a set of rules, the player is challenged to construct a sequence of characters that fulfills all specified requirements, similar to creating a password.These rules involve generating text based on character counts, incorporating English alphanumeric characters, distinguishing between uppercase and lowercase letters, and including special characters and Roman numerals.Additionally, we introduce more complex tasks that require commonsense knowledge, such as identifying the capital city or continent of a specified country.Furthermore, we add simple arithmetic constraints, such as "The text must include a number equal to seven times six."</p>
<p>Bracket Game</p>
<p>Given a concatenation of several English words, the player is tasked with enclosing segments of the text using four different types of parentheses: '[]', '{}', '()', and '&lt;&gt;'.These brackets must be correctly paired where each open bracket must have a corresponding close bracket, and vice versa.Additionally, there are requirements regarding bracket depth that the player must adhere to.</p>
<p>String Search</p>
<p>Given a random sequence of characters mixed with some valid English words, the player is challenged to find a substring-a consecutive sequence of characters-that meets a specified set of rules.These rules dictate conditions such as the length of the substring, required characters, prohibited characters, and whether the resulting substring must be a palindrome.</p>
<p>Crossword Arranger</p>
<p>Given a list of English words, each of length N , the player is tasked with arranging these words into a crossword puzzle.Without any repetitions, a total of 2N words from the list must be placed in either a horizontal or vertical orientation, forming a connected configuration within an N × N square grid.Blank cells are not used to separate the words.</p>
<p>Text Sudoku</p>
<p>Given a sparsely filled square grid of size N 2 ×N 2 , the player is tasked with filling the blank cells with numbers such that no identical numbers appear within the same row, column, or N × N sub-grid.</p>
<p>The player must fill only the blank cells, leaving the pre-filled cells unchanged.We utilize grids with N equal to 2 and 3, meaning the numbers range from 1 to 4 and 1 to 9, respectively.Alternatively, these numbers can be substituted with unique characters; for instance, we experiment with using Latin alphabets 'A' to 'I' in place of numbers 1 to 9.</p>
<p>Islands</p>
<p>Given a grid size of N , along with a specified set of rules, the player must construct an N × N square grid using the characters '.', '#', or 'o', which represent water, land, and coconut trees, respectively.A contiguous group of land tiles connected in the four cardinal directions forms an island.The task requires adherence to all rules, which govern the number of islands, the size of each island, and the allowable number of coconut trees.</p>
<p>Ordering Text</p>
<p>Given a set of scoring rules and a list of words, the player is tasked with sorting the list from the highest-scoring word to the lowest.The scoring rules encompass checks for the presence of specific character sequence patterns, the length of the words, as well as the prefixes and suffixes of the words.Points in each scoring rule can range from −100 to 100.
-Words = 3 -3 ≤ Word Length ≤ 8 -2 ≤ Rules ≤ 4 -4 ≤ Words ≤ 6 -3 ≤ Word Length ≤ 8 -4 ≤ Rules ≤ 8 -6 ≤ Words ≤ 10 -3 ≤ Word Length ≤ 15
Table 2: Difficulty levels of TEXTGAMES puzzle games detailed with associated constraints and rules.</p>
<p>Challenges and Difficulty Levels</p>
<p>For comprehensive details about the games, including formats, categories, and the reasoning skills required, please refer to Table For instance, the performance of LLMs on easy 2D puzzles is comparable to their performance on medium-difficulty 1D puzzles, while their performance on medium 2D puzzles parallels that on hard 1D puzzles.This is illustrated in Figure 1, highlighting the challenges LLMs face with 2D spatial reasoning.</p>
<p>Game Generation</p>
<p>For each game, we create instances by randomly sampling according to the specified rules for each difficulty level, resulting in 1,000 test samples per difficulty.This amounts to a total of 24,000 test samples across all games and difficulty levels.Additionally, we generate a number of training samples for few-shot learning across all difficulties, ensuring that these samples do not overlap with the test set.We refer to the test samples as D test and the training samples as D train .</p>
<p>TEXTGAMES Evaluation</p>
<p>For our TEXTGAMES, we design a game evaluation framework where LLMs emulate player behavior to play the games.This system uses a LLM to generate solutions and integrates a grader to verify their correctness.To further test models' performance, we implement multi-turn prompting, enabling the model to iteratively refine its responses.This iterative process involves receiving feedback from the grader, which allows the models to self reflect and attempt to correct the answers.</p>
<p>Prompt Generation</p>
<p>We utilize in-context learning prompts to generate answers and evaluate the capabilities of LLMs under two configurations: zero-shot and one-shot prompts.Our prompt is defined as P ← (T, C, E, I), where it is constructed using a prompt template T , along with constraints C, one-shot examples E, and relevant context I from previous interactions for multi-turn scenarios.We denote the LLMs used for inference as θ and the grader that evaluates the correctness of the answers as G.</p>
<p>Detailed information about the prompts for each game is provided in Appendix H.</p>
<p>Multi-Turn Prompting</p>
<p>Algorithm 1 outlines the procedure for implementing multi-turn prompting, a strategy that iteratively refines responses based on feedback from a grader.At each turn, the model generates a response given</p>
<p>Algorithm 1 TEXTGAMES Evaluation System</p>
<p>Require: LLM θ, Grader G, Template T , Dataset D {train,test} .Initialize:
Few-shot example(s) E ⊆ D train . Initialize: Maximum Turn N = 3. 1: for all Constraints C ∈ D test do 2: I ← [ ] 3: for i = 1, . . . , N do 4: P ← (T, C, E, I) ▷ Prompt construction 5: R ← θ(P ) ▷ LLM Response 6: S, F ← G(C, R) ▷ is_solve, feedback 7:
if S is True then 8:</p>
<p>Break the for loop 9: else 10:
I ← I + [R, F ] ▷ Update interactions 11: end if 12:
end for 13: end for the test constraint, few-shot examples, and previous interactions.The grader evaluates the response and provides feedback if errors are detected.The interaction history is updated with both the response and feedback, allowing the model to adjust its outputs in subsequent turns.The process terminates early if the grader confirms a correct response, ensuring adaptability while enabling iterative refinement.A complete list of feedback for all games can be found in Appendix J.</p>
<p>Experimental Setup</p>
<p>For each task described in Section 2, we begin by developing a grader to verify the correctness of the answers.These graders function similarly to those used on online judge platforms or in competitive programming contests, focusing solely on determining whether an answer is correct or incorrect.Subsequently, we evaluate the performance of various LLMs using these graders.Additionally, we have created a web-based platform to collect data for testing human performance on the same tasks, allowing for a comprehensive comparison between human and model capabilities.</p>
<p>Models</p>
<p>We employ several open-sourced LLMs known for their competitive performance on various benchmarks, including Gemma-2 9B and 27B Instruct (Team et al., 2024), Llama-3.1 8B Instruct, Llama-3.3 70B Instruct (Dubey et al., 2024), and the Qwen-2.5 instruct models of different scales (7B, 14B, 32B, and 72B) (Yang et al., 2024a).Additionally, we include proprietary closed models like GPT-4o Mini and GPT-3o Mini, given that mini models offer a good balance between performance and cost efficiency.For model inference, we implement greedy decoding to maintain deterministic outcomes.Specifically, for GPT-o3 Mini, we configure the settings to prioritize the shortest reasoning generation option.We use accuracy or solve rate as our evaluation metric to measure the correctness of the answer.</p>
<p>Human Annotation</p>
<p>To understand how humans play and to compare their abilities with those of LLMs, we develop a web-based interface 2 that enables human participants to engage with our games.Through this platform, we document interactions between participants and our grading system, capturing metrics such as solve rates, the number of attempts, and the time taken to solve.These data allow us to directly compare human capabilities to those of LLMs.Each participant is asked to solve 2 to 3 different sessions.Details regarding the demographics of the annotators are available in Appendix E.</p>
<p>Results and Analysis</p>
<p>Our findings indicate that our benchmark poses a considerable challenge for LLMs.Even at the easiest difficulty level, the majority of models struggle to solve the games.An exception is the highly capable GPT-o3 Mini, which succeeds on only a subset of the games.This highlights the persistent difficulty of our benchmark for LLMs, highlighting areas where further advancements are needed.</p>
<p>Model Scaling Improves Performance.Larger models generally exhibit superior performance, particularly when comparing models within the 2 https://huggingface.co/spaces/fhudi/textgames same family (e.g., Gemma-2 9B vs. 27B Instruct), where the larger model consistently outperforms its smaller counterpart.Notably, the Gemma-2 27B Instruct model remains highly competitive despite being significantly smaller than other 70B+ baselines.Typically, larger models excel on easier tasks; however, this advantage does not necessarily extend to more challenging tasks, such as those requiring reasoning in two-dimensional coordinates.This trend is illustrated in Figure 6 in the Appendix.</p>
<p>Multi-Turn Feedback Improves LLM Performance.While LLMs typically underperform on single-turn attempts, we observe noteworthy improvements when they receive feedback explaining why their previous responses were incorrect.These enhancements are most evident at the easy difficulty level.Figures 4 and 7 illustrate this posi- tive trend, showcasing how LLMs effectively use feedback from previous interactions to engage in self-reflection and refine their subsequent outputs.
Easy Medium Hard Model Turn # #1 #2 #3 #1 #2 #3 #1 #2 #3 Gemma-2
A similar trend is evident in the results for various models, as shown in Table 3 for 1D games and Table 4 for 2D games.</p>
<p>TEXTGAMES Are Solvable by Humans.When comparing LLM performance to human performance, we observe that humans can easily achieve full scores, especially on the easy difficulty.This is because some problems, particularly at lower difficulty levels, are arguably trivial for adult hu-mans.On average, humans could solve all the problems within 2 attempts except for Ordering Text on the medium difficulty.This finding is particularly interesting given that recent research suggests LLMs exhibit intelligence seemingly on par with humans (Achiam et al., 2023).Yet, these models struggle with tasks as simple as searching for a substring and placing a bracket around it or constructing a 2D string with a predefined number of "islands."At higher difficulty levels, we observe a decline in human performance, reflected in the lower one-turn solve rate and increased time re- quired to solve.However, while LLMs exhibit a similar trend, most models fail to solve any hard problems, whereas humans still manage to solve them in one turn.</p>
<p>Misaligned Difficulty Perception between LLMs and Humans.The "Islands" and "String Search" games are among the easiest problems for humans; even at the hardest difficulty, humans typically solve them in fewer than two turns, making them some of the fastest problems to complete.In contrast, LLMs struggle significantly with these tasks, generally exhibiting subpar performance.This highlights a discrepancy in difficulty perception between humans and LLMs and sheds light on the fundamental differences in how humans and LLMs approach constrained puzzle-solving.</p>
<p>Inverse-Scaling on Reasoning Length and Performance Previous studies have generally shown that longer reasoning sequences enhance performance.Interestingly, this pattern is not evident in GPT-o3 Mini (Figure 5).We observe that GPT-o3 Mini tends to produce incorrect answers more frequently with extended reasoning tokens, particularly in the Bracket Game, Islands, and Ordering Text.Although GPT-o3 Mini does not disclose its reasoning process, we hypothesize that it may become confused by its own extended reasoning, resulting in overcomplicated solutions or incorrect understanding.An empirical example is illustrated by the recent DeepSeek R1 hallucination, where the system initially provided a correct answer but, after further analysis and reasoning, can be misled into an incorrect conclusion, shown in</p>
<p>Conclusion</p>
<p>We present TEXTGAMES, a text-based puzzle game benchmark designed to evaluate the diverse reasoning abilities of LLMs, including pattern recognition, spatial awareness, arithmetic, and logical reasoning.In addition to only evaluating single-turn solve rate, our evaluation system also implement feedback in multi-turn gameplay settings and test whether models improve through self-reflection.Results show that while LLMs proficiently solve most easy and medium-level problems, they encounter significant challenges with more difficult tasks that demand comprehensive reasoning.In contrast, humans can solve all tasks given sufficient time.We show significant performance improvement with multi-turn prediction via self-reflection.We hope TEXTGAMES could contribute to uncovering and analyzing the weaknesses of LLMs in complex reasoning tasks.</p>
<p>Limitations</p>
<p>In this paper, we focus our investigation by not exhaustively evaluating every possible model, owing to resource constraints.Instead, our primary objective is to develop a benchmark that serves as a platform for future research exploration on reasoning.</p>
<p>Ethical Considerations</p>
<p>In conducting our research, which focuses on evaluating LLMs for complex reasoning tasks, we are committed to upholding the highest standards of transparency and fairness in all aspects of our data collection and evaluation processes.We ensure that the methodologies and criteria used for assessment are clearly documented and unbiased, promoting fair comparisons across different models.Our commitment to these principles aims to foster trust and accountability in our research outcomes.</p>
<p>A GPU computation usage and Hyperparameters</p>
<p>We employ NVIDIA GPUs, RTX A6000 (48GB) and RTX 6000 (48GB), to run inference for the whole open model which took us the equivalent of ∼650 GPU hours.We apply the default parameters as defined from each models respective Hugging-Face's page for all of our experiments.To allow reproducibility, we use greedy decoding, i.e. by setting parameter do_sample to False.</p>
<p>B Dataset License</p>
<p>We will release our dataset under the open-source CC-BY-SA 4.0 license, facilitating redistribution for future research.</p>
<p>C Attribution</p>
<p>The icon images on Figure 2 is taken from https: //flaticon.com.They are freely for personal and commercial use with attribution.</p>
<p>D Model Scale Improvement</p>
<p>E Annotator Demographic</p>
<p>There are 4 annotators, within the age range of 25-35 years old, voluntarily participating in our experiments.All annotators are from Computer Science background with a degree of magisterial or doctoral.All 4 annotators are fluent English speakers from Asia-based origins with experience living in English-speaking countries and have been using English for more than 15 years.All annotators have given consent for using, releasing and redistributing their annotations.</p>
<p>F Multi-turn Results Visualization</p>
<p>G Complete Experiment Results</p>
<p>We report the complete results that include all the models we evaluated on, as illustrated in Figure 8.The numerical results of these models can be found in Table 6 and Table 7, with the respective Zero-Shot setting performance in Table 8 and Table 9.We also report the performance of multi-turn settings for each game: Anagram Scribble in
Crossword Easy Med Hard Model Turn # #1 #2 #3 #1 #2 #3 #1 #2 #3 Gemma-2 9B Instruct 2.Turn # #1 #2 #3 #1 #2 #3 #1 #2 #3 Islands Easy Med Hard Model Turn # #1 #2 #3 #1 #2 #3 #1 #2 #3 Gemma-2</p>
<p>H Prompt Templates and Games Constraints</p>
<p>We detail the prompt templates and constraints for prompt constructions here: Anagram Scribble in    A group of connected land tiles in 4 cardinal directions forms an island.</p>
<p>Your 2D grid must follow the following rules: -There must be exactly [K] islands.</p>
<p>-The size of each island must be from [Ymin] to [Ymax] tiles each.</p>
<p>-There must be exactly [L] islands that have coconut trees on them.</p>
<p>-There must be exactly [C] total coconut trees.</p>
<p>Print only the answer.Crossword Arranger Mismatch answer length found!! Expected size of <integer>, got <integer>.Mismatch answer word found!! &lt;'Horizontal'|'Vertical'&gt; word <string> is not in the word set.</p>
<p>Text Sudoku</p>
<p>There are unfilled cells Your answer is wrong in shape, it should be <int>x<int> sudoku.There are unrecognized characters, or possibly unfilled cells."One or more characters are replaced" Islands 2D grid is not <int> x <int>.(<int pred > x <int pred >) 2D contains invalid character (<char>) There must be exactly <int> islands, but you provided <int> islands The size of each island must be from <int> to <int> tiles There must be exactly <int> islands that have coconut trees on them There must be exactly <int> total coconut trees.</p>
<p>Ordering Text</p>
<p>Your answer is too short.There should be <int> items.<str answer > is not supposed to be the <str ordinal_number > word in the order.</p>
<p>Figure 1 :
1
Figure 1: Single-turn performance on TEXTGAMES games across 1D and 2D Puzzles challenges with varying difficulty levels (top), alongside the improvement in accuracy achieved through increased turn attempts via self-reflection, with the x-axis representing the number of turns (bottom).</p>
<p>Figure 2 :
2
Figure 2: TEXTGAMES BENCHMARK consists of eight text-based puzzle games, each with unique constraints and gameplay mechanics.The top four games are 1D Puzzles, while the bottom four are 2D Puzzles.</p>
<p>≤ 10 characters -Text length ≤ 20 characters -Text length ≤ 40 characters -At most 2 constraints -At most 3 constraints -At most 5 constraints -Multiple solutions may exist -Multiple solutions may exist -Single solution -</p>
<p>Figure 3 :
3
Figure 3: LLM Results on TEXTGAMES BENCHMARK in the one-shot setting.Med indicates Medium-difficulty level.*For GPT-o3 Mini, we present the results from zero-shot setting.</p>
<p>Figure 4 :
4
Figure 4: LLM performance on the Bracket Game in the one-shot setting, excluding GPT models.The results show that increasing the number of turns generally enhances performance.A similar trend is evident in Crossword Arranger, as shown by Figure 7 in the Appendix F showing illustrations from all games</p>
<p>Figure 5 :
5
Figure 5: In hard games, the test-time scaling of GPT-o3 Mini displays inverse scaling behavior, with longer reasoning traces often leading to incorrect results.</p>
<p>Figure 6
6
Figure 6 illustrates how the scale of the model impacts performance, with variations depending on task difficulty.</p>
<p>Figure 6 :
6
Figure 6: Model scaling improves easier tasks.</p>
<p>Figure 7 :
7
Figure 7: Multi-turn with feedback based on the performance for each game puzzle.</p>
<p>Figure 8 :
8
Figure 8: Complete LLMs results against TEXTGAMES BENCHMARK.*We present zero-shot results as reference</p>
<p>You are given a text [S]Your job is to put some valid parenthesis brackets in the text such that:-[W1] is inside a [B1] bracket • • • -[WN] is inside a [BN] bracketThe open and close parenthesis for block is [ ], curly is , round is ( ), and angle is &lt; &gt;.The bracket depth must be [D] and print only the answer <Example> Constraints (C): The text is [S] = 'fabuloustextgames', and [W] = ['games', 'text', 'fabulous'] are inside [B] = [round, angle, block] bracket, respectively.Depth must be [vowels than consonants γ -has less vowels than consonants γ -has the same amount of vowels and consonants γ</p>
<p>Table 1 :
1
Detailed information on TEXTGAMES puzzle games, encompassing a broad spectrum of output formats, categories, skillsets, and reasoning types.
TaskOutput FormatCategorySkill-setsReasoning1D PuzzlesAnagram ScribbleSingle-line textEnglish wordsPattern Recognition, KnowledgeAbductivePassword GameSingle-line textNumbers &amp; CharactersArithmetic, KnowledgeAbductiveBracket GameSingle-line textCoordinatesCountingDeductiveString SearchSingle-line textString MatchingLogical ThinkingDeductive2D PuzzlesCrossword Arranger2D-GridEnglish wordsPattern Recognition, Spatial Awareness DeductiveText Sudoku2D-GridNumbers &amp; CharactersSpatial AwarenessDeductiveIslands2D-GridCoordinates &amp; GeometrySpatial AwarenessAbductiveOrdering TextMultiple wordsStrings &amp; SortingArithmetic, ComparativeDeductiveGameEasyMediumHardAnagram Scribble-3 to 5 letter English word -Character list ≤ 10 -Repeatable use of char-6 to 7 letter English word -Character list ≤ 10 -Repeatable use of char-8 to 10 letter English word -Character list ≤ 10 -Each char can only be used oncePassword -2 Rules-4 Rules-6 Rules</p>
<p>Table 3 :
3
Average solve rate (%) for multi-turn 1D Puzzles.
9B Instruct39.8 58.0 64.1 12.0 19.7 25.22.53.64.2Gemma-2 27B Instruct 50.7 77.0 82.4 18.9 37.5 46.74.17.19.3Llama-3.1 8B Instruct40.8 52.4 58.6 11.2 16.2 18.91.22.12.6Llama-3.3 70B Instruct 55.8 78.7 86.4 23.9 43.0 56.65.1 10.2 15.3Qwen-2.5 7B Instruct30.6 44.6 52.58.4 14.2 18.51.21.82.3Qwen-2.5 72B Instruct60.7 75.4 81.5 26.9 40.3 49.33.97.8 11.1GPT-4o Mini59.6 74.3 79.0 22.1 37.6 45.36.39.4 11.6GPT-o3 Mini96.5 98.9 99.4 87.2 96.2 97.4 45.1 69.5 78.0EasyMediumHardModelTurn ##1#2#3#1#2#3#1#2#3Gemma-2 9B Instruct19.1 30.4 36.72.95.17.90.30.81.6Gemma-2 27B Instruct 21.4 36.0 44.45.99.3 13.50.91.62.6Llama-3.1 8B Instruct6.6 12.7 20.71.42.03.70.10.30.7Llama-3.3 70B Instruct 23.2 38.0 48.44.07.3 10.70.92.23.4Qwen-2.5 7B Instruct12.5 20.7 26.32.64.86.50.30.71.3Qwen-2.5 72B Instruct28.2 43.1 51.1 10.4 15.7 19.10.92.13.4GPT-4o Mini20.7 34.7 40.95.19.5 12.40.71.82.6GPT-o3 Mini87.8 94.9 98.8 66.5 80.7 86.9 20.6 40.5 48.6</p>
<p>Table 4 :
4
Average solve rate (%) for multi-turn 2D Puzzles.
1 st Turn Solve Rate (%)Avg. AttemptsAvg. Time to Solve (s)Easy Medium Hard Easy Medium Hard Easy Medium Hard1D PuzzlesAnagram Scribble100.087.557.1 1.001.12 2.14 11.782.6 263.5Password Game88.9100.044.4 1.221.00 1.78 27.244.473.4Bracket Game100.075.075.0 1.001.25 1.25 29.348.971.2String Search100.0100.075.0 1.001.00 1.38 14.617.441.42D PuzzlesCrossword Arranger77.8100.088.9 1.331.00 1.11 32.2138.7 128.2Text Sudoku100.0100.077.8 1.001.00 1.78 11.729.5 536.3Islands100.0100.0 100.0 1.001.00 1.00 12.225.541.4Ordering Text55.657.142.9 1.673.14 2.00 72.3127.5 424.3</p>
<p>Table 5 :
5
Performance of human annotators on playing TEXTGAMES BENCHMARK.</p>
<p>(Guan et al., 2025;aHu et al., 2024b;dvancement ofLLMs, recent works examine their capabilities in playing games or assisting humans in gameplay(Hu et al., 2024a).Classical games like Go(Silver et al., 2017), chess(Feng et al., 2024), Poker(Huang et al., 2024a)have been used as initial testbeds for evaluating models' planning and decision-making abilities.More recently, more works have explored other genres for more dynamic and complex situations like text-based games(Xiao and Yang, 2024; Stojanovski, 2024;  Kazemi et al., 2024), communication games(Guan et al., 2025; Xu et al., 2025), and modern strategic video games(Zhang et al., 2023;Hu et al., 2024b;
6 Related WorkText-based Reasoning. Text-based reasoninghas been extensively studied across various do-mains, including commonsense reasoning (Rajaniet al., 2019; Bhargava and Ng, 2022; Zhao et al.,2023), mathematical reasoning (Patel et al., 2021;Zhao et al., 2022; Lu et al., 2023), logical reason-ing (Pan et al., 2023), causal reasoning (Wang,2024; Jin et al., 2024), and agent-based reason-ing (Motwani et al., 2024). While existing bench-marks assess different aspects of reasoning, theyoften evaluate these abilities in isolation. In con-trast, TEXTGAMES assesses LLMs' capacity for in-tegrating multiple reasoning skills, offering a richerevaluation of model strengths and weaknesses.
Qi et al., 2024;Rao et al., 2024;Ma et al., 2025);Ma et al., 2025).In comparison, TEXTGAMES takes inspiration from real-life text puzzle games and emphasizes evaluating LLM's capabilities in simple logic reasoning.Additionally, each game come with different level of difficulty for assessing the models' robustness.</p>
<p>Chang Xiao and Brenda Z Yang.2024.Llms may not be human-level players, but they can be testers: Measuring game difficulty with llm agents.arXiv preprint arXiv:2410.02829.
David Silver, Julian Schrittwieser, Karen Simonyan,Ioannis Antonoglou, Aja Huang, Arthur Guez,Thomas Hubert, Lucas Baker, Matthew Lai, AdrianBolton, et al. 2017. Mastering the game of go withouthuman knowledge. nature, 550(7676):354-359.Zelai Xu, Wanjun Gu, Chao Yu, Yi Wu, and Yu Wang.Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez,2025. Learning strategic language agents in the were-Dongwei Jiang, Manya Wadhwa, Prasann Singhal,wolf game with iterative latent space policy optimiza-Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Dur-tion. arXiv preprint arXiv:2502.04686.rett. 2024. To cot or not to cot? chain-of-thoughthelps mainly on math and symbolic reasoning. arXivAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,preprint arXiv:2409.12183.Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,technical report. arXiv preprint arXiv:2412.15115.Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabili-ties of language models. Transactions on MachineSohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. 2024b. Do large lan-guage models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837.Learning Research.Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, andZafir Stojanovski. 2024. Do you even "word game bench" bro? https://wordgamebench.github.io. Word Game Bench is nothing without its single un-Zongqing Lu. 2023. Creative agents: Empowering agents with imagination for creative tasks. arXiv preprint arXiv:2312.02519.employed maintainer.Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang.Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowl-edge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for2022. Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6588-6600.Computational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages 4149-4158.Zirui Zhao, Wee Sun Lee, and David Hsu. 2023. Large language models as commonsense knowledge for large-scale task planning. Advances in Neural Infor-Gemma Team, Morgane Riviere, Shreya Pathak,mation Processing Systems, 36:31967-31987.Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-raju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118.Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evalu-ation for large language models. arXiv preprint arXiv:2311.07911.Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He,Andrea Zugarini, Kamyar Zeinalipour, Surya Sai Kadali,and Thang Luong. 2024. Solving olympiad ge-Marco Maggini, Marco Gori, and Leonardo Rigutini.ometry without human demonstrations. Nature,2024. Clue-instruct: Text-based clue generation for625(7995):476-482.educational crossword puzzles. In Proceedings ofthe 2024 Joint International Conference on Compu-Zeyu Wang. 2024. Causalbench: A comprehensivetational Linguistics, Language Resources and Evalu-benchmark for evaluating causal reasoning capabil-ation (LREC-COLING 2024), pages 3347-3356.ities of large language models. In Proceedings ofthe 10th SIGHAN Workshop on Chinese LanguageProcessing (SIGHAN-10), pages 143-151.Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:24824-24837.
Genta Indra Winata, Ruochen Zhang, and David Ifeoluwa Adelani.2024.Miners: Multilingual language models as semantic retrievers.arXiv preprint arXiv:2406.07424.</p>
<p>Table 10 ,
10
Password Games inTable 11, Bracket Game in Table 12, String Search in Table 13, Crossword Arranger in Table 14, Text Sudoku in Table 15, Islands in Table 16, and Ordering Text in Table 17.</p>
<p>† Indicates first 20% of dataset only.</p>
<p>Table 6 :
6
Complete Average Results (%) for 1D Puzzles.<em>Zero-shot results as reference
CrosswordSudokuIslandsOrderingEasy Med Hard Easy Med Hard Easy Med Hard Easy Med HardGemma-2 9B Instruct2.10.00.0 25.94.80.0 22.82.90.3 25.54.00.9Gemma-2 27B Instruct7.10.50.0 38.8 13.60.0 14.55.81.9 25.43.81.5Llama-3.1 8B Instruct2.20.00.00.10.00.03.52.70.1 20.42.80.5Llama-3.1 70B Instruct8.90.40.1 14.72.80.0 31.29.11.0 24.53.61.0Llama-3.3 70B Instruct9.70.60.0 12.93.10.0 45.88.13.1 24.34.20.7Qwen-2.5 7B Instruct2.20.20.0 18.04.80.06.92.40.3 22.83.00.9Qwen-2.5 14B Instruct0.00.00.0 35.9 17.70.38.21.20.5 27.44.10.5Qwen-2.5 32B Instruct2.30.00.0 41.7 22.00.4 43.38.12.2 31.95.70.4Qwen-2.5 72B Instruct5.20.00.0 43.0 22.10.4 35.8 13.02.5 28.86.50.8GPT-4o Mini6.90.40.0 25.54.90.0 19.69.11.9 30.65.80.9GPT-o3 Mini</em>57.98.00.7 99.2 80.92.0 95.5 81.1 57.4 98.6 96.0 22.5DeepSeek-R1-Distill 8B  †</p>
<p>Table 7 :
7
Complete Average Results (%) for 2D Puzzles.*Zero-shot results as reference
AnagramPasswordBracketStringEasy Med Hard Easy Med Hard Easy Med Hard Easy Med HardGemma-2 9B Instruct77.7 14.21.0 36.4 13.84.62.30.30.0 37.7 20.25.3Gemma-2 27B Instruct88.0 31.83.1 45.9 18.76.6 29.85.40.0 44.1 23.74.4Llama-3.1 8B Instruct56.37.50.1 31.87.21.30.10.00.0 17.26.90.3Llama-3.1 70B Instruct69.5 27.72.5 45.1 19.05.6 23.06.00.4 45.9 25.45.8Llama-3.3 70B Instruct77.4 30.93.2 47.5 20.05.9 34.0 14.00.6 45.0 26.35.4Qwen-2.5 7B Instruct8.20.20.0 32.99.21.36.41.10.0 25.4 11.11.4Qwen-2.5 14B Instruct23.17.20.6 34.0 12.02.4 22.93.70.0 32.7 14.41.8Qwen-2.5 32B Instruct68.7 16.72.3 47.2 20.26.0 47.6 15.80.3 41.4 27.06.1Qwen-2.5 72B Instruct30.00.40.1 50.0 21.88.3 58.3 19.90.0 46.7 26.77.0GPT-4o Mini79.9 26.75.7 46.9 18.96.5 26.67.20.0 39.8 28.17.6GPT-o3 Mini99.6 91.6 37.4 90.1 74.6 51.9 97.3 84.9 21.5 99.2 97.9 69.8DeepSeek-R1-Distill 8B  †</p>
<p>Table 8 :
8
Complete Average Results (%) for 1D Puzzles (Zero-Shot).
CrosswordSudokuIslandsOrderingEasy Med Hard Easy Med Hard Easy Med Hard Easy Med HardGemma-2 9B Instruct1.10.00.0 24.33.10.01.80.00.5 20.62.60.9Gemma-2 27B Instruct6.60.00.0 39.1 15.30.0 10.32.40.0 21.92.91.2Llama-3.1 8B Instruct0.00.00.00.00.00.00.01.30.00.30.00.0Llama-3.1 70B Instruct12.20.00.07.22.70.00.00.00.08.40.90.4Llama-3.3 70B Instruct5.50.10.07.11.40.01.93.20.82.10.10.0Qwen-2.5 7B Instruct0.00.00.00.00.00.00.00.00.0 22.12.00.3Qwen-2.5 14B Instruct1.30.00.0 31.1 15.10.30.81.30.3 18.62.70.8Qwen-2.5 32B Instruct7.30.00.0 34.2 15.50.50.00.00.0 27.35.20.7Qwen-2.5 72B Instruct0.00.00.0 42.7 20.00.10.00.00.0 22.94.60.6GPT-4o Mini14.04.60.11.10.20.0 31.85.20.8 22.62.51.0GPT-o3 Mini</p>
<p>Table 9 :
9
Complete Average Results (%) for 2D Puzzles (Zero-Shot).Table 10: 3-Turns Accuracy (%) of Anagram Scribble.Table 12: 3-Turns Accuracy (%) of Bracket Game.
AnagramEasyMedHardModelTurn ##1#2#3#1#2#3#1#2#3Gemma-2 9B Instruct63.4 80.4 83.1 13.6 26.2 36.91.62.02.3Gemma-2 27B Instruct 77.1 91.9 94.0 20.4 41.1 45.64.76.47.5Llama-3.1 8B Instruct73.4 80.0 82.6 23.4 29.0 32.01.11.72.3Llama-3.1 70B Instruct 84.0 91.1 93.4 25.1 40.6 49.35.07.07.8Llama-3.3 70B Instruct 72.7 88.6 92.5 18.1 36.7 49.83.96.07.2Qwen-2.5 7B Instruct31.5 51.6 63.69.3 14.2 19.30.60.71.1Qwen-2.5 14B Instruct64.0 78.0 83.9 15.4 21.1 26.30.51.31.9Qwen-2.5 32B Instruct67.9 83.6 88.4 20.8 32.9 42.52.93.54.0Qwen-2.5 72B Instruct75.4 82.2 88.4 17.4 26.4 35.60.22.22.8GPT-4o Mini84.5 93.6 95.6 19.4 36.7 45.36.58.5 10.7GPT-o3 Mini99.6 99.9 99.9 91.6 96.8 98.3 37.4 50.8 57.5PasswordEasyMedHardModelTurn ##1#2#3#1#2#3#1#2#3Gemma-2 9B Instruct35.6 47.0 50.1 15.4 21.4 23.35.28.29.5Gemma-2 27B Instruct 57.7 68.2 73.1 26.9 36.0 42.18.0 14.6 19.3Llama-3.1 8B Instruct29.1 42.6 50.59.6 15.2 19.02.54.55.5Llama-3.1 70B Instruct 58.1 73.2 79.4 27.5 40.8 47.78.3 15.7 21.5Llama-3.3 70B Instruct 60.1 74.0 81.0 29.2 40.9 47.4 10.3 16.9 21.6Qwen-2.5 7B Instruct37.7 45.1 47.69.4 14.3 17.01.42.43.0Qwen-2.5 14B Instruct44.9 61.8 67.2 15.8 26.4 32.73.77.69.6Qwen-2.5 32B Instruct54.8 68.7 74.4 23.1 36.3 43.37.3 14.5 18.2Qwen-2.5 72B Instruct55.0 66.1 72.7 25.5 37.2 43.59.3 14.8 17.6GPT-4o Mini51.2 60.9 64.5 22.4 30.5 34.08.1 13.3 15.6GPT-o3 Mini90.1 96.2 98.0 74.6 89.8 91.7 51.9 70.9 79.4Table 11: 3-Turns Accuracy (%) of Password Game.BracketEasyMedHardModelTurn ##1#2#3#1#2#3#1#2#3Gemma-2 9B Instruct26.6 57.7 67.13.59.3 12.40.00.00.0Gemma-2 27B Instruct 27.4 87.5 92.9 11.6 40.2 56.40.10.20.4Llama-3.1 8B Instruct27.4 40.3 46.93.89.5 12.20.00.50.7Llama-3.1 70B Instruct 42.7 81.6 94.3 23.9 43.7 68.90.34.3 19.2Llama-3.3 70B Instruct 46.5 87.3 96.3 20.4 48.9 72.70.06.6 17.4Qwen-2.5 7B Instruct30.6 44.3 51.62.76.79.20.00.00.0Qwen-2.5 14B Instruct45.7 61.3 69.9 10.4 16.1 23.90.00.10.8Qwen-2.5 32B Instruct66.5 82.1 87.1 25.1 41.4 50.10.12.86.2Qwen-2.5 72B Instruct65.9 88.9 92.9 39.3 60.1 74.70.34.7 11.4GPT-4o Mini51.5 76.4 84.6 14.1 37.4 49.30.02.35.0GPT-o3 Mini97.3 99.8 99.9 84.9 98.3 99.6 21.5 63.6 77.2</p>
<p>Table 14 :
14
3-Turns Accuracy (%) of Crossword Arranger.
SudokuEasyMedHardModel</p>
<p>Table 16: 3-Turns Accuracy (%) of Islands.
9B Instruct22.830.231.82.93.13.60.30.40.9Gemma-2 27B Instruct 14.522.531.45.87.48.71.92.93.5Llama-3.1 8B Instruct3.55.96.42.73.33.40.10.10.1Llama-3.1 70B Instruct 31.235.436.49.1 14.7 17.01.03.55.2Llama-3.3 70B Instruct 45.858.863.18.1 13.6 18.13.15.77.7Qwen-2.5 7B Instruct6.97.38.22.43.93.90.30.40.6Qwen-2.5 14B Instruct8.212.11.21.62.00.51.22.0Qwen-2.5 32B Instruct43.356.460.68.1 11.4 16.12.23.34.9Qwen-2.5 72B Instruct35.848.056.7 13.0 18.8 21.22.55.17.1GPT-4o Mini19.631.032.39.1 11.9 13.81.93.74.5GPT-o3 Mini95.5 100.0 100.0 81.1 95.9 98.9 57.4 80.5 88.1OrderingEasyMedHardModelTurn ##1#2#3#1#2#3#1#2#3Gemma-2 9B Instruct25.5 59.081.04.0 10.3 19.80.92.65.6Gemma-2 27B Instruct 25.4 64.783.93.8 11.6 25.51.53.36.9Llama-3.1 8B Instruct20.4 41.071.02.84.8 11.50.51.22.8Llama-3.1 70B Instruct 24.5 56.479.13.6 12.3 20.91.03.46.6Llama-3.3 70B Instruct 24.3 54.877.84.2 10.4 17.30.73.15.7Qwen-2.5 7B Instruct22.8 50.670.43.09.2 15.80.92.54.5Qwen-2.5 14B Instruct27.4 63.782.34.1 14.1 23.10.52.84.8Qwen-2.5 32B Instruct31.9 69.384.05.7 18.5 27.60.43.16.8Qwen-2.5 72B Instruct28.8 64.780.86.5 16.0 25.50.82.86.0GPT-4o Mini30.6 65.683.85.8 17.3 25.40.93.75.9GPT-o3 Mini98.6 99.9 100.0 96.0 99.0 99.5 22.5 73.8 89.0Table 17: 3-Turns Accuracy (%) of Ordering Text.</p>
<p>Table 18 ,
18
Password Games inTable 19, Bracket Game in Table 20, String Search in Table 21, Crossword Arranger in Table 22, Text Sudoku in Table 23, Islands in Table 24, and Ordering Text in Table 25.
<Prompt Template (P)>Construct a valid [N]-character English word from the following letters:'[C 1 ]', '[C 2 ]', . . ., '[C N+M ]'.Each character can be used multiple times. Please write None if there is no valid combination.Print only the answer.<Example>Constraints (C):-[N]=6-character English word.-Letters [C 1. . .8 ] = 'e', 'l', 'o', 'd', 'p', 'h', 'i'.Possible Answer:hoodie</p>
<p>Table 18 :
18
Anagram Scribble.
<Prompt Template (P)>Please write a text string without any space by following a set of given rules. Please write only theanswer and follow the following criteria:-the text has [C1]• • •-the text has [Cα]<Example>Constraints (C):-[C1] = 6 English characters-[C2] = 0 uppercase characterPossible Answer:hoodie<Possible Rules [Cχ]><Type><Repeatable>-only [N] characterscountingno-[N] uppercase characterscountingno-[N] lowercase characterscountingno-[N] latin charactercountingno-[N] number digitscountingno-[N] number of roman digitscountingno-[N] special characters, including '!', '@', '#', '$', '%', ' ', '&amp;', '*'countingno-[N] [Ch] charactercountingyes-[S] stringstring-matchingyes-the capital city of [S]knowledgeyes-the continent of [S]knowledgeyes-a number that equals to [Emath]mathyes-a number that equals to [Eword]mathyes<Parameters>• [N] ∈ Z
+ ; [Ch] ∈ {'A'...'Z', 'a'...'z'}; • [S] is a random English word; • [Emath]is an arithmetical expression written in number and symbols, e.g."4 + 2"; • [Eword] is an arithmetical expression written in words, e.g."four plus two";</p>
<p>Table 19 :
19
Password Game.</p>
<p>Table 21 :
21
String Search.You are asked to construct a 2D [N] x [N] grid, consisting of water tiles (denoted by '.'), land tiles (denoted by '#'), and coconut tree tiles (denoted by 'o').Coconut tree tiles are also considered as land tiles.
<Prompt Template (P)></p>
<p>Table 24 :
24
Islands.Your answer must be exactly <int> characters long Your answer must only contain the characters provided Your answer must not contain repeated characters Your answer is not a valid English word Password <str prediction > is not satisfying this rule: <str rule >.Bracket GameYou are not allowed to change the character sequence of base text <string> There is a closing bracket without an open bracket The depth of the bracket is <int>.The expected depth is <integer> The text '<str rule_text >' is not found in your answer.The text '<str rule_text >' is not inside any &lt;'block'|'curly'|'round'|'angle'&gt; bracket <char bracket_open > <char bracket_close > String Search <str answer > is not <int> characters long.<str answer > does not exist in <str input_text >. <str answer > does not have 2 consecutive consonants <str answer > has 2 consecutive consonants <str answer > does not have 2 consecutive vowels <str answer > has 2 consecutive vowels <str answer > has less or equal vowels than consonants <str answer > has more or equal vowels than consonants <str answer > does not have the same amount of vowels and consonants <char> does not appear in <str answer >. <char> exists in <str answer >. <str answer > is not a palindrome.
J Feedback TemplatesGameFeedbackAnagramScribble</p>
<p>Table 27 :
27
List of Feedback.</p>
<p>The code can be accessed at https://github.com/ fhudi/textgames.
AcknowledgmentsWe extend our gratitude to members of NAIST NLP Laboratory for their support of our project, with spacial thanks to Huayang Li for the insightful discussions and valuable recommendations.Given a set of rules to calculate point, sort the set of words in decreasing order.When there 2 or more words with same point, sort lexicographically.Rules:Print only the answer.L o o k i n g a t row 1 : _ , A, N .Maybe " man " c a n go h e r e : M, A, N .So row 1 : M, A, N .Then , column 1 : M, _ , _ .Maybe " map " v e r t i c a l l y : M, A, P .But row 1 , column 1 i s M, s o row 2 , column 1 would be A, and row 3 , column 1 would be P .So row 2 : A, G, O ; row 3 : P , E , R .Now , l e t ' s c h e c k t h e words u s e d : " n o r " i n column 3 , " a g e " i n column 2 , " man " i n row 1 , " map " i n column 1 .R e m a i n i n g words : ago , dub , p e r , pop .L o o k i n g a t row 2 : A, G, O .That ' s " ago " , which i s i n o u r l i s t .So row 2 : A, G, O .Now , row 3 : P , E , R .That ' s " p e r " , which i s i n o u r l i s t .So t h e g r i d would l o o k l i k e :Row 1 : M, A, N Row 2 : A, G, O Row 3 : P , E , R But w a i t , " map " i s M, A, P , b u t i n t h i s c o n f i g u r a t i o n , column 1 i s M, A, P , which i s " map " .So t h a t works .Now , l e t ' s c h e c k a l l words u s e d : man , ago , map , p e r , n o r .Wait , " n o r " i s N, O, R , b u t i n t h i s g r i d , column 3 i s N, O, R , which i s " n o r " .So t h a t ' s c o r r e c t .But we s t i l l h a v e " dub " and " pop " l e f t .Hmm, t h a t ' s a p r o b l e m b e c a u s e we ' ve u s e d a l l t h e words e x c e p t t h o s e two , b u t t h e y don ' t f i t i n t o t h e g r i d .Wait , no , l e t ' s c o u n t : we h a v e 8 words , and t h e g r i d u s e s 5 words : man , ago , map , p e r , n o r .So w e ' r e m i s s i n g " dub " and " pop " .T h a t means t h i s c o n f i g u r a t i o n d o e s n ' t u s e a l l t h e words , which i s r e q u i r e d .• • •Table26: DeepSeek-R1-Distill 14B Hallucinated despite getting the correct answer along the way and ended up changing the answer to the wrong one.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Exploring the psychology of llms' moral and legal reasoning. José Luiz Guilherme Fcf Almeida, Neele Nunes, Alex Engelmann, Marcelo Wiegmann, Araújo De, Artificial Intelligence. 3331041452024</p>
<p>Automatic crossword clues extraction for language learning. Santiago Berruti, Arturo Collazo, Diego Sellanes, Aiala Rosá, Luis Chiruzzo, Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024). the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)2024</p>
<p>Commonsense knowledge reasoning and generation with pretrained language models: A survey. Prajjwal Bhargava, Vincent Ng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on robot learning. PMLR2023</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Chessgpt: Bridging policy learning and language modeling. Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Jun Wang, Advances in Neural Information Processing Systems. 202436</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Richelieu: Self-evolving llmbased agents for ai diplomacy. Zhenyu Guan, Xiangyu Kong, Fangwei Zhong, Yizhou Wang, Advances in Neural Information Processing Systems. 202537</p>
<p>Can large language models play games? a case study of a self-play approach. Hongyi Guo, Zhihan Liu, Yufeng Zhang, Zhaoran Wang, arXiv:2403.056322024arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu, arXiv:2404.02039A survey on large language model-based game agents. 2024aarXiv preprint</p>
<p>Pok\'ellmon: A human-parity agent for pok\'emon battles with large language models. Sihao Hu, Tiansheng Huang, Ling Liu, arXiv:2402.011182024barXiv preprint</p>
<p>Pokergpt: An end-to-end lightweight solver for multi-player texas hold'em via large language model. Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, Yanru Zhang, arXiv:2401.067812024aarXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, ACM Transactions on Information Systems. 2024b</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 2023aACM Computing Surveys55</p>
<p>Towards mitigating llm hallucination via self reflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023b</p>
<p>Boardgameqa: A dataset for natural language reasoning with contradictory information. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, Advances in Neural Information Processing Systems, 36. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran. 2024. 202436Advances in Neural Information Processing Systems</p>
<p>The flan collection: designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, arXiv:2310.02255Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint</p>
<p>Large language models play starcraft ii: Benchmarks and a chain of summarization approach. Weiyu Ma, Qirui Mi, Yongcheng Zeng, Xue Yan, Runji Lin, Yuqiao Wu, Jun Wang, Haifeng Zhang, Advances in Neural Information Processing Systems. 202537</p>
<p>On faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Ramesh Sumeet, Chandler Motwani, Smith, Jyoti Rocktim, Markian Das, Rybchuk, Ivan Philip Hs Torr, Fabio Laptev, Ronald Pizzati, Christian Clark, Schroeder De Witt, arXiv:2412.01928Malt: Improving reasoning with multi-agent llm training. 2024arXiv preprint</p>
<p>Improving coherence and consistency in neural sequence models with dualsystem, neuro-symbolic reasoning. Maxwell Nye, Michael Tessler, Josh Tenenbaum, Brenden M Lake, Advances in Neural Information Processing Systems. 202134</p>
<p>Langbirds: An agent for angry birds using a large language model. Seungwon Oh, Insik Chung, Kyung-Joong Kim, 2024 IEEE Conference on Games (CoG). IEEE2024</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Civrealm: A learning and reasoning odyssey in civilization for decision-making agents. Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, arXiv:2401.105682024arXiv preprint</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Collaborative quest completion with llm-driven non-player characters in minecraft. Sudha Rao, Weijia Xu, Michael Xu, Jorge Leandro, Ken Lobb, Gabriel Desgarennes, Chris Brockett, Bill Dolan, arXiv:2407.034602024arXiv preprint</p>
<p>Self-evaluation improves selective generation in large language models. Jie Ren, Yao Zhao, Tu Vu, Peter J Liu, Balaji Lakshminarayanan, Proceedings on. onPMLR2023</p>
<p>Soumadeep Saha, Sutanoya Chakraborty, Saptarshi Saha, Utpal Garain, arXiv:2406.09043Language models are crossword solvers. 2024arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>8B Instruct 0. </p>
<p>14.7 24.8 29.170B Instruct. </p>
<p>. Qwen-2, </p>
<p>7B Instruct 18. </p>
<p>. Qwen-2, </p>
<p>35.9 44.1 47.3 17.7 22.1 24.9 0.3 0.3 0.314B Instruct. </p>
<p>. Qwen-2, </p>
<p>. Qwen-2, </p>
<p>. Gpt-4o Mini, </p>
<p>3-Turns Accuracy (%) of Text Sudoku. Table. 15</p>            </div>
        </div>

    </div>
</body>
</html>
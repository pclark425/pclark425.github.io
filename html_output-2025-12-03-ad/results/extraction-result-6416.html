<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6416 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6416</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6416</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-266174219</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.07413v1.pdf" target="_blank">AI capabilities can be significantly improved without expensive retraining</a></p>
                <p><strong>Paper Abstract:</strong> State-of-the-art AI systems can be significantly improved without expensive retraining via"post-training enhancements"-techniques applied after initial training like fine-tuning the system to use a web browser. We review recent post-training enhancements, categorizing them into five types: tool-use, prompting methods, scaffolding, solution selection, and data generation. Different enhancements improve performance on different tasks, making it hard to compare their significance. So we translate improvements from different enhancements into a common currency, the compute-equivalent gain: how much additional training compute would be needed to improve performance by the same amount as the enhancement. Our non-experimental work shows that post-training enhancements have significant benefits: most surveyed enhancements improve benchmark performance by more than a 5x increase in training compute, some by more than 20x. Post-training enhancements are relatively cheap to develop: fine-tuning costs are typically<1% of the original training cost. Governing the development of capable post-training enhancements may be challenging because frontier models could be enhanced by a wide range of actors.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6416.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6416.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that appends few-shot examples which include step-by-step reasoning chains, causing LLMs to produce intermediate reasoning steps and improving multi-step mathematical and symbolic problem solving, especially in larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (examples: PaLM-8B, PaLM-62B, PaLM-540B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer, decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B, 62B, 540B (examples reported in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large web-scale corpora (PaLM family reported as trained on ~780B tokens); no special math-only pretraining required for CoT itself.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MAWPS, Letter Concat (symbolic), Sports (commonsense); other math/symbolic benchmarks referenced</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic reasoning and symbolic reasoning (word problems, arithmetic puzzles, symbolic composition)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems and symbolic expressions requiring multistep solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school multi-step problems (GSM8K), harder symbolic tasks (Letter Concat); varies by benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot Chain-of-Thought prompting (include solved examples with intermediate steps in the context)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percent correct) on benchmark datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported: CoT improves PaLM-62B on GSM8K and MAWPS more than scaling to PaLM-540B (paper reports an approximate CEG ≈ 9 for these math benchmarks); on some symbolic tasks (Letter Concat) CoT yields much larger gains (CEG > 67 in one comparison). Inference cost can increase up to ~10× due to longer outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic interpretability provided in this survey; described as leveraging the model's ability to use context examples of reasoning to produce intermediate steps. Paper notes that CoT effectiveness typically grows with model scale and that subsequent work has refined prompting variants (zero-shot CoT, planning+CoT, self-ask, fine-tuning on reasoning chains).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Smaller models benefit little or not at all; if models are undertrained relative to scale comparisons, CEG estimates can be misleading; CoT can produce incorrect internal steps that nonetheless lead to wrong final answers (no guarantee of correctness); increased inference cost and risk of verbose but incorrect reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Benefits generally increase with model size — larger models show bigger CoT gains; some datasets show stronger scale dependence than others.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI capabilities can be significantly improved without expensive retraining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6416.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6416.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer (calculator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: language models that teach themselves to use tools (calculator etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning approach that teaches an LLM to detect when to call external tools (e.g., a calculator) and to generate API-style tool calls, improving performance on tasks that require precise arithmetic or external computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Toolformer (based on GPT-J family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer, decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B (Toolformer model reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Starts from a pretrained GPT-J-like model (pretraining on large web corpora / The Pile); fine-tuned with automatically generated supervision to insert tool calls (calculator, search, translator, Q&A) into contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Various benchmark suites for factual knowledge, math, and temporal questions (tables reported in Schick et al.); math-specific benchmarks referenced in the Toolformer paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic tasks requiring accurate computation (and other tool-requiring tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language questions where tool use (calculator) can be invoked for numeric computation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varies by downstream task; enough that Toolformer outperforms much larger non-tool models on some math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuning to insert explicit tool-call tokens (automatic generation of examples showing when to call the tool); uses the tool at inference time to get exact numeric results</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Benchmark scores / accuracy relative to baseline LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Toolformer (6.7B) outperformed GPT-3 (175B) on certain factual and math benchmarks; paper estimates a compute-equivalent gain > 20× for math/factual benchmarks versus non-tool GPT-3 baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic probe of numeric representation reported; improvement attributed to offloading exact arithmetic to a deterministic external calculator and to the model learning when to invoke it via fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When tool interface or retrieval fails, arithmetic correctness is lost; the method didn't produce notable gains for some tasks (e.g., translation despite access to translator); small models may struggle to learn reliable tool-invocation without sufficient fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Gains vary by task; tool-use can allow small models to outperform much larger models that lack tool access — therefore gains are task-dependent and can substitute for scaling in numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI capabilities can be significantly improved without expensive retraining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6416.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6416.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minerva (data cleaning + majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minerva: solving quantitative reasoning problems with language models (data cleaning + majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning and inference-aggregation pipeline that preserves mathematical notation in fine-tuning data and uses majority-vote/self-consistency over multiple sampled solutions to substantially improve performance on math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving quantitative reasoning problems with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Minerva (PaLM fine-tuned variants: Minerva-8B, Minerva-62B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer, decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B and 62B Minerva variants compared to PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on a cleaned 118GB dataset of scientific/math papers where formatting and math symbols are preserved (improved data cleaning for math notation).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, OCWCourses, GSM8K, MMLU-STEM (math and STEM benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical problem solving and quantitative reasoning (multi-step math problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math problems with math notation; preserved formatting (LaTeX-style) in fine-tuning corpus</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varies; MATH and OCWCourses are among the hardest benchmarks evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuning on cleaned math corpora + generation of multiple solutions at inference followed by majority voting/self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / benchmark score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Minerva-8B outperformed PaLM-540B on the hardest benchmarks (MATH, OCWCourses); authors report that fine-tuning alone yields CEG > 67 (comparing 8B vs 540B) and the paper's log-linear scaling estimates produced much larger CEGs (e.g., Table 3 reports CEGs up to ~1700 for MATH and extremely large values when majority voting added: the combined fine-tuning+majority-vote row lists CEGs that the authors caution may be implausibly high).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>The paper emphasizes that preserving mathematical symbols and formatting in training data materially improves the model's ability to express and manipulate math; no mechanistic probing of numeric token representations is provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Gains vary by benchmark — some math/STEM benchmarks see much larger improvement than others; majority-voting can still produce wrong consensus answers if the model's sampling distribution is biased; log-linear CEG estimates may overstate true generalizable gains.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning on domain-specific cleaned data can outperform large-scale parameter increases; combining majority voting with fine-tuning produces much larger gains than either alone (but with caveats and potential diminishing returns).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI capabilities can be significantly improved without expensive retraining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6416.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6416.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verification (verifier models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verifier-based solution selection for math problems (outcome and process supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate many candidate solutions with an LLM, then use a separately fine-tuned verifier to score candidates and select the highest-scoring answer; process-supervision (step-level labels) for the verifier further improves selection quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Base generator: GPT-family code/QA-tuned models (examples: 6B, 175B); Verifier: fine-tuned model (size varied across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer, decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Generator examples reported at 6B and 175B; verifiers trained smaller (paper-specific sizes varied)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Generator produces many candidate solutions; verifier fine-tuned on large sets of generated solutions with either outcome supervision (final answer) or process supervision (human step-level correctness labels).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Mathematical word-problem datasets used in the verifier papers (e.g., GSM8K-like math word problem sets and internal evaluation sets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math word problems—multi-step arithmetic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems; many candidate solutions are sampled and scored</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Moderate-to-hard math word problems (7500 problems used for training/verifier experiments in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Solution sampling (thousands of candidates), verifier scoring; process supervision trains verifier to judge step-level correctness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / solve rate (percentage of problems solved by at least one submitted candidate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Cobbe et al. report large improvements from verification; fine-tuning the verifier used ~0.05% of pretraining compute. Lightman et al. (process supervision) report that process supervision improves verifier performance by ~8 percentage points relative to outcome supervision (estimated CEG ~8). Overall, verification + sampling yields substantial gains versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper contrasts outcome supervision (verifier rewarded if final answer matches) versus process supervision (verifier trained on human step-level correctness), showing that process supervision gives more fine-grained signal and reduces verifier reward for lucky-but-flawed solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Outcome-supervised verifiers can be misled when flawed reasoning happens to yield the correct final answer ('lucky' solutions); verifiers require labeled step-level data for best effect (expensive); generating and scoring many candidates increases inference cost substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Verification benefits combine strongly with increased sampling; fine-grained supervision (process) yields additional gains and can be more effective than simply scaling model size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI capabilities can be significantly improved without expensive retraining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6416.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6416.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts: deliberate problem solving with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Scaffolding method that frames reasoning as search in a tree of intermediate 'thought' nodes, generating and heuristically evaluating multiple candidate thoughts per step and navigating the tree with classic search algorithms to solve problems requiring exploration (including arithmetic puzzles).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applications used standard LLMs (e.g., GPT-family or PaLM-family models used as the thought proposer/evaluator in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer, decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not always fixed in reported experiments; improvements reported across tested LLMs (paper uses models of various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Uses pretrained LLMs as components; no extra pretraining but requires many inference-time calls</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game of 24 (arithmetic puzzle), Mini Crosswords, Creative Writing (used as tasks where ToT outperforms baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic puzzle solving (Game of 24) and general problems that require search/planning/exploration</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Symbolic/natural-language tasks: Game of 24 requires composing arithmetic operations to reach a target value from given numbers</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Puzzles requiring search and branching (Game of 24 examples); harder than simple step-by-step problems for which CoT suffices</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Generate multiple candidate thoughts per step, evaluate each candidate with heuristics or the model, and use search (BFS/DFS/MCTS-like) to expand promising branches</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Solve rate / success rate (task-specific accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ToT significantly improved over chain-of-thought and majority-vote baselines on Game of 24, Mini Crosswords, and Creative Writing in the reported experiments. ToT often required visiting ~70 nodes and multiple samples per node; inference compute increased by orders of magnitude (authors estimate >100× relative to a single CoT generation in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Methodology explicitly externalizes search and evaluation strategies; analysis focuses on search behavior (node expansion, evaluation) rather than internal neuron-level mechanisms for numeric processing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Very high inference-time compute; not cost-effective for tasks solvable by single-pass CoT; scaling to large/continuous problems may be impractical; success depends on good heuristic/evaluator quality.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Particularly helpful on problems with many possible reasoning paths where single-pass generation fails; effectiveness depends on evaluator accuracy (which tends to improve with larger LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI capabilities can be significantly improved without expensive retraining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6416.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6416.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad / Intermediate Computation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpad prompting for intermediate computation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique where the model is asked to write intermediate results (a 'scratchpad') before producing the final answer; shown to aid arithmetic and code-simulation tasks by making intermediate arithmetic explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show your work: Scratchpads for intermediate computation with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various autoregressive LMs used in the referenced work (GPT-family style models in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer, decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Experiments used multiple sizes (paper demonstrates benefits for models that have sufficient capacity)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Uses pretrained LMs; scratchpad is a prompting technique rather than additional pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Arithmetic problems and code-simulation tasks referenced in the original scratchpad paper (used as canonical examples demonstrating improved multi-step computation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic (multi-step) and program simulation (stepwise computation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language problems augmented with explicit instruction to produce intermediate computation steps prior to final answer</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Multi-step arithmetic and computation-heavy tasks where intermediate steps matter</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Scratchpad prompting (explicitly request intermediate computations that the model writes out before finalizing answer)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / success rate on arithmetic and simulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported improvements on arithmetic problems and on tasks requiring stepwise computation/simulation; specific magnitudes depend on model size and task (no single number reported in this survey), but the technique helped arithmetic reasoning and code-execution simulation relative to direct answer prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Technique suggests that exposing intermediate computations in the output helps models coordinate multi-step transformations; no neuron-level analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Benefits correlate with model capacity; smaller models still struggle even with scratchpad prompting; produced intermediate steps can still be incorrect leading to wrong final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>More effective with larger models that can use context examples to condition on stepwise procedures; related to CoT but presented as an explicit intermediate-computation buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI capabilities can be significantly improved without expensive retraining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6416.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6416.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority voting / Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority Voting / Self-Consistency over sampled solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Solution choice technique that generates many independent solutions (via sampling) and selects the most common answer (majority vote) or uses self-consistency across sampled chains-of-thought to choose the final answer, reducing sampling variance and often improving arithmetic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving quantitative reasoning problems with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM/Minerva variants (and generic LLMs when applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer, decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported for Minerva/PaLM variants (8B, 62B, 540B comparisons), but method is model-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>No additional pretraining required; used at inference time by sampling multiple outputs from a fine-tuned or prompted LLM</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, OCWCourses (math benchmarks where majority voting/self-consistency used)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-sample answer aggregation for arithmetic and quantitative reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math problems; multiple independent generation samples per problem are produced and aggregated</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Moderate to hard math problems (benefits most on harder problems with high sampling variance)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Generate many sampled solutions (e.g., tens to thousands), then apply majority vote (or cluster and pick diverse representatives) or self-consistency over sampled chains-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percent correct) after aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Minerva authors report that combining fine-tuning with majority voting gives much larger gains than fine-tuning alone; Table 3 in the survey reports very large computed CEGs when majority voting is added (e.g., combined fine-tuning+majority-vote CEGs for some math benchmarks reach very large numbers such as ~1,200,000 by their log-linear method), though the authors caution that such log-linear extrapolations may be implausible.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Aggregation reduces sampling noise and cancels some stochastic errors; clustering of candidate solutions (as in AlphaCode/Lightman et al.) can improve diversity of submitted answers and selection quality.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If the model's sampling distribution is biased (systematic error), majority will choose the wrong answer; requires large numbers of samples which increases inference cost; not effective if model rarely generates correct solutions even under sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Aggregation gains can substitute for scaling, enabling smaller tuned models to match larger models with fewer samples; benefits interact with model size and fine-tuning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI capabilities can be significantly improved without expensive retraining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models. <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 2)</em></li>
                <li>Let's verify step by step. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 1)</em></li>
                <li>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6416",
    "paper_id": "paper-266174219",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique that appends few-shot examples which include step-by-step reasoning chains, causing LLMs to produce intermediate reasoning steps and improving multi-step mathematical and symbolic problem solving, especially in larger models.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "PaLM (examples: PaLM-8B, PaLM-62B, PaLM-540B)",
            "model_family": "Transformer, decoder-only",
            "model_size": "8B, 62B, 540B (examples reported in the paper)",
            "training_data_description": "Pretrained on large web-scale corpora (PaLM family reported as trained on ~780B tokens); no special math-only pretraining required for CoT itself.",
            "benchmark_name": "GSM8K, MAWPS, Letter Concat (symbolic), Sports (commonsense); other math/symbolic benchmarks referenced",
            "task_type": "Multi-step arithmetic reasoning and symbolic reasoning (word problems, arithmetic puzzles, symbolic composition)",
            "problem_format": "Natural-language word problems and symbolic expressions requiring multistep solution",
            "difficulty_level": "Grade-school multi-step problems (GSM8K), harder symbolic tasks (Letter Concat); varies by benchmark",
            "prompting_method": "Few-shot Chain-of-Thought prompting (include solved examples with intermediate steps in the context)",
            "performance_metric": "Accuracy (percent correct) on benchmark datasets",
            "performance_value": "Reported: CoT improves PaLM-62B on GSM8K and MAWPS more than scaling to PaLM-540B (paper reports an approximate CEG ≈ 9 for these math benchmarks); on some symbolic tasks (Letter Concat) CoT yields much larger gains (CEG &gt; 67 in one comparison). Inference cost can increase up to ~10× due to longer outputs.",
            "internal_analysis": "No mechanistic interpretability provided in this survey; described as leveraging the model's ability to use context examples of reasoning to produce intermediate steps. Paper notes that CoT effectiveness typically grows with model scale and that subsequent work has refined prompting variants (zero-shot CoT, planning+CoT, self-ask, fine-tuning on reasoning chains).",
            "failure_modes": "Smaller models benefit little or not at all; if models are undertrained relative to scale comparisons, CEG estimates can be misleading; CoT can produce incorrect internal steps that nonetheless lead to wrong final answers (no guarantee of correctness); increased inference cost and risk of verbose but incorrect reasoning.",
            "scaling_trend": "Benefits generally increase with model size — larger models show bigger CoT gains; some datasets show stronger scale dependence than others.",
            "uuid": "e6416.0",
            "source_info": {
                "paper_title": "AI capabilities can be significantly improved without expensive retraining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Toolformer (calculator)",
            "name_full": "Toolformer: language models that teach themselves to use tools (calculator etc.)",
            "brief_description": "Fine-tuning approach that teaches an LLM to detect when to call external tools (e.g., a calculator) and to generate API-style tool calls, improving performance on tasks that require precise arithmetic or external computation.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools.",
            "mention_or_use": "use",
            "model_name": "Toolformer (based on GPT-J family)",
            "model_family": "Transformer, decoder-only",
            "model_size": "6.7B (Toolformer model reported)",
            "training_data_description": "Starts from a pretrained GPT-J-like model (pretraining on large web corpora / The Pile); fine-tuned with automatically generated supervision to insert tool calls (calculator, search, translator, Q&A) into contexts.",
            "benchmark_name": "Various benchmark suites for factual knowledge, math, and temporal questions (tables reported in Schick et al.); math-specific benchmarks referenced in the Toolformer paper",
            "task_type": "Arithmetic tasks requiring accurate computation (and other tool-requiring tasks)",
            "problem_format": "Natural-language questions where tool use (calculator) can be invoked for numeric computation",
            "difficulty_level": "Varies by downstream task; enough that Toolformer outperforms much larger non-tool models on some math benchmarks",
            "prompting_method": "Fine-tuning to insert explicit tool-call tokens (automatic generation of examples showing when to call the tool); uses the tool at inference time to get exact numeric results",
            "performance_metric": "Benchmark scores / accuracy relative to baseline LLMs",
            "performance_value": "Toolformer (6.7B) outperformed GPT-3 (175B) on certain factual and math benchmarks; paper estimates a compute-equivalent gain &gt; 20× for math/factual benchmarks versus non-tool GPT-3 baselines.",
            "internal_analysis": "No internal mechanistic probe of numeric representation reported; improvement attributed to offloading exact arithmetic to a deterministic external calculator and to the model learning when to invoke it via fine-tuning.",
            "failure_modes": "When tool interface or retrieval fails, arithmetic correctness is lost; the method didn't produce notable gains for some tasks (e.g., translation despite access to translator); small models may struggle to learn reliable tool-invocation without sufficient fine-tuning data.",
            "scaling_trend": "Gains vary by task; tool-use can allow small models to outperform much larger models that lack tool access — therefore gains are task-dependent and can substitute for scaling in numeric tasks.",
            "uuid": "e6416.1",
            "source_info": {
                "paper_title": "AI capabilities can be significantly improved without expensive retraining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Minerva (data cleaning + majority voting)",
            "name_full": "Minerva: solving quantitative reasoning problems with language models (data cleaning + majority voting)",
            "brief_description": "A fine-tuning and inference-aggregation pipeline that preserves mathematical notation in fine-tuning data and uses majority-vote/self-consistency over multiple sampled solutions to substantially improve performance on math benchmarks.",
            "citation_title": "Solving quantitative reasoning problems with language models.",
            "mention_or_use": "use",
            "model_name": "Minerva (PaLM fine-tuned variants: Minerva-8B, Minerva-62B)",
            "model_family": "Transformer, decoder-only",
            "model_size": "8B and 62B Minerva variants compared to PaLM-540B",
            "training_data_description": "Fine-tuned on a cleaned 118GB dataset of scientific/math papers where formatting and math symbols are preserved (improved data cleaning for math notation).",
            "benchmark_name": "MATH, OCWCourses, GSM8K, MMLU-STEM (math and STEM benchmarks)",
            "task_type": "Mathematical problem solving and quantitative reasoning (multi-step math problems)",
            "problem_format": "Natural-language math problems with math notation; preserved formatting (LaTeX-style) in fine-tuning corpus",
            "difficulty_level": "Varies; MATH and OCWCourses are among the hardest benchmarks evaluated",
            "prompting_method": "Fine-tuning on cleaned math corpora + generation of multiple solutions at inference followed by majority voting/self-consistency",
            "performance_metric": "Accuracy / benchmark score",
            "performance_value": "Minerva-8B outperformed PaLM-540B on the hardest benchmarks (MATH, OCWCourses); authors report that fine-tuning alone yields CEG &gt; 67 (comparing 8B vs 540B) and the paper's log-linear scaling estimates produced much larger CEGs (e.g., Table 3 reports CEGs up to ~1700 for MATH and extremely large values when majority voting added: the combined fine-tuning+majority-vote row lists CEGs that the authors caution may be implausibly high).",
            "internal_analysis": "The paper emphasizes that preserving mathematical symbols and formatting in training data materially improves the model's ability to express and manipulate math; no mechanistic probing of numeric token representations is provided in this survey.",
            "failure_modes": "Gains vary by benchmark — some math/STEM benchmarks see much larger improvement than others; majority-voting can still produce wrong consensus answers if the model's sampling distribution is biased; log-linear CEG estimates may overstate true generalizable gains.",
            "scaling_trend": "Fine-tuning on domain-specific cleaned data can outperform large-scale parameter increases; combining majority voting with fine-tuning produces much larger gains than either alone (but with caveats and potential diminishing returns).",
            "uuid": "e6416.2",
            "source_info": {
                "paper_title": "AI capabilities can be significantly improved without expensive retraining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Verification (verifier models)",
            "name_full": "Verifier-based solution selection for math problems (outcome and process supervision)",
            "brief_description": "Generate many candidate solutions with an LLM, then use a separately fine-tuned verifier to score candidates and select the highest-scoring answer; process-supervision (step-level labels) for the verifier further improves selection quality.",
            "citation_title": "Training verifiers to solve math word problems.",
            "mention_or_use": "use",
            "model_name": "Base generator: GPT-family code/QA-tuned models (examples: 6B, 175B); Verifier: fine-tuned model (size varied across experiments)",
            "model_family": "Transformer, decoder-only",
            "model_size": "Generator examples reported at 6B and 175B; verifiers trained smaller (paper-specific sizes varied)",
            "training_data_description": "Generator produces many candidate solutions; verifier fine-tuned on large sets of generated solutions with either outcome supervision (final answer) or process supervision (human step-level correctness labels).",
            "benchmark_name": "Mathematical word-problem datasets used in the verifier papers (e.g., GSM8K-like math word problem sets and internal evaluation sets)",
            "task_type": "Math word problems—multi-step arithmetic reasoning",
            "problem_format": "Natural-language word problems; many candidate solutions are sampled and scored",
            "difficulty_level": "Moderate-to-hard math word problems (7500 problems used for training/verifier experiments in cited work)",
            "prompting_method": "Solution sampling (thousands of candidates), verifier scoring; process supervision trains verifier to judge step-level correctness",
            "performance_metric": "Accuracy / solve rate (percentage of problems solved by at least one submitted candidate)",
            "performance_value": "Cobbe et al. report large improvements from verification; fine-tuning the verifier used ~0.05% of pretraining compute. Lightman et al. (process supervision) report that process supervision improves verifier performance by ~8 percentage points relative to outcome supervision (estimated CEG ~8). Overall, verification + sampling yields substantial gains versus baselines.",
            "internal_analysis": "Paper contrasts outcome supervision (verifier rewarded if final answer matches) versus process supervision (verifier trained on human step-level correctness), showing that process supervision gives more fine-grained signal and reduces verifier reward for lucky-but-flawed solutions.",
            "failure_modes": "Outcome-supervised verifiers can be misled when flawed reasoning happens to yield the correct final answer ('lucky' solutions); verifiers require labeled step-level data for best effect (expensive); generating and scoring many candidates increases inference cost substantially.",
            "scaling_trend": "Verification benefits combine strongly with increased sampling; fine-grained supervision (process) yields additional gains and can be more effective than simply scaling model size.",
            "uuid": "e6416.3",
            "source_info": {
                "paper_title": "AI capabilities can be significantly improved without expensive retraining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Tree of Thoughts (ToT)",
            "name_full": "Tree of Thoughts: deliberate problem solving with language models",
            "brief_description": "Scaffolding method that frames reasoning as search in a tree of intermediate 'thought' nodes, generating and heuristically evaluating multiple candidate thoughts per step and navigating the tree with classic search algorithms to solve problems requiring exploration (including arithmetic puzzles).",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "use",
            "model_name": "Applications used standard LLMs (e.g., GPT-family or PaLM-family models used as the thought proposer/evaluator in experiments)",
            "model_family": "Transformer, decoder-only",
            "model_size": "Not always fixed in reported experiments; improvements reported across tested LLMs (paper uses models of various sizes)",
            "training_data_description": "Uses pretrained LLMs as components; no extra pretraining but requires many inference-time calls",
            "benchmark_name": "Game of 24 (arithmetic puzzle), Mini Crosswords, Creative Writing (used as tasks where ToT outperforms baselines)",
            "task_type": "Arithmetic puzzle solving (Game of 24) and general problems that require search/planning/exploration",
            "problem_format": "Symbolic/natural-language tasks: Game of 24 requires composing arithmetic operations to reach a target value from given numbers",
            "difficulty_level": "Puzzles requiring search and branching (Game of 24 examples); harder than simple step-by-step problems for which CoT suffices",
            "prompting_method": "Generate multiple candidate thoughts per step, evaluate each candidate with heuristics or the model, and use search (BFS/DFS/MCTS-like) to expand promising branches",
            "performance_metric": "Solve rate / success rate (task-specific accuracy)",
            "performance_value": "ToT significantly improved over chain-of-thought and majority-vote baselines on Game of 24, Mini Crosswords, and Creative Writing in the reported experiments. ToT often required visiting ~70 nodes and multiple samples per node; inference compute increased by orders of magnitude (authors estimate &gt;100× relative to a single CoT generation in some settings).",
            "internal_analysis": "Methodology explicitly externalizes search and evaluation strategies; analysis focuses on search behavior (node expansion, evaluation) rather than internal neuron-level mechanisms for numeric processing.",
            "failure_modes": "Very high inference-time compute; not cost-effective for tasks solvable by single-pass CoT; scaling to large/continuous problems may be impractical; success depends on good heuristic/evaluator quality.",
            "scaling_trend": "Particularly helpful on problems with many possible reasoning paths where single-pass generation fails; effectiveness depends on evaluator accuracy (which tends to improve with larger LMs).",
            "uuid": "e6416.4",
            "source_info": {
                "paper_title": "AI capabilities can be significantly improved without expensive retraining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Scratchpad / Intermediate Computation",
            "name_full": "Scratchpad prompting for intermediate computation",
            "brief_description": "Prompting technique where the model is asked to write intermediate results (a 'scratchpad') before producing the final answer; shown to aid arithmetic and code-simulation tasks by making intermediate arithmetic explicit.",
            "citation_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "mention_or_use": "use",
            "model_name": "Various autoregressive LMs used in the referenced work (GPT-family style models in experiments)",
            "model_family": "Transformer, decoder-only",
            "model_size": "Experiments used multiple sizes (paper demonstrates benefits for models that have sufficient capacity)",
            "training_data_description": "Uses pretrained LMs; scratchpad is a prompting technique rather than additional pretraining",
            "benchmark_name": "Arithmetic problems and code-simulation tasks referenced in the original scratchpad paper (used as canonical examples demonstrating improved multi-step computation)",
            "task_type": "Arithmetic (multi-step) and program simulation (stepwise computation)",
            "problem_format": "Natural-language problems augmented with explicit instruction to produce intermediate computation steps prior to final answer",
            "difficulty_level": "Multi-step arithmetic and computation-heavy tasks where intermediate steps matter",
            "prompting_method": "Scratchpad prompting (explicitly request intermediate computations that the model writes out before finalizing answer)",
            "performance_metric": "Accuracy / success rate on arithmetic and simulation tasks",
            "performance_value": "Reported improvements on arithmetic problems and on tasks requiring stepwise computation/simulation; specific magnitudes depend on model size and task (no single number reported in this survey), but the technique helped arithmetic reasoning and code-execution simulation relative to direct answer prompting.",
            "internal_analysis": "Technique suggests that exposing intermediate computations in the output helps models coordinate multi-step transformations; no neuron-level analysis provided here.",
            "failure_modes": "Benefits correlate with model capacity; smaller models still struggle even with scratchpad prompting; produced intermediate steps can still be incorrect leading to wrong final answers.",
            "scaling_trend": "More effective with larger models that can use context examples to condition on stepwise procedures; related to CoT but presented as an explicit intermediate-computation buffer.",
            "uuid": "e6416.5",
            "source_info": {
                "paper_title": "AI capabilities can be significantly improved without expensive retraining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Majority voting / Self-consistency",
            "name_full": "Majority Voting / Self-Consistency over sampled solutions",
            "brief_description": "Solution choice technique that generates many independent solutions (via sampling) and selects the most common answer (majority vote) or uses self-consistency across sampled chains-of-thought to choose the final answer, reducing sampling variance and often improving arithmetic accuracy.",
            "citation_title": "Solving quantitative reasoning problems with language models.",
            "mention_or_use": "use",
            "model_name": "PaLM/Minerva variants (and generic LLMs when applied)",
            "model_family": "Transformer, decoder-only",
            "model_size": "Reported for Minerva/PaLM variants (8B, 62B, 540B comparisons), but method is model-agnostic",
            "training_data_description": "No additional pretraining required; used at inference time by sampling multiple outputs from a fine-tuned or prompted LLM",
            "benchmark_name": "MATH, GSM8K, OCWCourses (math benchmarks where majority voting/self-consistency used)",
            "task_type": "Multi-sample answer aggregation for arithmetic and quantitative reasoning",
            "problem_format": "Natural-language math problems; multiple independent generation samples per problem are produced and aggregated",
            "difficulty_level": "Moderate to hard math problems (benefits most on harder problems with high sampling variance)",
            "prompting_method": "Generate many sampled solutions (e.g., tens to thousands), then apply majority vote (or cluster and pick diverse representatives) or self-consistency over sampled chains-of-thought",
            "performance_metric": "Accuracy (percent correct) after aggregation",
            "performance_value": "Minerva authors report that combining fine-tuning with majority voting gives much larger gains than fine-tuning alone; Table 3 in the survey reports very large computed CEGs when majority voting is added (e.g., combined fine-tuning+majority-vote CEGs for some math benchmarks reach very large numbers such as ~1,200,000 by their log-linear method), though the authors caution that such log-linear extrapolations may be implausible.",
            "internal_analysis": "Aggregation reduces sampling noise and cancels some stochastic errors; clustering of candidate solutions (as in AlphaCode/Lightman et al.) can improve diversity of submitted answers and selection quality.",
            "failure_modes": "If the model's sampling distribution is biased (systematic error), majority will choose the wrong answer; requires large numbers of samples which increases inference cost; not effective if model rarely generates correct solutions even under sampling.",
            "scaling_trend": "Aggregation gains can substitute for scaling, enabling smaller tuned models to match larger models with fewer samples; benefits interact with model size and fine-tuning quality.",
            "uuid": "e6416.6",
            "source_info": {
                "paper_title": "AI capabilities can be significantly improved without expensive retraining",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models.",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Let's verify step by step.",
            "rating": 2,
            "sanitized_title": "lets_verify_step_by_step"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 1,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.",
            "rating": 1,
            "sanitized_title": "planandsolve_prompting_improving_zeroshot_chainofthought_reasoning_by_large_language_models"
        }
    ],
    "cost": 0.02240975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI capabilities can be significantly improved without expensive retraining
12 Dec 2023</p>
<p>Tom Davidson 
Jean-Stanislas Denain 
Pablo Villalobos 
Guillem Bas 
AI capabilities can be significantly improved without expensive retraining
12 Dec 202391753F29D7A9FE88E0E72DFD32BB8DA3arXiv:2312.07413v1[cs.AI]
State-of-the-art AI systems can be significantly improved without expensive retraining via "posttraining enhancements"-techniques applied after initial training like fine-tuning the system to use a web browser.We review recent post-training enhancements, categorizing them into five types: tool-use, prompting methods, scaffolding, solution selection, and data generation.Different enhancements improve performance on different tasks, making it hard to compare their significance.So we translate improvements from different enhancements into a common currency, the compute-equivalent gain: how much additional training compute would be needed to improve performance by the same amount as the enhancement.Our non-experimental work shows that post-training enhancements have significant benefits: most surveyed enhancements improve benchmark performance by more than a 5x increase in training compute, some by more than 20x.Posttraining enhancements are relatively cheap to develop: fine-tuning costs are typically &lt;1% of the original training cost.Governing the development of capable post-training enhancements may be challenging because frontier models could be enhanced by a wide range of actors.</p>
<p>Executive summary</p>
<p>It is important to understand the drivers of AI progress, to inform both forecasts of future progress and initiatives for AI governance.Previous analyses have mostly focused on the initial development of an AI system, called pre-training: computationally expensive training runs where models learn from massive amounts of data.Researchers have measured how quickly the inputs to training runs have been increasing -more computational resources ("compute") (Sevilla et al.,  2022), more data (Villalobos et al., 2022), better algorithms 1 Open Philantropy 2 UC Berkeley 3 Epoch 4 Observatorio de Riesgos Catastróficos Globales.Correspondence to: Tom Davidson <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#116;&#111;&#109;&#64;&#111;&#112;&#101;&#110;&#112;&#104;&#105;&#108;&#97;&#110;&#116;&#114;&#111;&#112;&#121;&#46;&#111;&#114;&#103;">&#116;&#111;&#109;&#64;&#111;&#112;&#101;&#110;&#112;&#104;&#105;&#108;&#97;&#110;&#116;&#114;&#111;&#112;&#121;&#46;&#111;&#114;&#103;</a>.(Hernandez and Brown, 2020; Erdil and Besiroglu, 2023) and how bigger training runs translate into improved performance on downstream tasks (Srivastava et al., 2023).</p>
<p>There has been little focus in the AI forecasting literature on what we call "post-training enhancements": techniques for improving the performance of an AI model after it is initially developed. 1Post-training enhancements are commonly used in deployed AI systems.Examples include: teaching the model to browse the web, asking the model to "think stepby-step", fine-tuning 2 on task-specific curated data sets, and using the model to power an autonomous AI agent (e.g.AutoGPT). 3 is hard to meaningfully compare the benefits of posttraining enhancements that apply to different domains.For example, how does 10% greater accuracy on the MATH benchmark (Hendrycks et al., 2021) compare to 10% greater accuracy in a multiple choice knowledge test, or to 10% lower perplexity in a language modeling task?To address this problem, we translate performance gains on different benchmarks into a common currency.In particular, we estimate how much additional training compute would have been needed to improve benchmark performance by as much as the post-training enhancement. 4We call this the "compute-equivalent gain" (CEG).In the toy example below, the post-training enhancement improves performance by the same amount as increasing the training compute by 5×; so the CEG is 5 (see Figure 1). 1 Many papers introduce and evaluate individual enhancements, but there is little work systematically evaluating multiple enhancements.(Villalobos and Atkinson, 2023) discuss techniques for improving performance at the cost of more expensive inference, which count as post-training enhancements under our definition.We focus on a wider set of enhancements, most of which do not make inference much more expensive.(Anderljung et al., 2023)  briefly discusses various "post-deployment enhancements", which are a subset of post-training enhancements.</p>
<p>2 Fine-tuning refers to additional training that occurs after pretraining, often to elicit specific skills and tendencies from the model.Fine-tuning typically uses a much smaller quantity of data than pre-training.</p>
<p>3 See Mialon et al. (2023) for a survey of some of these enhancements, which they call augmentations. 4 Assuming that the additional training compute is used to train further on the pre-training distribution, rather than on some taskspecific dataset.A CEG of 5 for a particular benchmark doesn't necessarily imply that the post-training enhancement is as useful overall as increasing the training compute by 5×.Post-training enhancements are often, though not always, domain-specific (e.g.learning to use a calculator), whereas the performance gains from additional training compute tend to be more general. 5As we illustrate in this paper, the CEG is a useful way to get a rough sense of the significance of an enhancement.</p>
<p>We provide an overview of recent post-training enhancement techniques.We are not exhaustive, but rather present a broadly representative set of examples.We divide posttraining enhancements into five categories:6 1. Tool enhancements: teaching an AI system to use new tools, like a web browser.</p>
<ol>
<li>
<p>Prompting enhancements: changing the text-based input to the model to steer its behavior and reasoning, e.g.including an example response to a similar question.</p>
</li>
<li>
<p>Scaffolding enhancements: programs that structure the model's reasoning and the flow of information between different copies of the model (e.g.producing AI agents).</p>
</li>
<li>
<p>Solution choice enhancements: techniques for generating and then choosing between multiple candidate solutions to a problem.</p>
</li>
<li>
<p>Data enhancements: techniques for generating more, higher-quality data for fine-tuning.</p>
</li>
</ol>
<p>We quantify the benefits of post-training enhancements by calculating the CEG.We also estimate each enhancement's costs: the one-time cost of compute for fine-tuning the model to be able to make use of the enhancement, and any increased inference cost associated with the enhancement (Figure 2).</p>
<p>Most of the post-training enhancements we studied have a CEG above 5, and many are above 20.This corresponds to significant performance improvements in particular domains; improvements that would have been very expensive to achieve via spending more on pre-training.We find that the fine-tuning costs are low: typically &lt; 1% and sometimes &lt; 0.01% of the pre-training cost.The inference costs vary: often there is no additional inference cost and other times inference becomes 100× more expensive.Post-training enhancements with a higher CEG tend to have higher costs in either fine-tuning or inference, which is unsurprising given the performance benefits of additional training and inference compute (Villalobos and Atkinson, 2023).There are development costs we do not estimate, like human labor.</p>
<p>We did not conduct our own experiments to measure the CEG, but used results from other research papers.This made it challenging to get exact estimates, and there were sometimes confounding factors (see section 4).For this reason, we don't have high confidence in each individual CEG estimate, but we think that in aggregate they are informative about the typical benefit produced by an enhancement.</p>
<p>We predict that cheap post-training enhancements will continue improving model capabilities.Researchers have combined together multiple different post-training enhancements for increased benefit, for example by using both majority vote and a new technique for data cleaning (Lewkowycz et al., 2022).Many post-training enhancements improved upon previous versions; for example, the LATS agent outperforms previous agents and "chain of thought" prompting has since been improved upon in multiple papers (Wang et al., 2023a; Press et al., 2022; Huang  et al., 2022).Often post-training enhancements only become effective when models are sufficiently large (e.g.(Wei et al.,  2022; Schick et al., 2023)).These patterns suggest that researchers will continue to improve frontier AI capabilities Post-training enhancements are already enabling new and beneficial applications of AI, allowing systems to be specialized to particular use cases.7However, post-training enhancements could enable some dangerous applications.Firstly, they could make a model more generally capable at both benign and illicit tasks, e.g.creating a generalist agent that could be used to write phishing emails (Kinniment et al., 2023).Secondly, they could enhance capabilities in a narrow but potentially dangerous domain, e.g.connecting a model to robotic hardware for synthesizing chemicals (Boiko et al., 2023).</p>
<p>Post-training enhancements pose a distinctive governance challenge.Training a frontier AI model today is expensive -the cost of compute for training GPT-4 is estimated to be around $50 million (Epoch, 2023).This means that few actors can advance frontier capabilities via pre-training.By contrast, post-training enhancements are often cheap.The compute cost of fine-tuning is sometimes &lt; 0.01% the cost of pre-training and, though post-training enhancements can increase inference costs, the inference costs for a single user are still extremely low compared to the cost of pre-training.So a wider range of actors could expand the capabilities of frontier AI models via post-training enhancements, potentially in unanticipated ways.</p>
<p>When evaluating a model's safety for release, it may be necessary to consider not only its current capabilities but also those that could be unlocked by future post-training enhancements.Recognizing the potential for these enhancements to significantly raise a model's dangerous capabilities, it could be prudent to use a 'safety buffer', as discussed in Section 6, where protective measures are activated at capability levels lower than those presently identified as hazardous.</p>
<p>The paper is structured as follows:</p>
<p>• Section 2 lays out our conceptual framework for quantifying the benefits (via the CEG) and compute costs of post-training enhancements.</p>
<p>• Section 3 applies this framework to analyze fourteen examples of post-training enhancements from the literature, and discusses many others.</p>
<p>• Section 4 describes limitations of our CEG estimates.</p>
<p>• Section 5 argues that post-training enhancements will continue to improve capabilities in the future, that they can often be combined, and that they can be either narrow or general.</p>
<p>• Section 6 considers policy implications, especially relating to dangerous capabilities.</p>
<p>• Section 7 presents possible directions for future work.</p>
<p>• Section 8 concludes.</p>
<p>Our core contribution is analyzing the benefits (via the CEG) and the compute costs of a wide range of post-training enhancements.Table 1 summarizes our analysis.</p>
<p>Table 1: Summary of post-training enhancements (PTEs), their compute-equivalent gain (CEG) and their associated costs.</p>
<p>Tool enhancements</p>
<p>Toolformer</p>
<p>Fine-tune a model to use a calculator, a Q&amp;A system, a search engine, a translation system, and a calendar.</p>
<blockquote>
<p>20 in benchmarks for factual knowledge, math, and temporal questions ∼ 0.1% 1</p>
</blockquote>
<p>WebGPT</p>
<p>Fine-tune a model to use a web browser to answer factual questions and provide citations &gt; 13 on a question-answering benchmark.</p>
<p>Larger gains when combined with best-of-n ∼ 0.01% 1</p>
<p>Memory retrieval</p>
<p>The model retrieves text that is similar to the text it is predicting, and uses it to inform its predictions.</p>
<blockquote>
<p>43 on one next-word-prediction dataset; lower for others.
&lt; 3.3% &lt; 1.1
Prompting enhancements</p>
</blockquote>
<p>Scaffolding enhancements</p>
<p>Tree of thoughts</p>
<p>The model generates thought candidates at each step, evaluates their progress heuristically, and uses search algorithms to navigate the thought tree.</p>
<p>Not enough data to measure the CEG.</p>
<p>Better at deduction problems with multiple solution paths.</p>
<p>0 100</p>
<p>Parsel</p>
<p>The model decomposes a complex task into natural language function descriptions, generates modular implementations for each, and searches over combinations of these implementations by testing against constraints.</p>
<p>Not enough data to measure the CEG.</p>
<p>Better at programming tasks: 75% higher pass rate than directly sampling AlphaCode or Codex.</p>
<p>Agents</p>
<p>A model assigns sub-tasks to copies of itself, reads and writes to memory, has a chance to learn from their mistakes, etc.</p>
<blockquote>
<p>10 at HumanEval, comparing the LATS agent with basic few-shot prompting.0 ∼ 80× (for LATS at HumanEval)</p>
</blockquote>
<p>Solution choice enhancements</p>
<p>Continued on next page</p>
<p>Conceptual framework</p>
<p>Benefits of post-training enhancements</p>
<p>Measuring the performance improvement from a posttraining enhancement is not straightforward: there are hundreds of different benchmarks for AI capabilities, and different benchmarks often use different units to measure performance.8A 5% improvement in one benchmark may reflect a greater capability gain than a 5% improvement in another.This makes it hard to compare the performance gains from different enhancements, when they improve performance on different benchmarks.</p>
<p>To address this problem, we would like to translate all performance gains into a common unit.A good candidate unit is training compute.The literature on scaling laws has identified a strong relationship between the compute spent on training and the general capabilities of ML models, so train-ing compute is a good proxy for capability (Henighan et al.,  2020; Hoffmann et al., 2022; Srivastava et al., 2023).</p>
<p>Concretely, we can calculate the additional compute that would be required to match the performance improvement from an enhancement.We call this quantity the computeequivalent gain (CEG).9It can be calculated in the following manner:</p>
<ol>
<li>Measure the performance p of an AI model trained with training compute C without the post-training enhancement.The model should be trained compute-optimally, so that C is the minimal compute required to attain performance p. 4. The CEG is given by C ′ /C</li>
</ol>
<p>A similar approach was used by (Hilton et al., 2023)  Gains from enhancements and gains from scaling might interact and compound in nontrivial ways.For example, chain-of-thought prompting typically improves performance more in bigger models.We don't attempt to quantify these effects.We calculate the CEG at the scale used in the paper that introduces the post-training enhancement.We calculate the CEG using a reproducible methodology -see appendix A for a detailed description.</p>
<p>Cost of post-training enhancements</p>
<p>Once again, quantifying the cost of enhancing a model is not trivial.There are many factors that influence cost: the technical labor needed to incorporate the technique, the cost of the hardware, the effect on inference latency, etc (Cottier, 2023).We will focus on the compute costs of these enhancements because these are relevant for certain governance questions (see Section 6) and can be readily calculated.</p>
<p>Firstly, we calculate a one-time compute cost for fine-tuning.</p>
<p>Even if the post-training enhancement is not centrally about adapting a model to a downstream task, fine-tuning may be needed to train a verifier, teach an AI to use a new tool, or to add on a memory retrieval mechanism.Secondly, we calculate any ongoing higher inference costs from an enhancement.</p>
<p>The economic relevance of these costs depends on the use case.For example if one is primarily concerned with demonstrating a certain capability, the one-time training costs will be more relevant because inference costs for a single use are typically very low.Meanwhile, inference costs will be more relevant for large-scale deployment due to the large number of inferences that will be performed.</p>
<p>Analysis of post-training enhancements</p>
<p>In this section we examine the performance gains from many post-training enhancements, each time estimating benefits (via the CEG), the one-time cost of compute for fine-tuning, and any increased inference cost associated with the enhancement.Table 1, at the end of Section 1, summarizes the analysis.</p>
<p>Tool enhancements</p>
<p>The post-training enhancements in this section give models access to tools to improve their performance on downstream tasks.ChatGPT can already use many such tools via various plugins.</p>
<p>TOOLFORMER (Schick et al., 2023) augment a model with tools to improve performance on a variety of downstream tasks.They introduce Toolformer, a 6.7B parameter language model fine-tuned to use a number of tools: a calculator, a Q&amp;A system, a search engine, a translation system, and a calendar.We estimate that the compute used for fine-tuning is ∼ 0.1% of the compute used for pre-training. 12olformer uses different tools for different downstream tasks and the performance gain varies by task.</p>
<p>• Toolformer outperforms the 175B GPT-3 model on benchmarks for factual knowledge (table 3), math (table 4) and temporal questions (table 7).We estimate GPT-3 is trained with 20× more compute than Toolformer, 13 so the CEG is &gt; 20.</p>
<p>• Toolformer outperforms the 66B OPT model on benchmarks for question answering (table 5).We estimate OPT is trained with 7× more compute than Toolformer, 14 so the CEG is &gt; 7.</p>
<p>12 Toolformer is based on GPT-J, which took 1.5e22 FLOP to train (Wang and Komatsuzaki, 2021).Toolformer was trained for 2000 steps, with a batch size of 128, and sequence length of 1024 (Appendix B of Schick et al. (2023)).Since the model size is 6B, we estimate the fine-tuning process took 2000 * 128 * 1024 * 6e9 * 6 ≈ 1e19 FLOP. 13As stated in a previous footnote, we estimate Toolformer was trained with 1.5e22 FLOP.This contrasts with an estimated 3.1e23 FLOP for GPT-3 (see Schick et al. (2023)).</p>
<p>14 As stated in a previous footnote, we estimate Toolformer was trained with 1.5e22 FLOP.Meanwhile, OPT-66B was trained for 140k steps, using a batch size of 2M tokens (see the OPT baselines logbook and Table 1 in Zhang et al. (2022), respectively), so training took 140e3 * 2e6 * 66e9 * 6 = 1.1e23FLOP.The compute gain is then 1.1e23/1.5e22≈ 7.</p>
<p>• Toolformer doesn't show significant improvement in translation (table 6) despite having access to a translator.</p>
<p>So the CEG ranges from 1 (no gain) to &gt; 20 depending on the downstream task.</p>
<p>WEBGPT Nakano et al. (2021) train language models to use a web browser to answer long-form questions more accurately.They equip GPT-3 with access to a web browser, and collect demonstrations of humans using the web browser tool to answer questions from the ELI5 dataset.They then fine-tune the GPT-3 model on those demonstrations.We estimate that the compute used for fine-tuning is ∼ 0.01% of the compute used for pre-training. 15gure 3 shows the scaling trends with fine-tuning data.It can be seen that increasing the fine-tuning dataset by 8× leads to more improvement than scaling model size from 13B to 175B. 16We estimate that this corresponds to a CEG of ∼ 13×. 17</p>
<p>In addition, Nakano et al. (2021) collect human comparisons between different model-generated responses to questions, and train a reward model on those comparisons.This reward model is used to perform reinforcement learning and best-ofn sampling -a solution choice enhancement where multiple candidate solutions are generated and the one with the highest score (according to the reward model) is submitted.We estimate that the compute used for training the reward model is ∼ 0.01% of the compute used for pre-training. 19ey compare GPT-3 with WebGPT + best-of-n on the benchmark TruthfulQA.The performance of the 760M model -on both "% truthful" and "% truthful and informative" -improves more from adding WebGPT than from moving to the 175B model (which has 220× more training compute, see Figure 4).So the CEG is &gt; 220.</p>
<p>15 The dataset consisted of 6209 demonstrations, and training ranged from 2 to 5 epochs (see Appendixes B and E of Nakano et al.  (2021)).Assuming an average of 800 tokens per demonstration (obtained by scraping 100 demonstrations from the database), we get 10 to 25 million tokens seen during training.In comparison to the 300B tokens seen during pretraining (see Brown et al. (2020)), this requires ∼ 0.01% as much compute. 16Increasing the size of the fine-tuning dataset by a larger amount would probably lead to larger improvements. 17All GPT-3 models were trained on 300B tokens (see Table D.1 in Brown et al. ( 2020)), so the parameter increase is proportional to the compute increase.</p>
<p>18 Data from Figure 6 of Nakano et al. (2021). 19The dataset consisted of 21548 demonstrations, and training took 1 epoch (see Appendixes B and E of Nakano et al. (2021)).Assuming an average of 1600 tokens per comparison (two demonstrations), we get 35 million tokens seen, which again corresponds to about 0.01% of the pre-training compute.MEMORY RETRIEVAL Borgeaud et al. (2021) add a text retrieval mechanism to a pre-trained LLM via fine-tuning.They call this process "RETROfitting".Retrieval mechanisms work by storing a large number of short text sequences in a database.When the model is predicting a new token, the current text is compared to the stored text and the most similar stored text sequences are selected. 21The information from these stored sequences is used to inform predictions of upcoming tokens.This architecture is illustrated in the following figure.</p>
<p>While retrieval models can be trained from scratch, here we are interested in adding a retrieval mechanism to a pretrained model.This can be done with some additional finetuning.In the case of RETRO, the fine-tuning compute is &lt; 3.3% of the pre-training compute. 23The retrieval mechanism increases the cost of inference by less than 10%. 24e results can be seen in Figure 6.The improvement varies by dataset: • In C4 (far left), the 172M-parameter fine-tuned model achieves lower perplexity than the 1.5B-parameter baseline.In addition, the 425M model outperforms the 7.5B baseline.This corresponds to CEGs of 8 and 17. 25 • In Curation Corpus and LAMBADA there is almost no improvement: the accuracy of the fine-tuned models is close to their respective baselines.</p>
<p>• In Wikitext103 all the RETRO fine-tuned models significantly outperform all the baselines.This corresponds to a CEG of 43. 26 Prompting enhancements</p>
<p>FEW-SHOT PROMPTING</p>
<p>In one of the earliest examples of prompting innovations, Brown et al. ( 2020) prompt large language models with a series of solved examples of the task.While smaller models don't benefit much from adding these examples, larger models are able to learn how to perform the task solely from context, without any fine-tuning.</p>
<p>The gains vary by task (see Figure 7): in PhysicalQA and COQA, a 6.7B-parameter model using few-shot prompt-25 All models were trained on 420B tokens (see Appendix C.1 of Borgeaud et al. (2021)), so the training compute is proportional to the number of parameters.1.5B / 172M ≈ 8 and 7.5B / 425M ≈ 17.</p>
<p>26 All models were trained on 420B tokens (see Appendix C.1 of Borgeaud et al. (2021)), so the training compute is proportional to the number of parameters.7.5B/172M ≈ 43.ing outperforms a 13B-parameter model with one-shot and zero-shot prompting, respectively.This corresponds to a CEG of ∼ 2. Meanwhile, in SuperGLUE the gain is more significant: the 6.7B-parameter model with one-shot and few-shot prompting outperforms the 175B-parameter model with zero-shot prompting, corresponding to a CEG &gt; 26.27 CHAIN OF THOUGHT Wei et al. (2022) improve the reasoning ability of large language models by prompting them to generate a "chain of thought" (CoT), i.e. to think through their reasoning step-bystep.Language models have limited performance in tasks which require several serial steps of reasoning, like arithmetic or logical tasks.The authors mitigate this problem by including a few examples of correct reasoning processes in the language model's context window.The model then uses a similar reasoning process to answer subsequent questions.The method requires no additional training and doesn't specialize the model in any particular task.</p>
<p>While this post-training enhancement does not require any additional training compute, it usually increases the compute usage per inference.We estimate that the inference cost increases by up to 10×. 29 They test the approach in three domains: arithmetic reasoning, commonsense reasoning, and symbolic reasoning.The improvement from chain-of-thought usually increases with the scale of the model.Figure 6: Performance of "RETRO-fitted" models on four benchmarks.Note that C4, CC and Wikitext103 measure performance in perplexity (ppl, lower is better), while LAMBADA measures it in accuracy (acc, higher is better). 22gure 7: Performance of GPT-3 with few shot prompting on three benchmarks. 28e performance gain varies significantly by task, as can be seen in Figure 8:</p>
<p>• Mathematical reasoning: in GSM8K and MAWPS, chain-of-thought prompting improves the performance of PaLM-62B more than scaling the model size to 540B.This corresponds to a CEG of ∼ 9. 31 The improvements in other math benchmarks are smaller.</p>
<p>• Commonsense reasoning: in the Sports dataset, chainof-thought improves the performance of PaLM-8B and PaLM-62B more than increasing model size by 9×, corresponding to a CEG of 9.The improvement in other benchmarks is smaller.</p>
<p>• Symbolic reasoning: in within-domain Letter Concat, chain-of-thought improves performance by more than the increase from PaLM-8B to PaLM-540B.That corresponds to a CEG &gt; 67.In the out-of-distribution Letter Concat chain-of-thought improves performance by more than the increase from PaLM-62B to PaLM-540B, and the CEG is &gt; 9.</p>
<p>In Appendix B, we also obtained CEG estimates for chain of thought prompting on different datasets using the more recent models in Lanham et al. (2023), and found smaller gains.</p>
<p>There are many variations of chain of thought prompting.In a previous approach, Nye et al. (2021) prompted the model to write intermediate results in a "scratchpad" before outputting the final answer, which helped the model answer arithmetic problems and simulate code execution.There have also been subsequent enhancements that improve over chain-of-thought:</p>
<p>• Wang et al. (2023a) add an additional planning step to the prompt, which encourages the model to divide the task into subproblems and then solve them individually.Their zero-shot method roughly matches and sometimes improves over the few-shot chain-ofthought prompting on arithmetic, symbolic and commonsense reasoning tasks.</p>
<p>• Press et al. ( 2022) introduce the self-ask technique, where a model asks itself follow-up questions to aid its reasoning.This technique outperforms chain of thought on compositional reasoning tasks.</p>
<p>• Huang et al. (2022) fine-tune the model on its own generated reasoning chains.They show that this finetuning improves performance in several mathematical and reasoning benchmarks, both by itself and in combination with chain-of-thought prompting.This work combines together prompting enhancements and data enhancements.</p>
<p>Scaffolding enhancements</p>
<p>By default, large language models take one string of text as input and output one string of text as output.This inputoutput structure is poorly suited to performing many tasks.For example, large language models struggle with tasks that: have multiple subtasks, involve trial and error, must be carried out over long time horizons, require storing and retrieving memories, or involve continuous learning.Scaffolding enhancements structure the model's thinking and the flow of information between different instances of the model, allowing the resultant system to tackle a wider array of problems.</p>
<p>In many cases below, the modified model is capable of performing an entirely novel task (e.g.synthesizing chemicals, playing Minecraft).The CEG cannot meaningfully quantify such improvements because a bare language model cannot perform the task at all, and for this reason we don't provide CEG estimates for many enhancements in this category.This issue is discussed further in Section 4 TREE OF THOUGHTS   Yao et al. (2023) present Tree of Thoughts (ToT), a method for using LLMs to deliberately solve problems that require search, planning, or exploration of multiple reasoning paths.They frame the problem solving process as searching through a tree, where each node is a "thought" -a language sequence that represents an intermediate reasoning step.Tree of Thoughts allows the LLM to generate multiple thought candidates at each step and evaluate its progress heuristically.The tree can be navigated using standard search algorithms like BFS or DFS.</p>
<p>Figure 9 shows how Tree of Thoughts uses a language model (LM) to (a) propose and (b) evaluate thoughts on the "Game of 24" task:</p>
<p>Game of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic arithmetic operations (+ − * /) to obtain 24 .For example, given input "4 9 10 13", a solution output could be "(10 − 4) * (13 − 9) = 24".The authors tested Tree of Thoughts on 3 novel tasks -Game of 24, Creative Writing, Mini Crosswords -that other techniques such as Chain of Thought prompting and majority voting struggle with.On all 3 tasks, Tree of Thoughts significantly improved performance over these baselines.While these results are promising, the evaluation tasks remain fairly simple compared to real-world applications such as code generation or robotics.Moreover, because it systematically explores the space of possible thoughts, this method uses a lot of compute at inference time.For example, in the Game of 24 experiment, ToT visited ∼ 70 nodes, and sampled values 3 times per node, for a total of 4 inferences per node.Even if we assume that expanding and evaluating one node is cheaper than generating a full solution with CoT, ToT would likely consume more than 100× more compute than generating a single CoT solution.</p>
<p>AGENTS</p>
<p>Researchers have developed scaffolding programs that allow a large language model to power a simple autonomous agent.For example, Significant Gravitas (2023) prompts GPT-4 to generate sub-tasks, prioritise them, store them to memory, and execute them.This loop helps GPT-4 stay continuously focussed on the high-level task.AutoGPT also gives GPT-4 access to the internet, and this could be extended to additional tools and plug-ins.AutoGPT was the top trending Github repo in April 2023, and many other agent wrappers have been developed.Reflexion Shinn et al. (2023) show that their Reflexion agent significantly improves GPT-4's performance on benchmarks for multi-step text tasks, reasoning ability and programming.Reflexion gives GPT-4 information about its previous action and outcome of that action and then asks it to reflect on how it could have performed better.This reflection is included in the prompt next time GPT-4 attempts the task.</p>
<p>Voyager Wang et al. (2023c) introduce Voyager, a Minecraft agent.Voyager significantly improves on baselines from AutoGPT and Reflexion for learning skills over long time horizons and zero-shot generalization to new tasks.</p>
<p>Voyager benefits from an automated curriculum that provides it with a steady stream of tasks suited to its current abilities.Voyager writes code for performing specific actions and saves the code to a skill library; it writes code for complex actions by calling functions for simpler composite actions.This coherence is caused in part by GPT-4's role in the agent's memory.GPT-4 rates the importance of memories and their relevance to new situations.It is also periodically prompted to reflect on its memories and summarise the important high-level takeaways.These summaries are  After receiving a single high-level prompt (eg: "Synthesize Ibuprofen") the agent is able to search the web for information about the assigned task and develop an experimental procedure to achieve it, including writing low-level code for the robotic hardware and instructions for human handlers.</p>
<p>It can also interpret experimental results and detect failures in its own generated code.</p>
<p>LATS Zhou et al. ( 2023) unify many of the previous techniques into an agent architecture inspired by Monte Carlo Tree Search (MCTS).The model can take actions in an environment (for example, testing code it has written) and iteratively explores the action space in a tree, estimating the value of actions using a mix of the observations from the environment, evaluation by the model itself, and reflection on past failures.The estimated values of each action are then used to guide further exploration.They call this architecture Language Agent Tree Search (LATS).</p>
<p>They evaluate this architecture in three domains: question answering, programming, and online shopping.They obtain significant improvements over previous agent architectures (e.g.ReAct, Reflexion, Tree of Thoughts), at the cost of increased inference compute.The exact inference compute increase depends on the task and on the parameters of LATS, but for programming problems in HumanEval, we estimate that LATS increased inference compute by 80×. 32</p>
<p>In HumanEval, an implementation of LATS using GPT-3.5 outperforms GPT-4 with basic few-shot prompting. 33We estimate that GPT-4 was trained using 10× more compute than GPT-3.5, 34 so the CEG is &gt; 10.</p>
<p>In HotpotQA, we can compare LATS with ReAct (Yao et al.,  2023).Using a log-linear scaling model, we estimate the CEG is ∼ 400. 35However, we only have two datapoints to fit the log-linear scaling law, so this estimate is very uncertain</p>
<p>Solution choice enhancements</p>
<p>This category of post-training enhancements includes techniques for generating multiple candidate solutions and selecting which of these to submit as the final answer.Some examples are using a language model to rate candidate solutions, or a sampling strategy that increases the variety of generated solutions.</p>
<p>VERIFICATION</p>
<p>Cobbe et al. ( 2021) use a post-training enhancement to significantly improve performance on mathematical word 32 In the case of HumanEval, LATS samples 5 children at each depth, and uses a maximum depth of 8.Moreover, for each child, LATS calls the model twice: for sampling and for reflection.Assuming sampling and reflection contain the same amount of tokens, this means that LATS requires 8×5×2 = 80 times more inference compute than directly sampling from the model. 33See Table 3 of Zhou et al. (2023). 34The exact training compute of GPT-3.5 and GPT-4 is not known.However, common estimates are ∼ 2e25 FLOP for GPT-4 and ∼ 2e24 FLOP for GPT-3.5 (Epoch, 2022).So the ratio of training compute is ∼ 10×.</p>
<p>35 Chen et al. (2023) report the performance of several methods using GPT-4 and GPT-3.5 (Table 1).The differences in scores between the two models using input-output prompting is 15 points.ReAct achieves a score of 32, while LATS achieves 71 (Table 2 of Zhou et al. ( 2023)), for a difference of 39 points.This difference corresponds to 10 (39/15) = 400×.</p>
<p>problems.The key innovation is to fine-tune a large language model to assess how likely a proposed solution is to be correct.First, a large language model attempts solutions at 7500 training set questions.Then that same model is fine-tuned to predict how likely these solutions are to be correct.The fine-tuned model is called a "verifier".The compute used for fine-tuning is ∼ 0.05% of the compute used for training. 36 test time, the original model submits 100 candidate solutions and the verifier rates each of them.The solution with the highest rating is submitted.This technique is called "verification".</p>
<p>They compare verification with a baseline where the original model is fine-tuned to give correct answers on the training set. Figure 15 shows the performance of both techniques on a 6B parameter and 175B parameter model.Previously, the final answer of each training solution was checked automatically and the verifier was rewarded if its prediction matches this final answer.If the reasoning in a solution was flawed, but the final answer was correct by luck, the verifier is rewarded.This is "outcomes supervision" because the training signal is based on the ultimate outcome (the final answer).2023) instead train the verifier by having a human check whether each reasoning step of the training solution is correct.The verifier then predicts the correctness of each individual step, and is rewarded if its prediction matches the human's judgment.The verifier receives more fine-grained feedback than previously.This is "process supervision" because the training signal is based on the process used to generate the final answer.We estimate that the compute used for fine-tuning is ∼ 0.001% of the compute used for pre-training. 39However, gathering the 39 The fine-tuning dataset contains 800k reasoning steps, and the verifiers were trained for 2 epochs (see respectively section 2.4 and Appendix F.1 of Lightman et al. ( 2023)).If each step contains around 50 tokens, the total number of tokens seen is 2 * 800e3 * 50 = 80M.It's not known how many tokens GPT-4 was trained on, but 10T should be in the right ballpark, given estimates of GPT4's training compute of 2e25 FLOP and the Chinchilla scaling rule of tokens ≈ 2 * sqrt(FLOP).80M/10T = 0.001%.The context for these enhancements is as follows.A large language model, fine-tuned on coding problems, generates thousands of candidate solutions to each coding problem.These candidates are filtered based on whether they pass several example tests given in the problem statement, and then 10 of the remaining candidates are randomly chosen to be submitted.The coding problem is considered solved if any of the 10 submitted solutions is correct.This baseline set is already complex, involving various techniques for augmenting their data and efficiently sampling large numbers of candidate solutions.On top of this baseline, Li et al. ( 2022) make a number of additional posttraining enhancements.</p>
<p>Lightman et al. (</p>
<p>First, they make several post-training enhancements to the fine-tuning process so that the model learns more from each data point.This improves the quality of candidate solutions.The specific enhancements are described in section 4.3.</p>
<p>Second, they fine-tuned a model to generate additional tests of the candidate solutions, and cluster candidates solutions according to their behavior on these tests.Then one solution from each of the 10 biggest clusters is submitted.This improves the ability to select correct solutions from the many candidates.We estimate that the compute used for the second fine-tuning process is ∼ 0.45% of the compute used for pre-training. 4240 The dataset contains 800k step-level labels.Even if it only took 5 seconds to label one example, the total cost would reach thousands of hours of labor.</p>
<p>41 Figure 3 of the paper shows the performance of a larger model that was pretrained with roughly 200 times more compute; the outcome based performance of the larger model is 72%.Using 200× more training compute improves performance by 20% (from 52% to 72%).Assuming a log-linear relationship between training compute and performance, the 8% performance improvement from process-based feedback would be produced by a 8× increase in training compute: 200 (8/20) ≈ 8. 42 The pre-training dataset contained 715 GB, whereas the fine- Comparing the first row and the last row, a 300M model with all the post-training enhancements outperforms a 1B model with no post-training enhancements. 44We estimate that the 1B model took 5× more compute to train than the 300M model. 45So the CEG from these techniques in combination is greater than 6.</p>
<p>We can also calculate the CEG from an even simpler solution choice enhancement: running more samples.Figure 17 shows performance using all the post-training enhancements, but with a different number of samples.</p>
<p>The 300M model with 1 million samples outperforms the 9B model with 1000 samples.We estimate that the 9B model took 100× more compute to train than the 300M model. 47ning dataset contained only 3 GiB of data (see section 3 of Li et al. ( 2022)).3 * 2 * * 30/715e9 ≈ 0.45%. 43The paper also contains results for when 10,000, 100,000 and 1 million solutions are sampled.Our latter claim that the CEG exceeds 5 holds for each of these cases.</p>
<p>44 This is robustly true, i.e. it's still true in the other settings in the paper when 10,000, 100,000 and 1 million solutions are sampled.</p>
<p>45 From Table 3 of Li et al. (2022), the 300M model was trained on 354B tokens and had 284M parameters, while the 1B model was trained on 590B tokens and had 1.1B parameters.1.1e9 * 590e9/(284e6 * 354e9) ≈ 6.</p>
<p>46 Data from Li et al. (2022). 47From Table 3 of Li et al. (2022), the 300M model was trained on 354B tokens and had 284M parameters, while the 9B model was trained on 1250B tokens and had 8.7B parameters.8.7e9 * So the CEG from using 1000 times as many samples, once these other post-training enhancements are in place, is &gt; 100.</p>
<p>Data enhancements</p>
<p>Fine-tuning is a classic post-training enhancement.It significantly improves performance on certain tasks with relatively 1250e9/(284e6 * 354e9) ≈ 102.</p>
<p>small amounts of additional training.The post-training enhancements in this section increase the quality and quantity of available fine-tuning data.DATA CLEANING Lewkowycz et al. (2022) apply post-training enhancements to significantly improve the math performance of large language models.</p>
<p>The main innovation was to create a high quality data set for fine-tuning.The authors improved the data cleaning procedures used to process high quality math and science papers.They write:</p>
<p>Standard text cleaning procedures often remove symbols and formatting that are essential to the semantic meaning of mathematical expressions.By maintaining this information in the training data, the model learns to converse using standard mathematical notation.They fine-tune the large language model PaLM on a 118GB dataset of scientific papers cleaned using their improved procedure.We estimate that the compute used for finetuning was ∼ 10% of the compute used for pre-training. 48ey also use a post-training enhancement called majority voting 49 In this technique the model generates multiple solutions to each problem and submits the most common answer.For example, if it generates fifty solutions with the answer "5" and twenty with the answer "4", it submits "5".</p>
<p>Figure 19 shows that these techniques significantly improve performance on four challenging STEM benchmarks.</p>
<p>What is the CEG on these benchmarks?For the hardest benchmarks, MATH and OCWCourses, "Minerva 8B" and 48 Minerva employed 1024 TPUv4 for 29 days during finetuning, for a total of 1024 * 29 * 24 ≈ 710, 000 TPU-hours.Meanwhile, pre-training was done on 6144 chips for 1200 hours, for a total of 7,400,000 TPU-hours.So the ratio is 710, 000/7, 400, 000 ≈ 10%.</p>
<p>49 Majority voting is a "solution choice" technique, not a "data" technique.We discuss it in this section to avoid repeating material. 50Data from Lewkowycz et al. (2022).</p>
<p>"PaLM 540B").Minerva 8B performs better than PaLM 540B.Fine-tuning on their cleaned data set improves performance more than increasing the number of parameters from 8B to 540B, a factor of 67.We estimate 67× more training compute was required to train the 540B model than the 8B model. 51So the CEG from fine-tuning alone is greater than 67.</p>
<p>For the easier benchmarks, GSM8k and MMLU-STEM, we can compare "Minerva 62B" and "PaLM 540B").The performance of Minerva 62B is slightly worse than PaLM 540B.We estimate that 9× more training compute was required for the 540B model than the 62B model. 52So the CEG from fine-tuning alone is below 9.</p>
<p>We estimated the exact CEGs using a simple model in which performance increases linearly with log(training FLOP)see Appendix A. Some of these estimates seem implausibly high, suggesting that the log-linear model is not adequately capturing the true scaling behavior.</p>
<p>Post-training enhancements</p>
<p>Compute-equivalent gain 60</p>
<p>MATH OCWCourses GSM8k MMLU-STEM</p>
<p>Fine tuning 1,700 67 5 5</p>
<p>Fine tuning and majority voting 1,200,000 2,400 6 12</p>
<p>Table 3: Estimate of the CEG from Minerva based on a log-linear scaling law, rounded to 2 SD.</p>
<p>LEARNING FROM A TEACHER MODEL Mukherjee et al. (2023) use a post-training enhancement to improve the performance of instruction-tuned language models.They accomplish this by fine-tuning a small model on outputs from a larger model.In particular, they have GPT-3.5 and GPT-4 answer a large and varied corpus of complex questions, and fine-tune a 13B parameter model called Orca to imitate these answers.The teacher models are prompted to include detailed explanations and reasoning chains in their answers.This fine-tuning takes ∼ 2.5% of Orca's original training compute. 61</p>
<p>51 See Table 21 of Chowdhery et al. (2022). 52See Table 21 of Chowdhery et al. (2022). 61Orca is a 13B model fine-tuned on 6M samples for 4 epochs (see Mukherjee et al. (2023), section 3); if each example has an average of 1000 tokens this implies 13e9 * 6e6 * 4 * 1000 * 6 ≈ 1.9e21 FLOP for fine-tuning Orca.(A similar number, 2.2e21 FLOP, is reached by using their reported compute consumption: 200 hours on 20 A100, assuming As can be seen in Section 3, Orca roughly matches ChatGPT in a broad range of domains. 62We estimate that ChatGPT took ∼ 10× more compute to train than Orca-13. 63So the CEG from fine-tuning on LLM outputs is ∼ 10.</p>
<p>Importantly, this technique cannot be used to improve the largest existing models, but only smaller models, since it requires having a better, larger model as teacher.</p>
<p>GENERATING YOUR OWN FINE-TUNING DATA Haluptzok et al. (2022) use a post-training enhancement to significantly improve coding performance.The enhancement is to allow language models to generate their own fine-tuning data.A model first writes coding puzzles, then it suggests solutions to those puzzles.The solutions are verified in a Python interpreter and, if they are correct, the model is fine-tuned on those solutions.The process then repeats, with the improved model generating additional puzzles and solutions.</p>
<p>Figure 21 shows results from figure 7 of the paper: 65 Fine-312TFLOPS at a 50% utilization rate).In addition, 8e22 FLOP were required for pretraining Orca (13B parameters trained on 1T tokens, see the LLaMA model card, 13e9 * 1e12 * 6 ≈ 8e22).So we have 2.2e21/8e22 = 2.5%.</p>
<p>62 This type of training may only improve performance only in tasks for which training data is available (Gudibande et al.,  2023).As a consequence, the generality of the student model might be lower than that of the teacher model.However, Orca seems remarkably general, so the importance of this phenomenon is unclear. 63The total FLOP for pre-training and fine-tuning Orca is 8.2e22, as shown in a previous footnote.It's not clear how much compute GPT 3.5 took, but assuming it is more than GPT-3 (which took 3e23 FLOP) and less than 1/10th GPT-4 (that is, 2e24 FLOP), this is between 4× and 24× more compute.Picking the geometric midpoint, that's a factor of ∼ 10×.</p>
<p>64 Data from tables 8 and 11 of Mukherjee et al. ( 2023) 65 The table shows the pass@100 results, meaning that the model tuning with 1M samples takes ∼ 0.04% of the original training compute, 66 but it significantly improves performance.</p>
<p>Taking the smallest model as a baseline, fine-tuning improves performance more than moving to the largest model.</p>
<p>We estimate the largest model took 22× more training compute 67 than the smallest model, so the CEG is greater than 22.</p>
<p>The baseline intervention here is no fine-tuning for coding.</p>
<p>The improvement on a more competitive baseline would be smaller, though the paper does separately demonstrate meaningful improvement on a more competitive baseline. 68e use of a compiler to verify solutions limits the applicability of this post-training enhancement to coding tasks.However, Huang et al. (2022) also demonstrate significant improvements in question-answering by fine-tuning on a model's own outputs.Instead of an external verification tool (like a compiler) they use self-consistency to select gets 100 attempts at each problem.The CEG from pass@10 is significantly bigger. 66GPT Neo models were trained on The Pile, which has around 260B tokens (825GiB with 0.29 tokens/byte using the GPT-2 tokenizer, see Gao et al. (2020)).In addition, from the configuration files in Black et al. (2021) we know training took 4e5 steps with a batch size of 512.Assuming a sequence length of 1024 tokens, the GPT Neo models were trained on 4e5 * 512 * 1024 = 210B tokens, which closely matches a single epoch on the Pile.Meanwhile, fine-tuning was performed for one epoch on the 1M puzzle datasets described in Haluptzok et al. (2022), which have about 100M tokens.So inference took 100M/260B = 0.04% of the training compute. 67All the baseline models are pre-trained on the same data set (The Pile, as stated in Black et al. (2021)), so we just take the ratio between the parameter counts.2.7e9/1.25e8= 22.</p>
<p>68 In a separate experiment, a smaller model fine-tunes on solutions from a larger model both with and without a python interpreter verifying the solutions.Table 1 of the paper shows the results.The performance increase with the interpreter (38.2% − 7.5% = 30.7%) is more than twice the performance increase without it (21.5% − 7.5% = 14%).</p>
<p>Performance evaluations for AGIEval (left) and BIG-Bench-Hard (right).Note that the compute estimates for GPT-3.5 and GPT-4 are uncertain.The shaded region represents a 90% confidence interval. 64higher-quality solutions.</p>
<p>INSTRUCTGPT</p>
<p>Instruction fine-tuning and reinforcement learning from human feedback (RLHF) are now standard practices to improve LLM performance at following user instructions.Ouyang et al. (2022) introduced these techniques for instruction-following large scale models.</p>
<p>The first post-training enhancement is collecting human demonstrations of instruction following, and then finetuning the model on these demonstrations in a supervised fashion.The amount of compute used in fine-tuning is ∼ 0.1% of the amount used for pre-training. 7069 Data from Haluptzok et al. (2022). 70The dataset consisted of ∼ 13000 demonstrations, and the model was trained for 16 epochs.Assuming an average sequence length of 2048 tokens (GPT-3 context window size, see Brown et al. ( 2020)) the number of tokens seen during fine-tuning is 13000 * 16 * 2048 ∼ 426M.This is 0.14% the number of tokens</p>
<p>The second enhancement is collecting human comparisons between several generated responses and training a reward model on this data.This reward model learns to predict the rating that a human would give to a generated answer.Then, the reward model can be used to train the original model via reinforcement learning.This process takes ∼ 0.2% as much compute as pre-training. 71ey compared the rate at which human judges preferred various models' responses to a supervised fine-tuning baseline.This "win rate" is shown in the following figure.</p>
<p>Prompting GPT produces a larger improvement than scaling the model size by 130×.Fine-tuning further improves over this more than scaling by 30×, and RLHF further improves over fine-tuning more than scaling by 130×.Combining the benefits from supervised fine-tuning and RLHF (the two data enhancements), we estimate a total CEG of &gt; 3900. 73is metric is overly optimistic since it is precisely the opused for pre-training (300B, see Brown et al. (2020)). 71The reward model has 6B parameters and is fine-tuned on 33k sequences for 1 epoch (see Appendix C.3 and Table 6 in</p>
<p>Limitations to quantifying performance gains via the CEG</p>
<p>To quantify how much post-training enhancements increase model capabilities, we measured the compute-equivalent gain (CEG): the increase in pretraining compute that would be required to match the performance improvement from the post-training enhancement.</p>
<p>However, this metric can be difficult to both measure and interpret.</p>
<p>When measuring the CEG we would ideally compare models from the same family that were trained computeoptimally.But this is rarely the case in our examples.</p>
<p>With regards to interpretation, benchmarks may have been selected to exaggerate the effect of the post-training enhancements.In addition, a high CEG might not indicate that the post-training enhancement significantly improves performance, but instead indicate that additional training compute doesn't improve performance. 74Once again, the CEG is proportional to the increase in parameters, in this case 175B/1.3B= 130.</p>
<p>Rater preference for GPT models fine-tuned on instructions and human feedback. 72fficulties in measuring the CEG In this paper, we did not run our own experiments to calculate the CEG.Instead, we used results from other papers.This made it difficult to obtain rigorous estimates, since our methodology can only provide bounds on the CEG or point estimates based on scaling laws fit from very few datapoints.</p>
<p>VARIATION WITH SCALE</p>
<p>The performance improvement from a given enhancement might increase or decrease with the scale of the model (see Figure 22 for a toy example).For this reason, the CEG of an enhancement might not be the same at all scales.In this paper we only make use of the CEG to quantify the gain from each enhancement at a particular scale, 75 and avoid extrapolating these gains to larger-scale models.</p>
<p>This dependence of the CEG on scale introduces some potential ambiguity: we define the CEG as the additional compute that a non-enhanced model would need to match the performance of an enhanced one.But an equally valid alternative definition would be the reduction in compute that can be achieved with an enhanced model without reducing performance.If the CEG is not scale-invariant, these definitions will give different values.</p>
<p>Ultimately, both definitions capture the effect of a posttraining enhancement on the performance scaling curve, but care should be taken when comparing between them.</p>
<p>MODELS FROM DIFFERENT FAMILIES</p>
<p>In two cases we compare models from different families, introducing noise into the CEG estimates.To estimate the CEG, we compare the performance of two models: a smaller model with the enhancement and a larger model without it.However, sometimes the small and large models differ in other ways than the amount of pretraining compute (e.g.different architectures or different training data), and these confounding factors make the estimate of the CEG less reliable.This is the case for two out of our 13 estimates of the CEG. 76r example, we compared Orca (Mukherjee et al. (2023)), a fine-tuned 13B LLaMa model, with ChatGPT.These models have different architectures and training data, which adds uncertainty to our estimated CEG.</p>
<p>SUBOPTIMALLY SCALED MODELS</p>
<p>In many cases we compare models that are not trained compute-optimally, biasing the CEG estimate.Even when models come from the same family and differ only by the amount of pretraining compute, estimates of the CEG can still be misleading if the family of models were not trained compute-optimally.If the family of models had been trained compute-optimally then the performance gain from increasing training compute would be different, and so the calculated CEG would be different.</p>
<p>For example, when estimating the CEG of chain-of-thought prompting, we compare the performance of PALM-540B to that of PALM-8B.But PALM-540B was undertrained relative to its size, and the performance gain would have been larger if both models had been trained compute-optimally.This means we overestimate the CEG. 77Indeed, in Appendix B we got much lower estimates using data from the models in Lanham et al. (2023).</p>
<p>Pitfalls in interpreting the CEG</p>
<p>HIGH CEG MIGHT INDICATE THAT MORE COMPUTE DOESN'T IMPROVE PERFORMANCE</p>
<p>Even if we estimate the CEG using optimally scaled models from the same family, a high CEG on a dataset doesn't necessarily mean that the post-training enhancement is useful.It could also mean that for this particular dataset, the baseline of spending more compute on training is ineffective (see Figure 22).</p>
<p>Relatedly, some post-training enhancements allow models to perform tasks that would be impossible for any model 76 For Orca and Toolformer. 77Conversely, we would have underestimated the CEG if PALM-540B had been trained compute-optimally and PALM-8B had been over-trained.without it.In these cases, even if the enhancement greatly improves performance, the CEG is not meaningful.For example, many "LLM agent" scaffolds significantly increase the capabilities of language models by allowing them to execute code, store memories, or browse the web.Since no raw language model could perform these tasks, calculating CEG by comparing a smaller LLM agent to a larger raw language model is not appropriate.More promising would be to calculate the CEG by comparing two different agent architectures, but there is little available data for this.</p>
<p>METRIC AND DATASET SELECTION BIAS</p>
<p>Authors may be tempted to report metrics and evaluate on datasets that exaggerate the size of their contributions.This will increase estimates of the CEG.</p>
<p>In addition, authors are generally more likely to compare their enhancement against a baseline of spending more compute -thereby providing enough data to estimate a CEGif this emphasizes their method's strengths.These selection biases mean that the highest CEG numbers may be due to weak scaling baselines (see previous section), and may not transfer to other datasets.</p>
<p>Takeaway</p>
<p>Due to these limitations, individual CEG estimates can be noisy.Nonetheless, we think that the estimates provide a convenient and intuitive measure of the effectiveness of post-training enhancements.Situations in which the CEG estimates are most misleading (such as inflated estimates in cases where compute scaling does not improve performance) can be identified experimentally, and the corresponding estimates can be corrected or excluded from the analysis.Finally, many uncertain estimates can still be informative in aggregate.In particular, they indicate that post-training enhancements can cheaply provide performance improvements that would require scaling pre-training by one or two orders of magnitude.</p>
<p>Discussion</p>
<p>This section comments on trends in the examples discussed above.It points out that multiple enhancements can be combined together, argues that post-training enhancements will continue to improve capabilities in the future, and suggests that enhancements could make models more dangerous.</p>
<p>Multiple post-training enhancements can be combined together</p>
<p>We've seen many examples of multiple post-training enhancements being combined together:</p>
<p>• Lewkowycz et al. (2022) combines data cleaning with majority voting in Minerva, reaching a CEG of 30 in STEM benchmarks and 2400 in math benchmarks.</p>
<p>• Nakano et al. (2021) combines tool-use with best-of-n in WebGPT, reaching a CEG of 220 in TruthfulQA.</p>
<p>• AI agents typically combine multiple post-training enhancements relating to prompting, scaffolding, and tool-use.</p>
<p>• Ouyang et al. (2022) combines supervised fine-tuning with RLHF in InstructGPT, reaching a CEG of 130 in some NLP benchmarks and 3900 in preference ratings.</p>
<p>The CEG achieved by these combined enhancements are the highest that we have observed, significantly above the median, which indicates large benefits from combining enhancements.However, we have reasons to doubt each of these individual estimates (see Section 3) so we don't place much confidence in this conclusion.</p>
<p>When combining multiple post-training enhancements, it might be the case that the CEG of the combination is not the same as the product of the individual CEGs, as one might naively expect.We suspect there are diminishing returns to using more post-training enhancements for the same downstream task, such that the combined CEG becomes much lower than the product of individual CEGs.Unfortunately, we do not have enough data to study this question, as there are few studies that report results for both individual and combined enhancements.Larger models often benefit more from post-training enhancements than smaller models.For example, chain-ofthought has bigger benefits for larger models and small versions of Toolformer struggled to learn to use tools.This suggests that, as model size continues to increase, new posttraining enhancements will become accessible.</p>
<p>So we expect future post-training enhancements to contribute significantly to improvements in model capabilities.</p>
<p>Post-training enhancements have varied skill profiles</p>
<p>Some post-training enhancements are fairly general.Chainof-thought is an example (though it improves performance more in math and reasoning tasks), as are majority voting and best-of-n.Agent architectures enhance some skills (e.g.autonomy, planning and error-correction) that are useful across a wide range of domains.</p>
<p>Many post-training enhancements only improve performance in a narrow domain.For example, teaching the Toolformer (Schick et al., 2023) to use a calculator only improves its performance on tasks involving arithmetic, and "training a verifier to evaluate math solutions" (Cobbe et al.,  2021; Lightman et al., 2023) only improves performance on math problem solving.</p>
<p>Even for these narrow enhancements, it often seems like similar post-training enhancements might lead to improvements in a different domain.For example, the capabilities enhanced by Toolformer depend on the tools that are used, and one could train verifiers on many tasks (e.g. on instructions for carrying out biological experiments).Indeed, Boiko et al. ( 2023) enhanced capabilities in a narrow domain by connecting an AI system with robotic hardware for synthesizing chemicals.</p>
<p>Policy Implications</p>
<p>In this section, we discuss implications of post-training enhancements relating to dangerous AI capabilities and compute governance.</p>
<p>Recent AI governance proposals (Shevlane et al., 2023; Anderljung et al., 2023) recommend that frontier AI models be evaluated for dangerous capabilities before they are deployed.Shevlane et al. (2023) argue a model should be treated as highly dangerous "if it has a capability profile that would be sufficient for extreme harm, assuming misuse and/or misalignment", for example, if the model can gain access to weapons or acquire political influence, and appropriate defenses against these threats have not been implemented.Models exhibiting highly dangerous capabilities then might not be deployed at all, or structured access might be provided (Shevlane et al., 2023; Solaiman, 2023) This has a number of implications for dangerous capability evaluations.First, the best available post-training enhancements should be used when evaluating a model's dangerous capabilities, so that evaluations consider all the capabilities that could be unlocked by post-training enhancements rather than the capabilities of the pretrained model alone (Kinniment et al., 2023).</p>
<p>Second, measurements of a model's dangerous capabilities should be treated as a lower bound on what is possible, as future enhancements may increase those capabilities, perhaps significantly.Developers could err on the side of caution by incorporating a safety buffer into their deployment decisions (as in Anthropic ( 2023)).Specifically, protective measures (like restricting deployment) would be triggered at lower capability levels than those that have been defined as concerning.This way, a model's dangerous capabilities will remain unconcerning even with additional post-training enhancements.In practice, this could, for example, translate into lowering the success rate required to count a task as passed or the number of tasks a model must pass to consider it has a certain capability. 78wever, both the need for safety buffers and exhaustive use of enhancements during evaluations might be lower if developers monitor the model's dangerous capabilities along the value chain and respond to its evolution accordingly.More specifically, developers could supervise downstream uses by analyzing API inputs and outputs, extending existing practices (OpenAI, 2023; Anthropic, 2023).Then, developers could prevent or abort risky enhancements, for instance, through capability or feature restrictions and access frequency limits (O'Brien et al., 2023).</p>
<p>Future Work</p>
<p>Future research into post-training enhancements could improve estimates of the CEG, investigate the gains from combining multiple post-training enhancements together, and explore policy implications.</p>
<p>Improving estimates of the CEG</p>
<p>We have only provided rough bounds and estimates of the CEG from these techniques.There are multiple ways to improve these estimates.</p>
<p>COLLECT MORE DATA POINTS ON EACH POST-TRAINING</p>
<p>ENHANCEMENT</p>
<p>We only used data from one paper for each post-training enhancement.</p>
<p>Researchers could collect data for a wider range of benchmarks for each enhancement.This could clarify the kind of capabilities that a given post-training enhancement can improve, how general these improvements are, and how consistent the CEG is across similar benchmarks (e.g. is the CEG much bigger on some math benchmarks than others?).</p>
<p>In addition, researchers could collect data for a wider range of model sizes.This could shed light on how the effect of an enhancement changes as models become larger.Researchers could also calculate scaling laws for benchmark performance with and without the enhancement and use these to calculate the CEG (see Appendix A.1).</p>
<p>RUN EXPERIMENTS TO CALCULATE THE CEG</p>
<p>Even better, researchers could run controlled experiments: using the same model family for enhanced and nonenhanced models, training compute-optimally, and testing in pre-specified benchmarks.They could isolate the effect of the post-training enhancement and avoid many of the limitations of our methodology discussed in section 4.</p>
<p>CALCULATE THE CEG FOR AGENT SCAFFOLDING</p>
<p>As we mentioned in section 4, estimating the CEG from agent scaffolding is often difficult because usually bare LLMs simply can't perform the type of tasks that agents are designed to perform.</p>
<p>There are several possible approaches to mitigate this problem.Researchers could find more benchmarks in which bare LLMs can compete with agents, like HumanEval.But this would only work in a few domains.Researchers could also compare the performance of different agent architectures on the same agent benchmark.This would require innovations in benchmarking LLM agents, for example by building on Ruan et al. (2023), Liu et al. (2023), or Yang et al. (2023).</p>
<p>Investigate the combined impact of multiple post-training enhancements on AI capabilities Some of the individual post-training enhancements we mentioned, like chain-of-thought, have been intensely studied.However, the combined effects of multiple enhancements is also relevant for policy.There are several ways to improve our understanding of these effects.</p>
<p>Policy questions</p>
<p>The policy implications of post-training enhancements that we outlined in Section 6 raise some further questions.</p>
<p>• How can we quickly identify new and potentially dangerous post-training enhancements?</p>
<p>• How can we prevent dangerous post-training enhancements from being applied to models?</p>
<p>• Which actors have historically found improved posttraining enhancements, and what resources were necessary to discover them?</p>
<p>• How should we govern complex ecosystems composed of many interacting AI systems and software systems from multiple providers?</p>
<p>Conclusion</p>
<p>We have introduced a basic framework for quantifying the benefits and costs of post-training enhancements, most notably measuring the benefits via the CEG.We applied this framework to a representative collection of post-training enhancements and saw that while the performance improvements can be significant, the fine-tuning costs are typically very small compared to the cost of pre-training.It seems likely that new post-training enhancements will continue to improve AI capabilities, though it's unclear how much total room for further improvement there is.The potential for a wide range of actors to improve frontier AI capabilities poses a distinct challenge for AI governance.</p>
<p>The results can be seen in Table 4. Overall, these figures are significantly lower than the CEG estimates derived from Wei et al. (2022).This can be attributed to a few factors.First, the tasks are less symbolic or mathematical, with the exception of Aqua where the chain-of-thought prompting proves to be most beneficial.Second, the models are not as undertrained as those found within the PaLM family.On the Aqua dataset, the 175B parameter model does worse than the 52B parameter model, so our estimate is meaningless.However, chain-of-thought prompting greatly increases the performance of the 175B model (by more than 10 percentage points).This suggests that the CEG at larger model sizes would be even larger on that dataset.However, the benefits of chain-of-thought prompting don't noticeably increase between 52B and 175B on the other datasets.</p>
<p>information, and models of different size are trained on the same number of tokens in Askell et al. (2021).</p>
<p>Figure 1 :
1
Figure 1: Illustration of the compute-equivalent gain.The enhanced model has the same performance as a nonenhanced model trained with 5× more compute.</p>
<p>Figure 2 :
2
Figure 2: Distribution of CEG and additional costs of the techniques we studied.The one-time cost is given as a fraction of pre-training, the runtime cost is relative to the runtime cost without the enhancement.Enhancements without one-time cost are shown with an arrow on the y axis.</p>
<p>2.</p>
<p>Measure the performance p * of an AI model trained optimally with training compute C with the post-training enhancement.3. Estimate the training compute C ′ needed to achieve performance p * without the post-training enhancement, again training optimally.</p>
<p>Figure 3 :
3
Figure 3: Performance of WebGPT with different quantities of demonstrations used for fine-tuning. 18</p>
<p>Figure 4 :
4
Figure 4: Performance of WebGPT and GPT-3 on Truth-fulQA. 20</p>
<p>Figure 5 :
5
Figure 5: RETRO architecture.Extracted from Borgeaud et al. (2021).</p>
<p>Figure 8 :
8
Figure 8: Performance of Chain of Thought and standard prompting for PaLM in several benchmarks. 30</p>
<p>Figure 9 :
9
Figure 9: Example of Tree of Thoughts.Extracted from Yao et al. (2023).</p>
<p>PARSEL</p>
<p>Zelikman et al. (2023) introduce a framework that uses LLMs to solve complex algorithmic problems.A language model first writes a decomposition of the problem in an intermediate language called Parsel, in which functions are defined through natural language descriptions and constraints (e.g. unit tests).Then, a code LLM generates modular implementations of all the functions in the Parsel program.Finally, the method searches over combinations of these modular implementations, guided by the constraints in the Parsel program.</p>
<p>Figure 10 :
10
Figure 10: Diagram of the Parsel framework.Extracted from Zelikman et al. (2023).</p>
<p>Figure 11 :
11
Figure 11: AutoGPT allows GPT-4 to power an agent.Extracted from Mowshowitz (2023).</p>
<p>Figure 12 :
12
Figure 12: Voyager gets more items than previous methods with the same number of agent interactions.Extracted from Wang et al. (2023c).</p>
<p>Figure 13 :
13
Figure 13: Agent architecture from Park et al. (2023).</p>
<p>Figure 14 :
14
Figure 14: Architecture of the Agent from Boiko et al. (2023).</p>
<p>Figure 15 :
15
Figure15: Performance of GPT-3 fine-tuned on GSM8K, with and without verification.37</p>
<p>Figure 16 :
16
Figure 16: Comparison between outcome supervision and process supervision at several numbers of samples.Extracted from Lightman et al. (2023)</p>
<p>Figure 16 compares process supervision (PRM) with two types of outcome supervision (ORM).Process-based supervision improves performance by ∼ 8% compared to outcome-based supervision.We estimate this corresponds to a CEG of ∼ 8. 41 Note that in this example a post-training enhancement significantly improves upon the baseline from a previous enhancement, implying that there can be continuous improvement from successive post-training enhancements.</p>
<p>Figure 17 :
17
Figure 17: Performance of AlphaCode enhancements with 1k and 1M samples. 46</p>
<p>Figure 18 :
18
Figure 18: Example of data processing used in Minerva.Extracted from Lewkowycz et al. (2022).</p>
<p>Figure 19 :
19
Figure 19: Performance of Minerva on four STEM benchmarks. 50</p>
<p>Figure 20 :
20
Figure 20: Self-improvement pipeline from Haluptzok et al. (2022).</p>
<p>Brown  et al. (2020)).This gives 6 * 6e9 * 33e3 * 2048 = 2.4e18 FLOP, five orders of magnitude less than the pre-training cost.Training via reinforcement learning takes 256k episodes.Assuming an average episode length of 2048, we get 256e3 * 2048 = 5.24e8 tokens, which is 0.17% of those used in pre-training.72Data from Figure1ofOuyang et al. (2022)  73 Since all GPT-3 variants were trained on 300B tokens, the amount of training compute is proportional to the model size.4 * 130 = 520.</p>
<p>Figure 21 :
21
Figure 21: Performance achieved by code models fine-tuned on synthetic data. 69</p>
<p>74</p>
<p>Figure 22 :
22
Figure 22: Simulated example illustrating how the CEG of an enhancement can vary with scale.If scaling the baseline does not improve performance, the CEG stops being meaningful.</p>
<p>Table 1 :
1
Summary of post-training enhancements (PTEs), their compute-equivalent gain (CEG) and their associated costs.
(Continued)TechniqueExplanationCompute-equivalent gain (CEG)One-time cost (fraction of training compute)New inference cost (multiple of initial cost)AlphaCode sample selectionSix techniques for choosing out of 1000 s of candidates. which coding solutions to submit∼ 6 on a coding benchmark∼ 0.45%&lt; 2Data enhancementsData cleaningFine-tune a large language model on a carefully cleaned STEM dataset.∼ 5 in STEM benchmarks; &gt; 67 in math benchmarks∼ 10%1Data cleaning + majority votingAs above, but also generate multiple solutions and submit the most common answer.&gt; 6 in two STEM benchmarks. &gt; 2000 in two math benchmarks∼ 10%&gt; 16 for STEM benchmarks &gt; 64 for math benchmarksLearning from a teacher modelFine-tune a small model on detailed explanations produced by a larger model∼ 10 on a range of benchmarks∼ 2.5%1Generating your own fine-tuning dataModels write coding puzzles and on correct solutions solutions; solutions are automatically checked; fine-tune&gt; 22 in a coding for coding benchmark, compared to a baseline with no fine-tuning∼ 0.04%1Fine-tune a model on examples&gt; 3900 at instructionof humans followingfollowing;Instruct GPTinstructions; fine-tune against a reward model&gt; 130 on some other NLP benchmarks;∼ 0.31%1trained to predict humanno gain on many NLPpreferences.benchmarks</p>
<p>Table 2 :
2
Cumulative enhancements applied in Li et al. (2022).The table combines information from table 8 and figure 6a.
Post-training enhancementsModel sizeSolve rateNone1B6.7%+ MLM (Masked Language Modeling) loss1B6.6%+ Tempering1B7.7%+ Tags and Ratings1B6.8%+ Value conditioning1B10.6%+ GOLD (Generation by Off-policy Learning from Demonstrations)1B12.4%+ Clustering (All post-training enhancements)12.2%All post-training enhancements300M7.5%
Table 2 shows the cumulative effect of adding six successive post-training enhancements when 1000 candidate solutions 43 are initially produced:</p>
<p>STUDY THE DIMINISHING RETURNS TO ADDITIONAL POST-TRAINING ENHANCEMENTSIt seems unlikely that the CEG of multiple enhancements is as high as the product of the individual CEGs, since this would quickly result in an extremely high CEG.So there are likely diminishing returns as the number of enhancements applied to a model increases.Researchers could study this by running controlled experiments with different combinations of enhancements.Researchers could also study whether there is a ceiling to the total improvement you can get from post-training enhancements, no matter how many you apply.If policy makers could put a ceiling on the total improvement from future post-training enhancements, they could better predict how much more dangerous a model might become in the future.INVESTIGATE THE "SKILL PROFILE" OF POST-TRAININGENHANCEMENTSThe gains from a single post-training enhancement are often narrow, but different enhancements might help with different tasks.This raises the question of how much more general the gains might be from multiple enhancements combined together.If policy makers knew the breadth of capability improvements that post-training enhancements provide in combination, they could again better predict how much more dangerous a model might become in the future.STUDY THE RATE OF IMPROVEMENT FROM POST-TRAINING ENHANCEMENTS OVER TIMEAs more post-training enhancements are developed, and existing ones are improved, estimates of the CEG from the best enhancements will likely increase.Tracking how the CEG increases over time gives a rate of improvement for post-training enhancements.Comparing this rate of improvement with the rates of improvement from compute scaling or other algorithmic improvement, would give a better sense for how important post-training enhancements are overall for driving AI progress.It would also be informative for the design of safety buffers.</p>
<p>Table 4 :
4
Estimates of the CEG of chain-of-thought prompting using data fromLanham et al. (2023).
Aqua0.05TruthfulQA1.4MMLU1.2OpenBookQA1.7ARC (challenge)1.5LogiQA1.3ARC (Easy)1.4HellaSwag2
The fact that pre-training helps in a wide variety of tasks has been widely documented(Brown et al., 2020;Srivastava et al., 2023). In particular, pre-training scaling usually also improves performance in the tasks in which each particular posttraining enhancement claims benefits (see, for example,(Press  et al., 2022)  and(Wei et al., 2022) for chain-of-thought prompting,(Lewkowycz et al., 2022) for data cleaning,(Parisi et al., 2022) for tool use, and(Li et al., 2022) for solution choice
).6  Note that some of these categories often overlap. For example, scaffolding enhancements commonly also generate multiple candidate solutions.
For example, Klarity uses post-training enhancements to help automate accounting work.
For example, language modeling benchmarks commonly use perplexity or cross-entropy, whereas question-answering or problemsolving benchmarks tend to use accuracy (see(Srivastava et al., 2023) or(Hoffmann et al., 2022) for some examples).
  9  Note that the exact relation between training compute and capabilities depends on the model family, so our definition of computerelative gain is also relative to a model family. See Appendix A for more details.
The CEG can be seen as the ratio of the intrinsic performances of two models, one in the family of enhanced models and the other in the original family of models.
See Appendix A for more details on the problems associated with these bounds and possible solutions.
Data from Figure 3 of Nakano et al. (2021).
The nearest neighbors are computed using the embeddings produced by a frozen BERT model.
Data from Figure5ofBorgeaud et al. (2021).
The additional weights of the retrieval mechanism are less than 10% of the weights of the original model, and the model is fine-tuned on 3% of the original pre-training data. Therefore, the compute used in fine-tuning is less than 0.03 * 1.10 = 3.3%.
The additional weights of the retrieval mechanism are less than 10% of the weights of the original model. The cost of computing nearest neighbors is amortized over many tokens and therefore we ignore it.
All GPT-3 models were trained on 300B tokens, so their training compute is proportional to the model size(Table D.1 of Brown  et al. (2020)). 13B/6.7B ≈ 2, 175B / 6.7B ≈ 26.
28 Data from figures 3.6, 3.7 and 3.8 ofBrown et al. (2020
).29  Based on the ratio between the length in tokens of the chains of thought and the answers themselves, in the prompts provided by the authors. The chains of thought are up to 10× longer than the answers (see Appendix G ofWei et al. (2022)).
Data from figures 4, 7 and 8 ofWei et al. (2022).
All PaLM models were trained on 780B tokens (see Chowdhery et al. (2022)), so the CEG is equal to the parameter increase. 540B/62B ≈ 9
GPT models were trained on 300B tokens. The verification dataset consists of 7500 problems, with 100 solutions per problem. The datapoints have an average of ∼ 200 tokens (see the original data), and the verifiers were fine-tuned for one epoch (Appendix B ofCobbe et al. (2021)). So the ratio of tokens used in fine-tuning is 200 * 100 * 7500/300B = 0.05%.
Data from Figure5ofCobbe et al. (2021).
All GPT-3 models were trained on 300B tokens, so their training compute is proportional to the model size(Table D.1 of Brown  et al. (2020)). 175B/6B ≈ 26
Although the scale is different for each enhancement. If all of them show increasing gains with scale, this would bias any comparison of CEGs between enhancements.
For example,Anthropic (2023) counts a task as "passed" if the model succeeds 10% of the time, and considers an evaluation threshold as met if at least 50% of the tasks are passed. The organization presents these numbers as a conservative approach that accounts for a safety buffer.
The amortized cost of compute for training GPT-4 is estimated to be between $30 and $90 million(Epoch, 2023) 
In some cases, one could use scaling laws to compensate for the difference in architecture, although we did not actually do this.
AcknowledgmentsWe would like to express our thanks to the people who have offered feedback and input on the ideas in this paper, including Tamay Besiroglu, Ege Erdil, Matthew Barnett, Jaime Sevilla, Girish Sastry, Sam Bowman, Toby Shevlane, Lewis Ho, Markus Anderljung, Sébastien Krier, Cullen O'Keefe, Holden Karnofsky, Roger Grosse, Hjalmar Wijk, Jacob Hilton, Jacob Steinhardt, Marius Hobbhahn, Lukas Finnveden, Josh You and Eli Lifland.A. A reproducible for calculating the compute-equivalent gainThis methodology seeks to produce either a lower bound or a point estimate of the CEG of a post-training enhancement In the following, C(M ) denotes the training compute of model M , and P (M ) denotes the performance of model M in a certain task or benchmark.1. Pick a paper which tests an enhancement on one or more benchmarks.2. Identify all benchmarks used to evaluate models with and without the enhancement.For each such benchmark:(a) Identify all cases where a smaller enhanced model outperforms a larger model without the enhancement.More precisely, identify all pairs of models ML (larger model) and ME (enhanced model) that satisfy these four requirements: i. ML is not enhanced ii.ME is enhanced iii.ML was trained using more compute than ME iv.(if possible) ME performs better than ML according to the metric -P(ME) &gt; P(ML) v. (if possible) The two models have the same architecture and training method, except for the model size and data size (and the enhancement).(b) The difference in training compute between ME and ML will be used to calculate the CEG.There are several possibilities here: i.If all the above requirements are met, the CEG is lower bounded by C(ML) / C(ME).ii.If requirement (iv) can't be satisfied, but P(ML) -P(ME) is small relative to the overall variation in performance from scaling, then we can use C(ML)/C(ME) as a rough point estimate for the CEG.iii.If requirement (iv) can't be satisfied, but there is enough data at several scales to fit a scaling law to models without the enhancement, the CEG can be estimated directly using this scaling law.See appendix A.1.iv.If requirement (v) can't be satisfied, we can try to find a pair such that the architectures and training methods are as similar as possible. 80(c) Keep the highest CEG of all the pairings in the benchmark, unless there are specific reasons to reject some pairs.We came across four such reasons: i.If there were multiple non-enhanced baselines, we chose the best non-enhanced baseline, which leads to a lower CEG.This was the case for verification with process-based feedback, where we used the best outcome-based baseline.ii.If multiple enhancements are used in a model but we are evaluating the gains from one particular enhancement are of interest, then we discarded the pairs with multiple enhancements.We did this for InstructGPT and We-bGPT.iii.If the same benchmark is evaluated using finetuning datasets with several different sizes, we use the largest dataset for calculating the CEG because we are more interested in more capable models.This was the case for verification with process-based feedback, where we took the largest dataset size.iv.If the same benchmark is evaluated multiple times with different settings that don't change the CEG, we arbitrarily choose one of them.For example, AlphaCode evaluates a benchmark using different numbers of total samples.We just report the CEG for 10@1K -and not for 10@10K, 10@100K, and 10@1M -but a different choice would not have changed the result.(d) Note explicitly any confounders that might lead to over-or under-estimation of the CEG.3. Note that we don't always report the CEG for all tasks that are presented in a paper, but only those that we believe are most representative of the actual gains.In particular, some confounders and concerns that lead us to reject a task/metric/benchmark are:• Using LLM-based evaluation.• Benchmarks that measures fairness, alignment, safety or other metrics that are less related to capabilities.A.1.Compute CEG using a scaling lawWhen the baseline (non-enhanced) architecture is evaluated at at least three different scales, and a simple log-linear or power-law scaling curve fits the data well, we can use this fitted scaling law to estimate the CEG.If the fitted scaling law is P = f (C), then we can approximate the CEG as f −1 (P (M E))/C(M E).An example of this approach for Minerva can be seen in Figure23.81Note that benchmark performance often exhibits complicated scaling behavior (e.g.: plateaus or inverse scaling), so CEG estimates based on simple scaling laws should be taken with a grain of salt.To make this method more reliable, it would be necessary to precisely characterize the scaling laws for the benchmark, including any nonlinearities, which we have not done.Fitting performance scaling laws could make it easier to evaluate how performance improvements change with scale, since we could predict how larger models would benefit from an enhancement.We have not done this analysis.81The code we used is in this Colab notebook.B. A second estimate of the CEG of chain-of-thought promptingIn section 3, we estimated the CEG of Chain of Thought prompting based on data from Wei et al. (2022).However, this paper reports results from the PaLM family of models, which uses a suboptimal scaling strategy: some of the PaLM models are undertrained.This makes the resulting CEG estimates less reliable.Therefore, we also estimated the CEG using from Figure10in Lanham et al. (2023), which presents the accuracy on 8 datasets of language models of different sizes, with and without chain of thought prompting.Although the authors do not provide detailed information about pretraining, we expect these models to be less undertrained than the PaLM models, since the paper dates from long after the improvement in optimal scaling laws from Hoffmann et al.(2022).We estimate the CEG at 52B parameters using the method in Appendix A1:• We first fit a log-affine scaling law to the model size f (N ) = m log(N ) + p between 52B and 175B parameters.• We then measure the accuracy of ME, the 52B parameter model with chain-of-thought prompting: P (M E) = Acc (52B, with CoT)• Finally, we obtain the CEG as the quotient f −1 (P (M E))/N (M E) 8282 This assumes that the 52B and 175B parameter models were trained on the same number of tokens, so that the ratio of model sizes is the ratio of training computes.The paper does not mention this explicitly, but they refer to past Anthropic papers for more
Compute trends across three eras of machine learning. Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, Pablo Villalobos, 2022</p>
<p>Will we run out of data? an analysis of the limits of scaling datasets in machine learning. Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, Anson Ho, 2022</p>
<p>Measuring the algorithmic efficiency of neural networks. Danny Hernandez, Tom B Brown, 2020</p>
<p>Algorithmic progress in computer vision. Ege Erdil, Tamay Besiroglu, 2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Transactions on Machine Learning Research. 2835-88562023BIG-bench collaboration)</p>
<p>blog/trading-off-compute-in-t raining-and-inference. Pablo Villalobos, David Atkinson, 2023Trading off compute in training and inference</p>
<p>Frontier ai regulation: Managing emerging risks to public safety. Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen Okeefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cassbeggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett Yonadav Shavit. Divya Siddarth, Robert Trager, and Kevin Wolf2023</p>
<p>Augmented language models: a survey. Roberto Gregoire Mialon, Maria Dessi, Christoforos Lomeli, Ram Nalmpantis, Roberta Pasunuru, Raileanu, Timo Baptiste Roziere, Jane Schick, Asli Dwivediyu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 2023</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Measuring and narrowing the compositionality gap in language models. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Gretchen Herbertvoss, Tom Krueger, Rewon Henighan, Aditya Child, Ramesh, M Daniel, Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec Mccandlish, Ilya Radford, Dario Sutskever, Amodei, 2020. 2022Ofir PressMuru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike LewisLanguage models are few-shot learners</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2022</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutmansolo, Yuhuai Wu, Behnam Neyshabur, Guy Gurari, Vedant Misra, 2022</p>
<p>Talm: Tool augmented language models. Aaron Parisi, Yao Zhao, Noah Fiedel, 2022</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson Dautume, Igor Babuschkin, Xinyun Chen, Posen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Kawei Lee, Eepeng Lim, 2023a</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang Hongkun, Jiawei Yu, Han, 2022</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivediyu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Evaluating language-model agents on realistic autonomous tasks. Megan Kinniment, Lucas Jun, Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Hjalmar Tao R Lin, Joel Wijk, Aaron Burget, Elizabeth Ho, Paul Barnes, Christiano, July 2023</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, 2023</p>
<p>Key trends and figures in machine learning. Epoch, 2023</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Casas, Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican George Van Den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Jack W Rae, Laurent Vinyals, Sifre, 2022</p>
<p>Scaling laws for autoregressive generative modeling. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M Ziegler, John Schulman, Dario Amodei, Sam Mccandlish, 2020</p>
<p>Scaling laws for single-agent reinforcement learning. Jacob Hilton, Jie Tang, John Schulman, 2023</p>
<p>Trends in the dollar training cost of machine learning systems. Ben Cottier, 2023-the-dollar-train ing-cost-of-machine-learning-systems</p>
<p>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. Ben Wang, Aran Komatsuzaki, May 2021</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022</p>
<p>Webgpt: Browser-assisted questionanswering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, 2021</p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jeanbaptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Oriol Irving, Simon Vinyals, Karen Osindero, Simonyan, Erich Jack W Rae, Laurent Elsen, Sifre, 2021</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern2022Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. Palm: Scaling language modeling with pathways</p>
<p>Measuring faithfulness in chain-ofthought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukovsiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam Mccandlish Sandipan, Saurav Kundu, Shannon Kadavath, Thomas Yang, Timothy Henighan, Timothy Maxwell, Tristan Telleenlawton, Zac Hume, Jared Hatfield-Dodds, Jan Kaplan, Brauner, Ethan Samuel R Bowman, Perez, 2023</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gurari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, 2021</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Karthik Thomas L Griffiths Yuan Cao, Narasimhan, 2023</p>
<p>Parsel: Algorithmic reasoning with language models by composing decompositions. Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, Nick Haber, 2023</p>
<p>. Significant Gravitas, Autogpt, 10-11-2023Zvi Mowshowitz. On autogpt. 2023. 2023</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen, A survey on large language model based autonomous agents. 2023b</p>
<p>Cognitive architectures for language agents. Shunyu Theodore R Sumers, Karthik Yao, Thomas L Narasimhan, Griffiths, 2023</p>
<p>Llm-powered autonomous agents. lilianweng.github.io. Lilian Weng, 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, 2023c</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Sung Joon, Joseph C Park, Carrie J Obrien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang2023. 2023Generative agents: Interactive simulacra of human behavior</p>
<p>Parameter, compute and data trends in machine learning. 2022</p>
<p>Fireact: Toward language agent fine-tuning. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Burda, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan Leike. 2023Harri Edwards</p>
<p>Orca: Progressive learning from complex explanation traces of gpt-4. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, 2023</p>
<p>The false promise of imitating proprietary llms. Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, 2023</p>
<p>Language models can teach themselves to program better. Patrick Haluptzok, Matthew Bowers, Adam Tauman, Kalai , 2022</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, 2020</p>
<p>GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Sid Black, Gao Leo, Phil Wang, Connor Leahy, Stella Biderman, 10.5281/zenodo.5297715August 2021</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins Been, Iason Kim, Vijay Gabriel, Jack Bolina, Clark, Yoshua Bengio, Paul Christiano, and Allan Dafoe. Model evaluation for extreme risks. 2023</p>
<p>The gradient of generative ai release: Methods and considerations. Irene Solaiman, 2023</p>
<p>Anthropic's responsible scaling policy. Anthropic, 2023Accessed 20-11-2023</p>
<p>Gpt-4 system card. 20-11-20232023OpenAI</p>
<p>Deployment corrections: An incident response framework for frontier ai models. O' Joe, Shaun Brien, Zoe Ee, Williams, 2023Accessed 20-11-2023</p>
<p>Oversight for frontier ai through a know-your-customer scheme for compute providers. Janet Egan, Lennart Heim, 2023</p>
<p>What does it take to catch a chinchilla? verifying rules on large-scale neural network training via compute monitoring. Yonadav Shavit, 2023</p>
<p>Identifying the risks of lm agents with an lm-emulated sandbox. Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, Tatsunori Hashimoto, 2023</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. 2023</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>A general language assistant as a laboratory for alignment. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Jared Kaplan, 2021</p>            </div>
        </div>

    </div>
</body>
</html>
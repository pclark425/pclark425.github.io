<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6896 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6896</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6896</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-133.html">extraction-schema-133</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <p><strong>Paper ID:</strong> paper-254591185</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2212.06726v2.pdf" target="_blank">Semantic Brain Decoding: from fMRI to conceptually similar image reconstruction of visual stimuli</a></p>
                <p><strong>Paper Abstract:</strong> Brain decoding is a field of computational neuroscience that uses measurable brain activity to infer mental states or internal representations of perceptual inputs. Therefore, we propose a novel approach to brain decoding that also relies on semantic and contextual similarity. We employ an fMRI dataset of natural image vision and create a deep learning decoding pipeline inspired by the existence of both bottom-up and top-down processes in human vision. We train a linear brain-to-feature model to map fMRI activity features to visual stimuli features, assuming that the brain projects visual information onto a space that is homeomorphic to the latent space represented by the last convolutional layer of a pretrained convolutional neural network, which typically collects a variety of semantic features that summarize and highlight similarities and differences between concepts. These features are then categorized in the latent space using a nearest-neighbor strategy, and the results are used to condition a generative latent diffusion model to create novel images. From fMRI data only, we produce reconstructions of visual stimuli that match the original content very well on a semantic level, surpassing the state of the art in previous literature. We evaluate our work and obtain good results using a quantitative semantic metric (the Wu-Palmer similarity metric over the WordNet lexicon, which had an average value of 0.57) and perform a human evaluation experiment that resulted in correct evaluation, according to the multiplicity of human criteria in evaluating image similarity, in over 80% of the test set.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6896.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6896.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hub-and-spoke</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hub-and-spoke theory of semantic representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurocognitive theory proposing that conceptual knowledge arises from interactions between modality-specific 'spokes' (sensory/motor/emotional cortical areas) and an amodal 'hub' (anterior temporal lobe) that integrates these modality-specific features into coherent, generalizable concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The neural and computational bases of semantic cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Hub-and-spoke theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hybrid distributed + amodal hub (relational/integrative network)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented by distributed modality-specific feature representations ('spokes') whose patterns of co-activation are integrated by a central, a-modal hub (anterior temporal lobe) that forms abstract, generalizable conceptual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains generalization across modalities and contexts, supports abstraction beyond modality-specific features, enables linking of co-occurring sensory/motor/affective features into coherent concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>neuropsychological/lesion studies and fMRI (cited reviews and empirical papers)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>semantic tasks, lesion-deficit correlation, fMRI semantic tasks (e.g., semantic feature verification, category tasks, multi-modal stimulus presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>The paper cites the hub-and-spoke account as the prevailing hypothesis: modality-specific cortical regions (spokes) interact with an a-modal hub (ATL) to form conceptual knowledge; this organization supports generalization and cross-modal semantic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>The present paper notes that the exact mechanisms and topology remain incompletely elucidated and that their analysis only used visual cortices (VC), so they do not directly test hub contributions (e.g., ATL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Ralph et al., 2017</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6896.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6896.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Continuous semantic space</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous semantic space (brain-wide semantic maps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The view that conceptual knowledge is organized as a continuous, high-dimensional semantic space mapped across cortex, where similar concepts occupy nearby positions and cortical voxels respond systematically across that space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Continuous semantic space</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>high-dimensional feature/embedding space</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented continuously as points or vectors in a high-dimensional semantic space distributed across cortex; population responses vary smoothly across this space so that semantic similarity corresponds to neural response similarity/topography.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Captures graded similarity relations among thousands of object/action categories, explains distributed activation patterns across cortex and ability to generalize to novel categories.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>fMRI encoding/mapping studies (voxelwise modeling of naturalistic stimuli)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Natural vision / movie or large stimulus set presentation with voxelwise semantic feature modeling (e.g., natural movie regression and mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Cited evidence indicates that a continuous semantic space can describe representations of many object and action categories across human cortex, supporting the idea of graded, distributed semantic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Not discussed in detail in this paper; limitations include incomplete characterization of topology and possible attention/state modulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Huth et al., 2012</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6896.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6896.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN-latent-homeomorphism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis: brain visual semantic features are homeomorphic to CNN latent space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's central hypothesis that visual cortical activity projects visual information into a latent semantic space that is homeomorphic (structurally similar) to the feature space of the last convolutional layer of a CNN (ResNet50), enabling linear mapping from fMRI to CNN features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>CNN-latent-space homeomorphism hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature‑based vector / high-dimensional space</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally, the visual cortex encodes semantic features of images in a high-dimensional latent space whose geometry and semantic clustering are similar to those learned by deep CNN classifiers; therefore a linear mapping can translate fMRI patterns into CNN latent vectors that reflect conceptual content.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Predicts that a linear transformation from VC fMRI to CNN latent vectors is sufficient to recover semantic features of seen images, enabling generalization across categories and conditioning of generative models to reconstruct semantically similar images.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>fMRI decoding experiment (this paper): computational mapping (ridge regression), semantic evaluation metrics, human behavioral evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Image-presentation fMRI (GOD dataset); train ridge regression mapping VC voxels to ResNet50 last-conv features on 1,200 training images; predict features for test images and evaluate semantic similarity (Wu-Palmer), FID, and human choice tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>A linear brain-to-feature (ridge) model mapping VC fMRI to ResNet50 features produced reconstructions that matched semantic content: Wu-Palmer similarity ~0.57 (test), FID ~10.58, and humans preferred model-generated images over random in ~81% of test trials, supporting the functional viability of the mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Authors note limitations: only visual cortices were used (no ATL), small dataset and SNR constraints, possible bias in ResNet50 feature space, and that results do not prove true isomorphism (only functional utility of the mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Ferrante, Boccato, Passamonti, Toschi (paper provided; year not specified)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6896.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6896.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear brain-to-feature mapping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear mapping (ridge regression) from fMRI to CNN feature vectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical model used in this paper: a regularized linear regression (ridge) that maps preprocessed visual-cortex fMRI data to high-level CNN feature vectors (last ResNet50 convolutional layer) to recover semantic image features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Linear mapping hypothesis (ridge regression)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>linear transform in high-dimensional feature space</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally posits that the relationship between VC population activity and semantic feature vectors can be approximated by a linear mapping (with L2 regularization), allowing fMRI-derived signals to predict CNN latent features corresponding to conceptual content.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>A simple linear mapping is sufficient to recover semantic-level features from noisy fMRI signals, enabling downstream semantic classification and conditioning of generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>fMRI decoding experiment (this paper) with quantitative semantic and human-evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Train/test supervised decoding on GOD dataset: fit ridge regression from VC voxel patterns to ResNet50 features using 1,200 training images; predict features for held-out test images presented many times to improve SNR.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Ridge regression produced feature predictions whose nearest neighbors in ResNet50 latent space yielded labels that, when used to condition a latent diffusion model, generated images judged semantically similar by experts and naïve human raters; quantitative Wu-Palmer similarity on the test set ~0.571 and human selection ~81%.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Authors caution that success does not imply an exact brain-CNN isomorphism; mapping quality depends on SNR, dataset overlap, and biases in CNN feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Ferrante, Boccato, Passamonti, Toschi (paper provided; year not specified)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6896.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6896.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-down / predictive processing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-down (predictive) processing / brain-as-prediction-machine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional conceptual framework asserting that perception is the result of both bottom-up sensory inputs and top-down predictions (priors) that shape internal representations, yielding rapid initial estimates refined by contextual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Brain States: Top-Down Influences in Sensory Processing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Predictive/top-down processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hierarchical predictive processing (probabilistic/predictive)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Perception arises from iterative interactions between ascending sensory evidence (bottom-up) and descending predictions/priors (top-down) within a hierarchical system; top-down priors bias sensory representations toward contextually plausible interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains rapid initial estimates, context-dependent disambiguation, perceptual biases, and how prior knowledge shapes semantic representation and recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>reviewed neurophysiology and fMRI studies; theoretical and empirical work (cited reviews and experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Various: sensory discrimination, contextual modulation, top-down attention tasks, fMRI/EEG demonstrating feedback influences on early sensory areas.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>The paper uses this framework to motivate a two-stage decoding pipeline: a bottom-up brain-to-feature estimate (linear mapping) followed by a top-down nearest-neighbor selection in CNN latent space to impose priors from learned image statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Not challenged in this paper; authors note practical limitations (SNR, dataset size) that could confound interpretations of top-down effects in their decoding pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Gilbert & Sigman, 2007</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6896.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6896.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention-modulated semantic space</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention warps semantic representation across the human brain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The claim that attentional state modulates the structure and mapping of semantic representations in cortex, effectively warping the semantic space depending on task demands and focus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention during natural vision warps semantic representation across the human brain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Attention-modulated semantic representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dynamic/high-dimensional feature space (state-dependent modulation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Semantic representations in the brain are not fixed: attention selectively amplifies or reshapes representation along semantic dimensions, thereby altering distances and topology in the semantic space depending on behavioral goals.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains context- and attention-dependent variability in semantic encoding and decoding accuracy; predicts that attentional state will change how well semantic features can be recovered from fMRI.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>fMRI study (natural vision with attentional manipulations)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Natural vision with directed attention tasks and voxelwise semantic modeling to assess changes in semantic tuning under attention</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Cited evidence shows attention changes semantic representations across cortex (i.e., 'warps' semantic space), which the present paper notes as a potential source of variability for decoding (e.g., subject fatigue/boredom altering semantic encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>The present paper does not present counter-evidence but highlights attention as a confound/factor to consider in fMRI decoding work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Çukur et al., 2013</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain <em>(Rating: 2)</em></li>
                <li>The neural and computational bases of semantic cognition <em>(Rating: 2)</em></li>
                <li>Generic decoding of seen and imagined objects using hierarchical visual features <em>(Rating: 2)</em></li>
                <li>Attention during natural vision warps semantic representation across the human brain <em>(Rating: 2)</em></li>
                <li>Brain States: Top-Down Influences in Sensory Processing <em>(Rating: 1)</em></li>
                <li>Deep language algorithms predict semantic comprehension from brain activity <em>(Rating: 1)</em></li>
                <li>Shared computational principles for language processing in humans and deep language models <em>(Rating: 1)</em></li>
                <li>High-resolution image reconstruction with latent diffusion models from human brain activity <em>(Rating: 2)</em></li>
                <li>Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6896",
    "paper_id": "paper-254591185",
    "extraction_schema_id": "extraction-schema-133",
    "extracted_data": [
        {
            "name_short": "Hub-and-spoke",
            "name_full": "Hub-and-spoke theory of semantic representation",
            "brief_description": "A neurocognitive theory proposing that conceptual knowledge arises from interactions between modality-specific 'spokes' (sensory/motor/emotional cortical areas) and an amodal 'hub' (anterior temporal lobe) that integrates these modality-specific features into coherent, generalizable concepts.",
            "citation_title": "The neural and computational bases of semantic cognition",
            "mention_or_use": "mention",
            "theory_name": "Hub-and-spoke theory",
            "theory_type": "hybrid distributed + amodal hub (relational/integrative network)",
            "theory_description": "Concepts are represented by distributed modality-specific feature representations ('spokes') whose patterns of co-activation are integrated by a central, a-modal hub (anterior temporal lobe) that forms abstract, generalizable conceptual representations.",
            "functional_claims": "Explains generalization across modalities and contexts, supports abstraction beyond modality-specific features, enables linking of co-occurring sensory/motor/affective features into coherent concepts.",
            "evidence_source": "neuropsychological/lesion studies and fMRI (cited reviews and empirical papers)",
            "experimental_paradigm": "semantic tasks, lesion-deficit correlation, fMRI semantic tasks (e.g., semantic feature verification, category tasks, multi-modal stimulus presentation)",
            "key_result": "The paper cites the hub-and-spoke account as the prevailing hypothesis: modality-specific cortical regions (spokes) interact with an a-modal hub (ATL) to form conceptual knowledge; this organization supports generalization and cross-modal semantic behavior.",
            "supports_theory": true,
            "counter_evidence": "The present paper notes that the exact mechanisms and topology remain incompletely elucidated and that their analysis only used visual cortices (VC), so they do not directly test hub contributions (e.g., ATL).",
            "citation": "Ralph et al., 2017",
            "uuid": "e6896.0"
        },
        {
            "name_short": "Continuous semantic space",
            "name_full": "Continuous semantic space (brain-wide semantic maps)",
            "brief_description": "The view that conceptual knowledge is organized as a continuous, high-dimensional semantic space mapped across cortex, where similar concepts occupy nearby positions and cortical voxels respond systematically across that space.",
            "citation_title": "A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain",
            "mention_or_use": "mention",
            "theory_name": "Continuous semantic space",
            "theory_type": "high-dimensional feature/embedding space",
            "theory_description": "Concepts are represented continuously as points or vectors in a high-dimensional semantic space distributed across cortex; population responses vary smoothly across this space so that semantic similarity corresponds to neural response similarity/topography.",
            "functional_claims": "Captures graded similarity relations among thousands of object/action categories, explains distributed activation patterns across cortex and ability to generalize to novel categories.",
            "evidence_source": "fMRI encoding/mapping studies (voxelwise modeling of naturalistic stimuli)",
            "experimental_paradigm": "Natural vision / movie or large stimulus set presentation with voxelwise semantic feature modeling (e.g., natural movie regression and mapping)",
            "key_result": "Cited evidence indicates that a continuous semantic space can describe representations of many object and action categories across human cortex, supporting the idea of graded, distributed semantic representations.",
            "supports_theory": true,
            "counter_evidence": "Not discussed in detail in this paper; limitations include incomplete characterization of topology and possible attention/state modulation.",
            "citation": "Huth et al., 2012",
            "uuid": "e6896.1"
        },
        {
            "name_short": "CNN-latent-homeomorphism",
            "name_full": "Hypothesis: brain visual semantic features are homeomorphic to CNN latent space",
            "brief_description": "The paper's central hypothesis that visual cortical activity projects visual information into a latent semantic space that is homeomorphic (structurally similar) to the feature space of the last convolutional layer of a CNN (ResNet50), enabling linear mapping from fMRI to CNN features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "CNN-latent-space homeomorphism hypothesis",
            "theory_type": "feature‑based vector / high-dimensional space",
            "theory_description": "Functionally, the visual cortex encodes semantic features of images in a high-dimensional latent space whose geometry and semantic clustering are similar to those learned by deep CNN classifiers; therefore a linear mapping can translate fMRI patterns into CNN latent vectors that reflect conceptual content.",
            "functional_claims": "Predicts that a linear transformation from VC fMRI to CNN latent vectors is sufficient to recover semantic features of seen images, enabling generalization across categories and conditioning of generative models to reconstruct semantically similar images.",
            "evidence_source": "fMRI decoding experiment (this paper): computational mapping (ridge regression), semantic evaluation metrics, human behavioral evaluation",
            "experimental_paradigm": "Image-presentation fMRI (GOD dataset); train ridge regression mapping VC voxels to ResNet50 last-conv features on 1,200 training images; predict features for test images and evaluate semantic similarity (Wu-Palmer), FID, and human choice tasks.",
            "key_result": "A linear brain-to-feature (ridge) model mapping VC fMRI to ResNet50 features produced reconstructions that matched semantic content: Wu-Palmer similarity ~0.57 (test), FID ~10.58, and humans preferred model-generated images over random in ~81% of test trials, supporting the functional viability of the mapping.",
            "supports_theory": true,
            "counter_evidence": "Authors note limitations: only visual cortices were used (no ATL), small dataset and SNR constraints, possible bias in ResNet50 feature space, and that results do not prove true isomorphism (only functional utility of the mapping).",
            "citation": "Ferrante, Boccato, Passamonti, Toschi (paper provided; year not specified)",
            "uuid": "e6896.2"
        },
        {
            "name_short": "Linear brain-to-feature mapping",
            "name_full": "Linear mapping (ridge regression) from fMRI to CNN feature vectors",
            "brief_description": "An empirical model used in this paper: a regularized linear regression (ridge) that maps preprocessed visual-cortex fMRI data to high-level CNN feature vectors (last ResNet50 convolutional layer) to recover semantic image features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "Linear mapping hypothesis (ridge regression)",
            "theory_type": "linear transform in high-dimensional feature space",
            "theory_description": "Functionally posits that the relationship between VC population activity and semantic feature vectors can be approximated by a linear mapping (with L2 regularization), allowing fMRI-derived signals to predict CNN latent features corresponding to conceptual content.",
            "functional_claims": "A simple linear mapping is sufficient to recover semantic-level features from noisy fMRI signals, enabling downstream semantic classification and conditioning of generative models.",
            "evidence_source": "fMRI decoding experiment (this paper) with quantitative semantic and human-evaluation metrics",
            "experimental_paradigm": "Train/test supervised decoding on GOD dataset: fit ridge regression from VC voxel patterns to ResNet50 features using 1,200 training images; predict features for held-out test images presented many times to improve SNR.",
            "key_result": "Ridge regression produced feature predictions whose nearest neighbors in ResNet50 latent space yielded labels that, when used to condition a latent diffusion model, generated images judged semantically similar by experts and naïve human raters; quantitative Wu-Palmer similarity on the test set ~0.571 and human selection ~81%.",
            "supports_theory": true,
            "counter_evidence": "Authors caution that success does not imply an exact brain-CNN isomorphism; mapping quality depends on SNR, dataset overlap, and biases in CNN feature space.",
            "citation": "Ferrante, Boccato, Passamonti, Toschi (paper provided; year not specified)",
            "uuid": "e6896.3"
        },
        {
            "name_short": "Top-down / predictive processing",
            "name_full": "Top-down (predictive) processing / brain-as-prediction-machine",
            "brief_description": "A functional conceptual framework asserting that perception is the result of both bottom-up sensory inputs and top-down predictions (priors) that shape internal representations, yielding rapid initial estimates refined by contextual knowledge.",
            "citation_title": "Brain States: Top-Down Influences in Sensory Processing",
            "mention_or_use": "mention",
            "theory_name": "Predictive/top-down processing",
            "theory_type": "hierarchical predictive processing (probabilistic/predictive)",
            "theory_description": "Perception arises from iterative interactions between ascending sensory evidence (bottom-up) and descending predictions/priors (top-down) within a hierarchical system; top-down priors bias sensory representations toward contextually plausible interpretations.",
            "functional_claims": "Explains rapid initial estimates, context-dependent disambiguation, perceptual biases, and how prior knowledge shapes semantic representation and recognition.",
            "evidence_source": "reviewed neurophysiology and fMRI studies; theoretical and empirical work (cited reviews and experiments)",
            "experimental_paradigm": "Various: sensory discrimination, contextual modulation, top-down attention tasks, fMRI/EEG demonstrating feedback influences on early sensory areas.",
            "key_result": "The paper uses this framework to motivate a two-stage decoding pipeline: a bottom-up brain-to-feature estimate (linear mapping) followed by a top-down nearest-neighbor selection in CNN latent space to impose priors from learned image statistics.",
            "supports_theory": true,
            "counter_evidence": "Not challenged in this paper; authors note practical limitations (SNR, dataset size) that could confound interpretations of top-down effects in their decoding pipeline.",
            "citation": "Gilbert & Sigman, 2007",
            "uuid": "e6896.4"
        },
        {
            "name_short": "Attention-modulated semantic space",
            "name_full": "Attention warps semantic representation across the human brain",
            "brief_description": "The claim that attentional state modulates the structure and mapping of semantic representations in cortex, effectively warping the semantic space depending on task demands and focus.",
            "citation_title": "Attention during natural vision warps semantic representation across the human brain",
            "mention_or_use": "mention",
            "theory_name": "Attention-modulated semantic representation",
            "theory_type": "dynamic/high-dimensional feature space (state-dependent modulation)",
            "theory_description": "Semantic representations in the brain are not fixed: attention selectively amplifies or reshapes representation along semantic dimensions, thereby altering distances and topology in the semantic space depending on behavioral goals.",
            "functional_claims": "Explains context- and attention-dependent variability in semantic encoding and decoding accuracy; predicts that attentional state will change how well semantic features can be recovered from fMRI.",
            "evidence_source": "fMRI study (natural vision with attentional manipulations)",
            "experimental_paradigm": "Natural vision with directed attention tasks and voxelwise semantic modeling to assess changes in semantic tuning under attention",
            "key_result": "Cited evidence shows attention changes semantic representations across cortex (i.e., 'warps' semantic space), which the present paper notes as a potential source of variability for decoding (e.g., subject fatigue/boredom altering semantic encoding).",
            "supports_theory": true,
            "counter_evidence": "The present paper does not present counter-evidence but highlights attention as a confound/factor to consider in fMRI decoding work.",
            "citation": "Çukur et al., 2013",
            "uuid": "e6896.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain",
            "rating": 2,
            "sanitized_title": "a_continuous_semantic_space_describes_the_representation_of_thousands_of_object_and_action_categories_across_the_human_brain"
        },
        {
            "paper_title": "The neural and computational bases of semantic cognition",
            "rating": 2,
            "sanitized_title": "the_neural_and_computational_bases_of_semantic_cognition"
        },
        {
            "paper_title": "Generic decoding of seen and imagined objects using hierarchical visual features",
            "rating": 2,
            "sanitized_title": "generic_decoding_of_seen_and_imagined_objects_using_hierarchical_visual_features"
        },
        {
            "paper_title": "Attention during natural vision warps semantic representation across the human brain",
            "rating": 2,
            "sanitized_title": "attention_during_natural_vision_warps_semantic_representation_across_the_human_brain"
        },
        {
            "paper_title": "Brain States: Top-Down Influences in Sensory Processing",
            "rating": 1,
            "sanitized_title": "brain_states_topdown_influences_in_sensory_processing"
        },
        {
            "paper_title": "Deep language algorithms predict semantic comprehension from brain activity",
            "rating": 1,
            "sanitized_title": "deep_language_algorithms_predict_semantic_comprehension_from_brain_activity"
        },
        {
            "paper_title": "Shared computational principles for language processing in humans and deep language models",
            "rating": 1,
            "sanitized_title": "shared_computational_principles_for_language_processing_in_humans_and_deep_language_models"
        },
        {
            "paper_title": "High-resolution image reconstruction with latent diffusion models from human brain activity",
            "rating": 2,
            "sanitized_title": "highresolution_image_reconstruction_with_latent_diffusion_models_from_human_brain_activity"
        },
        {
            "paper_title": "Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion",
            "rating": 2,
            "sanitized_title": "braindiffuser_natural_scene_reconstruction_from_fmri_signals_using_generative_latent_diffusion"
        }
    ],
    "cost": 0.01378,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Semantic Brain Decoding: from fMRI to conceptually similar image reconstruction of visual stimuli</p>
<p>Matteo Ferrante matteo.ferrante@uniroma2.it 
Department of Biomedicine and Prevention
Tor Vergata (IT)
University of Rome</p>
<p>Tommaso Boccato tommaso.boccato@uniroma2.it 
Department of Biomedicine and Prevention
Tor Vergata (IT)
University of Rome</p>
<p>Luca Passamonti luca.passamonti@cnr.it 
Istituto di Bioimmagini e Fisiologia Molecolare
CNR
Milano</p>
<p>Nicola Toschi nicola.toschi@uniroma2.it 
Department of Biomedicine and Prevention
Tor Vergata (IT)
University of Rome</p>
<p>Martinos Center for Biomedical Imaging, MGH and Harvard Medical School</p>
<p>Semantic Brain Decoding: from fMRI to conceptually similar image reconstruction of visual stimuli
visual stimuli reconstruction · fMRI decoding · semantic reconstruction · brain decoding
Brain decoding is a field of computational neuroscience that uses measurable brain activity to infer mental states or internal representations of perceptual inputs. We propose a novel approach to brain decoding that relies on semantic and contextual similarity. We employ an fMRI dataset of where vision of natural images was employed as stimuli and create a deep learning decoding pipeline inspired by the existence of both bottom-up and top-down processes in human vision. We train a linear brain-to-feature model to map fMRI activity features to visual stimuli features, assuming that the brain projects visual information onto a space that is homeomorphic to the latent space represented by the last convolutional layer of a pretrained convolutional neural network, which typically collects a variety of semantic features that summarize and highlight similarities and differences between concepts. These features are then categorized in the latent space using a nearest-neighbor strategy, and the results are used to condition a generative latent diffusion model to create novel images. From fMRI data only, we produce reconstructions of visual stimuli that match the original content very well, surpassing the state of the art in previous literature. We evaluate our work and obtain good results using a quantitative semantic metric (Wu-Palmer similarity metric over the WordNet lexicon, average = 0.57). We also perform a human evaluation experiment intended to reproduce the multiplicity of conscious and unconscious criteria that humans use to evaluate image similarity. This resulted in correct evaluation in over 80% of the test set.</p>
<p>Introduction</p>
<p>Background and Motivation</p>
<p>Brain decoding attempts to infer internal representations of perceptual stimuli from measurable brain activity. Isolated attempts have been made to use deep Our proposed architecture. According to our hypothesis, the brain processes information by extracting visual features from images and projecting them onto a latent semantic space similar to the one formed by a convolutional neural network (CNN), termed "classifier" in this figure (in this paper we employed the ResNet50 architecture) when trained for object categorization. We developed a regression model that maps fMRI brain data to the CNN's latent space and used a k-nearest-neighbor (kNN) method to predict the related classes. Finally, we conditioned a latent diffusion model to generate novel images that are semantically similar to the visual stimuli from the predicted classes.</p>
<p>learning to 1) identify complex brain data patterns and 2) reconstruct the stimuli that have generated such patterns using noninvasive neuromonitoring data such as functional magnetic resonance imaging (fMRI) or electroencephalography (EEG) [31]. While these activities are in very early stages, they also carry great promise for the development of novel strategies to diagnose and treat neurological or neuropsychiatric conditions. However, such endeavors carry many challenges. Noninvasive data, for example, have lower temporal or spatial resolution than that of neural firing, resulting in a potential upper limit on the granularity of information that may be retrieved. The latter is also degraded by physiological noise and signal/image artifacts.</p>
<p>Visual Cortex and Pathways</p>
<p>Vision has been extensively studied along with its brain representations (i.e., the visual cortex). The latter are organized hierarchically into sections that respond to specific stimuli (commonly termed V1, V2, V3, V4, and the lower and upper visual cortices). Simple visual inputs tend to elicit V1 responses, while V2 responds to texture, color, and more complex outlines. There is strong evidence that information flows from the visual cortex (VC) to the rest of the brain through two separate routes, the what and where pathways [2,27,12,11]. The what pathway connects the VC to the inferior temporal lobe (IT) and is involved in object recognition, whereas the where pathway connects the VC to the parietal lobe and is primarily involved in movement and position recognition.</p>
<p>Semantic Representations in the Brain</p>
<p>In vision, the bottom-up information extraction described above is accompanied by a top-down mechanism [9] where semantic prior knowledge of the world is exploited to create internal representations of external stimuli. This results in a combination of context-given prediction and purely external signals relayed from the retina to the brain. According to the 'hub-and-spoke' theory of semantic representation, conceptual knowledge arises from the progressive learning of the statistical regularities of our multi-sensorial experiences. In other words, we learn how to recognize an ever-changing environment by systematically linking apparently separate aspects of our experiences (e.g., color, motion, sounds, sensory-motor actions associated with an object, etc.) that tend to co-occur. Such learning processes transform a sensory 'cacophony' into a coherent, contextspecific, and behaviorally-relevant semantic representation of the stimuli.</p>
<p>Semantic Cognition and Modality-Specific Brain Areas</p>
<p>The brain mechanisms underlying semantic cognition have not been fully elucidated, but the prevailing hypothesis suggests that modality-specific brain areas, also known as the 'spokes' (e.g., visual cortices, auditory cortices, motor areas, emotional systems), interact via a central and a-modal 'hub' region (the anterior temporal lobe) to form conceptual knowledge. This process shapes the semantic representation through various experiences, such as visual, auditory, verbal, and tactile, and critically promotes the ability to generalize across different items and variable contexts. Interestingly, there are indications of the existence of a continuous semantic space representation [16] in the human brain. Though the structure and topology of this putative semantic space have been poorly investigated, there is evidence that fMRI data from occipital brain regions collected during a visual task can be linked to features learned by a convolutional neural network (CNN) [17], with a particular focus on the early and middle CNN layers.</p>
<p>Decoding Visual Stimuli from fMRI Data</p>
<p>In this paper, we tackle the problem of decoding (i.e., reconstructing) visual stimuli (images) from fMRI data only, by proposing the hypothesis that deep convolutional layers can operate as a proxy for parts of the brain that extract semantic features from images. We propose a cascade of deep learning models that builds convincing semantic reconstructions of the stimulus presented at acquisition time. It is important to note that the aim of this paper is not to create exact reconstructions of the images presented under fMRI. Instead, our objectives are to either a) generate realistic visual representations that capture the main concepts contained in the original stimulus, or b) create synthetic images that can trigger similar brain activity when employed as stimuli. Achieving either of these results can pave the way for a more general understanding of cognitive-visual information storage and retrieval.</p>
<p>Related Work</p>
<p>Reconstructing Information from fMRI Data</p>
<p>In recent years, several attempts have been made to reconstruct information from noninvasively acquired brain data, particularly fMRI data. This has been fueled by the increasing availability of public datasets, advances in computational power, and more sophisticated nonlinear analytic approaches, such as deep neural networks. While challenges related to signal-to-noise ratio (SNR), duration of acquisition session, and HRF variability remain, fMRI appears capable of extracting useful information in a wide range of situations and tasks, including vision and visual stimulus classification.</p>
<p>Existing Approaches and Challenges</p>
<p>Various modeling frameworks have been employed in brain decoding literature, where the input is usually preprocessed fMRI time series. These data are referred to as "fMRI data", "fMRI patterns", and "fMRI activations", terms used interchangeably in this paper. Existing approaches to brain decoding include:</p>
<p>-Variational autoencoder with a generative adversarial component (VAE-GAN) for encoding latent representations of human faces [29]. -Sparse linear regression over preprocessed fMRI data for predicting features extracted by multiple early convolutional layers from a pretrained CNN [15]. -An adversarial strategy employing a generator and discriminator to differentiate between real and reconstructed images, further improved by a perceptual loss and a comparator network [25]. -A dual VAEGAN consisting of two linked variational autoencoders for representing both stimuli and fMRI patterns [24]. -An unsupervised technique using two encoders and two decoders learning separately how to reconstruct fMRI data and stimuli, bound by a supervised loss [8]. -Optimizing pretrained architectures' latent spaces, such as BigBiGAN [19] and IC-GAN [20], to reconstruct high-quality images from fMRI patterns. - [5] performed a direct estimation of the latent space of a latent diffusion model from fMRI data, employing a pre-trained autoencoder to reduce the dimensionality of fMRI representations. By combining the HCP [28] (1,200 subjects) and GOD datasets, they achieved a substantial sample size to learn self-supervised representations and fine-tune them for inferring the latent representations of images with limited labeled pairs. Our work is closely related due to the utilization of the same GOD dataset and latent diffusion models for image reconstruction; however, our main distinction lies in the development of an ad-hoc pipeline to address the small sample size of the GOD dataset independently, whereas they relied on external fMRI acquisitions to learn self-supervised representations. -Latent diffusion models have been recently employed as image generators in [26] and [21], where the authors utilized the Natural Scenes Dataset [1], containing 70,000 images acquired with a 7T scanner. This extensive dataset significantly enhances the quality and quantity of input data for brain decoding tasks. In the first study, the authors directly optimized the latent space of the diffusion model, while in the second, an initial guess image reconstruction was obtained by mapping fMRI data into the latent space of a deep variational autoencoder trained for image reconstruction. These initial guesses encompass information about shape, color, and pose of images, and can be combined with predicted conditioning in latent diffusion models through image-to-image pipelines to improve reconstruction quality. An important distinction between our work and these studies is that our dataset's sample size is nearly two orders of magnitude smaller, and was acquired on a 3T rather than a 7T acquisition. This implies that the performances of our model have been obtained with a small fraction of the information and of the signal-to-noise ratio available to other models.</p>
<p>Focus on Semantic Content</p>
<p>Most of the research in brain decoding has focused on extracting either low-level visual stimulus characteristics or reconstructing whole images in pixel space. While these studies capture forms, colors, or images that look similar to the original stimuli, reconstructions are often blurred and mix elements from unrelated concepts. In this paper, we focus on context, i.e., the semantic content of presented stimuli, with the aim of reconstructing images that resemble the original ones and can elicit the same fMRI activity. We hypothesize that this approach may add ecological relevance to our findings in terms of future applications for understanding visual information representation in the brain.</p>
<p>Methods</p>
<p>In this section, we describe the implementation aspects of our study. We used Python 3.9 along with the PyTorch and scikit-learn libraries to develop our models. The experiments were conducted on a server equipped with two Intel Xeon Gold processors, 512 GB RAM, and an NVIDIA A6000 GPU with 48 GB RAM. Our code is available at https://github.com/matteoferrante/sema ntic-brain-decoding, and the preprocessed data can be accessed at https: //figshare.com/articles/dataset/Generic Object Decoding/7387130. Unprocessed fMRI data is available at https://openneuro.org/datasets/ds0 01246/versions/1.2.1.</p>
<p>Data and Preprocessing</p>
<p>We utilize the publicly accessible Generic Object Decoding (GOD) dataset [15], which comprises fMRI data from 5 subjects who participated in either an image presentation experiment or an imagery experiment. The GOD dataset has been instrumental in developing previous brain decoding models and is emerging as a valuable benchmark for decoding visual stimuli from fMRI data. All visual stimuli in the GOD dataset originate from the ImageNet database (http:// www.image-net.org/, Fall 2011 release), which is categorized into various classes, including animals (e.g., "goldfish," "swarm," and "tiger") and objects (e.g., "airplane," "hat," or "knife"). The image presentation experiment involved separate training and test sessions. In the training session, 1,200 images from 150 object categories (8 images per category) were presented once. In the test session, 50 images from 50 object categories (1 image per category) were shown 35 times each. Each stimulus was displayed for nine seconds. No overlap existed between the categories of training and test images. In this dataset, a single fMRI acquisition is called a "run," with 24 runs for training images and 35 runs for testing images performed for each subject. The fMRI protocol was based on an EPI sequence with T R = 3000 ms, T E = 30 ms, flip angle=80, and a voxel size of 3 mm 3 .</p>
<p>Data were preprocessed in native subject space by performing 3D motion correction, linear trend removal, and coregistration to a high-resolution common anatomical template. Reference masks for the visual cortex (VC) and several other brain areas, such as the face fusiform area (FFA), the high VC (HVC), and the low VC (LVC), were provided for each subject. In this study, we used data extracted from the VC (approximately 4,500 voxels per subject) as our input space. The data were normalized runwise, ensuring each voxel-specific timeseries had a zero mean and unit variance. Subsequently, data were averaged over time using nonoverlapping 9-second windows and effectively shifted forward by 3 seconds (i.e., three volumes per average, corresponding to the length of a stimulus presentation). This process helped reduce complexity and account for delays induced by the hemodynamic response function (HRF) convolution.</p>
<p>Subject-specific Brain Activity Models</p>
<p>We developed individual models for each subject to decode their brain activity, as intersubject functional variability could be greater than the impact we aim to extract. Our hypothesis is that the brain processes sensory input in the VC to extract relevant features from images for object recognition, employing a hierarchical approach similar to convolutional neural networks (CNNs).</p>
<p>We propose a linear mapping between processed fMRI data and the last convolutional layer of the ResNet50 [13] architecture, trained on the ImageNet dataset. The objective is to find the optimal weights W that minimize the regularized loss described in Eq. (1):
min(|W x(s) − f (s)| 2 + λ|W | 2 )(1)
Here, s represents the image/stimulus presented during the experiment, f is the neural network that projects s into the latent space, and x(s) is the preprocessed brain activity associated with viewing the stimulus. W maps fMRI data into image features in the latent space generated by ResNet50. λ is a hyperparameter for L2 regularization on the weights. We optimized λ using a 90 − 10 training/validation split and grid search.</p>
<p>Subsequently, we generated the conditioning for the generative model that synthesizes the final output. We used ResNet50 to compute the latent representation of a subset of 500K randomly selected ImageNet pictures and stored their latent representation and ground truth labels. From the image features h = W x(s) predicted from brain activity, we identified the five nearest neighbors in the latent space and used their labels as candidates for classification.</p>
<p>This strategy accounts for the poor signal-to-noise ratio in fMRI data and the limited dataset size. Assuming that similar semantic concepts lead to similar features within the ResNet50 latent space, the features generated by our brainto-features model (ridge regression) are likely to be close to concepts semantically close to the target.</p>
<p>Bottom-up and Top-down Processes</p>
<p>We now discuss the combination of predicted features that simulate the bottomup process in vision (where the brain computes stimuli) and the use of a nearestneighbor-based algorithm to mimic top-down connections that modulate the signal we perceive according to our knowledge of the world. We also address the domain adaptation technique employed to predict the test set features from brain activity, the use of latent diffusion models as image generators, and the evaluation of semantic content through metrics such as the Wu-Palmer distance. In this study, we combine predicted features to simulate the bottom-up process in vision, where the brain computes stimuli, while using a nearest-neighbor-based algorithm to mimic top-down connections that modulate the signal we perceive according to our knowledge of the world [14,7].</p>
<p>Domain Adaptation Technique</p>
<p>There is no overlap between training and test categories in the GOD dataset, and test images are displayed numerous times to achieve a higher SNR. Since the brain-to-feature model is trained using training data, we employ a simple domain adaptation technique to predict test set features from brain activity, which involves replacing the mean and standard deviation of predicted features from the test set with those from the training set [14,7].</p>
<p>Latent Diffusion Models as Image Generators</p>
<p>To generate images (i.e., reconstruct visual stimuli), we rely on a powerful, recent pretrained image generator belonging to the family of denoising probabilistic diffusion models [14]. Diffusion models are generative architectures that learn how to reverse a diffusion process, which in this context refers to the progressive addition of Gaussian noise to an image. This family of models is far more robust in training than other generative models, such as generative adversarial networks (GANs), and has greater mode coverage [7].</p>
<p>Evaluating Semantic Content</p>
<p>Our primary objective is to produce images that are close (in a semantic space) to the real visual stimuli shown to participants during the fMRI experiment. We create two metrics specifically designed to evaluate the quality of the generated images. First, we use the Wu-Palmer distance metric [22] between the real and predicted classes in the WordNet lexicon to estimate a quantifiable measure of semantic similarity. This is a well-established metric that measures the similarity of two different nodes (i.e., synsets) in the WordNet graph and can be computed as described in Eq (2), where s is the similarity metric, lcs stands for "least common subsumer" and is a function that returns the deepest common ancestor in the taxonomy between the two synsets s1, s2 and depth is a function that computes the depth in the graph. This metric is bounded in the interval [0, 1], where higher values mean that two synsets are more similar. A simplified graphical representation of the WordNet subgraph is shown in Fig. ?? along with some examples of Wu-Palmer distances.</p>
<p>Additionally, to quantify the performance of our model we used the Fréchet inception distance score (FID) [30]. In this metric, two sets of images (real and generated) are compared as multivariate Gaussian distributions in the feature space of a pretrained neural network (InceptionV3). While this metric mainly measures the quality of images both in terms of feature distribution and of visual quality, and thus is likely to mainly reflect the performance of our image generation model, the projection into the feature space of InceptionV3 is likely to also endow this metric with elements of semantic similarity. We used the torchmetrics [6] library implementation and compared the images generated for each subject with those used as stimuli. It should be noted that the main evaluation criterion used in brain decoding literature is a visual comparison between reconstructions of the same image across models.
s wup = depth(lcs(s1, s2)) depth(s1) + depth(s2)(2)
In addition to the Wu-Palmer distance metric, we conducted a human evaluation to assess the semantic similarity of the reconstructed images.</p>
<p>Human Evaluation of Image Reconstructions</p>
<p>We designed a human evaluation paradigm as follows. A local web page was created, which displayed the original image alongside five model-generated reconstructions in one row and five random reconstructions in another row (Fig.  2). Volunteers were instructed to examine the similarities between the images and select the row (first or second) that appeared closest to the original image. To minimize priming, the row positions were continuously randomized between "top" and "bottom."</p>
<p>Seven observers (5 males, 2 females, aged 25-33, with normal eyesight) participated in this evaluation, covering all subjects in the GOD dataset. Each observer assessed the 50 images in the test set and a common random subset of 50 images from the training set, resulting in a total of 350 evaluations. When performing this task, the human observers likely focused on various elements, including broad features like shapes and colors, as well as more semantically related aspects, such as "wild animals" or "furnishings." We believe this natural flexibility in judgment is relevant to our study, as the model utilizes features extracted by a classifier trained on the ImageNet dataset. These features can represent different levels of complexity based on the difficulty of the task, and similar comparison operations might be performed by our brains in everyday life. To further minimize priming, the row positions were continuously randomized between "top" and "bottom." Fig. 2. Example taken from the local human assessment local web page. The target image is presented on the left. The subject is instructed to assess the overall resemblance of the original stimulus (left) to the 5 images in the top and bottom rows on the right and to pick "TOP" or "BOTTOM " accordingly.</p>
<p>Results</p>
<p>Visual Comparison and Qualitative Results</p>
<p>The primary objective of our study is to generate images that are realistic reconstructions of visual inputs that semantically match the target image, which is the image used as a stimulus in the fMRI experiment. Fig. 3 presents a comparison with state-of-the-art reconstruction approaches over the same dataset, demonstrating qualitative differences between our approach and others. Our diffusion model generates images that are crisp and sharp and convey clear and specific content, which helps recognize similarities between images and distinguish between failed and successful semantic reconstructions.</p>
<p>We propose a paradigm shift in our approach to reconstruction. Rather than focusing on obtaining accurate reconstructions in pixel space, we aim to produce novel images that are semantically and contextually as close to the target visual stimulus as possible. For instance, reconstructions of "fish" and "airplane" (see Fig. 3, first and fourth rows, with the first column showing original images and the second column showing our reconstructions) are among our best results, as they clearly portray the same concepts as the original image. Other images that match the stimulus on a semantic level, such as the swan that is reconstructed as a parrot (both birds), the snowmobile that is reconstructed as a motorbike (both vehicles), or the colorful church window reconstructed as a church, are instances of visuals that match the content and context without being exact pixelwise reconstructions. Fig. 4 and the Supplementary Material show additional reconstruction examples for all subjects. One can see that our model provides a plausible reconstruction that matches the original at some contextual level in the majority of cases, although with a natural degree of variation that reflects the breadth of possible semantic similarities.</p>
<p>Quantitative semantic distance</p>
<p>We achieved a FID score of 10.58 ± 1.95 (mean ± standard deviation, test set) and an average Wu-Palmer distance of 0.811 ± 0.204 over the training set and 0.571 ± 0.157 over the test set (Fig ??). It is important to note that the images in the test set correspond to categories that do not overlap with those in the training set. Therefore, the quality of prediction in the test set is determined by the number of features shared by the two sets. However, there is a notable factor of similarity between original and generated images, even in the test dataset, suggesting that the brain-to-feature model can estimate semantic features related to groups of objects, such as wings, fur, and buildings, correctly. This result holds even though the model is trained on data with different categories and data distribution. In other words, our model performs well in spite of the non-overlap between training and test categories. While a simple classifier would likely not be able to generalize to this particular test set, our model performs well and demonstrates the potential for brain decoding to generalize to new categories and data distributions.</p>
<p>Human Evaluation</p>
<p>Humans perform well in complex assessments with wide criteria and can naturally examine images at numerous levels of semantic information as well as shapes, colors, and many more. Fig. 2 and Table 1 show the results of human evaluation for both the training and test sets. On average, human observers selected the images generated from the model (as opposed to the randomly generated images) in 95 ± 3% of the cases for images from the training set and in 81 ± 4% of the cases for images from the test set. In all cases, human observers chose the model-generated images far more frequently than what would have been the chance level, supporting the hypothesis that our computational approach can correctly capture various semantic features of the images in a manner that corresponds well to the way the human brain evaluates this type of content and context.   </p>
<p>Discussion</p>
<p>Developing the Brain-to-Feature Model and Reconstruction Pipeline</p>
<p>Grounded in the assumption that fMRI data from the VC during a visual task can be used as a proxy for the last layer of a convolutional neural network (CNN) trained for image classification, we developed a brain-to-feature model. This model is a trained ridge regression between fMRI and image features extracted from the original visual stimuli images through ResNet50, establishing univocal relationships between fMRI data and the ResNet50 features [17,18,16].</p>
<p>We subsequently employed a nearest neighbor-like technique to map these features into object "categories." These categories were then used to condition a pretrained latent diffusion model to produce novel images from text prompts corresponding to the synset name of the related WordNet class. Our reconstruction pipeline incorporates these hypotheses through the mapping between fMRI and ResNet50 latent space, the use of the k-nearest neighbors algorithm, and reliance on a powerful image generator.</p>
<p>Subject</p>
<p>Human </p>
<p>Bottom-Up and Top-Down Processes in Vision</p>
<p>Our brain-to-feature model represents the bottom-up process in vision, a rapid initial estimate of relevant features. This estimate is refined by our top-down approach, represented by the choice of the nearest neighbor in the latent space to condition the generative model. This component of our architecture is supported by prior knowledge of the world, contained in the ResNet50 latent space representation of a subset of the ImageNet database. This, in turn, allows us to evaluate the "distance" between concepts.</p>
<p>Evaluating Performance Through Semantic-Related Measures</p>
<p>We assessed our work both qualitatively (visually) and quantitatively through semantic-related measures. We employed the Wu-Palmer distance to analyze similarities between concepts in the WordNet lexicon, discovering a good average similarity. Additionally, we included an assessment of the contextual distance between original and reconstructed stimuli by naïve human observers to allow for additional flexibility and human-like semantic evaluation. Our results suggested that the model performed well in selecting relevant features and producing images closer to the original than any other image.</p>
<p>Reconstruction Performance and Categories</p>
<p>We found that with all assessment techniques, reconstructed images are rarely noticeably distant from the target, similar to the results reported in [16]. Specifically, original images of animals generated reconstructions that accurately depicted other animals, with striking accuracy in high-level features such as "species". Similarly, original images of non-animated objects, such as vehicles, exhibited comparable behavior, giving rise to accurate renderings of planes, motorbikes, tractors, and carriages. While a similar behavior occurred for most visual stimuli, some categories appeared to be "misunderstood" by our model, such as the cowboy hat or the guitar (see Supplementary Material). In this context, it is possible that the traits associated with certain test images are underrepresented in the training set, increasing the difficulty of capturing relevant semantics.</p>
<p>Brain and Deep Learning Models</p>
<p>Our brain can be thought of as a prediction machine that utilizes past knowledge in the form of top-down processing of external inputs. We found that in the VC, this might produce a feature space that is homeomorphic to the latent space of a CNN. Notably, a linear (ridge regression) model was sufficient to achieve convincing reconstruction results. These findings are in line with evidence that deep learning models and brain activity prompted by language converge [4,3,10,18] in terms of behavioral, physiological, and fMRI data, supporting our key hypothesis that context and semantics play a significant role in how we process sensory information. These ideas bear similarities to the concepts of attention-based deep learning models with convolutional layers.</p>
<p>Semantic Cognition and Reconstruction</p>
<p>Semantic cognition refers to a group of neuropsychological processes that sustain not only conceptual representation and formation but also the manipulation of semantic knowledge to influence context-relevant behavior. These brain mechanisms are thought to depend on a constant flow of top-down and bottom-up interactions between posterior and anterior areas, including occipito-temporal cortices and prefrontal networks. In the visual domain, the 'bottom-up' and 'top-down' interplay between multiple occipitotemporal cortices might allow the 'distillation' of a latent space of features that are believed to be at the 'core' of semantic representation. Our reconstruction approach, which utilized a combination of brain-to-feature and generative models, allowed us to recreate the original visual stimuli and obtain reconstructions of the images that surpass the state-of-the-art in the literature, particularly at the semantic level of reconstruction. This supports our approach's validity and its ability to mimic the way the human brain extracts, categorizes, and internally represents visually acquired information. We employed a deep latent diffusion model to generate novel images that could evoke similar brain activity, featuring images with congruent semantic content. This capacity to synthesize images with precise content directly from brain activity lays the foundation for more advanced analyses and reconstructions. For instance, utilizing an image-to-image diffusion model that starts with an initial guess containing low-level aspects such as colors and shapes can lead to more accurate and plausible reconstructions.</p>
<p>Neurobiological considerations</p>
<p>It is important to note that our deep learning architecture is conceptually inspired by the current understanding of the neural mechanisms underlying semantic cognition. However, our model only employed fMRI data from a group of visual cortices (V1, V2, V3, V4) due to practical and computational considerations [23]. This choice does not deny the critical role of other brain regions, such as the anterior temporal lobe (ATL), in semantic cognition. The spoke-hub theory of semantic cognition clearly states that semantic cognition arises from the interplay of modality-specific (sensory, motor cortices) and a-modal regions (ATL, prefrontal cortex, etc.) [?]. Future investigations could explore the role of other brain regions, such as the ATL, and determine whether the features extracted from those regions are superior to those of other regions of the brain when decoding the "mental states" associated with visual processing.</p>
<p>Limitations</p>
<p>The fMRI experiments used to collect the data were restricted in length because individuals need to be exposed to images slowly enough for the brain response to stabilize. As a result, the applicability of end-to-end deep learning algorithms is limited. In addition, because the categories in the training and test sets in the dataset we used do not overlap, the model's performance depends on the relationship between the fMRI data and image features in the training set. The assumption is that this relationship is sufficient to detect variations in unseen categories. Our model demonstrated good generalization capabilities, suggesting that semantic feature content, rather than precise train/test class overlap, may be predominant in determining performance. However, if the categories are highly dissimilar between the test and training set, it is conceivable that their essential properties are underrepresented in the training set, limiting the model's performance capabilities in the test . Also, it is important to note that we outperformed all models trained on the same 3T dataset, hence potentially widening the applicability of our methods to an extremely large number of centers wich do not have access to ultra-high-filed (7T or more) scanners. Furthermore, there are numerous potential sources of error that can arise between the vision process and the generation of the image feature space. These include fMRI acquisition noise, bias in the feature space of the ResNet50 architecture, bias introduced by the limited sample size in the brain-to-feature model, and errors introduced by the conditioning algorithm. These circumstances can be responsible for cases where the performance of our model in reconstructing context is poor.</p>
<p>Additionally, mental attention may warp the semantic space in the human brain [32]. When subjects become tired or bored during fMRI sessions, the encoded stimuli may change, introducing another source of variability that is not under experimental control. These limitations and sources of error should be taken into account when interpreting the results and considering future research directions.</p>
<p>Future directions</p>
<p>Future research could delve into the role of mental attention in semantic cognition and examine whether attentional states can modulate the distributed neural representations of semantic concepts in the visual cortex [32]. Such investigations would contribute to our understanding of how attention influences decoding accuracy and the neural mechanisms underlying semantic cognition. The growing availability of extensive open fMRI datasets will likely enable us to enhance brain decoding results using diffusion models as image generators, by conditioning these models in various ways. Interestingly, the majority of work in this field, including our own, currently focuses on subject-wise reconstruction. It would be intriguing to develop models capable of decoding intra-subject activity. This could pave the way for large-scale decoding on new subjects by merely fine-tuning a more extensive model, thus bypassing the need for lengthy fMRI acquisitions for each individual. Another crucial next step is decoding imagery activity, reconstructing examples of images seen exclusively by the mind's eye. Naturally, this raises ethical concerns regarding privacy and confidentiality, as decoding brain activity entails accessing an individual's internal mental state, potentially revealing sensitive information about their thoughts, emotions, and behavior. There is a risk that such information could be misused or disclosed without the person's consent, leading to privacy breaches. Ethical questions also arise concerning the accuracy of decoded images, which may produce a distorted version of a person's perception due to model imperfections. Nonetheless, this type of research can lead to numerous beneficial applications. For instance, a completely new form of art could emerge from the interaction between the physics of fMRI acquisition, the artist's thoughts and perceptions, and the artificial intelligence used for decoding. This technology could also enable individuals with locked-in syndrome to communicate through images.</p>
<p>Moreover, future investigations could employ other brain imaging modalities, such as EEG or MEG, to investigate the temporal dynamics of the neural representations of semantic concepts and how they evolve over time during visual processing. Additionally, future studies could employ multi-modal data fusion methods to combine fMRI data with other modalities, such as behavioral data or natural language descriptions of visual stimuli, to gain a more comprehensive understanding of the neural basis of semantic cognition.</p>
<p>Conclusions</p>
<p>Our study proposes a pipeline to synthesize images that are conceptually and semantically similar to the original stimuli, starting from fMRI data only. We assume that measurable neural correlates can be linearly mapped onto the latent space of a convolutional neural network that represents a semantic description of the image. The overall objective is to replicate the way humans process information by combining bottom-up visual inputs with top-down cognitive descriptions of the environment, which is known to aid in "classification" processes in the brain.</p>
<p>We evaluated our reconstructions qualitatively and quantitatively and discovered a good Wu-Palmer similarity metric on the WordNet lexicon (0.57±0.15) between true and predicted concepts, as well as high performance in the test set (0.81 ± 0.04) when human observers evaluated the quality of our reconstructions in a double-blind process. Our work represents an improvement in the decoding of visual stimuli with respect to previous studies by including a semantic-based hypothesis in our reconstruction pipeline.</p>
<p>In summary, our study provides evidence that measurable neural correlates can be linearly mapped onto the latent space of a convolutional neural network to synthesize images that are conceptually and semantically similar to the original stimuli. The findings have implications for both cognitive neuroscience and artificial intelligence, as they shed light on the neural mechanisms underlying visual perception and suggest promising avenues for future research.         </p>
<p>Fig. 1 .
1Fig. 1. Our proposed architecture. According to our hypothesis, the brain processes information by extracting visual features from images and projecting them onto a latent semantic space similar to the one formed by a convolutional neural network (CNN), termed "classifier" in this figure (in this paper we employed the ResNet50 architecture) when trained for object categorization. We developed a regression model that maps fMRI brain data to the CNN's latent space and used a k-nearest-neighbor (kNN) method to predict the related classes. Finally, we conditioned a latent diffusion model to generate novel images that are semantically similar to the visual stimuli from the predicted classes.</p>
<p>Fig. 3 .
3Comparison with previous approaches in brain decoding of visual stimuli over the GOD dataset. The first column shows original images used as stimuli, while other columns are reconstructions from different works. Our results are depicted in the second column.</p>
<p>Fig. 4 .
4Some examples Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 5 .
5Wu-Palmer distances (mean +/-s.d.) between original image stimuli shown to the subjects under fMRI for all subjects for both training (blue) and test (orange) sets.</p>
<p>Fig. 6 .
6Human evaluation: Rate of selection (mean ± std) of images generated by our model versus random images from human evaluators for images in the training (blue) and testing (orange) set s.</p>
<p>Fig. 8 .
8Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 9 .
9Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 10 .
10Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 11 .
11Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 12 .
12Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 13 .
13Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 14 .
14Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 15 .
15Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.</p>
<p>Fig. 16 .
16Examples of our semantic reconstructions over the test set. Left columns: original image stimulus shown to the subjects under fMRI. Other columns: semantic reconstructions for each subject in the GOD dataset.
Supplementary MaterialHere are presented all reconstructions obtained by our model for the entire test set, for each subject. The first column shows the images used as stimuli, so the target of the reconstruction. The other columns show semantic reconstructions of the target image for all subjects.
A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. E J Allen, G St-Yves, Y Wu, J L Breedlove, J S Prince, L T Dowdle, M Nau, B Caron, F Pestilli, I Charest, J B Hutchinson, T Naselaris, K Kay, 10.1038/s41593-021-00962-xNature Neuroscience. 251Allen, E.J., St-Yves, G., Wu, Y., Breedlove, J.L., Prince, J.S., Dowdle, L.T., Nau, M., Caron, B., Pestilli, F., Charest, I., Hutchinson, J.B., Naselaris, T., Kay, K.: A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience 25(1), 116-126 (Jan 2022). https://doi.org/10.1038/s41593- 021-00962-x, https://doi.org/10.1038/s41593-021-00962-x</p>
<p>Visual objects in context. M Bar, 10.1038/nrn1476Nature Reviews Neuroscience. 58Nature Publishing GroupBar, M.: Visual objects in context. Nature Reviews Neuroscience 5(8), 617-629 (Aug 2004). https://doi.org/10.1038/nrn1476, https://www.nature.com/article s/nrn1476, number: 8 Publisher: Nature Publishing Group</p>
<p>Deep language algorithms predict semantic comprehension from brain activity. C Caucheteux, A Gramfort, J R King, 10.1038/s41598-022-20460-9Scientific Reports. 12116327Caucheteux, C., Gramfort, A., King, J.R.: Deep language algorithms predict se- mantic comprehension from brain activity. Scientific Reports 12(1), 16327 (Sep 2022). https://doi.org/10.1038/s41598-022-20460-9, https://www.nature.com/a rticles/s41598-022-20460-9</p>
<p>Brains and algorithms partially converge in natural language processing. C Caucheteux, J R King, Communications Biology. 51134Caucheteux, C., King, J.R.: Brains and algorithms partially converge in nat- ural language processing. Communications Biology 5(1), 134 (Dec 2022).</p>
<p>. 10.1038/s42003-022-03036-1https://doi.org/10.1038/s42003-022-03036-1, https://www.nature.com/artic les/s42003-022-03036-1</p>
<p>Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. Z Chen, J Qing, T Xiang, W L Yue, J H Zhou, Chen, Z., Qing, J., Xiang, T., Yue, W.L., Zhou, J.H.: Seeing beyond the brain: Con- ditional diffusion model with sparse masked modeling for vision decoding (2022)</p>
<p>Torchmetrics -measuring reproducibility in pytorch. N S Detlefsen, J Borovec, J Schock, A H Jha, T Koker, L D Liello, D Stancl, C Quan, M Grechkin, W Falcon, 10.21105/joss.04101Journal of Open Source Software. 7704101Detlefsen, N.S., Borovec, J., Schock, J., Jha, A.H., Koker, T., Liello, L.D., Stancl, D., Quan, C., Grechkin, M., Falcon, W.: Torchmetrics -measuring re- producibility in pytorch. Journal of Open Source Software 7(70), 4101 (2022). https://doi.org/10.21105/joss.04101, https://doi.org/10.21105/joss.04101</p>
<p>Diffusion models beat gans on image synthesis. P Dhariwal, A Nichol, CoRR abs/2105.05233Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. CoRR abs/2105.05233 (2021), https://arxiv.org/abs/2105.05233</p>
<p>Self-supervised Natural Image Reconstruction and Large-scale Semantic Classification from Brain Activity. G Gaziv, R Beliy, N Granot, A Hoogi, F Strappini, T Golan, M Irani, NeuroImage. 254119121Gaziv, G., Beliy, R., Granot, N., Hoogi, A., Strappini, F., Golan, T., Irani, M.: Self-supervised Natural Image Reconstruction and Large-scale Seman- tic Classification from Brain Activity. NeuroImage 254, 119121 (Jul 2022).</p>
<p>. 10.1016/j.neuroimage.2022.119121https://doi.org/10.1016/j.neuroimage.2022.119121, https://linkinghub.elsev ier.com/retrieve/pii/S105381192200249X</p>
<p>C D Gilbert, M Sigman, Brain States: Top-Down Influences in Sensory Processing. 54Gilbert, C.D., Sigman, M.: Brain States: Top-Down Influ- ences in Sensory Processing. Neuron 54(5), 677-696 (Jun 2007).</p>
<p>. 10.1016/j.neuron.2007.05.019https://doi.org/10.1016/j.neuron.2007.05.019, h t t p s : / / w w w . s c i e n c e d i r e c t.com/science/article/pii/S0896627307003765</p>
<p>Shared computational principles for language processing in humans and deep language models. A Goldstein, Z Zada, E Buchnik, M Schain, A Price, B Aubrey, S A Nastase, A Feder, D Emanuel, A Cohen, A Jansen, H Gazula, G Choe, A Rao, C Kim, C Casto, L Fanda, W Doyle, D Friedman, P Dugan, L Melloni, R Reichart, S Devore, A Flinker, L Hasenfratz, O Levy, A Hassidim, M Brenner, Y Matias, K A Norman, O Devinsky, U Hasson, 10.1038/s41593-022-01026-4Nature Neuroscience. 253Goldstein, A., Zada, Z., Buchnik, E., Schain, M., Price, A., Aubrey, B., Nastase, S.A., Feder, A., Emanuel, D., Cohen, A., Jansen, A., Gazula, H., Choe, G., Rao, A., Kim, C., Casto, C., Fanda, L., Doyle, W., Friedman, D., Dugan, P., Melloni, L., Reichart, R., Devore, S., Flinker, A., Hasenfratz, L., Levy, O., Hassidim, A., Brenner, M., Matias, Y., Norman, K.A., Devinsky, O., Hasson, U.: Shared compu- tational principles for language processing in humans and deep language models. Nature Neuroscience 25(3), 369-380 (Mar 2022). https://doi.org/10.1038/s41593- 022-01026-4, https://www.nature.com/articles/s41593-022-01026-4</p>
<p>Separate visual pathways for perception and action. M A Goodale, A D Milner, 10.1016/0166-2236(92)90344-8Trends in Neurosciences. 151Goodale, M.A., Milner, A.D.: Separate visual pathways for perception and action. Trends in Neurosciences 15(1), 20-25 (Jan 1992). https://doi.org/10.1016/0166- 2236(92)90344-8</p>
<p>Visual properties of neurons in inferotemporal cortex of the Macaque. C G Gross, C E Rocha-Miranda, D B Bender, 10.1152/jn.1972.35.1.96Journal of Neurophysiology. 351Gross, C.G., Rocha-Miranda, C.E., Bender, D.B.: Visual properties of neurons in inferotemporal cortex of the Macaque. Journal of Neurophysiology 35(1), 96-111 (Jan 1972). https://doi.org/10.1152/jn.1972.35.1.96</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 10.48550/ARXIV.1512.03385He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015). https://doi.org/10.48550/ARXIV.1512.03385, https://arxiv.org/abs/ 1512.03385</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, 10.48550/ARXIV.2006.11239Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models (2020). https://doi.org/10.48550/ARXIV.2006.11239, https://arxiv.org/abs/2006.1 1239</p>
<p>Generic decoding of seen and imagined objects using hierarchical visual features. T Horikawa, Y Kamitani, 10.1038/ncomms15037Nature Communications. 8115037Horikawa, T., Kamitani, Y.: Generic decoding of seen and imagined objects us- ing hierarchical visual features. Nature Communications 8(1), 15037 (Aug 2017). https://doi.org/10.1038/ncomms15037, http://www.nature.com/articles/ncom ms15037</p>
<p>A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain. A Huth, S Nishimoto, A Vu, J Gallant, Neuron. 766Huth, A., Nishimoto, S., Vu, A., Gallant, J.: A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain. Neuron 76(6), 1210-1224 (Dec 2012).</p>
<p>. 10.1016/j.neuron.2012.10.014https://doi.org/10.1016/j.neuron.2012.10.014, https://linkinghub.elsevier. com/retrieve/pii/S0896627312009348</p>
<p>Convolutional neural networks as a model of the visual system: Past, present, and future. G W Lindsay, J. Cogn. Neurosci. 3310Lindsay, G.W.: Convolutional neural networks as a model of the visual system: Past, present, and future. J. Cogn. Neurosci. 33(10), 2017-2031 (Sep 2021)</p>
<p>Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity. E Matsuo, I Kobayashi, S Nishimoto, S Nishida, H Asoh, 10.18653/v1/P16-3004Proceedings of the ACL 2016 Student Research Workshop. the ACL 2016 Student Research WorkshopBerlin, GermanyAssociation for Computational LinguisticsMatsuo, E., Kobayashi, I., Nishimoto, S., Nishida, S., Asoh, H.: Generat- ing Natural Language Descriptions for Semantic Representations of Human Brain Activity. In: Proceedings of the ACL 2016 Student Research Workshop. pp. 22-29. Association for Computational Linguistics, Berlin, Germany (2016). https://doi.org/10.18653/v1/P16-3004, http://aclweb.org/anthology/P16-300</p>
<p>Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN. M Mozafari, L Reddy, R Vanrullen, 2020 International Joint Conference on Neural Networks (IJCNN). Mozafari, M., Reddy, L., VanRullen, R.: Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN. In: 2020 International Joint Conference on Neural Networks (IJCNN). pp. 1-8 (Jul 2020).</p>
<p>. 10.1109/IJCNN48605.2020.9206960arXiv:2001.11761cs, eess, q-biohttps://doi.org/10.1109/IJCNN48605.2020.9206960, h t tp : / / a r x i v . o r g / a b s/2001.11761, arXiv:2001.11761 [cs, eess, q-bio]</p>
<p>Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs. F Ozcelik, B Choksi, M Mozafari, L Reddy, R Vanrullen, arXiv:2202.12692cs, eess, q-bioOzcelik, F., Choksi, B., Mozafari, M., Reddy, L., VanRullen, R.: Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs (Feb 2022), http://arxiv.org/abs/2202.12692, arXiv:2202.12692 [cs, eess, q-bio]</p>
<p>Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion. F Ozcelik, R Vanrullen, Ozcelik, F., VanRullen, R.: Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion (2023)</p>
<p>Wordnet::similarity -measuring the relatedness of concepts. T Pedersen, S Patwardhan, J Michelizzi, Pedersen, T., Patwardhan, S., Michelizzi, J.: Wordnet::similarity -measuring the relatedness of concepts (04 2004)</p>
<p>The neural and computational bases of semantic cognition. M A L Ralph, E Jefferies, K Patterson, T T Rogers, Nat. Rev. Neurosci. 181Ralph, M.A.L., Jefferies, E., Patterson, K., Rogers, T.T.: The neural and compu- tational bases of semantic cognition. Nat. Rev. Neurosci. 18(1), 42-55 (Jan 2017)</p>
<p>Reconstructing Perceived Images from Brain Activity by Visually-guided Cognitive Representation and Adversarial Learning. Z Ren, J Li, X Xue, X Li, F Yang, Z Jiao, X Gao, arXiv:1906.12181csRen, Z., Li, J., Xue, X., Li, X., Yang, F., Jiao, Z., Gao, X.: Reconstructing Per- ceived Images from Brain Activity by Visually-guided Cognitive Representation and Adversarial Learning (Oct 2019), http://arxiv.org/abs/1906.12181, arXiv:1906.12181 [cs]</p>
<p>End-to-end deep image reconstruction from human brain activity p. G Shen, K Dwivedi, K Majima, T Horikawa, Y Kamitani, 24Shen, G., Dwivedi, K., Majima, K., Horikawa, T., Kamitani, Y.: End-to-end deep image reconstruction from human brain activity p. 24</p>
<p>High-resolution image reconstruction with latent diffusion models from human brain activity. Y Takagi, S Nishimoto, bioRxiv. Takagi, Y., Nishimoto, S.: High-resolution image reconstruction with latent diffusion models from human brain activity. bioRxiv (2023).</p>
<p>. 10.1101/2022.11.18.5170042023/03/11/2022.11.18.517004https://doi.org/10.1101/2022.11.18.517004, h t t p s : / / w w w . b i o r x i v . o r g / c o ntent/early/2023/03/11/2022.11.18.517004</p>
<p>What' and 'where' in the human brain. L G Ungerleider, J V Haxby, 10.1016/0959-4388(94)90066-3Current Opinion in Neurobiology. 42Ungerleider, L.G., Haxby, J.V.: 'What' and 'where' in the human brain. Current Opinion in Neurobiology 4(2), 157-165 (Apr 1994). https://doi.org/10.1016/0959- 4388(94)90066-3</p>
<p>D C Van Essen, S M Smith, D M Barch, T E J Behrens, E Yacoub, K Ugurbil, Hcp Wu-Minn, Consortium, The WU-Minn human connectome project: an overview. 80Van Essen, D.C., Smith, S.M., Barch, D.M., Behrens, T.E.J., Yacoub, E., Ugurbil, K., WU-Minn HCP Consortium: The WU-Minn human connectome project: an overview. Neuroimage 80, 62-79 (Oct 2013)</p>
<p>Reconstructing faces from fmri patterns using deep generative neural networks. R Vanrullen, L Reddy, Communications Biology. 21193VanRullen, R., Reddy, L.: Reconstructing faces from fmri patterns using deep generative neural networks. Communications Biology 2(1), 193 (May 2019).</p>
<p>. 10.1038/s42003-019-0438-yhttps://doi.org/10.1038/s42003-019-0438-y, https://doi.org/10.1038/s42003 -019-0438-y</p>
<p>Frechet inception distance (fid) for evaluating gans. Y Yu, W Zhang, Y Deng, Yu, Y., Zhang, W., Deng, Y.: Frechet inception distance (fid) for evaluating gans (09 2021)</p>
<p>Decoding of visual information from human brain activity: A review of fMRI and EEG studies. R Zafar, A S Malik, N Kamel, S C Dass, J M Abdullah, F Reza, A H Karim, http:/www.worldscientific.com/doi/abs/10.1142/S0219635215500089Journal of Integrative Neuroscience. 1402Zafar, R., Malik, A.S., Kamel, N., Dass, S.C., Abdullah, J.M., Reza, F., Ab- dul Karim, A.H.: Decoding of visual information from human brain activity: A review of fMRI and EEG studies. Journal of Integrative Neuroscience 14(02), 155- 168 (Jun 2015). https://doi.org/10.1142/S0219635215500089, http://www.worlds cientific.com/doi/abs/10.1142/S0219635215500089</p>
<p>Attention during natural vision warps semantic representation across the human brain. T Ç Ukur, S Nishimoto, A G Huth, J L Gallant, 10.1038/nn.3381Nature Neuroscience. 166Ç ukur, T., Nishimoto, S., Huth, A.G., Gallant, J.L.: Attention during natural vision warps semantic representation across the human brain. Nature Neuroscience 16(6), 763-770 (Jun 2013). https://doi.org/10.1038/nn.3381, http://www.nature .com/articles/nn.3381</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2010 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2010</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2010</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-279260810</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.08978v1.pdf" target="_blank">Propositional Logic for Probing Generalization in Neural Networks</a></p>
                <p><strong>Paper Abstract:</strong> The extent to which neural networks are able to acquire and represent symbolic rules remains a key topic of research and debate. Much current work focuses on the impressive capabilities of large language models, as well as their often ill-understood failures on a wide range of reasoning tasks. In this paper, in contrast, we investigate the generalization behavior of three key neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a controlled task rooted in propositional logic. The task requires models to generate satisfying assignments for logical formulas, making it a structured and interpretable setting for studying compositionality. We introduce a balanced extension of an existing dataset to eliminate superficial patterns and enable testing on unseen operator combinations. Using this dataset, we evaluate the ability of the three architectures to generalize beyond the training distribution. While all models perform well in-distribution, we find that generalization to unseen patterns, particularly those involving negation, remains a significant challenge. Transformers fail to apply negation compositionally, unless structural biases are introduced. Our findings highlight persistent limitations in the ability of standard architectures to learn systematic representations of logical operators, suggesting the need for stronger inductive biases to support robust rule-based reasoning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2010.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2010.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-abs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer encoder-decoder (absolute positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard Transformer encoder-decoder trained end-to-end to generate satisfying (partial) assignments for propositional formulas in Polish notation; uses absolute positional encodings on inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (absolute positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (6 layers encoder, 6 layers decoder, 4 attention heads, FF size 512, encoder embedding 128, decoder embedding 64) with absolute positional encodings on inputs; decoder cross-attends to encoder final layer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈1.8M</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>self-attention encoder, cross-attention decoder, absolute positional encodings (sequence-based); no explicit tree bias</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical/logical (propositional logic variable assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prop35 variable-assignment / templated held-out-pattern splits</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a propositional formula (Polish/prefix notation, up to length 35, max 5 variables, operators ¬, ∧, ∨, ↔, ⊕), generate a satisfying (partial) assignment. Compositionality arises from nested logical operators and combinations of operators and negation; generalization tests hold out specific parent-child operator patterns (P1–P7) and evaluate length generalization (Prop50) and templated contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Formulas length 5–35 tokens (nesting depth variable up to those lengths); exact numeric nesting depth not explicitly reported</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>nested logical operations; novel operator combinations (especially negation applied to binary operators); novel variable/operator pairings</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>IID training (Prop35Balanced); length generalization (Prop50); systematic generalization: held-out structural patterns (P1–P7) where a single parent-child pattern is omitted during training and reintroduced at test time</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised learning (teacher forcing), Adam optimizer, Noam LR schedule, 800K training examples, 128 epochs (≈200k steps with batch size 512)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Semantic accuracy ≈94% on IID Prop35Balanced / overall unseen data; syntactic accuracy lower (≈58% on training set example reported), templated test set ≳98% for base models</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Strong on many unseen combinations (most held-out patterns P4–P7), but severe failures on held-out negation-of-binary patterns (P1: !&, P2: !|, P3: !xor). For negation patterns (P1–P3) Transformer with absolute encodings often 'ignores' negation leading to near-zero correct handling on some simple templated sentences; tree encodings mitigate some failures (see Transformer-tree). Exact per-pattern numbers for Transformer-abs not uniformly reported, but qualitative: large drop for P1–P3 compared to IID.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Substantial gap for specific unseen structural patterns: IID semantic ≈94% vs near 0% (or very low) for simplest examples of some negated-operator patterns (P1), while other held-out patterns show minor gaps; cannot compute single numeric gap for all patterns due to mixed reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Per-type breakdown: nested negation applied to binary operators (P1–P3) is the weakest (large drops); combinations not involving negated operators (P4–P7) largely preserved (high accuracy); templated test contexts show base models ≳98% accuracy when pattern was seen during training.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against: Transformer with tree positional encodings, GCN encoder + Transformer decoder, and LSTM encoder + Transformer decoder. On IID data the Transformer variants achieve highest semantic accuracy (≈94%); on some OOD splits (especially negation patterns), Transformer-abs performs worst and shows more 'ignoring negation' behaviour than tree-encoded Transformer, GCN, or LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Transformers (absolute) vs Transformer (tree) vs GCN vs LSTM: All reach high IID semantic accuracy, but on systematic held-out-pattern splits GCN and LSTM outperform Transformer-abs on several OOD splits; Transformer-tree reduces some failures (helps P2 and P3).</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard Transformer with absolute positional encodings attains high IID semantic accuracy (~94%) but fails systematically on certain compositional generalization cases, notably negation applied to unseen binary operators (P1–P3), frequently 'ignoring' the negation; adding structural bias (tree positional encodings) improves but does not fully eliminate these failures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Primary failures are on unseen combinations where NOT is applied to a binary operator (¬(A ∧ B), ¬(A ∨ B), ¬(A ⊕ B)). Models often produce the same output as if the negation were absent (classified as 'ignoring' negation). Double negation patterns were absent from training and represent another untested area.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeeds when held-out patterns do not involve negated binary operators (P4–P7) or when structural biases (tree encodings) are present; base IID training with broad coverage of operator contexts enables generalization for many novel combinations but not for negation-of-operator patterns.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2010.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2010.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer encoder-decoder with tree positional encodings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same Transformer encoder-decoder architecture as base but with tree-structured positional encodings on the input; introduces explicit tree-structure bias to aid parsing and compositional representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (tree positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (same hyperparameters as base) where input positional encodings reflect tree structure rather than absolute sequence positions; decoder cross-attends to encoder final layer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈1.8M</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>self-attention encoder, cross-attention decoder, tree-structured positional encodings (structural bias), attention-based processing</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical/logical (propositional logic variable assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prop35 variable-assignment / templated held-out-pattern splits</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate satisfying partial assignments for propositional formulas; tests include IID Prop35Balanced, length generalization (Prop50), and systematic held-out-pattern splits P1–P7 to test compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Formulas length 5–35 tokens (nesting depth variable up to those lengths); exact numeric nesting depth not explicitly reported</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>nested logical operations; novel combinations of operators and negation</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>IID (Prop35Balanced); length (Prop50); systematic held-out operator/negation patterns (P1–P7)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised learning (teacher forcing), Adam optimizer, Noam LR schedule, same data and steps as other models</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Semantic accuracy comparable to base Transformer on IID data (≈94%); training semantic accuracy example reported ≈93.77%</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Performs better than Transformer-abs on several held-out patterns, notably improves on P2 and P3 (negation-of-OR and negation-of-XOR), though performance on negation patterns still lags behind other (non-negation) held-out patterns; retains strong templated-set performance (≳98%).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Reduced relative to Transformer-abs for some patterns (P2, P3), but gaps remain for negation-of-operator patterns (P1–P3); exact numeric gaps not uniformly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Improved handling of negated binary operators P2 and P3 compared to absolute encodings; still worse on P1 (!&), indicating partial mitigation by structural bias.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to Transformer-abs, GCN, and LSTM. Transformer-tree shows better length generalization (Prop50) and improved systematic generalization for some negation patterns (P2, P3) relative to Transformer-abs, but not full recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Tree positional encodings provide an inductive bias that helps compositionally combine operators in some withheld configurations; however, LSTM/GCN sometimes outperform Transformer-tree on specific held-out patterns (e.g., LSTM best on P3).</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Introducing tree-structured positional encodings improves both length generalization and some types of systematic generalization (notably P2 and P3), indicating that explicit structural bias helps but is not sufficient to ensure full compositional generalization for all negation-related patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Although improved, Transformer-tree still shows significant weaknesses on certain negation-of-operator patterns (particularly P1, negated AND) and does not fully close the gap to IID performance on these cases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Works best when structural (tree) positional information is provided and when withheld patterns do not critically involve negation combined with certain binary operators; tree encodings help but do not fully solve negation composition failures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2010.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2010.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Network encoder + Transformer decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph Convolutional Network encoder that explicitly encodes tree structure (nodes are symbols connected to parents, children, self) combined with a Transformer decoder to generate satisfying assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GCN (Graph Convolutional Network encoder + Transformer decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder composed of stacked GCN blocks (each block followed by LayerNorm, ReLU, residual connections); input symbols are nodes connected to children, parents and self to encode tree structure explicitly; decoder is standard Transformer that cross-attends to encoder final hidden states.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈900k</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>explicit graph/tree structural encoding via GCN blocks, residual connections, no positional encodings; attention in decoder</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical/logical (propositional logic variable assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prop35 variable-assignment / templated held-out-pattern splits</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same variable-assignment task; GCN encoder is chosen to introduce an inductive bias for processing tree-structured formulas and to evaluate whether structural encoder biases improve systematic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Formulas length 5–35 tokens; exact nesting depth not reported</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>nested logical operations; compositionality of operators and negation</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>IID (Prop35Balanced); length (Prop50); systematic held-out-pattern splits P1–P7</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised learning (teacher forcing), Adam optimizer, Noam LR schedule; hyperparameter search indicated 16 encoder layers best on validation</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Semantic IID accuracy lower than best Transformer variants but still high (reported ≈87% overall when replacing Transformer encoder with GCN/LSTM in aggregate comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Performs well on many held-out patterns (P4–P7). Specific failures: on pattern (P6) 'AND XOR' the GCN accuracy is reported as 76% overall and only 57% on sentences starting with that pattern (i.e., more severe failure on certain placements). On negation patterns P1–P3 GCN improves relative to Transformer-abs in some cases, but does not fully solve P1.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Compared to IID (≈87% for GCN or ≈94% for Transformer IID), GCN shows moderate drops on specific held-out patterns (e.g., 76% on P6 vs IID ≈87% → ~11% gap for that pattern; larger gaps for negation patterns in some contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>GCN fairs well on many composition types, but shows a notable weakness on AND-with-XOR (P6) especially when pattern appears in root/start position; mixed results on negation-of-operators.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to Transformer-abs, Transformer-tree, and LSTM: GCN underperforms the best Transformer IID semantic scores but outperforms Transformer-abs on some OOD splits; however GCN lags on certain patterns (P6).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>GCN's explicit structural encoding helps on some systematic generalization splits but does not universally outperform other inductive biases (e.g., LSTM beats others on some negation patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GCN encoders, which explicitly encode tree structure, provide a useful inductive bias that helps on many compositional cases, but they can still fail on specific operator-composition phenomena (notably P6 AND-XOR in root position) and do not universally solve negation composition problems.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Concrete failure case: AND with XOR child (P6) — overall accuracy 76%, but only 57% for sentences starting with the pattern; failures remain on negated-operator patterns (P1–P3) in several contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeeds when structural relationships are crucial and when held-out patterns do not place challenging operator compositions in root/start positions; explicit graph encoding helps but is not sufficient for all compositional generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2010.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2010.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM encoder + Transformer decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent encoder (6-layer LSTM) without attention used in place of Transformer encoder, paired with a Transformer decoder; tests whether recurrence as an inductive bias aids compositional generalization, especially for negation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM encoder + Transformer decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>6-layer LSTM encoder (no attention) with learned initial hidden/cell states; decoder is Transformer that cross-attends to encoder final hidden states; smaller parameter count than Transformer base.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈1.4M</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>recurrence-based encoder (hierarchical sequence processing), no self-attention in encoder, decoder cross-attention to final encoder layer; no explicit positional encodings</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical/logical (propositional logic variable assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prop35 variable-assignment / templated held-out-pattern splits</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same variable-assignment task with held-out-pattern systematic splits and length generalization tests; evaluates whether recurrence supports systematic composition of operators and negation.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Formulas length 5–35 tokens; exact nesting depth not reported</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>nested logical operations; novel operator combinations; negation composition</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>IID (Prop35Balanced); length (Prop50); systematic held-out-pattern splits P1–P7</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised learning (teacher forcing), Adam optimizer with learning rate 5e-5 for LSTMs, Noam LR schedule otherwise similar</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Semantic IID accuracy somewhat lower than best Transformer but in the same ballpark (paper reports architectures replacing Transformer encoder with GCN or LSTM obtain ≈87% semantic accuracy in aggregate comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>LSTM shows notable improvements over standard Transformer on some negation-related held-out patterns: specifically, the LSTM outperforms all models on P3 (not xor) and improves over Transformer on P2 and P3. However, like other architectures, the LSTM does not reliably solve pattern P1 (not AND) in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Smaller gap than Transformer-abs on certain negation patterns (e.g., P3) but still present for P1; precise numeric gaps not uniformly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Better handling of negation-of-XOR (P3) relative to Transformers; relative strengths on other pattern types mixed but generally competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to Transformer-abs, Transformer-tree, and GCN; LSTM improves on some systematic generalization cases involving negation (P3) and sometimes P2, suggesting recurrence is a useful inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Recurrence (LSTM) provides an inductive bias distinct from structural (GCN/tree) biases and improves compositional generalization for some negation patterns; nonetheless does not fully solve all negation-of-operator failures (e.g., P1).</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LSTM encoders yield better systematic generalization on certain negation-related held-out patterns (notably P3), indicating recurrence can be a beneficial inductive bias for compositionality. Overall, however, LSTM does not eliminate failures on all negated-operator cases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Still fails reliably on P1 (negated AND) across architectures; produces mixed errors (sometimes 'ignoring' negation, sometimes other incorrect worlds).</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeeds more often when the withheld compositional pattern involves XOR or OR with negation (P2, P3) and when the recurrent inductive bias aligns with the task structural regularities.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching Temporal Logics to Neural Networks <em>(Rating: 2)</em></li>
                <li>Can Neural Networks Understand Logical Entailment? <em>(Rating: 2)</em></li>
                <li>NeuroSAT: Learning a SAT Solver from Single-Bit Supervision <em>(Rating: 2)</em></li>
                <li>SCAN: Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks <em>(Rating: 2)</em></li>
                <li>COGS: A Compositional Generalization Challenge Based on Semantic Interpretation <em>(Rating: 2)</em></li>
                <li>Compositionality Decomposed: How Do Neural Networks Generalise <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2010",
    "paper_id": "paper-279260810",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "Transformer-abs",
            "name_full": "Transformer encoder-decoder (absolute positional encodings)",
            "brief_description": "Standard Transformer encoder-decoder trained end-to-end to generate satisfying (partial) assignments for propositional formulas in Polish notation; uses absolute positional encodings on inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (absolute positional encodings)",
            "model_description": "Encoder-decoder Transformer (6 layers encoder, 6 layers decoder, 4 attention heads, FF size 512, encoder embedding 128, decoder embedding 64) with absolute positional encodings on inputs; decoder cross-attends to encoder final layer.",
            "model_size": "≈1.8M",
            "is_pretrained": false,
            "architectural_features": "self-attention encoder, cross-attention decoder, absolute positional encodings (sequence-based); no explicit tree bias",
            "task_domain": "mathematical/logical (propositional logic variable assignment)",
            "task_name": "Prop35 variable-assignment / templated held-out-pattern splits",
            "task_description": "Given a propositional formula (Polish/prefix notation, up to length 35, max 5 variables, operators ¬, ∧, ∨, ↔, ⊕), generate a satisfying (partial) assignment. Compositionality arises from nested logical operators and combinations of operators and negation; generalization tests hold out specific parent-child operator patterns (P1–P7) and evaluate length generalization (Prop50) and templated contexts.",
            "compositional_depth": "Formulas length 5–35 tokens (nesting depth variable up to those lengths); exact numeric nesting depth not explicitly reported",
            "composition_type": "nested logical operations; novel operator combinations (especially negation applied to binary operators); novel variable/operator pairings",
            "split_type": "IID training (Prop35Balanced); length generalization (Prop50); systematic generalization: held-out structural patterns (P1–P7) where a single parent-child pattern is omitted during training and reintroduced at test time",
            "training_strategy": "Supervised learning (teacher forcing), Adam optimizer, Noam LR schedule, 800K training examples, 128 epochs (≈200k steps with batch size 512)",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Semantic accuracy ≈94% on IID Prop35Balanced / overall unseen data; syntactic accuracy lower (≈58% on training set example reported), templated test set ≳98% for base models",
            "compositional_performance": "Strong on many unseen combinations (most held-out patterns P4–P7), but severe failures on held-out negation-of-binary patterns (P1: !&, P2: !|, P3: !xor). For negation patterns (P1–P3) Transformer with absolute encodings often 'ignores' negation leading to near-zero correct handling on some simple templated sentences; tree encodings mitigate some failures (see Transformer-tree). Exact per-pattern numbers for Transformer-abs not uniformly reported, but qualitative: large drop for P1–P3 compared to IID.",
            "generalization_gap": "Substantial gap for specific unseen structural patterns: IID semantic ≈94% vs near 0% (or very low) for simplest examples of some negated-operator patterns (P1), while other held-out patterns show minor gaps; cannot compute single numeric gap for all patterns due to mixed reporting.",
            "performance_by_depth": null,
            "performance_by_composition_type": "Per-type breakdown: nested negation applied to binary operators (P1–P3) is the weakest (large drops); combinations not involving negated operators (P4–P7) largely preserved (high accuracy); templated test contexts show base models ≳98% accuracy when pattern was seen during training.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against: Transformer with tree positional encodings, GCN encoder + Transformer decoder, and LSTM encoder + Transformer decoder. On IID data the Transformer variants achieve highest semantic accuracy (≈94%); on some OOD splits (especially negation patterns), Transformer-abs performs worst and shows more 'ignoring negation' behaviour than tree-encoded Transformer, GCN, or LSTM.",
            "architectural_comparison": "Transformers (absolute) vs Transformer (tree) vs GCN vs LSTM: All reach high IID semantic accuracy, but on systematic held-out-pattern splits GCN and LSTM outperform Transformer-abs on several OOD splits; Transformer-tree reduces some failures (helps P2 and P3).",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Standard Transformer with absolute positional encodings attains high IID semantic accuracy (~94%) but fails systematically on certain compositional generalization cases, notably negation applied to unseen binary operators (P1–P3), frequently 'ignoring' the negation; adding structural bias (tree positional encodings) improves but does not fully eliminate these failures.",
            "failure_analysis": "Primary failures are on unseen combinations where NOT is applied to a binary operator (¬(A ∧ B), ¬(A ∨ B), ¬(A ⊕ B)). Models often produce the same output as if the negation were absent (classified as 'ignoring' negation). Double negation patterns were absent from training and represent another untested area.",
            "success_conditions": "Succeeds when held-out patterns do not involve negated binary operators (P4–P7) or when structural biases (tree encodings) are present; base IID training with broad coverage of operator contexts enables generalization for many novel combinations but not for negation-of-operator patterns.",
            "uuid": "e2010.0"
        },
        {
            "name_short": "Transformer-tree",
            "name_full": "Transformer encoder-decoder with tree positional encodings",
            "brief_description": "Same Transformer encoder-decoder architecture as base but with tree-structured positional encodings on the input; introduces explicit tree-structure bias to aid parsing and compositional representation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (tree positional encodings)",
            "model_description": "Encoder-decoder Transformer (same hyperparameters as base) where input positional encodings reflect tree structure rather than absolute sequence positions; decoder cross-attends to encoder final layer.",
            "model_size": "≈1.8M",
            "is_pretrained": false,
            "architectural_features": "self-attention encoder, cross-attention decoder, tree-structured positional encodings (structural bias), attention-based processing",
            "task_domain": "mathematical/logical (propositional logic variable assignment)",
            "task_name": "Prop35 variable-assignment / templated held-out-pattern splits",
            "task_description": "Generate satisfying partial assignments for propositional formulas; tests include IID Prop35Balanced, length generalization (Prop50), and systematic held-out-pattern splits P1–P7 to test compositionality.",
            "compositional_depth": "Formulas length 5–35 tokens (nesting depth variable up to those lengths); exact numeric nesting depth not explicitly reported",
            "composition_type": "nested logical operations; novel combinations of operators and negation",
            "split_type": "IID (Prop35Balanced); length (Prop50); systematic held-out operator/negation patterns (P1–P7)",
            "training_strategy": "Supervised learning (teacher forcing), Adam optimizer, Noam LR schedule, same data and steps as other models",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Semantic accuracy comparable to base Transformer on IID data (≈94%); training semantic accuracy example reported ≈93.77%",
            "compositional_performance": "Performs better than Transformer-abs on several held-out patterns, notably improves on P2 and P3 (negation-of-OR and negation-of-XOR), though performance on negation patterns still lags behind other (non-negation) held-out patterns; retains strong templated-set performance (≳98%).",
            "generalization_gap": "Reduced relative to Transformer-abs for some patterns (P2, P3), but gaps remain for negation-of-operator patterns (P1–P3); exact numeric gaps not uniformly reported.",
            "performance_by_depth": null,
            "performance_by_composition_type": "Improved handling of negated binary operators P2 and P3 compared to absolute encodings; still worse on P1 (!&), indicating partial mitigation by structural bias.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to Transformer-abs, GCN, and LSTM. Transformer-tree shows better length generalization (Prop50) and improved systematic generalization for some negation patterns (P2, P3) relative to Transformer-abs, but not full recovery.",
            "architectural_comparison": "Tree positional encodings provide an inductive bias that helps compositionally combine operators in some withheld configurations; however, LSTM/GCN sometimes outperform Transformer-tree on specific held-out patterns (e.g., LSTM best on P3).",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Introducing tree-structured positional encodings improves both length generalization and some types of systematic generalization (notably P2 and P3), indicating that explicit structural bias helps but is not sufficient to ensure full compositional generalization for all negation-related patterns.",
            "failure_analysis": "Although improved, Transformer-tree still shows significant weaknesses on certain negation-of-operator patterns (particularly P1, negated AND) and does not fully close the gap to IID performance on these cases.",
            "success_conditions": "Works best when structural (tree) positional information is provided and when withheld patterns do not critically involve negation combined with certain binary operators; tree encodings help but do not fully solve negation composition failures.",
            "uuid": "e2010.1"
        },
        {
            "name_short": "GCN",
            "name_full": "Graph Convolutional Network encoder + Transformer decoder",
            "brief_description": "Graph Convolutional Network encoder that explicitly encodes tree structure (nodes are symbols connected to parents, children, self) combined with a Transformer decoder to generate satisfying assignments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GCN (Graph Convolutional Network encoder + Transformer decoder)",
            "model_description": "Encoder composed of stacked GCN blocks (each block followed by LayerNorm, ReLU, residual connections); input symbols are nodes connected to children, parents and self to encode tree structure explicitly; decoder is standard Transformer that cross-attends to encoder final hidden states.",
            "model_size": "≈900k",
            "is_pretrained": false,
            "architectural_features": "explicit graph/tree structural encoding via GCN blocks, residual connections, no positional encodings; attention in decoder",
            "task_domain": "mathematical/logical (propositional logic variable assignment)",
            "task_name": "Prop35 variable-assignment / templated held-out-pattern splits",
            "task_description": "Same variable-assignment task; GCN encoder is chosen to introduce an inductive bias for processing tree-structured formulas and to evaluate whether structural encoder biases improve systematic generalization.",
            "compositional_depth": "Formulas length 5–35 tokens; exact nesting depth not reported",
            "composition_type": "nested logical operations; compositionality of operators and negation",
            "split_type": "IID (Prop35Balanced); length (Prop50); systematic held-out-pattern splits P1–P7",
            "training_strategy": "Supervised learning (teacher forcing), Adam optimizer, Noam LR schedule; hyperparameter search indicated 16 encoder layers best on validation",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Semantic IID accuracy lower than best Transformer variants but still high (reported ≈87% overall when replacing Transformer encoder with GCN/LSTM in aggregate comparisons).",
            "compositional_performance": "Performs well on many held-out patterns (P4–P7). Specific failures: on pattern (P6) 'AND XOR' the GCN accuracy is reported as 76% overall and only 57% on sentences starting with that pattern (i.e., more severe failure on certain placements). On negation patterns P1–P3 GCN improves relative to Transformer-abs in some cases, but does not fully solve P1.",
            "generalization_gap": "Compared to IID (≈87% for GCN or ≈94% for Transformer IID), GCN shows moderate drops on specific held-out patterns (e.g., 76% on P6 vs IID ≈87% → ~11% gap for that pattern; larger gaps for negation patterns in some contexts).",
            "performance_by_depth": null,
            "performance_by_composition_type": "GCN fairs well on many composition types, but shows a notable weakness on AND-with-XOR (P6) especially when pattern appears in root/start position; mixed results on negation-of-operators.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to Transformer-abs, Transformer-tree, and LSTM: GCN underperforms the best Transformer IID semantic scores but outperforms Transformer-abs on some OOD splits; however GCN lags on certain patterns (P6).",
            "architectural_comparison": "GCN's explicit structural encoding helps on some systematic generalization splits but does not universally outperform other inductive biases (e.g., LSTM beats others on some negation patterns).",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "GCN encoders, which explicitly encode tree structure, provide a useful inductive bias that helps on many compositional cases, but they can still fail on specific operator-composition phenomena (notably P6 AND-XOR in root position) and do not universally solve negation composition problems.",
            "failure_analysis": "Concrete failure case: AND with XOR child (P6) — overall accuracy 76%, but only 57% for sentences starting with the pattern; failures remain on negated-operator patterns (P1–P3) in several contexts.",
            "success_conditions": "Succeeds when structural relationships are crucial and when held-out patterns do not place challenging operator compositions in root/start positions; explicit graph encoding helps but is not sufficient for all compositional generalization.",
            "uuid": "e2010.2"
        },
        {
            "name_short": "LSTM",
            "name_full": "LSTM encoder + Transformer decoder",
            "brief_description": "Recurrent encoder (6-layer LSTM) without attention used in place of Transformer encoder, paired with a Transformer decoder; tests whether recurrence as an inductive bias aids compositional generalization, especially for negation patterns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM encoder + Transformer decoder",
            "model_description": "6-layer LSTM encoder (no attention) with learned initial hidden/cell states; decoder is Transformer that cross-attends to encoder final hidden states; smaller parameter count than Transformer base.",
            "model_size": "≈1.4M",
            "is_pretrained": false,
            "architectural_features": "recurrence-based encoder (hierarchical sequence processing), no self-attention in encoder, decoder cross-attention to final encoder layer; no explicit positional encodings",
            "task_domain": "mathematical/logical (propositional logic variable assignment)",
            "task_name": "Prop35 variable-assignment / templated held-out-pattern splits",
            "task_description": "Same variable-assignment task with held-out-pattern systematic splits and length generalization tests; evaluates whether recurrence supports systematic composition of operators and negation.",
            "compositional_depth": "Formulas length 5–35 tokens; exact nesting depth not reported",
            "composition_type": "nested logical operations; novel operator combinations; negation composition",
            "split_type": "IID (Prop35Balanced); length (Prop50); systematic held-out-pattern splits P1–P7",
            "training_strategy": "Supervised learning (teacher forcing), Adam optimizer with learning rate 5e-5 for LSTMs, Noam LR schedule otherwise similar",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Semantic IID accuracy somewhat lower than best Transformer but in the same ballpark (paper reports architectures replacing Transformer encoder with GCN or LSTM obtain ≈87% semantic accuracy in aggregate comparisons).",
            "compositional_performance": "LSTM shows notable improvements over standard Transformer on some negation-related held-out patterns: specifically, the LSTM outperforms all models on P3 (not xor) and improves over Transformer on P2 and P3. However, like other architectures, the LSTM does not reliably solve pattern P1 (not AND) in all cases.",
            "generalization_gap": "Smaller gap than Transformer-abs on certain negation patterns (e.g., P3) but still present for P1; precise numeric gaps not uniformly reported.",
            "performance_by_depth": null,
            "performance_by_composition_type": "Better handling of negation-of-XOR (P3) relative to Transformers; relative strengths on other pattern types mixed but generally competitive.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to Transformer-abs, Transformer-tree, and GCN; LSTM improves on some systematic generalization cases involving negation (P3) and sometimes P2, suggesting recurrence is a useful inductive bias.",
            "architectural_comparison": "Recurrence (LSTM) provides an inductive bias distinct from structural (GCN/tree) biases and improves compositional generalization for some negation patterns; nonetheless does not fully solve all negation-of-operator failures (e.g., P1).",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "LSTM encoders yield better systematic generalization on certain negation-related held-out patterns (notably P3), indicating recurrence can be a beneficial inductive bias for compositionality. Overall, however, LSTM does not eliminate failures on all negated-operator cases.",
            "failure_analysis": "Still fails reliably on P1 (negated AND) across architectures; produces mixed errors (sometimes 'ignoring' negation, sometimes other incorrect worlds).",
            "success_conditions": "Succeeds more often when the withheld compositional pattern involves XOR or OR with negation (P2, P3) and when the recurrent inductive bias aligns with the task structural regularities.",
            "uuid": "e2010.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching Temporal Logics to Neural Networks",
            "rating": 2
        },
        {
            "paper_title": "Can Neural Networks Understand Logical Entailment?",
            "rating": 2
        },
        {
            "paper_title": "NeuroSAT: Learning a SAT Solver from Single-Bit Supervision",
            "rating": 2
        },
        {
            "paper_title": "SCAN: Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
            "rating": 2
        },
        {
            "paper_title": "COGS: A Compositional Generalization Challenge Based on Semantic Interpretation",
            "rating": 2
        },
        {
            "paper_title": "Compositionality Decomposed: How Do Neural Networks Generalise",
            "rating": 2
        }
    ],
    "cost": 0.015834749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Propositional Logic for Probing Generalization in Neural Networks
10 Jun 2025</p>
<p>Anna Langedijk annalangedijk@gmail.com 
Willem Zuidema w.h.zuidema@uva.nl 
Jaap Jumelet j.w.d.jumelet@rug.nl </p>
<p>University of Amsterdam
The Netherlands</p>
<p>JAAP JUMELET
CLCG
Rijksuniversiteit GroningenThe Netherlands</p>
<p>ILLC
University of Amsterdam
The Netherlands</p>
<p>University of Amsterdam
AmsterdamThe Netherlands</p>
<p>Jaap Jumelet</p>
<p>CLCG
Rijksuniversiteit Groningen, Groningen, Willem ZuidemaThe Netherlands</p>
<p>ILLC
University of Amsterdam
AmsterdamThe Netherlands</p>
<p>Propositional Logic for Probing Generalization in Neural Networks
10 Jun 20255AB2EA4D6DA3B204BAA80B922FEFFA33arXiv:2506.08978v1[cs.LG]
The extent to which neural networks are able to acquire and represent symbolic rules remains a key topic of research and debate.Much current work focuses on the impressive capabilities of large language models, as well as their often ill-understood failures on a wide range of reasoning tasks.In this paper, in contrast, we investigate the generalization behavior of three key neural architectures -Transformers, Graph Convolution Networks and LSTMs -in a controlled task rooted in propositional logic.The task requires models to generate satisfying assignments for logical formulas, making it a structured and interpretable setting for studying compositionality.We introduce a balanced extension of an existing dataset to eliminate superficial patterns and enable testing on unseen operator combinations.Using this dataset, we evaluate the ability of the three architectures to generalize beyond the training distribution.While all models perform well in-distribution, we find that generalization to unseen patterns, particularly those involving negation, remains a significant challenge.Transformers fail to apply negation compositionally, unless structural biases are introduced.Our findings highlight persistent limitations in the ability of standard architectures to learn systematic representations of logical operators, suggesting the need for stronger inductive biases to support robust rule-based reasoning.</p>
<p>Introduction</p>
<p>The relation between neural networks and symbolic logic has been a topic of debate for decades, centered around the question whether sub-symbolic systems are able to build up robust representations of symbolic rules by learning from data.With the recent advances in deep learning, and specifically the successes of 'Large Reasoning Models' such as OpenAI-O3 and DeepSeek-R1 [5], the debate is shifting.When reasoning capabilities of state-of-the-art language models are rigorously evaluated, researchers report both impressive generalization beyond the training data and unexpected failure models, where the models seem to rely on heuristics and spurious correlations instead of learning generalizable rules [11,12,27,28,39].These unexpected failures highlight the fact that, at a fundamental level, the reasoning abilities of large neural models remain insufficiently understood.</p>
<p>Learning logic using neural networks.Using neural networks to solve exact problems differs fundamentally from using classical solvers.Neural networks often require large amounts of training data to learn a task, and even then, their outputs are not guaranteed to be correct.Therefore, a practical way to use neural methods for logic is to use them in combination with symbolic solvers.For instance, deep neural networks can be used as a variable selection heuristic [e.g . 42].Neurosymbolic methods combine reasoning with learning [4], leveraging the ability of neural networks to find complex statistical patterns, without incorporating their noisy behaviour into the output.In the current paper, however, we focus on attempts to train end-to-end neural networks to solve logical problems.An advantage of this approach is that, once the network is trained, it can be used to generate possible solutions in polynomial time.</p>
<p>One approach to teaching logic reliably to neural models is to impose the right kind of inductive bias: for example, NeuroSAT [31] uses a Graph Neural Network (GNN) to solve satisfiability of non-trivial propositional formulas.Formulas that are limited to a common format (CNF) can be represented as graphs in the network.Relevant clauses and literals are connected by edges in the input graph to the network.The GNN is trained to only detect whether the input formula is satisfiable or not, which it can do with 85% accuracy.For the majority of satisfiable formulas, an actual assignment to the variables can be extracted from the network's hidden activations, even though this was not part of the training objective.Moreover, the learned weights generalize to formulas that are larger than the network was trained on.This implies that with the right induced structure, neural networks can learn generalizable logical operations to some extent.</p>
<p>Evans et al. [6] propose a method called PossibleWorldNet that is not limited to inputs in CNF, but instead uses a custom tree-based recurrent network to solve the task of propositional logical entailment.PossibleWorldNet also generalizes to sentences of unseen lengths and variable amounts.Neither Neu-roSAT nor PossibleWorldNet explicitly provide the semantics of logic to their machine learning model, but instead make architectural changes so that the neural model may more easily learn the semantics by itself.</p>
<p>Our goals are different from these papers: instead of using neural methods to ultimately improve performance on logical tasks, we use a relatively simple dataset of propositional logic to set out to investigate how and how well neural models can learn systematic representations.This brings us to the topics of compositionality and generalization: do neural models perform well on out-of-distribution data that is longer, more difficult, or structurally different?</p>
<p>Testing for compositional generalization in neural models.Systematic generalization is a vital aspect of human cognition [7,22,26].Generalization has become an important way to assess the robustness of reasoning patterns [40], and as a general tool to understand the nature of model behaviour [14].One way of testing whether models learn to reason is by simply studying their behaviour on different distributions of data.Synthetic data allows full control over the train and test distributions, and often comes with the benefit of being fully interpretable, in contrast to objectives like language modeling or complex image classification tasks.Existing models are evaluated on challenging test sets or datapoints with minimal, systematic edits.The change in behaviour can then be used to assess how the model interprets these new inputs.If the model behaviour is correct, regardless of its impaired training, the model must have learned some generalizable, compositional information [13].</p>
<p>Various tasks to investigate compositional generalization have been proposed in recent years.Lake and Baroni [21] introduce a simple synthetic sequence-to-sequence task consisting of commands and actions (for example, the input "jump left twice after turn left" leads to output "LTURN JUMP JUMP") in which, during training, certain combinations of actions are never seen.They then show that RNNs fail to compose these unseen functions at test time.Similarly, Hupkes et al. [13] design a string editing task (for example, the input "repeat swap A B" leads to output "B A B A"), along with several different training splits, to highlight that different kinds of neural architectures generalize and fail in different ways.Kim and Linzen [18] also create a sequence-to-sequence semantic parsing task, COGS, in which the goal is to transform a sentence in English to a corresponding lambda-expression, again leaving systematic gaps in the training data.Performance of RNN-based models as well as Transformers drops drastically when the models are presented with novel examples requiring structural generalization.</p>
<p>In this paper, we introduce a synthetic dataset grounded in propositional logic.Our experimental setup departs from prior work on compositional generalization in several important ways.Unlike SCAN-like tasks, our benchmark is more deeply rooted in a well-established academic domain, propositional logic and SAT solving.Moreover, in contrast to arithmetic-based tasks, propositional variable assignment is inherently non-local, presenting a significantly greater challenge for learning systems.Finally, unlike COGS, our generalization experiments are designed such that task difficulty remains consistent across all training conditions, enabling us to more precisely isolate compositional behavior.</p>
<p>Data</p>
<p>We start our experiments by reproducing the results of Hahn et al. [8], showing that Transformer models, Graph Convolutional Networks and LSTMs can succesfully learn to generate solutions to propositional logic formulae(sections 3 and 4).After our reproduction, we move to the generalization experiments using alternative training sets (sections 5 and 6).For all our experiments, we use a dataset that is based on the PropRandom35-dataset generated by Hahn et al.Hahn et al. [8].In this section we briefly describe how the original dataset was generated, and then detail some of its limitations as well as a few small modifications.</p>
<p>Original dataset</p>
<p>The original dataset contains one million randomly generated satisfiable propositional formulas along with a (partial) assignment of variables that satisfies the formula.The task of the model is to generate a satisfying assignment given a propositional formula.</p>
<p>The target assignments are generated by the pyaiger [38] library, which is based on Glucose, a modern symbolic SAT solver [2].These assignments can be either partial or complete: a complete assignment assigns a value to every variable in the sentence.A partial assignment only assigns a value to some variables, while still satisfying the formula.For instance, {a = true} is a satisfying partial assignment to the sentence a ∨ b: this sentence will evaluate to true regardless of whether b is set to true or false.(Note that during the training of our deep learning models later on, only one target assignment is available.However, at inference time, we consider any model output that is a satisfying (partial) world for the formula correct, as there are multiple possible outputs for most formulas).</p>
<p>To avoid the need for parenthesis tokens, input formulas are given in Polish notation, also known as prefix notation: every operator precedes its arguments.This way, there is no ambiguity even without the use of parentheses.Datapoint examples can be seen in Table 1.</p>
<p>Formulas have a minimum length of 5 and a maximum length of 35.The data is balanced with respect to length: each length ≥ 8 accounts for roughly 3% of the formulas in the dataset.</p>
<p>Propositional formula</p>
<p>Input in Polish notation Output
¬a ∧ (b ∨ c) &amp; ! a | b c a 0 b 1 a ⊕ ¬e
xor a !e a 1 e 1</p>
<p>Shortcomings and modifications of the original datatset</p>
<p>We have studied the dataset made available by Hahn et al., and have found that it is a useful resource for our purposes, although the dataset also has a number of limitations that are useful to state explicitly:</p>
<p>Imbalance.Firstly, we find that the generated data consists of trees that are imbalanced in one direction: right subtrees of the formulas are significantly larger than the subtrees on the right.This leads to models favouring those imbalanced sentences, and signficantly changes the pattern of results: we find that training on imbalanced data results in a ≥25% drop in accuracy when models are tested on inverted trees.</p>
<p>For our experiments, we therefore decide to re-balance all trees, which can be done computationally efficiently by rotating subtrees of existing trees in the dataset.The process and results are described in appendix A.1.</p>
<p>Absence of double negations.Secondly, we observe that there are no double negations in the dataset: !! is an unseen pattern.We leave this portion of the dataset unchanged, but in section 5 we will return to the topic of unseen patterns, and in fact use deliberate omissions to systematically test generalization performance.</p>
<p>No unsatisfiability.A bigger shortcoming of the dataset is that there are no unsatisfiable sentences by design.The authors state: "Entailment is a subproblem of satisfiability and could be encoded in the same form as our propositional formulas.(...) In contrast [to previous methods such as NeuroSAT] we apply a generic sequenceto-sequence model to predict the solutions to formulas, not only whether there is a solution" [8].However, the model cannot output whether there is a solution, it can only output a solution.Arguably, a wrong prediction could be interpreted as the model's implicit way of predicting UNSAT, which is feasible because solution correctness can be checked quickly.We are aware that this limitation also limits the practical use of the models we train, but we have no easy fix and therefore proceed with a dataset without unsatisfiable formulas. 1arying levels of difficulty.Lastly, since the sentences are generated randomly, they are not guaranteed to be difficult.possible outputs the model can generate. 2A maximum of five variables means there are at most 242 possible outputs to choose from (of which 2 5 = 32 are complete assignments and the rest are partial assignments).This makes the task considerably easier. 3For instance, formulas starting with a disjunction account for around 25% of the dataset.These formulas are easier to guess a correct assignment for, as only one of its children has to be true for the entire sentence to be true.For each formula in the validation set, we estimate its difficulty by calculating the relative number of satisfying complete assignments, or its "possible worlds".As an example, for the sentence a ∧ b, only 1 out of 4 possible worlds is valid (a 1 b 1), resulting in a valid assignment rate of 25%.The rate of valid assignments is high: 50% on average for the validation set.A distribution can be seen in the Appendix in Figure 4(a).The majority of sentences (66%) have a valid assignment rate of 40% or higher.The number of partial assignments that are valid solutions is lower: 29% on average.Notably, longer formulas are not necessarily harder (Figure 4(b)).</p>
<p>Given the lack of a straightforward solution, we proceed with the dataset as is, but emphasize that the accuracy scores we obtain must be assessed keeping this in mind.</p>
<p>Reproduction and extension to other architectures 4.1 Experimental setup</p>
<p>We use an encoder-decoder architecture as shown in Figure 1.The encoder and decoder are both standard Transformer models [37].The model's hyperparameters are based on the best model reported by Hahn et al. [8].Both the encoder and decoder have six layers, four attention heads, and a feedforward size of 512.We use an embedding size of 128 in the encoder and one of 64 in the decoder.In addition to self-attention within each layer, there are cross-attention weights from every layer in the decoder to the encoder's final hidden states in layer six, but not to earlier encoder layers.The total number of model parameters is approximately 1.8 million.We vary the input positional encodings to be either absolute or based on tree structure [8,33].</p>
<p>Other encoder architectures</p>
<p>To compare Transformer models to other architectures, we replace the encoder of the model with a different architecture.</p>
<p>GCN.We replace the Transformer encoder with a Graph Neural Network, specifically a Graph Convolutional Network (GCN) [19].Each symbol in the input sentence is a node, that is connected to its children, its parents, and itself.he architecture of the GCN explicitly encodes tree structure into the way the encoder processes its input.To keep the architecture similar to the Transformers, we simply replace the self-attention module by a GCN block.Thus, each encoder layer consists of a GCN block, followed by LayerNorm, followed by a ReLU activation.Residual connections are also kept between the layers.After a small hyperparameter search, we find that 16 encoder layers gives the best performance on the validation set.However, even with 16 layers, this model has around 900k parameters, which is less than both the Transformer and LSTM encoder variants.</p>
<p>LSTM.We replace the Transformer encoder with a recurrent 6-layer LSTM-based encoder without attention.The decoder cross-attends to the final layer of the encoder.This allows us to test the effect of recurrence instead of self-attention.The initial hidden and cell state are learned.This model has 1.4M parameters in total, which is less than the Transformer models.Neither the LSTM nor the GCN model make use of positional encodings.</p>
<p>Training</p>
<p>The training set contains 800K examples.The models are trained for 128 epochs, which is equivalent to 200k training steps with a batch size of 512.We use the Adam optimizer and 4000 warmup steps using the Noam learning rate scheduler.The model is trained using teacher forcing.At test time, we use greedy decoding. 4The output of the model is all tokens before the first end-of-sequence token.We train at least two different model seeds for each combination of training set and model type, and report their average performance.</p>
<p>Evaluation</p>
<p>We report the performance of the base models on unseen data, both in terms of syntactic accuracy (the model output exactly matches the ground truth output) and semantic accuracy (the model output was a correct assignment, but not necessarily equal to the ground truth output).</p>
<p>Input Ground truth output Model output Correctness
&amp; ! a | b c a 0 b 1 a 1 b 0 Incorrect &amp; ! a | b c a 0 b 1 a 0 b 0 c 1 Semantically correct &amp; ! a | b c a 0 b 1 a 0 c 1 Semantically correct &amp; ! a | b c a 0 b 1 a 0 b 1 Syntactically correct</p>
<p>Results</p>
<p>The main results can be seen in Table 3.Our Transformer models achieve similar accuracy to Hahn et al. [8].Interestingly, syntactic and semantic accuracy on the training set is 57.99% and 93.77% respectively.These scores are similar to those of unseen data: this indicates that the models do not memorize the target assignments of the training data. 5odels are then tested on formulas of unseen lengths using the Prop50-dataset [8].We are able to partially reproduce the finding that tree positional encodings indeed help with length generalization.However, the semantic accuracy is lower overall.This difference could at least partially be explained by the original dataset imbalance, which is amplified in Prop50: the models of Hahn et al. [8] may have a bias for parsing and predicting imbalanced trees. 6he alternative architectures (GCN and LSTM) have slightly lower scores than the Transformer models.However, on out-of-distribution data, they perform better than the Transformer with absolute positional encodings.Table 3. Performance of all base models on unseen data.We report both syntactic and semantic accuracy.Semantically correct means that the model output is a valid partial assignment, whereas the output is syntactically correct only if the model output exactly matches the output produced by the symbolic solver.( †): We report our reproduced accuracies on the test set ofProp35Balanced; for comparison, we also give the scores from Hahn et al. [8] on the original, unbalanced test set Prop35 as listed in their paper (reported between brackets).Note that results on the unbalanced data may overestimate the learning abilities of the networks.</p>
<p>Prop35Balanced</p>
<p>Generalization to Unseen Patterns: Methods</p>
<p>Up to this point, we have successfully replicated the Hahn et al. results with Transfomrers, and extended their work to other deep learning architectures, while training on the entire train set and reporting aggregate accuracies over the entire test set.In this section, we consider the problem of how to give a more finegrained assessment of the type of generalizations that the trained models have learned or can learn.</p>
<p>The previous section already considered one aspect of generalization: length generalization (sometimes called productivity).It tests how well models understand inputs (and in the context of sequence generation, outputs) that are longer than those seen during training [e.g.6,13,31].As seen in subsection 4.5, our models (perhaps with the exception of the Transformer with absolute positional encodings) perform reasonably well when tested on Prop50: they show productive behaviour.The formulas in Prop50 provide some syntactic challenge -they are longer and may be harder to parse.However, the data is generated according to the exact same distribution, meaning statistical artifacts remain [43].There is also no distribution shift in outputs -as discussed in subsection 3.2, valid assignment rates in Prop50 are similar to the original Prop35, despite the change in length.In those terms, the dataset is not necessarily "harder", either. 7herefore, a more interesting way to measure the generalization capacities of models is to test the model on combinations of tokens that are unseen in the training phase.This type of generalization is called systematic generalization [13,21,24].If the model has truly learned the semantics of propositional logic, it should be able to combine these unseen structures, and its performance should not, or barely, decrease.</p>
<p>Since all possible combinations of tokens 8   The first four patterns all assess the genericness of the NOT-operator.However, they may not be equally difficult.For instance, a model trained without pattern (4) may still be able to perform well on it, as it still needs to learn to apply the NOT-operator to four other distinct variables during training.Another aspect is that for any model, a formula starting with the pattern !&amp; is easier to guess correctly compared to ! |, since on average, there are more worlds in which the former formula is true.All first four patterns include completely unseen sequences of symbols in the flat string.For instance, a !-token is never followed by a |-token for pattern (1).Pattern ( 5) tests an unseen flat string in the input: &amp; is never followed by !.The AND-operator can however still be seen in combination with a NOT-subtree, but only if that subtree is its righthand child.Finally, patterns ( 6) and ( 7) test for the compositionality of the AND-operator and the IFF-operator, in the same way that the first four patterns did for the NOT-operator.Leaving out AND may be less of an impairment than leaving out NOT or IFF: AND is fully compositional in the sense that the truth values of its subtrees can stay intact upon combination.</p>
<p>It is important to note that omitting any of these patterns does not reduce the semantic expressivity seen during training, as formulas containing these patterns can be expressed as differently structured sentences with the exact same possible worlds.Similar to the productive generalization split, the heldout data is not "harder" in terms of possible correct outputs.This allows us to test for systematic generalization without also changing the output distribution: the outputs do not become longer, more complex, or harder to guess: it is only the input that changes, thus, we are able to test for true structural generalization.</p>
<p>Details on the creation of different splits</p>
<p>We create these splits by either rewriting the formulas in the existing dataset to a logically equivalent formula, or by simply removing sentences that include the pattern.The rewriting approach has the advantage of maintaining the same training set size, including the ground truth outputs, without the need to generate more data.Any double negations (! !...) that occur during this rewriting process are removed, as they are not present in the original dataset.For pattern (P4) and the final two patterns (P6) and (P7), we simply remove them from the training data.</p>
<p>Templated test set</p>
<p>As described in subsection 3.2, longer formulas are not necessarily harder to solve.Furthermore, it is difficult to gauge how relevant a subpattern in a randomly generated formula is.For instance, to solve a formula with or as the main connective, the model may ignore half of the formula, which makes it harder to interpret its behaviour.To address this, we also test all models on a test set of shorter sentences constructed based on templates.</p>
<p>Using templated data, we have some control over whether a model needs to understand(/solve) the subpattern in order to understand(/solve) the whole sentence.For a set of 12 simple patterns (based on the unseen patterns), we generate sentences with the pattern in a number of contexts, to systematically test the model ability to correctly interpret the pattern in these contexts.For instance, to test whether models understand the pattern ¬(A ⊕ B) (negation applied to the xor-operator) in different contexts, we generate for instance at least the following sentences:</p>
<p>• ¬(a ⊕ c) (Basic understanding)</p>
<p>• ¬(¬e ⊕ ¬c) (Applied to negated variables)</p>
<p>• (¬(a ⊕ c)) ∧ e (Understanding in context of an and)</p>
<p>• (¬(a ⊕ c)) ↔ (¬e) (Understanding in context of an iff)</p>
<p>• (¬(a ⊕ c)) ∨ (¬e ∧ e) (Understanding in context of an or, the other half of the subtree is unsatisfiable) Table 5. Outputs for some simple formulas including the first 3 held-out patterns.</p>
<p>A full description of the templated test set is listed in Appendix A.3.Note that the templated dataset may contain instances seen during training.All models, including the GCN and LSTM, get at least 98% accuracy on the templated dataset.</p>
<p>Generalization to Unseen Patterns: Results</p>
<p>We now turn to the evaluation of the models trained on systematically different data.All seven models are evaluated on the same (original) validation set.We report performance on relevant subsets of the validation set.We will refer to models trained on the original training set, that was evaluated in subsection 4.5, as the base models.All base models, including the GCN and LSTM, reach at least 97.5% semantic accuracy on the templated test set.The results for all other models are summarized visually in Figure 2: each model is evaluated on a subset of the validation set that contains the unseen pattern.Importantly, the performance for all models stays equal on sentences where the unseen pattern is not present.</p>
<p>Transformer models.When comparing Transformer models trained without patterns (4) through (7) to the base model, the difference in performance on the unseen pattern is relatively small, or in some cases nonexistent.This indicates that these unseen patterns can be structurally combined at test time.</p>
<p>In contrast, there is a significant drop in performance when either of patterns (1), ( 2) or (3) are left out of training.These are all patterns that test the generalizability of the NOT-operator.Whereas models trained without pattern (4) are able to combine negation with an unseen variable, these models are not able to combine negation with an unseen operator.Sentences that do not contain the unseen pattern are unaffected, yielding scores similar to the base models.</p>
<p>Tree positional encodings help systematic generalization for (P2) and (P3), but performance on these patterns still lags significantly behind compared to the rest of the patterns.</p>
<p>Other architectures.The GCN and LSTM models also perform relatively well on patterns (P4) through (P7).Surprisingly, the GCN architecture lags behind on pattern (P6).Its accuracy on sentences containing the pattern AND XOR is 76%, its accuracy on sentences starting with this pattern is even lower, 57%.Are other outputs of non-generalizing models affected?All models perform well on subsets of the validation set that do not include the held-out pattern.We now investigate whether these (correct) predictions are different to those of other models.</p>
<p>All base models have around 83% overlap in their predictions, meaning only 17% of predictions are different between different seeds of the same architecture.</p>
<p>Predictions produced by the seven models with alternative training sets do not differ more than 18% from the base model or from one another, when only looking at those predictions on sentences that do not include the relevant pattern.When only comparing predictions for the subset of sentences that contain the relevant binary operator, there are no changes either.Thus, in terms of model behaviour, individual operators are not affected by the modified training set.</p>
<p>Discussion and Conclusion</p>
<p>We briefly reflect on our findings, and present an outlook for future work.We examined whether neural models trained on limited data learn the semantics of propositional logic.We designed several generalization splits based on semantic consistency.</p>
<p>When trained on i.i.d.data, Transformer models are able to produce novel correct outputs to unseen sentences with an accuracy of 94%.When replacing the Transformer encoder with a GCN or LSTM, these models still obtain an accuracy of 87%. 10ven when trained on structurally divergent data, these models demonstrate remarkable flexibility, even without specialized tree positional encodings.They are able to combine novel combinations of operators (such as (P7), in which the models are able to apply bi-implication to negated nodes (↔ to ¬), while never having seen the combination in this order) and variables (such as (P4), in which the models never saw ¬ applied to the variable b).The ability of the Transformers to represent most operators in a way that they can be combined at test time is one indication of the learnability of systematic compositionality.</p>
<p>Another indication is their capacity to generically represent variables as instances of the same type [25].</p>
<p>Interestingly, however, models do struggle on generalizing to unseen negated operators.Models that were trained without !&amp; (P1), !| (P2), or !xor (P3) all failed to apply negation to the binary operator when the pattern is reintroduced at test time.Adding explicit structure to the encoder architecture can improve performance for some patterns (such as (P2) and (P3)), but not all.Notably, the LSTM-based models also show improvements over a standard Transformer on these unseen patterns, even though they do not have access to the underlying graph structure, suggesting that recurrence is useful as an inductive bias for systematic generalization.None of the architectures trained without the pattern "not or" can solve the simple sentence ¬(a ∨ b).</p>
<p>Even though the generalization performance improves for these architectures, their performance on patterns with negation does not match the generalization performance on patterns that do not include a negated operator.This suggests that while Transformer models are able to learn generalizable representations for variables and binary operators, they fail to form a unified representation of negation.This issue extends beyond our logic-based tasks: modern LLMs, such as those from the Llama-3 or GPT-4 family, notoriously struggle to understand linguistic negation as well [17,32,35,39].The failure to learn a generalizable negation operator may suggest that not all types of systematicity can be learned by Transformers simply using next-word prediction and backpropagation.A solution to this problem may be found in alternative optimization strategies, incorporating reinforcement learning, diffusion, and neurosymbolic methods in the training of our models [36,41,42].</p>
<p>Outlook.We started our introduction citing recent work on reasoning in Large Language Models and Large Reasoning Models (LRMs); are there any lessons from our detailed case study of propositional logic learning to how we evaluate these models?One possible route is to use the variable assignment task we studied in this paper as just another "logic puzzle" to be added to the already long list of reasoning tasks that are included in benchmarks for these large models.This seems to be the approach in, for instance, LogicBench [29].But such approaches often end up, again, with summarizing the performance of models with a single score, aggregated over all the puzzles.One of the lessons from our case study was that aggregate scores hide much of the interesting differences between models.</p>
<p>Another possible route would be to apply our methodology of systematically leaving out specific patterns from the training set to test for true generalization.But such an approach runs into major computational challenges: training LRMs is so costly, that it is unfeasible to train 7 different models just to obtain insights into their generalization behavior.</p>
<p>A The Dataset A.1 Imbalance in the dataset</p>
<p>Preliminary inspection of the dataset showed that the generated formulas in Prop35Random are imbalanced in one direction: left subtrees had an average size of 2.7 tokens, whereas right subtrees were larger, containing 5.5 tokens on average.This harms the ability of the model to solve left-imbalanced trees.In a preliminary experiment, we test models trained on right-imbalanced data on inverted trees (that are left-imbalanced -inverting trees is possible because of the aforementioned commutativity of all binary operators used), which results for drops in accuracy for both absolute and tree positional encodings, as can be seen in Table 6.</p>
<p>To mitigate the subtree imbalance, we rebuild the dataset by randomly flipping 50% of the subtrees to create Prop35Balanced.Table 7 shows the average subtree sizes before and after balancing.We keep the target sequences the same, as they are still semantically correct.Note that the symbolic solver may give a different assignment out of all possible assignments. 11The model now does not have access to the exact symbolic solver output for a formula, which may improve its semantic understanding, but hurt its ability to emulate the solver.Our results from It is important to note that imbalanced trees still exist in Prop35Balanced, just not in (overwhelmingly) one direction.Models trained on this new dataset perform equally well on balanced trees and imbalanced trees: i.e., both the original imbalanced validation set, and the inverted imbalanced validation set.</p>
<p>A.2 Difficulty of the dataset</p>
<p>A.3 The templated dataset</p>
<p>We use a set of 17 templates.We use E as a placeholder for subformulas.</p>
<p>(1) E The formula itself.</p>
<p>(</p>
<p>With a limited number of variables, there are also a limited number of correctly formatted</p>
<p>Fig. 1 .
1
Fig. 1.Illustration of the model architecture and task.The decoder is a standard Transformer model, for the encoder we experiment with three model architectures: Transformers, Graph Convolutional Networks, and LSTMs.</p>
<p>Table 4 .
4
The unseen patterns, their description, and their overall occurence in the original training data.All unseen patterns consider some form of direct parent-child combinations.The first four patterns test the compositionality of the NOT-operator (!).Patterns (P5) tests unseen types of trees, and patterns (P6) and (P7) test the compositionality of operators &amp; and &lt;-&gt;.Some combinations of operators may be more challenging than others.</p>
<p>Specifically, using the DeMorgan's laws, pattern (P2) is rewritten into pattern &amp; !A ! B, and pattern (P1) is be rewritten to | !A ! B. Pattern (P3) is equivalent to &lt;-&gt;, and can also be rewritten.For pattern (P4), we rewrite any occurrences of &amp; !A ! B with !| A B, and occurrences of &amp; !A C with &amp; C ! A.</p>
<p>•</p>
<p>. . .(Combinations of the above, see Appendix A.3) Preprint, Vol. 1, Article .Publication date: June 2025.not xor Transformer-tree, Transformer-abs ¬(a ⊕ b) a 1 b 0 N (P3) not xor LSTM, GCN ¬(a ⊕ b) a 1 b 1 Y (P3) not xor Transformer-tree, GCN ¬((a ⊕ b) ⊕ e) a 1 b 1 e 1 N (P3) not xor LSTM ¬((a ⊕ b) ⊕ e) a 1 b 1 e 0 Y</p>
<p>Fig. 3 .
3
Fig.3.Unchanged predictions, per model, per pattern.Results are shown on the templated test set (top row) and PropRan-dom35 validation set (bottom row).'Modified' denotes the sentence where the relevant pattern is non-negated.When the model prediction is the same there as for the original sentence, we can say that the model is ignoring the negation operator.</p>
<p>Figure 3
3
Figure3shows an overview of these four classes of model behaviour for patterns (P1), (P2) and (P3).Transformers with absolute encodings show the most 'ignoring' behaviour for all unseen patterns.The GCN and LSTM baselines are the least likely to ignore the negation operator.</p>
<p>Figure 4 6 #
46
Figure 4  shows that length of the input formula does not correlate with the difficulty of the output in terms of possible correct outputs.</p>
<p>Fig. 4 .
4
Fig. 4. Distribution of possible worlds in the validation set.</p>
<p>( 6 ) 2 ( 10 ) 2 ( 12 ) 2 ( 14 ) 2 ( 17 )
6210212214217
) &amp; E e Ability (simple) to process in ∧-root, on the left side (3) &amp; !e E Ability (simple) to process in ∧-root, on the right side (4) &lt;-&gt; E e Ability (simple) to process in ↔ (5) | E &amp; e ! e Ability (simple) to process in ∨-root (other half is UNSAT) xor e E Ability to process in ⊕-root, on the right side (7) &lt;-&gt; E e Ability to process in ↔-root, on the left side (8) !xor E e Nested ¬⊕ (9) !xor ! e E Nested ¬⊕, !&amp; !e E Nested ¬∧ (11) !&amp; e E Nested ¬∧, !| E e Nested ¬∨ (13) !| ! e E Nested ¬∨, &amp; !E e Nested ∧¬ (15) &amp; e xor E e Nested ∧⊕ (16) &amp; xor E ! e e Nested ∧⊕, &lt;-&gt; !E e Nested ↔ ¬ We then define 13 possible subformulas E. They include all the possible held-out patterns.The variables A and B are used as placeholders for actual variables or small formulas, where A ∈ { a, e, !e, &amp; a e, !&amp; a e, | !e !a } and B ∈ {b, c, !c, &amp; c d, xor b c, &lt;-&gt; c b}.(i) !xor A B (ii) !&lt;-&gt; A B (iii) !| A B (iv) !&amp; A B (v) xor A B (vi) &lt;-&gt; A B(vii) | A B (viii) &amp; !A B (ix) &amp; !A ! B Preprint, Vol. 1, Article .Publication date: June 2025.</p>
<p>Fig. 6 .
6
Fig.6.Generalization on the normal test set, for all architectures.Each model (except the base models) is evaluated on sentences containing its respective held-out pattern..</p>
<p>Table 1 .
1
Example datapoints.Outputs are always alphabetically sorted.Note that the first assignment is partial: the value of c may be either 0 or 1, and may therefore be omitted.
The formulas contain at most five propositional variables (a, b, c, d, e) and are constructed using fivepropositional operators: NOT (¬/!), AND (∧/&amp;), OR (∨/|), IFF (↔/&lt;-&gt;) and XOR (⊕/xor). Note that allbinary operators are commutative, since operators such as implication (→) are not included, meaningAϕB ≡ BϕA holds for all operators ϕ.</p>
<p>Table 2 .
2
Evaluation of model outputs for a single example formula (¬a ∧ (b ∨ c)).The model output can be semantically correct without exactly matching the given SAT solver output.</p>
<p>are present in Prop35, we design seven alternative training sets.For each new training set, exactly one pattern is left out of the model training data.Crucially, we do use the pattern at test time, to assess the extent to which the trained models can generalize to this unseen pattern.The set of patterns for the generalization splits is listed in Table4.The variety of patterns allows us to test for multiple kinds of systematic generalization.</p>
<h1>PatternDescription%(P1) ! &amp;A negated AND-node41.06(P2) ! |A negated OR-node40.96(P3) ! xorA negated XOR-node24.56(P4) ! bA negated b-node.47.42</h1>
<p>(P5) &amp; !An AND-node with a left child that is negated 52.07(P6) &amp; xor _ C, &amp; Cxor _ An AND-node with a child that is a XOR-node 25.93 (P7) &lt;-&gt; !A B, &lt;-&gt; A ! B An IFF-node with a child that is negated 49.21</p>
<p>Table 6 .
6
Table 6however show that there is no drop in accuracy, Performance (semantic accuracy) of models trained on right-imbalanced data (first two rows) versus models trained on balanced data (third row).The right-imbalanced models have an advantage on right-imbalanced Prop50, but do not perform well on balanced data.suggesting that rewriting the data, even in a way that would change ground truth outputs, does not harm logical understanding.
Model (training set)Performance on validation setProp35 Prop35-Inverted Prop35Balanced Prop50Abs. (Prop35)94.556.671.273.7Tree (Prop35)93.768.581.280.3Abs. (Prop35Balanced)94.594.694.667.9Prop35Prop50Prop35BalancedLeft Right Left RightLeftRightRoot node sizes 6.12 14.03 14.726.7 10.0710.08All node sizes2.735.463.76.94.054.05</p>
<p>Table 7 .
7
Average subtree sizes of the training set, before and after balancing.</p>
<p>Preprint, Vol. 1, Article . Publication date: June 2025.
Preprint, Vol. 1, Article . Publication date: June 2025. Propositional Logic for Probing Generalization in Neural Networks • 5
In preliminary experiments, adding unsatisfiable formulas to the training set (along with a special UNSAT prediction token) did not significantly affect performance of the models trained on all data, nor the generalization capacities of models trained on the systematically different training sets.Preprint, Vol. 1, Article . Publication date: June
.
This is in contrast to the trace generation task for Temporal Logic[8]. Traces can have arbitrary length, making a sequence-tosequence setup more well-suited.
3 In other propositional logic datasets, the number of variables can range from twenty to over a hundred[6,31].Preprint, Vol. 1, Article . Publication date: June 2025. Propositional Logic for Probing Generalization in Neural Networks • 7
Beam search is used in Hahn et al.[8], but, during preliminary experiments, a beam search with K ∈ {5, 10} yielded only very small improvements for our models (approximately +1% accuracy).
This holds for models trained on Prop35 as well as Prop35Balanced.
Also see subsection A.1. Preprint, Vol. 1, Article . Publication date: June 2025. Propositional Logic for Probing Generalization in Neural Networks • 9
Note that for some logic datasets, such as ones that are constrained to Conjunctive Normal Form (CNF), there does exist a strong correlation between length and possible-world difficulty -which is why it is more impressive for models trained on this data to generalize productively[31].
Except for double negation (! !).Preprint, Vol. 1, Article . Publication date: June 2025.
We cannot completely exclude the possibility that the lower accuracy of GCNs and LSTMs is due to a relatively limited hyperparameter optimization for these models. We did our best to find the best possible settings, but only for the Transformer models we could make fruitful use of the hyperparameters found by Hahn et al. [8]. Preprint, Vol. 1, Article . Publication date: June 2025. Propositional Logic for Probing Generalization in Neural Networks • 15
This happens in about 50% of datapoints in the validation set. Preprint, Vol. 1, Article . Publication date: June 2025.
None of the architectures reliably solve pattern (P1).None of the models produce correct outputs for the simplest sentences for pattern (P1), although the models seem to produce different wrong answers, as shown in Table5.Both the GCN and LSTM show some improvement over Transformers on pattern (P2), and the LSTM outperforms all models on pattern (P3).Behavioural resultsAre non-generalizing models simply ignoring the NOT-operator in unseen combinations?We test whether a model ignores the negation-operator by replacing any occurences of ¬ϕ with ϕ, and checking if the model behaviour changes on this new input.At this point, we classify model behaviour into four categories.After introducing the negation operator, the model either:(A) Correct(ly changes its prediction) 9 -correctly processing the negation-operator (B) Does not change its prediction (ignores the negation-operator) (C) Changes its prediction to an alternative output for the original sentence (may be ignoring the negation-operator) (D) Changes its prediction to something else 9 Note that correct possible worlds for the original and modified sentence may overlap.In this case it is not necessary to change the prediction, the result will be counted as correct.Preprint, Vol. 1, Article .Publication date: June 2025.How may we then systematically test generalization behavior of LRMs on logic puzzles, without retraining these enormous models?We suggest a possible third route is to study small and large model side by side, and use the small models as surrogate models to predict the generalization behavior of large models.Such an approach would need a variety of candidate surrogate models, and a variety of techniques from the field of interpretability to investigate which surrogate model best approaches the learned solutions for a specific task in the large models.For propositional logic variable assignment, the candidate surrogate models could, perhaps, include the 4 models we have presented in our case study; to measure how similar their model internals are to large models, techniques such as Centered Kernel Alignment[20]and automatic circuit discovery[3]could prove useful.We leave such exciting new research directions for future work.For example, template(14)with chosen E = subformula (vii) yields 36 possible formulas:We generate all possible combinations of E, A and B for each template.For each resulting formula containing ¬ψ (ψ ∈ {∨, ∧, ⊕}), we also add the formula in which instances of ¬ψ are replaced with ψ¬. 12 If a resulting formula contains a double negation or is unsatisfiable, we remove it.We remove any duplicate formulas.The final templated test set contains 8301 possible formulas.A.4 Detailed experimental setupFor all Transformer and GCN models, we use a learning rate of 1e −4 .For the LSTMs, we use a learning rate of 5e −5 .For the Transformer models trained with absolute positional encodings and the LSTMs, every input sentence is wrapped between two extra tokens indicating the beginning-of-sequence and end-of-sequence.The outputs for all models are followed by an end-of-sequence token.12 This allowed us to run additional behavioural tests to see whether models trained without ¬ψ systematically produce (incorrect) answers that correspond to the modified formula containing only ψ¬, instead.After preliminary experiments, however, during preliminary experiments, we find that the 'ignore' hypothesis (results in Figure3) is a better explanation of model behaviour in terms of systematic mistakes in the output.
Transferring Inductive Biases through Knowledge Distillation. CoRR abs. Samira Abnar, Mostafa Dehghani, Willem H Zuidema, arXiv:2006.005552020. 2006. 2020555</p>
<p>On the Glucose SAT Solver. Gilles Audemard, Laurent Simon, 10.1142/S0218213018400018International Journal on Artificial Intelligence Tools. 2718400012018. Feb. 2018</p>
<p>Towards automated circuit discovery for mechanistic interpretability. Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Artur D'avila Garcez, Sebastian Bader, Howard Bowman, Luis C Lamb, Leo De Penning, B V Illuminoo, Hoifung Poon, Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. Neuro-Symbolic Artificial Intelligence: The State of the Art. 2022. 2022342327</p>
<p>. Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J L Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R J Chen, R L Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, S S Shuting Pan, Shuang Li, Shaoqing Zhou, Shengfeng Wu, Tao Ye, Tian Yun, Tianyu Pei, T Sun, Wangding Wang, Wanjia Zeng, Wen Zhao, Wenfeng Liu, Wenjun Liang, Wenqin Gao, Wentao Yu, W L Zhang, Wei Xiao, Xiaodong An, Xiaohan Liu, Xiaokang Wang, Xiaotao Chen, Xin Nie, Xin Cheng, Xin Liu, Xingchao Xie, Xinyu Liu, Xinyuan Yang, Xuecheng Li, Xuheng Su, X Q Lin, Xiangyue Li, Xiaojin Jin, Xiaosha Shen, Xiaowen Chen, Xiaoxiang Sun, Xinnan Wang, Xinyi Song, Xianzu Zhou, Xinxia Wang, Y K Shan, Y Q Li, Y X Wang, Yang Wei, Yanhong Zhang, Yao Xu, Yao Li, Yaofeng Zhao, Yaohui Sun, Yi Wang, Yichao Yu, Yifan Zhang, Yiliang Shi, Ying Xiong, Yishi He, Yisong Piao, Yixuan Wang, Yiyang Tan, Yiyuan Ma, Yongqiang Liu, Yuan Guo, Yuduan Ou, Yue Wang, Yuheng Gong, Yujia Zou, Yunfan He, Yuxiang Xiong, Yuxiang Luo, Yuxuan You, Yuyang Liu, Y X Zhou, Yanhong Zhu, Yanping Xu, Yaohui Huang, Yi Li, Yuchen Zheng, Yunxian Zhu, Ying Ma, Yukun Tang, Yuting Zha, Z Z Yan, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhenda Xu, Zhengyan Xie, Zhewen Zhang, Zhicheng Hao, Zhigang Ma, Zhiyu Yan, Zihui Wu, Zijia Gu, Zijun Zhu, Zilin Liu, Ziwei Li, Xie, 10.48550/arXiv.2501.12948arXiv:2501.12948Ziyang Song, Zizheng PanZhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</p>
<p>Richard Evans, David Saxton, David Amos, Pushmeet Kohli, Edward Grefenstette, Can Neural Networks Understand Logical Entailment? 6th International Conference on Learning Representations, ICLR 2018 -Conference Track Proceedings. 2018. 2018</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, 10.1016/0010-0277(88)90031-5Cognition. 281988. 1988</p>
<p>Teaching Temporal Logics to Neural Networks. Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus N Rabe, Bernd Finkbeiner, International Conference on Learning Representations. Virtual Event. Austria2021. May 3-7, 2021</p>
<p>. Preprint, Article . Publication date. 1June 2025</p>
<p>Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity. Yiding Hao, Dana Angluin, Robert Frank, 10.1162/tacl_a_00490Transactions of the Association for Computational Linguistics. 102022. July 2022</p>
<p>Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition. Rishi Hazra, Gabriele Venturato, Pedro Zuidberg Dos Martires, Luc De, Raedt , 10.48550/arXiv.2504.03930arXiv:2504.039302025</p>
<p>Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning. Chadi Helwe, Chloe Clavel, Fabian Suchanek, 10.24432/C5W3003rd Conference on Automated Knowledge Base Construction. 2021</p>
<p>An Analysis of Natural Language Inference Benchmarks through the Lens of Negation. Md Mosharaf Hossain, Venelin Kovatchev, Pranoy Dutta, Tiffany Kao, Elizabeth Wei, Eduardo Blanco, 10.18653/v1/2020.emnlp-main.732Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberOnline2020</p>
<p>Compositionality Decomposed: How Do Neural Networks Generalise. Dieuwke Hupkes, Verna Dankers, Mathijs Mul, Elia Bruni, 10.1613/jair.1.11674Journal of Artificial Intelligence Research. 672020. 2020</p>
<p>State-of-the-art generalisation research in NLP: a taxonomy and review. Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell, Zhijing Jin, 2022. 2022CoRR</p>
<p>Visualisation and 'diagnostic Classifiers' Reveal How Recurrent and Recursive Neural Networks Process Hierarchical Structure. Dieuwke Hupkes, Willem Zuidema, IJCAI International Joint Conference on Artificial Intelligence. 2018. 2018-July (2018</p>
<p>Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution. Jaap Jumelet, Willem Zuidema, arXiv:2310.148402023</p>
<p>Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk. Nora Kassner, Hinrich Schütze, arXiv:1911.03343But Cannot Fly. 2020</p>
<p>COGS: A Compositional Generalization Challenge Based on Semantic Interpretation. Najoung Kim, Tal Linzen, 10.18653/v1/2020.emnlp-main.731Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online2020</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, 10.48550/arXiv.1609.02907arXiv:1609.029072017cs, stat</p>
<p>Similarity of neural network representations revisited. Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton, International conference on machine learning. PMLR2019</p>
<p>Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. Brenden Lake, Marco Baroni, 35th International Conference on Machine Learning, ICML 2018. 20187</p>
<p>Building Machines That Learn and Think Like People. M Brenden, Lake, D Tomer, Joshua B Ullman, Samuel J Tenenbaum, Gershman, 10.1017/S0140525X16001837Behavioral and Brain Sciences. 20122016. 2016</p>
<p>Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He, arXiv:2505.19641[cs.AISynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond. 2025</p>
<p>Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks. João Loula, Marco Baroni, Brenden Lake, 10.18653/v1/w18-54132019. 2019</p>
<p>The Algebraic Mind: Integrating Connectionism and Cognitive Science. G F Marcus, 2003MIT Press</p>
<p>Rethinking Eliminative Connectionism. F Gary, Marcus, 10.1006/cogp.1998.0694Cognitive Psychology. 371998. 1998</p>
<p>R Thomas Mccoy, Ellie Pavlick, Tal Linzen, Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. 2019. 20192</p>
<p>Philipp Mondorf, Barbara Plank, arXiv:2404.01869Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -A Survey. 2024</p>
<p>LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, 10.18653/v1/2024.acl-long.739Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability. Kyle Richardson, Ashish Sabharwal, arXiv:2112.090542021</p>
<p>. Preprint, Article . Publication date. 1June 2025</p>
<p>Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo De Moura, David L Dill, arXiv:1802.03685Learning a SAT Solver from Single-Bit Supervision. 2019</p>
<p>ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning. Jingyuan S She, Christopher Potts, Samuel R Bowman, Atticus Geiger, 10.18653/v1/2023.acl-short.154Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20232Short Papers)</p>
<p>Novel Positional Encodings to Enable Tree-Based Transformers. Leonardo Vighnesh, Chris Shiv, Quirk, 2019. 201911</p>
<p>Scaling Laws vs Model Architectures: How Inductive Bias Influence Scaling. Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Tran, Dani Yogatama, Donald Metzler, 10.18653/v1/2023.findings-emnlp.825Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Language Models Are Not Naysayers: An Analysis of Language Models on Negation Benchmarks. Hung Thinh, Timothy Truong, Karin Baldwin, Trevor Verspoor, Cohn, 10.18653/v1/2023.starsem-1.10Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (<em>SEM 2023. Alexis Palmer, Jose Camacho-Collados, the 12th Joint Conference on Lexical and Computational Semantics (</em>SEM 2023Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Emile Van Krieken, Pasquale Minervini, Edoardo Ponti, Antonio Vergari, arXiv:2505.13138[cs.LGNeurosymbolic Diffusion Models. 2025</p>
<p>Attention Is All You Need. Ashish Vaswani, Google Brain, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems Nips. 2017. 2017</p>
<p>. Marcell Vazquez, -Chanlatte , Markus N Rabe, 10.5281/zenodo.14057812018mvcisback/py-aiger: v2.0.0.</p>
<p>A &amp; B == B &amp; A: Triggering Logical Reasoning Failures in Large Language Models. Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-Tse Huang, Pinjia He, Wenxiang Jiao, Michael R Lyu, 10.48550/arXiv.2401.00757arXiv:2401.007572024</p>
<p>Sean Welleck, Peter West, Jize Cao, Yejin Choi, arXiv:2109.13986Symbolic Brittleness in Sequence Models: On Systematic Generalization in Symbolic Mathematics. 2022</p>
<p>Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning. Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong, The Thirteenth International Conference on Learning Representations, ICLR 2025. Singapore2025. April 24-28, 2025</p>
<p>. Emre Yolcu, Barnabás Póczos, n. d.</p>
<p>Learning Local Search Heuristics for Boolean Satisfiability. </p>
<p>On the Paradox of Learning to Reason from Data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, arXiv:2205.115022022</p>            </div>
        </div>

    </div>
</body>
</html>
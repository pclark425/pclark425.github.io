<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8644 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8644</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8644</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273549777</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.18136v1.pdf" target="_blank">Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Designing functional transition metal complexes (TMCs) faces challenges due to the vast search space of metals and ligands, requiring efficient optimization strategies. Traditional genetic algorithms (GAs) are commonly used, employing random mutations and crossovers driven by explicit mathematical objectives to explore this space. Transferring knowledge between different GA tasks, however, is difficult. We integrate large language models (LLMs) into the evolutionary optimization framework (LLM-EO) and apply it in both single- and multi-objective optimization for TMCs. We find that LLM-EO surpasses traditional GAs by leveraging the chemical knowledge of LLMs gained during their extensive pretraining. Remarkably, without supervised fine-tuning, LLMs utilize the full historical data from optimization processes, outperforming those focusing only on top-performing TMCs. LLM-EO successfully identifies eight of the top-20 TMCs with the largest HOMO-LUMO gaps by proposing only 200 candidates out of a 1.37 million TMCs space. Through prompt engineering using natural language, LLM-EO introduces unparalleled flexibility into multi-objective optimizations, thereby circumventing the necessity for intricate mathematical formulations. As generative models, LLMs can suggest new ligands and TMCs with unique properties by merging both internal knowledge and external chemistry data, thus combining the benefits of efficient optimization and molecular generation. With increasing potential of LLMs as pretrained foundational models and new post-training inference strategies, we foresee broad applications of LLM-based evolutionary optimization in chemistry and materials design.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8644.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8644.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-EO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-based Evolutionary Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A workflow that integrates pretrained large language models into an evolutionary optimization loop to propose, evaluate, and iteratively improve transition metal complexes (TMCs) via prompt-based few-shot generation and retention of historical evaluation data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>framework (uses commercial LLM checkpoints such as o1-preview, claude-3.5-sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-based evolutionary optimization using transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not fine-tuned on TMCs; relies on LLMs pretrained on large, diverse text corpora (commercial checkpoints).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials chemistry — design of transition metal complexes (Pd(II) square planar TMCs); multi-objective materials optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based few-shot generation and iterative proposal (direct few-shot proposals, iterative LLM proposals merged with evaluated data into prompts), retention strategies include top-k retention or keeping all historical evaluated records; also used to generate new ligands (one-shot) and de novo TMCs (iterative).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Enables generation beyond an initial enumerated ligand pool; produced novel ligands/TMCs outside the original 50-ligand library and constructed ~2,200 new TMCs from 10 LLM-generated ligands; quantitative novelty metrics relative to training not reported, but generated TMCs exceeded property bounds of the predefined 1.37M space (e.g., polarisability up to >1200 a.u.).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Guided by natural-language objective statements in prompts (e.g., maximize HOMO-LUMO gap, maximize polarisability, Pareto frontier expansion, maximize polarisability with HOMO-LUMO <1 eV); specificity enforced by including evaluated examples and property constraints in the prompt and by retaining historical evaluations to steer proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Quantum-derived properties (HOMO-LUMO gap, polarizability computed via molSimplify + GFN2-xTB), validity and uniqueness of proposed molecules, counts of hits in top-k of the full 1.37M space, Pareto frontier Area Under Curve (AUC) comparisons, distribution shifts, fraction meeting constraints (e.g., HOMO-LUMO <1 eV), and product of properties for multi-objective scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM-EO substantially outperformed random selection and genetic algorithms (GA) on multiple tasks: with limited evaluations (e.g., 200-400 proposals) LLM-EO found candidates in extreme tails of property distributions (e.g., TMCs with HOMO-LUMO >4.45 eV in top 0.002% of the 1.37M space). Retaining all historical evaluations improved performance; with 200 proposals LLM-EO (with o1-preview) recovered 8 of the top-20 HOMO-LUMO-gap TMCs from the 1.37M space. In multi-objective tasks, LLM-EO found nine TMCs on the true Pareto frontier within 400 proposals and identified many top candidates satisfying complex constraints. Generative modes produced TMCs with polarisabilities twice as high as the best in the original 1.37M enumeration and a single TMC >1200 a.u.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to GA and random sampling, LLM-EO required far fewer evaluations to find top candidates and produced superior top-20 performance and Pareto frontier coverage; GA lagged behind especially at early iterations. LLM-EO also enabled de novo ligand generation beyond the enumerated space, a capability not present in baseline GA enumeration experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Performance is strongly model-dependent; relies on quality of pretrained LLM and post-training inference mechanisms. No supervised fine-tuning was performed, so success depends on LLM internal chemical knowledge. Prompt engineering is required (though authors report robustness within reason). Some objectives (e.g., further increasing HOMO-LUMO gap) are physically constrained by chemistry (Pd(II) square planar gap ~5 eV). Geometry optimization failures (bad geometry or connectivity changes) occur and are treated as failed attempts; such failures are included in prompts to teach LLMs, but they indicate practical limits. Training data details of commercial LLMs are unavailable; potential equity and reproducibility concerns noted (commercial vs open-source model performance differences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8644.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8644.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o1-preview (OpenAI commercial checkpoint with post-training inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial foundational LLM checkpoint used without supervised fine-tuning; reported as the best-performing model in LLM-EO for proposing high-quality TMCs in few-shot and iterative settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>foundational transformer-based LLM with post-training inference (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large diverse corpora (proprietary); no supervised fine-tuning on TMCs performed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Design and generative optimization of transition metal complexes (Pd(II) square planar TMCs) for properties such as HOMO-LUMO gap and polarizability.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based few-shot generation (given SMILES of 50 ligands and 20 example TMCs with properties) and iterative proposal within the LLM-EO loop; also used to generate novel ligands (one-shot) and new TMCs iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generated ligands and TMCs outside the original enumerated 50-ligand pool and produced TMCs with properties beyond the original 1.37M space (e.g., minimum polarisability from generated-ligand space 447 a.u.; top polarisability >1200 a.u.). Exact overlap with any training set not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Prompts encoded specific property targets, constraints, and example TMCs; retention of evaluated examples (top-k or all historical) used to refine specificity iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>HOMO-LUMO gap and polarisability computed via molSimplify + GFN2-xTB, validity/uniqueness rates, hits in top-k of the full enumerated space, Pareto frontier AUC, and property-product metrics for multi-objective tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>o1-preview, within LLM-EO, identified extremely high-quality candidates: with 400 proposals it found TMCs with HOMO-LUMO >4.45 eV (in top 0.002% of 1.37M) and, when retaining all history, recovered 8 out of the top-20 HOMO-LUMO-gap TMCs after evaluating only 200 candidates. It produced many valid/unique proposals (>71% validity/uniqueness reported) and yielded Pareto frontier and constrained multi-objective successes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed GA and random baselines in both single-objective and multi-objective optimization tasks; outperformed other tested LLM checkpoints (gpt-4o, o1-mini) in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Model size and exact pretraining corpora unknown; performance depends on post-training inference mechanisms. While effective, o1-preview sometimes emphasized polarisability over HOMO-LUMO in de novo generative tasks (likely due to property scaling), and generated proposals can fail geometry optimization steps (practical synthetic/structural validity constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8644.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8644.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>claude-3.5-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial LLM checkpoint from Anthropic used in few-shot proposals and LLM-EO experiments; demonstrated strong performance in proposing TMCs with large HOMO-LUMO gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>claude-3.5-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>foundational transformer-based LLM (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large text corpora (proprietary); not fine-tuned on the TMC dataset for these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Design of transition metal complexes (Pd(II) square planar TMCs) with target properties like HOMO-LUMO gap.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based few-shot generation (provided ligand SMILES and example TMCs with properties) and used within iterative LLM-EO experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Proposed TMCs within and beyond the enumerated search space; generated valid and unique proposals at >71% rates in few-shot tests. Exact novelty vs training data unreported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specific objectives encoded via natural-language prompts and examples; improved performance when increasing the number of known initial TMCs provided in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>HOMO-LUMO gap and polarisability via molSimplify + GFN2-xTB, validity/uniqueness, top-20 distribution metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>In few-shot settings, claude-3.5-sonnet produced top-20 TMCs with significantly larger HOMO-LUMO gaps than GA and random baselines, and performance improved with more example TMCs in the prompt. It achieved high validity/uniqueness rates and identified mechanisms (e.g., preference for electron-withdrawing/strong-field ligands) consistent with chemical knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed GA and random selection baselines; performed comparably to or slightly below o1-preview depending on the amount of few-shot data, but overall was among the best-performing LLMs tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Model-dependent behavior observed; required prompt tuning initially. Exact model internals, size, and training corpora are proprietary and not specified. Performance gains depended on number of examples in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8644.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8644.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o (OpenAI foundational model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A top-performing foundational model (not optimized for post-training reasoning) evaluated as a baseline LLM in LLM-EO experiments but performed worse than o1-preview and claude-3.5-sonnet on the TMC tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>foundational transformer-based LLM (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large text corpora (proprietary); no supervised fine-tuning on TMCs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Tested for proposing transition metal complexes (Pd(II) square planar TMCs) within LLM-EO</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based few-shot generation within the LLM-EO framework.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generated proposals but with lower quality/metrics compared to o1-preview and claude-3.5-sonnet; no quantitative novelty vs training data reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Prompt-specified objectives similar to other LLMs but showed limited ability to translate few-shot examples to high-quality proposals in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>HOMO-LUMO gap, polarisability, validity/uniqueness, top-20 distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>gpt-4o produced significantly worse results across evaluation metrics (top-20 HOMO-LUMO gaps, validity/uniqueness, distribution shifts) compared to o1-preview and claude-3.5-sonnet in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Underperformed relative to best commercial checkpoints (o1-preview, claude-3.5-sonnet) and also lagged behind GA on certain metrics in early iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Authors attribute worse performance to lacking post-training inference mechanisms (reasoning enhancements) used by o1-preview and possibly to differences in chemical knowledge captured during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8644.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8644.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o1-mini (OpenAI smaller foundational model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller OpenAI foundational LLM checkpoint using similar post-training inference techniques as o1-preview but with reduced capacity; evaluated and found to underperform for TMC proposal tasks relative to larger checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>smaller foundational transformer-based LLM (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large corpora (proprietary); not fine-tuned on TMCs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Tested for proposing TMCs within LLM-EO framework.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based few-shot generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Produced proposals with substantially lower quality metrics (fewer high-gap TMCs, lower validity/uniqueness) than larger models; novelty vs training not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Followed prompt objectives but struggled to generate high-quality, application-specific TMCs compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>HOMO-LUMO gap, polarisability, validity/uniqueness, top-20 metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>o1-mini performed significantly worse across all evaluation metrics compared to o1-preview and claude-3.5-sonnet, indicating capacity/size limitations for these chemical design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Underperformed relative to larger commercial LLMs and was generally inferior to GA and best LLM-EO configurations in terms of top candidate discovery efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Smaller model capacity likely reduces chemical knowledge and reasoning abilities in few-shot/in-context learning for complex multi-block chemical design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient evolutionary search over chemical space with large language models <em>(Rating: 2)</em></li>
                <li>Multi-objective Evolution of Heuristic Using Large Language Model <em>(Rating: 2)</em></li>
                <li>Accelerated chemical science with ai <em>(Rating: 1)</em></li>
                <li>Generative Models as an Emerging Paradigm in the Chemical Sciences <em>(Rating: 1)</em></li>
                <li>Leveraging large language models for predictive chemistry <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8644",
    "paper_id": "paper-273549777",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "LLM-EO",
            "name_full": "Large Language Model-based Evolutionary Optimization",
            "brief_description": "A workflow that integrates pretrained large language models into an evolutionary optimization loop to propose, evaluate, and iteratively improve transition metal complexes (TMCs) via prompt-based few-shot generation and retention of historical evaluation data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "framework (uses commercial LLM checkpoints such as o1-preview, claude-3.5-sonnet)",
            "model_type": "prompt-based evolutionary optimization using transformer LLMs",
            "model_size": null,
            "training_data": "Not fine-tuned on TMCs; relies on LLMs pretrained on large, diverse text corpora (commercial checkpoints).",
            "application_domain": "Materials chemistry — design of transition metal complexes (Pd(II) square planar TMCs); multi-objective materials optimization.",
            "generation_method": "Prompt-based few-shot generation and iterative proposal (direct few-shot proposals, iterative LLM proposals merged with evaluated data into prompts), retention strategies include top-k retention or keeping all historical evaluated records; also used to generate new ligands (one-shot) and de novo TMCs (iterative).",
            "novelty_of_chemicals": "Enables generation beyond an initial enumerated ligand pool; produced novel ligands/TMCs outside the original 50-ligand library and constructed ~2,200 new TMCs from 10 LLM-generated ligands; quantitative novelty metrics relative to training not reported, but generated TMCs exceeded property bounds of the predefined 1.37M space (e.g., polarisability up to &gt;1200 a.u.).",
            "application_specificity": "Guided by natural-language objective statements in prompts (e.g., maximize HOMO-LUMO gap, maximize polarisability, Pareto frontier expansion, maximize polarisability with HOMO-LUMO &lt;1 eV); specificity enforced by including evaluated examples and property constraints in the prompt and by retaining historical evaluations to steer proposals.",
            "evaluation_metrics": "Quantum-derived properties (HOMO-LUMO gap, polarizability computed via molSimplify + GFN2-xTB), validity and uniqueness of proposed molecules, counts of hits in top-k of the full 1.37M space, Pareto frontier Area Under Curve (AUC) comparisons, distribution shifts, fraction meeting constraints (e.g., HOMO-LUMO &lt;1 eV), and product of properties for multi-objective scoring.",
            "results_summary": "LLM-EO substantially outperformed random selection and genetic algorithms (GA) on multiple tasks: with limited evaluations (e.g., 200-400 proposals) LLM-EO found candidates in extreme tails of property distributions (e.g., TMCs with HOMO-LUMO &gt;4.45 eV in top 0.002% of the 1.37M space). Retaining all historical evaluations improved performance; with 200 proposals LLM-EO (with o1-preview) recovered 8 of the top-20 HOMO-LUMO-gap TMCs from the 1.37M space. In multi-objective tasks, LLM-EO found nine TMCs on the true Pareto frontier within 400 proposals and identified many top candidates satisfying complex constraints. Generative modes produced TMCs with polarisabilities twice as high as the best in the original 1.37M enumeration and a single TMC &gt;1200 a.u.",
            "comparison_to_other_methods": "Compared to GA and random sampling, LLM-EO required far fewer evaluations to find top candidates and produced superior top-20 performance and Pareto frontier coverage; GA lagged behind especially at early iterations. LLM-EO also enabled de novo ligand generation beyond the enumerated space, a capability not present in baseline GA enumeration experiments.",
            "limitations_and_challenges": "Performance is strongly model-dependent; relies on quality of pretrained LLM and post-training inference mechanisms. No supervised fine-tuning was performed, so success depends on LLM internal chemical knowledge. Prompt engineering is required (though authors report robustness within reason). Some objectives (e.g., further increasing HOMO-LUMO gap) are physically constrained by chemistry (Pd(II) square planar gap ~5 eV). Geometry optimization failures (bad geometry or connectivity changes) occur and are treated as failed attempts; such failures are included in prompts to teach LLMs, but they indicate practical limits. Training data details of commercial LLMs are unavailable; potential equity and reproducibility concerns noted (commercial vs open-source model performance differences).",
            "uuid": "e8644.0",
            "source_info": {
                "paper_title": "Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "o1-preview",
            "name_full": "o1-preview (OpenAI commercial checkpoint with post-training inference)",
            "brief_description": "A commercial foundational LLM checkpoint used without supervised fine-tuning; reported as the best-performing model in LLM-EO for proposing high-quality TMCs in few-shot and iterative settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o1-preview",
            "model_type": "foundational transformer-based LLM with post-training inference (OpenAI)",
            "model_size": null,
            "training_data": "Pretrained on large diverse corpora (proprietary); no supervised fine-tuning on TMCs performed in this work.",
            "application_domain": "Design and generative optimization of transition metal complexes (Pd(II) square planar TMCs) for properties such as HOMO-LUMO gap and polarizability.",
            "generation_method": "Prompt-based few-shot generation (given SMILES of 50 ligands and 20 example TMCs with properties) and iterative proposal within the LLM-EO loop; also used to generate novel ligands (one-shot) and new TMCs iteratively.",
            "novelty_of_chemicals": "Generated ligands and TMCs outside the original enumerated 50-ligand pool and produced TMCs with properties beyond the original 1.37M space (e.g., minimum polarisability from generated-ligand space 447 a.u.; top polarisability &gt;1200 a.u.). Exact overlap with any training set not reported.",
            "application_specificity": "Prompts encoded specific property targets, constraints, and example TMCs; retention of evaluated examples (top-k or all historical) used to refine specificity iteratively.",
            "evaluation_metrics": "HOMO-LUMO gap and polarisability computed via molSimplify + GFN2-xTB, validity/uniqueness rates, hits in top-k of the full enumerated space, Pareto frontier AUC, and property-product metrics for multi-objective tasks.",
            "results_summary": "o1-preview, within LLM-EO, identified extremely high-quality candidates: with 400 proposals it found TMCs with HOMO-LUMO &gt;4.45 eV (in top 0.002% of 1.37M) and, when retaining all history, recovered 8 out of the top-20 HOMO-LUMO-gap TMCs after evaluating only 200 candidates. It produced many valid/unique proposals (&gt;71% validity/uniqueness reported) and yielded Pareto frontier and constrained multi-objective successes.",
            "comparison_to_other_methods": "Outperformed GA and random baselines in both single-objective and multi-objective optimization tasks; outperformed other tested LLM checkpoints (gpt-4o, o1-mini) in these experiments.",
            "limitations_and_challenges": "Model size and exact pretraining corpora unknown; performance depends on post-training inference mechanisms. While effective, o1-preview sometimes emphasized polarisability over HOMO-LUMO in de novo generative tasks (likely due to property scaling), and generated proposals can fail geometry optimization steps (practical synthetic/structural validity constraints).",
            "uuid": "e8644.1",
            "source_info": {
                "paper_title": "Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "claude-3.5-sonnet",
            "name_full": "Claude 3.5 Sonnet (Anthropic)",
            "brief_description": "A commercial LLM checkpoint from Anthropic used in few-shot proposals and LLM-EO experiments; demonstrated strong performance in proposing TMCs with large HOMO-LUMO gaps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "claude-3.5-sonnet",
            "model_type": "foundational transformer-based LLM (Anthropic)",
            "model_size": null,
            "training_data": "Pretrained on large text corpora (proprietary); not fine-tuned on the TMC dataset for these experiments.",
            "application_domain": "Design of transition metal complexes (Pd(II) square planar TMCs) with target properties like HOMO-LUMO gap.",
            "generation_method": "Prompt-based few-shot generation (provided ligand SMILES and example TMCs with properties) and used within iterative LLM-EO experiments.",
            "novelty_of_chemicals": "Proposed TMCs within and beyond the enumerated search space; generated valid and unique proposals at &gt;71% rates in few-shot tests. Exact novelty vs training data unreported.",
            "application_specificity": "Specific objectives encoded via natural-language prompts and examples; improved performance when increasing the number of known initial TMCs provided in the prompt.",
            "evaluation_metrics": "HOMO-LUMO gap and polarisability via molSimplify + GFN2-xTB, validity/uniqueness, top-20 distribution metrics.",
            "results_summary": "In few-shot settings, claude-3.5-sonnet produced top-20 TMCs with significantly larger HOMO-LUMO gaps than GA and random baselines, and performance improved with more example TMCs in the prompt. It achieved high validity/uniqueness rates and identified mechanisms (e.g., preference for electron-withdrawing/strong-field ligands) consistent with chemical knowledge.",
            "comparison_to_other_methods": "Outperformed GA and random selection baselines; performed comparably to or slightly below o1-preview depending on the amount of few-shot data, but overall was among the best-performing LLMs tested.",
            "limitations_and_challenges": "Model-dependent behavior observed; required prompt tuning initially. Exact model internals, size, and training corpora are proprietary and not specified. Performance gains depended on number of examples in prompts.",
            "uuid": "e8644.2",
            "source_info": {
                "paper_title": "Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "gpt-4o",
            "name_full": "gpt-4o (OpenAI foundational model)",
            "brief_description": "A top-performing foundational model (not optimized for post-training reasoning) evaluated as a baseline LLM in LLM-EO experiments but performed worse than o1-preview and claude-3.5-sonnet on the TMC tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_type": "foundational transformer-based LLM (OpenAI)",
            "model_size": null,
            "training_data": "Pretrained on large text corpora (proprietary); no supervised fine-tuning on TMCs in this study.",
            "application_domain": "Tested for proposing transition metal complexes (Pd(II) square planar TMCs) within LLM-EO",
            "generation_method": "Prompt-based few-shot generation within the LLM-EO framework.",
            "novelty_of_chemicals": "Generated proposals but with lower quality/metrics compared to o1-preview and claude-3.5-sonnet; no quantitative novelty vs training data reported.",
            "application_specificity": "Prompt-specified objectives similar to other LLMs but showed limited ability to translate few-shot examples to high-quality proposals in this work.",
            "evaluation_metrics": "HOMO-LUMO gap, polarisability, validity/uniqueness, top-20 distributions.",
            "results_summary": "gpt-4o produced significantly worse results across evaluation metrics (top-20 HOMO-LUMO gaps, validity/uniqueness, distribution shifts) compared to o1-preview and claude-3.5-sonnet in the reported experiments.",
            "comparison_to_other_methods": "Underperformed relative to best commercial checkpoints (o1-preview, claude-3.5-sonnet) and also lagged behind GA on certain metrics in early iterations.",
            "limitations_and_challenges": "Authors attribute worse performance to lacking post-training inference mechanisms (reasoning enhancements) used by o1-preview and possibly to differences in chemical knowledge captured during pretraining.",
            "uuid": "e8644.3",
            "source_info": {
                "paper_title": "Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "o1-mini",
            "name_full": "o1-mini (OpenAI smaller foundational model)",
            "brief_description": "A smaller OpenAI foundational LLM checkpoint using similar post-training inference techniques as o1-preview but with reduced capacity; evaluated and found to underperform for TMC proposal tasks relative to larger checkpoints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o1-mini",
            "model_type": "smaller foundational transformer-based LLM (OpenAI)",
            "model_size": null,
            "training_data": "Pretrained on large corpora (proprietary); not fine-tuned on TMCs.",
            "application_domain": "Tested for proposing TMCs within LLM-EO framework.",
            "generation_method": "Prompt-based few-shot generation",
            "novelty_of_chemicals": "Produced proposals with substantially lower quality metrics (fewer high-gap TMCs, lower validity/uniqueness) than larger models; novelty vs training not reported.",
            "application_specificity": "Followed prompt objectives but struggled to generate high-quality, application-specific TMCs compared to larger models.",
            "evaluation_metrics": "HOMO-LUMO gap, polarisability, validity/uniqueness, top-20 metrics.",
            "results_summary": "o1-mini performed significantly worse across all evaluation metrics compared to o1-preview and claude-3.5-sonnet, indicating capacity/size limitations for these chemical design tasks.",
            "comparison_to_other_methods": "Underperformed relative to larger commercial LLMs and was generally inferior to GA and best LLM-EO configurations in terms of top candidate discovery efficiency.",
            "limitations_and_challenges": "Smaller model capacity likely reduces chemical knowledge and reasoning abilities in few-shot/in-context learning for complex multi-block chemical design.",
            "uuid": "e8644.4",
            "source_info": {
                "paper_title": "Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient evolutionary search over chemical space with large language models",
            "rating": 2,
            "sanitized_title": "efficient_evolutionary_search_over_chemical_space_with_large_language_models"
        },
        {
            "paper_title": "Multi-objective Evolution of Heuristic Using Large Language Model",
            "rating": 2,
            "sanitized_title": "multiobjective_evolution_of_heuristic_using_large_language_model"
        },
        {
            "paper_title": "Accelerated chemical science with ai",
            "rating": 1,
            "sanitized_title": "accelerated_chemical_science_with_ai"
        },
        {
            "paper_title": "Generative Models as an Emerging Paradigm in the Chemical Sciences",
            "rating": 1,
            "sanitized_title": "generative_models_as_an_emerging_paradigm_in_the_chemical_sciences"
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry",
            "rating": 1,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        }
    ],
    "cost": 0.012996249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models</p>
<p>Jieyu Lu 
Deep Principle Inc
02139CambridgeMA</p>
<p>Zhangde Song 
Deep Principle Inc
02139CambridgeMA</p>
<p>Qiyuan Zhao 
Deep Principle Inc
02139CambridgeMA</p>
<p>Yuanqi Du 
Department of Computer Science
Cornell University
14850IthacaNY</p>
<p>Yirui Cao 
Deep Principle Inc
02139CambridgeMA</p>
<p>Haojun Jia haojunjia@deepprinciple.com 
Deep Principle Inc
02139CambridgeMA</p>
<p>Chenru Duan duanchenru@gmail.com 
Deep Principle Inc
02139CambridgeMA</p>
<p>Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models
FFD767E6D8FA89F4304E4CBF111B3779
The design of functional transition metal complexes (TMCs) is hindered by the combinatorial explosion of the search space spanned by various metals and ligands, necessitating efficient multiobjective optimization strategies.Traditional genetic algorithms (GAs) are frequently employed in this domain, utilizing random mutations and crossovers steered by explicit mathematical objective formulations to navigate the search space.The transfer and sharing of knowledge across different GA optimization tasks, however, remain challenging.Here, we introduce the integration of large language models (LLMs) into the evolutionary optimization framework (LLM-EO) for TMCs.LLM-EO significantly outperforms traditional GAs due to the intrinsic chemical knowledge embedded within LLMs, acquired during their extensive pretraining.Notably, without the need for supervised fine-tuning, LLMs can leverage the entirety of historical data amassed during the optimization processes, demonstrating superior performance compared to LLMs that are limited to the best TMCs identified in the evolutionary cycle.Specifically, LLM-EO identifies eight out of the top 20 TMCs with the largest HOMO-LUMO gaps by interrogating merely 200 candidates within a vast search space of 1.37 million TMCs.Through prompt engineering using natural language, LLM-EO introduces unparalleled flexibility in multi-objective optimizations, especially when guided by seasoned researchers, thereby circumventing the necessity for intricate mathematical formulations.As generative models, LLMs possess the capability to propose novel ligands and TMCs with unique chemical properties by amalgamating both internal knowledge and external chemistry data, thus combining the benefits of efficient optimization and molecular generation.With the increasing potential of LLMs, both in their capacity as pretrained foundational models and new strategies in post-training inference, we anticipate broad applications of LLM-based evolutionary optimization in the fields of chemistry and materials design.</p>
<p>Introduction</p>
<p>2][3][4][5] Drawing inspiration from biological evolution, EO algorithms incorporate operations such as selection, mutation, recombination, and reproduction to iteratively evolve solutions.Genetic algorithms (GAs) are a prominent subset of EO methods, 6 employing a fitness function to evaluate and rank the quality of each solution.These algorithms apply evolutionary operators to the highest-ranked candidates, iteratively enhancing solution quality through mutation and crossover until predefined performance criteria are achieved.Despite their simplicity, GAs have demonstrated effectiveness comparable to machine learning (ML) methods for certain molecular optimization tasks. 7,8 oreover, the integration arXiv:2410.18136v1[physics.chem-ph]0][11][12] The adaptability and robustness of EO render it particularly suited for complex optimization challenges in chemistry, which are often characterized by large, multidimensional search spaces. 13,14 igning chemical systems composed of multiple building blocks, such as transition metal complexes (TMCs), presents significant challenges. 2,15 he combinatorial explosion resulting from the diverse combinations of metals and ligands makes systematic exploration of the TMC chemical space difficult, surpassing the capabilities of current theoretical and experimental screening methods. 2,16 hile exhaustive enumeration is impractical, efficiently navigating this vast chemical space is essential for discovering TMCs with optimized properties.Furthermore, the need of multi-objective optimization, which is often necessary in the design of functional materials and catalysts, adds an additional layer of complexity to the design of fitness functions in standard GA. 17Existing EO applications in TMC design have shown promise in optimizing properties such as catalytic activity and stability. 2,18 or instance, Kneiding et al. developed the Pareto-Lighthouse Multiobjective Genetic Algorithm (PL-MOGA), providing fine-grained control over multiple optimization objectives for optimizing properties of TMCs in a 1.37M chemical space. 19However, two factors limit the generative power of current EO approaches for TMCs: the inherent complexity of formulating multi-objective fitness functions and the trade-offs between operating on smaller ligand fragments and whole ligands.</p>
<p>Operating on whole ligands is computationally efficient because it limits the chemical space, making exploration faster.In contrast, smaller fragments increase diversity by allowing more combinations and novel structures, but they expand the chemical space, making it computationally demanding to explore. 20These challenges necessitate further methodological innovations to unlock the full potential of EO in designing new TMCs.</p>
<p>Recent advancements in large language models (LLMs), initially developed for natural language processing, have generated substantial interest in their application across a broad range of scientific disciplines.0][41][42][43] By integrating with evolutionary algorithms, LLMs have demonstrated enhanced capabilities, advancing the exploration and optimization processes in evolving concept libraries 44 and heuristic design. 45Recently, Wang et al. employed LLMs for the design of small organic molecules via SMILES string generation, highlighting the potential of LLMs as a powerful tool for chemical discovery. 46Nevertheless, their applications in complex chemical systems or materials composed of multiple building blocks under multi-objective optimization have yet to be realized.</p>
<p>In this study, we introduce LLM-EO, an integration of LLMs into the framework of EO, and apply it to both single-and multi-objective optimization tasks for transition metal complexes (TMCs).With only a few examples, LLMs can propose TMCs with desired properties by leveraging their extensive chemistry knowledge acquired during pretraining on a large corpus.In addition, LLM-EO significantly outperforms GA in optimizing TMCs for a single objective.Its performance is further enhanced when all historical data from the optimization process is retained, offering application scenarios akin to closed-loop optimizations in a laboratory setting.By utilizing natural language instructions, LLM-EO demonstrates exceptional flexibility in multi-objective optimizations, reducing the need for complex mathematical formulations.Lastly, LLM-EO leverages the generative capabilities of LLMs to generate entirely new ligands and TMCs, significantly accelerating the optimization process.By exploring an boundless design space that goes beyond the conventional chemical spaces defined by chemical intuition, LLM-EO opens up limitless possibilities for innovative compound design.Our results highlight LLM-EO as a continuously improvable framework for the design of functional materials.</p>
<p>Results</p>
<p>Overview of LLM-EO.LLM-EO describes the integration of LLMs with EO, wherein potential candidates for exploration are proposed by the LLMs (see Implementation Details of LLM-EO).Drawing inspiration from the principles of GA, we developed an iterative optimization workflow (Fig. 1b).In each iteration, a prompt is meticulously crafted, comprising generic instructions alongside specific information, constraints, and objectives, and is then presented to an LLM (see Prompt Engineering).The LLM subsequently generates a new set of task-specific TMCs, which are evaluated and integrated into the prompt for the subsequent iteration, effectively completing the EO cycle.To evaluate the intrinsic chemical knowledge acquired by LLMs during pretraining, we utilize commercially available LLM checkpoints without applying explicit supervised fine tuning on any TMCs (Supplementary Table TA).</p>
<p>Despite its widespread application across various physical science disciplines, we examine the potential of LLM-EO in single-and multi-objective optimization tasks.Specifically, we utilize a chemical space comprising 1.37 million TMCs from Kneiding et al. 19 to evaluate the performance of LLM-EO (see Fig. 1a and 1.37M square planar TMC space).This dataset includes Pd(II) square planar complexes constructed using four monodentate ligands chosen from a pool of 50 ligands, which consists of 25 neutral and 25 monoanionic ligands, with the added constraint that the total charge of a TMC must be -1, 0, or 1.Two properties of these 1.37M TMCs, i.e., HOMO-LUMO gap and polarisability, are computed using molSimplify 47 and GFN2-xTB 48,49 (see Evaluating new TMCs).Compared to conventional EO algorithms such as GA, LLM-EO provides three key advantages: First, it offers flexible objective control, enabling the articulation of more adaptable and nuanced optimization objectives in natural language as opposed to complex mathematical formulations.Second, it enhances optimization efficiency through the rich chemical knowledge embedded in LLMs and their ability to learn from few-shot samples.Third, it facilitates the generative design of new chemistry.The inherent generative capabilities of LLMs allow LLM-EO not only to efficiently navigate a predefined search space but also to creatively generate novel TMCs.These characteristics position LLM-EO as an emerging and powerful tool for chemical design.</p>
<p>LLMs proposing high-quality TMCs in few-shot learning.An important question to address is whether current</p>
<p>LLMs have acquired chemical knowledge during their pretraining on extensive corpora, and if so, whether they can utilize this knowledge to derive meaningful chemical designs.Here, we focus on a challenging task of designing TMCs with maximized HOMO-LUMO gaps for single-objective optimization, which represent the energy difference between the highest occupied molecular orbital (HOMO) and the lowest unoccupied molecular orbital (LUMO), a complex task due to the intricate electronic structures involved.To evaluate the ability of LLMs to learn structureproperty relationships from limited data and to propose novel TMC designs within a constrained chemical space, we provide the models with 50 ligands represented in SMILES notation, along with 20 randomly sampled initial TMCs from the 1.37M TMC space, each accompanied by their corresponding HOMO-LUMO gaps (see Fig. 1b, 1.37M square planar TMC space, and Supplementary Text SA).To demonstrate the current upper limit of LLM-EO, we employ the best commercially available LLMs from Anthropic (claude-3.5-sonnet)and OpenAI (o1-preview)</p>
<p>within the LLM-EO framework (see Prompt Engineering).According to OpenAI, o1-preview represents a marriage between strong pretrained foundational models with post-training inference to enhance reasoning capabilities. 50For comparative analysis, we also include gpt-4o, a top-performing foundational model that does not prioritize reasoning, and o1-mini, a smaller foundational model utilizing the same post-training inference techniques as o1-preview does.</p>
<p>Throughout this work, results from random selection and GA are used as baseline (see Genetic algorithms).With few-shot learning, the top-20 TMCs proposed by claude-3.5-sonnetand o1-preview exhibit significantly larger HOMO-LUMO gaps compared to those obtained by random selection and GA (Fig. 2a).With only 400 proposals, both claude-3.5-sonnetand o1-preview identify candidate TMCs with HOMO-LUMO gaps greater than 4.45 eV, which places them in the top 0.002 % (32 TMCs in total) of the 1.37M TMC space, underscoring their ability to pinpoint highly promising candidates for design objectives.For instance, both models recognize that ligands with strong electron-withdrawing effects (e.g., CF − 3 ) and strong-field ligands (e.g., CN − ) contribute to large HOMO-LUMO gaps in square planar Pd(II) TMCs (Fig. 2d).Additionally, the overall distribution of TMCs proposed by claude-3.5-sonnetand o1-preview shifts towards larger HOMO-LUMO gaps compared to baseline methods (Fig. 2b).Despite the numerous constraints on the design space and the limited examples provided during few-shot learning, both claude-3.5-sonnetand o1-preview achieve a high rate of validity and uniqueness, exceeding 71 % (Fig. 2c).In contrast, o1-mini and gpt-4o perform significantly worse across all evaluation metrics compared to o1-preview (Fig. 2a-c).This disparity is attributed to either a lack of chemical knowledge due to the smaller size of the foundation model or insufficient reasoning capabilities during post-training inference.These observations highlight the model-dependent nature of LLM-EO, while also indicating its potential for sustainable improvement, which would benefit from the scaling law for foundation model pretraining and recent advancements in post-training inferences, such as chain-of-thought reasoning 51 and self-play reinforcement learning. 52,53 investigate the influence of the number of initial TMCs provided in a few-shot learning setup, we vary the quantity of known TMCs included in the prompt to assess the resulting quality of the top-performing TMCs generated by LLM-EO.As one may expect, we observe that increasing the number of known initial TMCs enhances the HOMO-LUMO gaps of top-20 TMCs found by both claude-3.5-sonnetand o1-preview models (Fig. 2d).However, the improvement is more pronounced in claude-3.5-sonnet,possibly due to the exceptional performance of o1-preview with very limited known TMCs.Notably, despite only a single TMC is provided, o1-preview achieves superior performance, demonstrating a 1 eV larger HOMO-LUMO gap in the top-20 TMCs compared to the baseline of random selection.These findings indicate that LLMs can yield promising results even with limited data due to the chemical knowledge acquired during pretraining, offering a valuable balance between data requirements and performance.</p>
<p>Efficient optimization of HOMO-LUMO gap with LLM-EO.Upon confirming that LLMs can propose TMCs capable of achieving specific objectives using their inherent chemical knowledge, we progress towards closing the EO loop by integrating the 10 newly proposed TMCs with the original set (Fig. 3a and Supplementary Text SB).Initially, a retention strategy operates concurrently with GA, where only TMCs exhibiting the highest fitness values-specifically, the top-20 largest HOMO-LUMO gaps-are retained in the prompt for subsequent iterations (see Implementation Details of LLM-EO).Given that the o1-preview model yields the best results in few-shot learning, we choose to utilize o1-preview as the core model for LLM-EO throughout the remainder of this work.At an early stage of optimization, with only 50 TMC proposals, LLM-EO demonstrates its efficacy in identifying TMCs with significant HOMO-LUMO gaps, outpacing GA, which has not yet shown substantial improvement over random selection (Fig. 3b).As the number of iterations increases, GA also begins to show advantages over random selection; however, it continues to lag behind LLM-EO.Similar trends are observed when focusing on the top-20 TMCs found truncated at each iteration, where LLM-EO consistently outperforms GA and random selection (Fig. 3c).The improvement on HOMO-LUMO gaps for top-performing TMCs is much more significant compared to the overall distribution, suggesting the exceptional efficiency of LLM-EO on identifying TMCs extreme properties, a feature that is often desired in materials optimization.</p>
<p>In contrast to GA where only top-performing candidates are retained, there is a natural desire to utilize all historical data during the optimization process.Consequently, we adopt a different retention strategy than GA, wherein all previously evaluated TMCs are preserved, irrespective of their HOMO-LUMO gaps (see Fig. 3a and Implementation Details of LLM-EO).At early iterations, the distinction between retaining only top-performing and all TMCs is minimal, owing to the limited number of TMCs explored (Fig. 3b and c).However, as more TMCs are assessed during optimization, we observe significant benefits in maintaining records of all TMCs, which not only enhances the overall distribution of HOMO-LUMO gaps but also bolsters the identification of the top-20 TMCs.</p>
<p>These findings underscore the capacity of LLMs to learn from an increasing volume of evaluations, even when certain experiments may be deemed unsuccessful due to low fitness levels.</p>
<p>By leveraging historical data during optimization, LLM-EO efficiently identifies top-tier candidates within a vast design space.Notably, by retaining all historical data, LLM-EO consistently identifies 8 out of the top-20 TMCs with the largest HOMO-LUMO gaps, despite evaluating only 200 TMCs-a mere 0.015% of the 1.37M design space (Fig. 3d).In addition 37M space (green) and 400 TMCs explored by o1-preview (gray).The Pareto frontier (PF) for the 1.37 TMC space (red) is also shown.f.Percentage of TMCs with HOMO-LUMO gap &lt; 1 eV at various top-k polarisability by random sampling (blue), o1-preview at single-objective optimization for only maximizing polarisability (black), and o1-preview at multi-objective optimization for maximizing polarisability while requiring HOMO-LUMO gap &lt; 1 eV (gray).Select TMCs are shown for each case as insets.Atoms are colored as follows: Pd for sky blue, C for gray, N for blue, O for red, P for orange, F for green, and H for white.Purple dashed lines in TMCs represent dative bonds between Pd(II) and ligands.</p>
<p>accumulated knowledge positions LLM-EO as an exceptional laboratory assistant, which minimizes the time and effort needed to discover functional metal complexes by assimilating comprehensive historical data.</p>
<p>Flexible multi-objective optimization guided by natural language.Unlike traditional EO algorithms, such as GA, which require explicit mathematical formulations for multi-objective optimization, the LLM-EO approach utilizes the flexibility of natural language instructions.We illustrate this adaptability through three distinct multi-objective (both targeting on HOMO-LUMO gap and polarisability) optimization tasks within the 1.37M TMC space, where all historical data are retained during the execution of LLM-EO with o1-preview: First, Pareto frontier (PF) optimization, second, maximizing both the HOMO-LUMO gap and polarisability, and lastly, maximizing polarisability with a constraint that the HOMO-LUMO gap remains below 1 eV (see Implementation Details of LLM-EO).</p>
<p>The PF represents the set of optimal solutions in which enhancement in one objective requires a trade-off in another.With only 400 proposals, LLM-EO identifies nine TMCs that reside on the true (that is, 1st) PF of the 1.37M space (Fig. 4a and Supplementary Text SC).The PF established by LLM-EO successfully identifies TMCs with electron-withdrawing ligands, such as Pd(II)(CF − 3 ) 3 (P(CH 3 ) 3 ), which maximize the HOMO-LUMO gap, as well as those with highly polarized ligands, such as 3-Oxo-1,2-benzisothiazol-2-ide 1,1-dioxide (C 7 H 4 NO 3 S − ), which maximizes polarisability.All TMCs located on the PF found by LLM-EO within 400 proposals surpass the 3rd PF of the 1.37M space (Fig. 4a).The area under the curve (AUC) can be utilized to assess the positioning of a PF in the solution space, where a larger AUC indicates more effective trade-offs among multiple objectives.By exploring only 0.03 % of the total space, the AUC of LLM-EO's PF ranks already between that of the 1st and 2nd PFs of the 1.37M space (Fig. 4b).This highlights the efficacy of LLM-EO in identifying top-performing candidate TMCs within a multi-dimensional space.</p>
<p>When tasked with maximizing both the HOMO-LUMO gap and polarisability, LLM-EO effectively proposes TMCs concentrated in the top-right quadrant of the 1.37M space, illustrating its capability to target both objectives simultaneously (Fig. 4c and Supplementary Text SD).By evaluating only 400 TMCs, LLM-EO identifies 18 TMCs that overlap with the top-200 theoretically optimal TMCs, judged by the product of the HOMO-LUMO gap and polarisability.Among these, Pd(II)(P(CH 3 ) 2 (C 6 H 5 )) 3 (C 6 F 5 ) − emerges as the TMC with the highest score (product of two properties), achieving a balanced emphasis on the HOMO-LUMO gap and polarisability by incorporating a polar C 6 H 5 group on the phosphorous ligand (Fig. 4c).In contrast to random selection and PF exploration, LLM-EO distinctly produces TMCs with an overall higher product of the HOMO-LUMO gap and polarisability when tasked with maximizing both properties, demonstrating the effectiveness of prompt instruction (Fig. 4d).</p>
<p>Finally, we assess the effectiveness of incorporating constraints within the LLM-EO for multi-objective optimization, a scenario frequently encountered in practical functional materials design.Specifically, we task the LLM-EO with maximizing the polarisability of a TMC while maintaining a very small HOMO-LUMO gap of less than 1 eV, where only 3.8 % of TMCs in the 1.37M space meets this criterion (Supplementary Figure FB and Supplementary Text SE).With these new instructions in the prompt, the LLM-EO predominantly explores TMCs in the top-left corner of the 1.37M space, identifying 19 of the top-200 TMCs with the largest polarisability that also satisfy the HOMO-LUMO gap constraints within 400 proposals (Fig. 4e).Among these LLM-EO finds a heteroleptic TMC with four distinct ligands, achieving a extremely small HOMO-LUMO gap of 0.3 eV and a high polarisability of 430 a.u.The products of HOMO-LUMO gap and polarisability for the 400 TMCs proposed by LLM-EO are naturally lower than those from random selection due to the constraints imposed on HOMO-LUMO gap (Fig. 4d).Since TMCs with the highest polarisability do not inherently have small HOMO-LUMO gaps, it is exceedingly rare (&lt; 2 %) for them to achieve a HOMO-LUMO gap under 1 eV, as demonstrated by both random selection and single-objective optimization for polarisability using LLM-EO (Fig. 4f).Incorporating the HOMO-LUMO gap constraint into the prompt, however, significantly increases the number of TMCs with a HOMO-LUMO smaller than 1 eV while maintaining top-tier polarisability.This result highlights the potential of LLM-EO to accurately control and optimize desired properties through natural language instructions.Such precise regulation eliminates the necessity for complex scoring function formulations in multi-objective optimizations, thus enabling efficient exploration of diverse chemical spaces with targeted property combinations.</p>
<p>Utilizing the generative power of LLMs for de novo TMC design.To fully harness the generative capabilities of LLMs, we delve deeper into the potential of LLM-EO for designing novel ligands and TMCs beyond a predefined chemical space limited by an initial ligand pool (Supplementary Figure FC).We employ two distinct strategies for this purpose: First, one-shot generation approach for novel ligands (Supplementary Text SF).By leveraging the top-performing TMCs and the initial set of 50 ligands, the o1-preview model is utilized to generate a new pool of 10 ligands in a single shot (see Fig. 5a).These newly generated ligands are subsequently used to construct a chemical space comprising 2,200 square planar TMCs through enumeration (see Implementation Details of LLM-EO).</p>
<p>Lastly, HOMO-LUMO gaps and polarizabilities of these TMCs are calculated using molSimplify and GFN2-xTB (see Evaluating new TMCs).Second, iterative generation for novel TMCs (Supplementary Text SG).Similarly to the first approach, we start with the top-performing TMCs and the original set of 50 ligands.The o1-preview model generates 10 new TMCs, each incorporating at least one ligand not present in the initial ligand pool (see Fig. 5b).These 10 new TMCs are subsequently decomposed to extract a set of novel ligands generated by the o1-preview model as by-products.Finally, both the newly generated TMCs and ligands are integrated into the prompt for subsequent iterations of LLM-EO exploration (see Implementation Details of LLM-EO).</p>
<p>We initially test two generative LLM-EO approaches on enhancing the polarisability of Pd(II)-based TMCs in a square planar geometry.Utilizing the 10 newly generated ligands from the o1-preview model, we synthesize 2,200 novel TMCs, which demonstrate substantially higher polarisability compared to those in the existing 1.37M TMC space, with a minimum polarisability recorded at 447 a.u.(Fig. 5c).These newly generated ligands are often bulky, featuring extended conjugated systems or electron-rich moieties-a trait consistent with the chemical intuition that larger and more diffuse electron clouds enhance polarisability (Supplementary Figure FD).By iteratively generating new TMCs, the LLM-EO approach is also able to identify complexes with significantly greater polarisability (Fig. 5c).</p>
<p>When examining the top-performing 10 TMCs, both methods produced TMCs with polarisability values twice as high as the best TMCs found in the 1.37M space (Fig. 5d).Moreover, direct iterative generation via LLM-EO identifies a TMC with an exceptionally high polarisability exceeding 1,200 a.u., characterized by all four ligands containing extended conjugated rings and engaging in five subtle π − π interactions (Fig. 5d).This enhanced polarisability illustrates the generative capacity of LLM-EO in exploring chemical spaces that are typically inaccessible through conventional single-objective optimization methods.</p>
<p>To further assess the limits of LLM-EO, we subjecte it to a multi-objective optimization task aimed at simultaneously maximizing polarisability and the HOMO-LUMO gap.Both approaches of LLM-EO-generating new ligands in a single step and the iterative generation of TMCs-successfully identify TMCs that significantly exceed the boundaries of the 1.37M space (Fig. 4e).However, both methods exhibit a tendency to prioritize optimizing polarisability over the HOMO-LUMO gap.This may be due to the inherent constraints on the HOMO-LUMO gap of a Pd(II) square planar TMC, which is typically limited to approximately 5 eV, it more challenging to extend in comparison to polarisability. 54Nonetheless, with the iterative approach, LLM-EO is capable of generating TMCs that achieve a balanced effect between ligand field strength and conjugation.This results in TMCs with relatively large HOMO-LUMO gaps (&gt; 3 eV) while maintaining significant polarisability (&gt; 600 a.u.) (Fig. 4f).By harnessing the inherent generative nature of LLM-EO, TMCs with exceptional properties can be identified efficiently with only a few hundred trials.This demonstrates a significant improvement in efficacy compared to the conventional forward design approach, which involves screening a large, predefined chemical space.</p>
<p>Discussion</p>
<p>Efficient navigation through vast chemical spaces is crucial for designing functional materials, particularly those composed of multiple building blocks, such as TMCs.We developed LLM-EO, a workflow that leverages the predictive and generative capabilities of LLMs for evolutionary optimization.We demonstrated the effectiveness of LLM-EO in both single-and multi-objective optimization tasks for TMCs.inherent generative abilities of LLMs enable LLM-EO to undertake de novo design, creating TMCs that excel in their design objectives, thus surpassing the limitations of predefined chemical spaces.</p>
<p>We observe significant variation in the performance of LLM-EO when utilizing different LLMs.The inherent characteristics of LLMs are prominently exhibited in the LLM-EO framework.For instance, retaining all historical data during the optimization process is more effective than merely preserving the top-performing candidates, as seen in genetic algorithms (GA).This efficacy is attributed to the capability of LLMs to efficiently learn from expansive contexts.Furthermore, LLM-EO allows for instruction using natural language, eliminating the necessity for explicit mathematical task formulations.This feature provides unparalleled flexibility for addressing complex multi-objective optimization challenges, such as those encountered in practical materials design, where multiple metrics need to be satisfied.Finally, due to its generative nature, LLM-EO can discover entirely new materials possessing exceptional properties that are not present within a predefined chemical space, which might typically rely on intuition or enumeration.</p>
<p>Within LLM-EO framework, the two models, o1-preview and claude-3.5-sonnet,demonstrate significantly better performance compared to open-source models and those developed in certain regions in our internal tests.</p>
<p>Despite we do not aim this work as a benchmark study, this difference highlights potential challenges in ensuring equity in scientific progress toward artificial general intelligence, particularly as some organizations may focus on ambitious, revenue-oriented product development strategies.Although we demonstrate the utility of LLM-EO in optimizing TMCs for HOMO-LUMO gaps and polarisability, it holds promise for numerous applications across biology, chemistry, and materials science, including DNA sequence design, the discovery of both homogeneous and heterogeneous catalysts, and the construction of functional materials with multiple building blocks, such as metal-organic frameworks.As LLM capabilities continue to expand, we anticipate that the LLM-EO framework introduced in this work will become increasingly beneficial for other domains in scientific discovery.</p>
<p>Methods</p>
<p>Genetic algorithms.Genetic algorithms (GAs) are optimization techniques inspired by natural selection and genetics, effective in constructing systems from components like TMCs.In this work, GA is used as the baseline for the performance in proposing new TMCs.The GA process begins with the same initial population sampled from the 1.37M compound space.HOMO-LUMO gap values were obtained from PL-MOGA, 19 calculated using the GFN2-xTB method.Then an evolutional optimization (EO) process iteratively updates the population through genetic operations, namely crossover and mutation, to optimize the objective function:</p>
<ol>
<li>
<p>Crossover.This operation involved combining parts from two parent solutions to produce offspring.In our experiment, pairs of TMCs were selected and their ligands exchanged to create new candidate complexes.</p>
</li>
<li>
<p>Mutation.After crossover, mutation introduced random alterations to the offspring's chemical structure.</p>
</li>
</ol>
<p>Specifically, one or more ligands from parental TMCs were replaced with random ligands from a library of 50 ligands.</p>
<p>Implementation Details of LLM-EO.In this study, the intrinsic chemical knowledge of Large Language Models (LLMs) and their few-shot learning capabilities are leveraged within an LLM-based evolutionary optimization framework (LLM-EO).At each iteration, new TMCs are proposed by the LLM, subject to specific constraints.This process is guided by design objectives articulated in natural language, obviating the need for explicit mathematical formulations.The proposed TMCs are then integrated into the prompt, serving as the knowledge base for subsequent TMC proposals in the next iteration.This LLM-EO approach differs from traditional evolutionary optimization algorithms (EO) such as Genetic Algorithms (GA), where operations are limited to a fixed set of manipulations like mutation and crossover.Furthermore, unlike Bayesian optimization, 12 evaluation of the fitness function across the entire design space at each iteration is not required.Several LLM-EO configurations were explored in this work, and are detailed below.</p>
<p>Fig. 1 |
1
Fig. 1 | Overview of the design space of TMCs and LLM-EO.a. Construction of 1.37M space of square planar TMCs.The building blocks of TMCs consist of Pd in oxidation of II (i.e., metal) and a ligand pool with 25 neutral ligands and 25 ionic ligands.By assembling them in a square planar geometry with the constraint of total TMC charge being -1, 0, or 1, a space of 1.37M TMC is constructed.b.Workflow of LLM-EO.A prompt is engineered towards designing TMCs with both generic instructions (text in orange) and certain information, constraints, and objectives (text in blue).This prompt interacts with LLM (e.g., through API calls), which returns a new set of TMCs.These new TMCs are fed into the prompt with updated information, closing the loop of evolutionary optimization.Atoms are colored as follows: Pd for sky blue, C for gray, N for blue, O for red, P for orange, S for yellow, I for purple, and H for white.Purple dashed lines in TMCs represent dative bonds between Pd(II) and ligands.</p>
<p>Fig. 2 |
2
Fig. 2 | Proposing new TMCs with few-shot LLMs.a. Box plot (notched) for the distribution of HOMO-LUMO gap of top-20 TMCs among the 200 TMCs proposed by different approaches, providing 20 TMCs as known.Random is for blue, GA for red, claude-3.5-sonnetfor orange, o1-preview for green, o1-mini for purple, and gpt-4o for sky blue.b.Cumulative probability of HOMO-LUMO gaps of all TMCs proposed by various approaches and their overall distribution (box plot at top margin).c.Fraction of both valid and unique TMCs in 200 TMC proposals.d.Box plot (notched) for the distribution of HOMO-LUMO gap of top-20 TMCs among the 200 TMCs proposed, measured with a varying number of known TMCs provided in the prompt.Only the results for claude-3.5-sonnet(orange), o1-preview (green) are shown.The TMC with largest HOMO-LUMO gap for claude-3.5-sonnetand o1-preview is shown, correspondingly.The mean of random top-20 TMCs is shown with a blue dashed line.Atoms are colored as follows: Pd for sky blue, C for gray, N for blue, O for red, P for purple, F for green, and H for white.Purple dashed lines in TMCs represent dative bonds between Pd(II) and ligands.</p>
<p>Fig. 3 |
3
Fig. 3 | Maximizing HOMO-LUMO gap with LLM-EO.a. Schematic of LLM-EO, with the option of keeping top-20 TMCs with largest HOMO-LUMO gaps in the prompt (green) or keeping all historical data (gray) during the evolutionary optimization.b.Cumulative probability of HOMO-LUMO gaps of all TMCs proposed by various approaches at iteration = 5 (left) and iteration = 20 (right) during the evolutionary optimization.Random sampling is used as blue, GA as red, o1-preview with only top-20 TMCs kept in green, and o1-preview with all historical data in gray.The overall distribution of each approach is shown as box plot at top margin.c.Box plot (notched) for the distribution of HOMO-LUMO gap of top-20 TMCs among the 200 TMCs proposed during the evolutionary optimization, providing 20 random TMCs as initial samples.d.Number of hits as top-20 TMCs with largest HOMO-LUMO gap among the 1.37M space versus the number of TMCs proposed during the evolutionary optimization.Solid lines are the average of three independent runs at different random seed, and their shedding corresponds to the standard deviation.</p>
<p>Fig. 4 |
4
Fig.4| Multi-objective optimization with LLM-EO for Pareto frontier exploration (left), maximizing both HOMO-LUMO gap and polarisability (α, middle), and maximizing HOMO-LUMO gap while keeping α &lt; 1 eV.a. Polarisability versus HOMO-LUMO gap for random 400 TMCs (blue).The Pareto frontier (PF) for the 1.37 TMC space (red), the 3rd PF for the 1.37M TMC space (green), and the PF found by o1-preview with 400 LLM-EO exploration (gray), are shown.A solid step-wise line is drawn to shown the frontier area obtained by o1-preview in 400 TMCs exploration.b.Area under PF curves for the true 1st PF (red), 2rd PF (orange), 3rd PF (green), 4th PF (purple) for the 1.37M TMC space, and PF found by o1-preview (gray).c.Polarisability versus HOMO-LUMO gap for random 400 TMCs (blue) and top-200 TMCs with largest multiplication of HOMO-LUMO gap and polarisability for the 1.37M space (green) and 400 TMCs explored by o1-preview (gray).The Pareto frontier (PF) for the 1.37M TMC space (red) is also shown.d.Box plot (notched) for the distribution of HOMO-LUMO gap and polarisability multiplication of random TMCs (blue) and TMCs proposed by o1-preview (gray) during PF exploration, maximizing both gap and polarisability, and maximizing polarisability while keeping gap &lt; 1 eV.e. Polarisability versus HOMO-LUMO gap for random 400 TMCs (blue) and top-200 TMCs with largest polarisability while gap &lt; 1 eV for the 1.37M space (green) and 400 TMCs explored by o1-preview (gray).The Pareto frontier (PF) for the 1.37 TMC space (red) is also shown.f.Percentage of TMCs with HOMO-LUMO gap &lt; 1 eV at various top-k polarisability by random sampling (blue), o1-preview at single-objective optimization for only maximizing polarisability (black), and o1-preview at multi-objective optimization for maximizing polarisability while requiring HOMO-LUMO gap &lt; 1 eV (gray).Select TMCs are shown for each case as insets.Atoms are colored as follows: Pd for sky blue, C for gray, N for blue, O for red, P for orange, F for green, and H for white.Purple dashed lines in TMCs represent dative bonds between Pd(II) and ligands.</p>
<p>Fig. 5 |
5
Fig. 5 | Generating new ligands and TMCs with LLMs.a. Workflow for assembling new TMC space by asking LLMs to propose new ligands outside of the original ligand pool.b.Workflow for direct iterative TMC optimization by asking LLMs to propose new TMCs that are made by new ligands outside of the original ligand pool.c.Cumulative probability of polarisability of TMCs proposed by various approaches.Random sampling is used as blue, using 10 ligands generated by o1-preview in one shot in red, LLM-EO by o1-preview for generating new TMCs iteratively in orange.d.Box plot (notched) for the distribution of polarisability for TMCs explored by various approaches.The best TMC for each approach is shown.e. Polarisability versus HOMO-LUMO gap for random 400 TMCs (blue), TMCs constructed by 10 ligands generated by o1-preview in one shot (red), and TMCs generated by LLM-EO using o1-preview iteratively (orange).Box plots for the distribution of each property are shown at margins.f.Box plot (notched) for the distribution of the product of HOMO-LUMO-gap and polarisability (that is, α) for TMCs explored by various approaches.The best TMC for each approach is shown.Atoms are colored as follows: Pd for sky blue, C for gray, N for blue, O for red, F for green, P for orange, S for yellow, and H for white.Purple dashed lines in TMCs represent dative bonds between Pd(II) and ligands.</p>
<p>Both the quality of the pretrained foundation model and the mechanism of post-training inference are crucial factors, as exemplified by a case where o1-preview substantially outperforms gpt-4o and o1-mini in optimizing the HOMO-LUMO gap of TMCs with LLM-EO.Interestingly, claude-3.5-sonnetdemonstrates superior performance in a similar context compared to gpt-4o, suggesting its potential to surpass o1-preview once post-training inference mechanisms are enhanced for better reasoning capabilities.Moreover, the consistently improved performance of LLM-EO with each new release of LLMs indicates an almost limitless potential for performance enhancement in the LLM-EO workflow for chemical design.This is particularly promising when strategically incorporating scientific literature into the corpus during the pretraining of an LLM.</p>
<p>1 .
1
37M square planar TMC space.With the initial 50-ligand pool introduced by Kneiding et al.,19 a space of 1.37M Pd(II) square planer TMCs is constructed considering the total charge being -1, 0, or 1.The acyclic symmetry of square planer TMCs, meaning Pd(L 1 )(L 2 )(L 3 )(L 4 ) is the same as Pd(L 2 )(L 3 )(L 4 )(L 1 ), Pd(L 3 )(L 4 )(L 1 )(L 3 ), and Pd(L 4 )(L 1 )(L 2 )(L 3 ), is considered when building this 1.37 TMC space.It is worth noting that the 1.37M TMC dataset from Kneiding et al. was released on Feb 15, 2024, which is beyond the knowledge cutoff of OpenAI (Oct 2023) and close to the cutoff of claude-3.5-sonnetfrom Anthropic (Apr 2024), minimizing the risk of direct data leakage.Evaluating new TMCs.Similar to Kneiding et al.,19 a new Pd(II) square planar complexes is generated with molSimplify by specifying the metal center, coordinating ligands represented by SMILES string, and their connecting atom indexes.GFN2-xTB is then used to optimize the geometry generated by molSimplify and calculate HOMO-LUMO gap and polarisability.TMCs that result in bad geometry either due to atom clashing or change of connectivity during the geometry optimization are considered as failed attempts, which are included in the prompt with reasons of failure to guide LLMs learning from failure.55</p>
<p>Strandgaard, M., Seumer, J. &amp; Jensen, J. H. Discovery of molybdenum based nitrogen fixation catalysts with genetic algorithms. Chem. Sci. 15, 10638-10650, DOI: 10.1039/D4SC02227K (2024).
AcknowledgementWe would like to thank our entire team from Deep Principle for helpful discussions and support.C.D. thanks Haorui Wang, Jingru Gan, Yanjiao Zhu, and Peichen Zhong for valuable discussions.Code and data availabilityCode and data are currently under review and will be available as a open source repository on github.Direct few-shot proposal.The process begins with a pool of 50 ligands, each characterized by its structure, ligand ID, and connecting atom information as part of generic instructions.Additionally, 20 initial TMCs are provided within the prompt to guide a LLM (e.g., o1-preview).The LLM is asked to propose 10 new TMCs that optimize a given objective (e.g., maximizing the HOMO-LUMO gap).This request is repeated 20 times with the same set of known TMCs, resulting in a total of 200 LLM-proposed TMCs.To mitigate bias introduced by the initial TMC sampling, two different random seeds are used for this process.Evolutionary optimizations with LLMs.Starting with 20 initial TMCs, an LLM (e.g., o1-preview) is then employed to propose 10 new TMCs.These new TMCs are merged with the existing data.From this combined pool, the top-20 candidates with the highest fitness values are selected as the known TMCs for the next iteration.This iterative process is repeated for 20 iterations in single-property optimization tasks (Fig.3) and 40 iterations in multi-objective optimization tasks (Fig.4).Similarly, to minimize bias from the initial TMC sampling, two different random seeds are used to sample the initial known TMCs.Utilizing all historical records.Selecting the top-k samples in traditional evolutionary optimization requires explicit mathematical formulation of objectives, which can be challenging for complex multi-objective tasks.Therefore, we further explore a setup where all historical data are preserved during the evolutionary optimization using LLM.Specifically, all TMCs explored during the optimization process, regardless of their fitness values, are included in the prompt for the next iteration.This mimics the human learning experience where one can learn from both good and bad examples.To prevent the LLM from overemphasizing specific records, the historical TMC data within the prompt is randomly shuffled before each iteration.This randomization enhances sampling diversity by presenting the LLM with different prompt orderings while maintaining the same underlying set of known TMCs.Flexible 2D optimizations with pure natural language.LLM-EO can be readily extended to multi-objective optimizations by tuning the languages describing the task, circumventing the necessity for explicit mathematical formulations.This capability is demonstrated with three multi-objective optimization tasks:1. Pareto Frontier Optimization.-AnLLM aims to identify solutions that are non-dominated across the considered objectives, effectively balancing trade-offs between the targets (for example, HOMO-LUMO gap and polarisability).A sentence, "My design objective is to expand the Pareto frontier (maximizing) of my TMCs spanned by two properties, HOMO-LUMO gap and polarisability", is used in the prompt to achieve this design objective.2. Maximizing both HOMO-LUMO Gap and polarisability.-AnLLM aims to propose TMCs that increase both the HOMO-LUMO gap and the polarisability.A sentence, "My design objective is to find TMCs that simultaneously maximize both HOMO-LUMO gap (&gt; 4 eV) and polarisability (&gt; 400 au)", is added in the prompt.3. Maximizing polarisability at a small HOMO-LUMO gap.-AnLLM aims to propose TMCs with extremely large polarisability that has a small HOMO-LUMO gap (&lt; 1 eV).A sentence, "My design objective is to design TMCs with maximized polarisability (&gt; 450 au) and minimized HOMO-LUMO gap (&lt; 1.0 eV)", is included in the prompt.Generating new ligands and TMCs.Besides asking LLMs to propose TMCs with optimized properties in a confined space (for example, the 1.37M TMC design space), one would expect unlocking the generative power of LLMs by requesting LLMs to generate new ligands and TMCs without constraining them to only use a small pool of ligands.Provided the original 50 ligands and the top-100 TMCs made by these ligands in Pd(II) square planar geometry, two different setups are investigated in this work:1. Generating a new ligand pool in one shot.-AnLLM is asked to generate 10 new ligands: five neutral and five monoanionic.The key criterion was to ensure these new ligands were distinct from those in the existing pool of 50 ligands.Additionally, these ligands were designed to further optimize specific objectives (see Fig.5a).To achieve this, the prompt provided to the LLM included the original 50 ligands' molecular structures in SMILES format, along with relevant data such as charges and connecting atom information.Furthermore, examples of TMCs with their calculated properties were included to guide the LLM's generation process.The new chemical space was constructed by enumerating all possible combinations of these 10 newly generated ligands, applying the same constraint on the total charge (-1, 0, or 1) as the existing pool.This process led to the creation of approximately 2,200 new TMCs.Finally, these new TMCs were evaluated using the same procedure as for the original 1.37 million TMC space, ensuring consistency in evaluation methodology (refer to Evaluating new TMCs).2.Generating new TMCs with new ligands in an iterative manner.-AnLLM is asked to generate 10 new TMCs, potentially using new ligands that are outside of the 50-ligand pool, that would further optimize certain objectives (Fig.5b).These 10 new TMCs are decomposed to obtain a certain number of new ligands.The new ligands, together with the new TMCs, are used to update the known information in the prompt.Then a new iteration follows.This evolutionary optimization process is repeated for a total of 20 iterations for single property optimization tasks (that is, maximizing polarisability) and 40 iterations for multi-objective optimization tasks (that is, maximizing both HOMO-LUMO gap and polarisability).Prompt Engineering.Writing good prompts is essential to guide LLMs to behave properly and exploit their performance.We find, however, the performance of LLM-EO is not very sensitive to the prompt, as long as the description of known information and task is clear.In practice, we tune the prompt against claude-3.5-sonnetto remove edge cases and confirm the output is in a proper format.The prompt is then directly used to interact with o1-preview without any changes.Author contributionsCompeting interestsJ.L., Z.S., H.J., and C.D. are co-inventors on a provisional patent application that incorporates discoveries described in this manuscript.
Discovery and Optimization of Materials Using Evolutionary Approaches. T C Le, D A Winkler, 10.1021/acs.chemrev.5b00691Chem. Rev. 1162016</p>
<p>Computational Discovery of Transition-metal Complexes: From High-throughput Screening to Machine Learning. A Nandy, 10.1021/acs.chemrev.1c00347Chem. Rev. 1212021</p>
<p>Generative Models as an Emerging Paradigm in the Chemical Sciences. D M Anstine, O Isayev, 10.1021/jacs.2c13467J. Am. Chem. Soc. 1452023</p>
<p>Accelerated chemical science with ai. S Back, 10.1039/D3DD00213FDigit. Discov. 32024</p>
<p>Machine learning-aided generative molecular design. Y Du, 10.1038/s42256-024-00843-5Nat. Mach. Intell. 62024</p>
<p>Genetic Algorithms in Search. D E Goldberg, Optimization and Machine Learning. Addison-Wesley Longman Publishing Co., Inc1989</p>
<p>A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space. J H Jensen, 10.1039/C8SC05372CChem. Sci. 102019</p>
<p>Sample efficiency matters: A benchmark for practical molecular optimization. W Gao, T Fu, J Sun, C Coley, Advances in Neural Information Processing Systems. S Koyejo, Curran Associates, Inc202235</p>
<p>Accelerating Chemical Discovery with Machine Learning: Simulated Evolution of Spin Crossover Complexes with an Artificial Neural Network. J P Janet, L Chan, H J Kulik, 10.1021/acs.jpclett.8b00170J. Phys. Chem. Lett. 92018</p>
<p>Genetic algorithms for computational materials discovery accelerated by machine learning. P C Jennings, S Lysgaard, J S Hummelshøj, T Vegge, T Bligaard, 10.1038/s41524-019-0181-4Comput. Mater. 52019</p>
<p>Parallel tempered genetic algorithm guided by deep neural networks for inverse molecular design. A Nigam, R Pollice, A Aspuru-Guzik, 10.1039/D2DD00003BDigit. Discov. 12022</p>
<p>Active learning exploration of transition-metal complexes to discover method-insensitive and synthetically accessible chromophores. C Duan, A Nandy, G G Terrones, D W Kastner, H J Kulik, 10.1021/jacsau.2c00547JACS Au. 32023</p>
<p>Delocalized, asynchronous, closed-loop discovery of organic laser emitters. F Strieth-Kalthoff, 10.1126/science.adk9227Science. 38492272024</p>
<p>Autonomous, multiproperty-driven molecular discovery: From predictions to measurements and back. B A Koscher, 10.1126/science.adi1407Science. 38214072023</p>
<p>Ai is a viable alternative to high throughput screening: a 318-target study. I Wallach, 10.1038/s41598-024-54655-zSci. Reports. 1475262024</p>
<p>Genetic Optimization of Homogeneous Catalysts. R Laplaza, S Gallarati, C Corminboeuf, 10.1002/cmtd.202100107Chem. Methods. 2e2021001072022</p>
<p>Discovery of molybdenum based nitrogen fixation catalysts with genetic algorithms. M Strandgaard, J Seumer, J H Jensen, 10.1039/D4SC02227KChem. Sci. 152024</p>
<p>Directional multiobjective optimization of metal complexes at the billionsystem scale. H Kneiding, A Nova, D Balcells, 10.1038/s43588-024-00616-5Nat. Comput. Sci. 42024</p>
<p>Beyond Predefined Ligand Libraries: A Genetic Algorithm Approach for De Novo Discovery of Catalysts for the Suzuki Coupling Reactions. J Seumer, J H Jensen, 10.26434/chemrxiv-2024-9xh38-v22024</p>
<p>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, 10.1021/jacs.3c05819J. Am. Chem. Soc. 1452023</p>
<p>MatSciBERT: A materials domain language model for text mining and information extraction. T Gupta, M Zaki, N M A Krishnan, Mausam, 10.1038/s41524-022-00784-wComput. Mater. 82022</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. J Lee, 10.1093/bioinformatics/btz682Bioinformatics. 362020</p>
<p>An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining. Y Peng, Q Chen, Z Lu, 10.48550/arXiv.2005.027992020. 2005.02799</p>
<p>A M Bran, 10.48550/arXiv.2304.053762304.05376Augmenting large-language models with chemistry tools. 2023</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 6242023</p>
<p>ReactionT5: a large-scale pre-trained model towards application of limited reaction data. T Sagawa, R Kojima, 10.48550/arXiv.2311.067082311.067082023</p>
<p>Unified Deep Learning Model for Multitask Reaction Predictions with Explanation. J Lu, Y Zhang, 10.1021/acs.jcim.1c01467J. Chem. Inf. Model. 622022</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, 10.1088/2632-2153/ac3ffbMach. Learn. Sci. Technol. 3150222022</p>
<p>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction. P Schwaller, 10.1021/acscentsci.9b00576ACS Cent. Sci. 52019</p>
<p>An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation. G W Kyro, A Morgunov, R I Brent, V S Batista, Chemspaceal, 10.1021/acs.jcim.3c01456J. Chem. Inf. Model. 642024</p>
<p>Domain-Agnostic Molecular Generation with Chemical Feedback. Y Fang, 10.48550/arXiv.2301.112592301.112592023</p>
<p>Molecular Generation Using a Transformer-Decoder Model. V Bagal, R Aggarwal, P K Vinod, U D Priyakumar, Molgpt, 10.1021/acs.jcim.1c00600J. Chem. Inf. Model. 622022</p>
<p>Y Liu, S Ding, S Zhou, W Fan, Q Tan, Moleculargpt, 10.48550/arXiv.2406.129502406.12950Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction. 2024</p>
<p>J Shen, N Tenenholtz, J B Hall, D Alvarez-Melis, N Fusi, Tag-Llm, 10.48550/arXiv.2402.051402402.05140Repurposing General-Purpose LLMs for Specialized Domains. 2024</p>
<p>Catalyst Energy Prediction with CatBERTa: Unveiling Feature Exploration Strategies through Large Language Models. J Ock, C Guntuboina, A Barati Farimani, 10.1021/acscatal.3c04956ACS Catal. 132023</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, 10.1038/s42256-022-00580-7Nat. Mach. Intell. 42022</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.1038/s42256-023-00788-1Nat. Mach. Intell. 62024</p>
<p>Large Language Models are Catalyzing Chemistry Education. Y Du, 10.26434/chemrxiv-2024-h722v2024</p>
<p>Z Zhang, 10.48550/arXiv.2406.19226Simulating Classroom Education with LLM-Empowered Agents. 2024. 192262406</p>
<p>ChatGPT -Reshaping medical education and clinical management. Pak. R A Khan, M Jawaid, A R Khan, M Sajjad, 10.12669/pjms.39.2.7653J. Med. Sci. 396052023</p>
<p>Exploring the use of large language models (LLMs) in chemical engineering education: Building core course problem models with Chat-GPT. M.-L Tsai, C W Ong, C.-L Chen, 10.1016/j.ece.2023.05.001Educ. Chem. Eng. 442023</p>
<p>Natural language processing models that automate programming will transform chemistry research and teaching. G M Hocky, A D White, 10.1039/D1DD00009HDigit. Discov. 12022</p>
<p>Symbolic regression with a learned concept library. A Grayeli, A Sehgal, O Costilla-Reyes, M Cranmer, S Chaudhuri, arXiv:2409.093592024arXiv preprint</p>
<p>Multi-objective Evolution of Heuristic Using Large Language Model. S Yao, 10.48550/arXiv.2409.168672409.168672024</p>
<p>Efficient evolutionary search over chemical space with large language models. H Wang, arXiv:2406.169762024arXiv preprint</p>
<p>A toolkit for automating discovery in inorganic chemistry. E I Ioannidis, T Z H Gani, H J Kulik, Molsimplify, 10.1002/jcc.24437J. Comput. Chem. 372016</p>
<p>A robust and accurate tight-binding quantum chemical method for structures, vibrational frequencies, and noncovalent interactions of large molecular systems parametrized for all spd-block elements. S Grimme, C Bannwarth, P Shushkov, J. chemical theory computation. 132017</p>
<p>Gfn2-xtb-an accurate and broadly parametrized self-consistent tightbinding quantum chemical method with multipole electrostatics and density-dependent dispersion contributions. C Bannwarth, S Ehlert, S Grimme, J. chemical theory computation. 152019</p>
<p>Openai, Openai o1 techinical report: Learning to reason with llms. 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, 2201.119032023</p>
<p>Mastering the game of go without human knowledge. D Silver, 10.1038/nature24270Nature. 5502017</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Z Chen, Y Deng, H Yuan, K Ji, Q Gu, 2401.013352024</p>
<p>tmqm dataset-quantum geometries and properties of 86k transition metal complexes. D Balcells, B B Skjelstad, 10.1021/acs.jcim.0c01041J. Chem. Inf. Model. 602020</p>
<p>Learning from failure: Predicting electronic structure calculation outcomes with machine learning models. C Duan, J P Janet, F Liu, A Nandy, H J Kulik, 10.1021/acs.jctc.9b00057J. Chem. Theory Comput. 152019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-813 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-813</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-813</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-265294417</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.10775v1.pdf" target="_blank">ToolTalk: Evaluating Tool-Usage in a Conversational Setting</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Many recent works seek to augment LLM-based assistants with external tools so they can access private or up-to-date information and carry out actions on behalf of users. To better measure the performance of these assistants, this paper introduces ToolTalk, a benchmark consisting of complex user intents requiring multi-step tool usage specified through dialogue. ToolTalk contains 28 tools grouped into 7 plugins, and includes a complete simulated implementation of each tool, allowing for fully automated evaluation of assistants that rely on execution feedback. ToolTalk also emphasizes tools that externally affect the world rather than only tools for referencing or searching information. We evaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and 50% respectively. Our analysis of the errors reveals three major categories and suggests some future directions for improvement. We release ToolTalk at https://github.com/microsoft/ToolTalk.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e813.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e813.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo-0613 (OpenAI Chat Completions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI conversational large language model accessed via the Chat Completions API with function-calling support; evaluated in this paper as an assistant that can predict tool invocations in a conversational setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based conversational LLM provided by OpenAI, used via the Chat Completions API with function-calling (tool invocation) capability; no model architecture changes or additional external modules were applied in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolTalk (easy and hard subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / conversational multi-step tool orchestration (action tools + non-action tools)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Easy: success 85.7% (precision 42.4%, recall 89.3%, incorrect action rate 5.0%); Hard: success 26.0% (precision 54.6%, recall 69.7%, incorrect action rate 23.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Function-calling tool-use interface via OpenAI Chat Completions API; no additional planning module, external memory, or RL components reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / API usage only (no additional fine-tuning or RL reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>ablation study (documentation removal) / prompting/system-message configuration</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Ablation removing all tool and parameter documentation (keeping only tool names and parameter types) was evaluated to measure effect of documentation on tool-use performance; the assistant otherwise used the same API/function-calling interface and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Removing documentation decreased performance: Easy success 85.7% -> 82.1%; Hard success 26.0% -> 16.0%. (See precision/recall/incorrect-action rates in Tables 1 and 3 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper attributes failures (and the gap between simple QA-style tool use and complex conversational tool orchestration) to: premature tool calls (hallucinated/missing arguments), faulty planning (omitting needed tool calls or using wrong tools in multi-step flows), and incorrect invocations (wrong/missing arguments or misunderstanding docs). The authors suggest lack of self-reflection/grounding for arguments and limited planning as causes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolTalk: Evaluating Tool-Usage in a Conversational Setting', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e813.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e813.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4-0613 (OpenAI Chat Completions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 conversational model used with function-calling to predict tool invocations during multi-turn conversations; evaluated on ToolTalk and compared to GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Larger-capacity OpenAI transformer conversational LLM accessed via Chat Completions API with function-calling; used without architectural augmentation beyond the API's function-calling interface.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolTalk (easy and hard subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / conversational multi-step tool orchestration (action tools + non-action tools)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Easy: success 92.8% (precision 69.2%, recall 96.4%, incorrect action rate 3.8%); Hard: success 50.0% (precision 74.9%, recall 79.0%, incorrect action rate 25.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Function-calling tool-use interface via OpenAI Chat Completions API; no separate planner, verifier, or external memory described in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / API usage only (no additional model fine-tuning or RL described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>ablation study (documentation removal) / prompting/system-message configuration</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Ablation removing tool and parameter documentation (retaining only names and parameter types) to test reliance on documentation for correct tool invocation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Removing documentation decreased performance: Easy success 92.8% -> 85.7%; Hard success 50.0% -> 34.0% (see Tables 1 and 3). Precision/recall also decreased across most conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper suggests that despite strong language capabilities, GPT-4 still struggles with multi-step interactive tool orchestration due to premature tool invocation, planning failures (omitting or ordering tool calls), and incorrect argument formatting or misunderstanding of documentation; recommends self-reflection/grounding, clearer API interfaces, and simpler argument sets as remedies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolTalk: Evaluating Tool-Usage in a Conversational Setting', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e813.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e813.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolTalk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolTalk benchmark (conversational tool-usage evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new benchmark introduced in this paper consisting of 78 multi-turn conversations requiring multi-step tool usage across 28 tools (7 plugins), with simulated executable tool implementations and automated evaluation metrics that distinguish action vs non-action tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolTalk (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset and evaluation suite (not a model): multi-turn conversational scenarios that require planning, sequential tool calls, and handling of tools with side effects; includes simulated tool implementations and an evaluation protocol calculating recall, precision, incorrect action rate, and conversation success.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolTalk (this benchmark itself)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step conversational procedural tasks (action tools, planning, sequential decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Used to evaluate models in the paper: GPT-3.5 and GPT-4 achieved the success rates reported above (Easy/Hard subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Benchmark assumes agents expose a tool-use interface (function-calling) and provides execution feedback; evaluates planning and argument grounding but does not mandate any particular agent architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>dataset/ablations (documentation ablation) and analysis-driven recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>The paper performs an ablation removing tool documentation to test the effect of documentation on tool-use performance, and analyzes error categories (premature calls, faulty planning, incorrect argument invocations). It also recommends interventions: self-reflection/grounding for argument values, simplifying tool argument sets, and redesigning API interfaces to better suit LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Documentation ablation measurably reduced success rates for evaluated models (e.g., GPT-4 Hard: 50.0% -> 34.0%; GPT-3.5 Hard: 26.0% -> 16.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>ToolTalk authors argue that conventional QA-style benchmarks do not exercise action-oriented, multi-step procedural skills and therefore overestimate LLM assistants' practical automation ability; the gap arises from planning, argument grounding, and execution-feedback handling rather than pure knowledge retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolTalk: Evaluating Tool-Usage in a Conversational Setting', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>AgentBench: Evaluating llms as agents <em>(Rating: 2)</em></li>
                <li>ToolBench <em>(Rating: 2)</em></li>
                <li>ToolLLM: Facilitating large language models to master 16000+ real-world apis <em>(Rating: 1)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-813",
    "paper_id": "paper-265294417",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-3.5",
            "name_full": "gpt-3.5-turbo-0613 (OpenAI Chat Completions)",
            "brief_description": "An OpenAI conversational large language model accessed via the Chat Completions API with function-calling support; evaluated in this paper as an assistant that can predict tool invocations in a conversational setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "gpt-3.5-turbo-0613",
            "model_description": "Transformer-based conversational LLM provided by OpenAI, used via the Chat Completions API with function-calling (tool invocation) capability; no model architecture changes or additional external modules were applied in this study.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolTalk (easy and hard subsets)",
            "interactive_task_type": "tool use / conversational multi-step tool orchestration (action tools + non-action tools)",
            "interactive_performance": "Easy: success 85.7% (precision 42.4%, recall 89.3%, incorrect action rate 5.0%); Hard: success 26.0% (precision 54.6%, recall 69.7%, incorrect action rate 23.9%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Function-calling tool-use interface via OpenAI Chat Completions API; no additional planning module, external memory, or RL components reported.",
            "training_method": "prompting / API usage only (no additional fine-tuning or RL reported in this paper).",
            "intervention_type": "ablation study (documentation removal) / prompting/system-message configuration",
            "intervention_description": "Ablation removing all tool and parameter documentation (keeping only tool names and parameter types) was evaluated to measure effect of documentation on tool-use performance; the assistant otherwise used the same API/function-calling interface and prompts.",
            "intervention_effect": "Removing documentation decreased performance: Easy success 85.7% -&gt; 82.1%; Hard success 26.0% -&gt; 16.0%. (See precision/recall/incorrect-action rates in Tables 1 and 3 of the paper.)",
            "hypothesized_cause_of_gap": "Paper attributes failures (and the gap between simple QA-style tool use and complex conversational tool orchestration) to: premature tool calls (hallucinated/missing arguments), faulty planning (omitting needed tool calls or using wrong tools in multi-step flows), and incorrect invocations (wrong/missing arguments or misunderstanding docs). The authors suggest lack of self-reflection/grounding for arguments and limited planning as causes.",
            "uuid": "e813.0",
            "source_info": {
                "paper_title": "ToolTalk: Evaluating Tool-Usage in a Conversational Setting",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "gpt-4-0613 (OpenAI Chat Completions)",
            "brief_description": "OpenAI's GPT-4 conversational model used with function-calling to predict tool invocations during multi-turn conversations; evaluated on ToolTalk and compared to GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "gpt-4-0613",
            "model_description": "Larger-capacity OpenAI transformer conversational LLM accessed via Chat Completions API with function-calling; used without architectural augmentation beyond the API's function-calling interface.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolTalk (easy and hard subsets)",
            "interactive_task_type": "tool use / conversational multi-step tool orchestration (action tools + non-action tools)",
            "interactive_performance": "Easy: success 92.8% (precision 69.2%, recall 96.4%, incorrect action rate 3.8%); Hard: success 50.0% (precision 74.9%, recall 79.0%, incorrect action rate 25.1%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Function-calling tool-use interface via OpenAI Chat Completions API; no separate planner, verifier, or external memory described in the experiments.",
            "training_method": "prompting / API usage only (no additional model fine-tuning or RL described in this paper).",
            "intervention_type": "ablation study (documentation removal) / prompting/system-message configuration",
            "intervention_description": "Ablation removing tool and parameter documentation (retaining only names and parameter types) to test reliance on documentation for correct tool invocation.",
            "intervention_effect": "Removing documentation decreased performance: Easy success 92.8% -&gt; 85.7%; Hard success 50.0% -&gt; 34.0% (see Tables 1 and 3). Precision/recall also decreased across most conditions.",
            "hypothesized_cause_of_gap": "The paper suggests that despite strong language capabilities, GPT-4 still struggles with multi-step interactive tool orchestration due to premature tool invocation, planning failures (omitting or ordering tool calls), and incorrect argument formatting or misunderstanding of documentation; recommends self-reflection/grounding, clearer API interfaces, and simpler argument sets as remedies.",
            "uuid": "e813.1",
            "source_info": {
                "paper_title": "ToolTalk: Evaluating Tool-Usage in a Conversational Setting",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ToolTalk",
            "name_full": "ToolTalk benchmark (conversational tool-usage evaluation)",
            "brief_description": "A new benchmark introduced in this paper consisting of 78 multi-turn conversations requiring multi-step tool usage across 28 tools (7 plugins), with simulated executable tool implementations and automated evaluation metrics that distinguish action vs non-action tools.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ToolTalk (benchmark)",
            "model_description": "Dataset and evaluation suite (not a model): multi-turn conversational scenarios that require planning, sequential tool calls, and handling of tools with side effects; includes simulated tool implementations and an evaluation protocol calculating recall, precision, incorrect action rate, and conversation success.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolTalk (this benchmark itself)",
            "interactive_task_type": "tool use / multi-step conversational procedural tasks (action tools, planning, sequential decision-making)",
            "interactive_performance": "Used to evaluate models in the paper: GPT-3.5 and GPT-4 achieved the success rates reported above (Easy/Hard subsets).",
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": "Benchmark assumes agents expose a tool-use interface (function-calling) and provides execution feedback; evaluates planning and argument grounding but does not mandate any particular agent architecture.",
            "training_method": null,
            "intervention_type": "dataset/ablations (documentation ablation) and analysis-driven recommendations",
            "intervention_description": "The paper performs an ablation removing tool documentation to test the effect of documentation on tool-use performance, and analyzes error categories (premature calls, faulty planning, incorrect argument invocations). It also recommends interventions: self-reflection/grounding for argument values, simplifying tool argument sets, and redesigning API interfaces to better suit LLMs.",
            "intervention_effect": "Documentation ablation measurably reduced success rates for evaluated models (e.g., GPT-4 Hard: 50.0% -&gt; 34.0%; GPT-3.5 Hard: 26.0% -&gt; 16.0%).",
            "hypothesized_cause_of_gap": "ToolTalk authors argue that conventional QA-style benchmarks do not exercise action-oriented, multi-step procedural skills and therefore overestimate LLM assistants' practical automation ability; the gap arises from planning, argument grounding, and execution-feedback handling rather than pure knowledge retrieval.",
            "uuid": "e813.2",
            "source_info": {
                "paper_title": "ToolTalk: Evaluating Tool-Usage in a Conversational Setting",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        },
        {
            "paper_title": "AgentBench: Evaluating llms as agents",
            "rating": 2,
            "sanitized_title": "agentbench_evaluating_llms_as_agents"
        },
        {
            "paper_title": "ToolBench",
            "rating": 2
        },
        {
            "paper_title": "ToolLLM: Facilitating large language models to master 16000+ real-world apis",
            "rating": 1,
            "sanitized_title": "toolllm_facilitating_large_language_models_to_master_16000_realworld_apis"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        }
    ],
    "cost": 0.010916499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TOOLTALK: EVALUATING TOOL USAGE IN A CON-VERSATIONAL SETTING
15 Nov 2023</p>
<p>Nicholas Farn nifarn@microsoft.com 
Microsoft Corporation</p>
<p>Richard Shin 
Microsoft Corporation</p>
<p>TOOLTALK: EVALUATING TOOL USAGE IN A CON-VERSATIONAL SETTING
15 Nov 20239F3DB1DA7D8C843058512DD291B9913CarXiv:2311.10775v1[cs.CL]
Large language models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users.Many recent works seek to augment LLM-based assistants with external tools so they can access private or up-to-date information and carry out actions on behalf of users.To better measure the performance of these assistants, this paper introduces ToolTalk, a benchmark consisting of complex user intents requiring multi-step tool usage specified through dialogue.ToolTalk contains 28 tools grouped into 7 plugins, and includes a complete simulated implementation of each tool, allowing for fully automated evaluation of assistants that rely on execution feedback.ToolTalk also emphasizes tools that externally affect the world rather than only tools for referencing or searching information.We evaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and 50% respectively.Our analysis of the errors reveals three major categories and suggests some future directions for improvement.We release ToolTalk at https://github.com/microsoft/ToolTalk.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) can perform impressive feats in natural language understanding, generation, and other tasks involving manipulation of text.With appropriate adjustments after pretraining, they can hold fluent and natural conversations with users.However, the scope of such conversations is still limited by LLMs lacking access to knowledge outside of their training data, exhibiting limited mathematical reasoning and computational abilities, and otherwise being unable to interact with the outside world.</p>
<p>To overcome these limitations, various prior works have proposed integrating LLM-powered chatbots with the ability to use tools such as search engines (Nakano et al., 2022), calculators, or web APIs (Mialon et al., 2023).Making meaningful progress in tool use requires relevant benchmarks and evaluation datasets that can fully exercise these systems with realistic and challenging conversations.In this paper, we introduce ToolTalk as a step towards this goal.ToolTalk consists of 78 conversations with 178 total turns, making use of 28 unique tools grouped into 7 categories, along with an evaluation methodology tailored towards measuring accurate tool use.</p>
<p>Several considerations informed our design of ToolTalk in order to best simulate typical conversations that a user may wish to have with an LLM-based assistant.First, we wanted to ensure that ToolTalk is conversational, and allows for multiple rounds of dialogue between the user and the assistant for a single intent; reflecting how users may not always wish to formulate their full request in one utterance and can add additional qualifiers or issue corrections after receiving some feedback from the assistant.This allows us to include user intents requiring a complex series of tool invocations without having unnaturally long utterances.Second, we include a ground-truth set of tool calls that should have been made for each user utterance, suitable for use in an automated evaluation comparing against the tool calls predicted by an assistant.Third, ToolTalk includes executable implementations of every tool included in the dataset, to facilitate the evaluation of assistants that may consider results from prior tool invocations to decide which ones to make next.Fourth, ToolTalk includes tools intended to have side effects (such as sending emails, or adding/deleting calendar events), which we refer to as "action tools", rather than only making database queries (such as searching for emails containing a particular keyword).Such action tools are necessary if the assistant is to automate the user's tasks.</p>
<p>We tailor our evaluation methodology towards the particulars of our dataset design, going beyond common metrics like exact-match accuracy.In particular, we separately consider invocations of action and non-action tools, considering that incorrect invocations to action tools, such as sending a message to the wrong person, may have particularly negative effects for the user.On the other hand, if the assistant makes both correct non-action tool invocations and some incorrect extraneous ones, the extraneous ones may still provide useful information to the user (even if it's not what the user directly requested).As such, we use tool invocation recall and incorrect action rate as the primary metrics within a single conversational turn, and define a conversation-level notion of success.</p>
<p>We apply ToolTalk on two assistants implemented using the function calling support of OpenAI's Chat completions API with the GPT-3.5 and GPT-4 models.</p>
<p>We found that gpt-3.5-turbo-0613and gpt-4-0613 achieve a conversation-level success rate of 26% and 50% respectively, demonstrating that tool usage in a conversational setting is still a difficult task for even some of the most state-of-the-art models.We then conduct further analyses to determine reasons why GPT-3.5 and GPT-4 fail on conversations.We find that both GPT-3.5 and GPT-4 can hallucinate arguments, fail to understand documentation, and even outright claim to have accomplished a task without calling any tools.</p>
<p>Our paper makes the following contributions:</p>
<p>• We introduce a conversational dataset for tool-using LLM-powered assistants, containing a broad range of tools and example conversations with ground truth annotations for tool invocations that allow for an automated evaluation.• We ensure that the dataset contains multi-turn conversations requiring use of multiple tools, including tools with side effects, to better simulate how users may interact with a tool-using assistant.• We develop an evaluation methodology which reflects the differences between tools with side effects and tools without them.• We evaluate assistants built using GPT-3.5 and GPT-4 using our dataset and analyze their errors, finding issues such as hallucinated arguments and misunderstood documentation.</p>
<p>DATASET DESIGN</p>
<p>PLUGINS AND TOOLS</p>
<p>ToolTalk is designed for a paradigm where individual users will be able to customize a personal assistant with a number of plugins available through various online stores.This can be seen as similar to how a user might customize their phone with apps of various functionality.Each plugin contains a set of tools designed around a single purpose such as managing a calendar, buying movie tickets, or listening to music.We define a tool as a single function needed to accomplish that purpose such as creating an event, searching for movies, or playing a song.We assume that most plugins will need to contain multiple tools.For example, a theoretical "Calendar" plugin should not only have the ability to create events, but also to then search, modify, and delete these events.</p>
<p>For our dataset, we defined 7 plugins containing a total of 28 tools (see Appendix A for the full list).</p>
<p>Using similar domains as those in Li et al. (2023), we created the following plugins:</p>
<p>• AccountTools: containing tools for account management such as logging in and out, updating account information, or looking up other users.• Alarm: adding, deleting, and finding alarms.</p>
<p>• Calendar: creating, modifying, deleting, and searching events and meetings • Email: searching inbox and sending emails • Message: sending and reading messages from other users [Execute] SearchEmail • From: sallee@fakemail.com,Subject: Come visit me in Edinburgh… • From: hot-dealz-noreply@dealz.com, Subject: FLASH SALE in Edinburgh...</p>
<p>[LLM] Tool Call SearchEmail(query="Edinburgh")</p>
<p>[LLM] Tool Call SearchEmail()</p>
<p>Conversation History</p>
<p>Figure 1: ToolTalk methodology.A system prompt, user and assistance utterances, and ground truth tool calls are fed as conversation history to the LLM.We prompt the LLM for a tool call prediction and simulate execution.This is added to the conversation history and the LLM is prompted for another prediction.This continues until the LLM predicts an assistant response.LLM predictions are then forgotten and the process is repeated for the next assistant turn.Predicted tool calls are then compared against ground truth tool calls.</p>
<p>• Reminder: setting, completing, and deleting reminders on a to do list</p>
<p>• Weather: querying current weather, weather forecasts, and historic weather data based on location</p>
<p>To teach the LLM about how to use the tools, each tool contains a high-level description, verbose documentation about each of its parameters, and a description of its return value.To facilitate evaluation, each tool has a simulated implementation in Python, along with a method to judge whether two invocations of the same tool with different parameters should be considered equivalent.We also note for each tool whether it is considered an action (has side effects) or not.We also include accompanying databases with mock information about fictional existing users, emails, reminders, and so on, for the simulated tool implementations to use.</p>
<p>CREATING CONVERSATIONS</p>
<p>To help create realistic conversations that exercise our tools and plugins, we used GPT-4.For each subset of 3 plugins from the 7 plugins we have defined, we create prompts which lists the documentation for all the tools in these 3 plugins, and instructs GPT-4 to create 3 realistic scenarios involving a user trying to accomplish a task that uses at least 5 tool calls from the random subset of plugins.</p>
<p>We create as many prompts as the number of tools that exist in the subset of 3 plugins currently under consideration, such that each prompt instructs GPT-4 to specifically use one of the tools in the subset of 3 plugins.We provide the prompt template used in Appendix B.</p>
<p>The above procedure results in the generation of ∼400 scenarios.We then repeatedly sampled a scenario evenly from all tools, discarding sampled scenarios that do not involve the required tool, hallucinate non-existent tools, or seem implausible.Using a sampled scenario as general guidance, we manually create a conversation, writing down all of its parts by hand.</p>
<p>Each conversation consists of a user utterance, the tool calls that the assistant should make given that utterance, the expected return values for those calls, and the assistant's natural language responses given the user's utterances plus the tool calls and their results, repeating in that order until the conversation is finished.As metadata for the conversation, we also specified a timestamp for the conversation, and the user's location and username. 1We ensure that each conversation contains at least 3 tool calls.We repeat the above sampling of scenarios until we have written 50 conversations.</p>
<p>Additionally, we create 28 "easy" conversations completely by hand, one for each tool.This easy version of ToolTalk consists of a few turns of user-assistant dialogue followed by a single tool call.Combined with the prior 50 "hard" examples, we create a total of 78 conversations comprising ToolTalk.</p>
<p>After constructing conversations, we ensure that the databases used by our simulated tool implementations contain the necessary content so that when we execute the ground truth tool calls as listed in the conversations we have created, they return the same ground truth values.</p>
<p>EVALUATION METHODOLOGY</p>
<p>Evaluation of a tool-using assistant with ToolTalk consists of two phases.In the first phase, for each conversation, we take all prefixes that end in a user utterance (which could have been preceded by prior user utterances, the tool calls made for those utterances, the results of those calls, and the assistant's response considering all of the above).We run the assistant with this prefix, where it can either predict a tool call or generate a response given the calls already made and their results; if the assistant predicts a tool call, we execute it using our simulated tool implementations and then provide the assistant with the result.In the second phase, for each conversation prefix, we compare the tool calls predicted for that prefix against its corresponding ground truth, computing the tool invocation recall and incorrect action rate as described below.</p>
<p>TOOL CALL CORRECTNESS</p>
<p>As described in Section 2.1, for each action tool, we defined a function to compare a predicted and a ground truth invocation of that tool (considering the arguments in the invocations), to help us determine whether a predicted tool call should be considered equivalent to one in the ground truth.For example, if an email is required to be sent to multiple people, we only check that the set of emails are the same instead of requiring the exact same order.</p>
<p>For argument fields that accept free-form natural language inputs, such as message bodies and event descriptions, we compute their embeddings with DistilBERT using sent2vec2 and check whether their cosine similarity is above 0.9.</p>
<p>For optional arguments, if the ground truth invocation has a value for one, then we compare its value against the one in the predicted invocation; if the ground truth invocation is missing a value for an optional argument, then it is entirely disregarded and the predicted call may have any value for that argument (or none at all) while still being considered correct.For example, the description of a calendar event is an optional argument, and if it is not explicitly mentioned in the conversation, then it is unlikely to impact the correctness of a predicted call whether or not it is filled out.</p>
<p>For the non-action tools (which are generally tools for searching over a database), we do not compare the arguments in the tool calls, but rather compare the execution results of the predicted and ground truth tool calls.They are considered equivalent of the results are identical.</p>
<p>CONVERSATION SIMULATION</p>
<p>Algorithm 1 shows the general pseudocode for conversation simulation.To simulate a conversation, we first reset the state of the world (e.g.databases get reset to their initial state).For each turn in the ground truth (consisting of a user's utterance, tool calls for that utterance, and the assistant's reply), we provide the information from all previous turns, followed by the current turn's user utterance, to the model.We then let the model predict as many tool calls as it wants, executing them one at a time until the prediction model produces a reply to the user instead of a tool call.</p>
<p>INCORRECT ACTIONS</p>
<p>Each tool is labeled as being either an action or not.We consider a tool an action if its execution has the ability to affect the external world such as sending messages or deleting calendar events.In comparison, non-action tools only passively references knowledge from the outside world such as looking up the weather or calling a calculator.We make this distinction between action and nonaction tools because incorrect calls to action tools are much more consequential.For example, an incorrect call to the DeleteAlarm tool could result in the user over-sleeping.While an assistant could theoretically realize that it made an incorrect action tool call and make a different one to reverse its effects, not all actions are reversible.</p>
<p>Thus, during evaluation, we also track "incorrect" actions.We consider an action "incorrect" if the tool called is labeled as an action, it fails to match any call in the ground truth, and if the tool call executed without any errors (including by having the correct number of arguments and passing the correct types).3</p>
<p>METRICS
∀g ∈ G; g ∈ M ⇐⇒ ∃p ∈ P where f tool (p, g) (1) success = (M == G) ∧ (I == ∅)(2)
We use the tool call correctness function, f tool , to compare each prediction to all tool calls in the ground truth; as described in Algorithm 2, each ground truth tool call can only match once to a predicted tool call.Given a set of M predictions matching ground truth (defined in equation 1), the set of all predictions P , and the set of all ground truth tool calls G we calculate precision and recall Additionally, we compute success as a boolean value for each conversation, following Equation 2.</p>
<p>The assistant succeeds at a conversation if and only if it has perfect recall and no incorrect actions.We take success rate over all conversations as our key metric.Since success rate is a composite of two scores, we keep recall and incorrect action rate as additional metrics to provide more detail.We also include precision as a measure of efficiency in tool prediction; a higher precision indicates that there were fewer predicted tool calls that are unnecessary according to the ground truth.</p>
<p>EXPERIMENTS AND ANALYSIS</p>
<p>EXPERIMENTS</p>
<p>We evaluate GPT-3.5 (gpt-3.5-turbo-0613)and GPT-4 (gpt-4-0613) on ToolTalk using the functions functionality as part of OpenAI's Chat completions API (OpenAI).This API takes as input an optional system message, a history of messages between a user and an assistant, tool documentation, and any previous tool invocations and their responses, and produces as output either a tool invocation or an assistant message.</p>
<p>In the system message, we include the conversation's location, timestamp, and (if present) username.</p>
<p>We supply documentation for all 28 tools at once to simulate a user with all 7 plugins enabled.We then simulate and evaluate all conversations in the easy and hard subsets of ToolTalk, following Algorithms 1 and 2.</p>
<p>Table 1 shows the results.We get success rates of 85.7% and 92.8% for GPT-3.5 and GPT-4 on the easy version of ToolTalk, and success rates of 26.0% and 50.0% on the hard version.GPT-4 outperforms GPT-3.5, but still achieves similar incorrect action rates.From precision, we can see that GPT-4 is also more efficient than GPT-3.5.However, performance for both models are low, showing the difficulty of tool usage in conversation settings.</p>
<p>ANALYSIS</p>
<p>We analyze the conversations that either GPT-4 or GPT-3.5 fail on.We notice that for both LLMs, there are three major reasons that they can fail.First, the model may predict a tool call prematurely on a turn before a user has provided the necessary information.Second, the model may exhibit poor planning, resulting in omitting or using the wrong tools.Third, it may have picked the correct tool to use, but invoked it with incorrect or missing arguments, failing to follow the tool's function signature described in the documentation.GPT-3.5 is more susceptible to these errors, but they manifest as well for GPT-4.</p>
<p>Premature tool calls.This usually occurs when the user has a clear intent, e.g."I want to create an event", but has yet to provide the necessary information to provide as arguments.It then manifests as hallucinating plausible values to supply as arguments.This is harmless when predicting reference tools but is a direct contribution to failure when predicting action tools.Concerningly, even when the hallucinated arguments will result in execution errors, the model will persist in hallucinating more arguments.Despite these issues, both GPT-3.5 and GPT-4 will generally choose the correct tools to accomplish the intent.Faulty reasoning.Ultimately, premature tool calls could be mostly explained by faulty reasoning, where the LLM fails to reflect that it does not have all the information it needs to accomplish a task and needs to ask the user to provide more clarification.Similarly, omission or the usage of wrong tools can also be explained by faulty reasoning skills; rather than reflecting and realizing it needs to ask the user to provide more clarification, the LLM fails to realize that it needs to call additional tools in order to accomplish a task.</p>
<p>For example, the SendEmail tool requires a recipient email address, which can be obtained from a username with the QueryUser tool.However, instead of using QueryUser and then passing its result to SendEmail, the model may instead hallucinate a plausible email address belonging the user.In other circumstances, the model will forget specifics of the task and fail to call the corresponding tools.For example, if a user wants to both send a message and change their calendar, the model will only change the calendar and not send the message.In egregious cases, both LLMs can hallucinate tools or not predict any tool usage at all and confidently state that it has accomplished the task.</p>
<p>Incorrect invocations of the correct tool.Even if the model picks the correct tool, it can invoke the tool with incorrect arguments, by missing values or supplying wrong values.This can happen from failing to understand documentation, failing to understand the output of previous tool invocations, or weak mathematical skills.Examples include supplying 2 PM as "2:00" instead of "14:00"; calculating a 10 hour event ending at 6 PM as 6 PM to 12 AM; incorrectly supplying a reminder it had just created to the DeleteReminder tool.</p>
<p>Quantitative results.Table 2 shows the number of turns in which the above error types occur, in our evaluation of GPT-4 and GPT-3.5.We determine error types automatically by comparing predictions for a single turn with the ground truth for the same turn and seeing which predictions and ground truth tool calls fail to find a match.GPT-4 overall produces fewer errors for each category than GPT-3.5.However, GPT-4 generally fails for the same reasons as GPT-3.5 in cases where both fail on the same conversation.GPT-4 does demonstrate a clear improvement in planning over GPT-3.5 as GPT-4 will generally be able to determine all tools needed to accomplish a task.</p>
<p>Lessons.Our results and analyses suggest a few ways to improve tool usage and design for LLMs.Some form of self-reflection or grounding for argument values seems key to reduce premature invocation of tools.This can also help LLMs determine if it has all the tools necessary to complete a task.For GPT-3.5 in particular, minimizing the number of arguments in tools seems likely to lead to good improvements.This is because unlike GPT-4, GPT-3.5 has more difficulty recovering from errors, often giving up.</p>
<p>EXPERIMENT REMOVING DOCUMENTATION</p>
<p>We perform an ablation study to measure the effect of tool documentation by removing all tool and parameter descriptions keeping only names and parameter types.We re-evaluate GPT-3.5 and GPT-4 on ToolTalk producing Table 3.We also re-run our analysis on error types producing Table 4. Performance on ToolTalk significantly decreases across the board except for incorrect action rate.</p>
<p>The decrease in incorrect action rate could be due to tools being harder to use, resulting in less successful tool executions overall, whether or not it matches ground truth.</p>
<p>From Table 4 we can see that faulty planning accounts for the majority of errors produced by GPT-3.5 and GPT-4.We perform a qualitative analysis and discover both models tend to call tools with incorrectly formatted arguments, receive errors in execution feedback, then persist in the same incorrect format.This results in both models eventually giving up and predicting an assistant reply thereby missing all other tool calls in the ground truth.</p>
<p>RELATED WORK</p>
<p>In Section 1, we described our desired criteria for evaluating tool-using LLM-based assistants: using dialogue to specify intents requiring multi-step tool invocations, and actions rather than only retrieving information, for a fully automated evaluation not requiring human judgement over the outputs of the system under test.Table 5 summarizes how other work about evaluating tool-using LLMs compares along these factors.We describe the related work in greater detail below.</p>
<p>Tool-augmented LLMs are also known as tool-augmented learning, tool LLMs, tool-learning, augmented language models (ALMs), or tool manipulation with LLMs (Xu et al., 2023;Mialon et al., 2023;Qin et al., 2023a).Development in this area consists of improving LLM performance in traditional tasks by giving them access to tools such as a calculator or search engine (Lu et al., 2023a;Yao et al., 2022b;Paranjape et al., 2023;Hao et al., 2023).It can also include applying LLMs to traditional automation tasks such as embodied robotics or browsing the web (Liu et al., 2023b;Deng et al., 2023;Yao et al., 2022a;Liang et al., 2023), dubbed "LLM-as-agent" by AgentBench (Liu et al., 2023a).</p>
<p>Traditional tasks that tool-augmented LLMs have been applied to include question answering such as ScienceQA (Saikh et al., 2022) or HotPotQA (Yang et al., 2018), mathematical reasoning (Cobbe et al., 2021;Lu et al., 2023b;Qiao et al., 2023), multilingual translation and QA (Lewis et al., 2020;Scarton et al., 2019), open-domain QA (Zhu et al., 2021), and commonsense QA (Talmor et al., 2019) to name a few.These tasks are useful for demonstrating the benefits of augmenting LLMs with tool usage, but fail to fully distinguish how much LLMs rely on internal knowledge vs good usage of tools (Zhuang et al., 2023).They also fail to incorporate the use of tools that affect the external world since they are unnecessary for those tasks.</p>
<p>Common agent benchmarks that have been applied to tool-augmented LLMs include WebShop (Yao et al., 2022a), Tabletop (Liang et al., 2023), Mind2Web (Deng et al., 2023), and ALFWorld (Shridhar et al., 2020).Additionally, AgentBench compiles Mind2Web, WebShop, and ALFWorld into a unified benchmark while adding additional agent environments such as interacting with a bash terminal, creating SQL commands to query a database, interacting with a knowledge graph, digital card game simulations, and lateral thinking puzzles (Liu et al., 2023a).ToolBench does something similar by compiling Tabletop and Webshop while introducing a variety of other tasks consisting of predicting a single API call.These benchmarks are useful for evaluating the effectiveness of tool-augmented LLMs in a variety of autonomous situations.However, none of them test tool-augmented LLMs in a conversational setting.Furthermore, tasks in these benchmarks consist of issuing a single utterance which an agent then tries to accomplish without any further human interaction.This is in contrast to ToolTalk, where a conversation will consist of multiple utterances with multiple intermediary tasks.</p>
<p>Past works have also created datasets for evaluating tool-augmented LLM-based assistants.Examples include ToolLLM (Qin et al., 2023b), API-Bank (Li et al., 2023), TPTU (Ruan et al., 2023), Gorilla (Patil et al., 2023), RestGPT (Song et al., 2023), GPT4Tools (Yang et al., 2023), and ToolAlpaca (Tang et al., 2023) among others.Unfortunately, many of these datasets require manual inspec-</p>
<p>No. of tools Dialogue Complex Actions Automated</p>
<p>ReAct (Yao et al., 2022b) 3 ✗ ✗<em> ✗ ✓ ART (Paranjape et al., 2023) 3 ✗ ✗</em> ✗ ✓ Tool Learning (Qin et al., 2023a) 17 ✗ ✓ ✓ ✓ Toolformer (Schick et al., 2023) 5 ✗ ✓ ✗ ✓ Chameleon (Lu et al., 2023a) 15 ✗ ✓ ✗ ✓ ToolkenGPT (Hao et al., 2023) 58 ✗ ✓ ✓ ✓ ToolQA (Zhuang et al., 2023) 13 ✗ ✓ ✗ ✓ API-Bank (Li et al., 2023) 53 ✓ ✗<em> ✓ ✓</em> ToolBench (Xu et al., 2023) 232 ✗ ✗<em> ✓ ✓ AgentBench (Liu et al., 2023a) 100+ ✗ ✓ ✓ ✓ TPTU (Ruan et al., 2023) 12 ✗ ✗ ✓ ✓ Gorilla (Patil et al., 2023) 1,645 ✗ ✗ ✓ ✓ RestGPT (Song et al., 2023) 94 ✗ ✓ ✓ ✗ GPT4Tools (Yang et al., 2023) 31 ✗ ✗ ✓ ✓ ToolLLM (Qin et al., 2023b) 16,464 ✗ ✓ ✓ ✓ ToolAlpaca (Tang et al., 2023) 400
✗ ✓ ✓ ✗ ToolTalk 28 ✓ ✓ ✓ ✓
Table 5: Comparison of evaluation used in prior work with ToolTalk.We note total number of tools used (No. of tools), if any task is specified over multiple user utterances (dialogue), if any task requires more than 1-2 tools to complete (complex), if any task requires the use of action tools (actions), and if all evaluation is done automatically (automated).We note nuances in prior work denoted by "</em>" in Appendix D.</p>
<p>tion of the outputs of the assistant under test to perform a complete evaluation.A lot of them also have unrealistic queries, and do not reflect questions or intents humans are likely to say in real life. 4any of them are also simple, where the solution requires one or two tool calls (Li et al., 2023;Ruan et al., 2023;Yang et al., 2023;Tang et al., 2023).Except for Li et al. (2023), these consider users' utterances in isolation rather than as part of a conversation or dialogue.</p>
<p>There also exists a corpus of work on task-oriented dialogue systems.This area of research is focused on collecting realistic, task-oriented dialogue for the tasks of intent classification and slot filling (Larson &amp; Leach, 2022).Some popular task-oriented dialogue datasets include Mul-tiWoz (Budzianowski et al., 2018), Taskmaster and TicketTalk (Byrne et al., 2019;2020), andSTAR andSTARv2 (Mosig et al., 2020;Zhao et al., 2022).The goals of creating realistic dialogue and evaluating on intent classification and slot filling have some overlap with ToolTalk.However, taskoriented dialogue datasets usually only predict a single intent per user utterance, do not simulate plugins or tools, and do not provide execution feedback for predicted tool calls.TicketTalk (Byrne et al., 2020) is notable in that it does provide a simulation of a movie booking API, however this API does not provide execution feedback and is not rigorously defined allowing for loose arguments like "here" or "now".</p>
<p>CONCLUSION</p>
<p>We present ToolTalk, a new benchmark for evaluating tool-augmented LLMs in a conversational setting.Our benchmark emphasizes complex orchestration of multiple tools in a conversational setting.We provide simulated implementations of all tools, allowing for a fully automated evaluation where the LLM can decide which tools to further invoke based on the results of prior tool calls.Finally, we also introduce a unique form of evaluating correctness that takes into account unique aspects of individual tools and whether a tool usage system produces incorrect actions.We evaluate GPT-3.5 and GPT-4 using our dataset and methodology and analyze their errors, finding three major categories: premature tool calls, faulty reasoning, and incorrect invocations of the correct tool.In the future, we hope to expand the scope of this dataset to more conversations and simulate even more, diverse plugins.We also hope to see future research look into how to better redesign existing API interfaces for LLMs.</p>
<p>• DeleteAlarm Removes an alarm given an alarm id.</p>
<p>• FindAlarms Finds alarms a user has set.</p>
<p>Calendar This API lets a users manage events in their calendar.</p>
<p>• CreateEvent Adds events to a user's calendar.</p>
<p>• DeleteEvent Deletes events from a user's calendar.</p>
<p>• ModifyEvent Allows modification of an existing event.</p>
<p>• QueryCalendar Queries for events that occur in a time range.</p>
<p>Email This API lets a user search and send emails.</p>
<p>• SearchInbox Searches for emails matching filters returning 5 most recent results.</p>
<p>• SendEmail Sends an email on behalf of a given user.</p>
<p>Message This API lets a user search and send messages.</p>
<p>• SearchMessages Searches messages matching filters returning 5 most recent results.</p>
<p>• SendMessage Sends a message to another user.</p>
<p>Reminder A suite of APIs for managing reminders for a TODO list.</p>
<p>• AddReminder Add a reminder with an optional due date.</p>
<p>• CompleteReminder Complete a reminder.</p>
<p>• DeleteReminder Delete a reminder.</p>
<p>• GetReminders Get a list of reminders.</p>
<p>Weather Get weather information of a location.</p>
<p>• CurrentWeather Get the current weather of a location.</p>
<p>• ForecastWeather Get the 3-day weather forecast of a location.</p>
<p>• HistoricWeather Get historic weather information of a location by month.</p>
<p>B SCENARIO PROMPT</p>
<h1>Task You will be provided with a list of APIs.These APIs will have a description and a list of parameters and return types for each tool.Your task involves creating 3 varied, complex, and detailed user scenarios that require at least 5 API calls to complete involving at least 3 different APIs.One of these APIs will be explicitly provided and the other two will be chosen by you.</h1>
<p>For instance, given the APIs: SearchHotels, BookHotel, CancelBooking, GetNFLNews.Given that GetNFLNews is explicitly provided, your scenario should articulate something akin to: "The user wants to see if the Broncos won their last game (GetNFLNews).They then want to see if that qualifies them for the playoffs and who they will be playing against (GetNFLNews).The Broncos did make it into the playoffs, so the user wants watch the game in person.They want to look for hotels where the playoffs are occurring (GetNBANews + SearchHotels).After looking at the options, the user chooses to book a 3-day stay at the cheapest 4-star option (BookHotel)."</p>
<p>This scenario exemplifies a scenario using 5 API calls.The scenario is complex, detailed, and concise as desired.The scenario also includes two APIs used in tandem, the required API, GetNBANews to search for the playoffs location and SearchHotels to find hotels based on the returned location.Usage of multiple APIs in tandem is highly desirable and will receive a higher score.Ideally each scenario should contain one or more instances of multiple APIs being used in tandem.</p>
<p>Note that this scenario does not use all the APIs given and re-uses the " GetNBANews" API.Re-using APIs is allowed, but each scenario should involve at least 3 different APIs.Note that API usage is also included in the scenario, but exact parameters are not necessary.You must use a different combination of APIs for each scenario.All APIs must be used in at least one scenario.You can only use the APIs provided in the APIs section.</p>
<p>Note that API calls are not explicitly mentioned and their uses are included in parentheses.This behaviour should be mimicked in your response.</p>
<p>Deliver your response in this format: ''' -Scenario 1: <Scenario1> -Scenario 2: <Scenario2> -Scenario 3: <Scenario3> ''' # APIs ''' {{API_DOCS}} ''' # Response Required API: {{REQUIRED_API}} Scenarios with &gt;=5 API calls: ''' -Scenario 1:</p>
<p>C UNREALISTIC QUERIES Below are some examples of unrealistic queries gathered from various sources.These queries are useful for generating potentially complex tool interactions or unusual combinations of tools.However, as a consequence, they are unrealistic for various reasons such as forcing the usage of disparate APIs in situations a human is unlikely to ask for, explicitly asking for API endpoints which end users are unlikely to know of, or generally being unnaturally long and explicit.</p>
<p>• "I'm working on a logistics project for my company and need to check the health of the SQUAKE API.Can you verify the API health by calling the 'Checkhealth' API endpoint?Additionally, I would like to retrieve the list of projects using the 'Projects' API endpoint."(Qin et al., 2023b) • "How many singers have the average number of albums of singers in Beijing?Gives the square root of this number."(Ruan et al., 2023) • "I am looking for x-large, red color women faux fur lined winter warm jacket coat, and price lower than 70.00 dollars."Yao et al. (2022a) • "Can you retrieve the contact details of the 'Gondrand' customs agency in New Caledonia?I'm particularly interested in their postal code, email, name, and phone number.Also, please provide a list of all available transitaires.Begin!!!" Qin et al. (2023b) D NUANCES COMPARING PRIOR WORK Nuances comparing prior work from Table 5. ReAct and ART evaluate on hard QA tasks, but these tasks traditionally do not require the usage of tools to complete.We also note API-Bank fits all criteria.However, its level 1-2 examples are automated but simple.In comparison, its level 3 examples are complex but require manual evaluation.ToolBench's tasks require hard to use tool, but have shallow solution paths.Their more complex environments have deeper solution paths, but re-use the existing datasets of WebShop (Yao et al., 2022a) and TableTop (Liang et al., 2023).</p>
<p>[</p>
<p>LLM] Assistant (Finished Predictions) "I found two emails related to Edinburgh…"[Execute] SearchEmail • From: sallee@fakemail.com,Subject:Come visit me in Edinburgh… • From: hot-dealz-noreply@dealz.com, Subject: FLASH SALE in Edinburgh... Assistant (Finished Predictions) "I found two emails related to Edinburgh…"</p>
<p>[Execute] SearchEmail SearchEmail missing required argument "query" no match match Predict Predict Predict [LLM] Tool Call SearchEmail</p>
<p>(to= "jesse@wrongmail.com"…)</p>
<p>[Execute] SendEmail Success, email_id = 67890 incorrect action Predict LLM [Ground Truth] SearchEmail
• From: sallee@fakemail.com, Subject:Come visit me in Edinburgh…• From: hot-dealz-noreply@dealz.com,Subject: FLASH SALE in Edinburgh...</p>
<p>[Ground Truth] Tool Call SearchEmail(query="Edinburgh") [Ground Truth] SendEmail Success, email_id = 12345 [Ground Truth] Tool Call
SendEmail(to= "jesse@fmail.com",subject=...)</p>
<p>missing Ground Truth [Ground Truth] SearchEmail
• From: sallee@fakemail.com, Subject:Come visit me in Edinburgh…• From: hot-dealz-noreply@dealz.com,Subject: FLASH SALE in Edinburgh...</p>
<p>[Ground Truth] Tool Call SearchEmail(query="Edinburgh") [Ground Truth] SendEmail Success, email_id = 12345 [Ground Truth] Tool Call
Conversation HistoryGround TruthSystem SystemYou are a helpful assistant. Here is some user You are a helpful assistant. Here is some userdata: data:location: London location: Londontimestamp: 2023-09-14 09:00:00 timestamp: 2023-09-14 09:00:00username (if logged in): decture username (if logged in): dectureUser UserI want to go to Edinburgh this weekend, can I want to go to Edinburgh this weekend, canyou check the weather conditions? you check the weather conditions?[Ground Truth] Tool Call [Ground Truth] Tool CallForecastWeather(location="Edinburgh") ForecastWeather(location="Edinburgh")SendEmail(to= "jesse@fmail.com",[Ground Truth] ForecastWeather [Ground Truth] ForecastWeathersubject=...)9/15 60% rainy …, 9/16 Cloudy …, 9/17 Light 9/15 60% rainy …, 9/16 Cloudy …, 9/17 Light rain ... rain ...missing[Ground Truth] Assistant [Ground Truth] AssistantThe weather in Edinburgh will be rainy The weather in Edinburgh will be rainytomorrow, cloudy the day after, and drizzly the tomorrow, cloudy the day after, and drizzly theday after that. day after that.User UserPerfect, can you look up my email for anything Perfect, can you look up my email for anythingabout Edinburgh and send an email to my about Edinburgh and send an email to myfriend saying I'm coming to visit? Their address friend saying I'm coming to visit? Their addressis jesse@fmail.com. is jesse@fmail.com.</p>
<p>A ← ∀p ∈ P where p is action 11: I ← ∀a ∈ A where a / ∈ M 12: precision ← |M |/|P | 13: recall ← |M |/|G| 14: incorrect action rate ← |I|/|A| 15: success ← (M == G) ∧ (I == ∅) 16: return precision, recall, incorrect action rate, success
Algorithm 1 Conversation simulationAlgorithm 2 ToolTalk evaluationRequire: conversation T an array of turnsRequire: tool predictions for single conversa-Require: Each turn contains a user utterance,tion Pground truth tool calls, and a ground truthRequire: ground truth tool calls for single con-assistant replyversation GRequire: tool prediction function LLM1: M ← ∅ # matchesRequire: tool execution function Exec2: for g ∈ G do1: h ← [] # conversation history3:for p ∈ P do2: p ← [] # predictions4:if g.match(p) then3: for t ∈ T do5:M ← M ∪ {p}4:h.append(t.user utterance)6:break5:u ← [] # turn history7:end if6:c ← LLM (h + u) # current prediction8:end for7:while c is not assistant reply do9: end for8:c.exec feedback ← Exec(r)10:9:u.append(c)10:p.append(c)11:c ← LLM (h + u)12:end while13:h.extend(t.ground truth tools)14:h.append(t.ground truth assistant reply)15: end for16: return p</p>
<p>Table 1 :
1
and GPT-4 evaluated on easy and hard versions of ToolTalk.as |M |/|P | and |M |/|G| respectively.Additionally, we define A as the set of all actions predicted and I as the set of incorrect actions and calculate incorrect action rate as |I|/|A|.
ModelSubset Success rate Precision Recall Incorrect action rateGPT-3.5Easy85.7%42.4%89.3%5.0%GPT-4Easy92.8%69.2%96.4%3.8%GPT-3.5Hard26.0%54.6%69.7%23.9%GPT-4Hard50.0%74.9%79.0%25.1%</p>
<p>Table 2 :
2
Percent of failing error types out of all failing turns for GPT-3.5 and GPT-4.
ModelPremature tool calls Faulty planning Incorrect tool invocations Total failuresGPT-3.526.9%53.7%19.4%67GPT-432.0%42.0%26.0%50</p>
<p>Table 3 :
3
and GPT-4 evaluated without documentation on ToolTalk.
ModelSubset Success rate Precision Recall Incorrect action rateGPT-3.5Easy82.1%35.8%85.7%2.2%GPT-4Easy85.7%52.0%92.9%5.7%GPT-3.5Hard16.0%40.1%62.6%11.8%GPT-4Hard34.0%40.6%64.3%13.0%ModelPremature tool calls Faulty planning Incorrect tool invocations Total failuresGPT-3.512.3%71.2%16.4%73GPT-416.2%60.3%23.5%68</p>
<p>Table 4 :
4
Error types without documentation for GPT-3.5 and GPT-4.</p>
<p>For scenarios that use tools such as UserLogin or RegisterAccount, we omit the username to simulate a user that has yet to log in or have an account.
https://github.com/pdrm83/sent2vec
For the SendEmail and SendMessage tools, we ignore errors which occur due to invalid recipient emails or usernames.
We include a few examples from various papers in Appendix C.
https://github.com/microsoft/ToolTalk
REPRODUCIBILITYWe make ToolTalk more widely available by releasing it on github 5 .We include the exact versions of  and GPT-4 (gpt-4-0613) available through the OpenAI API to be able to reproduce our results after release.We include the prompt used to generate our scenarios in Appendix B. We include information on system prompts and our application of OpenAI's Chat completions API in Section 4.1.A COMPLETE LIST OF TOOLSWe include the complete list of plugins and tools used in ToolTalk, and their corresponding descriptions.AccountTools This API contains tools for account management.• ChangePassword Changes the password of an account.• DeleteAccount Deletes a user's account, requires user to be logged in.• GetAccountInformation Retrieves account information of logged in user.• LogoutUser Logs user out.• QueryUser Finds users given a username or email.• RegisterUser Register a new user.• ResetPassword Resets the password of a user using a verification code.• SendVerificationCode Initiates a password reset for a user by sending a verification code to a backup email.• UpdateAccountInformation Updates account information of a user.• UserLogin Logs in a user.Alarm This API contains tools for managing alarms.• AddAlarm Sets an alarm for a specific time.
Multiwoz -a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gasic, Conference on Empirical Methods in Natural Language Processing. 2018</p>
<p>Taskmaster-1: Toward a realistic and diverse dialog dataset. Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Daniel Duckworth, Semih Yavuz, Ben Goodrich, Amit Dubey, Andy Cedilnik, Kyu-Young Kim, ArXiv, abs/1909.053582019</p>
<p>Tickettalk: Toward humanlevel performance with end-to-end, transaction-based dialog systems. Bill Byrne, Karthik Krishnamoorthi, Saravanan Ganesh, Mihir Kale, ArXiv, abs/2012.124582020</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Bo Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, ArXiv, abs/2306.060702023</p>
<p>Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu, 2023</p>
<p>A survey of intent classification and slot-filling datasets for taskoriented dialog. Stefan Larson, Kevin Leach, ArXiv, abs/2207.132112022</p>
<p>Mlqa: Evaluating cross-lingual extractive question answering. Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, Holger Schwenk, 2020</p>
<p>Apibank: A benchmark for tool-augmented llms. Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, ArXiv, abs/2304.082442023</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023</p>
<p>Agentbench: Evaluating llms as agents. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang, ArXiv, abs/2308.036882023a</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Thi Phí, Haiquan Mùi, Wang, ArXiv, abs/2308.05960Caiming Xiong, and Silvio Savarese. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. 2023b</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, ArXiv, abs/2304.098422023a</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, 2023b</p>
<p>. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 2023Augmented language models: a survey</p>
<p>Star: A schema-guided dialog dataset for transfer learning. E M Johannes, Shikib Mosig, Thomas Mehri, Kober, ArXiv, abs/2010.118532020</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2022</p>
<p>Function calling and other api updates. Openai, </p>
<p>Automatic multi-step reasoning and tool-use for large language models. Bhargavi Paranjape, Scott M Lundberg, Sameer Singh, Hanna Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , ArXiv, abs/2303.090142023Art</p>
<p>Gorilla: Large language model connected with massive apis. G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, ArXiv, abs/2305.153342023</p>
<p>Making language models better tool learners with execution feedback. Shuofei Qiao, Honghao Gui, Huajun Chen, Ningyu Zhang, ArXiv, abs/2305.130682023</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu, Zhenning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin Zhao, Yuxiang Huang, Jun-Han Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, ArXiv, abs/2304.083542023aZhiyuan Liu, and Maosong Sun</p>
<p>Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H Gerstein, Dahai Li, ArXiv, abs/2307.16789Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023b</p>
<p>Tptu: Task planning and tool usage of large language model-based ai agents. Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, ArXiv, abs/2308.034272023</p>
<p>Scienceqa: a novel resource for question answering on scholarly articles. Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, Pushpak Bhattacharyya, International Journal on Digital Libraries. 232022</p>
<p>Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of mt quality. Carolina Scarton, Mikel L Forcada, Miquel Esplà-Gomis, Lucia Specia, 2019</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, ArXiv, abs/2302.047612023</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew J Hausknecht, ArXiv, abs/2010.037682020</p>
<p>Restgpt: Connecting large language models with real-world restful apis. Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Chengzu Li, Ke Wang, Rong Yao, Ye Tian, Sujian Li, 2023</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 2019</p>
<p>Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Le Sun, ArXiv, abs/2306.053012023</p>
<p>On the tool manipulation capability of open-source large language models. Qiantong Xu, Fenglu Hong, B Li, Changran Hu, Zhe Chen, Jian Zhang, ArXiv, abs/2305.165042023</p>
<p>Gpt4tools: Teaching large language model to use tools via self-instruction. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, ArXiv, abs/2305.187522023</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing. 2018</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, ArXiv, abs/2207.012062022a</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, ArXiv, abs/2210.036292022b</p>
<p>Anytod: A programmable task-oriented dialog system. Jeffrey Zhao, Yuan Cao, Raghav Gupta, Harrison Lee, Abhinav Rastogi, Mingqiu Wang, Hagen Soltau, Izhak Shafran, Yonghui Wu, ArXiv, abs/2212.099392022</p>
<p>Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and reading: A comprehensive survey on open-domain question answering. Fengbin Zhu, Wenqiang Lei, Chao Wang, 2021</p>
<p>Toolqa: A dataset for llm question answering with external tools. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang, ArXiv, abs/2306.133042023</p>            </div>
        </div>

    </div>
</body>
</html>
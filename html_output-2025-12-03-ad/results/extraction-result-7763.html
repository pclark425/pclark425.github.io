<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7763 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7763</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7763</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-268351639</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.06749v1.pdf" target="_blank">Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies</a></p>
                <p><strong>Paper Abstract:</strong> . Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7763.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7763.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long Context Window</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Context Window Capability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model capability requirement that the LLM can ingest and reason over very large textual inputs (event logs, process model specifications) without truncation or severe degradation of output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>process mining / computer science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>capability criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Context-window capacity assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Measure whether the model can accept and correctly process textual abstractions of event logs or process models within its token/context limit; may include stress tests with progressively larger prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>maximum effective context length</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Number of input tokens the model can accept while producing outputs that retain fidelity to the whole input (tokens); often reported as token count (e.g., 8k, 32k).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BAMBOO (long-text benchmarks) and related long-context tests</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not specified in paper; typically an automated input-size sweep with human spot-checks for fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Quality may decline as context window is extended; some methods extend window without tuning but may reduce quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7763.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accepting Visual Prompts / Visual Understanding Capability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The LLM/LVM must accept and interpret visualizations used in process mining (e.g., dotted charts, performance spectrum) to extract features and answer analytical questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Vision Models / Multimodal LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>process mining / multimodal ML</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>capability criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Visual benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Assess model accuracy interpreting PM visualizations via multimodal benchmarks and task-specific image+query tests (e.g., identifying patterns, orientations, color/size encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>visual task accuracy; OCR/recognition scores; interpretation correctness</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage or score of correctly answered visual queries (0–100%), OCR accuracy (character/word accuracy), task-specific correctness measures.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MMBench; MM-Vet; MMBench / MM-Vet noted as existing multimodal benchmarks but potentially insufficient for PM visuals</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Paper suggests human evaluation for interpretive quality but provides no numeric details.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Existing multimodal benchmarks may not capture PM-specific visual features (line orientations, point size/color semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7763.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-to-SQL Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-to-SQL Automatic Evaluation (formal accuracy and conciseness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic evaluation method for code-generation tasks where natural language prompts are converted into SQL queries and evaluated for formal correctness and conciseness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating the Text-to-SQL Capabilities of Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>process mining / databases</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method (query generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Formal SQL accuracy and conciseness</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Execute generated SQL against ground-truth databases or compare query ASTs to gold queries; measure whether produced queries return correct results and how concise the query is (e.g., token length).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>formal correctness (binary or percentage), execution accuracy, conciseness (query length)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Formal correctness: proportion of generated queries that are syntactically correct and return expected results (0–100%); conciseness: length in tokens or characters (lower is more concise).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SPIDER / SPIDER-realistic; APPS (for code generation) cited as relevant</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automatic evaluation preferred; human inspection used for ambiguous cases but details not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>SQL evaluation focuses on formal correctness but may not capture conceptual appropriateness for PM concepts without domain-aware gold queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7763.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic evaluation (textual outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Evaluation for LLM Outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated methods to evaluate LLM outputs, especially suitable for structured outputs (e.g., SQL, declarative constraints) via formal correctness checks and conciseness metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>process mining / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Formal correctness and conciseness automated checks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Automated syntactic/semantic evaluation (e.g., execute SQL, validate declarative constraints against schema/rules), and measure conciseness through token/character counts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>formal accuracy, conciseness (length)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Formal accuracy: percent of outputs passing automated correctness tests; conciseness: length measure in tokens/characters.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Task-appropriate evaluation suites (e.g., text-to-SQL benchmarks like SPIDER); no single PM-specific dataset cited.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable for purely automated tests; paper recommends human evaluation for open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Automated checks work well for structured outputs but are insufficient for open-ended analyses (hypotheses, root-cause narratives).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7763.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation (recall/precision)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Evaluation using Recall and Precision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human judges assess direct-query and hypothesis-generation outputs for completeness (recall) and correctness (precision), especially for anomaly detection and root cause analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>process mining / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert rating (recall & precision)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human experts compare model-generated insights against an expected set of insights; recall = fraction of expected insights found, precision = fraction of model insights that are correct.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>recall, precision, possibly F1</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Recall = (true positives) / (true positives + false negatives); Precision = (true positives) / (true positives + false positives); reported as percentages (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Task-specific event logs and gold sets of expected insights (not specified concretely in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Paper recommends human evaluation but provides no numeric details (no number of reviewers, scales, or inter-rater agreement reported).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human evaluation is resource-intensive and subjective; no standardized PM-specific human evaluation protocol is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7763.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-evaluation techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Self-Evaluation Techniques (chain-of-thought, confidence scores, ensembling, self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of LLM-side techniques to reduce hallucination and improve reliability: chain-of-thought reasoning, confidence scoring, ensembling across runs, and self-reflection/error checking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Evaluation Improves Selective Generation in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / process mining</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation/self-assessment methods</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Chain-of-thought, confidence scoring, ensembling, self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Chain-of-thought: elicit stepwise reasoning to expose and check logic; confidence scores: model estimates reliability of outputs to filter low-confidence items; ensembling: aggregate multiple independent outputs (majority or confidence-weighted); self-reflection: model critiques and corrects outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>self-reported confidence, agreement rates, ensemble consensus rate, error-detection rate</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Confidence: scalar score per output (often 0–1 or 0–100); ensemble consensus: fraction of runs agreeing on an output; error-detection rate: proportion of incorrect outputs detected by self-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Techniques referenced across studies (Chain-of-Thought by Wei et al., Ren et al. for self-eval); no PM-specific dataset cited.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Paper suggests combining self-eval with human review; no concrete human-eval protocol provided.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Self-evaluation methods can reduce but not eliminate hallucination; confidence calibration issues (confidence-competence gap) are noted in referenced literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7763.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factuality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factuality / Hallucination Measurement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Criterion measuring an LLM's tendency to produce factually correct outputs versus hallucinated or fabricated statements, often mitigated by grounding to external data sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>process mining / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Factuality checks via external grounding</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Cross-check model outputs against external databases, knowledge bases, or web search to verify claims; measure frequency of hallucinations or contradictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>hallucination rate, factual accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Hallucination rate: proportion of statements that are incorrect or fabricated (0–100%); factual accuracy: proportion of verifiable claims that are true (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>No single dataset cited; suggested use of external DBs, KBs, or web browsing; references include works on hallucination quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human judges often required to adjudicate ambiguous cases; paper does not report numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires grounded data and up-to-date sources; web access improves factuality but introduces its own challenges (retrieval quality, latency).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7763.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGIEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AGIEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-centric benchmark assessing foundation models on standardized exam-style tasks, measuring reasoning and instruction-following abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general reasoning / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Exam-style benchmark assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate LLMs by scoring model answers to exam-like questions across domains; typically human-graded or using gold answers for automated scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy / score on exam items (percentage or points)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Score often reported as percentage of items answered correctly or scaled exam score.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AGIEval benchmark (collection of exam questions across topics)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Benchmark is human-centric; some items require human grading; specific reviewer counts not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Exam-style benchmarks assess general reasoning but may not capture PM-specific needs (visualization interpretation, SQL over event logs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7763.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAMBOO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BAMBOO (Benchmark for Long Text Modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark suite designed to evaluate long-text modeling capacities of LLMs, useful for assessing models' behavior on tasks requiring long context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / long-text evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Long-text task suite</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collection of tasks requiring long-context reasoning and generation to measure performance degradation with increasing input lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>task-specific accuracy / quality metrics as a function of context length</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported as task accuracy or quality measures plotted/compared across different input lengths (tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BAMBOO benchmark suite</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Some tasks may require human judgments; paper does not provide specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Designed for general long-texts; may not fully cover PM-specific structured long inputs like large event logs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7763.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XIEZHI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XIEZHI Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ever-updating benchmark for holistic domain knowledge evaluation across multiple fields (economics, science, engineering), useful for assessing domain knowledge LLMs need in PM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>domain-knowledge evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Domain knowledge question-answer evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Pose domain-specific knowledge questions and evaluate model correctness against curated gold answers, capturing breadth and recency of knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy across domain questions (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of domain questions answered correctly (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>XIEZHI benchmark dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Benchmark curators likely provide gold answers; no human rater counts provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Domain knowledge benchmarks may not directly test PM-specific reasoning over structured event data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7763.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Advanced Reasoning Benchmark (ARB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark targeting advanced reasoning capabilities (mathematics, science) of LLMs to assess deeper logical reasoning required in PM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ARB: Advanced Reasoning Benchmark for Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reasoning / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Advanced reasoning problem set evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate LLMs on a suite of reasoning-intensive tasks and measure correctness and chain-of-thought coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy on reasoning tasks, reasoning step correctness</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of reasoning tasks correctly solved (0–100%); may include stepwise correctness measures.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ARB benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>May require human validation of reasoning; paper does not give specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Focuses on general reasoning not PM-specific sequence/temporal reasoning over event logs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7763.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMBench / MM-Vet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MMBench and MM-Vet (Multimodal Benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks for evaluating multimodal models on integrated capabilities like image recognition, OCR, and cross-modal reasoning; referenced as relevant to PM visualization tasks but possibly insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MMBench: Is Your Multi-modal Model an Allaround Player?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multimodal LLMs / LVMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multimodal evaluation / process mining visuals</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multimodal task suite</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Assess models on a variety of image+text tasks including recognition, OCR, and interpretive questions to evaluate cross-modal understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>task accuracy, OCR accuracy, recognition metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent correct answers for tasks, OCR character/word accuracy (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MMBench; MM-Vet</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Some tasks may require human scoring; not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>May not capture PM-specific visual features (orientation, encodings) necessary for dotted chart/performance spectrum interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7763.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DecodingTrust</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DecodingTrust (Trustworthiness Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive assessment framework for LLM trustworthiness covering toxicity, bias, robustness, privacy, ethics, and fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>trustworthiness / ethics / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark/framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Trustworthiness assessment across multiple axes</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Measure model outputs across standardized tasks and metrics for toxicity, bias, robustness, privacy leaks, and fairness to derive a composite trustworthiness profile.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>toxicity scores, bias metrics, robustness/error rates, privacy leakage rates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Task-specific scales (e.g., toxicity measured by classifier scores or human ratings, bias measured by disparity metrics, all typically reported as percentages or scalar scores).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DecodingTrust benchmark (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>May combine automatic detectors with human judgments; paper does not provide reviewer counts.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Designed for general LLM trustworthiness; PM-specific fairness checks (e.g., group disparity in process metrics) may need domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7763.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7763.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis Generation Evaluation (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation strategies for LLM-generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Suggested evaluation approaches for hypotheses produced by LLMs, combining human assessment for recall/precision, SQL- or data-backed verification, and self-assessment techniques; explicit PM-specific benchmarks are lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>process mining / scientific hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hybrid evaluation: human review + automatic verification + self-eval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human experts assess relevance and correctness (recall/precision); generated SQL or scripts are executed to verify hypotheses against event data; self-eval used to filter low-confidence hypotheses and ensemble for consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>recall, precision, verification pass rate, confidence threshold pass rate</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Recall/precision as defined above; verification pass rate = fraction of hypotheses that are empirically supported by executing verification queries (0–100%); confidence pass rate = fraction above chosen confidence threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>No PM-specific hypothesis benchmark exists per paper; related evaluation approaches referenced in literature on scientific hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Paper recommends human evaluation but provides no numeric details (number of experts, scales, or agreement measures not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lack of standardized PM-specific benchmarks for hypothesis generation; human evaluation required and costly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. <em>(Rating: 2)</em></li>
                <li>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. <em>(Rating: 2)</em></li>
                <li>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. <em>(Rating: 2)</em></li>
                <li>ARB: Advanced Reasoning Benchmark for Large Language Models. <em>(Rating: 2)</em></li>
                <li>MMBench: Is Your Multi-modal Model an Allaround Player?. <em>(Rating: 2)</em></li>
                <li>Evaluating the Text-to-SQL Capabilities of Large Language Models. <em>(Rating: 2)</em></li>
                <li>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. <em>(Rating: 2)</em></li>
                <li>Self-Evaluation Improves Selective Generation in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7763",
    "paper_id": "paper-268351639",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Long Context Window",
            "name_full": "Long Context Window Capability",
            "brief_description": "A model capability requirement that the LLM can ingest and reason over very large textual inputs (event logs, process model specifications) without truncation or severe degradation of output quality.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "process mining / computer science",
            "theory_type": "capability criterion",
            "evaluation_method_name": "Context-window capacity assessment",
            "evaluation_method_description": "Measure whether the model can accept and correctly process textual abstractions of event logs or process models within its token/context limit; may include stress tests with progressively larger prompts.",
            "evaluation_metric": "maximum effective context length",
            "metric_definition": "Number of input tokens the model can accept while producing outputs that retain fidelity to the whole input (tokens); often reported as token count (e.g., 8k, 32k).",
            "dataset_or_benchmark": "BAMBOO (long-text benchmarks) and related long-context tests",
            "human_evaluation_details": "Not specified in paper; typically an automated input-size sweep with human spot-checks for fidelity.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Quality may decline as context window is extended; some methods extend window without tuning but may reduce quality.",
            "uuid": "e7763.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Visual Prompting",
            "name_full": "Accepting Visual Prompts / Visual Understanding Capability",
            "brief_description": "The LLM/LVM must accept and interpret visualizations used in process mining (e.g., dotted charts, performance spectrum) to extract features and answer analytical questions.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Large Vision Models / Multimodal LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "process mining / multimodal ML",
            "theory_type": "capability criterion",
            "evaluation_method_name": "Visual benchmark evaluation",
            "evaluation_method_description": "Assess model accuracy interpreting PM visualizations via multimodal benchmarks and task-specific image+query tests (e.g., identifying patterns, orientations, color/size encodings).",
            "evaluation_metric": "visual task accuracy; OCR/recognition scores; interpretation correctness",
            "metric_definition": "Percentage or score of correctly answered visual queries (0–100%), OCR accuracy (character/word accuracy), task-specific correctness measures.",
            "dataset_or_benchmark": "MMBench; MM-Vet; MMBench / MM-Vet noted as existing multimodal benchmarks but potentially insufficient for PM visuals",
            "human_evaluation_details": "Paper suggests human evaluation for interpretive quality but provides no numeric details.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Existing multimodal benchmarks may not capture PM-specific visual features (line orientations, point size/color semantics).",
            "uuid": "e7763.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Text-to-SQL Evaluation",
            "name_full": "Text-to-SQL Automatic Evaluation (formal accuracy and conciseness)",
            "brief_description": "Automatic evaluation method for code-generation tasks where natural language prompts are converted into SQL queries and evaluated for formal correctness and conciseness.",
            "citation_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "process mining / databases",
            "theory_type": "method (query generation)",
            "evaluation_method_name": "Formal SQL accuracy and conciseness",
            "evaluation_method_description": "Execute generated SQL against ground-truth databases or compare query ASTs to gold queries; measure whether produced queries return correct results and how concise the query is (e.g., token length).",
            "evaluation_metric": "formal correctness (binary or percentage), execution accuracy, conciseness (query length)",
            "metric_definition": "Formal correctness: proportion of generated queries that are syntactically correct and return expected results (0–100%); conciseness: length in tokens or characters (lower is more concise).",
            "dataset_or_benchmark": "SPIDER / SPIDER-realistic; APPS (for code generation) cited as relevant",
            "human_evaluation_details": "Automatic evaluation preferred; human inspection used for ambiguous cases but details not provided.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "SQL evaluation focuses on formal correctness but may not capture conceptual appropriateness for PM concepts without domain-aware gold queries.",
            "uuid": "e7763.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Automatic evaluation (textual outputs)",
            "name_full": "Automatic Evaluation for LLM Outputs",
            "brief_description": "Automated methods to evaluate LLM outputs, especially suitable for structured outputs (e.g., SQL, declarative constraints) via formal correctness checks and conciseness metrics.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "process mining / NLP evaluation",
            "theory_type": "evaluation method",
            "evaluation_method_name": "Formal correctness and conciseness automated checks",
            "evaluation_method_description": "Automated syntactic/semantic evaluation (e.g., execute SQL, validate declarative constraints against schema/rules), and measure conciseness through token/character counts.",
            "evaluation_metric": "formal accuracy, conciseness (length)",
            "metric_definition": "Formal accuracy: percent of outputs passing automated correctness tests; conciseness: length measure in tokens/characters.",
            "dataset_or_benchmark": "Task-appropriate evaluation suites (e.g., text-to-SQL benchmarks like SPIDER); no single PM-specific dataset cited.",
            "human_evaluation_details": "Not applicable for purely automated tests; paper recommends human evaluation for open-ended tasks.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Automated checks work well for structured outputs but are insufficient for open-ended analyses (hypotheses, root-cause narratives).",
            "uuid": "e7763.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Human evaluation (recall/precision)",
            "name_full": "Human Evaluation using Recall and Precision",
            "brief_description": "Human judges assess direct-query and hypothesis-generation outputs for completeness (recall) and correctness (precision), especially for anomaly detection and root cause analysis.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "process mining / evaluation methodology",
            "theory_type": "evaluation method",
            "evaluation_method_name": "Human expert rating (recall & precision)",
            "evaluation_method_description": "Human experts compare model-generated insights against an expected set of insights; recall = fraction of expected insights found, precision = fraction of model insights that are correct.",
            "evaluation_metric": "recall, precision, possibly F1",
            "metric_definition": "Recall = (true positives) / (true positives + false negatives); Precision = (true positives) / (true positives + false positives); reported as percentages (0–100%).",
            "dataset_or_benchmark": "Task-specific event logs and gold sets of expected insights (not specified concretely in paper).",
            "human_evaluation_details": "Paper recommends human evaluation but provides no numeric details (no number of reviewers, scales, or inter-rater agreement reported).",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Human evaluation is resource-intensive and subjective; no standardized PM-specific human evaluation protocol is provided.",
            "uuid": "e7763.4",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Self-evaluation techniques",
            "name_full": "LLM Self-Evaluation Techniques (chain-of-thought, confidence scores, ensembling, self-reflection)",
            "brief_description": "A suite of LLM-side techniques to reduce hallucination and improve reliability: chain-of-thought reasoning, confidence scoring, ensembling across runs, and self-reflection/error checking.",
            "citation_title": "Self-Evaluation Improves Selective Generation in Large Language Models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "NLP / process mining",
            "theory_type": "evaluation/self-assessment methods",
            "evaluation_method_name": "Chain-of-thought, confidence scoring, ensembling, self-reflection",
            "evaluation_method_description": "Chain-of-thought: elicit stepwise reasoning to expose and check logic; confidence scores: model estimates reliability of outputs to filter low-confidence items; ensembling: aggregate multiple independent outputs (majority or confidence-weighted); self-reflection: model critiques and corrects outputs.",
            "evaluation_metric": "self-reported confidence, agreement rates, ensemble consensus rate, error-detection rate",
            "metric_definition": "Confidence: scalar score per output (often 0–1 or 0–100); ensemble consensus: fraction of runs agreeing on an output; error-detection rate: proportion of incorrect outputs detected by self-evaluation.",
            "dataset_or_benchmark": "Techniques referenced across studies (Chain-of-Thought by Wei et al., Ren et al. for self-eval); no PM-specific dataset cited.",
            "human_evaluation_details": "Paper suggests combining self-eval with human review; no concrete human-eval protocol provided.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Self-evaluation methods can reduce but not eliminate hallucination; confidence calibration issues (confidence-competence gap) are noted in referenced literature.",
            "uuid": "e7763.5",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Factuality",
            "name_full": "Factuality / Hallucination Measurement",
            "brief_description": "Criterion measuring an LLM's tendency to produce factually correct outputs versus hallucinated or fabricated statements, often mitigated by grounding to external data sources.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "process mining / NLP",
            "theory_type": "evaluation criterion",
            "evaluation_method_name": "Factuality checks via external grounding",
            "evaluation_method_description": "Cross-check model outputs against external databases, knowledge bases, or web search to verify claims; measure frequency of hallucinations or contradictions.",
            "evaluation_metric": "hallucination rate, factual accuracy",
            "metric_definition": "Hallucination rate: proportion of statements that are incorrect or fabricated (0–100%); factual accuracy: proportion of verifiable claims that are true (0–100%).",
            "dataset_or_benchmark": "No single dataset cited; suggested use of external DBs, KBs, or web browsing; references include works on hallucination quantification.",
            "human_evaluation_details": "Human judges often required to adjudicate ambiguous cases; paper does not report numbers.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Requires grounded data and up-to-date sources; web access improves factuality but introduces its own challenges (retrieval quality, latency).",
            "uuid": "e7763.6",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "AGIEval",
            "name_full": "AGIEval",
            "brief_description": "A human-centric benchmark assessing foundation models on standardized exam-style tasks, measuring reasoning and instruction-following abilities.",
            "citation_title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "general reasoning / evaluation",
            "theory_type": "benchmark",
            "evaluation_method_name": "Exam-style benchmark assessment",
            "evaluation_method_description": "Evaluate LLMs by scoring model answers to exam-like questions across domains; typically human-graded or using gold answers for automated scoring.",
            "evaluation_metric": "accuracy / score on exam items (percentage or points)",
            "metric_definition": "Score often reported as percentage of items answered correctly or scaled exam score.",
            "dataset_or_benchmark": "AGIEval benchmark (collection of exam questions across topics)",
            "human_evaluation_details": "Benchmark is human-centric; some items require human grading; specific reviewer counts not provided here.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Exam-style benchmarks assess general reasoning but may not capture PM-specific needs (visualization interpretation, SQL over event logs).",
            "uuid": "e7763.7",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "BAMBOO",
            "name_full": "BAMBOO (Benchmark for Long Text Modeling)",
            "brief_description": "A benchmark suite designed to evaluate long-text modeling capacities of LLMs, useful for assessing models' behavior on tasks requiring long context windows.",
            "citation_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "NLP / long-text evaluation",
            "theory_type": "benchmark",
            "evaluation_method_name": "Long-text task suite",
            "evaluation_method_description": "Collection of tasks requiring long-context reasoning and generation to measure performance degradation with increasing input lengths.",
            "evaluation_metric": "task-specific accuracy / quality metrics as a function of context length",
            "metric_definition": "Reported as task accuracy or quality measures plotted/compared across different input lengths (tokens).",
            "dataset_or_benchmark": "BAMBOO benchmark suite",
            "human_evaluation_details": "Some tasks may require human judgments; paper does not provide specifics.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Designed for general long-texts; may not fully cover PM-specific structured long inputs like large event logs.",
            "uuid": "e7763.8",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "XIEZHI",
            "name_full": "XIEZHI Benchmark",
            "brief_description": "An ever-updating benchmark for holistic domain knowledge evaluation across multiple fields (economics, science, engineering), useful for assessing domain knowledge LLMs need in PM.",
            "citation_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "domain-knowledge evaluation",
            "theory_type": "benchmark",
            "evaluation_method_name": "Domain knowledge question-answer evaluation",
            "evaluation_method_description": "Pose domain-specific knowledge questions and evaluate model correctness against curated gold answers, capturing breadth and recency of knowledge.",
            "evaluation_metric": "accuracy across domain questions (percentage)",
            "metric_definition": "Proportion of domain questions answered correctly (0–100%).",
            "dataset_or_benchmark": "XIEZHI benchmark dataset",
            "human_evaluation_details": "Benchmark curators likely provide gold answers; no human rater counts provided here.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Domain knowledge benchmarks may not directly test PM-specific reasoning over structured event data.",
            "uuid": "e7763.9",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ARB",
            "name_full": "Advanced Reasoning Benchmark (ARB)",
            "brief_description": "A benchmark targeting advanced reasoning capabilities (mathematics, science) of LLMs to assess deeper logical reasoning required in PM tasks.",
            "citation_title": "ARB: Advanced Reasoning Benchmark for Large Language Models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "reasoning / evaluation",
            "theory_type": "benchmark",
            "evaluation_method_name": "Advanced reasoning problem set evaluation",
            "evaluation_method_description": "Evaluate LLMs on a suite of reasoning-intensive tasks and measure correctness and chain-of-thought coherence.",
            "evaluation_metric": "accuracy on reasoning tasks, reasoning step correctness",
            "metric_definition": "Proportion of reasoning tasks correctly solved (0–100%); may include stepwise correctness measures.",
            "dataset_or_benchmark": "ARB benchmark",
            "human_evaluation_details": "May require human validation of reasoning; paper does not give specifics.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Focuses on general reasoning not PM-specific sequence/temporal reasoning over event logs.",
            "uuid": "e7763.10",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MMBench / MM-Vet",
            "name_full": "MMBench and MM-Vet (Multimodal Benchmarks)",
            "brief_description": "Benchmarks for evaluating multimodal models on integrated capabilities like image recognition, OCR, and cross-modal reasoning; referenced as relevant to PM visualization tasks but possibly insufficient.",
            "citation_title": "MMBench: Is Your Multi-modal Model an Allaround Player?.",
            "mention_or_use": "mention",
            "model_name": "Multimodal LLMs / LVMs (unspecified)",
            "model_size": null,
            "scientific_domain": "multimodal evaluation / process mining visuals",
            "theory_type": "benchmark",
            "evaluation_method_name": "Multimodal task suite",
            "evaluation_method_description": "Assess models on a variety of image+text tasks including recognition, OCR, and interpretive questions to evaluate cross-modal understanding.",
            "evaluation_metric": "task accuracy, OCR accuracy, recognition metrics",
            "metric_definition": "Percent correct answers for tasks, OCR character/word accuracy (0–100%).",
            "dataset_or_benchmark": "MMBench; MM-Vet",
            "human_evaluation_details": "Some tasks may require human scoring; not specified in paper.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "May not capture PM-specific visual features (orientation, encodings) necessary for dotted chart/performance spectrum interpretation.",
            "uuid": "e7763.11",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "DecodingTrust",
            "name_full": "DecodingTrust (Trustworthiness Benchmark)",
            "brief_description": "A comprehensive assessment framework for LLM trustworthiness covering toxicity, bias, robustness, privacy, ethics, and fairness.",
            "citation_title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "trustworthiness / ethics / evaluation",
            "theory_type": "benchmark/framework",
            "evaluation_method_name": "Trustworthiness assessment across multiple axes",
            "evaluation_method_description": "Measure model outputs across standardized tasks and metrics for toxicity, bias, robustness, privacy leaks, and fairness to derive a composite trustworthiness profile.",
            "evaluation_metric": "toxicity scores, bias metrics, robustness/error rates, privacy leakage rates",
            "metric_definition": "Task-specific scales (e.g., toxicity measured by classifier scores or human ratings, bias measured by disparity metrics, all typically reported as percentages or scalar scores).",
            "dataset_or_benchmark": "DecodingTrust benchmark (as cited)",
            "human_evaluation_details": "May combine automatic detectors with human judgments; paper does not provide reviewer counts.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Designed for general LLM trustworthiness; PM-specific fairness checks (e.g., group disparity in process metrics) may need domain adaptation.",
            "uuid": "e7763.12",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Hypothesis Generation Evaluation (general)",
            "name_full": "Evaluation strategies for LLM-generated hypotheses",
            "brief_description": "Suggested evaluation approaches for hypotheses produced by LLMs, combining human assessment for recall/precision, SQL- or data-backed verification, and self-assessment techniques; explicit PM-specific benchmarks are lacking.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "scientific_domain": "process mining / scientific hypothesis generation",
            "theory_type": "evaluation framework",
            "evaluation_method_name": "Hybrid evaluation: human review + automatic verification + self-eval",
            "evaluation_method_description": "Human experts assess relevance and correctness (recall/precision); generated SQL or scripts are executed to verify hypotheses against event data; self-eval used to filter low-confidence hypotheses and ensemble for consensus.",
            "evaluation_metric": "recall, precision, verification pass rate, confidence threshold pass rate",
            "metric_definition": "Recall/precision as defined above; verification pass rate = fraction of hypotheses that are empirically supported by executing verification queries (0–100%); confidence pass rate = fraction above chosen confidence threshold.",
            "dataset_or_benchmark": "No PM-specific hypothesis benchmark exists per paper; related evaluation approaches referenced in literature on scientific hypothesis generation.",
            "human_evaluation_details": "Paper recommends human evaluation but provides no numeric details (number of experts, scales, or agreement measures not reported).",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Lack of standardized PM-specific benchmarks for hypothesis generation; human evaluation required and costly.",
            "uuid": "e7763.13",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.",
            "rating": 2,
            "sanitized_title": "agieval_a_humancentric_benchmark_for_evaluating_foundation_models"
        },
        {
            "paper_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models.",
            "rating": 2,
            "sanitized_title": "bamboo_a_comprehensive_benchmark_for_evaluating_long_text_modeling_capacities_of_large_language_models"
        },
        {
            "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation.",
            "rating": 2,
            "sanitized_title": "xiezhi_an_everupdating_benchmark_for_holistic_domain_knowledge_evaluation"
        },
        {
            "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models.",
            "rating": 2,
            "sanitized_title": "arb_advanced_reasoning_benchmark_for_large_language_models"
        },
        {
            "paper_title": "MMBench: Is Your Multi-modal Model an Allaround Player?.",
            "rating": 2,
            "sanitized_title": "mmbench_is_your_multimodal_model_an_allaround_player"
        },
        {
            "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models.",
            "rating": 2,
            "sanitized_title": "evaluating_the_texttosql_capabilities_of_large_language_models"
        },
        {
            "paper_title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
            "rating": 2,
            "sanitized_title": "decodingtrust_a_comprehensive_assessment_of_trustworthiness_in_gpt_models"
        },
        {
            "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models.",
            "rating": 2,
            "sanitized_title": "selfevaluation_improves_selective_generation_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery.",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        }
    ],
    "cost": 0.01537475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies
5 Apr 2024</p>
<p>Alessandro Berti alessandro.berti@fit.fraunhofer.de 0000-0002-3279-4795
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Humam Kourani humam.kourani@fit.fraunhofer.de 0000-0003-2375-2152
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Hannes Häfke hannes.haefke@fit.fraunhofer.de 0000-0002-2845-3998
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Chiao-Yun Li chiao-yun.li@fit.fraunhofer.de 0009-0002-3767-7915
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Daniel Schuster daniel.schuster@fit.fraunhofer.de 0000-0002-6512-9580
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies
5 Apr 20242EF1C7951936BFA3FE2194CB50117E45arXiv:2403.06749v3[cs.DB]Large Language Models (LLMs)Output EvaluationBenchmarking Strategies
Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results.However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks.This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks?The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</p>
<p>Introduction</p>
<p>Process mining (PM) is a data science field focusing on deriving insights about business process executions from event data recorded by information systems [1].Several types of PM exist, including process discovery (learning process models from event data), conformance checking (comparing event data with process models), and process enhancement (adding frequency/performance metrics to process models).Although many automated methods exist for PM, human analysts usually handle process analysis due to the need for domain knowledge.Recently, LLMs have emerged as conversational interfaces trained on extensive data [28], achieving near-human performance in various general tasks [38].Their potential in PM lies in embedded domain knowledge useful for generating database queries and insights [21], logical and temporal reasoning capabilities [2,16], inference abilities over structured data [12].Prior research has asserted the usage of LLMs for PM tasks [3,4].However, a comprehensive discussion on Fig. 1: Outline of the contributions of this paper.necessary capabilities for PM, LLMs' suitability evaluation for process analytics, and assessment of LLMs' outputs in the PM context is lacking.</p>
<p>The three main contributions of this paper are summarized in Fig. 1.First, building upon prior work [3,4] proposing textual abstractions of process mining artifacts and an experimental evaluation of LLMs' responses, the essential capabilities that LLMs must have for PM tasks are derived in Section 3.1.The aforementioned capabilities allow us to narrow down the field of LLMs to those that meet these requirements.Next, evaluation benchmarks for selecting suitable LLMs are introduced in Section 3.2, incorporating both process-miningspecific and general criteria such as reasoning, visual understanding, factuality, and trustworthiness.Finally, we suggest automatic, human, and self-assessment methods for evaluating LLMs' outputs on specific tasks in Section 3.3, aiming to establish a comprehensive PM benchmark and enhance confidence in LLMs' usage, addressing potential issues like hallucination.</p>
<p>This paper provides an orientation to process mining researchers investigating the usage of LLMs, i.e., this paper aims to facilitate PM-on-LLMs research.</p>
<p>Background</p>
<p>LLMs enhance PM with superior capabilities, handling complex tasks through data understanding and natural language processing.This section covers PM tasks with LLMs (Section 2.1) and the adopted implementation paradigms (Section 2.2) along with the provision of additional domain knowledge.</p>
<p>Process Mining Tasks for LLMs</p>
<p>This subsection explores a range of PM tasks in which LLMs have already been adopted for process mining research.LLMs facilitate the automation of generating textual descriptions from process data, handling inputs such as event logs or formal process models [4].They also generate process models from textual descriptions, with studies showing LLMs creating BPMN models and declarative constraints from text [7].In the realm of anomaly detection, LLMs play a crucial role in identifying process data anomalies, including unusual activities and performance bottlenecks, offering context-aware detection that adapts to new patterns through prompt engineering.This improves versatility over traditional methods [3,4].For root cause analysis, LLMs analyze event logs to suggest causes of anomalies or inefficiencies, linking delays to specific conditions or events.This goes beyond predefined logic, employing language processing for context-aware analysis [3,4] In ensuring fairness, LLMs identify and mitigate bias in processes, suggesting adjustments.They analyze processes like recruitment to detect disparities in rejection rates or delays by gender or nationality, aiding in fair decision-making [22,4].LLMs can also interpret and explain visual data, including complex visualizations, by describing event flows in dotted charts and identifying specific patterns, such as batch processing.For process improvement, after PM tasks identify and analyze problems, LLMs can suggest actions and propose new process constraints [22,4].</p>
<p>Implementation Paradigms of Process Mining on LLMs</p>
<p>To effectively employ LLMs for PM tasks, specific implementation paradigms are required [3,4].This section outlines key approaches for implementing LLMs in PM tasks.We distinguish three main strategies:</p>
<p>-Direct provision of insights: A prompt is generated that merges data abstractions with a query about the task.Also, interactive dialogue between the LLM and the user is possible for step-by-step analysis.The user starts with a query and refines or adjusts it based on the LLM's feedback, continuing until achieving the desired detail or accuracy, such as pinpointing process inefficiencies.For instance, to have LLMs identifying unusual behavior in an event log, we combine a textual abstraction of the log (such as the directly-follows graph or list of process variants) with a question like "Can you analyze the log to detect any unusual behavior?"-Code generation: LLMs can be used to create structured queries, like SQL, for advanced PM tasks [11].Rather than directly asking LLMs for answers, users command LLMs to craft database queries from natural language.These queries are then executed on the databases holding PM information.It is applicable to PM tasks that can be converted into database queries, such as filtering event logs or computing the average duration of process steps.Also, LLMs can be used to generate executable programs that use existing PM libraries to infer insights over the event data [9].-Automated hypotheses generation: Combining the previous strategies by using textual data abstraction to prompt LLMs for autonomous hypotheses generation [3,4].The hypotheses are accompanied by SQL queries for verification against event data.Results confirm or refute these hypotheses, with potential for LLM-suggested refinements of hypotheses.</p>
<p>LLMs may require additional knowledge about processes and databases to implement PM tasks, for example, in anomaly detection and crafting accurate database queries.Some strategies are used to equip LLMs with this additional domain knowledge [14], including fine-tuning and prompt engineering.</p>
<p>Evaluating LLMs in Process Mining</p>
<p>This section introduces criteria for selecting LLMs that are suitable for PM tasks.Moreover, we introduce criteria for evaluating their outputs.First, in Section 3.1, we discuss the fundamental capabilities needed for PM (long context window, acceptance of visual prompts, coding, factuality).Then, we introduce in Section 3.2 general-purpose and process-mining-specific benchmarks to measure the different LLMs on process-mining-related tasks.To foster the development of process-mining-specific benchmarks and to be able to evaluate a given output, we propose in Section 3.3 different methods to evaluate the output of an LLM.</p>
<p>LLMs Capabilities Needed for Process Mining</p>
<p>In this section, we discuss four important capabilities of LLMs for PM tasks:</p>
<p>-Long Context Window : Event logs in PM often include a vast amount of cases and events, challenging the context window limit of LLMs, which restricts the token count in a prompt [13].Moreover, also the textual specification of process models requires a significant amount of information.The context window limit can be severe in many currently popular LLMs. 3 Even simple abstractions like the ones introduced in [3] (directly-follows graph, list of process variants) may exceed this limitation.The context window, which is set during model training, must be large enough for the data size.Recent efforts aim to extend this limit, though quality may decline [13,20].-Accepting Visual Prompts: Visualizations in PM, such as the dotted chart and the performance spectrum [15], summarizing process behavior, empower analysts to spot interesting patterns not seen in tables.Interpreting visual prompts is key for semi-automated PM.Large Visual Models (LVMs) use architectures similar to language models trained on annotated image datasets [31].They perform tasks like object detection and image synthesis, recognizing patterns, textures, shapes, colors, and spatial relations. 4Coding (Text-to-SQL) Capabilities: With the context window limit preventing full event log inclusion in prompts, generating scripts and database queries is crucial for analyzing event data.As discussed in Section 2.2, text-to-SQL assists in filtering and analyzing event data.Key requirements for text-to-SQL in PM include understanding database schemas, performing complex joins, using database-specific operators (e.g., for calculating date differences), and translating PM concepts into queries.Overall, modern LLMs offer excellent coding capabilities [3].</p>
<p>-Factuality: LLM hallucination involves generating incorrect or fabricated information [24].Factuality measures an LLM's ability to cross-check its outputs against real facts or data, crucial for PM tasks like anomaly detection and root cause analysis.This may involve leveraging external databases [19], knowledge bases, or internet search [32] for validation.For instance, verifying the sequence Cancel Order" followed by Deliver Order" against public data in anomaly detection.LLMs with web browsing can access up-to-date information, enhancing factuality. 5</p>
<p>Relevant LLMs Benchmarks</p>
<p>After identifying the required capabilities for LLMs in PM, benchmarking strategies are essential to measure the quality of the textual outputs returned by the LLMs satisfying such capabilities.</p>
<p>Considering the wide array of available benchmarks for assessing LLMs behavior, we focus on identifying those most relevant to PM capabilities.In [5], a comprehensive collection of benchmarks is introduced.This section aims to select and utilize some of these benchmarks to evaluate various aspects of LLMs' performance in PM contexts.</p>
<p>-Traditional benchmarks: Textual prompts are crucial for LLMs evaluation in PM.Benchmarks like AGIEval assess models via standardized exams [37], and MT-Bench focuses on conversational and instructional capabilities [36].</p>
<p>Another benchmark evaluates LLMs on prompts of long size [6].-Domain knowledge benchmarks: Domain knowledge is essential for LLMs in PM to identify anomalies using metrics and context.Benchmarks like XIEZHI assess knowledge across different fields (economics, science, engineering) [8], while ARB evaluates expertise in areas like mathematics and natural sciences [26].-Visual benchmarks: Understanding PM visualizations, such as dotted charts, is essential (c.f.Section 3.1).LLMs must accurately process queries on these visualizations.MMBench tests models on image tasks [17], and MM-Vet assesses recognition, OCR, among others [35].Yet, they may not fully meet PM visualization analysis needs, particularly in evaluating line orientations and point size/color.-Benchmarks for Text-to-SQL: In PM, generating SQL from natural language is key for tasks like event log filtering.Benchmarks such as SPIDER and SPIDER-realistic test LLMs on text-to-SQL conversion [23].The APPS benchmark evaluates broader code generation abilities [10].-Fairness benchmarks: they evaluate LLM fairness in PM by analyzing group treatment and bias detection.DecodingTrust measures LLM trustworthiness, covering toxicity, bias, robustness, privacy, ethics, and fairness [30].-Benchmarking the generation of hypotheses: LLMs' ability to generate hypotheses from event data is vital to implement semi-autonomous PM agents.
X X Process Modeling X X X X X X X Anomaly Detection X X X X X X Root Cause Analysis X X X X X X Ensuring Fairness X X X X X X X Expl. and Interpreting X X X Visualizations Process Improvement X X X X X X X X
While specific benchmarks for hypothesis generation are lacking, related studies like [29] and [34] evaluate LLMs using scientific papers.</p>
<p>In Table 1, we link process mining (PM) tasks to implementation paradigms and benchmarks.We discuss these tasks:</p>
<p>-Process description requires understanding technical terms relevant to the domain, crucial for accurately describing processes.-Process modeling involves generating models from text, using SQL for declarative and BPMN XML for procedural models.LLMs should offer various model hypotheses.-Anomaly detection and root cause analysis need domain knowledge to analyze process sequences or identify event attribute combinations causing issues.-Fairness involves detecting biases by analyzing event attributes and values, necessitating hypothesis generation by LLMs.-Explaining and interpreting visualizations requires extracting features from images and texts, offering contextual insights, like interpreting performance spectrum visualization [15].-Process improvement entails suggesting text proposals or new constraints to enhance current models, leveraging code generation capabilities and understanding process limitations.</p>
<p>While general-purpose benchmarks are already developed and are easily accessible, they are not entirely suited for the task of PM-on-LLMs.In particular, visual capabilities (explaining and interpreting PM visualizations) and autonomous hypotheses generation require more PM-specific benchmarks.However, little research exists on PM-specific benchmarks [3,4].</p>
<p>How to Evaluate LLMs Outputs</p>
<p>This section outlines criteria for assessing the quality of outputs generated by LLMs in PM tasks, serving two primary objectives.The first objective is to as-sist users in identifying and addressing hallucinations and inaccuracies in LLMs' outputs.The second aim is to establish criteria for developing an extensive benchmark specifically tailored to PM applications of LLMs.The strategies follow:</p>
<p>-Automatic evaluation is particularly suited for text-to-SQL tasks.In this context, the formal accuracy and conciseness (indicated by the length of the produced query) of the SQL queries generated can be efficiently assessed.</p>
<p>Additionally, the creation of declarative constraints, designed to enhance process execution, can also be evaluated in terms of their formal correctness.-Human evaluation is essential for LLM tasks like direct querying and hypothesis generation.For direct querying tasks such as anomaly detection and root cause analysis, important criteria are recall (the model's ability to identify expected insights) and precision (the correctness of insights).These criteria also apply to hypothesis generation.Additionally, evaluating the feedback cycle's effectiveness in validating original hypotheses is crucial for these tasks.-Self-evaluation in LLMs tackles hallucinations, as noted by [24].Techniques include chain-of-thought, where LLMs detail their reasoning, enhancing explanations [33].Confidence scores let LLMs assess their insights' reliability, discarding uncertain outputs for quality [27].Ensembling, or using results from multiple LLM sessions, increases accuracy via majority voting or confidence checks [18].Self-reflection, an LLM reviewing its or another's output, detects errors [25].In anomaly detection, using confidence scores to exclude doubtful anomalies and ensembling to confirm detections across sessions improves reliability.</p>
<p>Conclusion</p>
<p>This paper examines LLM applications in PM, offering three main contributions: identification of necessary LLM capabilities for PM, review of benchmarks from literature, and strategies for evaluating LLM outputs in PM tasks.These strategies aim to build confidence in LLM use and establish benchmarks to assess LLM effectiveness across PM implementations.</p>
<p>Our discussion centers on current generative AI capabilities within PM, anticipating advancements like deriving event logs from videos.Despite future enhancements, the criteria discussed here should remain pertinent.Benchmarking for PM tasks on large language models (LLMs) will evolve, including both general and PM-specific benchmarks, yet the foundational aspects and methodologies are expected to stay consistent.</p>
<p>Table 1 :
1
Implementation paradigms and benchmarks for LLMs in the context of different PM tasks.
TaskParadigmsBenchmarks ClassesDirect ProvisionCode GenerationHypotheses GenerationTraditionalDomain KnowledgeVisual PromptsText-to-SQLFairnessHypotheses GenerationProcess DescriptionX
https://community.openai.com/t/are-the-full-8k-gpt-4-tokens-available-on-chatgpt/237999
 and Google Bard/Gemini are popular models supporting both visual and textual prompts.
https://cointelegraph.com/news/chat-gpt-ai-openai-browse-internet-no-longer-limited-info-2021</p>
<p>W M P Van Der Aalst, Process Mining -Data Science in Action. ess Mining -Data Science in ActionSpringer2016Second Edition</p>
<p>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. Y Bang, S Cahyawijaya, 10.48550/arXiv.2302.040232023</p>
<p>Leveraging Large Language Models (LLMs) for Process Mining. A Berti, M S Qafari, 10.48550/arXiv.2307.127012023Technical Report</p>
<p>A Berti, D Schuster, W M P Van Der Aalst, 10.48550/arXiv.2307.02194Abstractions, Scenarios, and Prompt Definitions for Process Mining with LLMs: A Case Study. 2023</p>
<p>A Survey on Evaluation of Large Language Models. Y Chang, X Wang, 10.48550/arXiv.2307.031092023</p>
<p>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. Z Dong, T Tang, 10.48550/arXiv.2309.133452023</p>
<p>Large Language Models Can Accomplish Business Process Management Tasks. M Grohs, L Abb, BPM 2023 International Workshops. Lecture Notes in Business Information Processing. Springer2023492</p>
<p>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. Z Gu, X Zhu, 10.48550/arXiv.2306.057832023</p>
<p>Conceptual model interpreter for Large Language Models. F Härer, ER 2023. CEUR Workshop Proceedings. 20233618</p>
<p>Measuring Coding Challenge Competence With APPS. D Hendrycks, S Basart, NeurIPS Datasets and Benchmarks. 20212021</p>
<p>Chit-Chat or Deep Talk: Prompt Engineering for Process Mining. U Jessen, M Sroka, D Fahland, 10.48550/arXiv.2307.099092023</p>
<p>StructGPT: A General Framework for Large Language Model to Reason over Structured Data. J Jiang, K Zhou, EMNLP 2023. Association for Computational Linguistics2023</p>
<p>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. H Jin, X Han, 10.48550/arXiv.2401.013252024</p>
<p>T Kampik, C Warmuth, 10.48550/arXiv.2309.00900Large Process Models: Business Process Management in the Age of Generative AI. 2023</p>
<p>Performance Mining for Batch Processing Using the Performance Spectrum. E L Klijn, D Fahland, BPM 2019 International Workshops. Lecture Notes in Business Information Processing. Springer2019362</p>
<p>H Liu, R Ning, 10.48550/arXiv.2304.03439Evaluating the Logical Reasoning Ability of Chat-GPT and GPT-4. 2023</p>
<p>MMBench: Is Your Multi-modal Model an Allaround Player?. Y Liu, H Duan, 10.48550/arXiv.2307.062812023</p>
<p>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. K Lu, H Yuan, 10.48550/arXiv.2311.086922023</p>
<p>Unifying Large Language Models and Knowledge Graphs: A Roadmap. S Pan, L Luo, 10.48550/arXiv.2306.083022023</p>
<p>YaRN: Efficient Context Window Extension of Large Language Models. B Peng, J Quesnelle, 10.48550/arXiv.2309.000712023</p>
<p>F Petroni, T Rocktäschel, Language Models as Knowledge Bases? In: EMNLP-IJCNLP 2019. Association for Computational Linguistics2019</p>
<p>M S Qafari, W M P Van Der Aalst, Fairness-Aware Process Mining. Lecture Notes in Computer Science. Springer2019. 201911877</p>
<p>Evaluating the Text-to-SQL Capabilities of Large Language Models. N Rajkumar, R Li, D Bahdanau, 10.48550/arXiv.2204.004982022</p>
<p>The Troubling Emergence of Hallucination in Large Language Models -An Extensive Definition, Quantification, and Prescriptive Remediations. V Rawte, S Chakraborty, EMNLP 2023. Association for Computational Linguistics2023</p>
<p>Self-Evaluation Improves Selective Generation in Large Language Models. J Ren, Y Zhao, 2023</p>
<p>ARB: Advanced Reasoning Benchmark for Large Language Models. T Sawada, D Paleka, 10.48550/arXiv.2307.136922023</p>
<p>The Confidence-Competence Gap in Large Language Models: A Cognitive Study. A K Singh, S Devkota, 10.48550/arXiv.2309.161452023</p>
<p>Welcome to the Era of ChatGPT et al. T Teubner, C M Flath, Bus. Inf. Syst. Eng. 6522023</p>
<p>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph. S Tong, K Mao, Z Huang, Y Zhao, K Peng, 2023</p>
<p>B Wang, W Chen, 10.48550/arXiv.2306.11698DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. 2023</p>
<p>Review of Large Vision Models and Visual Prompt Engineering. J Wang, Z Liu, 10.48550/arXiv.2307.008552023</p>
<p>L Wang, C Ma, 10.48550/arXiv.2308.11432A Survey on Large Language Model based Autonomous Agents. 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, NeurIPS. 20222022</p>
<p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. Z Yang, X Du, 10.48550/arXiv.2309.027262023</p>
<p>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. W Yu, Z Yang, 10.48550/arXiv.2308.024902023</p>
<p>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. L Zheng, W Chiang, 10.48550/arXiv.2306.056852023</p>
<p>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. W Zhong, R Cui, 10.48550/arXiv.2304.063642023</p>
<p>Large Language Models are Human-Level Prompt Engineers. Y Zhou, A I Muresanu, ICLR 2023. OpenReview.net2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2530 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2530</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2530</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-0717e64a010a8cc45da64127b5bce10afc9d6d01</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0717e64a010a8cc45da64127b5bce10afc9d6d01" target="_blank">A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind.</p>
                <p><strong>Paper Abstract:</strong> The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the learning target moves. Disparate streams of research have approached non-stationarity from several angles, which make a variety of implicit assumptions that make it hard to keep an overview of the state of the art and to validate the innovation and significance of new works. This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits. Further, we reflect on the principle approaches how algorithms model and cope with this non-stationarity, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind. A wide range of state-of-the-art algorithms is classified into a taxonomy, using these categories and key characteristics of the environment (e.g., observability) and adaptation behaviour of the opponents (e.g., smooth, abrupt). To clarify even further we present illustrative variations of one domain, contrasting the strengths and limitations of each category. Finally, we discuss in which environments the different approaches yield most merit, and point to promising avenues of future research.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2530",
    "paper_id": "paper-0717e64a010a8cc45da64127b5bce10afc9d6d01",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004781,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity</h1>
<p>Pablo Hernandez-Leal<br>Michael Kaisers<br>Tim Baarslag<br>Intelligent and Autonomous Systems Group<br>Centrum Wiskunde \&amp; Informatica<br>Amsterdam, The Netherlands<br>Enrique Munoz de Cote<br>JEMC@INAOEP.MX<br>Instituto Nacional de Astrofísica, Óptica y Electrónica, Puebla, México<br>PROWLER.io Ltd., Cambridge, United Kingdom</p>
<p>Editor:</p>
<h4>Abstract</h4>
<p>The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the learning target moves. Disparate streams of research have approached nonstationarity from several angles, which make a variety of implicit assumptions that make it hard to keep an overview of the state of the art and to validate the innovation and significance of new works. This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits. Further, we reflect on the principle approaches how algorithms model and cope with this non-stationarity, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind. A wide range of state-of-the-art algorithms is classified into a taxonomy, using these categories and key characteristics of the environment (e.g., observability) and adaptation behaviour of the opponents (e.g., smooth, abrupt). To clarify even further we present illustrative variations of one domain, contrasting the strengths and limitations of each category. Finally, we discuss in which environments the different approaches yield most merit, and point to promising avenues of future research.</p>
<p>Keywords: Multiagent learning, reinforcement learning, multi-armed bandits, game theory</p>
<h2>1. Introduction</h2>
<p>There are many successful applications of multiagent systems (MAS) in the real world. Examples are ubiquitous in energy applications, for example, to implement a network to distribute electricity (Pipattanasomporn et al., 2009) or to coordinate the charging of electric vehicles (Valogianni et al., 2015), in security, to patrol the Los Angeles airport (Pita et al., 2009) and in disaster management to assign a set of resources to tasks (Ramchurn et al., 2010). Multiagent systems include a set of autonomous entities (agents) that share</p>
<p>a common environment and where each agent can independently perceive the environment, act according to its individual objectives and as a consequence, modify the environment.</p>
<p>How the environment changes as a consequence of an agent exerting an action is known as the environment dynamics. In order to act optimally with respect to its objectives these dynamics need to either be known (a priori) by the agent or otherwise be learned by experience, i.e., by interacting many times with the environment. Once the environment dynamics have been learned, the agent can then adapt its behaviour and act according to its target objective. We know a lot about the single agent case, where only one agent is learning and adapting its behaviour, but most of the results break apart when two or more agents share an environment and they all learn and adapt their behaviour concurrently.</p>
<p>The problem with this concurrency is that the action executed by one agent affects the goals and objectives of the rest, and vice-versa. To tackle this, each agent will need to account for how the other agents are behaving and adapt according to the joint behaviour. Needless to say, this joint behaviour needs to be learned by each agent, and due to the fact that all agents are performing the same operations of learning and adapting concurrently, the joint behaviour - and therefore the environment - is perceived by each agent as nonstationary. This non-stationarity (sometimes referred to as the moving target problem, see Tuyls and Weiss, 2012) sets multiagent learning apart from single-agent learning, for which it suffices to converge to a fixed optimal strategy.</p>
<p>Most learning algorithms to date are not well suited to deal with non-stationary environments, ${ }^{1}$ and usually, such non-stationarity is caused by changes in the behaviour of the participating agents. For example, a charging vehicle in the smart grid might change its behavioural pattern (Marinescu et al., 2015); robot soccer teams may change between pre-defined behaviours depending on the situation (MacAlpine et al., 2012); and attackers change their behaviours to keep security guards guessing in domains involving frequent adversary interactions, such as wildlife and fishery protection (Fang et al., 2015).</p>
<p>Previous works in reinforcement learning (RL), MAS and multi-armed bandits (to name a few) have all acknowledged the fact that specialized targeted work is needed that explicitly addresses non-stationary environments (Sutton et al., 2007; Panait and Luke, 2005; Garivier and Moulines, 2011; Matignon et al., 2012; Lakkaraju et al., 2017). Against this background, this survey fills this gap with an extensive analysis of the state of the art. Previous surveys have proposed different ways to categorise MAS algorithms (Panait and Luke, 2005; Shoham et al., 2007; Busoniu et al., 2010), others have divided them by the type of learning (Tuyls and Weiss, 2012; Bloembergen et al., 2015) and another group have proposed properties that MAS algorithms should have (Bowling and Veloso, 2002; Powers et al., 2007; Crandall and Goodrich, 2011). In contrast, we propose another view, which has been mostly neglected, focused on how algorithms deal with non-stationarity, providing an illustrative categorization with increasing order of sophistication where each algorithm is analysed along with related characteristics (observability and opponent adaptation).</p>
<p>The questions addressed by the surveyed algorithms are illustrated by the following simple scenario comprising two agents:</p>
<p>Predator. The agent under our control.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>A Survey of Learning in Multiagent Environments</h1>
<p>Prey. The opponent agent. ${ }^{2}$
Both agents engage in repeated rounds of interactions (possibly infinite), there is no communication and the rewards received depend on the joint action. The prey has several (possibly infinite) strategies at its disposal (ways to select its actions) and it can change from one to another during the interaction. In this context we can raise several questions:</p>
<ul>
<li>Should the predator assume the prey will behave in a certain way (e.g., minimizing the predator's reward, enacting their part of the Nash Equilibrium or playing as a teammate)?</li>
<li>Should the predator learn to optimise against a single opponent strategy or should it generalise by learning a more robust strategy against a class of possible strategies?</li>
<li>Should the predator assume the prey is modelling the predator's strategy?</li>
<li>Should the predator assume the prey will use a stationary strategy? If not, will the prey change its behaviour slowly or drastically?</li>
</ul>
<p>Different research communities make different assumptions that give rise to distinct answers to these questions. While there is some awareness within each community of the work outside that community, it remains a challenge to keep up to date with the recent literature due to this fragmentation, which impedes AI research in its entirety (Eaton et al., 2016).</p>
<p>For example, many game theory algorithms focus on finding equilibria in self-play. Multiarmed bandits either assume a stochastic or adversarial setting and try to optimize against that behaviour. Some basic approaches of reinforcement learning ignore other agents and optimise a policy assuming a stationary environment, essentially treating non-stationary aspects like stochastic fluctuations. Other approaches learn a model of the other agents to predict their actions to remove the non-stationary behaviour. Finally, algorithms from behavioural game theory and planning have proposed recursive modelling approaches that assume opponents are capable of performing strategic reasoning and modelling of the rest of the agents.</p>
<p>In this context, the main contributions of this survey are the following:</p>
<ul>
<li>Provide a coherent view of how state-of-the-art algorithms in reinforcement learning, multi-armed bandits and game theory tackle the planning problem of long-term sum of expected rewards in non-stationary environments.</li>
<li>Propose a new framework for multiagent systems (see Section 3.2). This framework allows to describe a categorisation with increasing order of sophistication with respect to how non-stationarity is handled, arriving at five categories: ignore, forget, respond to target opponents, learn opponent models and theory of mind (see Section 3.3).</li>
<li>Describe the fundamental algorithms of each category using an illustrative example highlighting their strengths and limitations (see Section 4).</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>Categorise most significant learning algorithms while also describing their main characteristics with respect to the environment and opponent assumptions (see Section 5).</li>
<li>Provide a structured set of open questions with promising avenues of future research in multiagent learning (see Section 6.5).</li>
</ul>
<p>With its tailored scope, this survey aims to establish a structure to think clearly about all the assumptions, characteristics and concepts related to the challenge of addressing non-stationarity in multiagent learning.</p>
<h1>1.1 Related work and demarcation</h1>
<p>Multiagent learning has received a lot of attention in the past years and some previous surveys have emerged with different motivations and outcomes. Shoham et al. (2007) presented a general survey of multiagent learning providing some interesting foundational questions and identifying five different agendas in this research community. Tuyls and Weiss (2012) presented a bird's eye view about the AI problem of multiagent learning, identifying the milestones achieved by the community and mentioning the open challenges at the time. ${ }^{3}$ Panait and Luke (2005) presented an extensive analysis of cooperative multiagent learning algorithms, dividing them into two categories: single learner in a multiagent problem (team learning) and multiple learners (concurrent learning). Matignon et al. (2012) focused on the evaluation of independent RL algorithms on cooperative stochastic games. Busoniu et al. (2010) presented a thorough survey on multiagent RL where they identified a taxonomy and several properties for algorithms in multiagent reinforcement learning (MARL). Crandall and Goodrich (2011) assessed the state of the art in two-player repeated games with respect to three properties: security, cooperation and compromise, which they propose as important to act in a variety of different games. Müller and Fischer (2014) presented an application-oriented survey, highlighting applications that use or are based on MAS. Weiss (2013) edited a book about multiagent systems; in particular there is a chapter dedicated to multiagent learning where they present state-of-the-art algorithms dividing them into joint action, gradient, Nash and other learners (see Weiss, 2013, chap. 10). A recent survey analysed methods from evolutionary game theory and its relation with multiagent learning (Bloembergen et al., 2015). Finally, the recent area of multiagent deep reinforcement learning gained a lot of interest with two recent surveys (Nguyen et al., 2018; Hernandez-Leal et al., 2018). None of these survey articles provide an explicit treatment of the non-stationarity approaches taken in various algorithms.</p>
<p>Our survey exceeds previous work in scope of different domains and coverage measured by number of algorithms, and fills the gap of reflecting on non-stationarity. In contrast to previous works, we provide a detailed analysis of algorithms from multi-armed bandits (for stochastic and adversarial environments), single agent RL (model-based and model-free approaches), multiagent RL and game theory (mainly for repeated and stochastic games) in both competitive and cooperative scenarios. We provide a full taxonomy of how algorithms cope with non-stationarity, and describe opponent and environment characteristics.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This survey does not cover work related to learning in dynamic environments that do not have other active autonomous and automated agents, such as recommender systems of news articles (Liu et al., 2010) or online supervised learning scenarios (see Section 6.4).</p>
<h1>1.2 How to read this survey</h1>
<p>Since different audiences are expected to read this survey, this section provides forward references to key insights and sections for different target groups:</p>
<ul>
<li>For researchers seeking an introduction to multiagent learning we propose to follow the current structure of the paper sequentially, progressing through each section in order.</li>
<li>For experienced researchers we recommend starting with the new framework proposed in Section 3.2, followed by the high level vision of the categorisation of algorithms depicted in Figure 4 in Section 4; to be followed by the extensive categorization in Section 5; in particular given in Table 2 and Figure 5.</li>
<li>We encourage researchers seeking guidance on promising research directions to consult the discussion in Section 6, in particular to find common types of results in Section 6.3 and interesting open problems in Section 6.5.</li>
</ul>
<p>Finally, we encourage all readers to position their future work in this framework, as delineated in Section 3, for ease of reference and navigation of related (future) work.</p>
<h3>1.3 Paper overview</h3>
<p>This paper aims to provide a general overview of how different algorithms cope with the problem of learning in multiagent systems where it is necessary to deal with non-stationary behaviour. In Section 2, we review formal models used in this context ; in particular we review multi-armed bandits, reinforcement learning and game theory. Section 3 describes the main challenge of non-stationarity in multiagent systems together with a new framework that naturally models its key elements, and lastly presents the proposed categorization of how algorithms deal with non-stationarity. Section 4 illustrate the categories using a simple scenario. Section 5 presents an extensive list of works of multi-armed bandits, RL and game theory categorised by the taxonomy proposed in this survey. Section 6 provides a discussion about the strengths and limitations of each category, describes the common experimental settings, presents a summary of the theoretical results and pinpoints interesting open problems highlighting promising lines of future research. Finally, Section 7 summarizes the conclusions and contributions of this survey.</p>
<h2>2. Formal approaches from different domains that model non-stationarity</h2>
<p>This section describes the formal models used in multi-armed bandits, reinforcement learning, and game theory, and contrasts how they capture non-stationarity. Each domain makes different assumptions about a priori information about the interaction, as well as about online observability of the environment and opponents during the interaction. This discrimination forms the basis of the environment characteristics in the next section. In line</p>
<p>with available information, the solution concept for finding good behaviour may be characterized correspondingly by more a priori or more online reasoning, exhibiting characteristic behaviour and the ability to cope with certain types of non-stationarity in opponent behaviour. In order to present our behavioural categorization, we here present a synopsis of the different approaches from the literature.</p>
<p>Note that different areas provide different terminology. Therefore, we will use the terms player and agent interchangeably; similarly for reward and payoff; and for rounds and steps. Finally, we will refer to other agents in the environment as opponents irrespective of the domain's or agent's cooperative or adversarial nature.</p>
<h1>2.1 Multi-armed bandits</h1>
<p>The simplest possible reinforcement-learning problem is known as the multi-armed bandit problem (Robbins, 1985): the agent is in a room with multiple gambling machines (called "one-armed bandits"). At each time-step the agent pulls the arm of one of the machines and receives a reward. The agent is permitted a fixed number of pulls. The agent's purpose is to maximise its total reward over a sequence of trials. Usually each arm is assumed to have a different distribution of rewards, therefore, the goal is to find the arm with the best expected return as early as possible, and then to keep gambling using that arm.</p>
<p>A $K$-armed (stochastic) bandit can be formalised as a set of real distributions $\mathcal{B}=$ $\left{R_{1}, R_{2}, \ldots, R_{k}\right}$, with the set of arms $I={1, \ldots, K}$, such that each arm yields a stochastic reward $r_{i}$ following the distribution $R_{i}$. Let $\mu_{1}, \mu_{2}, \ldots, \mu_{k}$ be the mean values associated with these reward distributions. A policy, or allocation strategy, is an algorithm that chooses the next machine to play based on the sequence of past plays and obtained rewards. The policy selects one arm at each round and observes the reward, this process is repeated for $T$ rounds. This problem illustrates the fundamental trade-off between exploration and exploitation: should the agent choose the arm with the highest average reward observed so far (exploit), or should it choose another one for which it has less information, so as to find out if it in fact exceeds the first one (explore)?</p>
<p>The regret $\Delta_{R}$ is a common measure used to evaluate different algorithms in multiarmed bandits. The regret is the difference (necessarily a loss) between the chosen policy $\pi$ and the optimal policy $\pi^{<em>}$. In the multi-armed bandit setting the optimal policy would choose the arm $i^{</em>}$ with the highest expected reward at all times, i.e., $\forall t: \pi_{t}^{<em>}=i^{</em>}$, while $\pi_{t}=i(t)$ may vary over time. For an episode of $T$ steps, the stochastic regret yields</p>
<p>$$
\Delta_{R}=\sum_{t=1}^{T} r_{i^{*}}-\sum_{t=1}^{T} r_{i(t)}
$$</p>
<p>With this concept in mind, some approaches guarantee low regret under certain conditions. These policies work by associating a quantity called upper confidence index to each arm. This index relies on the sequence of rewards obtained so far from a given arm and is used by the policy as an estimate for the corresponding reward expectation (Auer et al., 2002a). The UCB1 (Upper Confidence Bounds) algorithm achieves logarithmic regret assuming bounded rewards, without further constraints on the reward distributions (Auer et al., 2002a). UCB uses the principle of optimism in the face of uncertainty to select its actions, i.e., the algorithm selects arms by an optimistic estimate on the expected rewards of certain</p>
<p>arms to balance exploration and exploitation. The UCB1 algorithm is extremely simple, it initially plays each arm once, and subsequently selects the arm $i(t)$ :</p>
<p>$$
i(t)=\underset{j}{\arg \max }\left(\bar{r}<em j="j">{j}+\sqrt{\frac{2 \ln t}{n</em>\right)
$$}}</p>
<p>where $\bar{r}<em j="j">{j}$ is the average reward obtained from arm $j, n</em>$ is the number of times arm $j$ has been played, and $t$ is the total number of rounds so far.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">Multi</span><span class="o">-</span><span class="nx">armed</span><span class="w"> </span><span class="nx">bandit</span><span class="p">:</span><span class="w"> </span><span class="nx">stochastic</span><span class="w"> </span><span class="p">(</span><span class="nx">s</span><span class="p">)</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">adversarial</span><span class="w"> </span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">Bubeck</span><span class="w"> </span><span class="k">and</span>
<span class="nx">Slivkins</span><span class="p">,</span><span class="w"> </span><span class="mi">2012</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">arms</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">rounds</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">T</span><span class="w"> </span><span class="err">\</span><span class="nx">geq</span><span class="w"> </span><span class="nx">K</span><span class="w"> </span><span class="err">\</span><span class="nx">geq</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="p">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">T</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Algorithm</span><span class="w"> </span><span class="nx">chooses</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">arm</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">K</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Adversary</span><span class="w"> </span><span class="nx">selects</span><span class="w"> </span><span class="nx">rewards</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">K</span><span class="p">,</span><span class="w"> </span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="p">{</span><span class="nx">K</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Stochastic</span><span class="w"> </span><span class="nx">environment</span><span class="w"> </span><span class="nx">produces</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nx">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="nx">R_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">drawn</span><span class="w"> </span><span class="nx">independently</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Receive</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">(</span><span class="nx">t</span><span class="p">),</span><span class="w"> </span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">does</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">other</span><span class="w"> </span><span class="nx">arms</span><span class="p">)</span>
<span class="w">    </span><span class="nx">The</span><span class="w"> </span><span class="nx">goal</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">minimise</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">regret</span><span class="p">,</span><span class="w"> </span><span class="nx">defined</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="p">:</span>
<span class="w">        </span><span class="nx">In</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">adversarial</span><span class="w"> </span><span class="nx">model</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Delta_</span><span class="p">{</span><span class="nx">r</span><span class="p">}=</span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">j</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">K</span><span class="err">\</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">t</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="nx">t</span><span class="p">}</span><span class="o">-</span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">t</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">(</span><span class="nx">t</span><span class="p">),</span><span class="w"> </span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">In</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">stochastic</span><span class="w"> </span><span class="nx">model</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Delta_</span><span class="p">{</span><span class="nx">r</span><span class="p">}=</span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">t</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">j</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">K</span><span class="err">\</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">mu_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="o">-</span><span class="err">\</span><span class="nx">mu_</span><span class="p">{</span><span class="nx">i</span><span class="p">(</span><span class="nx">t</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>The stochastic bandit scenario is useful to model decision-making in stationary but stochastic settings. A direct extension of this setting is the adversarial model, which assumes that rewards of each arm are controlled by an adversary, i.e., the reward distribution associated with each arm at every round is fixed in advance by an adversary before the game starts (Auer et al., 2002b); see Algorithm 1 that juxtaposes both scenarios. However, when relaxing the assumptions made in the problem definition even more by assuming online adaptive adversaries, the standard definition of regret is no longer adequate (due to adaptivity of the adversary, the optimal action might change at different steps, see Arora et al., 2012). Because of that, different variations of the regret measure have been proposed (Arora et al., 2012; Crandall, 2014). It is worth mentioning that there are further extensions to the bandit scenario (Pandey et al., 2007; Beygelzimer et al., 2011; Tran-Thanh et al., 2012), which are beyond the scope of this survey. However, we refer the interested reader to the discussion in related work (Bubeck and Cesa-Bianchi, 2012).</p>
<h1>2.2 Reinforcement Learning</h1>
<p>Reinforcement learning (RL) is one important area of machine learning that formalises the interaction of an agent with its environment (Puterman, 1994). A Markov Decision Process (MDP) can be seen as a model of an agent interacting with the world (see Figure 1), where the agent takes the state $s$ of the world as input and generates an action $a$ as output that affects the world. There is a transition function $T$ that describes how an action affects the environment in a given state. The component $Z$ represents the agent's perception function, which is used to obtain an observation $z$ from the state $s$. In an MDP it is assumed there is no uncertainty in where the agent is. This implies that the agent has full and perfect</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The agent interacts with the environment, performing an action $a$ that affects the state environment according to a function $T$, producing the state $s$. The agent perceives an observation $z$ about the environment (given by a function $Z$ ) and obtains a reward $r$ (given by a function $R$ ).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A Markov decision process (MDP) with four states $S 0, S 1, S 2, S 3$ and two actions $a 1, a 2$. The arrows denote the tuple: action, transition probability and reward.
perception capabilities and knows the true state of the environment (what it perceives is the actual state, $z=s$ ). The component $R$ is the reward function, the rewards give an indication of the quality of which actions the agent needs to choose. However, the reward function is not always simple to define (for example, it may be stochastic or delayed). Formally,</p>
<p>Definition 1 (Markov decision process) An MDP is defined by the tuple $\langle S, A, R, T\rangle$ where $S$ represent the world divided up into a finite set of possible states. A represents a finite set of available actions. The transition function $T: S \times A \rightarrow \Delta(S)$ maps each state-action pair to a probability distribution over the possible successor states, where $\Delta(S)$ denotes the set of all probability distributions over $S$. Thus, for each $s, s^{\prime} \in S$ and $a \in A$, the function $T$ determines the probability of a transition from state $s$ to state $s^{\prime}$ after executing action a. The reward function $R: S \times A \times S \rightarrow \mathbb{R}$ defines the immediate and possibly stochastic reward that an agent would receive for being in state $s$, executing action $a$ and transitioning to state $s^{\prime}$.</p>
<p>An example of an MDP with 4 states and 2 actions is depicted in Figure 2, where ovals represent states of the environment. Each arrow has a triplet $a_{n}, p, r$ representing the action, the transition probability and the reward, respectively.</p>
<p>The key assumption in defining MDPs is that they are stationary, i.e., particularly the transition probabilities and the reward distributions do not change in time. ${ }^{4}$ MDPs are adequate models to obtain optimal decisions in environments with a single agent. Solving an MDP will yield a policy $\pi: S \rightarrow A$, which is a mapping from states to actions. An optimal policy $\pi^{*}$ is the one that maximises the expected reward. There are different techniques for solving MDPs assuming a complete description of all its elements. One of the most common techniques is the value iteration algorithm (Bellman, 1957) which is based on the Bellman equation:</p>
<p>$$
V^{\pi}(s)=\sum_{a \in A} \pi(s, a) \sum_{s^{\prime} \in S} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V^{\pi}\left(s^{\prime}\right)\right]
$$</p>
<p>with $\gamma \in[0,1]$. This equation expresses the value of a state which can be used to obtain the optimal policy $\pi^{<em>}=\arg \max _{\pi} V^{\pi}(s)$, i.e., the one that maximises that value function, and the optimal value function $V^{</em>}(s)$.</p>
<p>$$
V^{*}(s)=\max _{\pi} V^{\pi}(s) \quad \forall s \in S
$$</p>
<p>Finding the optimal policy for an MDP using value iteration requires the MDP to be fully known, including a complete and accurate representation of states, actions, rewards and transitions. However, this may be difficult if not impossible to obtain in many domains. For this reason, RL algorithms have been devised that learn the optimal policy from experience and without having a complete description of the MDP a priori.</p>
<p>An RL agent interacts with the environment in discrete time-steps. At each time, the agent chooses an action from the set of actions available, which is subsequently executed in the environment. The environment moves to a new state and the reward associated with the transition is emitted (see Figure 1). The goal of a RL agent is to maximise the expected reward. In this type of learning the learner is not told which actions to take, but instead must discover which actions yield the best reward by trial and error.
$Q$-learning (Watkins, 1989) is one well known value-based algorithm for RL. It has been devised for stationary, single-agent, fully observable environments with discrete actions. In its general form, a $Q$-learning agent can be in any state $s \in S$ and can choose an action $a \in A$. It keeps a data structure $\hat{Q}(s, a)$ that represents the estimate of its expected payoff starting in state $s$, taking action $a$. Each entry $\hat{Q}(s, a)$ is an estimate of the corresponding optimal $Q^{*}$ function that maps state-action pairs to the discounted sum of future rewards when starting with the given action and following the optimal policy thereafter. Each time the agent makes a transition from a state $s$ to a state $s^{\prime}$ via action $a$ receiving payoff $r$, the $Q$ table is updated as follows:</p>
<p>$$
\hat{Q}(s, a)=\hat{Q}(s, a)+\alpha\left[\left(r+\gamma \max _{b} \hat{Q}\left(s^{\prime}, b\right)\right)-\hat{Q}(s, a)\right]
$$</p>
<p>with the learning rate $\alpha$ and the discount factor $\gamma \in[0,1]$ being parameters of the algorithm, with $\alpha$ typically decreasing over the course of many iterations. $Q$-learning is proved</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>to converge towards $Q^{<em>}$ if each state-action pair is visited infinitely often under specific parameters (Watkins, 1989). $Q$-learning is said to be an off-policy method since it estimates the sum of discounted rewards of the optimal policy (aka. target policy) while actually executing an exploration policy (aka. behavior policy) distinct from it. ${ }^{5}$ In contrast, on-policy methods refer to algorithms that estimation the value of the executed (exploration) policy. Since the exploration policy is commonly non-stationary, primarily due to the decrease of exploration parameters over time, the target value $Q^{</em>}$ to approximate changes with it, making it more intricate to provide convergence results. ${ }^{6}$ One classic on-policy algorithm is SARSA (state $<em t="t">{t}$, action $</em>$, reward $<em t_1="t+1">{t}$, state $</em>$ ) (Sutton and Barto, 1998) which uses a variation of Equation (2):}$, action $_{t+1</p>
<p>$$
\hat{Q}(s, a)=\hat{Q}(s, a)+\alpha\left[\left(r+\gamma \hat{Q}\left(s^{\prime}, a^{\prime}\right)\right)-\hat{Q}(s, a)\right]
$$</p>
<p>By using $Q$-learning it is possible to learn an optimal policy without knowing $T$ or $R$ beforehand, and even without learning these functions (Littman, 1996). For this reason, this type of learning is known as model free RL. In contrast, model-based RL aims to learn a model of its environment, specifically approximating $T$ and $R$. Such models are then used by the agent to predict the consequences of actions before they are taken, facilitating planning ahead of time. One example of this type of algorithms is Dyna-Q (Sutton and Barto, 1998).</p>
<p>Exploration vs exploitation Similar to multi-armed bandits, in RL one main concern is to develop algorithms that balance exploration and exploitation well. However, in contrast to bandits where algorithms are evaluated in terms of regret, the RL community has proposed different measures to determine efficient exploration. An important concept is the sample complexity (Vapnik, 1998), which was first defined in the context of supervised learning. Loosely speaking, sample complexity is the number of examples needed to bring the estimate of a target function within a given error range. Kakade (2003) studied sample complexity in a RL context. Consider an agent interacting in an environment. The steps of the agent can be roughly classified into two categories: steps in which the agent acts near-optimally as "exploitation" and steps in which the agent is not acting near optimally as "exploration". Subsequently, it is possible to see the number of times in which the agent is not acting near-optimally as the sample complexity of exploration (Kakade, 2003) for which some algorithms have guarantees (Brafman and Tennenholtz, 2003).</p>
<p>Before formalizing the problem of learning in multiagent environments (Section 3) we will discuss game theory in the next section, as it is a classical area that addresses the interaction, reasoning and decision-making of multiple agents in strategic conflicts of interest.</p>
<h1>2.3 Game theory</h1>
<p>Game theory studies decision problems when several agents interact (Fudenberg and Tirole, 1991). The terminology in this area is different, agents are usually called players, a single</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: The normal-form representation of the prisoner's dilemma game. Each cell represents the utilities given to the players (left value for $\mathcal{A}$ and right one for $\mathcal{O}$ ), $r_{p d}, t_{p d}, s_{p d}, p_{p d} \in \mathbb{R}$ where the following conditions must hold $t_{p d}&gt;r_{p d}&gt;p_{p d}&gt;s_{p d}$ and $2 r_{p d}&gt;p_{p d}+s_{p d}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Player $\mathcal{O}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">cooperate</td>
<td style="text-align: center;">defect</td>
</tr>
<tr>
<td style="text-align: center;">Player $\mathcal{A}$</td>
<td style="text-align: center;">cooperate</td>
<td style="text-align: center;">$r_{p d}, r_{p d}$</td>
<td style="text-align: center;">$s_{p d}, t_{p d}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">defect</td>
<td style="text-align: center;">$t_{p d}, s_{p d}$</td>
<td style="text-align: center;">$p_{p d}, p_{p d}$</td>
</tr>
</tbody>
</table>
<p>interaction between players is represented as a game, and rewards obtained by the players are called payoffs.</p>
<p>The most common way of presenting a game is by using a matrix that denote the utilities obtained by each agent, this is the normal-form game.</p>
<p>Definition 2 (Normal-form game) $A$ (finite, $\mathcal{I}$-person) normal-form game $\Gamma$, is a tuple $\langle\mathcal{N}, A, u\rangle$, where:
$\mathcal{N}$ is a finite set of $\mathcal{I}$ players, indexed by $i$;
$A=A_{1} \times \cdots \times A_{\mathcal{I}}$, where $A_{i}$ is a finite set of actions available to player $i$. Each vector $a=\left(a_{1}, \ldots, a_{\mathcal{I}}\right) \in A$ is called an action profile;
$u=\left(u_{1}, \ldots, u_{\mathcal{I}}\right)$ where $u_{i}: A \mapsto \mathbb{R}$ is a real-valued utility or payoff function for player $i$.
For example, Table 1 shows a two-action two-player game, known as the Prisoner's Dilemma (PD). Each row corresponds to a possible action for player $\mathcal{A}$ and each column corresponds to a possible action for player $\mathcal{O}$. Player's payoffs are provided in the corresponding cells of the joint action, with player $\mathcal{A}$ 's utility listed first. In the example, each player has two actions {cooperate, defect}. A strategy specifies a method for choosing an action. One kind of strategy is to select a single action and play it, this is a pure strategy. In general, a mixed strategy specifies a probability distribution over actions.</p>
<p>Definition 3 (Mixed strategy) Let $(\mathcal{I}, A, u)$ be a normal-form game, and for any set $X$, let $\Delta(X)$ be the set of all probability distributions over $X$, then the set of mixed strategies for player $i$ is $\mathcal{S}<em i="i">{i}=\Delta\left(A</em>\right)$</p>
<p>In this context, it is important to define what is a good strategy, i.e., the best response.
Definition 4 (Best response) Player $i$ 's best response to the strategy profile $s_{-i}$ is a mixed strategy $s_{i}^{<em>} \in \mathcal{S}<em i="i">{i}$ such that $u</em>^{}\left(s_{i</em>}, s_{-i}\right) \geq u_{i}\left(s_{i}, s_{-i}\right)$ for all strategies $s_{i} \in \mathcal{S}<em -i="-i">{i}$.
where $s</em>$ represents the strategies of all players except $i$. Thus, a best response for an agent is the strategy (or strategies) that produce the most favourable outcome for a player, taking other players' strategies as given. Another common strategy is the minimax strategy that ensures a security level for the player.}=s_{1}, \ldots, s_{i-1}, s_{i+1}, \ldots, s_{n</p>
<p>Definition 5 (Minimax Strategy.) Strategy that maximizes its payoff assuming the opponent will make this value as small as possible.</p>
<p>Definition 6 (Security level) The security level is the expected payoff a player can guarantee itself using a minimax strategy.</p>
<p>In single-agent decision theory, the notion of optimal strategy refers to the one that maximises the agent's expected payoff for a given environment. In multiagent settings the situation is more complex, and the optimal strategy for a given agent may now vary, since the best response strategy depends on the choices of others. In order to draw conclusions on the joint behavior in games, game theory has identified certain subsets of outcomes, called solution concepts, such as the Nash equilibrium (NE). Suppose that all players have a fixed strategy profile in a given game, if no player can increase its utility by unilaterally changing its strategy, then the strategies are in Nash equilibrium. Formally it is defined by:</p>
<p>Definition 7 (Nash equilibrium; Nash, 1950b) A set of strategies $s=\left(s_{1}, \ldots, s_{n}\right)$ is a Nash equilibrium if, for all agents $i, s_{i}$ is a best response to $s_{-i}$.</p>
<p>Even when it is proved that in every game exists a Nash equilibrium, this solution concept has limitations. One problem is that there may be multiple equilibria in a game, and it is not an easy task to select one (Harsanyi and Selten, 1988). Also several experiments involving humans have shown that that people usually do not follow the actions prescribed by the theory (Kahneman and Tversky, 1979; Risse, 2000; Goeree and Holt, 2001; Camerer, 2003).</p>
<p>Extensive-form games Another common representation for games is the extensive-form in which it is easier to describe the sequential structure of the decisions (for example, this is useful to represent poker games). Commonly, the game is described as a tree where nodes represent actions taken by the players. Extensive-form games can be finite or infinite-horizon (regarding the length of the longest possible path), with observable or non-observable actions and with complete or incomplete information (observability of the opponent payoffs) (Fudenberg and Tirole, 1991). Most of the games represented in extensive form can be converted into a normal-form representation, however, this generally results in a matrix which is exponential in the size of the original game. For this reason, it is common to find a solution in the original game tree.</p>
<p>Repeated and stochastic games Previous concepts (e.g., best response, Nash equilibrium) were defined for one-shot games (one single interaction), however, it could be the case that more than one decision has to be made. For example, repeating the same game, or having a set of possible games.</p>
<p>Definition 8 (Stochastic game) A stochastic game (also known as a Markov game) is a tuple $(S, \mathcal{N}, A, T, R)$, where: $S$ is a finite set of states, $\mathcal{N}$ is a finite set of $\mathcal{I}$ players, $A=A_{1} \times \cdots \times A_{\mathcal{I}}$ where $A_{i}$ is finite set of actions available to player $i, T: S \times A \times S \rightarrow \mathbb{R}$ is the transition probability function; $T(s, a, \hat{s})$ is the probability of transitioning from state $s$ to state $\hat{s}$ after action profile $a$, and $R=r_{1}, \ldots, r_{\mathcal{I}}$ where $r_{i}: S \times A \rightarrow \mathbb{R}$ is a real valued payoff function for player $i$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) The automata that describes the TFT strategy, depending of the opponent action (c or d) it transitions between the two states C and D. (b) The automata describing Pavlov strategy, it consists of four states formed by the last action of both agents (CC, CD, DC, DD).</p>
<p>In a stochastic game, agents repeatedly play games (states) from a collection. The particular game played at any given iteration depends probabilistically on the previous played game (state), and on the actions taken by all agents in that game (Shoham and Leyton-Brown, 2008).</p>
<p>Definition 9 (Repeated game) A repeated game is a stochastic game in which there is only one game (called stage game).</p>
<p>To exemplify a repeated game, recall the prisoner's dilemma presented in Table 1. Repeating the game for a number of rounds results in the iterated prisoner's dilemma (iPD), which has been the subject of different experiments and for which there are diverse wellknown strategies. A successful strategy which won Axelrod's tournament ${ }^{7}$ is called Tit-for-Tat (TFT) (Axelrod and Hamilton, 1981); it starts by cooperating, and does whatever the opponent did in the previous round: it will cooperate if the opponent cooperated, and will defect if the opponent defected. Another important strategy is called Pavlov, which cooperates if both players performed the same action and defect whenever they used different actions in the past round. The finite-state machines describing TFT and Pavlov are depicted in Figure 3. It should be noticed that these strategies can be described in terms of the past actions and therefore do not depend on the time index; they are stationary strategies.</p>
<p>Having presented the formal models of multi-armed bandits, reinforcement learning and game theory, the next Section highlights the challenge of non-stationarity in multiagent systems, followed by a new framework for this setting. Moreover, we present our proposed categorisation on how algorithms cope with non-stationary behaviour.</p>
<h1>3. Learning in multiagent environments</h1>
<p>The following subsection pinpoints where and how the main challenge of non-stationarity arises in multiagent environments. This provides a crisp basis problem definition against which the approaches of algorithms can be positioned. Next, we present a new abstract</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>framework for multiagent learning algorithms that naturally models (and emphasizes) three components of accounting for other reasoning agents. Finally, we present a taxonomy of multiagent learning algorithms, aligned with assumptions they make in light of this framework.</p>
<h1>3.1 The problem</h1>
<p>Learning in a multiagent environments is inherently more complex than in the single-agent case, as agents interact at the same time with environment and potentially with each other (Busoniu et al., 2008). Transferring single-agent algorithms to the multiagent setting is a natural heuristic approach (see Section 3.3) - even if assumptions under which these algorithms were derived are violated. In particular the Markov property, denoting a stationary environment, does not hold, thus invalidating guarantees derived for the singleagent case (Tuyls and Weiss, 2012). Since this approach of applying single-agent algorithms ignores the multiagent nature of the setting entirely, it can fail when an opponent may adapt its choice of actions based on the past history of the game (Shoham et al., 2007).</p>
<p>In order to expose why multiagent domains are non-stationary from agents' local perspectives, consider a stochastic game $(S, \mathcal{N}, A, T, R)$. Given a learning agent $i$ and using the common shorthand notation $-\boldsymbol{i}=\mathcal{N} \backslash{i}$ for the set of opponents, the value function now depends on the joint action $\boldsymbol{a}=\left(a_{i}, \boldsymbol{a}<em j="j">{-\boldsymbol{i}}\right)$, and the joint policy $\boldsymbol{\pi}(s, \boldsymbol{a})=\prod</em>\right)$ :} \pi_{j}\left(s, a_{j</p>
<p>$$
V_{i}^{\boldsymbol{\pi}}(s)=\sum_{\boldsymbol{a} \in A} \boldsymbol{\pi}(s, \boldsymbol{a}) \sum_{s^{\prime} \in S} T\left(s, a_{i}, \boldsymbol{a}<em i="i">{-\boldsymbol{i}}, s^{\prime}\right)\left[R\left(s, a</em>}, \boldsymbol{a<em i="i">{-\boldsymbol{i}}, s^{\prime}\right)+\gamma V</em>\right)\right]
$$}\left(s^{\prime</p>
<p>Consequently, the optimal policy is a best response dependent on the other agents' policies,</p>
<p>$$
\begin{aligned}
&amp; \pi_{i}^{*}\left(s, a_{i}, \boldsymbol{\pi}<em i="i">{-\boldsymbol{i}}\right)=B R</em>}\left(\boldsymbol{\pi<em i="i">{-\boldsymbol{i}}\right)=\underset{\pi</em>}}{\arg \max } V_{i}^{\left(\pi_{i}, \boldsymbol{\pi<em i="i">{-\boldsymbol{i}}\right)}(s) \
&amp; =\underset{\pi</em>}}{\arg \max } \sum_{\boldsymbol{a} \in A} \pi_{i}\left(s, a_{i}\right) \boldsymbol{\pi<em -_boldsymbol_i="-\boldsymbol{i">{-\boldsymbol{i}}\left(s, \boldsymbol{a}</em>}}\right) \sum_{s^{\prime} \in S} T\left(s, a_{i}, \boldsymbol{a<em i="i">{-\boldsymbol{i}}, s^{\prime}\right)\left[R\left(s, a</em>}, \boldsymbol{a<em i="i">{-\boldsymbol{i}}, s^{\prime}\right)+\gamma V</em>\right)\right]
\end{aligned}
$$}^{\left(\pi_{i}, \boldsymbol{\pi}_{-\boldsymbol{i}}\right)}\left(s^{\prime</p>
<p>Specifically the opponents' joint policy $\boldsymbol{\pi}<em -_boldsymbol_i="-\boldsymbol{i">{-\boldsymbol{i}}\left(s, \boldsymbol{a}</em>\right)$ could be non-stationary, (for example when opponents' are learning) thus becoming the parameter of the best response function. If the opponents' are not learning, e.g., they are using a stochastic policy, then the environment is Markovian and single-agent learning algorithms suffice.}</p>
<p>Next, we propose a general framework for multiagent learning algorithms, separating three steps of modelling opponents' behaviour to tackle the problem of non-stationary opponent policies.</p>
<h3>3.2 A new framework for multiagent learning algorithms</h3>
<p>Before going into the formal definitions of the abstract concepts, consider an intuitive description of the three components of our proposed framework:</p>
<ul>
<li>Policy generating functions $\tau \in \mathcal{T}$, describe how an opponent $j$ obtains its policy $\pi_{j}$.</li>
<li>
<p>Belief $\beta_{j}$, i.e., a probability distribution over $\tau$, measures an agent's belief about each opponent's reasoning.</p>
</li>
<li>
<p>Influence function $\theta$ partitions beliefs according to equivalent best responses.</p>
</li>
</ul>
<p>Some definitions are required to describe each component in detail: let $h_{t}=\left(z_{0}, z_{1}, \ldots, z_{t}\right)$ denote the observation history of $t$ observations, and let $H^{t}$ be the set of all possible histories of this length. Note that observations in the stochastic game are given by state, action sequences, but this more general representation also subsumes models of partial observability, such as POMDPs. While rewards are commonly treated separately in the literature, they may simply be added as part of the observation history in our model. Some work may presume or learn a model of reward functions, as in POMDPs for the agent (Kaelbling et al., 1998) or the frame of I-POMDPs for opponents (Gmytrasiewicz and Doshi, 2005), which is equally compatible with our model.</p>
<p>Definition 10 (Policy generating functions) A policy generation function (PGF) $\tau$ maps the history of observations into a policy $\pi$,</p>
<p>$$
\tau: H^{t} \rightarrow \Pi
$$</p>
<p>On the one hand, this definition could be extended to the stochastic case $\tau: H^{t} \rightarrow \Delta(\Pi)$, where $\Delta(\cdot)$ indicates the simplex function, i.e., here denoting a probability distribution or probability mass function over policies. However, this complexity appears unnecessary for the exposition we aim for in this section. On the other hand, the composition of $\tau$ and $\pi$ could be chosen as an alternative definition, thus mapping histories directly to actions.</p>
<p>In contrast to alternative definitions, deterministic policy generating functions are a particularly relevant category since they capture memory-bounded models with hidden states, while maintaining the structure of policies. This enables additional assumptions over the rate of change in policies, or the set of policies that are (re-) visited by the algorithm. Such models subsume learning algorithms, e.g., $Q$-learning (Watkins, 1989), weight matrices describing neural networks (Bengio, 2009), and MDPs, e.g., mapping a sliding window of the histories to an action or policy, as finite state automata over a predefined set of policies (Banerjee and Peng, 2005; Chakraborty and Stone, 2013).</p>
<p>The PGFs capture the adaptation dynamics of agents, and research articles derive insights related to learning algorithms within a scope delimited by an implicitly or explicitly defined set of PGFs for any opponents. One of the more general assumptions is given by the frame defined in I-POMDPs (Gmytrasiewicz and Doshi, 2005), which assumes further structure on the PGFs, such as ascribing rewards and optimality criteria to opponents. Our taxonomy below employs PGF assumptions as a main criterion for classifying algorithms.</p>
<p>The next step is to define how the agent uses those PGFs. Note that observations are local to each agent, i.e., an agent $i$ can only infer another agent's local perceived observation history $h_{j}$ by a probability distribution $p\left(h_{j} \mid h_{i}\right)$ using its own observations $h_{i}$ together with any available a priori knowledge, e.g., about the structure of the game. In stochastic games, state, action sequences are joint observations, ${ }^{8}$ thus $h_{j}=h_{i}$ if rewards are treated separately.</p>
<p>Definition 11 (Belief) $A$ belief $\beta \in \mathcal{B}$ indicates for each opponent $j$ the likelihood $\beta_{j}\left(\tau \mid h_{j}\right)$ for each policy generating function $\tau$ given opponent experience $h_{j}$.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Since $h_{j}$ is local information, it must be inferred from agent $i$ 's observations, i.e., $h_{j} \sim$ $p\left(h_{j} \mid h_{i}\right)$. If the presumed set $\mathcal{T}$ gives rise to distinguishable policies, then the belief may identify each opponent's PGF in a crisp belief (assigning probability one to a specific $\tau$ for each opponent). Even if unique identification is not possible, this poses a classification task, and a unique assignment may be used as an approximation of the belief. On the other hand, full belief representations over multiple $\tau$ for each opponent are common in Bayesian reasoning (Ghavamzadeh et al., 2015).</p>
<p>The last step defines how the belief could be filtered (e.g., to reduce complexity) by means of an influence function.</p>
<p>Definition 12 (Influence function for multiagent learning) The co-domain of the influence function $\theta$ over the belief is a $k$-dimensional influence space $\Theta$ :</p>
<p>$$
\theta: \mathcal{B} \rightarrow \Theta
$$</p>
<p>Assumptions about the influence function may significantly alter the complexity of the algorithm, and the validity of such assumptions differentiates whether resulting model insights hold or reduce to heuristic approaches; below we provide some examples.</p>
<ul>
<li>In single-agent learning, the assumption is that $\theta$ maps onto a singleton set.</li>
<li>On the opposite side of the spectrum, taking the identity function as $\theta$ is equivalent to not modelling $\theta$ at all, thus also not limiting the validity at this step.</li>
<li>However, imposing - or learning - a structure of $\theta$ would cluster equivalent best responses (Bard et al., 2015) and may lead to more sample efficient learning of best response approximations. One example instantiation of the influence function may encode abductive reasoning by mapping mixed beliefs to crisp classifications, as mentioned in the above discussion of beliefs.</li>
<li>Furthermore, in symmetric games with distinguishable policies, $\theta$ may encode the strategy histogram (counting players for each $\tau$ ), as by definition the payoffs of a player only depend on the strategies employed, and not on who is playing them. This structure is used in heuristic payoff tables to compress utility representations and corresponding best response mappings (Walsh et al., 2002).</li>
</ul>
<p>Overall, an influence function typically reduces the complexity, either as a lossless compression or as a heuristic to reduce the set of best responses and the computational complexity of deriving them.</p>
<p>Definition 13 (Best response in multiagent learning) A multiagent learning algorithm computes the best response to the influence state of its belief, given an a priori assumed $\mathcal{T}$ :</p>
<p>$$
B R_{i}(\hat{\theta})=\pi_{i}^{*}(s, a, \hat{\theta})=B R_{i}\left(\boldsymbol{\pi}<em j="j">{-\boldsymbol{i}} \mid \pi</em>\right)\right)
$$} \sim \beta_{j}\left(\tau \mid h_{j}\right), h_{j} \sim p\left(h_{j} \mid h_{i</p>
<p>for any $\beta$ that satisfies $\theta(\beta)=\hat{\theta}$.
This new framework maintains agent independence by mutual non-observability of individual policies, and inherently models agent autonomy by the independent choice of best response policies.</p>
<h1>3.3 Taxonomic Categories: Environment, Opponent and Agent</h1>
<p>Now, we present a taxonomy in terms of the environment (observability) and the opponent characteristics (learning capabilities). Then, we provide an overview of the proposed categories of how algorithms deal with non-stationarity.</p>
<h3>3.3.1 ENVIRONMENT: OBSERVABILITY</h3>
<p>One crucial aspect that provides information on how to tackle a learning problem is observability of actions and rewards, for both the learning agent and the opponent. Depending on the restriction of the domain, there are four categories in increasing order of observability.</p>
<p>Local reward. The most basic information that an algorithm commonly observes are its own immediate rewards.</p>
<p>Opponent actions. Most algorithms also assume that is possible to observe the opponent actions (but not the their rewards).</p>
<p>Opponent actions and payoffs. Some algorithms assume to observe the action and also the actual payoffs of the opponents (which may hold more naturally in cooperative scenarios).</p>
<p>Complete a priori knowledge. Similar to the previous category algorithms observe rewards and actions, however, in this category the algorithms know from start the complete reward function.</p>
<h3>3.3.2 OPPORENT: ADAPTATION CAPABILITIES</h3>
<p>The capability of the opponent to adapt and change its behaviour provides another source of important information to be used while learning. Roughly, we distinguish three categories:</p>
<p>No adaptation. $\forall h^{t}: \tau\left(h^{t}\right)=\pi$ These are opponents that follow a stationary strategy during the complete period of interaction.</p>
<p>Slow adaptation. $\exists \epsilon&lt;&lt;1, \forall t: d\left(\tau\left(h^{t+1}\right), \tau\left(h^{t}\right)\right)&lt;\epsilon$ These opponents show non-stationary behaviour. However, it is a limited adaptation, for example providing bounds to the possible change in the current strategy between rounds. Candidate metrics are Manhattan distance $d_{1}$ or the average Jensen-Shannon distance over all states, which with base 2 logarithm is bound to $[0,1]: d\left(\tau\left(h^{t+1}\right), \tau\left(h^{t}\right)\right) \doteq \frac{1}{|S|} \sum_{s} \sqrt{J S D\left(\tau_{s}\left(h^{t+1}\right) | \tau_{s}\left(h^{t}\right)\right)}$.</p>
<p>Drastic or abrupt adaptation. If the above assumptions are not in place, non-stationary opponents may show abrupt changes in their behaviour, for example changing to a different strategy (no limits) from one step to the next.</p>
<h3>3.3.3 AGENT: DEALING WITH NON-STATIONARITY</h3>
<p>Previous surveys have proposed different ways to categorise algorithms in multiagent systems such as: team vs concurrent learning (Panait and Luke, 2005); temporal difference, game theory and direct policy search (Busoniu et al., 2010); model-based, model-free and regret minimization (Shoham et al., 2007); and joint action, gradient, Nash and other learners</p>
<p>(see Weiss, 2013, chap. 10). There are also some previous categories for the type of learning used: multiplied, divided and interactive (Tuyls and Weiss, 2012); and independent, joint-action and gradient ascent (Bloembergen et al., 2015).</p>
<p>Another group of works have proposed properties that MAS algorithms should have: Bowling and Veloso (2002) propose rationality and convergence. The former needs the learning algorithm to converge to a stationary policy that is a best-response to the other players policies if the other players policies converge to stationary policies; the latter refers to the need of the agent to necessarily converge to a stationary policy. Powers and Shoham (2004) proposed: targeted optimality, compatibility and safety. The first one needs the agent to achieve within $\epsilon$ of the expected value of the best response to the actual opponent. Compatibility needs the algorithm to achieve at least within $\epsilon$ of the payoff of some Nash equilibrium that is not Pareto dominated by another NE (during self-play), and safety needs the agent to receive at least within $\epsilon$ of the security value for the game. Crandall and Goodrich (2011) proposed: security, coordination and cooperation. Security refers to long-term average payoffs meet a minimum threshold, coordination refers to the ability to coordinate behaviour when associates share common interests, and cooperation is the ability to make compromises that approach or exceed the value of the Nash bargaining solution (Nash, 1950a) in games of conflicting interest.</p>
<p>In contrast with previous works, we propose another view focused on how algorithms deal with non-stationary behaviour. We propose five categories in increasing order of sophistication which we summarize as follows:</p>
<ol>
<li>Ignore. The most basic approach which assumes a stationary environment.</li>
<li>Forget. These algorithms adapt to the changing environment by forgetting information and at the same time updating with recent observations, usually they are model-free approaches.</li>
<li>Respond to target opponents. Algorithms in this group have a clear and defined target opponent in mind and optimize against that opponent strategy.</li>
<li>Learn opponent models. These are model-based approaches that learn how the opponent is behaving and use that model to derive an acting policy. When the opponent changes they need to update its model and policy.</li>
<li>Theory of mind. These algorithms model the opponent assuming the opponent is modelling them, creating a recursive reasoning.</li>
</ol>
<p>Note that this order is according to the sophistication in terms of complexity in assumptions and approach - it is throughout possible that the elegance of solutions does not follow this ordering. Moreover, we acknowledge that some algorithms could fit in more than one category. To better understand the high level behaviour of these categories, the next section presents an illustrative example using a simple domain.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Ignore: $\mathcal{A}$ assumes an opponent which is stationary $(S)$ for the complete interaction period. Examples are $Q$-learning (Watkins and Dayan, 1992) and fictitious play (Brown, 1951).
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Forget: $\mathcal{A}$ learns an initial strategy $(S 1)$ which is continually updated $\left(S 1^{\prime}, S 1^{\prime \prime}, \ldots\right)$ with recent observations, one example is WoLF-PHC (Bowling and Veloso, 2002).
<img alt="img-5.jpeg" src="img-5.jpeg" />
(c) Respond to target opponents: One example is Minimax-Q (Littman, 1994) where the learning agent assumes the opponent tries to minimise the rewards.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(d) Learn: $\mathcal{A}$ learns a model of the opponent strategy $(S ?)$ and derives an acting policy; opponent changes are infrequent, e.g., RL-CD (Da Silva et al., 2006).
<img alt="img-7.jpeg" src="img-7.jpeg" />
(e) Theory of mind: $\mathcal{O}$ reasons about how $\mathcal{A}$ might act and obtains a best response against that behaviour, $B R(\mathcal{A}) . \mathcal{A}$ repeats that process with the model of $\mathcal{O}$, $B R(B R(\mathcal{A}))$ (Gmytrasiewicz and Durfee, 2000).</p>
<p>Figure 4: A learning agent $\mathcal{A}$ (outside the cloud) and how it models one opponent $\mathcal{O}$ (inside the cloud) exemplifying the 5 categories of how to handle non-stationary behaviour.</p>
<h1>4. Illustrative Example - Iterated Prisoner's Dilemma</h1>
<p>In this section we exemplify each category and contrast them by using the same domain (see Figure 4). Our example is presented in the context of the iterated prisoner's dilemma (see Section 2.3) where two agents $\mathcal{A}$ and $\mathcal{O}$ play the infinite-horizon version of this game.</p>
<p>Definition 14 Prisoner's dilemma $(P D)$ is a normal-form game $\langle\mathcal{N}, A, u\rangle$, where:
The set of players $\mathcal{N}={\mathcal{A}, \mathcal{O}}$ are the two agents.
The set of actions is the same for both agents, they have two possible actions $A={C, D}$.
$u=\left(u_{\mathcal{A}}, u_{\mathcal{O}}\right)$ where $u_{i}: A \mapsto \mathbb{R}$ the payoff function for player $i$ as shown in Table 1, satisfying: $t_{p d}&gt;r_{p d}&gt;p_{p d}&gt;s_{p d}$ and $2 r_{p d}&gt;p_{p d}+s_{p d}$.</p>
<p>In the PD game when both players cooperate they both obtain the reward $r_{p d}$. If both defect, they get a punishment reward $p_{p d}$. If a player chooses to cooperate with someone who defects receives the sucker's payoff $s_{p d}$, whereas the defecting player gains the temptation to defect, $t_{p d}$.</p>
<p>We now present slight variations of the above scenario exemplifying the assumptions made by algorithms in each category, pointing out where they are most useful and where their main assumptions do not hold.</p>
<h3>4.1 Ignore</h3>
<p>In this category algorithms can be useful with simple opponents or by making probably unrealistic assumptions, ignoring the non-stationary behaviour. For example, assume the opponent uses a mixed (stationary) strategy, $\pi_{m}=(0.25,0.75)$ with higher probability of selecting defect. If the assumption is correct, the learning agent can use fictitious play (Brown, 1951) to learn an optimal policy against $\mathcal{O}$. However, consider the case that after $\mathcal{A}$ has learned the optimal policy, $\mathcal{O}$ decides to change to a Tit-for-Tat strategy $\pi_{T F T}$, thus $\mathcal{A}$ 's learned policy will no longer be optimal.</p>
<h3>4.2 Forget</h3>
<p>Now, consider a different set of assumptions where $\mathcal{A}$ is interested in converging to a stationary policy and $\mathcal{O}$ has the same interest. Thus, both agents need to adapt to the changing (non-stationary) behaviour of the other (see Figure 4(b)). One algorithm that is especially useful in this scenario is WoLF-PHC (Bowling and Veloso, 2002), the algorithm generalizes $Q$-learning, but it was proposed to converge to a stationary policy in self-play. We can view WOLF-PHC as continuously learning (and forgetting), adjusting its learning rate to cope with the changing behaviour of the opponent. Note that, if we remove the assumption of self-play (and we assume $\mathcal{O}$ uses a different behaviour), then WOLF-PHC loses its convergence guarantees.</p>
<h3>4.3 Respond to target opponents</h3>
<p>If the learning agent knows (or assumes) the opponent will behave in specific ways, that information can be used to target classes of opponents. For example, assume $\mathcal{A}$ knows that the opponent will use the set of strategies ${$ Tit-for-Tat, Pavlov, Bully $}$ and change</p>
<p>among them stochastically. In this case HM-MDPs (Choi et al., 1999) can target that type of opponent since they assume the environment can be represented in different stationary modes (MDPs) with stochastic transitions among themselves. Note that there are different algorithms that target a variety of classes (see Section 5.3). However, if the assumptions about the opponent do not hold (in this case, adding a new strategy to the initial set) these algorithms provide restricted adaptability and therefore the policy will be suboptimal after most opponent changes.</p>
<h1>4.4 Learn opponent models</h1>
<p>In this category, agent $\mathcal{A}$ learns a model of the opponent which is used to derive an optimal acting policy. In this case, the learning agent starts without predefined opponent strategies or policies (Da Silva et al., 2006; Hernandez-Leal et al., 2014a). Instead, $\mathcal{A}$ assumes the opponent will use several stationary strategies with infrequent changes among them. For example, in the iPD the opponent could start with Pavlov and later change to Tit-for-Tat. Moreover, if the opponent returns to a previous learned strategy, $\mathcal{A}$ should be able to detect and change its policy without relearning the model (Hernandez-Leal et al., 2016a). However, one limitation of these algorithms is that they do not consider the strategic behaviour of $\mathcal{O}$ (an opponent that reasons about the agent $\mathcal{A}$ ).</p>
<h3>4.5 Theory of mind</h3>
<p>In the last category, the learning agent assumes an opponent that is performing strategic reasoning. This is, in the lowest level $\mathcal{O}$ reasons about $\mathcal{A}$, in an upper level $\mathcal{A}$ reasons about $\mathcal{O}$ reasoning about $\mathcal{A}$. Best responding to a reasoning level is the way to obtain an acting policy. For example, assume the opponent thinks $\mathcal{A}$ uses a set of strategies to act ${$ Bully, random, Pavlov}, a distribution of those strategies represent the zero level or reasoning, $L_{0}$. With the previous information $\mathcal{O}$ can compute a best response $(B R)$ against $L_{0}$, called level 1 strategy, $L_{1}=B R\left(L_{0}\right)$. Moreover, $\mathcal{A}$ can compute a best response against a distribution of the previous two levels, to obtain an acting policy (level 2) $L_{2}=$ $B R\left(\left{L_{1}, L_{0}\right}\right)$. Note that this recursive reasoning could continue upwards and is the base of many approaches (Camerer et al., 2004; Gmytrasiewicz and Doshi, 2005; Wunder et al., 2009, 2012). A limitation is that the basic strategies need to be specified a priori and computing optimal policies can be computationally expensive (Gmytrasiewicz and Doshi, 2005).</p>
<p>In the next section we present an extensive list of state-of-the-art algorithms in from game theory, multi-armed bandits and RL and where they fall into each category of sophistication along with their environment and opponent characteristics.</p>
<h2>5. Algorithms</h2>
<p>In this section we present an extensive list of algorithms categorised with respect to how they deal with non-stationarity. Table 2 summarises this section by providing for each algorithm its category and some related characteristics such as observability, opponent adaptation and the environment it was designed for. Similarly, Figure 5 depicts a diagram highlighting</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>Stochastic games usually assume that agents have complete information about the state of the game, a more general model are partially observable stochastic games (POSGs) (Bernstein et al., 2004).</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
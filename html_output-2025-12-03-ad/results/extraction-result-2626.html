<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2626 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2626</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2626</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-51b67d3cfb6ad8a22054d4aaa0092a1c76cf19ee</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/51b67d3cfb6ad8a22054d4aaa0092a1c76cf19ee" target="_blank">Survey of multifidelity methods in uncertainty propagation, inference, and optimization</a></p>
                <p><strong>Paper Venue:</strong> SIAM Review</p>
                <p><strong>Paper TL;DR:</strong> In many situations across computational science and engineering, multiple computational models are available that describe a system of interest and these different models have varying evaluation costs.</p>
                <p><strong>Paper Abstract:</strong> In many situations across computational science and engineering, multiple computational models are available that describe a system of interest. These different models have varying evaluation costs...</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2626.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2626.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Monte Carlo (control-variate based multifidelity estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multifidelity Monte Carlo estimator that fuses a small number of high-fidelity evaluations with many low-fidelity evaluations as control variates and optimally allocates the evaluation budget to minimize MSE for a given computational budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multifidelity Monte Carlo (MFMC) control-variate estimator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs an unbiased estimator of an expectation of a high-fidelity model by combining a high-fidelity Monte Carlo estimate with a telescoping combination of low-fidelity Monte Carlo estimates weighted by control-variate coefficients. The method parameterizes per-model costs c_hi, c_lo^(i), variances sigma_hi^2, sigma_i^2, and Pearson correlations rho_i between models, and solves for optimal control coefficients alpha_i* and numbers of samples m_i* that minimize estimator MSE subject to a cost constraint gamma. Key closed-form results include alpha_i* = rho_i * (sigma_hi / sigma_i) and sample ratios m_i* proportional to r_i with r_i = sqrt( c_hi*(rho_i^2 - rho_{i+1}^2) / ( c_lo^(i)*(1 - rho_1^2) ) ).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Uncertainty propagation / Monte Carlo estimation (general computational science and engineering models)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Solve constrained optimization: minimize MSE (variance) of the multifidelity estimator subject to a fixed computational budget gamma and monotonicity constraints m0 <= m1 <= ... <= mk. The solution yields analytic optimal control-variate coefficients and optimal sample counts per model (m0*, m1*, ..., mk*) as functions of model variances, pairwise correlations, and per-evaluation costs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Abstract per-evaluation costs c_hi and c_lo^(i) (positive real numbers), total cost is sum m_i * c_i; optimization enforces total cost equals budget gamma (units: cost units proportional to wall-time or FLOP equivalent as modeled by c).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Variance reduction / mean-squared error (MSE) reduction of the estimator (analytic expression for estimator variance as function of m_i and alpha_i).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not framed as exploration/exploitation; tradeoff arises analytically between spending budget on many cheap, correlated low-fidelity evaluations (to reduce variance) versus fewer expensive high-fidelity evaluations that anchor unbiasedness; the allocation is determined by closed-form optimal ratios that depend on cost and correlation tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>None explicit — diversity of hypotheses is not an objective; the method instead leverages correlated low-fidelity evaluations to reduce estimator variance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed computational budget (gamma) expressed in the same cost units as per-model costs.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Explicit constrained optimization: choose m_i and alpha_i to minimize variance subject to total cost = gamma and ordering constraints; yields analytic solution under stated assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not applicable — objective is statistical estimation accuracy (expectation/variance) rather than 'breakthrough' event discovery; performance measured by MSE/variance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Estimator mean-squared error (MSE) / variance; variance expression: V[hat{s}^MF] = sigma_hi^2 / m0 + sum_{i=1}^k (1/m_{i-1} - 1/m_i) (alpha_i^2 sigma_i^2 - 2 alpha_i rho_i sigma_hi sigma_i). Also variance-reduction ratio relative to high-fidelity-only MC: ( sqrt(1 - rho_1^2) + sum_{i=1}^k sqrt( (c_lo^(i) / c_hi) * (rho_i^2 - rho_{i+1}^2) ) )^2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmark Monte Carlo using only the high-fidelity model (same total cost gamma).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Analytical variance-reduction ratio given above; multifidelity estimator is cheaper (lower MSE for same budget) when inequality sqrt(1 - rho_1^2) + sum_{i=1}^k sqrt( (c_lo^(i)/c_hi) * (rho_i^2 - rho_{i+1}^2) ) < 1 holds. No fixed numerical gains reported in survey (depends on correlations and costs).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Problem-dependent; when correlations are sufficiently high and low-fidelity costs sufficiently low the MFMC estimator yields substantial variance reduction for the same cost; no universal numeric factor reported in the survey (analytic condition provided).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — explicit analytic tradeoff between model correlation (information content) and per-evaluation cost. The method shows that a low-fidelity model's contribution depends on both its correlation with the high-fidelity model and its cost relative to the high-fidelity model, and also on interactions with other models in the hierarchy (rho_{i}^2 - rho_{i+1}^2 terms).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Closed-form optimal control coefficients alpha_i* = rho_i * (sigma_hi / sigma_i) and sample counts m0* = gamma / (c^T r), m_i* = r_i * m0* with r_i as above; ordering of models matters and should be chosen to minimize MSE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of multifidelity methods in uncertainty propagation, inference, and optimization', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2626.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2626.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MF-IS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Importance Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage multifidelity importance sampling approach that uses a cheap low-fidelity model to construct a biasing distribution (via density estimation on low-fidelity-predicted failure samples) and then evaluates the high-fidelity model under that biasing distribution to obtain unbiased estimates with reduced variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multifidelity Importance Sampling (low-fidelity constructed biasing density)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Step 1: evaluate the low-fidelity model on a large sample of inputs and collect those predicted to satisfy an event (e.g., f_lo(z)<0); fit a biasing density q (e.g., Gaussian mixture via EM) to those samples. Step 2: draw samples from q and evaluate the high-fidelity model; compute importance-sampling estimator with weights p/q to estimate the true probability. The approach is unbiased provided support(q) includes support(p) and aims to reduce estimator variance by focusing sampling on event-relevant regions identified cheaply by the low-fidelity model.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Rare-event probability estimation / uncertainty propagation / failure probability estimation</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate large budget to many cheap low-fidelity evaluations to learn an importance density q, then allocate remaining (expensive) budget to high-fidelity evaluations drawn from q; the split (n low-fidelity vs m high-fidelity) is chosen to make learning q feasible while keeping the final high-fidelity estimator unbiased and low-variance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of model evaluations weighted by per-evaluation costs; low-fidelity evaluations are cheap so n >> m is typical; no single unit specified beyond c_hi and c_lo.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Reduction in variance (MSE) of importance-sampling estimator; the biasing density aims to increase sampling density in regions that contribute most to the event probability (information about rare event).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration via dense low-fidelity sampling to discover candidate event regions; exploitation via focused high-fidelity evaluations drawn from the learned biasing distribution to refine probability estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicitly enforced if density estimator (e.g., Gaussian mixture) captures multiple modes of the low-fidelity-predicted event — no explicit diversity regularizer described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed computational budget split between many cheap low-fidelity evaluations (to fit q) and fewer expensive high-fidelity evaluations (to compute weighted estimates).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Heuristic two-stage allocation: choose n large enough to fit a biasing density q from low-fidelity samples, then sample m high-fidelity draws from q; unbiasedness maintained by importance weights and support condition.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not defined as 'breakthrough' — objective is accurate estimation of rare-event probability (low MSE); success measured by variance reduction in probability estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Estimator unbiasedness and MSE = Var_q[ I_hi * (p/q) ] / m; practical savings depend on low-fidelity model accuracy in identifying event region and on quality of the fitted q; no universal numeric results in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard Monte Carlo sampling from nominal density p (no importance sampling) and importance sampling with biasing densities built from high-fidelity evaluations only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Can achieve large runtime savings in construction of biasing density if low-fidelity model accurately captures event regions; unbiasedness retained for high-fidelity estimator if support condition holds. Specific gains depend on low-fidelity fidelity and density estimation quality (no fixed numbers provided).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Potential major reductions in the number of high-fidelity samples needed to estimate rare-event probabilities when low-fidelity identifies event regions well; survey provides qualitative description and conditions for unbiasedness but no fixed numerical factors.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — tradeoff between investing many cheap evaluations to learn q (risk of a poor q if low-fidelity is inaccurate) versus spending more expensive high-fidelity samples; importance-sampling variance depends critically on the quality of q.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>No single analytic allocation formula is given; key principle is to ensure the biasing density q covers the support of p and concentrates mass in regions where the true high-fidelity indicator is nonzero; quality of low-fidelity model in predicting event region governs achievable gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of multifidelity methods in uncertainty propagation, inference, and optimization', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2626.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2626.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-stage MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage / Delayed Acceptance MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multifidelity MCMC framework where candidate proposals are first screened using a cheap low-fidelity likelihood before the expensive high-fidelity likelihood is computed, reducing high-fidelity evaluations while preserving correct posterior sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Two-stage (delayed acceptance) MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each proposed sample, compute an acceptance decision with a low-fidelity model-induced likelihood first; only if the proposal passes the low-fidelity acceptance test is the high-fidelity model evaluated and the final Metropolis-Hastings decision taken. This filtering reduces the number of expensive high-fidelity likelihood evaluations while maintaining correctness of the high-fidelity posterior under the two-stage acceptance scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian inverse problems / statistical inference (MCMC sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Cheap low-fidelity evaluations are used as a prefilter to avoid unnecessary high-fidelity evaluations; high-fidelity evaluations are allocated only to proposals that pass the low-fidelity check, thereby adaptively allocating expensive evaluations to promising proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Counts of model evaluations (number of high-fidelity likelihood evaluations saved) weighted by per-evaluation costs; high-fidelity cost is the primary expense to minimize.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly framed as information gain; objective is to preserve correct posterior sampling while reducing costly evaluations — implicit information metric is acceptance probability under the true high-fidelity posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Mechanism focuses on reducing wasted exploitation of poor proposals by cheaply screening them; exploration/exploitation tradeoff managed indirectly via MCMC proposal and two-stage acceptance, not via an explicit acquisition function.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity promotion; diversity arises from the underlying MCMC proposal mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational cost minimization (reduce number of expensive model evaluations); no explicit fixed budget formulation in text but objective is efficiency of sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Filtering strategy: low-fidelity acceptance reduces frequency of high-fidelity evaluation; can be combined with adaptation of the low-fidelity model to further reduce cost.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not applicable; goal is faithful posterior sampling (convergence diagnostics, effective sample size) rather than breakthrough detection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reduction in number of high-fidelity model evaluations per effective sample; retention of correct posterior target distribution (theoretical correctness of delayed-acceptance scheme). No specific numerical results in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard single-fidelity MCMC that evaluates high-fidelity likelihood at each proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Two-stage MCMC reduces the number of expensive evaluations while still sampling from the correct posterior; actual savings depend on low-fidelity acceptance rates and fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Problem-dependent; potential large savings when low-fidelity model reliably rejects many low-probability proposals at low cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Implicit: savings require low-fidelity to be a sufficiently good discriminator; if low-fidelity wrongly passes many bad proposals or rejects good ones incorrectly, efficiency and/or correctness can suffer unless corrected in the two-stage acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Principle: design the low-fidelity filter to have high negative predictive value (reject bad proposals cheaply) while maintaining negligible bias via the high-fidelity second-stage acceptance; the survey notes two-stage formulations and adaptive variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of multifidelity methods in uncertainty propagation, inference, and optimization', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2626.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2626.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EGO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient Global Optimization (kriging-based surrogate optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surrogate-based global optimization framework that adaptively builds a Gaussian-process (kriging) surrogate of an expensive objective and selects new evaluation points to improve the global optimum estimate, leveraging low-fidelity models in multifidelity variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Efficient Global Optimization (EGO) with kriging</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Builds a kriging (Gaussian-process) surrogate of the expensive objective and iteratively selects new evaluation points based on an acquisition criterion (survey references its use as an adaptation strategy where the surrogate is updated each iteration). Multifidelity extensions adaptively construct or correct surrogates using low- and high-fidelity data (e.g., cokriging or hierarchical surrogates) to reduce the number of high-fidelity evaluations required to find the global optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Global optimization of expensive black-box objective functions (engineering design, simulation-based optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate expensive high-fidelity evaluations to points chosen by surrogate-based acquisition (surrogate indicates high expected utility for optimization); low-fidelity evaluations and surrogate corrections are used to cheaply explore and refine surrogate where needed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of high-fidelity model evaluations (primary cost) and cost of low-fidelity evaluations; surrogate update costs are also considered but typically small.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly specified in the survey text for EGO, but selection is surrogate-driven (acquisition functions in surrogate optimization typically trade expected improvement or uncertainty reduction vs predicted objective).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition-driven selection on surrogate balances exploring uncertain regions and exploiting promising optima (survey references EGO as an example of adaptation where surrogate is updated each iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism described in survey; surrogate acquisition inherently samples both promising and uncertain regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed limit on number of expensive objective evaluations (implicit in surrogate optimization practice).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Adaptive selection via acquisition function to maximize expected utility per expensive evaluation; low-fidelity models can be used to reduce number of required high-fidelity evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not defined in survey; objective is finding global optimum (improvement over incumbent objective value).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Optimization progress per number of high-fidelity evaluations; survey mentions EGO as a canonical method but does not report numerical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>High-fidelity-only optimization (direct search), random sampling, other surrogate schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Survey does not give specific numeric comparisons; EGO is cited as effective at reducing high-fidelity evaluations via surrogate-driven acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Method is widely used to reduce number of expensive evaluations but no quantitative gains given in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Survey positions EGO as adaptation strategy that trades off surrogate construction cost and high-fidelity evaluations; explicit tradeoff formulas not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Principle: allocate high-fidelity evaluations to points with high expected utility as predicted by surrogate; multifidelity variants can use low-fidelity data to improve surrogate and hence allocation efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of multifidelity methods in uncertainty propagation, inference, and optimization', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2626.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2626.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cokriging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cokriging-based multifidelity surrogate (co-kriging)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fusion-based multifidelity surrogate modeling approach that jointly models outputs from multiple fidelity levels (e.g., low- and high-fidelity) to produce predictions and uncertainty estimates that leverage cross-correlation for improved accuracy and reduced high-fidelity evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cokriging (multifidelity Gaussian-process fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cokriging constructs a joint Gaussian-process model over multiple information sources, capturing cross-covariances between low- and high-fidelity outputs; it can incorporate gradient information and is used in optimization and design to predict high-fidelity outputs while exploiting abundant low-fidelity data. Model management is fusion: evaluate multiple models and combine outputs statistically.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Design optimization, surrogate modeling across engineering disciplines</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Combine low- and high-fidelity evaluations to build a surrogate whose predictive uncertainty guides allocation; leveraging abundant cheap low-fidelity data reduces required high-fidelity samples for a target predictive accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of evaluations per fidelity level weighted by per-evaluation cost; surrogate model fitting cost (GP training) also considered but usually secondary.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Predictive uncertainty reduction in the Gaussian-process posterior (variance) and improved predictive accuracy of high-fidelity outputs via cross-covariance exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition on the fused surrogate (e.g., expected improvement or uncertainty sampling) can balance exploration/exploitation; survey mentions cokriging often used when gradient info available but does not detail specific acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity promotion beyond surrogate-driven exploration of uncertain regions; fusion can capture multiple modes via GP structure if supported by data.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget in terms of expensive evaluations; surrogate aims to minimize high-fidelity cost for given predictive goal.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Fuse abundant low-fidelity data with sparse high-fidelity data to meet target accuracy while limiting high-fidelity evaluations; selection of high-fidelity points typically guided by surrogate uncertainty or optimization acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not explicitly defined; surrogate aims to accurately predict high-fidelity outputs or optimize objectives, not labeled 'breakthrough' in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Predictive error and uncertainty reduction of high-fidelity predictions; when used in optimization, improvement in objective per high-fidelity evaluation (survey cites comparative studies but provides no numeric summary).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single-fidelity kriging, high-fidelity-only modeling, other multifidelity surrogates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported in referenced studies to often outperform single-fidelity surrogates when cross-fidelity correlation is strong; survey notes comparisons in cited works but gives no numeric aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Depends on cross-correlation and cost ratios; can substantially reduce required high-fidelity evaluations to reach target accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Survey highlights that cross-correlation and cost ratios govern efficiency, but no single formula like MFMC is presented for cokriging allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Principle: invest in low-fidelity sampling to learn cross-covariance structure and allocate high-fidelity evaluations where fused surrogate indicates high value (uncertainty or expected improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of multifidelity methods in uncertainty propagation, inference, and optimization', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2626.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2626.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Filtering-threshold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Filtering with threshold-based hybrid indicator for failure probability (filtering model-management)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A filtering strategy that uses a low-fidelity model to decide when to invoke the high-fidelity model by thresholding a low-fidelity prediction, thereby reducing high-fidelity evaluations while bounding error in estimated failure probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Threshold-based multifidelity filter for failure indicator approximation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Define an approximated indicator hat{I}(z) that returns 1 if f_lo(z) < -gamma (low-fidelity confidently indicates failure), returns 0 if f_lo(z) > gamma (confidently safe), and otherwise evaluates f_hi(z) to decide. The threshold gamma is chosen based on low-fidelity error bounds (if known) or via iterative heuristics to control the error between E_p[hat{I}] and E_p[I_hi]. This approach reduces high-fidelity evaluations by avoiding them when the low-fidelity prediction is confident.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Failure probability estimation / limit-state evaluation in uncertainty quantification</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Use cheap low-fidelity evaluations to pre-classify inputs into confident regions (failure/safe) and allocate expensive high-fidelity evaluations only in the uncertain band near the limit state |f_lo(z)| <= gamma; gamma sets the frequency of high-fidelity sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Counts of low- vs high-fidelity evaluations weighted by per-evaluation cost; high-fidelity evaluations are minimized subject to error tolerance.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly an information-gain metric; the criterion is confidence relative to low-fidelity error (reducing expected classification error or bounding estimation error |E_p[hat{I}] - E_p[I_hi]|).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration (evaluate f_lo widely) to identify uncertain regions; exploitation (evaluate f_hi) focused on the uncertain band near failure boundary to refine classification.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism; emphasis is on targeted high-fidelity evaluation where low-fidelity is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Implicit control via threshold gamma that determines fraction of inputs requiring high-fidelity evaluation; can be tuned to meet computational resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Choose gamma according to known low-fidelity error or via heuristic iterations to keep estimation error below a desired epsilon while minimizing high-fidelity evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not applicable; objective is accurate failure probability estimation (small absolute error) and efficient limit-state evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Bound on error |E_p[hat{I}] - E_p[I_hi]| as function of gamma and low-fidelity error; frequency of high-fidelity evaluations determined by gamma. No fixed numeric examples in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Pure high-fidelity evaluation of indicator at all samples (no filtering), importance sampling or other multifidelity schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Potentially large reduction in high-fidelity evaluations when low-fidelity model is accurate away from the failure boundary; theoretical convergence results are available when low-fidelity error is known.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Problem-dependent; can reduce high-fidelity calls dramatically if low-fidelity strongly classifies most inputs confidently.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — tradeoff between selecting a larger gamma (fewer high-fidelity evaluations but larger classification error) and smaller gamma (more accurate estimate but more high-fidelity evaluations); gamma selection depends on low-fidelity error characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key insight: set gamma based on low-fidelity error bounds to guarantee that the approximation error in estimated failure probability is below a target; if error bounds unknown, use iterative heuristics that refine gamma and high-fidelity sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of multifidelity methods in uncertainty propagation, inference, and optimization', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2626.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2626.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLMC / multilevel collocation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multilevel Monte Carlo / Multilevel Stochastic Collocation (multilevel fusion methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multilevel methods use a hierarchy of models of increasing fidelity (e.g., coarse-to-fine discretizations) and fuse their outputs (telescoping sums or multilevel estimators) to reduce computational cost for statistics estimation by allocating more samples to cheaper coarse models and fewer to expensive fines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multilevel Monte Carlo (MLMC) and Multilevel Stochastic Collocation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs estimators via telescoping sums over a fidelity hierarchy: E[f_L] = E[f_0] + sum_{l=1}^L E[f_l - f_{l-1}], sampling each difference with a number of samples chosen according to variance and cost rates. Efficiency gains rely on variance decay of level differences and controlled cost growth, and sample allocation is derived from these rates. Multilevel stochastic collocation applies the same multilevel idea to deterministic quadrature (sparse grids) across levels.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Uncertainty propagation for PDE-based models, stochastic differential equations, high-dimensional quadrature</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate many samples to cheap coarse models and few to expensive fine models based on variance and cost decay rates; determine sample counts per level to minimize total cost for a target MSE using rate-based formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-level evaluation cost (often proportional to discretization size) and total cost as sum of samples times per-sample cost; asymptotic cost-rate analysis (e.g., cost ~ epsilon^{-2} under certain decay rates).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Variance reduction per cost; estimator MSE reduction via multilevel telescoping decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not framed as exploration/exploitation; allocation exploits known decay rates to concentrate samples where variance-per-cost is highest (coarse levels) while ensuring bias control via fine levels.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit hypothesis diversity mechanism; diversity of samples across levels is inherent in multilevel sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Target MSE accuracy or fixed computational budget; sample counts chosen to meet accuracy at minimal cost.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Use asymptotic rate relationships between variance decay and cost growth to derive near-optimal sample allocations per level (analytic formulas in MLMC literature); survey references these rate-based allocation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not applicable; objective is estimator MSE (accuracy) and computational cost reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Asymptotic computational complexity to reach MSE epsilon^2, e.g., cost scaling results depending on variance and cost rates; survey references successful application in SDEs and PDEs but gives no single numeric summary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard Monte Carlo or stochastic collocation on the finest level only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MLMC can reduce computational cost dramatically compared to single-level Monte Carlo when variance decay with level is faster than cost growth; specifics depend on rates and problem.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Potentially large (often orders of magnitude in SDE settings); survey cites extensive success in contexts with favorable rate properties.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — allocation depends critically on rates describing variance decay of level differences and cost increase with fidelity; if rates are unfavorable, MLMC benefits diminish.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Sample allocation per level derived from variance and cost rates to minimize total cost for target MSE; multilevel theory gives prescriptions (see cited MLMC literature) for optimal samples per level.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey of multifidelity methods in uncertainty propagation, inference, and optimization', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient Global Optimization of Expensive Black-Box Functions <em>(Rating: 2)</em></li>
                <li>Markov Chain Monte Carlo Using an Approximation <em>(Rating: 2)</em></li>
                <li>Multilevel Monte Carlo Path Simulation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2626",
    "paper_id": "paper-51b67d3cfb6ad8a22054d4aaa0092a1c76cf19ee",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "MFMC",
            "name_full": "Multifidelity Monte Carlo (control-variate based multifidelity estimator)",
            "brief_description": "A multifidelity Monte Carlo estimator that fuses a small number of high-fidelity evaluations with many low-fidelity evaluations as control variates and optimally allocates the evaluation budget to minimize MSE for a given computational budget.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Multifidelity Monte Carlo (MFMC) control-variate estimator",
            "system_description": "Constructs an unbiased estimator of an expectation of a high-fidelity model by combining a high-fidelity Monte Carlo estimate with a telescoping combination of low-fidelity Monte Carlo estimates weighted by control-variate coefficients. The method parameterizes per-model costs c_hi, c_lo^(i), variances sigma_hi^2, sigma_i^2, and Pearson correlations rho_i between models, and solves for optimal control coefficients alpha_i* and numbers of samples m_i* that minimize estimator MSE subject to a cost constraint gamma. Key closed-form results include alpha_i* = rho_i * (sigma_hi / sigma_i) and sample ratios m_i* proportional to r_i with r_i = sqrt( c_hi*(rho_i^2 - rho_{i+1}^2) / ( c_lo^(i)*(1 - rho_1^2) ) ).",
            "application_domain": "Uncertainty propagation / Monte Carlo estimation (general computational science and engineering models)",
            "resource_allocation_strategy": "Solve constrained optimization: minimize MSE (variance) of the multifidelity estimator subject to a fixed computational budget gamma and monotonicity constraints m0 &lt;= m1 &lt;= ... &lt;= mk. The solution yields analytic optimal control-variate coefficients and optimal sample counts per model (m0*, m1*, ..., mk*) as functions of model variances, pairwise correlations, and per-evaluation costs.",
            "computational_cost_metric": "Abstract per-evaluation costs c_hi and c_lo^(i) (positive real numbers), total cost is sum m_i * c_i; optimization enforces total cost equals budget gamma (units: cost units proportional to wall-time or FLOP equivalent as modeled by c).",
            "information_gain_metric": "Variance reduction / mean-squared error (MSE) reduction of the estimator (analytic expression for estimator variance as function of m_i and alpha_i).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Not framed as exploration/exploitation; tradeoff arises analytically between spending budget on many cheap, correlated low-fidelity evaluations (to reduce variance) versus fewer expensive high-fidelity evaluations that anchor unbiasedness; the allocation is determined by closed-form optimal ratios that depend on cost and correlation tradeoffs.",
            "diversity_mechanism": "None explicit — diversity of hypotheses is not an objective; the method instead leverages correlated low-fidelity evaluations to reduce estimator variance.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed computational budget (gamma) expressed in the same cost units as per-model costs.",
            "budget_constraint_handling": "Explicit constrained optimization: choose m_i and alpha_i to minimize variance subject to total cost = gamma and ordering constraints; yields analytic solution under stated assumptions.",
            "breakthrough_discovery_metric": "Not applicable — objective is statistical estimation accuracy (expectation/variance) rather than 'breakthrough' event discovery; performance measured by MSE/variance.",
            "performance_metrics": "Estimator mean-squared error (MSE) / variance; variance expression: V[hat{s}^MF] = sigma_hi^2 / m0 + sum_{i=1}^k (1/m_{i-1} - 1/m_i) (alpha_i^2 sigma_i^2 - 2 alpha_i rho_i sigma_hi sigma_i). Also variance-reduction ratio relative to high-fidelity-only MC: ( sqrt(1 - rho_1^2) + sum_{i=1}^k sqrt( (c_lo^(i) / c_hi) * (rho_i^2 - rho_{i+1}^2) ) )^2.",
            "comparison_baseline": "Benchmark Monte Carlo using only the high-fidelity model (same total cost gamma).",
            "performance_vs_baseline": "Analytical variance-reduction ratio given above; multifidelity estimator is cheaper (lower MSE for same budget) when inequality sqrt(1 - rho_1^2) + sum_{i=1}^k sqrt( (c_lo^(i)/c_hi) * (rho_i^2 - rho_{i+1}^2) ) &lt; 1 holds. No fixed numerical gains reported in survey (depends on correlations and costs).",
            "efficiency_gain": "Problem-dependent; when correlations are sufficiently high and low-fidelity costs sufficiently low the MFMC estimator yields substantial variance reduction for the same cost; no universal numeric factor reported in the survey (analytic condition provided).",
            "tradeoff_analysis": "Yes — explicit analytic tradeoff between model correlation (information content) and per-evaluation cost. The method shows that a low-fidelity model's contribution depends on both its correlation with the high-fidelity model and its cost relative to the high-fidelity model, and also on interactions with other models in the hierarchy (rho_{i}^2 - rho_{i+1}^2 terms).",
            "optimal_allocation_findings": "Closed-form optimal control coefficients alpha_i* = rho_i * (sigma_hi / sigma_i) and sample counts m0* = gamma / (c^T r), m_i* = r_i * m0* with r_i as above; ordering of models matters and should be chosen to minimize MSE.",
            "uuid": "e2626.0",
            "source_info": {
                "paper_title": "Survey of multifidelity methods in uncertainty propagation, inference, and optimization",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "MF-IS",
            "name_full": "Multifidelity Importance Sampling",
            "brief_description": "A two-stage multifidelity importance sampling approach that uses a cheap low-fidelity model to construct a biasing distribution (via density estimation on low-fidelity-predicted failure samples) and then evaluates the high-fidelity model under that biasing distribution to obtain unbiased estimates with reduced variance.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Multifidelity Importance Sampling (low-fidelity constructed biasing density)",
            "system_description": "Step 1: evaluate the low-fidelity model on a large sample of inputs and collect those predicted to satisfy an event (e.g., f_lo(z)&lt;0); fit a biasing density q (e.g., Gaussian mixture via EM) to those samples. Step 2: draw samples from q and evaluate the high-fidelity model; compute importance-sampling estimator with weights p/q to estimate the true probability. The approach is unbiased provided support(q) includes support(p) and aims to reduce estimator variance by focusing sampling on event-relevant regions identified cheaply by the low-fidelity model.",
            "application_domain": "Rare-event probability estimation / uncertainty propagation / failure probability estimation",
            "resource_allocation_strategy": "Allocate large budget to many cheap low-fidelity evaluations to learn an importance density q, then allocate remaining (expensive) budget to high-fidelity evaluations drawn from q; the split (n low-fidelity vs m high-fidelity) is chosen to make learning q feasible while keeping the final high-fidelity estimator unbiased and low-variance.",
            "computational_cost_metric": "Number of model evaluations weighted by per-evaluation costs; low-fidelity evaluations are cheap so n &gt;&gt; m is typical; no single unit specified beyond c_hi and c_lo.",
            "information_gain_metric": "Reduction in variance (MSE) of importance-sampling estimator; the biasing density aims to increase sampling density in regions that contribute most to the event probability (information about rare event).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration via dense low-fidelity sampling to discover candidate event regions; exploitation via focused high-fidelity evaluations drawn from the learned biasing distribution to refine probability estimate.",
            "diversity_mechanism": "Implicitly enforced if density estimator (e.g., Gaussian mixture) captures multiple modes of the low-fidelity-predicted event — no explicit diversity regularizer described.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed computational budget split between many cheap low-fidelity evaluations (to fit q) and fewer expensive high-fidelity evaluations (to compute weighted estimates).",
            "budget_constraint_handling": "Heuristic two-stage allocation: choose n large enough to fit a biasing density q from low-fidelity samples, then sample m high-fidelity draws from q; unbiasedness maintained by importance weights and support condition.",
            "breakthrough_discovery_metric": "Not defined as 'breakthrough' — objective is accurate estimation of rare-event probability (low MSE); success measured by variance reduction in probability estimator.",
            "performance_metrics": "Estimator unbiasedness and MSE = Var_q[ I_hi * (p/q) ] / m; practical savings depend on low-fidelity model accuracy in identifying event region and on quality of the fitted q; no universal numeric results in survey.",
            "comparison_baseline": "Standard Monte Carlo sampling from nominal density p (no importance sampling) and importance sampling with biasing densities built from high-fidelity evaluations only.",
            "performance_vs_baseline": "Can achieve large runtime savings in construction of biasing density if low-fidelity model accurately captures event regions; unbiasedness retained for high-fidelity estimator if support condition holds. Specific gains depend on low-fidelity fidelity and density estimation quality (no fixed numbers provided).",
            "efficiency_gain": "Potential major reductions in the number of high-fidelity samples needed to estimate rare-event probabilities when low-fidelity identifies event regions well; survey provides qualitative description and conditions for unbiasedness but no fixed numerical factors.",
            "tradeoff_analysis": "Yes — tradeoff between investing many cheap evaluations to learn q (risk of a poor q if low-fidelity is inaccurate) versus spending more expensive high-fidelity samples; importance-sampling variance depends critically on the quality of q.",
            "optimal_allocation_findings": "No single analytic allocation formula is given; key principle is to ensure the biasing density q covers the support of p and concentrates mass in regions where the true high-fidelity indicator is nonzero; quality of low-fidelity model in predicting event region governs achievable gains.",
            "uuid": "e2626.1",
            "source_info": {
                "paper_title": "Survey of multifidelity methods in uncertainty propagation, inference, and optimization",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Two-stage MCMC",
            "name_full": "Two-stage / Delayed Acceptance MCMC",
            "brief_description": "A multifidelity MCMC framework where candidate proposals are first screened using a cheap low-fidelity likelihood before the expensive high-fidelity likelihood is computed, reducing high-fidelity evaluations while preserving correct posterior sampling.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Two-stage (delayed acceptance) MCMC",
            "system_description": "At each proposed sample, compute an acceptance decision with a low-fidelity model-induced likelihood first; only if the proposal passes the low-fidelity acceptance test is the high-fidelity model evaluated and the final Metropolis-Hastings decision taken. This filtering reduces the number of expensive high-fidelity likelihood evaluations while maintaining correctness of the high-fidelity posterior under the two-stage acceptance scheme.",
            "application_domain": "Bayesian inverse problems / statistical inference (MCMC sampling)",
            "resource_allocation_strategy": "Cheap low-fidelity evaluations are used as a prefilter to avoid unnecessary high-fidelity evaluations; high-fidelity evaluations are allocated only to proposals that pass the low-fidelity check, thereby adaptively allocating expensive evaluations to promising proposals.",
            "computational_cost_metric": "Counts of model evaluations (number of high-fidelity likelihood evaluations saved) weighted by per-evaluation costs; high-fidelity cost is the primary expense to minimize.",
            "information_gain_metric": "Not explicitly framed as information gain; objective is to preserve correct posterior sampling while reducing costly evaluations — implicit information metric is acceptance probability under the true high-fidelity posterior.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Mechanism focuses on reducing wasted exploitation of poor proposals by cheaply screening them; exploration/exploitation tradeoff managed indirectly via MCMC proposal and two-stage acceptance, not via an explicit acquisition function.",
            "diversity_mechanism": "No explicit diversity promotion; diversity arises from the underlying MCMC proposal mechanism.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Computational cost minimization (reduce number of expensive model evaluations); no explicit fixed budget formulation in text but objective is efficiency of sampling.",
            "budget_constraint_handling": "Filtering strategy: low-fidelity acceptance reduces frequency of high-fidelity evaluation; can be combined with adaptation of the low-fidelity model to further reduce cost.",
            "breakthrough_discovery_metric": "Not applicable; goal is faithful posterior sampling (convergence diagnostics, effective sample size) rather than breakthrough detection.",
            "performance_metrics": "Reduction in number of high-fidelity model evaluations per effective sample; retention of correct posterior target distribution (theoretical correctness of delayed-acceptance scheme). No specific numerical results in the survey.",
            "comparison_baseline": "Standard single-fidelity MCMC that evaluates high-fidelity likelihood at each proposal.",
            "performance_vs_baseline": "Two-stage MCMC reduces the number of expensive evaluations while still sampling from the correct posterior; actual savings depend on low-fidelity acceptance rates and fidelity.",
            "efficiency_gain": "Problem-dependent; potential large savings when low-fidelity model reliably rejects many low-probability proposals at low cost.",
            "tradeoff_analysis": "Implicit: savings require low-fidelity to be a sufficiently good discriminator; if low-fidelity wrongly passes many bad proposals or rejects good ones incorrectly, efficiency and/or correctness can suffer unless corrected in the two-stage acceptance.",
            "optimal_allocation_findings": "Principle: design the low-fidelity filter to have high negative predictive value (reject bad proposals cheaply) while maintaining negligible bias via the high-fidelity second-stage acceptance; the survey notes two-stage formulations and adaptive variants.",
            "uuid": "e2626.2",
            "source_info": {
                "paper_title": "Survey of multifidelity methods in uncertainty propagation, inference, and optimization",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "EGO",
            "name_full": "Efficient Global Optimization (kriging-based surrogate optimization)",
            "brief_description": "A surrogate-based global optimization framework that adaptively builds a Gaussian-process (kriging) surrogate of an expensive objective and selects new evaluation points to improve the global optimum estimate, leveraging low-fidelity models in multifidelity variants.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Efficient Global Optimization (EGO) with kriging",
            "system_description": "Builds a kriging (Gaussian-process) surrogate of the expensive objective and iteratively selects new evaluation points based on an acquisition criterion (survey references its use as an adaptation strategy where the surrogate is updated each iteration). Multifidelity extensions adaptively construct or correct surrogates using low- and high-fidelity data (e.g., cokriging or hierarchical surrogates) to reduce the number of high-fidelity evaluations required to find the global optimum.",
            "application_domain": "Global optimization of expensive black-box objective functions (engineering design, simulation-based optimization)",
            "resource_allocation_strategy": "Allocate expensive high-fidelity evaluations to points chosen by surrogate-based acquisition (surrogate indicates high expected utility for optimization); low-fidelity evaluations and surrogate corrections are used to cheaply explore and refine surrogate where needed.",
            "computational_cost_metric": "Number of high-fidelity model evaluations (primary cost) and cost of low-fidelity evaluations; surrogate update costs are also considered but typically small.",
            "information_gain_metric": "Not explicitly specified in the survey text for EGO, but selection is surrogate-driven (acquisition functions in surrogate optimization typically trade expected improvement or uncertainty reduction vs predicted objective).",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Acquisition-driven selection on surrogate balances exploring uncertain regions and exploiting promising optima (survey references EGO as an example of adaptation where surrogate is updated each iteration).",
            "diversity_mechanism": "No explicit diversity mechanism described in survey; surrogate acquisition inherently samples both promising and uncertain regions.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed limit on number of expensive objective evaluations (implicit in surrogate optimization practice).",
            "budget_constraint_handling": "Adaptive selection via acquisition function to maximize expected utility per expensive evaluation; low-fidelity models can be used to reduce number of required high-fidelity evaluations.",
            "breakthrough_discovery_metric": "Not defined in survey; objective is finding global optimum (improvement over incumbent objective value).",
            "performance_metrics": "Optimization progress per number of high-fidelity evaluations; survey mentions EGO as a canonical method but does not report numerical metrics.",
            "comparison_baseline": "High-fidelity-only optimization (direct search), random sampling, other surrogate schemes.",
            "performance_vs_baseline": "Survey does not give specific numeric comparisons; EGO is cited as effective at reducing high-fidelity evaluations via surrogate-driven acquisition.",
            "efficiency_gain": "Method is widely used to reduce number of expensive evaluations but no quantitative gains given in survey.",
            "tradeoff_analysis": "Survey positions EGO as adaptation strategy that trades off surrogate construction cost and high-fidelity evaluations; explicit tradeoff formulas not provided in text.",
            "optimal_allocation_findings": "Principle: allocate high-fidelity evaluations to points with high expected utility as predicted by surrogate; multifidelity variants can use low-fidelity data to improve surrogate and hence allocation efficiency.",
            "uuid": "e2626.3",
            "source_info": {
                "paper_title": "Survey of multifidelity methods in uncertainty propagation, inference, and optimization",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Cokriging",
            "name_full": "Cokriging-based multifidelity surrogate (co-kriging)",
            "brief_description": "A fusion-based multifidelity surrogate modeling approach that jointly models outputs from multiple fidelity levels (e.g., low- and high-fidelity) to produce predictions and uncertainty estimates that leverage cross-correlation for improved accuracy and reduced high-fidelity evaluations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Cokriging (multifidelity Gaussian-process fusion)",
            "system_description": "Cokriging constructs a joint Gaussian-process model over multiple information sources, capturing cross-covariances between low- and high-fidelity outputs; it can incorporate gradient information and is used in optimization and design to predict high-fidelity outputs while exploiting abundant low-fidelity data. Model management is fusion: evaluate multiple models and combine outputs statistically.",
            "application_domain": "Design optimization, surrogate modeling across engineering disciplines",
            "resource_allocation_strategy": "Combine low- and high-fidelity evaluations to build a surrogate whose predictive uncertainty guides allocation; leveraging abundant cheap low-fidelity data reduces required high-fidelity samples for a target predictive accuracy.",
            "computational_cost_metric": "Number of evaluations per fidelity level weighted by per-evaluation cost; surrogate model fitting cost (GP training) also considered but usually secondary.",
            "information_gain_metric": "Predictive uncertainty reduction in the Gaussian-process posterior (variance) and improved predictive accuracy of high-fidelity outputs via cross-covariance exploitation.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition on the fused surrogate (e.g., expected improvement or uncertainty sampling) can balance exploration/exploitation; survey mentions cokriging often used when gradient info available but does not detail specific acquisition.",
            "diversity_mechanism": "No explicit diversity promotion beyond surrogate-driven exploration of uncertain regions; fusion can capture multiple modes via GP structure if supported by data.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed budget in terms of expensive evaluations; surrogate aims to minimize high-fidelity cost for given predictive goal.",
            "budget_constraint_handling": "Fuse abundant low-fidelity data with sparse high-fidelity data to meet target accuracy while limiting high-fidelity evaluations; selection of high-fidelity points typically guided by surrogate uncertainty or optimization acquisition.",
            "breakthrough_discovery_metric": "Not explicitly defined; surrogate aims to accurately predict high-fidelity outputs or optimize objectives, not labeled 'breakthrough' in survey.",
            "performance_metrics": "Predictive error and uncertainty reduction of high-fidelity predictions; when used in optimization, improvement in objective per high-fidelity evaluation (survey cites comparative studies but provides no numeric summary).",
            "comparison_baseline": "Single-fidelity kriging, high-fidelity-only modeling, other multifidelity surrogates.",
            "performance_vs_baseline": "Reported in referenced studies to often outperform single-fidelity surrogates when cross-fidelity correlation is strong; survey notes comparisons in cited works but gives no numeric aggregate.",
            "efficiency_gain": "Depends on cross-correlation and cost ratios; can substantially reduce required high-fidelity evaluations to reach target accuracy.",
            "tradeoff_analysis": "Survey highlights that cross-correlation and cost ratios govern efficiency, but no single formula like MFMC is presented for cokriging allocation.",
            "optimal_allocation_findings": "Principle: invest in low-fidelity sampling to learn cross-covariance structure and allocate high-fidelity evaluations where fused surrogate indicates high value (uncertainty or expected improvement).",
            "uuid": "e2626.4",
            "source_info": {
                "paper_title": "Survey of multifidelity methods in uncertainty propagation, inference, and optimization",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Filtering-threshold",
            "name_full": "Filtering with threshold-based hybrid indicator for failure probability (filtering model-management)",
            "brief_description": "A filtering strategy that uses a low-fidelity model to decide when to invoke the high-fidelity model by thresholding a low-fidelity prediction, thereby reducing high-fidelity evaluations while bounding error in estimated failure probability.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Threshold-based multifidelity filter for failure indicator approximation",
            "system_description": "Define an approximated indicator hat{I}(z) that returns 1 if f_lo(z) &lt; -gamma (low-fidelity confidently indicates failure), returns 0 if f_lo(z) &gt; gamma (confidently safe), and otherwise evaluates f_hi(z) to decide. The threshold gamma is chosen based on low-fidelity error bounds (if known) or via iterative heuristics to control the error between E_p[hat{I}] and E_p[I_hi]. This approach reduces high-fidelity evaluations by avoiding them when the low-fidelity prediction is confident.",
            "application_domain": "Failure probability estimation / limit-state evaluation in uncertainty quantification",
            "resource_allocation_strategy": "Use cheap low-fidelity evaluations to pre-classify inputs into confident regions (failure/safe) and allocate expensive high-fidelity evaluations only in the uncertain band near the limit state |f_lo(z)| &lt;= gamma; gamma sets the frequency of high-fidelity sampling.",
            "computational_cost_metric": "Counts of low- vs high-fidelity evaluations weighted by per-evaluation cost; high-fidelity evaluations are minimized subject to error tolerance.",
            "information_gain_metric": "Not explicitly an information-gain metric; the criterion is confidence relative to low-fidelity error (reducing expected classification error or bounding estimation error |E_p[hat{I}] - E_p[I_hi]|).",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploration (evaluate f_lo widely) to identify uncertain regions; exploitation (evaluate f_hi) focused on the uncertain band near failure boundary to refine classification.",
            "diversity_mechanism": "No explicit diversity mechanism; emphasis is on targeted high-fidelity evaluation where low-fidelity is ambiguous.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Implicit control via threshold gamma that determines fraction of inputs requiring high-fidelity evaluation; can be tuned to meet computational resource constraints.",
            "budget_constraint_handling": "Choose gamma according to known low-fidelity error or via heuristic iterations to keep estimation error below a desired epsilon while minimizing high-fidelity evaluations.",
            "breakthrough_discovery_metric": "Not applicable; objective is accurate failure probability estimation (small absolute error) and efficient limit-state evaluation.",
            "performance_metrics": "Bound on error |E_p[hat{I}] - E_p[I_hi]| as function of gamma and low-fidelity error; frequency of high-fidelity evaluations determined by gamma. No fixed numeric examples in survey.",
            "comparison_baseline": "Pure high-fidelity evaluation of indicator at all samples (no filtering), importance sampling or other multifidelity schemes.",
            "performance_vs_baseline": "Potentially large reduction in high-fidelity evaluations when low-fidelity model is accurate away from the failure boundary; theoretical convergence results are available when low-fidelity error is known.",
            "efficiency_gain": "Problem-dependent; can reduce high-fidelity calls dramatically if low-fidelity strongly classifies most inputs confidently.",
            "tradeoff_analysis": "Yes — tradeoff between selecting a larger gamma (fewer high-fidelity evaluations but larger classification error) and smaller gamma (more accurate estimate but more high-fidelity evaluations); gamma selection depends on low-fidelity error characteristics.",
            "optimal_allocation_findings": "Key insight: set gamma based on low-fidelity error bounds to guarantee that the approximation error in estimated failure probability is below a target; if error bounds unknown, use iterative heuristics that refine gamma and high-fidelity sampling.",
            "uuid": "e2626.5",
            "source_info": {
                "paper_title": "Survey of multifidelity methods in uncertainty propagation, inference, and optimization",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "MLMC / multilevel collocation",
            "name_full": "Multilevel Monte Carlo / Multilevel Stochastic Collocation (multilevel fusion methods)",
            "brief_description": "Multilevel methods use a hierarchy of models of increasing fidelity (e.g., coarse-to-fine discretizations) and fuse their outputs (telescoping sums or multilevel estimators) to reduce computational cost for statistics estimation by allocating more samples to cheaper coarse models and fewer to expensive fines.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Multilevel Monte Carlo (MLMC) and Multilevel Stochastic Collocation",
            "system_description": "Constructs estimators via telescoping sums over a fidelity hierarchy: E[f_L] = E[f_0] + sum_{l=1}^L E[f_l - f_{l-1}], sampling each difference with a number of samples chosen according to variance and cost rates. Efficiency gains rely on variance decay of level differences and controlled cost growth, and sample allocation is derived from these rates. Multilevel stochastic collocation applies the same multilevel idea to deterministic quadrature (sparse grids) across levels.",
            "application_domain": "Uncertainty propagation for PDE-based models, stochastic differential equations, high-dimensional quadrature",
            "resource_allocation_strategy": "Allocate many samples to cheap coarse models and few to expensive fine models based on variance and cost decay rates; determine sample counts per level to minimize total cost for a target MSE using rate-based formulas.",
            "computational_cost_metric": "Per-level evaluation cost (often proportional to discretization size) and total cost as sum of samples times per-sample cost; asymptotic cost-rate analysis (e.g., cost ~ epsilon^{-2} under certain decay rates).",
            "information_gain_metric": "Variance reduction per cost; estimator MSE reduction via multilevel telescoping decomposition.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Not framed as exploration/exploitation; allocation exploits known decay rates to concentrate samples where variance-per-cost is highest (coarse levels) while ensuring bias control via fine levels.",
            "diversity_mechanism": "No explicit hypothesis diversity mechanism; diversity of samples across levels is inherent in multilevel sampling.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Target MSE accuracy or fixed computational budget; sample counts chosen to meet accuracy at minimal cost.",
            "budget_constraint_handling": "Use asymptotic rate relationships between variance decay and cost growth to derive near-optimal sample allocations per level (analytic formulas in MLMC literature); survey references these rate-based allocation strategies.",
            "breakthrough_discovery_metric": "Not applicable; objective is estimator MSE (accuracy) and computational cost reduction.",
            "performance_metrics": "Asymptotic computational complexity to reach MSE epsilon^2, e.g., cost scaling results depending on variance and cost rates; survey references successful application in SDEs and PDEs but gives no single numeric summary.",
            "comparison_baseline": "Standard Monte Carlo or stochastic collocation on the finest level only.",
            "performance_vs_baseline": "MLMC can reduce computational cost dramatically compared to single-level Monte Carlo when variance decay with level is faster than cost growth; specifics depend on rates and problem.",
            "efficiency_gain": "Potentially large (often orders of magnitude in SDE settings); survey cites extensive success in contexts with favorable rate properties.",
            "tradeoff_analysis": "Yes — allocation depends critically on rates describing variance decay of level differences and cost increase with fidelity; if rates are unfavorable, MLMC benefits diminish.",
            "optimal_allocation_findings": "Sample allocation per level derived from variance and cost rates to minimize total cost for target MSE; multilevel theory gives prescriptions (see cited MLMC literature) for optimal samples per level.",
            "uuid": "e2626.6",
            "source_info": {
                "paper_title": "Survey of multifidelity methods in uncertainty propagation, inference, and optimization",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient Global Optimization of Expensive Black-Box Functions",
            "rating": 2
        },
        {
            "paper_title": "Markov Chain Monte Carlo Using an Approximation",
            "rating": 2
        },
        {
            "paper_title": "Multilevel Monte Carlo Path Simulation",
            "rating": 2
        }
    ],
    "cost": 0.023153999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Survey of Multifidelity Methods in Uncertainty Propagation, Inference, and Optimization*</h1>
<p>Benjamin Peherstorfer ${ }^{\dagger}$<br>Karen Willcox ${ }^{\ddagger}$<br>Max Gunzburger ${ }^{\S}$</p>
<h4>Abstract</h4>
<p>In many situations across computational science and engineering, multiple computational models are available that describe a system of interest. These different models have varying evaluation costs and varying fidelities. Typically, a computationally expensive highfidelity model describes the system with the accuracy required by the current application at hand, while lower-fidelity models are less accurate but computationally cheaper than the high-fidelity model. Outer-loop applications, such as optimization, inference, and uncertainty quantification, require multiple model evaluations at many different inputs, which often leads to computational demands that exceed available resources if only the high-fidelity model is used. This work surveys multifidelity methods that accelerate the solution of outer-loop applications by combining high-fidelity and low-fidelity model evaluations, where the low-fidelity evaluations arise from an explicit low-fidelity model (e.g., a simplified physics approximation, a reduced model, a data-fit surrogate) that approximates the same output quantity as the high-fidelity model. The overall premise of these multifidelity methods is that low-fidelity models are leveraged for speedup while the highfidelity model is kept in the loop to establish accuracy and/or convergence guarantees. We categorize multifidelity methods according to three classes of strategies: adaptation, fusion, and filtering. The paper reviews multifidelity methods in the outer-loop contexts of uncertainty propagation, inference, and optimization.</p>
<p>Key words. multifidelity, surrogate models, model reduction, multifidelity uncertainty quantification, multifidelity uncertainty propagation, multifidelity statistical inference, multifidelity optimization</p>
<p>AMS subject classifications. 65-02, 62-02, 49-02
DOI. $10.1137 / 16 \mathrm{M} 1082469$</p>
<h2>Contents</h2>
<h2>I Introduction</h2>
<p>*Received by the editors June 20, 2016; accepted for publication (in revised form) September 13, 2017; published electronically August 8, 2018.
http://www.siam.org/journals/sirev/60-3/M108246.html
Funding: The first two authors acknowledge support of the AFOSR MURI on multiinformation sources of multiphysics systems under award FA9550-15-1-0038, the U.S. Department of Energy Applied Mathematics Program, awards DE-FG02-08ER2585 and DE-SC0009297, as part of the DiaMonD Multifaceted Mathematics Integrated Capability Center, DARPA EQUiPS award UTA15001067, and the MIT-SUTD International Design Center. The third author was supported by U.S. Department of Energy Office of Science grant DE-SC0009324 and U.S. Air Force Office of Research grant FA9550-15-1-0001.
${ }^{\dagger}$ Department of Mechanical Engineering and Wisconsin Institute for Discovery, University of Wisconsin-Madison, Madison, WI 53706 (peherstorfer@wisc.edu).
${ }^{\ddagger}$ Department of Aeronautics \&amp; Astronautics, MIT, Cambridge, MA 02139 (kwillcox@mit.edu).
${ }^{\S}$ Department of Scientific Computing, Florida State University, Tallahassee, FL 32306-4120 (gunzburg@fsu.edu).</p>
<p>1.1 Multifidelity Models ..... 551
1.2 Multifidelity Methods for the Outer Loop ..... 552
1.3 Types of Low-Fidelity Models ..... 556
1.4 Outer-Loop Applications ..... 558
1.5 Outline of the Paper ..... 559
2 Multifidelity Model Management Strategies ..... 559
2.1 Adaptation ..... 559
2.2 Fusion ..... 559
2.3 Filtering ..... 560
3 Multifidelity Model Management in Uncertainty Propagation ..... 560
3.1 Uncertainty Propagation and Monte Carlo Simulation ..... 560
3.2 Multifidelity Uncertainty Propagation Based on Control Variates ..... 561
3.3 Multifidelity Uncertainty Propagation Based on Importance Sampling ..... 566
3.4 Other Model Management Strategies for Probability Estimation and Limit State Function Evaluation ..... 568
3.5 Stochastic Collocation and Multifidelity ..... 569
4 Multifidelity Model Management in Statistical Inference ..... 570
4.1 Bayesian Framework for Inference ..... 570
4.2 Two-Stage Markov Chain Monte Carlo ..... 572
4.3 Markov Chain Monte Carlo with Adaptive Low-Fidelity Models ..... 573
4.4 Bayesian Estimation of Low-Fidelity Model Error ..... 575
5 Multifidelity Model Management in Optimization ..... 576
5.1 Optimization Using a Single High-Fidelity Model ..... 576
5.2 Global Multifidelity Optimization ..... 576
5.3 Local Multifidelity Optimization ..... 578
5.4 Multifidelity Optimization under Uncertainty ..... 580
6 Conclusions and Outlook ..... 580
References ..... 582
I. Introduction. We begin by introducing the setting and concepts surveyed in this paper: Section 1.1 defines the setting of multifidelity models, and section 1.2 introduces the concepts of multifidelity methods. Section 1.3 discusses different types of low-fidelity models that may arise in the multifidelity setting. Section 1.4 defines the three outer-loop applications of interest: uncertainty propagation, statistical inference, and optimization. Section 1.5 outlines the remainder of the paper.
I.I. Multifidelity Models. Models serve to support many aspects of computational science and engineering, from discovery to design to decision-making and more. In some of these settings, one primary purpose of a model is to characterize the inputoutput relationship of the system of interest - the input describes the relevant system properties and environmental conditions, and the output describes quantities of interest to the task at hand. In this context, evaluating a model means performing a numerical simulation that implements the model, computes a solution, and thus maps an input onto an approximation of the output. For example, the numerical simulation</p>
<p>might involve solving a partial differential equation (PDE), solving a system of ordinary differential equations, or applying a particle method. Mathematically, we denote a model as a function $f: \mathcal{Z} \rightarrow \mathcal{Y}$ that maps an input $\boldsymbol{z} \in \mathcal{Z}$ to an output $\boldsymbol{y} \in \mathcal{Y}$, where $\mathcal{Z} \subseteq \mathbb{R}^{d}$ is the domain of the inputs of the model, with dimension $d \in \mathbb{N}$, and $\mathcal{Y} \subseteq \mathbb{R}^{d^{\prime}}$ is the domain of the outputs of the model, with dimension $d^{\prime} \in \mathbb{N}$. Model evaluations (i.e., evaluations of $f$ ) incur computational costs $c \in \mathbb{R}<em _="+">{+}$that typically increase with the accuracy of the approximation of the output, where $\mathbb{R}</em>: x&gt;0}$ is the set of positive real numbers.}={x \in \mathbb{R</p>
<p>In many situations, multiple models are available that estimate the same output quantity with varying approximation qualities and varying computational costs. We define a high-fidelity model $f_{\mathrm{hi}}: \mathcal{Z} \rightarrow \mathcal{Y}$ as a model that estimates the output with the accuracy that is necessary for the task at hand. We define a low-fidelity model $f_{\mathrm{lo}}: \mathcal{Z} \rightarrow \mathcal{Y}$ as a model that estimates the same output with a lower accuracy than the high-fidelity model. The costs $c_{\mathrm{hi}} \in \mathbb{R}<em _mathrm_hi="\mathrm{hi">{+}$of the high-fidelity model $f</em>}}$ are typically higher than the costs $c_{\mathrm{lo}} \in \mathbb{R<em _mathrm_lo="\mathrm{lo">{+}$of a low-fidelity model $f</em>$.
I.2. Multifidelity Methods for the Outer Loop. The use of principled approximations to accelerate computational tasks has long been a mainstay of scalable numerical algorithms. For example, quasi-Newton optimization methods [57, 69, 31] construct approximations of Hessians and apply low-rank updates to these approximations during the Newton iterations. Solvers based on Krylov subspace methods [121, 9, 122, 184] and on Anderson relaxation [5, 211, 202] perform intermediate computations in low-dimensional subspaces that are updated as the computation proceeds. Whereas these methods-and many others across the broad field of numerical algorithms-embed principled approximations within a numerical solver, we focus in this paper on the particular class of multifidelity methods that invoke explicit approximate models in solution of an outer-loop problem. We define this class of methods more precisely below; first we introduce the notion of an outer-loop application problem.}}$. More generally, we consider $k \in \mathbb{N}$ low-fidelity models, $f_{\mathrm{lo}}^{(1)}, \ldots, f_{\mathrm{lo}}^{(k)}$, that each represent the relationship between the input and the output, $f_{\mathrm{lo}}^{(i)}: \mathcal{Z} \rightarrow \mathcal{Y}, i=1, \ldots, k$, and we denote the cost of evaluating model $f_{\mathrm{lo}}^{(i)}$ as $c_{\mathrm{lo}}^{(i)</p>
<p>We use the term outer-loop application to define computational applications that form outer loops around a model-where in each iteration an input $\boldsymbol{z} \in \mathcal{Z}$ is received and the corresponding model output $f(\boldsymbol{z})$ is computed, and an overall outer-loop result is obtained at the termination of the outer loop. For example, in optimization, the optimizer provides at each iteration the design variables to evaluate (the input) and the model must evaluate the corresponding objective function value, the constraint values, and possibly gradient information (the outputs). At termination, an optimal design is obtained (the outer-loop result). Another outer-loop application is uncertainty propagation, which can be thought of conceptually as a loop over realizations of the input, requiring a corresponding model evaluation for each realization. In uncertainty propagation, the outer-loop result is the estimate of the statistics of interest. Other examples of outer-loop applications are inverse problems, data assimilation, control problems, and sensitivity analysis. ${ }^{1}$ Note that although it is helpful</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>for the exposition to think of outer-loop applications as loops, they are often not implemented as such. For example, in uncertainty propagation, once the realizations of the input have been drawn, the model outputs can typically be computed in parallel.</p>
<p>The term many-query application is often used to denote applications that evaluate a model many times [182], a categorization that applies to most (if not all) outer-loop applications. We distinguish between many-query and outer-loop applications by considering the latter to be the class of applications that target a specific outer-loop result. In contrast, many-query applications do not necessarily target a specific outer-loop result (and thus the set of outer-loop applications is essentially a subset of the set of many-query applications). For example, performing a parameter study is many-query but does not necessarily lead to a specific outer-loop result. This distinction is important in the discussion of multifidelity methods, since accuracy and/or convergence will be assessed relative to a specific outer-loop result.</p>
<p>The accuracy of the outer-loop result, as required by the problem at hand, can be achieved by using the high-fidelity model $f_{\mathrm{hi}}$ in each iteration of the outer loop; however, evaluating the high-fidelity model in each iteration often leads to computational demands that exceed available resources. Simply replacing the high-fidelity model $f_{\mathrm{hi}}$ with a low-fidelity model $f_{\mathrm{lo}}$ can result in significant speedups but leads to a lower-and typically unknown-approximation quality of the outer-loop result. This is clearly unsatisfactory and motivates the need for multifidelity methods.</p>
<p>We survey here multifidelity methods for outer-loop applications. We consider the class of multifidelity methods that have two key properties: (1) They leverage a lowfidelity model $f_{\mathrm{lo}}$ (or in the general case multiple low-fidelity models $f_{\mathrm{lo}}^{(1)}, \ldots, f_{\mathrm{lo}}^{(k)}, k \in$ $\mathbb{N}$ ), to obtain computational speedups, and (2) they use recourse to the high-fidelity model $f_{\mathrm{hi}}$ to establish accuracy and/or convergence guarantees on the outer-loop result; see Figure 1. Thus, multifidelity methods use low-fidelity models to reduce the runtime where possible, but recourse to the high-fidelity model to preserve the accuracy of the outer-loop result that would be obtained with a method that uses only the high-fidelity model. The two key ingredients of multifidelity methods are (1) low-fidelity models $f_{\mathrm{lo}}^{(1)}, \ldots, f_{\mathrm{lo}}^{(k)}$, that provide useful approximations of the highfidelity model $f_{\mathrm{hi}}$, and (2) a model management strategy that distributes work among the models while providing theoretical guarantees that establish the accuracy and/or convergence of the outer-loop result.</p>
<p>Note that a crucial component of this characterization of multifidelity methods for outer-loop problems is the use of explicit low-fidelity models that approximate the same output quantity as the high-fidelity model. This distinguishes the methods from those that embed approximations within the solver itself, such as the quasi-Newton and Krylov subspace methods discussed above.</p>
<p>The multifidelity methods we survey apply to a broad range of problems, but of particular interest is the setting of a high-fidelity model that corresponds to a fine-grid discretization of a PDE that governs the system of interest. In this setting, coarse-grid approximations have long been used as cheaper approximations. Varying the discretization parameters generates a hierarchy of low-fidelity models. We are interested here in richer and more heterogeneous sets of models, including projectionbased reduced models [191, 182, 87, 19], data-fit interpolation and regression models [72, 70], machine-learning-based models such as support vector machines (SVMs) [207, 49, 38], and other simplified models [132, 151]; see Figure 2. We further discuss types of low-fidelity models in section 1.3. In a broader sense, we can think of the models as information sources that describe the input-output relationships of the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) single-fidelity approach with high-fidelity model
(b) single-fidelity approach with low-fidelity model
(c) multifidelity approach with high-fidelity model and multiple low-fidelity models</p>
<p>Fig. I Multifidelity methods combine the high-fidelity model with low-fidelity models. The lowfidelity models are leveraged for speedup, and the high-fidelity model is kept in the loop to establish accuracy and/or convergence guarantees on the outer-loop result.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2 In many situations, different types of low-fidelity models are available, e.g., coarse-grid approximations, projection-based reduced models, data-fit interpolation and regression models, machine-learning-based models, and simplified models. The low-fidelity models vary with respect to error and costs. Multifidelity methods leverage these heterogeneous types of lowfidelity models for speedup.
system of interest. In that broader sense, expert opinions, experimental data, and historical data are potential information sources. We restrict the following discussion to models, because all of the multifidelity methods that we survey are developed in the context of models; however, we note that many of these multifidelity methods could potentially be extended to this broader class of information sources.</p>
<p>Model management serves two purposes. First is to balance model evaluations among the models (i.e., to decide which model to evaluate when). Second is to guarantee the same accuracy in the outer-loop result as if only the high-fidelity model were used. We distinguish between three types of model management strategies (see Figure 3): (1) adapting the low-fidelity model with information from the high-fidelity</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3 We distinguish between three model management strategies: adaptation, fusion, and filtering.
model, (2) fusing low- and high-fidelity model outputs, and (3) filtering to use the high-fidelity model only when indicated by a low-fidelity filter. ${ }^{2}$ The appropriate model management strategy for the task at hand typically depends on the nature of the outer-loop application. We survey model management techniques that fall into these three categories in section 2.
I.2.I. Comparison to Multilevel Methods. Multilevel methods have a long history in computational science and engineering, e.g., multigrid methods [28, 93, 30, 142, 204], multilevel preconditioners [27, 55], and multilevel function representations $[216,14,56,32]$. Multilevel methods typically derive a hierarchy of low-fidelity models of the high-fidelity model by varying a parameter. For example, the parameter could be the mesh width, and thus the hierarchy of low-fidelity models would be the hierarchy of coarse-grid approximations. A common approach in multilevel methods is to describe the approximation quality and the costs of the low-fidelity model hierarchy with rates and then to use these rates to distribute work among the models. In this paper, we consider more-general low-fidelity models with properties that cannot necessarily be well described by rates. Even though many multilevel methods apply to more heterogeneous models than coarse-grid approximations, describing the model properties by rates only, and consequently distributing work with respect to rates, can be too coarse a description and can miss important aspects of the models. Furthermore, in our setting, low-fidelity models are often given and cannot be easily generated on request by varying a parameter (e.g., discretization). The multifidelity techniques that we describe here explicitly take such richer sets of models into account.
I.2.2. Comparison to Traditional Model Reduction. Traditionally, model reduction $[7,182,19]$ first constructs a low-fidelity reduced model and then replaces the high-fidelity model with the reduced model in an outer-loop application. Replacing the high-fidelity model often leads to significant speedups, but it also means that the accuracy of the outer-loop result depends on the accuracy of the reduced model. In some settings, error bounds or error estimates are available for the reduced-model outputs [182, 209, 86], and it may be possible to translate these error estimates on the model outputs into error estimates on the outer-loop result. In contrast, multifi-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4 We categorize low-fidelity models as being of three types: simplified models, projection-based models, and data-fit models.
delity methods establish accuracy and convergence guarantees - instead of providing error bounds and error estimates only - by keeping the high-fidelity model in the loop and thus trading some speedup for guarantees - even if the quality of the low-fidelity model is unknown.
I.3. Types of Low-Fidelity Models. We categorize low-fidelity models as being of three types: simplified low-fidelity models, projection-based low-fidelity models, and data-fit low-fidelity models. Figure 4 depicts this categorization. For a given application, knowledge of and access to the high-fidelity model affect what kind of low-fidelity models can be created. In some cases, the high-fidelity system has a known structure that can be exploited to create low-fidelity models. In other cases, the high-fidelity models are considered to be "black box": they can be evaluated at the inputs in $\mathcal{Z}$ to obtain outputs in $\mathcal{Y}$, but no details are available on how the outputs are computed.
I.3.I. Simplified Low-Fidelity Models. Simplified models are derived from the high-fidelity model by taking advantage of domain expertise and in-depth knowledge of the implementation details of the high-fidelity model. Domain expertise allows the derivation of several models with different computational costs and fidelities that all aim to estimate the same output of interest of the system. For example, in computational fluid dynamics, there is a clear hierarchy of models for analyzing turbulent flow. From high to low fidelity, these are direct numerical simulations (DNS), large eddy simulations (LES), and Reynolds averaged Navier-Stokes (RANS). All of these model turbulent flows, but DNS resolves the whole spatial and time domain to the scale of the turbulence, LES eliminates small-scale behavior, and RANS applies the Reynolds decomposition to average over time. In aerodynamic design, an often-employed hierarchy comprises RANS, Euler equations, and potential theory [97]. The supersonic aerodynamic design problem in $[43,42]$ employs the Euler equations, a vortex lattice model, and a classical empirical model. Similar hierarchies of models exist in other fields of engineering. Models for subsurface flows through karst aquifers reach from simple continuum pipe flow models [36] to coupled Stokes and Darcy systems [35]. In climate modeling, low-fidelity models consider only a limited number of atmospheric effects, whereas high-fidelity models are fully coupled atmospheric and oceanic simulation models [99, 132]. There are more general concepts to derive low-fidelity models by simplification, that also require domain expertise but that are applicable across disciplines. Coarse-grid discretizations are an important class of such approxima-</p>
<p>tions. As another example, in many settings a low-fidelity model can be derived by neglecting nonlinear terms. For example, lower-fidelity linearized models are common in aerodynamic and structural analyses [166]. Yet another example is that when the high-fidelity model relies on an iterative solver (e.g., Krylov subspace solvers or Newton's method), a low-fidelity model can be derived by loosening the residual tolerances of the iterative method-thus, to derive a low-fidelity approximation, the iterative solver is stopped earlier than if a high-fidelity output were computed.
I.3.2. Projection-Based Low-Fidelity Models. Model reduction derives lowfidelity models from a high-fidelity model by mathematically exploiting the problem structure rather than using domain knowledge of the problem at hand. These methods proceed by identifying a low-dimensional subspace that is constructed to retain the essential character of the system input-output map. Projecting the governing equations onto the low-dimensional subspace yields the reduced model. The projection step is typically (but not always) intrusive and requires knowledge of the highfidelity model structure. There are a variety of ways to construct the low-dimensional subspace; see [19] for a detailed review. One common method is proper orthogonal decomposition (POD) [191, 21, 175, 120, 119], which uses so-called snapshots-state vectors of the high-fidelity model at selected inputs-to construct a basis for the low-dimensional subspace. POD is a popular basis generation method because it applies to a wide range of problems, including time-dependent and nonlinear problems [39, 91, 119, 120, 186]. Another basis generation approach is based on centroidal Voronoi tessellation (CVT) [60], where a special Voronoi clustering of the snapshots is constructed. The reduced basis is then derived from the generators of the Voronoi clustering. The work [33] discusses details of CVT-based basis construction. A combination of POD- and CVT-based basis construction is introduced in [61]. There are also methods based on Krylov subspaces to generate a reduced basis [67, 75], including multivariate Padé approximations and tangential interpolation for linear systems [13, 17, 74, 87]. Dynamic mode decomposition is another basis generation method that is popular in the context of computational fluid dynamics [188, 205, 171]. Balanced truncation $[145,146]$ is a common basis construction method used in the systems and control theory community. For stable linear time-invariant systems, balanced truncation provides a basis that guarantees asymptotically stable reduced systems and provides an error bound [7, 88]. Another basis generation approach is the reducedbasis method [182, 86, 183, 181], where orthogonalized carefully selected snapshots are the basis vectors. Depending on the problem of interest, these reduced-basis models can be equipped with cheap a posteriori error estimators for the reduced-model outputs [107, 86, 92, 182, 206, 215]. Efficient error estimators can also sometimes be provided for other basis generation methods, such as POD [102, 103].
I.3.3. Data-Fit Low-Fidelity Models. Data-fit low-fidelity models are derived directly from inputs and the corresponding outputs of the high-fidelity model. Thus, data-fit models can be derived from black-box high-fidelity models because only the inputs and outputs of the high-fidelity model need to be available. In many cases, data-fit models are represented as linear combinations of basis functions. Data-fit models are constructed by fitting the coefficients of the linear combination via interpolation or regression to the inputs and the corresponding high-fidelity model outputs. The choice of the interpolation and regression bases is critical for the approximation quality of the data-fit models. Polynomials, e.g., Lagrange polynomials, are classical basis functions that can be used to derive data-fit models. Piecewise-polynomial interpolation approaches allow the use of low-degree polynomials, which avoids problems</p>
<p>with global polynomial interpolation of high degree, e.g., Runge's phenomenon. If the inputs are low dimensional, a multivariate data-fit model can be derived with tensor product approaches. In higher-dimensional settings, discretization methods based on sparse grids [32] can be employed. Radial basis functions are another type of basis functions that are widely used for constructing data-fit models [174, 70]. If based on the Gaussian density function, radial basis functions typically lead to numerically stable computations of coefficients of the linear combination of the data-fit model. The radial basis functions often depend on hyper-parameters, e.g., the bandwidth of the Gaussian density function. Well-chosen hyper-parameters can greatly improve the approximation accuracy of the data-fit model, but the optimization for these hyper-parameters is often computationally expensive [70]. A widely used approach to interpolation with radial basis functions is kriging, for which a sound theoretical understanding has been obtained and efficient approaches to optimize for the hyperparameters have been developed [141, 185, 112, 138, 174, 72]. In particular, kriging models are equipped with error indicators; see, e.g., [185]. There are also SVMs [207, 49, 23, 187], which have been developed by the machine-learning community for classification tasks but are now used as surrogates in science and engineering as well; see, e.g., $[70,16,59,164]$.
I.4. Outer-Loop Applications. We focus on three outer-loop applications for which a range of multifidelity methods exist: uncertainty propagation, statistical inference, and optimization.
I.4.I. Uncertainty Propagation. In uncertainty propagation, the model input is described by a random variable and we are interested in statistics of the model output. Using Monte Carlo simulation to estimate statistics of the model output often requires a large number of model evaluations to achieve accurate approximations of the statistics. A multifidelity method that combines outputs from computationally cheap low-fidelity models with outputs from the high-fidelity model can lead to significant reductions in runtime and provide unbiased estimators of the statistics of the high-fidelity model outputs [76, 150, 148, 198, 160]. Note that we consider probabilistic approaches to uncertainty propagation only; other approaches to uncertainty propagation are, e.g., fuzzy set approaches [22] and worst-case scenario analysis [12].
I.4.2. Statistical Inference. In inverse problems, an indirect observation of a quantity of interest is given. A classical example is that limited and noisy observations of a system output are given and one wishes to estimate the input of the system. In statistical inference [195, 194, 110, 192], the unknown input is modeled as a random variable and we are interested in sampling the distribution of this random variable to assess the uncertainty associated with the input estimation. Markov chain Monte Carlo (MCMC) methods are one way to sample the distribution of the input random variable. MCMC is an outer-loop application that requires evaluating the high-fidelity model many times. Multifidelity methods in MCMC typically use multistage adaptive delayed acceptance formulations that leverage low-fidelity models to speed up the sampling $[44,62,51,54]$.
I.4.3. Optimization. The goal of optimization is to find an input that leads to an optimal model output with respect to a given objective function. Optimization is typically solved using an iterative process that requires evaluations of the model in each iteration. Multifidelity optimization reduces the runtime of the optimization process by using low-fidelity models to accelerate the search $[24,112,71,70]$ or by using a low-fidelity model in conjunction with adaptive corrections and a trust-region</p>
<p>model management scheme $[1,2,24,135,172]$. Other multifidelity optimization methods build a surrogate using evaluations from multiple models and then optimize using this surrogate. For example, efficient global optimization (EGO) is a multifidelity optimization method that adaptively constructs a low-fidelity model by interpolating the objective function corresponding to the high-fidelity model with Gaussian process regression (kriging) [109].
I.5. Outline of the Paper. The remainder of this paper focuses on model management strategies. Section 2 overviews the model management strategies of adaptation, fusion, and filtering. Sections 3-5 survey specific techniques in the context of uncertainty propagation, inference, and optimization, respectively. The outlook in section 6 closes the survey.
2. Multifidelity Model Management Strategies. Model management in multifidelity methods defines how different models are employed during execution of the outer loop and how outputs from different models are combined. Models are managed such that low-fidelity models are leveraged for speedup, while judicious evaluations of the high-fidelity model establish accuracy and/or convergence of the outer-loop result. This section describes a categorization of model management methods into three types of strategies. The following sections then survey specific model management methods in the context of uncertainty propagation, statistical inference, and optimization.</p>
<p>As shown in Figure 3 (above), we distinguish between three types of model management strategies: adaptation, fusion, and filtering.
2.I. Adaptation. The first model management strategy uses adaptation to enhance the low-fidelity model with information from the high-fidelity model while the computation proceeds. One example of model management based on adaptation is global optimization with EGO, where a kriging model is adapted in each iteration of the optimization process [109, 208]. Another example is the correction of lowfidelity model outputs via updates, which are derived from the high-fidelity model. It is common to use additive updates, which define the correction based on the difference between sampled high-fidelity and low-fidelity outputs, and/or multiplicative updates, which define the correction based on the ratio between sampled high-fidelity and low-fidelity outputs [1, 2]. The correction model is then typically built using Taylor series expansion based on gradients and possibly also on higher-order derivative information [63]. In [113], low-fidelity models are corrected (calibrated) with Gaussian process models to best predict the output of the high-fidelity model. Another multifidelity adaptation strategy is via adaptive model reduction, where projection-based reduced models are efficiently adapted as more data of the high-fidelity model become available during solution of the outer-loop application problem. Key to online adaptive model reduction is an efficient adaptation process. In [162, 163], the basis and operators of projection-based reduced models are adapted with low-rank updates. In [37], an $h$-adaptive refinement of the basis vectors uses clustering algorithms to learn and adapt a reduced basis from high-fidelity model residuals. The work [4] adapts localized reduced bases to smooth the transition from one localized reduced basis to another localized basis.
2.2. Fusion. The second model management strategy is based on information fusion. Approaches based on fusion evaluate low- and high-fidelity models and then combine information from all outputs. An example from uncertainty propagation is the control variate framework [29, 96, 149], where the variance of Monte Carlo</p>
<p>estimators is reduced by exploiting the correlation between high- and low-fidelity models. The control variate framework leverages a small number of high-fidelity model evaluations to obtain unbiased estimators of the statistics of interest, together with a large number of low-fidelity model evaluations to obtain an estimator with a low variance. Another example from uncertainty propagation is the fusion framework introduced in [118], which is based on Bayesian regression.</p>
<p>Cokriging is another example of a multifidelity method that uses model management based on fusion. Cokriging derives a model from multiple information sources, e.g., a low- and a high-fidelity model $[6,147,165]$. Cokriging is often used in the context of optimization if gradient information of the high-fidelity model is available; see [71]. The work [123] compares kriging and cokriging models on aerodynamic test functions. In [214], gradients are computed cheaply with the adjoint method and then used to derive a cokriging model for design optimization in large design spaces. In [97], cokriging with gradients and further developments of cokriging are compared for approximating aerodynamic models of airfoils.
2.3. Filtering. The third model management strategy is based on filtering, where the high-fidelity model is invoked following the evaluation of a low-fidelity filter. This might entail evaluating the high-fidelity model only if the low-fidelity model is deemed inaccurate, or it might entail evaluating the high-fidelity model only if the candidate point meets some criterion based on the low-fidelity evaluation. One example of a multifidelity filtering strategy is a multistage MCMC algorithm. For example, in twostage MCMC [44, 73], a candidate sample first needs to be accepted by the likelihood induced by the low-fidelity model before the high-fidelity model is evaluated at the candidate sample. As another example, in the multifidelity stochastic collocation approach in [148], the stochastic space is explored with the low-fidelity model to derive sampling points at which the high-fidelity model is then evaluated. A third example is multifidelity importance sampling, where the sampling of the high-fidelity model is guided by an importance sampling biasing distribution that is constructed with a low-fidelity model [160].
3. Multifidelity Model Management in Uncertainty Propagation. Inputs of models are often formulated as random variables to describe the stochasticity of the system of interest. With random inputs, the output of the model becomes a random variable as well. Uncertainty propagation aims to estimate statistics of the output random variable [140]. Sampling-based methods for uncertainty propagation evaluate the model at a large number of inputs and then estimate statistics from the corresponding model outputs. Examples of sampling-based methods are Monte Carlo simulation and stochastic collocation approaches. In this section, we review multifidelity approaches for sampling-based methods in uncertainty propagation. These multifidelity approaches shift many of the model evaluations to low-fidelity models while evaluating the high-fidelity model a small number of times to establish unbiased estimators. Section 3.1 introduces the problem setup and briefly overviews the Monte Carlo simulation method. Sections 3.2-3.4 discuss multifidelity methods for Monte Carlo based on control variates, importance sampling, and other techniques, respectively. Multifidelity methods for stochastic collocation are discussed in section 3.5 .
3.I. Uncertainty Propagation and Monte Carlo Simulation. Consider the highfidelity model $f_{\mathrm{hi}}: \mathcal{Z} \rightarrow \mathcal{Y}$, and let the uncertainties in the inputs be represented by a random variable, $Z$, with probability density function $p$. At this point, the only</p>
<p>assumption we make on the random variable $Z$ is that the distribution is absolutely continuous such that a density function exists. In particular, the random variable $Z$ can be a non-Gaussian random variable. The goal of uncertainty propagation is to estimate statistics of the random variable $f_{\text {hi }}(Z)$, e.g., the expectation,</p>
<p>$$
\mathbb{E}\left[f_{\mathrm{hi}}\right]=\int_{\mathcal{Z}} f_{\mathrm{hi}}(\boldsymbol{z}) p(\boldsymbol{z}) \mathrm{d} \boldsymbol{z}
$$</p>
<p>and the variance,</p>
<p>$$
\mathbb{V}\left[f_{\mathrm{hi}}\right]=\mathbb{E}\left[f_{\mathrm{hi}}^{2}\right]-\mathbb{E}\left[f_{\mathrm{hi}}\right]^{2}
$$</p>
<p>which we assume to exist.
The Monte Carlo method draws $m \in \mathbb{N}$ independent and identically distributed (i.i.d.) realizations $\boldsymbol{z}<em m="m">{1}, \ldots, \boldsymbol{z}</em>\right]$ as} \in \mathcal{Z}$ of the random variable $Z$ and estimates the expectation $\mathbb{E}\left[f_{\text {hi }</p>
<p>$$
\hat{s}<em i="1">{m}^{\mathrm{hi}}=\frac{1}{m} \sum</em>\right)
$$}^{m} f_{\mathrm{hi}}\left(\boldsymbol{z}_{i</p>
<p>The Monte Carlo estimator is an unbiased estimator $\check{s}<em _mathrm_hi="\mathrm{hi">{m}^{\mathrm{hi}}$ of $\mathbb{E}\left[f</em>}}\right]$, which means that $\mathbb{E}\left[\check{s<em _mathrm_hi="\mathrm{hi">{m}^{\mathrm{hi}}\right]=\mathbb{E}\left[f</em>$ is therefore}}\right]$. The mean squared error (MSE) of the Monte Carlo estimator $\check{s}_{m}^{\mathrm{hi}</p>
<p>$$
e\left(\check{s}<em _mathrm_hi="\mathrm{hi">{m}^{\mathrm{hi}}\right)=\frac{\mathbb{V}\left[f</em>
$$}}\right]}{m</p>
<p>The convergence rate, $\mathcal{O}\left(m^{-1 / 2}\right)$, of the root mean squared error (RMSE) $\sqrt{e\left(\check{s}<em _hi="{hi" _text="\text">{m}^{\mathrm{hi}}\right)}$ is low if compared to deterministic quadrature rules (see section 3.5); however, the rate is independent of the smoothness of the integrand and the dimension, $d$, of the input, $\boldsymbol{z}$, which means that the Monte Carlo method is well suited for high dimensions $d$ and, in fact, is often the only choice available if $d$ is large. Typically more important in practice, however, is the preasymptotic behavior of the RMSE of the Monte Carlo estimator. In the preasymptotic regime, the variance, $\mathbb{V}\left[f</em>(Z)$, of interest and an auxiliary random variable. Multifidelity methods construct the auxiliary random variable using low-fidelity models. We discuss multifidelity methods for variance reduction based on control variates in section 3.2 and variance reduction based on importance sampling in section 3.3.
3.2. Multifidelity Uncertainty Propagation Based on Control Variates. The control variate framework $[96,29,149]$ aims to reduce the estimator variance of a random variable by exploiting the correlation with an auxiliary random variable. In the classical control variate method, as discussed in, e.g., [96], the statistics of the auxiliary random variable are known. Extensions relax this requirement by estimating the statistics of the auxiliary random variable from prior information [65, 159]. We now discuss multifidelity approaches that construct auxiliary random variables from low-fidelity models.}}\right]$, dominates the RMSE. Variance reduction techniques reformulate the estimation problem such that a function with a lower variance is integrated instead of directly integrating $f_{\mathrm{hi}}(Z)$. Examples of variance reduction techniques are antithetic variates, control variates, importance sampling, conditional Monte Carlo sampling, and stratified sampling [96, 177]. Variance reduction techniques often exploit the correlation between the random variable, $f_{\mathrm{hi}</p>
<p>3.2.I. Control Variates Based on Low-Fidelity Models. Consider the highfidelity model $f_{\mathrm{hi}}$ and $k \in \mathbb{N}$ low-fidelity models $f_{\mathrm{lo}}^{(1)}, \ldots, f_{\mathrm{lo}}^{(k)}$. In [150, 164], a multifidelity method is introduced that uses the random variables $f_{\mathrm{lo}}^{(1)}(Z), \ldots, f_{\mathrm{lo}}^{(k)}(Z)$ stemming from the low-fidelity models as control variates for estimating statistics of the random variable $f_{\mathrm{hi}}(Z)$ of the high-fidelity model. An optimal model management is derived that minimizes the MSE of the multifidelity estimator for a given computational budget. In the numerical experiments, high-fidelity finite element models are combined with projection-based models, data-fit models, and SVMs, which demonstrates that the multifidelity approach is applicable to a wide range of low-fidelity model types.</p>
<p>Let $m_{0} \in \mathbb{N}$ be the number of high-fidelity model evaluations. Let $m_{i} \in \mathbb{N}$ be the number of evaluations of the low-fidelity model $f_{\mathrm{lo}}^{(i)}$ for $i=1, \ldots, k$, where $0&lt;m_{0} \leq m_{1} \leq \cdots \leq m_{k}$. The multifidelity approach presented in [164] draws $m_{k}$ realizations</p>
<p>$$
\boldsymbol{z}<em m__k="m_{k">{1}, \ldots, \boldsymbol{z}</em>
$$}</p>
<p>from the random variable $Z$ and computes the model outputs $f_{\mathrm{hi}}\left(\boldsymbol{z}<em _mathrm_hi="\mathrm{hi">{1}\right), \ldots, f</em>}}\left(\boldsymbol{z<em 0="0">{m</em>\right)$ and}</p>
<p>$$
f_{\mathrm{lo}}^{(i)}\left(\boldsymbol{z}<em _mathrm_lo="\mathrm{lo">{1}\right), \ldots, f</em>}}^{(i)}\left(\boldsymbol{z<em i="i">{m</em>\right)
$$}</p>
<p>for $i=1, \ldots, k$. These model outputs are used to derive Monte Carlo estimates</p>
<p>$$
\bar{s}<em 0="0">{m</em>}}^{\mathrm{hi}}=\frac{1}{m_{0}} \sum_{j=1}^{m_{0}} f_{\mathrm{hi}}\left(\boldsymbol{z<em m__i="m_{i">{j}\right), \quad \bar{s}</em>\right), \quad i=1, \ldots, k
$$}}^{(i)}=\frac{1}{m_{i}} \sum_{j=1}^{m_{i}} f_{\mathrm{lo}}^{(i)}\left(\boldsymbol{z}_{j</p>
<p>and</p>
<p>$$
\bar{s}<em i-1="i-1">{m</em>\right), \quad i=1, \ldots, k
$$}}^{(i)}=\frac{1}{m_{i-1}} \sum_{j=1}^{m_{i-1}} f_{\mathrm{lo}}^{(i)}\left(\boldsymbol{z}_{j</p>
<p>Note that the estimates (3.8) use the first $f_{\mathrm{lo}}^{(i)}\left(\boldsymbol{z}<em _mathrm_lo="\mathrm{lo">{1}\right), \ldots, f</em>}}^{(i)}\left(\boldsymbol{z<em i-1="i-1">{m</em>\right]$ is}}\right)$ model outputs of (3.6) only, whereas the estimate (3.7) uses all $m_{i}$ model outputs (3.6) for $i=1, \ldots, k$. Following [164], the multifidelity estimator of $\mathbb{E}\left[f_{\mathrm{hi}</p>
<p>$$
\bar{s}^{\mathrm{MF}}=\bar{s}<em 0="0">{m</em>}}^{\mathrm{hi}}+\sum_{i=1}^{k} \alpha_{i}\left(\bar{s<em i="i">{m</em>}}^{(i)}-\bar{s<em i-1="i-1">{m</em>\right)
$$}}^{(i)</p>
<p>The control variate coefficients $\alpha_{1}, \ldots, \alpha_{k} \in \mathbb{R}$ balance the term $\bar{s}<em 0="0">{m</em>}}^{\mathrm{hi}}$ stemming from the high-fidelity model and the terms $\bar{s<em i="i">{m</em>}}^{(i)}-\bar{s<em i-1="i-1">{m</em>$ from the low-fidelity models. The multifidelity estimator (3.9) based on the control variate framework evaluates the highand the low-fidelity model and fuses both outputs into an estimate of the statistics of the high-fidelity model. The multifidelity estimator (3.9) therefore uses a model management based on fusion; see section 2.2. We note that (3.9) could also be viewed as a correction, although the correction is to the estimators stemming from the lowfidelity models, not to the low-fidelity model outputs directly.}}^{(i)</p>
<p>Properties of the Multifidelity Estimator. The multifidelity estimator $\bar{s}^{\mathrm{MF}}$ is an unbiased estimator of $\mathbb{E}\left[f_{\mathrm{hi}}\right]$ because</p>
<p>$$
\mathbb{E}\left[\bar{s}^{\mathrm{MF}}\right]=\mathbb{E}\left[\bar{s}<em 0="0">{m</em>}}^{\mathrm{hi}}\right]+\sum_{i=1}^{k} \alpha_{i} \mathbb{E}\left[\bar{s<em i="i">{m</em>}}^{(i)}-\bar{s<em i-1="i-1">{m</em>\right]
$$}}^{(i)}\right]=\mathbb{E}\left[f_{\mathrm{hi}</p>
<p>Therefore, the MSE of the estimator $\bar{s}^{\mathrm{MF}}$ is equal to the variance $\mathbb{V}\left[\bar{s}^{\mathrm{MF}}\right]$ of the estimator, $e\left(\bar{s}^{\mathrm{MF}}\right)=\mathbb{V}\left[\bar{s}^{\mathrm{MF}}\right]$. The costs of the multifidelity estimator are</p>
<p>$$
c\left(\bar{s}^{\mathrm{MF}}\right)=m_{0} c_{\mathrm{hi}}+\sum_{i=1}^{k} m_{i} c_{\mathrm{lo}}^{(i)}=\boldsymbol{m}^{T} \boldsymbol{c}
$$</p>
<p>where $\boldsymbol{m}=\left[m_{0}, m_{1}, \ldots, m_{k}\right]^{T}$ and $\boldsymbol{c}=\left[c_{\mathrm{hi}}, c_{\mathrm{lo}}^{(1)}, \ldots, c_{\mathrm{lo}}^{(k)}\right]^{T}$. The high-fidelity model is evaluated at $m_{0}$ realizations and the low-fidelity model $f_{\mathrm{lo}}^{(i)}$ at $m_{i}$ realizations of $Z$, for $i=1, \ldots, k$.
3.2.2. Multifidelity Monte Carlo. The multifidelity estimator (3.9) depends on the control variate coefficients $\alpha_{1}, \ldots, \alpha_{k}$ and on the number of model evaluations $m_{0}, m_{1}, \ldots, m_{k}$. In $[150,164]$, these parameters are chosen such that the MSE of the estimator (3.9) is minimized for a given computational budget $\gamma \in \mathbb{R}_{+}$. The solution to the optimization problem</p>
<p>$$
\begin{array}{cl}
\min <em 0="0">{\substack{m</em> \
\alpha_{1}, \ldots, \alpha_{k}}} &amp; e\left(\bar{s}^{\mathrm{MF}}\right) \
\text { s.t. } &amp; m_{0}&gt;0 \
&amp; m_{i} \geq m_{i-1}, \quad i=1, \ldots, k \
&amp; \boldsymbol{m}^{T} \boldsymbol{c}=\gamma
\end{array}
$$}, m_{1}, \ldots, m_{k</p>
<p>gives the coefficients $\alpha_{1}^{<em>}, \ldots, \alpha_{k}^{</em>}$ and the number of model evaluations $m_{0}^{<em>}, \ldots, m_{k}^{</em>}$ that minimize the MSE of the multifidelity estimator $\bar{s}^{\mathrm{MF}}$ for the given computational budget $\gamma$. The constraints impose that $0&lt;m_{0}^{<em>} \leq m_{1}^{</em>} \leq \cdots \leq m_{k}^{*}$ and that the costs $c\left(\bar{s}^{\mathrm{MF}}\right)$ of the estimator equal the computational budget $\gamma$.</p>
<p>Variance of the Multifidelity Estimator. Since the multifidelity estimator $\bar{s}^{\mathrm{MF}}$ is unbiased, we have $e\left(\bar{s}^{\mathrm{MF}}\right)=\mathbb{V}\left[\bar{s}^{\mathrm{MF}}\right]$, and therefore the objective of minimizing the MSE $e\left(\bar{s}^{\mathrm{MF}}\right)$ can be replaced with the variance $\mathbb{V}\left[\bar{s}^{\mathrm{MF}}\right]$ in the optimization problem (3.10). The variance $\mathbb{V}\left[\bar{s}^{\mathrm{MF}}\right]$ of the multifidelity estimator $\bar{s}^{\mathrm{MF}}$ is</p>
<p>$$
\mathbb{V}\left[\bar{s}^{\mathrm{MF}}\right]=\frac{\sigma_{\mathrm{hi}}^{2}}{m_{0}}+\sum_{i=1}^{k}\left(\frac{1}{m_{i-1}}-\frac{1}{m_{i}}\right)\left(\alpha_{i}^{2} \sigma_{i}^{2}-2 \alpha_{i} \rho_{i} \sigma_{\mathrm{hi}} \sigma_{i}\right)
$$</p>
<p>where $-1 \leq \rho_{i} \leq 1$ is the Pearson correlation coefficient of the random variables $f_{\mathrm{hi}}(Z)$ and $f_{\mathrm{lo}}^{(i)}(Z)$ for $i=1, \ldots, k$. The quantities</p>
<p>$$
\sigma_{\mathrm{hi}}^{2}=\mathbb{V}\left[f_{\mathrm{hi}}\right], \quad \sigma_{i}^{2}=\mathbb{V}\left[f_{\mathrm{lo}}^{(i)}\right], \quad i=1, \ldots, k
$$</p>
<p>are the variances of $f_{\mathrm{hi}}(Z)$ and $f_{\mathrm{lo}}^{(i)}(Z)$, respectively.</p>
<h1>Optimal Selection of the Number of Samples and Control Variate Coefficients.</h1>
<p>Under certain conditions on the low- and the high-fidelity model, the optimization problem (3.10) has a unique analytic solution [164]. The optimal control variate coefficients are</p>
<p>$$
\alpha_{i}^{*}=\rho_{i} \frac{\sigma_{\mathrm{hi}}}{\sigma_{i}}, \quad i=1, \ldots, k
$$</p>
<p>The optimal numbers of evaluations $m_{0}^{<em>}, m_{1}^{</em>}, \ldots, m_{k}^{*}$ are</p>
<p>$$
m_{0}^{<em>}=\frac{\gamma}{\boldsymbol{c}^{T} \boldsymbol{r}}, \quad m_{i}^{</em>}=r_{i} m_{0}, \quad i=1, \ldots, k
$$</p>
<p>where the components of the vector $\boldsymbol{r}=\left[1, r_{1}, \ldots, r_{k}\right]^{T} \in \mathbb{R}^{k+1}$ are given as</p>
<p>$$
r_{i}=\sqrt{\frac{c_{\mathrm{hi}}\left(\rho_{i}^{2}-\rho_{i+1}^{2}\right)}{c_{\mathrm{lo}}^{(i)}\left(1-\rho_{1}^{2}\right)}}, \quad i=1, \ldots, k
$$</p>
<p>Note that the convention $\rho_{k+1}=0$ is used in (3.14). We refer to [164] for details.
Interaction of Models in Multifidelity Monte Carlo. We compare the multifidelity estimator $\bar{s}^{\mathrm{MF}}$ to a benchmark Monte Carlo estimator $\bar{s}^{\mathrm{MC}}$ that uses the highfidelity model alone. The multifidelity estimator $\bar{s}^{\mathrm{MF}}$ and the benchmark estimator $\bar{s}^{\mathrm{MC}}$ have the same costs $\gamma$. With the MSE $e\left(\bar{s}^{\mathrm{MF}}\right)$ of the multifidelity estimator and the MSE $e\left(\bar{s}^{\mathrm{MC}}\right)$ of the benchmark Monte Carlo estimator, the variance reduction ratio is</p>
<p>$$
\frac{e\left(\bar{s}^{\mathrm{MF}}\right)}{e\left(\bar{s}^{\mathrm{MC}}\right)}=\left(\sqrt{1-\rho_{1}^{2}}+\sum_{i=1}^{k} \sqrt{\frac{c_{\mathrm{lo}}^{(i)}}{c_{\mathrm{hi}}}\left(\rho_{i}^{2}-\rho_{i+1}^{2}\right)}\right)^{2}
$$</p>
<p>The ratio (3.15) quantifies the variance reduction achieved by the multifidelity estimator compared to the benchmark Monte Carlo estimator. The variance reduction ratio is a sum over the costs $c_{\mathrm{hi}}, c_{\mathrm{lo}}^{(1)}, \ldots, c_{\mathrm{lo}}^{(k)}$ and the correlation coefficients $\rho_{1}, \ldots, \rho_{k}$ of all models in the multifidelity estimator. This shows that the contribution of a low-fidelity model to the variance reduction of the multifidelity estimator cannot be determined by the properties of that low-fidelity model alone but only by taking into account all other models that are used in the multifidelity estimator. Thus, the interaction between the models is what drives the efficiency of the multifidelity estimator $\bar{s}^{\mathrm{MF}}$. We refer to [164] for an in-depth discussion of the interaction between the models and for a more detailed analysis.</p>
<p>Efficiency of the Multifidelity Estimator. It is shown in [164] that the multifidelity Monte Carlo estimator $\bar{s}^{\mathrm{MF}}$ is computationally cheaper than the benchmark Monte Carlo estimator that uses the high-fidelity model $f_{\mathrm{hi}}$ alone if</p>
<p>$$
\sqrt{1-\rho_{1}^{2}}+\sum_{i=1}^{k} \sqrt{\frac{c_{\mathrm{lo}}^{(i)}}{c_{\mathrm{hi}}}\left(\rho_{i}^{2}-\rho_{i+1}^{2}\right)}&lt;1
$$</p>
<p>The inequality (3.16) emphasizes that both correlation and costs of the models are critical for an efficient multifidelity estimator.</p>
<p>Algorithm 3.1 Multifidelity Monte Carlo
procedure $\operatorname{MFMC}\left(f_{\mathrm{hi}}, f_{\mathrm{lo}}^{(1)}, \ldots, f_{\mathrm{lo}}^{(k)}, \sigma_{\mathrm{hi}}, \sigma_{1}, \ldots, \sigma_{k}, \rho_{1}, \ldots, \rho_{k}, c_{\mathrm{hi}}, c_{\mathrm{lo}}^{(1)}, \ldots, c_{\mathrm{lo}}^{(k)}, \gamma\right)$
2: $\quad$ Ensure $f_{\mathrm{hi}}, f_{\mathrm{lo}}^{(1)}, \ldots, f_{\mathrm{lo}}^{(k)}$ are ordered as described in [164, section 3.5]
3: $\quad$ Set $\rho_{k+1}=0$ and define vector $\boldsymbol{r}=\left[1, r_{1}, \ldots, r_{k}\right]^{T} \in \mathbb{R}_{+}^{k+1}$ as</p>
<p>$$
r_{i}=\sqrt{\frac{c_{\mathrm{hi}}\left(\rho_{i}^{2}-\rho_{i+1}^{2}\right)}{c_{\mathrm{lo}}^{(i)}\left(1-\rho_{1}^{2}\right)}}, \quad i=1, \ldots, k
$$</p>
<p>4: Select number of model evaluations $\boldsymbol{m}^{\star} \in \mathbb{R}_{+}^{k+1}$ as</p>
<p>$$
\boldsymbol{m}^{\star}=\left[\frac{\gamma}{\boldsymbol{c}^{T} \boldsymbol{r}}, r_{1} m_{0}^{<em>}, \ldots, r_{k} m_{0}^{</em>}\right]^{T} \in \mathbb{R}_{+}^{k+1}
$$</p>
<p>5: Set coefficients $\boldsymbol{\alpha}^{\star}=\left[\alpha_{1}^{<em>}, \ldots, \alpha_{k}^{</em>}\right]^{T} \in \mathbb{R}^{k}$ to</p>
<p>$$
\alpha_{i}^{*}=\frac{\rho_{i} \sigma_{\mathrm{hi}}}{\sigma_{i}}, \quad i=1, \ldots, k
$$</p>
<p>6: Draw $\boldsymbol{z}<em m__k="m_{k">{1}, \ldots, \boldsymbol{z}</em>^{<em>}} \in \mathcal{Z}$ realizations of $Z$
7: Evaluate high-fidelity model $f_{\mathrm{hi}}$ at realizations $\boldsymbol{z}<em m__0="m_{0">{1}, \ldots, \boldsymbol{z}</em>^{</em>}}$
8: Evaluate model $f_{\mathrm{lo}}^{(i)}$ at realizations $\boldsymbol{z}<em m__i="m_{i">{1}, \ldots, \boldsymbol{z}</em>$ for $i=1, \ldots, k$
9: Compute the multifidelity estimate $\hat{s}^{\mathrm{MF}}$ as in (3.9)
10: return multifidelity estimate $\hat{s}^{\mathrm{MF}}$
11: end procedure}^{*}</p>
<p>Algorithm. Algorithm 3.1 summarizes the multifidelity Monte Carlo method as presented in [164]. Inputs are the models $f_{\mathrm{hi}}, f_{\mathrm{lo}}^{(1)}, \ldots, f_{\mathrm{lo}}^{(k)}$ and the variances $\sigma_{\mathrm{hi}}$ and $\sigma_{1}, \ldots, \sigma_{k}$. The inputs $\rho_{i}$ are the correlation coefficients of the random variable $f_{\mathrm{hi}}(Z)$ stemming from the high-fidelity model and the random variables $f_{\mathrm{lo}}^{(i)}(Z)$ for $i=1, \ldots, k$. The costs of the models are $c_{\mathrm{hi}}, c_{\mathrm{lo}}^{(1)}, \ldots, c_{\mathrm{lo}}^{(k)}$, and the computational budget is $\gamma$. Line 2 of Algorithm 3.1 ensures that the ordering of the models is used that minimizes the MSE of the multifidelity estimator; see [164, section 3.5] for details. Line 3 defines the vector of ratios $\boldsymbol{r}=\left[r_{0}, r_{1}, \ldots, r_{k}\right]^{T}$; cf. (3.14). The numbers of model evaluations $m_{0}^{<em>}, m_{1}^{</em>}, \ldots, m_{k}^{<em>}$ of the models $f_{\mathrm{hi}}, f_{\mathrm{lo}}^{(1)}, \ldots, f_{\mathrm{lo}}^{(k)}$ are derived from $\boldsymbol{r}$ as in (3.13). The control variate coefficients $\alpha_{1}^{</em>}, \ldots, \alpha_{k}^{<em>}$ are obtained as in (3.12). In line $6, m_{k}^{</em>}$ realizations $\boldsymbol{z}<em m__k="m_{k">{1}, \ldots, \boldsymbol{z}</em>^{<em>}}$ are drawn from the random variable $Z$. The high-fidelity model $f_{\mathrm{hi}}$ is evaluated at the realizations $\boldsymbol{z}<em m__0="m_{0">{1}, \ldots, \boldsymbol{z}</em>^{</em>}}$, and models $f_{\mathrm{lo}}^{(i)}$ are evaluated at $\boldsymbol{z}<em m__i="m_{i">{1}, \ldots, \boldsymbol{z}</em>$ for $i=1, \ldots, k$. The multifidelity estimate is obtained as in (3.9) and returned.
3.2.3. Other Uses of Control Variates as a Multifidelity Technique. In [25, 26], the reduced-basis method is used to construct control variates. The reduced-basis models are built with greedy algorithms that use a posteriori error estimators to particularly target variance reduction. The work [210] uses error estimators to combine reduced-basis models with control variates. The StackMC method presented in [203] successively constructs machine-learning-based low-fidelity models and combines them with the control variate framework. In [151], the multifidelity control variate method is used in the context of optimization, where information of previous iterations of}^{*}</p>
<p>the optimization problem are used as control variates. This means that data from previous iterations serve as a kind of low-fidelity "model."</p>
<p>The multilevel Monte Carlo method [98, 76] uses the control variate framework to combine multiple low-fidelity models with a high-fidelity model. Typically, in multilevel Monte Carlo, the low-fidelity models are coarse-grid approximations, where the accuracy and costs can be controlled by a discretization parameter. The properties of the low-fidelity models are therefore often described with rates. For example, the rate of the decay of the variance of the difference of two successive coarse-grid approximations and the rate of the increase of the costs with finer grids play a critical role in determining the efficiency of the multilevel Monte Carlo method. Additionally, rates are used to determine the number of evaluations of each low-fidelity model and the high-fidelity model; see, e.g., [45, Theorem 1]. In the setting of stochastic differential equations and coarse-grid approximations, multilevel Monte Carlo has been very successful; see, e.g., $[45,199]$, the recent advances on multiindex Monte Carlo [95], and the nesting of multilevel Monte Carlo and control variates [155] for detailed studies and further references.</p>
<h1>3.3. Multifidelity Uncertainty Propagation Based on Importance Sampling.</h1>
<p>Importance sampling [96] uses a problem-dependent sampling strategy. The goal is an estimator with a lower variance than a Monte Carlo estimator such as (3.3). Problemdependent sampling means that samples are drawn from a biasing distribution instead of directly from the distribution of the random variable $Z$ of interest, and then the change of the distribution is compensated with a reweighting. Importance sampling is particularly useful in the case of rare event simulation, where the probability of the event of interest is small and therefore many realizations of the random variable $Z$ are necessary to obtain a Monte Carlo estimate of reasonable accuracy. Importance sampling with a suitable biasing distribution can explicitly target the rare event and reduce the number of realizations required to achieve an acceptable accuracy. The challenge of importance sampling is the construction of a biasing distribution, which is usually problem dependent and typically requires model evaluations. We discuss multifidelity methods that use low-fidelity models to construct biasing distributions.
3.3.1. Importance Sampling. Consider the indicator function $I_{\mathrm{hi}}: \mathcal{Z} \rightarrow{0,1}$ defined as</p>
<p>$$
I_{\mathrm{hi}}(\boldsymbol{z})= \begin{cases}1, &amp; f_{\mathrm{hi}}(\boldsymbol{z})&lt;0 \ 0, &amp; f_{\mathrm{hi}}(\boldsymbol{z}) \geq 0\end{cases}
$$</p>
<p>We define the set $\mathcal{I}=\left{\boldsymbol{z} \in \mathcal{Z} \mid I_{\mathrm{hi}}(\boldsymbol{z})=1\right}$. The goal is to estimate the probability of the event $Z^{-1}(\mathcal{I})$, which is $\mathbb{E}<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>$ the distribution the expectation is taken with respect to.}}\right]$, with importance sampling. Note that we now explicitly denote in the subscript of $\mathbb{E</p>
<p>Step I: Construction of Biasing Distribution. Traditionally, importance sampling consists of two steps. In the first step, the biasing distribution with density $q$ is constructed. Let $Z^{\prime}$ be the biasing random variable with biasing density $q$. Recall that the input random variable $Z$ with the nominal distribution has nominal density $p$. Let</p>
<p>$$
\operatorname{supp}(p)={\boldsymbol{z} \in \mathcal{Z}: p(\boldsymbol{z})&gt;0}
$$</p>
<p>be the support of the density $p$. If the support of the nominal density $p$ is a subset of the support of the biasing density $q$, i.e., $\operatorname{supp}(p) \subset \operatorname{supp}(q)$, then the expectation</p>
<p>$\mathbb{E}<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>\right]$ can be rewritten in terms of the biasing density $q$ as}</p>
<p>$$
\mathbb{E}<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>}}\right]=\int_{\mathcal{Z}} I_{\mathrm{hi}}(\boldsymbol{z}) p(\boldsymbol{z}) \mathrm{d} \boldsymbol{z}=\int_{\mathcal{Z}} I_{\mathrm{hi}}\left(\boldsymbol{z}^{\prime}\right) q\left(\boldsymbol{z}^{\prime}\right) \frac{p\left(\boldsymbol{z}^{\prime}\right)}{q\left(\boldsymbol{z}^{\prime}\right)} \mathrm{d} \boldsymbol{z}^{\prime}=\mathbb{E<em _mathrm_hi="\mathrm{hi">{q}\left[I</em>\right]
$$}} \frac{p}{q</p>
<p>where the ratio $p / q$ serves as a weight.
Step 2: Deriving an Importance Sampling Estimate. In the second step, the importance sampling estimator</p>
<p>$$
\tilde{s}<em i="1">{m}^{\mathrm{IS}}=\frac{1}{m} \sum</em>}^{m} I_{\mathrm{hi}}\left(\boldsymbol{z<em i="i">{i}^{\prime}\right) \frac{p\left(\boldsymbol{z}</em>
$$}^{\prime}\right)}{q\left(\boldsymbol{z}_{i}^{\prime}\right)</p>
<p>is evaluated for realizations $\boldsymbol{z}<em m="m">{1}^{\prime}, \ldots, \boldsymbol{z}</em>$. The MSE of the estimator (3.17) is}^{\prime} \in \mathcal{Z}$ of the random variable $Z^{\prime</p>
<p>$$
e\left(\tilde{s}<em q="q">{m}^{\mathrm{IS}}\right)=\frac{\mathbb{V}</em>
$$}\left[I_{\mathrm{hi}} \frac{p}{q}\right]}{m</p>
<p>Variance of Importance Sampling Estimator. The variance in (3.18) is with respect to the biasing density $q$; cf. section 3.1 and the MSE of the Monte Carlo estimator (3.4). Therefore, the goal is to construct a biasing distribution with</p>
<p>$$
\mathbb{V}<em _mathrm_hi="\mathrm{hi">{q}\left[I</em>}} \frac{p}{q}\right]&lt;\mathbb{V<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>\right]
$$}</p>
<p>to obtain an importance sampling estimator $\tilde{s}<em m="m">{m}^{\mathrm{IS}}$ that has a lower MSE than the Monte Carlo estimator $\tilde{s}</em>$ for the same number of realizations $m$.}^{\mathrm{hi}</p>
<h1>3.3.2. Construction of the Biasing Distribution with Low-Fidelity Models.</h1>
<p>The multifidelity importance sampling approach introduced in [160] uses a low-fidelity model to construct the biasing distribution in the first step of importance sampling and derives the statistics using high-fidelity model evaluations in step 2. In that sense, multifidelity importance sampling uses a model management strategy based on filtering; see section 2.3 .</p>
<p>In step 1 , the low-fidelity model $f_{\mathrm{lo}}$ is evaluated at a large number $n \in \mathbb{N}$ of realizations $\boldsymbol{z}<em n="n">{1}, \ldots, \boldsymbol{z}</em>$. This is computationally feasible because the low-fidelity model is cheap to evaluate. A mixture model $q$ of Gaussian distributions is fitted with the expectation-maximization algorithm to the set of realizations}$ of the input random variable $\mathcal{Z</p>
<p>$$
\left{\boldsymbol{z}<em _mathrm_lo="\mathrm{lo">{i} \mid I</em>\right)=1, i=1, \ldots, n\right}
$$}}\left(\boldsymbol{z}_{i</p>
<p>for which the low-fidelity model predicts the event of interest with the indicator function $I_{\mathrm{lo}}: \mathcal{Z} \rightarrow{0,1}$ :</p>
<p>$$
I_{\mathrm{lo}}(\boldsymbol{z})= \begin{cases}1, &amp; f_{\mathrm{lo}}(\boldsymbol{z})&lt;0 \ 0, &amp; f_{\mathrm{lo}}(\boldsymbol{z}) \geq 0\end{cases}
$$</p>
<p>The mixture model $q$ serves as a biasing distribution. Note that other density estimation methods can be used instead of fitting a mixture model of Gaussian distributions with the expectation-maximization algorithm [190, 143, 161].</p>
<p>In step 2, the high-fidelity model is evaluated at realizations $\boldsymbol{z}<em m="m">{1}^{\prime}, \ldots, \boldsymbol{z}</em>}^{\prime}$ from the biasing random variable $Z^{\prime}$ with biasing density $q$. From the high-fidelity model evaluations $f_{\mathrm{hi}}\left(\boldsymbol{z<em _mathrm_hi="\mathrm{hi">{1}^{\prime}\right), \ldots, f</em>}}\left(\boldsymbol{z<em p="p">{m}^{\prime}\right)$ an estimate of the event probability $\mathbb{E}</em>\right]$ is obtained.}\left[I_{\mathrm{hi}</p>
<p>Under the condition that the support of the biasing density, $q$, includes the support of the nominal density, $p$, the multifidelity importance sampling approach leads to an unbiased estimator of the probability of the event. If the low-fidelity model is sufficiently accurate, then significant runtime savings can be obtained during the construction of the biasing distribution. Note that using $I_{\mathrm{lo}}$ in the second step of the multifidelity approach would lead to a biased estimator of $\mathbb{E}<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>}}\right]$ because $f_{\mathrm{lo}}$ is only an approximation of $f_{\mathrm{hi}}$ and thus $\mathbb{E<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>}}\right] \neq \mathbb{E<em _mathrm_lo="\mathrm{lo">{p}\left[I</em>\right]$ in general.}</p>
<h1>3.4. Other Model Management Strategies for Probability Estimation and</h1>
<p>Limit State Function Evaluation. The multifidelity approaches for estimating $\mathbb{E}<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>}}\right]$ that are discussed in section 3.3 first use the low-fidelity model to construct a biasing density $q$ and then the high-fidelity model to estimate the failure probability $\mathbb{E<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>}}\right]$ with importance sampling. Under mild conditions on the biasing density $q$ derived from the low-fidelity model, the importance sampling estimator using the high-fidelity model is an unbiased estimator of the failure probability $\mathbb{E<em _mathrm_hi="\mathrm{hi">{p}\left[I</em>}}\right]$. In this section, we review multifidelity methods that combine low- and high-fidelity model evaluations to obtain an indicator function $\hat{I}$ that approximates $I_{\mathrm{hi}}$ and that is computationally cheaper to evaluate than $I_{\mathrm{hi}}$. Thus, instead of exploiting the two-step importance sampling procedure as in section 3.3 , the techniques in this section leverage the low- and high-fidelity models to approximate $I_{\mathrm{hi}}$ with $\hat{I}$ with the aim that the error $\left|\mathbb{E<em p="p">{p}[\hat{I}]-\mathbb{E}</em>\right]\right|$ is small.}\left[I_{\mathrm{hi}</p>
<p>The multifidelity approach introduced in [128] is based on filtering to combine low- and high-fidelity model outputs to obtain an approximation $\hat{I}$ of $I_{\mathrm{hi}}$. Let $f_{\mathrm{lo}}$ be a low-fidelity model and $f_{\mathrm{hi}}$ the high-fidelity model. Let further $\gamma&gt;0$ be a positive threshold value. The approach in [128] considers the indicator function</p>
<p>$$
\hat{I}(\boldsymbol{z})= \begin{cases}1, &amp; f_{\mathrm{lo}}(\boldsymbol{z})&lt;-\gamma \text { or }\left(\left|f_{\mathrm{lo}}(\boldsymbol{z})\right| \leq \gamma \text { and } f_{\mathrm{hi}}(\boldsymbol{z})&lt;0\right) \ 0 &amp; \text { else }\end{cases}
$$</p>
<p>which evaluates to one at an input $\boldsymbol{z}$ if either $f_{\mathrm{lo}}(\boldsymbol{z})&lt;-\gamma$ or $\left|f_{\mathrm{lo}}(\boldsymbol{z})\right| \leq \gamma$ and $f_{\mathrm{hi}}(\boldsymbol{z})&lt;0$. Evaluating the indicator function $\hat{I}$ at $\boldsymbol{z}$ means that first the low-fidelity model $f_{\mathrm{lo}}$ is evaluated at $\boldsymbol{z}$. If $f_{\mathrm{lo}}(\boldsymbol{z})&lt;-\gamma$, then one is returned, and no high-fidelity model evaluation is necessary. Similarly, if $f_{\mathrm{lo}}(\boldsymbol{z})&gt;\gamma$, then the indicator function $\hat{I}$ evaluates to zero without requiring a high-fidelity model evaluation. However, if $\left|f_{\mathrm{lo}}(\boldsymbol{z})\right| \leq \gamma$, then the input $\boldsymbol{z}$ lies near the failure boundary, and the high-fidelity model is evaluated to decide whether the indicator function returns zero or one. How often the high-fidelity model is evaluated is determined by the positive threshold value $\gamma$. The choice of $\gamma$ directly depends on the error of the low-fidelity model $f_{\mathrm{lo}}$ in a certain norm. If the error of $f_{\mathrm{lo}}$ is known, the work [128] establishes a convergence theory under mild conditions on the error of $f_{\mathrm{lo}}$. In particular, the authors of [128] show that the error $\left|\mathbb{E}<em p="p">{p}[\hat{I}]-\mathbb{E}</em>$. If the error of the low-fidelity model is unknown, the work [128] proposes an iterative heuristic approach that avoids the choice of $\gamma$. In [127], the multifidelity approach of [128] is extended to importance sampling with the cross-entropy method. Similarly to the approaches in [128, 127], the work [41] switches between low- and high-fidelity model evaluations by relying on a posteriori error estimators for reduced-basis models to decide if either the reduced model or the high-fidelity model should be used.}\left[I_{\mathrm{hi}}\right]\right|$ can be reduced below any $\epsilon&gt;0$ by a choice of $\gamma$ that depends on the error of $f_{\mathrm{lo}</p>
<p>The multifidelity methods in $[16,59,163]$ all use a model management strategy based on adaptation to estimate a failure boundary or failure probability. In [16], an</p>
<p>SVM is used to derive a low-fidelity model of the limit state function in failure probability estimation and design. The authors decompose the input space using decision boundaries obtained via the SVM, and so handle the discontinuities that arise when approximating the limit state function. An adaptive sampling scheme is introduced that refines the low-fidelity model along the failure boundary. In [59], another approach is introduced that uses an SVM to approximate the failure boundary. It is proposed to train the SVM with data obtained from a low- and a high-fidelity model. With the low-fidelity model, an initial approximation of the failure boundary is obtained by extensively sampling the input domain, which is computationally tractable because the low-fidelity model is cheap to evaluate. The training of the SVM for approximating the failure boundary corresponding to the high-fidelity model is then initialized with the approximate boundary obtained with the low-fidelity model. Additional samples are drawn in regions where the low- and the high-fidelity failure boundaries differ, to refine the approximation of the high-fidelity failure boundary. The work [163] presents an online adaptive reduced modeling approach, which is demonstrated in the context of failure probability estimation. To increase the accuracy of the reduced model, it is adapted to the failure boundary as the estimation of the failure probability proceeds. The adaptation is performed via low-rank updates to the basis matrix of the reduced model. The low-rank updates are derived from sparse samples of the high-fidelity model.
3.5. Stochastic Collocation and Multifidelity. Stochastic collocation methods $[11,154,89]$ compute statistics such as the expectation (3.1) and the variance (3.2) by using a deterministic quadrature rule instead of the Monte Carlo method. The quadrature rules are often based on sparse grids [32,154] to perform the quadrature efficiently for high-dimensional inputs.</p>
<p>In [64], statistics are computed using stochastic collocation, where the outputs of a low-fidelity model are corrected with a discrepancy model that accounts for the difference between the high- and the low-fidelity model. The discrepancy model is then used to derive an additive correction, a multiplicative correction, or a weighted combination of additive and multiplicative corrections to the low-fidelity model outputs. Thus, this is another example of model management based on adaptation; see section 2.1. The authors of [64] point out that an adaptive refinement of the discrepancy model is necessary because the complexity of the discrepancy between the high- and the low-fidelity model varies distinctly in the stochastic domain. This is because low-fidelity models tend to approximate the high-fidelity model well only in certain regions of the stochastic domain, whereas in other regions they hardly match the high-fidelity model at all.</p>
<p>Another multifidelity stochastic collocation method is presented in [148]. This method is based on a filtering model management strategy. The low-fidelity model is first evaluated at a large number of collocation points to sample the stochastic domain. From these samples, a small number of points is selected via a greedy procedure, and the high-fidelity model is evaluated at these points. The state solutions of the highfidelity model at the selected collocation points span a space in which approximations of the high-fidelity model states at all other sampling points are derived.</p>
<p>In [198], a multilevel stochastic collocation method uses a hierarchy of models to accelerate convergence. Low-fidelity models are coarse-grid approximations of the high-fidelity model. Similarly to the multilevel Monte Carlo method, a reduction of the computational complexity can be shown if the errors of the models in the hierarchy decay with a higher rate than the rate of the increase of the costs. We categorize</p>
<p>this multilevel stochastic collocation method as model management based on fusion, because low- and high-fidelity model outputs are fused to estimate the statistics of the high-fidelity model.
4. Multifidelity Model Management in Statistical Inference. In a Bayesian setting, inverse problems are cast in a statistical formulation where the unknown input is modeled as a random variable and is described by its posterior distribution $[195,194,110,192]$. MCMC is a popular way to sample from the posterior distribution. Statistical inference raises several computational challenges, including the design of MCMC sampling schemes, the construction of approximate models that can reduce the costs of MCMC sampling, and the development of alternatives to MCMC sampling such as variational approaches [140]. Detailed discussions of these and many other important aspects of statistical inference can be found in the literature, e.g., $[177,110,192,130]$. We focus here on a few specific aspects of statistical inference in which multifidelity methods have been used. In particular, we survey multifidelity methods that use a two-stage formulation of MCMC, where a candidate sample has to be first accepted by a low-fidelity model before it is passed on to be either accepted or rejected by the high-fidelity model. Section 4.1 describes the problem setup. Section 4.2 describes a two-stage MCMC framework, and section 4.3 discusses a framework where a low-fidelity model is adapted while it is evaluated in an MCMC algorithm. A Bayesian approach to model and correct the error of low-fidelity models is discussed in section 4.4.
4.I. Bayesian Framework for Inference. Consider the high-fidelity model $f_{\text {hi }}$ that maps inputs $\boldsymbol{z} \in \mathcal{Z}$ onto outputs $\boldsymbol{y} \in \mathcal{Y}$. Let $p_{0}$ be a prior density that describes the input $\boldsymbol{z}$ before any measurements. Let further $\boldsymbol{y}_{\text {obs }}$ be noisy observational data with the stochastic relationship</p>
<p>$$
\boldsymbol{y}<em _mathrm_hi="\mathrm{hi">{\mathrm{obs}}=f</em>
$$}}(\boldsymbol{z})+\boldsymbol{\epsilon</p>
<p>where $\boldsymbol{\epsilon}$ is a random vector that captures the measurement error, noise, and other uncertainties of the observation $\boldsymbol{y}<em _boldsymbol_epsilon="\boldsymbol{\epsilon">{\text {obs }}$. In the following, the random vector $\boldsymbol{\epsilon}$ is modeled as a zero-mean Gaussian with covariance $\boldsymbol{\Sigma}</em>$. Define the data-misfit function as}} \in \mathbb{R}^{d \times d</p>
<p>$$
\Phi(\boldsymbol{z})=\frac{1}{2}\left|\boldsymbol{\Sigma}<em _mathrm_hi="\mathrm{hi">{\boldsymbol{\epsilon}}^{-\frac{1}{2}}\left(f</em>
$$}}(\boldsymbol{z})-\boldsymbol{y}_{\mathrm{obs}}\right)\right|^{2</p>
<p>with the norm $|\cdot|$. The likelihood function $L: \mathcal{Z} \rightarrow \mathbb{R}$ is then proportional to</p>
<p>$$
L\left(\boldsymbol{y}_{\mathrm{obs}} \mid \boldsymbol{z}\right) \propto \exp (-\Phi(\boldsymbol{z}))
$$</p>
<p>An evaluation of the likelihood $L$ entails an evaluation of the high-fidelity model $f_{\mathrm{hi}}$. The posterior probability density is</p>
<p>$$
p\left(\boldsymbol{z} \mid \boldsymbol{y}<em _mathrm_obs="\mathrm{obs">{\mathrm{obs}}\right) \propto L\left(\boldsymbol{y}</em>)
$$}} \mid \boldsymbol{z}\right) p_{0}(\boldsymbol{z</p>
<p>We note that there are many challenging and important questions regarding the formulation of an inverse problem in the Bayesian setting. For example, the selection of the prior is a delicate and problem-dependent question; see, e.g., [110, section 3.3] for a discussion of prior models. We do not address this and other issues here, but note that our stated goal of a multifidelity formulation is to recover a solution of the outer-loop problem (here the inverse problem) that retains the accuracy of the highfidelity formulation. Thus, the multifidelity approaches described below will inherit the choices made in the high-fidelity formulation of the Bayesian inverse problem.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Note that we use the term filter to denote selective evaluation based on the low-fidelity model. This differs from the predominant usage in signal processing and uncertainty quantification, where filtering describes the estimation of the state of a dynamical system from noisy and incomplete data (e.g., Kalman filter, particle filter).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
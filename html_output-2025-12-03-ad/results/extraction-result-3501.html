<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3501 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3501</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3501</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-a550f576ff20b8cce98f3ddad0043d3783fbc9b4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a550f576ff20b8cce98f3ddad0043d3783fbc9b4" target="_blank">Abductive Commonsense Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This study introduces a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations, and conceptualizes two new tasks -- Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and Abduction NLG: a conditional generation task for explaining given observations in natural language.</p>
                <p><strong>Paper Abstract:</strong> Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks -- (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform--despite their strong performance on the related but more narrowly defined task of entailment NLI--pointing to interesting avenues for future research.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3501.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3501.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-ft</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (fine-tuned variants: Hypothesis-only, O1-only, O2-only, Linear Chain, Fully Connected)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-Large transformer pre-trained and fine-tuned on the abductive tasks in this paper; evaluated in multiple input-format variants that implement different independence assumptions (hypothesis-only, single observation, linear-chain, fully-connected).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deep bidirectional Transformer (BERT-Large) pre-trained on masked language modeling and next-sentence prediction, then fine-tuned on the paper's αNLI classification task. Multiple input formats were used to encode independence assumptions (hypothesis-only, O1-only, O2-only, linear-chain using two φ networks, and fully-connected single-input).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Abductive NLI (αNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice abductive reasoning: given two observations O1 and O2, choose which of two candidate hypotheses is the more plausible explanation (non-monotonic, commonsense abduction rather than strict deductive entailment).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning BERT-Large on αNLI data using different input encodings to enforce independence assumptions (hypothesis-only, O1/O2-only, linear chain factorization, fully connected joint input). Also used as adversary in Adversarial Filtering (AF) during dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Best reported test accuracy on the filtered ART dataset: 68.9% (BERT-ft variant reported as best in text); table reports multiple numbers depending on AF regime: e.g., BERT-ft [Linear Chain] 68.9% ±0.5, BERT-ft [Fully Connected] 68.6% ±0.5 on final dataset; when used as adversary (different AF setting) BERT-ft [Fully Connected] reached 72.0% (GPT-adversary column). Standard deviations reported (e.g., ±0.5–1.4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Simple baselines: random ≈50% (two-way); majority ~50.1%; Infersent ~50.8%; ESIM+ELMo (entailment model) ~58.2% (table) / ~58.8% (text reported). Human performance: 91.4% (αNLI).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>BERT-ft substantially outperforms simple baselines and classical entailment models (e.g., Infersent, majority, random). Compared to ESIM+ELMo (58% range), BERT-ft improves roughly 10 percentage points on αNLI. However, still ~22–23 percentage points below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Large gap to humans (≈91.4% human vs ≈68.9% best model). Fails especially on numerical and spatial commonsense categories (BERT accuracies reported: Numerical 56.8%, Spatial 65.4%) and in cases where the incorrect hypothesis yields a plausible but less-likely narrative (BERT struggles most on 'Plausible' negative cases: 62.5% accuracy). Models plateau with dataset size (~10k examples). Performance degrades under adversarial filtering and differs by AF adversary used (AF disproportionately impacts the adversary model).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports ablations via input-format variants enforcing independence assumptions: hypothesis-only, O1-only, O2-only, linear-chain, fully-connected; generally the fully-connected model that jointly conditions on O1+O2 does best, and linear-chain assumptions tend to degrade performance in cases needing joint reasoning. The paper also analyzes categorical failures (numerical, spatial, emotional) and story-transition failure types (O1→h-, h-→O2, plausible negative). Learning curve analysis shows plateau after ~10k examples. Adversarial filtering (AF) experiments show dataset difficulty depends on adversary used (GPT vs BERT) and AF reduces model scores significantly compared to pre-filtered data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abductive Commonsense Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3501.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3501.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-ft / GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT (fine-tuned) / OpenAI GPT (Generative Pre-trained Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative pre-trained Transformer (GPT) fine-tuned for αNLI classification and used in adversary experiments; also baseline generative models (GPT2 family) used for αNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving language understanding by generative pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Left-to-right Transformer pre-trained as language model (OpenAI GPT), fine-tuned on αNLI classification by presenting context and hypothesis in particular concatenation format. Also used as an adversary in early AF experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Abductive NLI (αNLI) and used as adversary during dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>αNLI multiple-choice abductive reasoning (choose more plausible hypothesis given O1 and O2).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning of GPT on αNLI; used as an adversary in the adversarial filtering (AF) pipeline to produce challenging distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-ft reported test accuracies: 63.1% (on final ART dataset, R column); under GPT-adversary filtering column it achieved 52.6% (showing AF with GPT adversary reduced its own performance). Before AF (pre-filtered), OpenGPT scored ~80% (reported in analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to random/majority (≈50%) and classical NLI (ESIM+ELMo ≈58%), GPT-ft outperforms simple baselines but underperforms BERT-ft variants on the final filtered dataset (~63% vs ≈68.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improves over random/majority and older NLI models but is worse than best BERT-ft by ~5–6 percentage points on final αNLI test; AF can reduce GPT's accuracy significantly when GPT was used as the AF adversary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Substantially below human performance. Sensitive to adversarial filtering: using GPT as adversary reduces its own performance on the held-out test. Shows struggles similar to other pretrained LMs on joint abductive inference requiring integration of both observations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper contrasts GPT performance across AF regimes (GPT as adversary vs BERT as adversary) showing AF effects; reports input formatting choices for GPT (concatenation [START] O1+h [SEP] O2 [SEP]) and provides evidence that AF makes dataset more challenging for the adversary model and others.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abductive Commonsense Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3501.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3501.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2 / COMeT-augmented GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (and variants: GPT2-Fixed, COMeT-Txt+GPT2, COMeT-Emb+GPT2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-2 generative language model fine-tuned for αNLG; augmented variants integrate COMeT commonsense knowledge either as textual phrases or as embeddings appended to token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (and COMeT-augmented variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 language model fine-tuned to generate hypotheses h+ conditioned on O1 and O2. Two integration strategies with COMeT (a transformer trained on ATOMIC): (1) COMeT-Txt+GPT2 feeds textual COMeT outputs as text fields; (2) COMeT-Emb+GPT2 appends COMeT relation embeddings (nine relations × 2 observations) to token embeddings so the Transformer can attend to them.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Abductive NLG (αNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Conditional generation of a plausible explanatory hypothesis h+ given observations O1 and O2; evaluated with automatic metrics (BLEU, METEOR, ROUGE, CIDEr, BERT-Score) and human evaluation (correctness rate).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning GPT-2 on ART generation data; optional conditioning on COMeT commonsense inferences provided either as text or as learned embeddings appended to the input sequence (integrating ATOMIC knowledge via COMeT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Automatic metrics: best CIDEr ~33.54 (O1-O2-only baseline), best BLEU ~3.03 (COMeT-Emb+GPT2), METEOR up to 17.66, ROUGE ~22.93, BERT-Score up to 48.74. Human evaluation (percentage judged correct): GPT2-Fixed ~not reported/low; O1-O2-only 42.26% correctness; COMeT-Txt+GPT2 38.28%; COMeT-Emb+GPT2 44.56%. Human-written hypotheses correctness: 96.03%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Human ceiling: 96.03% correctness on αNLG; simple GPT2 baseline (O1-O2-only) achieved ~42.3% human correctness; GPT2-Fixed produced near-zero BLEU but low human correctness. COMeT-augmented variants gave small gains in automatic metrics and modest increase in human-judged correctness (COMeT-Emb+GPT2 best at 44.56%).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>COMeT embedding integration provided modest improvements over raw GPT2 fine-tuning in automatic metrics (slight BLEU/METEOR/CIDEr increases) and human correctness (≈2 percentage points over O1-O2-only baseline in best case), but overall still far below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Generative models struggle to produce commonsense-consistent, abductive hypotheses: best model correctness only ~45% vs 96% for humans. Models often fail to generate non-monotonic inferences or adhere to commonsense constraints; automatic metrics are very low (BLEU near zero to 3). COMeT integration helps slightly but does not close the large gap.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper compares GPT2 variants: GPT2-Fixed (not fine-tuned) vs fine-tuned O1-O2-only vs COMeT-Txt vs COMeT-Emb. COMeT-Emb+GPT2 yields best BLEU/METEOR/BERT-Score and best human correctness among models but still far from human. The paper also reports input-format details and that appending COMeT embeddings allows the model to attend to commonsense inferences; however gains are modest.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abductive Commonsense Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3501.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3501.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESIM+ELMo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ESIM model enhanced with ELMo contextual embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art (for NLI pre-BERT era) sentence-pair entailment model (ESIM) using ELMo embeddings, retrained as a baseline for αNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhanced lstm for natural language inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ESIM+ELMo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LSTM-based inter-sentence model (ESIM) augmented with deep contextualized ELMo embeddings, re-trained on the αNLI dataset as an entailment-model baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Abductive NLI (αNLI) (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as a baseline to test whether an entailment-style NLI model can solve abductive reasoning selection.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Retraining ESIM+ELMo on the αNLI data (no specialized abductive methods).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy in Table 1: 58.2% (in one AF column) and 50.8% (in alternate column); text reports ESIM+ELMo achieves ~58.8% on ART (showing substantial underperformance relative to humans and to fine-tuned LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to random/majority (~50%) and Infersent (~50.8%), ESIM+ELMo is slightly above random in one reporting but well below BERT-ft (~68.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Only modest improvement over trivial baselines; far worse than fine-tuned pretrained transformers, indicating entailment models do not transfer to abductive, commonsense selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails to capture abductive commonsense; performs substantially worse than human and fine-tuned LMs on αNLI. The paper notes that good performance on entailment datasets does not imply competence on abductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Used as a point of comparison to demonstrate that standard entailment NLI approaches need substantial augmentation to solve abductive tasks. No detailed ablation for ESIM in this paper beyond reported scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abductive Commonsense Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3501.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3501.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Infersent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InferSent sentence embedding model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sentence embedding model using a supervised BiLSTM max-pooling encoder (InferSent) used as a baseline for αNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Supervised learning of universal sentence representations from natural language inference data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InferSent</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Produces fixed sentence embeddings (BiLSTM with max-pooling) trained on NLI data (SNLI, etc.); used here as a baseline by embedding sentences and training a classifier on concatenated context+hypothesis embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Abductive NLI (αNLI) (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Baseline evaluation to check whether universal sentence representations transfer to abductive selection.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Train classifier on InferSent embeddings of O1, O2, and hypothesis concatenations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy ~50.1–50.8% (close to random/majority).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Comparable to random/majority baselines (~50%).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No meaningful improvement; indicates simple sentence embeddings are insufficient for abductive selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performs near chance, indicating inability to capture the commonsense abductive signals required by αNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Serves as a simple baseline; no further ablation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abductive Commonsense Reasoning', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Comet: Commonsense transformers for automatic knowledge graph construction <em>(Rating: 2)</em></li>
                <li>ATOMIC: an atlas of machine commonsense for if-then reasoning <em>(Rating: 2)</em></li>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Improving language understanding by generative pre-training <em>(Rating: 2)</em></li>
                <li>Enhanced lstm for natural language inference <em>(Rating: 1)</em></li>
                <li>A corpus and cloze evaluation for deeper understanding of commonsense stories <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3501",
    "paper_id": "paper-a550f576ff20b8cce98f3ddad0043d3783fbc9b4",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "BERT-ft",
            "name_full": "BERT (fine-tuned variants: Hypothesis-only, O1-only, O2-only, Linear Chain, Fully Connected)",
            "brief_description": "BERT-Large transformer pre-trained and fine-tuned on the abductive tasks in this paper; evaluated in multiple input-format variants that implement different independence assumptions (hypothesis-only, single observation, linear-chain, fully-connected).",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT (fine-tuned)",
            "model_description": "A deep bidirectional Transformer (BERT-Large) pre-trained on masked language modeling and next-sentence prediction, then fine-tuned on the paper's αNLI classification task. Multiple input formats were used to encode independence assumptions (hypothesis-only, O1-only, O2-only, linear-chain using two φ networks, and fully-connected single-input).",
            "model_size": null,
            "reasoning_task_name": "Abductive NLI (αNLI)",
            "reasoning_task_description": "Multiple-choice abductive reasoning: given two observations O1 and O2, choose which of two candidate hypotheses is the more plausible explanation (non-monotonic, commonsense abduction rather than strict deductive entailment).",
            "method_or_intervention": "Fine-tuning BERT-Large on αNLI data using different input encodings to enforce independence assumptions (hypothesis-only, O1/O2-only, linear chain factorization, fully connected joint input). Also used as adversary in Adversarial Filtering (AF) during dataset construction.",
            "performance": "Best reported test accuracy on the filtered ART dataset: 68.9% (BERT-ft variant reported as best in text); table reports multiple numbers depending on AF regime: e.g., BERT-ft [Linear Chain] 68.9% ±0.5, BERT-ft [Fully Connected] 68.6% ±0.5 on final dataset; when used as adversary (different AF setting) BERT-ft [Fully Connected] reached 72.0% (GPT-adversary column). Standard deviations reported (e.g., ±0.5–1.4).",
            "baseline_performance": "Simple baselines: random ≈50% (two-way); majority ~50.1%; Infersent ~50.8%; ESIM+ELMo (entailment model) ~58.2% (table) / ~58.8% (text reported). Human performance: 91.4% (αNLI).",
            "improvement_over_baseline": "BERT-ft substantially outperforms simple baselines and classical entailment models (e.g., Infersent, majority, random). Compared to ESIM+ELMo (58% range), BERT-ft improves roughly 10 percentage points on αNLI. However, still ~22–23 percentage points below human performance.",
            "limitations_or_failures": "Large gap to humans (≈91.4% human vs ≈68.9% best model). Fails especially on numerical and spatial commonsense categories (BERT accuracies reported: Numerical 56.8%, Spatial 65.4%) and in cases where the incorrect hypothesis yields a plausible but less-likely narrative (BERT struggles most on 'Plausible' negative cases: 62.5% accuracy). Models plateau with dataset size (~10k examples). Performance degrades under adversarial filtering and differs by AF adversary used (AF disproportionately impacts the adversary model).",
            "ablation_or_analysis": "Paper reports ablations via input-format variants enforcing independence assumptions: hypothesis-only, O1-only, O2-only, linear-chain, fully-connected; generally the fully-connected model that jointly conditions on O1+O2 does best, and linear-chain assumptions tend to degrade performance in cases needing joint reasoning. The paper also analyzes categorical failures (numerical, spatial, emotional) and story-transition failure types (O1→h-, h-→O2, plausible negative). Learning curve analysis shows plateau after ~10k examples. Adversarial filtering (AF) experiments show dataset difficulty depends on adversary used (GPT vs BERT) and AF reduces model scores significantly compared to pre-filtered data.",
            "uuid": "e3501.0",
            "source_info": {
                "paper_title": "Abductive Commonsense Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "GPT-ft / GPT",
            "name_full": "GPT (fine-tuned) / OpenAI GPT (Generative Pre-trained Transformer)",
            "brief_description": "Generative pre-trained Transformer (GPT) fine-tuned for αNLI classification and used in adversary experiments; also baseline generative models (GPT2 family) used for αNLG.",
            "citation_title": "Improving language understanding by generative pre-training",
            "mention_or_use": "use",
            "model_name": "GPT (fine-tuned)",
            "model_description": "Left-to-right Transformer pre-trained as language model (OpenAI GPT), fine-tuned on αNLI classification by presenting context and hypothesis in particular concatenation format. Also used as an adversary in early AF experiments.",
            "model_size": null,
            "reasoning_task_name": "Abductive NLI (αNLI) and used as adversary during dataset construction",
            "reasoning_task_description": "αNLI multiple-choice abductive reasoning (choose more plausible hypothesis given O1 and O2).",
            "method_or_intervention": "Fine-tuning of GPT on αNLI; used as an adversary in the adversarial filtering (AF) pipeline to produce challenging distractors.",
            "performance": "GPT-ft reported test accuracies: 63.1% (on final ART dataset, R column); under GPT-adversary filtering column it achieved 52.6% (showing AF with GPT adversary reduced its own performance). Before AF (pre-filtered), OpenGPT scored ~80% (reported in analysis).",
            "baseline_performance": "Compared to random/majority (≈50%) and classical NLI (ESIM+ELMo ≈58%), GPT-ft outperforms simple baselines but underperforms BERT-ft variants on the final filtered dataset (~63% vs ≈68.9%).",
            "improvement_over_baseline": "Improves over random/majority and older NLI models but is worse than best BERT-ft by ~5–6 percentage points on final αNLI test; AF can reduce GPT's accuracy significantly when GPT was used as the AF adversary.",
            "limitations_or_failures": "Substantially below human performance. Sensitive to adversarial filtering: using GPT as adversary reduces its own performance on the held-out test. Shows struggles similar to other pretrained LMs on joint abductive inference requiring integration of both observations.",
            "ablation_or_analysis": "Paper contrasts GPT performance across AF regimes (GPT as adversary vs BERT as adversary) showing AF effects; reports input formatting choices for GPT (concatenation [START] O1+h [SEP] O2 [SEP]) and provides evidence that AF makes dataset more challenging for the adversary model and others.",
            "uuid": "e3501.1",
            "source_info": {
                "paper_title": "Abductive Commonsense Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "GPT2 / COMeT-augmented GPT2",
            "name_full": "GPT-2 (and variants: GPT2-Fixed, COMeT-Txt+GPT2, COMeT-Emb+GPT2)",
            "brief_description": "GPT-2 generative language model fine-tuned for αNLG; augmented variants integrate COMeT commonsense knowledge either as textual phrases or as embeddings appended to token embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (and COMeT-augmented variants)",
            "model_description": "GPT-2 language model fine-tuned to generate hypotheses h+ conditioned on O1 and O2. Two integration strategies with COMeT (a transformer trained on ATOMIC): (1) COMeT-Txt+GPT2 feeds textual COMeT outputs as text fields; (2) COMeT-Emb+GPT2 appends COMeT relation embeddings (nine relations × 2 observations) to token embeddings so the Transformer can attend to them.",
            "model_size": null,
            "reasoning_task_name": "Abductive NLG (αNLG)",
            "reasoning_task_description": "Conditional generation of a plausible explanatory hypothesis h+ given observations O1 and O2; evaluated with automatic metrics (BLEU, METEOR, ROUGE, CIDEr, BERT-Score) and human evaluation (correctness rate).",
            "method_or_intervention": "Fine-tuning GPT-2 on ART generation data; optional conditioning on COMeT commonsense inferences provided either as text or as learned embeddings appended to the input sequence (integrating ATOMIC knowledge via COMeT).",
            "performance": "Automatic metrics: best CIDEr ~33.54 (O1-O2-only baseline), best BLEU ~3.03 (COMeT-Emb+GPT2), METEOR up to 17.66, ROUGE ~22.93, BERT-Score up to 48.74. Human evaluation (percentage judged correct): GPT2-Fixed ~not reported/low; O1-O2-only 42.26% correctness; COMeT-Txt+GPT2 38.28%; COMeT-Emb+GPT2 44.56%. Human-written hypotheses correctness: 96.03%.",
            "baseline_performance": "Human ceiling: 96.03% correctness on αNLG; simple GPT2 baseline (O1-O2-only) achieved ~42.3% human correctness; GPT2-Fixed produced near-zero BLEU but low human correctness. COMeT-augmented variants gave small gains in automatic metrics and modest increase in human-judged correctness (COMeT-Emb+GPT2 best at 44.56%).",
            "improvement_over_baseline": "COMeT embedding integration provided modest improvements over raw GPT2 fine-tuning in automatic metrics (slight BLEU/METEOR/CIDEr increases) and human correctness (≈2 percentage points over O1-O2-only baseline in best case), but overall still far below human performance.",
            "limitations_or_failures": "Generative models struggle to produce commonsense-consistent, abductive hypotheses: best model correctness only ~45% vs 96% for humans. Models often fail to generate non-monotonic inferences or adhere to commonsense constraints; automatic metrics are very low (BLEU near zero to 3). COMeT integration helps slightly but does not close the large gap.",
            "ablation_or_analysis": "Paper compares GPT2 variants: GPT2-Fixed (not fine-tuned) vs fine-tuned O1-O2-only vs COMeT-Txt vs COMeT-Emb. COMeT-Emb+GPT2 yields best BLEU/METEOR/BERT-Score and best human correctness among models but still far from human. The paper also reports input-format details and that appending COMeT embeddings allows the model to attend to commonsense inferences; however gains are modest.",
            "uuid": "e3501.2",
            "source_info": {
                "paper_title": "Abductive Commonsense Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "ESIM+ELMo",
            "name_full": "ESIM model enhanced with ELMo contextual embeddings",
            "brief_description": "A state-of-the-art (for NLI pre-BERT era) sentence-pair entailment model (ESIM) using ELMo embeddings, retrained as a baseline for αNLI.",
            "citation_title": "Enhanced lstm for natural language inference",
            "mention_or_use": "use",
            "model_name": "ESIM+ELMo",
            "model_description": "An LSTM-based inter-sentence model (ESIM) augmented with deep contextualized ELMo embeddings, re-trained on the αNLI dataset as an entailment-model baseline.",
            "model_size": null,
            "reasoning_task_name": "Abductive NLI (αNLI) (baseline comparison)",
            "reasoning_task_description": "Used as a baseline to test whether an entailment-style NLI model can solve abductive reasoning selection.",
            "method_or_intervention": "Retraining ESIM+ELMo on the αNLI data (no specialized abductive methods).",
            "performance": "Reported accuracy in Table 1: 58.2% (in one AF column) and 50.8% (in alternate column); text reports ESIM+ELMo achieves ~58.8% on ART (showing substantial underperformance relative to humans and to fine-tuned LMs).",
            "baseline_performance": "Compared to random/majority (~50%) and Infersent (~50.8%), ESIM+ELMo is slightly above random in one reporting but well below BERT-ft (~68.9%).",
            "improvement_over_baseline": "Only modest improvement over trivial baselines; far worse than fine-tuned pretrained transformers, indicating entailment models do not transfer to abductive, commonsense selection.",
            "limitations_or_failures": "Fails to capture abductive commonsense; performs substantially worse than human and fine-tuned LMs on αNLI. The paper notes that good performance on entailment datasets does not imply competence on abductive reasoning.",
            "ablation_or_analysis": "Used as a point of comparison to demonstrate that standard entailment NLI approaches need substantial augmentation to solve abductive tasks. No detailed ablation for ESIM in this paper beyond reported scores.",
            "uuid": "e3501.3",
            "source_info": {
                "paper_title": "Abductive Commonsense Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Infersent",
            "name_full": "InferSent sentence embedding model",
            "brief_description": "A sentence embedding model using a supervised BiLSTM max-pooling encoder (InferSent) used as a baseline for αNLI.",
            "citation_title": "Supervised learning of universal sentence representations from natural language inference data",
            "mention_or_use": "use",
            "model_name": "InferSent",
            "model_description": "Produces fixed sentence embeddings (BiLSTM with max-pooling) trained on NLI data (SNLI, etc.); used here as a baseline by embedding sentences and training a classifier on concatenated context+hypothesis embeddings.",
            "model_size": null,
            "reasoning_task_name": "Abductive NLI (αNLI) (baseline)",
            "reasoning_task_description": "Baseline evaluation to check whether universal sentence representations transfer to abductive selection.",
            "method_or_intervention": "Train classifier on InferSent embeddings of O1, O2, and hypothesis concatenations.",
            "performance": "Reported accuracy ~50.1–50.8% (close to random/majority).",
            "baseline_performance": "Comparable to random/majority baselines (~50%).",
            "improvement_over_baseline": "No meaningful improvement; indicates simple sentence embeddings are insufficient for abductive selection.",
            "limitations_or_failures": "Performs near chance, indicating inability to capture the commonsense abductive signals required by αNLI.",
            "ablation_or_analysis": "Serves as a simple baseline; no further ablation reported.",
            "uuid": "e3501.4",
            "source_info": {
                "paper_title": "Abductive Commonsense Reasoning",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Comet: Commonsense transformers for automatic knowledge graph construction",
            "rating": 2
        },
        {
            "paper_title": "ATOMIC: an atlas of machine commonsense for if-then reasoning",
            "rating": 2
        },
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Improving language understanding by generative pre-training",
            "rating": 2
        },
        {
            "paper_title": "Enhanced lstm for natural language inference",
            "rating": 1
        },
        {
            "paper_title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "rating": 1
        }
    ],
    "cost": 0.015352999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AbDUCTIVE COMMONSENSE REASONING</h1>
<p>Chandra Bhagavatula ${ }^{\ominus}$, Ronan Le Bras ${ }^{\ominus}$, Chaitanya Malaviya ${ }^{\ominus}$, Keisuke Sakaguchi ${ }^{\ominus}$, Ari Holtzman ${ }^{\ominus}$, Hannah Rashkin ${ }^{\ominus}$, Doug Downey ${ }^{\ominus}$, Scott Wen-tau Yih ${ }^{\text {, }}$, Yejin Choi ${ }^{\ominus}{ }^{\circ}$<br>${ }^{\ominus}$ Allen Institute for AI, Seattle, WA, USA, ${ }^{\text {F }}$ Facebook AI, Seattle, WA, USA<br>${ }^{\circ}$ Paul G. Allen School of Computer Science \&amp; Engineering, WA, USA<br>{chandrab,ronanlb, chaitanyam, keisukes}@allenai.org<br>{arih, hannahr, dougd}@allenai.org<br>{yejin}@cs.washington.edu<br>{scottyih}@fb.com*</p>
<h4>Abstract</h4>
<p>Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, $\mathcal{A B}$, that consists of over 20 k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks - (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves $68.9 \%$ accuracy, well below human performance of $91.4 \%$. On Abductive $N L G$, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform-despite their strong performance on the related but more narrowly defined task of entailment NLI-pointing to interesting avenues for future research.</p>
<h2>1 INTRODUCTION</h2>
<p>The brain is an abduction machine, continuously trying to prove abductively that the observables in its environment constitute a coherent situation.</p>
<ul>
<li>Jerry Hobbs, ACL 2013 Lifetime Achievement Award ${ }^{1}$</li>
</ul>
<p>Abductive reasoning is inference to the most plausible explanation for incomplete observations (Peirce, 1965a). Figure 1 illustrates an example. Given the incomplete observations about the world that $O_{1}$ : "Jenny cleaned her house and went to work, leaving the window just a crack open." and sometime later $O_{2}$ : "When Jenny returned home, she saw her house was a mess.", we can hypothesize different potential explanations and reason about which is the most likely. We can readily rule out $H_{3}$ since it fails to justify the observation $O_{2}$. While $H_{1}$ and $H_{2}$ are both plausible, the most likely explanation based on commonsense is $H_{1}$ as $H_{2}$ is somewhat implausible given $O_{1}$.
One crucial observation Peirce makes about abductive reasoning is that abduction is "the only logical operation which introduces any new ideas", which contrasts with other types of inference such as entailment, that focuses on inferring only such information that is already provided in the premise.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of Abductive Reasoning. Given observations $O_{1}$ and $O_{2}$, the $\alpha$ NLI task is to select the most plausible explanatory hypothesis. Since the number of hypotheses is massive in any given situation, we make a simplifying assumption in our $\mathcal{R} \mathcal{R} \mathcal{I}$ dataset to only choose between a pair of explanations.</p>
<p>Abductive reasoning has long been considered to be at the core of understanding narratives (Hobbs et al., 1988), reading between the lines (Norvig, 1987; Charniak \&amp; Shimony, 1990), reasoning about everyday situations (Peirce, 1965b; Andersen, 1973), and counterfactual reasoning (Pearl, 2002; Pearl \&amp; Mackenzie, 2018). Despite the broad recognition of its importance, however, the study of abductive reasoning in narrative text has very rarely appeared in the NLP literature, in large part because most previous work on abductive reasoning has focused on formal logic, which has proven to be too rigid to generalize to the full complexity of natural language.</p>
<p>In this paper, we present the first study to investigate the viability of language-based abductive reasoning. This shift from logic-based to language-based reasoning draws inspirations from a significant body of work on language-based entailment (Bowman et al., 2015; Williams et al., 2018b), language-based logic (Lakoff, 1970; MacCartney \&amp; Manning, 2007), and language-based commonsense reasoning (Mostafazadeh et al., 2016; Zellers et al., 2018). In particular, we investigate the use of natural language as the representation medium, and probe deep neural models on language-based abductive reasoning.</p>
<p>More concretely, we propose Abductive Natural Language Inference ( $\alpha$ NLI) and Abductive Natural Language Generation ( $\alpha$ NLG) as two novel reasoning tasks in narrative contexts. ${ }^{2}$ We formulate $\alpha$ NLI as a multiple-choice task to support easy and reliable automatic evaluation: given a context, the task is to choose the more likely explanation from a given pair of hypotheses choices. We also introduce a new challenge dataset, $\mathcal{R} \mathcal{R} \mathcal{I}$, that consists of 20 K narratives accompanied by over 200 K explanatory hypothesis. ${ }^{34}$ We then establish comprehensive baseline performance based on state-of-the-art NLI and language models. The best baseline for $\alpha$ NLI based on BERT achieves $68.9 \%$ accuracy, with a considerable gap compared to human performance of $91.4 \%(\S 5.2)$. The best generative model, based on GPT2, performs well below human performance on the $\alpha$ NLG task (§5.2). Our analysis leads to insights into the types of reasoning that deep pre-trained language models fail to perform - despite their strong performance on the closely related but different task of entailment NLI - pointing to future research directions.</p>
<h1>2 TASK DEFINITION</h1>
<p>Abductive Natural Language Inference We formulate $\alpha$ NLI as multiple choice problems consisting of a pair of observations as context and a pair of hypothesis choices. Each instance in $\mathcal{R} \mathcal{R} \mathcal{I}$ is defined as follows:</p>
<ul>
<li>$O_{1}$ : The observation at time $t_{1}$.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>$O_{2}$ : The observation at time $t_{2}&gt;t_{1}$.</li>
<li>$h^{+}$: A plausible hypothesis that explains the two observations $O_{1}$ and $O_{2}$.</li>
<li>$h^{-}$: An implausible (or less plausible) hypothesis for observations $O_{1}$ and $O_{2}$.</li>
</ul>
<p>Given the observations and a pair of hypotheses, the $\alpha$ NLI task is to select the most plausible explanation (hypothesis).</p>
<p>Abductive Natural Language Generation $\alpha$ NLG is the task of generating a valid hypothesis $h^{+}$ given the two observations $O_{1}$ and $O_{2}$. Formally, the task requires to maximize $P\left(h^{+}\left|O_{1}, O_{2}\right)\right.$.</p>
<h1>3 MODELS FOR ABDUCTIVE COMMONSENSE REASONING</h1>
<h3>3.1 AbDUCTIVE NATURAL LANGUAGE INFERENCE</h3>
<p>A Probabilistic Framework for $\alpha$ NLI: A distinct feature of the $\alpha$ NLI task is that it requires jointly considering all available observations and their commonsense implications, to identify the correct hypothesis. Formally, the $\alpha$ NLI task is to select the hypothesis $h^{*}$ that is most probable given the observations.</p>
<p>$$
h^{*}=\arg \max <em 1="1">{h^{i}} P\left(H=h^{i} \mid O</em>\right)
$$}, O_{2</p>
<p>Rewriting the objective using Bayes Rule conditioned on $O_{1}$, we have:</p>
<p>$$
P\left(h^{i} \mid O_{1}, O_{2}\right) \propto P\left(O_{2} \mid h^{i}, O_{1}\right) P\left(h^{i} \mid O_{1}\right)
$$</p>
<p>We formulate a set of probabilistic models for $\alpha$ NLI that make various independence assumptions on Equation 2 - starting from a simple baseline that ignores the observations entirely, and building up to a fully joint model. These models are depicted as Bayesian Networks in Figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the graphical models described in the probabilistic framework. The "Fully Connected" model can, in theory, combine information from both available observations.</p>
<p>Hypothesis Only: Our simplest model makes the strong assumption that the hypothesis is entirely independent of both observations, i.e. $\left(H \perp O_{1}, O_{2}\right)$, in which case we simply aim to maximize the marginal $P(H)$.</p>
<p>First (or Second) Observation Only: Our next two models make weaker assumptions: that the hypothesis depends on only one of the first $O_{1}$ or second $O_{2}$ observation.</p>
<p>Linear Chain: Our next model uses both observations, but considers each observation's influence on the hypothesis independently, i.e. it does not combine information across the observations. Formally, the model assumes that the three variables $\left\langle O_{1}, H, O_{2}\right\rangle$ form a linear Markov chain, where the second observation is conditionally independent of the first, given the hypothesis (i.e. $\left.\left(O_{1} \perp O_{2} \mid H\right)\right)$. Under this assumption, we aim to maximize a somewhat simpler objective than Equation 2:</p>
<p>$$
h^{*}=\arg \max <em 2="2">{h^{i}} P\left(O</em> \mid H\right)
$$} \mid h^{i}\right) P\left(h^{i} \mid O_{1}\right) \text { where }\left(O_{1} \perp O_{2</p>
<p>Fully Connected: Finally, our most sophisticated model jointly models all three random variables as in Equation 2, and can in principle combine information across both observations to choose the correct hypothesis.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overview of an $\alpha$ NLG model that integrates commonsense representations obtained from COMeT (Bosselut et al., 2019) with GPT2. Each observation is input to the COMeT model to obtain nine embeddings, each associated with one commonsense inference type.</p>
<p>To help illustrate the subtle distinction between how the Linear Chain and Fully Connected models consider both observations, consider the following example. Let observation $O_{1}$ : "Carl went to the store desperately searching for flour tortillas for a recipe." and $O_{2}$ : "Carl left the store very frustrated.". Then consider two distinct hypotheses, an incorrect $h^{1}$ : "The cashier was rude" and the correct $h^{2}$ : "The store had corn tortillas, but not flour ones.". For this example, a Linear Chain model could arrive at the wrong answer, because it reasons about the observations separately-taking $O_{1}$ in isolation, both $h^{1}$ and $h^{2}$ seem plausible next events, albeit each a priori unlikely. And for $O_{2}$ in isolation-i.e. in the absence of $O_{1}$, as for a randomly drawn shopper-the $h^{1}$ explanation of a rude cashier seems a much more plausible explanation of Carl's frustration than are the details of the store's tortilla selection. Combining these two separate factors leads the Linear Chain to select $h^{1}$ as the more plausible explanation. It is only by reasoning about Carl's goal in $O_{1}$ jointly with his frustration in $O_{2}$, as in the Fully Connected model, that we arrive at the correct answer $h^{2}$ as the more plausible explanation.</p>
<p>In our experiments, we encode the different independence assumptions in the best performing neural network model. For the hypothesis-only and single observation models, we can enforce the independencies by simply restricting the inputs of the model to only the relevant variables. On the other hand, the Linear Chain model takes all three variables as input, but we restrict the form of the model to enforce the conditional independence. Specifically, we learn a discriminative classifier:</p>
<p>$$
P_{\text {Linear Chain }}\left(h \mid O_{1}, O_{2}\right) \propto e^{\phi\left(O_{1}, h\right)+\phi^{\prime}\left(h, O_{2}\right)}
$$</p>
<p>where $\phi$ and $\phi^{\prime}$ are neural networks that produce scalar values.</p>
<h1>3.2 AbDUCTIVE NATURAL LANGUAGE GENERATION</h1>
<p>Given $h^{+}=\left{w_{1}^{h} \ldots w_{l}^{h}\right}, O_{1} \equiv\left{w_{1}^{o 1} \ldots w_{m}^{o 1}\right}$ and $O_{2} \equiv\left{w_{1}^{o 2} \ldots w_{n}^{o 2}\right}$ as sequences of tokens, the $\alpha$ NLG task can be modeled as $P\left(h^{+} \mid O_{1}, O_{2}\right)=\prod P\left(w_{i}^{h} \mid w_{&lt;i}^{h}, w_{1}^{o 1} \ldots w_{m}^{o 1}, w_{1}^{o 2} \ldots w_{n}^{o 2}\right)$ Optionally, the model can also be conditioned on background knowledge $\mathcal{K}$. Parameterized models can then be trained to minimize the negative log-likelihood over instances in $\mathcal{A R T}$ :</p>
<p>$$
\mathcal{L}=-\sum_{i=1}^{N} \log P\left(w_{i}^{h} \mid w_{&lt;i}^{h}, w_{1}^{o 1} \ldots w_{m}^{o 1}, w_{1}^{o 2} \ldots w_{n}^{o 2}, \mathcal{K}\right)
$$</p>
<h2>4 米T DATASET: AbDUCTIVE REASONING IN NARRATIVE TEXT</h2>
<p>$\mathcal{A R T}$ is the first large-scale benchmark dataset for studying abductive reasoning in narrative texts. It consists of $\sim 20 \mathrm{~K}$ narrative contexts (pairs of observations $\left\langle O_{1}, O_{2}\right\rangle$ ) with over 200K explanatory hypotheses. Table 6 in the Appendix summarizes corpus-level statistics of the $\mathcal{A R T}$ dataset. ${ }^{5}$ Figure 4 shows some illustrative examples from $\mathcal{A R T}$ (dev split). The best model based on BERT fails to correctly predict the first two dev examples.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Examples from $\mathcal{R} \mathcal{T}$ (dev split). The best model based on BERT fails to correctly predict the first two examples.</p>
<p>Collecting Observations: The pairs $O_{1}, O_{2}$ in $\mathcal{R} \mathcal{T}$ are drawn from the ROCStories dataset (Mostafazadeh et al., 2016). ROCStories is a large collection of short, manually curated fivesentence stories. It was designed to have a clear beginning and ending for each story, which naturally map to the first $\left(O_{1}\right)$ and second $\left(O_{2}\right)$ observations in $\mathcal{R} \mathcal{T}$.</p>
<p>Collecting Hypotheses Options: We crowdsourced the plausible and implausible hypotheses options on Amazon Mechanical Turk (AMT) in two separate tasks ${ }^{6}$ :</p>
<ol>
<li>Plausible Hypothesis Options: We presented $O_{1}$ and $O_{2}$ as narrative context to crowdworkers who were prompted to fill in "What happened in-between?" in natural language. The design of the task motivates the use of abductive reasoning to hypothesize likely explanations for the two given observations.</li>
<li>Implausible Hypothesis Options: In this task, we presented workers with observations $O_{1}, O_{2}$ and one plausible hypothesis option $h^{+} \in \mathcal{H}^{+}$collected from the previous task. Crowdworkers were instructed to make minimal edits (up to 5 words) to a given $h^{+}$to create implausible hypothesis variations for each plausible hypothesis.</li>
</ol>
<p>A significant challenge in creating datasets is avoiding annotation artifacts - unintentional patterns in the data that leak information about the target label - that several recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have reported on crowdsourced datasets. To tackle this challenge, we collect multiple plausible and implausible hypotheses for each $\left\langle O_{1}, O_{2}\right\rangle$ pair (as described above) and then apply an adversarial filtering algorithm to retain one challenging pair of hypotheses that are hard to distinguish between. We describe our algorithm in detail in Appendix A.5. While our final dataset uses BERT as the adversary, preliminary experiments that used GPT as an adversary resulted in similar drops in performance of all models, including all BERT variants. We compare the results of the two adversaries in Table 1.</p>
<h1>5 EXPERIMENTS AND RESULTS</h1>
<p>We now present our evaluation of finetuned state-of-the-art pre-trained language models on the $\mathcal{R} \mathcal{T}$ dataset, and several other baseline systems for both $\alpha$ NLI and $\alpha$ NLG. Since $\alpha$ NLI is framed as a binary classification problem, we choose accuracy as our primary metric. For $\alpha$ NLG, we report performance on automated metrics such as BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee \&amp; Lavie, 2005) and also report human evaluation results.</p>
<h3>5.1 AbDUCTIVE NATURAL LANGUAGE INFERENCE</h3>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">GPT AF</th>
<th style="text-align: right;">$\mathcal{R}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random (2-way choice)</td>
<td style="text-align: right;">Acc. (\%)</td>
<td style="text-align: right;">Acc. (\%)</td>
</tr>
<tr>
<td style="text-align: left;">Majority (from dev set)</td>
<td style="text-align: right;">50.1</td>
<td style="text-align: right;">50.4</td>
</tr>
<tr>
<td style="text-align: left;">Infersent (Conneau et al., 2017)</td>
<td style="text-align: right;">50.1</td>
<td style="text-align: right;">50.8</td>
</tr>
<tr>
<td style="text-align: left;">ESIM+ELMo (Chen et al., 2017)</td>
<td style="text-align: right;">58.2</td>
<td style="text-align: right;">50.8</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning Pre-trained LMs</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-ft</td>
<td style="text-align: right;">$52.6(0.9)$</td>
<td style="text-align: right;">$63.1(0.5)$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [ $h^{1}$ Only]</td>
<td style="text-align: right;">$55.9(0.7)$</td>
<td style="text-align: right;">$59.5(0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [ $O_{1}$ Only]</td>
<td style="text-align: right;">$63.9(0.8)$</td>
<td style="text-align: right;">$63.5(0.7)$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [ $O_{2}$ Only]</td>
<td style="text-align: right;">$68.1(0.6)$</td>
<td style="text-align: right;">$66.6(0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [Linear Chain]</td>
<td style="text-align: right;">$65.3(1.4)$</td>
<td style="text-align: right;">$68.9(0.5)$</td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [Fully Connected]</td>
<td style="text-align: right;">$72.0(0.5)$</td>
<td style="text-align: right;">$68.6(0.5)$</td>
</tr>
<tr>
<td style="text-align: left;">Human Performance</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">91.4</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance of baselines and finetuned-LM approaches on the test set of $\mathcal{R} \mathcal{T}$. Test accuracy is reported as the mean of five models trained with random seeds, with the standard deviation in parenthesis.</p>
<p>Despite strong performance on several other NLP benchmark datasets, the best baseline model based on BERT achieves an accuracy of just $68.9 \%$ on $\mathcal{R} \mathcal{T}$ compared to human performance of $91.4 \%$. The large gap between human performance and that of the best system provides significant scope for development of more sophisticated abductive reasoning models. Our experiments show that introducing the additional independence assumptions described in Section 3.1 over the fully connected model tends to degrade system performance (see Table 1) in general.</p>
<p>Human Performance We compute human performance using AMT. Each instance (two observations and two hypothesis choices) is shown to three workers who were prompted to choose the more plausible hypothesis choice. ${ }^{7}$ We compute majority vote on the labels assigned which leads to a human accuracy of $91.4 \%$ on the $\mathcal{R} \mathcal{T}$ test set.</p>
<p>Baselines We include baselines that rely on simple features to verify that $\mathcal{R} \mathcal{T}$ is not trivially solvable due to noticeable annotation artifacts, observed in several crowdsourced datasets. The accuracies of all simple baselines are close to chance-performance on the task - indicating that the dataset is free of simple annotation artifacts.</p>
<p>A model for the related but distinct task of entailment NLI (e.g. SNLI) forms a natural baseline for $\alpha$ NLI. We re-train the ESIM+ELMo (Chen et al., 2017; Peters et al., 2018) model as its performance on entailment NLI ( $88.9 \%$ ) is close to state-of-the-art models (excluding pre-trained language models). This model only achieves an accuracy of $58.8 \%$ highlighting that performing well on $\mathcal{R} \mathcal{T}$ requires models to go far beyond the linguistic notion of entailment.</p>
<p>Pre-trained Language Models BERT (Devlin et al., 2018) and GPT (Radford, 2018) have recently been shown to achieve state-of-the-art results on several NLP benchmarks (Wang et al., 2018). We finetune both BERT-Large and GPT as suggested in previous work and we present each instance in their natural narrative order. BERT-ft (fully connected) is the best performing model achieving $68.9 \%$ accuracy, compared to GPT's $63.1 \% .{ }^{8}$ Our AF approach was able to reduce BERT performance from over $88 \%$ by 20 points.</p>
<p>Learning Curve and Dataset Size While there is enough scope for considerably scaling up the dataset based on ROCStories, the learning curve in Figure
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: BERT learning curve on the $d e v$ set of $\mathcal{R} \mathcal{T}$. For each point on the x -axis, we fine-tune BERT with five random seeds. Human performance is $91.4 \%$.</p>
<p>5 shows that the performance of the best model plateaus after $\sim 10,000$ instances. In addition, there is still a wide gap $(\sim 23 \%)$ between the performance of the best model and human performance.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">ROUGE</th>
<th style="text-align: center;">CIDEr</th>
<th style="text-align: center;">BERT-Score</th>
<th style="text-align: center;">Human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT2-Fixed</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.29</td>
<td style="text-align: center;">9.99</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">36.69</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">$O_{1}-O_{2}$-Only</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">16.71</td>
<td style="text-align: center;">22.83</td>
<td style="text-align: center;">$\mathbf{3 3 . 5 4}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 7 4}$</td>
<td style="text-align: center;">42.26</td>
</tr>
<tr>
<td style="text-align: left;">COMeT-Txt+GPT2</td>
<td style="text-align: center;">2.29</td>
<td style="text-align: center;">16.73</td>
<td style="text-align: center;">22.51</td>
<td style="text-align: center;">31.99</td>
<td style="text-align: center;">48.46</td>
<td style="text-align: center;">38.28</td>
</tr>
<tr>
<td style="text-align: left;">COMeT-Emb+GPT2</td>
<td style="text-align: center;">$\mathbf{3 . 0 3}$</td>
<td style="text-align: center;">$\mathbf{1 7 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 9 3}$</td>
<td style="text-align: center;">32.00</td>
<td style="text-align: center;">48.52</td>
<td style="text-align: center;">$\mathbf{4 4 . 5 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Human-written Hypotheses</td>
<td style="text-align: center;">8.25</td>
<td style="text-align: center;">26.71</td>
<td style="text-align: center;">30.40</td>
<td style="text-align: center;">53.56</td>
<td style="text-align: center;">53.30</td>
<td style="text-align: center;">96.03</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of generative models on the test set of $\mathcal{R} \mathcal{T}$. All models except GPT2-Fixed are finetuned on $\mathcal{R} \mathcal{T}$.</p>
<p>GPT Adversary Table 1 also includes results of our experiments where GPT was used as the adversary. Notably, in this case, adversarially filtering the dataset brings down GPT performance under $53 \%$. On the other hand, the best BERT model, that encodes the fully connected bayesian network performs significantly better than the BERT model that encodes the linear chain assumptions $-72 \%$ compared to $65 \%$. Therefore, we use the BERT fully connected model as the adversary in $\mathcal{R} \mathcal{T}$. The gap between the linear chain and fully connected BERT models diminishes when BERT is used as an adversary - in spite of being a more powerful model - which indicates that adversarial filtering disproportionately impacts the model used as the adversary. However, the dataset also becomes more difficult for the other models that were not used as adversaries. For example, before any filtering, BERT scores $88 \%$ and OpenGPT gets $80 \%$, which is much higher than either model achieves in Table 1 when the other model is used for filtering. This result is a reasonable indicator, albeit not a guarantee, that $\mathcal{R} \mathcal{T}$ will remain challenging for new models released in the future.</p>
<h1>5.2 AbDUCTIVE NATURAL LANGUAGE GENERATION</h1>
<p>Generative Language Models As described in Equation 4, we train GPT2 conditioned on the tokens of the two observations $O_{1}$ and $O_{2}$. Both observations are enclosed with field-specific tags. ATOMIC (Sap et al., 2019), a repository of inferential if-then knowledge is a natural source of background commonsense required to reason about narrative contexts in $\mathcal{R} \mathcal{T}$. Yet, there is no straightforward way to include such knowledge into a neural model as ATOMIC's nodes are not canonicalized and are represented as short phrases of text. Thus, we rely on COMeT - a transformer model trained on ATOMIC that generates nine commonsense inferences of events in natural language. ${ }^{9}$ Specifically, we experiment with two ways of integrating information from COMeT in GPT2: (i) as textual phrases, and (ii) as embeddings.
Figure 3 shows how we integrate COMeT representations. Concretely, after the input tokens are embedded by the word-embedding layer, we append eighteen (corresponding to nine relations for each observation) embeddings to the sequence before passing through the layers of the Transformer architecture. This allows the model to learn each token's representation while attending to the COMeT embeddings - effectively integrating background commonsense knowledge into a language model. ${ }^{10}$</p>
<p>Discussion Table 2 reports results on the $\alpha$ NLG task. Among automatic metrics, we report BLEU4 (Papineni et al., 2002), METEOR (Banerjee \&amp; Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and BERT-Score (Zhang et al., 2019) (with the bert-base-uncased model). We establish human performance through crowdsourcing on AMT. Crowdworkers are shown pairs of observations and a generated hypothesis and asked to label whether the hypothesis explains the given observations. The last column reports the human evaluation score. The last row reports the score of a held-out human-written hypothesis and serves as a ceiling for model performance. Human-written hypotheses are found to be correct for $96 \%$ of instances, while our best generative models, even when enhanced with background commonsense knowledge, only achieve $45 \%$ indicating that the $\alpha$ NLG generation task is especially challenging for current state-of-the-art text generators.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>6 ANALYSIS</h1>
<h2>$6.1 \quad \alpha \mathrm{NLI}$</h2>
<p>Commonsense reasoning categories We investigate the categories of commonsense-based abductive reasoning that are challenging for current systems and the ones where the best model over-performs. While there have been previous attempts to categorize commonsense knowledge required for entailment (LoBue \&amp; Yates, 2011; Clark et al., 2007), crowdsourcing this task at scale with high fidelity and high agreement across annotators remains challenging. Instead, we aim to probe the model with soft categories identified by matching lists of category-specific keywords to the hypothesis choices.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: center;">Human <br> Accuracy</th>
<th style="text-align: center;">BERT <br> Accuracy</th>
<th style="text-align: center;">$\Delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">All $(1,000)$</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">22.6</td>
</tr>
<tr>
<td style="text-align: left;">Numerical $(44)$</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">21.8</td>
</tr>
<tr>
<td style="text-align: left;">Spatial $(130)$</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: left;">Emotional $(84)$</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">14.3</td>
</tr>
</tbody>
</table>
<p>Table 3: BERT's performance and human evaluation on categories for 1,000 instances from the test set, based on commonsense reasoning domains (Numerical, Spatial, Emotional). The number in parenthesis indicates the size of the category.</p>
<p>Table 3 shows the accuracy of the best model (BERT-ft) across various categories of commonsense knowledge. BERT-ft significantly underperforms on instances involving Numerical ( $56.8 \%$ ) and Spatial ( $65.4 \%$ ) commonsense. These two categories include reasoning about numerical quantities and the spatial location of agents and objects, and highlight some of the limitations of the language models. In contrast, it significantly overperforms on the Emotional category ( $72.6 \%$ ) where the hypotheses exhibit strong textual cues about emotions and sentiments.</p>
<p>Implausible transitions A model for an instance of the $\mathcal{R} T$ dataset should discard implausible hypotheses in the context of the two given observations. In narrative contexts, there are three main reasons for an implausible hypothesis to be labeled as such:</p>
<ol>
<li>$O_{1} \nrightarrow h^{-}: h^{-}$is unlikely to follow after the first observation $O_{1}$.</li>
<li>$h^{-} \nrightarrow O_{2}: h^{-}$is plausible after $O_{1}$ but unlikely to precede the second observation $O_{2}$.</li>
<li>Plausible: $\left\langle O_{1}, h^{-}, O_{2}\right\rangle$ is a coherent narrative and forms a plausible alternative, but it is less plausible than $\left\langle O_{1}, h^{+}, O_{2}\right\rangle$.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Story <br> Transition</th>
<th style="text-align: center;">\% of <br> Dataset</th>
<th style="text-align: center;">BERT-ft <br> Fully Connected <br> Acc. (\%)</th>
<th style="text-align: center;">BERT-ft <br> Linear Chain <br> Acc. (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$O_{1} \nrightarrow h^{-}$</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">71.6</td>
</tr>
<tr>
<td style="text-align: left;">$h^{-} \nrightarrow O_{2}$</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">70.5</td>
</tr>
<tr>
<td style="text-align: left;">Plausible</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">58.5</td>
</tr>
<tr>
<td style="text-align: left;">All $(1,000)$</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">68.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Fraction of dataset for which a particular transition in the story is broken for the negative hypothesis, for 1,000 random instances from the test set.</p>
<p>We analyze the prevalence of each of these reasons in $\mathcal{R} T$. We design a crowdsourcing task in which we show the implausible option along with the narrative context $\left\langle O_{1}, O_{2}\right\rangle$ and get labels for which transition $\left(O_{1} \nrightarrow h^{-}, h^{-} \nrightarrow O_{2}\right.$ or neither) in the narrative chain is broken. Table 4 shows the proportion of each category from a subset of 1,000 instances from the test set. While $h^{-} \nrightarrow O_{2}$ accounts for almost half of the implausible transitions in $\mathcal{R} T$, all three categories are substantially present in the dataset. BERT performance on each of these categories indicates that the model finds it particularly hard when the narrative created by the incorrect hypothesis is plausible, but less plausible than the correct hypothesis. On that subset of the test set, the fully connected model performs better than the linear chain model where it is important to consider both observations jointly to arrive at the more likely hypothesis.</p>
<h2>$6.2 \quad \alpha \mathrm{NLG}$</h2>
<p>Figure 6 shows some examples of generations from the trained models compared to human-written generations. The example on the left is an example of an instance that only humans could get correct, while for the one on the right, COMeT-Emb+GPT2also generates the correct explanation for the observations.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Examples of generated hypotheses from different models and human-written hypothesis for 2 instances from $\mathcal{A R} \mathcal{T}$.</p>
<h1>7 TRANSFER LEARNING FROM $\mathcal{A R} \mathcal{T}$</h1>
<p>$\mathcal{A R} \mathcal{T}$ contains a large number of questions for the novel abductive reasoning task. In addition to serving as a benchmark, we investigate if $\mathcal{A R} \mathcal{T}$ can be used as a resource to boost performance on other commonsense tasks. We apply transfer learning by first training a model on $\mathcal{A R} \mathcal{T}$, and subsequently training on four target datasets - WinoGrande Sakaguchi et al. (2020), WSC Levesque et al. (2011), DPR Rahman \&amp; Ng (2012) and HellaSwag Zellers et al. (2019). We show that compared to a model that is only trained on the target dataset, a model that is sequentially trained on $\mathcal{A R} \mathcal{T}$ first and then on the target dataset can perform better. In particular, pre-training on $\mathcal{A R} \mathcal{T}$ consistently improves performance on related datasets when they have relatively few training examples.</p>
<p>On the other hand, for target datasets with large amounts of training data, pre-training on $\mathcal{A R} \mathcal{T}$ does not provide a significant improvement.</p>
<h2>8 RELATED WORK</h2>
<h2>Cloze-Style Task vs. Abductive Reasoning</h2>
<p>Since abduction is fundamentally concerned with plausible chains of cause-and-effect, our work draws inspiration from previous works that deal with narratives such as script learning (Schank \&amp; Abelson, 1975) and the narrative cloze test (Chambers \&amp; Jurafsky, 2009; Jans et al., 2012; Pichotta \&amp; Mooney, 2014; Rudinger et al., 2015). Rather than learning prototypical scripts or narrative chains, we instead reason about the most plausible events conditioned on observations. We make use of the ROCStories dataset (Mostafazadeh et al., 2016), which was specifically designed for the narrative cloze task. But, instead of reasoning about plausible event sequences, our task requires reasoning about plausible explanations for narrative omissions.</p>
<p>Entailment vs. Abductive Reasoning The formulation of $\alpha$ NLI is closely related to entailment NLI, but there are two critical distinctions that make abductive reasoning uniquely challenging. First, abduction requires reasoning about commonsense implications of observations (e.g., if we observe that the "grass is wet", a likely hypothesis is that "it rained earlier") which go beyond the linguistic notion of entailment (also noted by Josephson (2000)). Second, abduction requires non-monotonic reasoning about a set of commonsense implications collectively, to check the potential contradictions against multiple observations and to compare the level of plausibility of different hypotheses. This makes abductive reasoning distinctly challenging compared to other forms of reasoning such as induction and deduction (Shank, 1998). Perhaps more importantly, abduction is closely related to the kind of reasoning humans perform in everyday situations, where information is incomplete and definite inferences cannot be made.</p>
<p>Generative Language Modeling Recent advancements in the development of large-scale pretrained language models (Radford, 2018; Devlin et al., 2018; Radford et al., 2019) have improved the quality and coherence of generated language. Although these models have shown to generate reasonably coherent text when condition on a sequence of text, our experiments highlight the limitations of these models to 1) generate language non-monotonically and 2) adhere to commonsense knowledge. We attempt to overcome these limitations with the incorporation of a generative commonsense model during hypothesis generation.</p>
<p>Related Datasets Our new resource $\mathcal{R} \mathcal{T}$ complements ongoing efforts in building resources for natural language inference (Dagan et al., 2006; MacCartney \&amp; Manning, 2009; Bowman et al., 2015; Williams et al., 2018a; Camburu et al., 2018). Existing datasets have mostly focused on textual entailment in a deductive reasoning set-up (Bowman et al., 2015; Williams et al., 2018a) and making inferences about plausible events (Maslan et al., 2015; Zhang et al., 2017). In their typical setting, these datasets require a system to deduce the logically entailed consequences of a given premise. In contrast, the nature of abduction requires the use of commonsense reasoning capabilities, with less focus on lexical entailment. While abductive reasoning has been applied to entailment datasets (Raina et al., 2005), they have been applied in a logical theorem-proving framework as an intermediate step to perform textual entailment - a fundamentally different task than $\alpha$ NLI.</p>
<h1>9 CONCLUSION</h1>
<p>We present the first study that investigates the viability of language-based abductive reasoning. We conceptualize and introduce Abductive Natural Language Inference ( $\alpha$ NLI) - a novel task focused on abductive reasoning in narrative contexts. The task is formulated as a multiple-choice questionanswering problem. We also introduce Abductive Natural Language Generation ( $\alpha$ NLG) - a novel task that requires machines to generate plausible hypotheses for given observations. To support these tasks, we create and introduce a new challenge dataset, $\mathcal{R} \mathcal{T}$, which consists of 20,000 commonsense narratives accompanied with over 200,000 explanatory hypotheses. In our experiments, we establish comprehensive baseline performance on this new task based on state-of-the-art NLI and language models, which leads to $68.9 \%$ accuracy with a considerable gap with human performance ( $91.4 \%$ ). The $\alpha$ NLG task is significantly harder - while humans can write a valid explanation $96 \%$ of times, the best generator models can only achieve $45 \%$. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform - despite their strong performance on the closely related but different task of entailment NLI - pointing to interesting avenues for future research. We hope that $\mathcal{R} \mathcal{T}$ will serve as a challenging benchmark for future research in languagebased abductive reasoning and the $\alpha$ NLI and $\alpha$ NLG tasks will encourage representation learning that enables complex reasoning capabilities in AI systems.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank the anonymous reviewers for their insightful feedback. This research was supported in part by NSF (IIS-1524371), the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1256082, DARPA CwC through ARO (W911NF15-1- 0543), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI. Computations on beaker. org were supported in part by credits from Google Cloud.</p>
<h2>REFERENCES</h2>
<p>Henning Andersen. Abductive and deductive change. Language, pp. 765-793, 1973. URL https: //www.jstor.org/stable/pdf/412063.pdf.</p>
<p>Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65-72, 2005.</p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. Comet: Commonsense transformers for automatic knowledge graph construction. arXiv preprint arXiv:1906.05317, 2019.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2015. URL https://nlp.stanford.edu/pubs/snli_paper.pdf.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems, pp. 9560-9572, 2018. URL https://papers.nips.cc/paper/ 8163-e-snli-natural-language-inference-with-natural-language-explanations. pdf.</p>
<p>Nathanael Chambers and Dan Jurafsky. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 602-610, Suntec, Singapore, August 2009. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P/P09/P09-1068.</p>
<p>Eugene Charniak and Solomon Eyal Shimony. Probabilistic semantics for cost based abduction. Brown University, Department of Computer Science, 1990. URL https://www.aaai.org/ Papers/AAAI/1990/AAAI90-016.pdf.</p>
<p>Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced lstm for natural language inference. In $A C L, 2017$. URL https://www.aclweb.org/anthology/ P17-1152.</p>
<p>Peter E. Clark, Philip Harrison, John A. Thompson, William R. Murray, Jerry R. Hobbs, and Christiane Fellbaum. On the role of lexical and world knowledge in rte3. In ACL-PASCAL@ACL, 2007. URL https://www.aclweb.org/anthology/W07-1409.</p>
<p>Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 670680, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070. URL https://www.aclweb.org/anthology/D17-1070.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pp. 177-190. Springer, 2006. URL http: //u.cs.biu.ac.il/ dagan/publications/RTEChallenge.pdf.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. URL https://arxiv.org/abs/1810.04805.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2017. URL https://www.aclweb.org/anthology/N18-2017.</p>
<p>Jerry R. Hobbs, Mark Stickel, Paul Martin, and Douglas Edwards. Interpretation as abduction. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pp. 95-103, Buffalo, New York, USA, June 1988. Association for Computational Linguistics. doi: 10.3115/982023.982035. URL https://www.aclweb.org/anthology/P88-1012.</p>
<p>Bram Jans, Steven Bethard, Ivan Vulić, and Marie-Francine Moens. Skip n-grams and ranking functions for predicting script events. In Proceedings of the 13th Conference of the European Chapter</p>
<p>of the Association for Computational Linguistics, pp. 336-344, Avignon, France, April 2012. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ E12-1034.</p>
<p>Susan G. Josephson. Abductive inference: Computation, philosophy, technology. 2000. URL https://philpapers.org/rec/JOSAIC.</p>
<p>George Lakoff. Linguistics and natural logic. Synthese, 22(1-2):151-271, 1970. URL https: //link.springer.com/article/10.1007/BF00413602.</p>
<p>Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In $K R$, 2011.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, 2004.</p>
<p>Peter LoBue and Alexander Yates. Types of common-sense knowledge needed for recognizing textual entailment. In ACL, 2011. URL https://www.aclweb.org/anthology/ P11-2057.</p>
<p>Bill MacCartney and Christopher D. Manning. Natural logic for textual inference. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 193-200, Prague, June 2007. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/W07-1431.</p>
<p>Bill MacCartney and Christopher D. Manning. An extended model of natural logic. In Proceedings of the Eight International Conference on Computational Semantics, pp. 140-156, Tilburg, The Netherlands, January 2009. Association for Computational Linguistics. URL https://www. aclweb.org/anthology/W09-3714.</p>
<p>Nicole Maslan, Melissa Roemmele, and Andrew S. Gordon. One hundred challenge problems for logical formalizations of commonsense psychology. In AAAI Spring Symposia, 2015. URL http://people.ict.usc.edu/ gordon/publications/AAAI-SPRING15.PDF.</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pp. 839-849. Association for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1098. URL http://aclweb.org/anthology/N16-1098.</p>
<p>Peter Norvig. Inference in text understanding. In AAAI, pp. 561-565, 1987. URL http:// norvig.com/aaai87.pdf.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In $A C L, 2002$.</p>
<p>Judea Pearl. Reasoning with cause and effect. AI Magazine, 23(1):95, 2002. URL https:// ftp.cs.ucla.edu/pub/stat_ser/r265-ai-mag.pdf.</p>
<p>Judea Pearl and Dana Mackenzie. The Book of Why: The New Science of Cause and Effect. Basic Books, Inc., New York, NY, USA, 1st edition, 2018. ISBN 046509760X, 9780465097609. URL https://dl.acm.org/citation.cfm?id=3238230.</p>
<p>Charles Sanders Peirce. Collected papers of Charles Sanders Peirce, volume 5. Harvard University Press, 1965a. URL http://www.hup.harvard.edu/catalog.php?isbn= 9780674138001 .</p>
<p>Charles Sanders Peirce. Pragmatism and pragmaticism, volume 5. Belknap Press of Harvard University Press, 1965b. URL https://www.jstor.org/stable/224970.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL https://www.aclweb.org/anthology/ D14-1162.</p>
<p>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227-2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https://www.aclweb.org/anthology/N18-1202.</p>
<p>Karl Pichotta and Raymond Mooney. Statistical script learning with multi-argument events. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pp. 220-229, Gothenburg, Sweden, April 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/E14-1024.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pp. 180-191, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/S18-2023. URL https: //www.aclweb.org/anthology/S18-2023.</p>
<p>Alec Radford. Improving language understanding by generative pre-training. 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.</p>
<p>Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: The winograd schema challenge. In EMNLP-CoNLL, 2012.</p>
<p>Rajat Raina, Andrew Y Ng, and Christopher D Manning. Robust textual inference via learning and abductive reasoning. In AAAI, pp. 1099-1105, 2005. URL https://nlp.stanford.edu/ -manning/papers/aaai05-learnabduction.pdf.</p>
<p>Rachel Rudinger, Pushpendre Rastogi, Francis Ferraro, and Benjamin Van Durme. Script induction as language modeling. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1681-1686, Lisbon, Portugal, September 2015. Association for Computational Linguistics. URL http://aclweb.org/anthology/D15-1195.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In AAAI, 2020.</p>
<p>Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. Atomic: an atlas of machine commonsense for if-then reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3027-3035, 2019.</p>
<p>Roger C. Schank and Robert P. Abelson. Scripts, plans, and knowledge. In Proceedings of the 4th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI'75, pp. 151-157, San Francisco, CA, USA, 1975. Morgan Kaufmann Publishers Inc. URL http://dl.acm.org/ citation.cfm?id=1624626.1624649.</p>
<p>Gary Shank. The extraordinary ordinary powers of abductive reasoning. Theory \&amp; Psychology, 8(6):841-860, 1998. URL https://journals.sagepub.com/doi/10.1177/ 0959354398086007.</p>
<p>Masatoshi Tsuchiya. Performance impact caused by hidden bias of training data for recognizing textual entailment. CoRR, abs/1804.08117, 2018. URL http://www.lrec-conf.org/ proceedings/lrec2018/pdf/786.pdf.</p>
<p>Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566-4575, 2015.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353-355, Brussels, Belgium, November 2018. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W18-5446.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112-1122. Association for Computational Linguistics, 2018a. URL http://aclweb.org/anthology/N18-1101.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112-1122, New Orleans, Louisiana, June 2018b. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb. org/anthology/N18-1101.</p>
<p>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. URL https://aclweb. org/anthology/D18-1009.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In ACL, 2019.</p>
<p>Sheng Zhang, Rachel Rudinger, Kevin Duh, and Benjamin Van Durme. Ordinal common-sense inference. Transactions of the Association for Computational Linguistics, 5:379-395, 2017. doi: 10.1162/tacl_a_00068. URL https://www.aclweb.org/anthology/Q17-1027.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.</p>
<h1>A APPENDICES</h1>
<h2>A. 1 Data Collection Details</h2>
<p>We describe the crowdsourcing details of our data collection method.
Task 1 - Plausible Hypothesis Options In this task, participants were presented an incomplete three-part story, which consisted of the first observation $\left(O_{1}\right)$ and the second observation $\left(O_{2}\right)$ of the story. They were then asked to complete the story by writing a probable middle sentence that explains why the second observation should follow after the first one. We instructed participants to make sure that the plausible middle sentence (1) is short (fewer than 10 words) and (2) simple as if narrating to a child, (3) avoids introducing any extraneous information, and (4) uses names instead of pronouns (e.g., he/she) wherever possible.
All participants were required to meet the following qualification requirements: (1) their location is in the US, (2) HIT approval rate is greater than $95(\%)$, and (3) Number of HITs approved is greater than 5,000 . The reward of this task was set to be $\$ 0.07$ per question ( $\$ 14 /$ hour in average), and each HIT was assigned to five different workers (i.e., 5 -way redundancy).</p>
<p>Task 2 - Implausible Hypothesis Options In this task, participants were presented a three-part story, which consisted of the first observation $\left(O_{1}\right)$, a middle sentence $\left(h^{+}\right)$collected in Task 1, and the second observation $\left(O_{2}\right)$ of the story. They were then asked to rewrite the middle sentence $\left(h^{+}\right)$ with minimal changes, so that the story becomes unlikely, implausible or inconsistent $\left(h^{-}\right)$. We asked participants to add or remove at most four words to $h^{+}$, while ensuring that the new middle sentence is grammatical. In addition, we asked them to stick to the context in the given story. For example, if the story talks about "doctors", they are welcome to talk about "health" or "diagnosis", but not mention "aliens". Finally, we also asked workers to verify if the given middle $\left(h^{+}\right)$makes a plausible story, in order to confirm the plausibility of $h^{+}$collected in Task 1.
With respect to this task's qualification, participants were required to fulfill the following requirements: (1) their location is the US or Canada, (2) HIT approval rate is greater than or equal to 99(\%), and (3) number of HITs approved is greater than or equal to 10,000 . Participants were paid $\$ 0.1$ per question ( $\$ 14 /$ hour in average), and each HIT was assigned to three different participants (i.e., 3 -way redundancy).</p>
<p>Task 3 - $\alpha$ NLI Human Performance Human performance was evaluated by asking participants to answer the $\alpha$ NLI questions. Given a narrative context $\left\langle O_{1}, O_{2}\right\rangle$ and two hypotheses, they were asked to choose the more plausible hypothesis. They were also allowed to choose "None of the above" when neither hypothesis was deemed plausible.</p>
<p>We asked each question to seven participants with the following qualification requirements: (1) their location is either in the US, UK, or Canada, (2) HIT approval rate is greater than 98(\%), (3) Number of HITs approved is greater than 10,000 . The reward was set to $\$ 0.05$ per HIT. We took the majority vote among the seven participants for every question to compute human performance.</p>
<h2>A. 2 A $\operatorname{AC}$ DATA STATISTICS</h2>
<p>Table 6 shows some statistics of the $\operatorname{AC}$ dataset.</p>
<h2>A. 3 Fine-Tuning BERT</h2>
<p>We fine-tuned the BERT model using a grid search with the following set of hyper-parameters:</p>
<ul>
<li>batch size: ${3,4,8}$</li>
<li>number of epochs: ${3,4,10}$</li>
<li>learning rate: ${1 \mathrm{e}-5,2 \mathrm{e}-5,3 \mathrm{e}-5,5 \mathrm{e}-5}$</li>
</ul>
<p>The warmup proportion was set to 0.2 , and cross-entropy was used for computing the loss. The best performance was obtained with a batch size of 4 , learning rate of $5 \mathrm{e}-5$, and number of epochs equal to 10 . Table 7 describes the input format for GPT and BERT (and its variants).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Total unique occurrences</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Contexts $\left\langle O_{1}, O_{2}\right\rangle$</td>
<td style="text-align: center;">17,801</td>
<td style="text-align: center;">1,532</td>
<td style="text-align: center;">3,059</td>
</tr>
<tr>
<td style="text-align: left;">Plausible hyp. $h^{+}$</td>
<td style="text-align: center;">72,046</td>
<td style="text-align: center;">1,532</td>
<td style="text-align: center;">3,059</td>
</tr>
<tr>
<td style="text-align: left;">Implausible hyp. $h^{-}$</td>
<td style="text-align: center;">166,820</td>
<td style="text-align: center;">1,532</td>
<td style="text-align: center;">3,059</td>
</tr>
<tr>
<td style="text-align: left;">Avg. size per context</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Plausible hyp. $h^{+}$</td>
<td style="text-align: center;">4.05</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Implausible hyp. $h^{-}$</td>
<td style="text-align: center;">9.37</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Avg. word length</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Plausible hyp. $h^{+}$</td>
<td style="text-align: center;">8.34</td>
<td style="text-align: center;">8.62</td>
<td style="text-align: center;">8.54</td>
</tr>
<tr>
<td style="text-align: left;">Implausible hyp. $h^{-}$</td>
<td style="text-align: center;">8.28</td>
<td style="text-align: center;">8.55</td>
<td style="text-align: center;">8.53</td>
</tr>
<tr>
<td style="text-align: left;">First observation $O_{1}$</td>
<td style="text-align: center;">8.09</td>
<td style="text-align: center;">8.07</td>
<td style="text-align: center;">8.17</td>
</tr>
<tr>
<td style="text-align: left;">Second observation $O_{2}$</td>
<td style="text-align: center;">9.29</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">9.31</td>
</tr>
</tbody>
</table>
<p>Table 6: Some statistics summarizing the $\mathcal{R} \mathcal{T}$ dataset. The train set includes all plausible and implausible hypotheses collected via crowdsourcing, while the dev and test sets include the hypotheses selected through the Adversarial Filtering algorithm.</p>
<h1>A. 4 BASELINES</h1>
<p>The SVM classifier is trained on simple features like word length, overlap and sentiment features to select one of the two hypothesis choices. The bag-of-words baseline computes the average of GloVe (Pennington et al., 2014) embeddings for words in each sentence to form sentence embeddings. The sentence embeddings in a story (two observations and a hypothesis option) are concatenated and passed through fully-connected layers to produce a score for each hypothesis. The accuracies of both baselines are close to $50 \%$ (SVM: 50.6; BOW: 50.5).</p>
<p>Specifically, we train an SVM classifier and a bag-of-words model using GLoVE embeddings. Both models achieve accuracies close to $50 \%$. An Infersent (Conneau et al., 2017) baseline that uses sentences embedded by max-pooling over Bi-LSTM token representations achieves only $50.8 \%$ accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Input Format</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT</td>
<td style="text-align: left;">[START]</td>
<td style="text-align: left;">$O_{1}+h^{i}$ [SEP]</td>
<td style="text-align: left;">$O_{2}$ [SEP]</td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [Hypothesis Only]</td>
<td style="text-align: left;">[CLS] $h^{i}$ [SEP]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [First Observation Only]</td>
<td style="text-align: left;">[CLS] $O_{1}$ [SEP] $h^{i}$ [SEP]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [Second Observation Only]</td>
<td style="text-align: left;">[CLS] $h^{i}$ [SEP] $O_{2}$ [SEP]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [Linear Chain]</td>
<td style="text-align: left;">[CLS] $O_{1}$ [SEP] $h^{i}$ [SEP] ; [CLS] $h^{i}$ [SEP] $O_{2}$ [SEP]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BERT-ft [Fully Connected]</td>
<td style="text-align: left;">[CLS] $O_{1}+O_{2}$ [SEP] $h^{i}$ [SEP]</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Input formats for GPT and BERT fine-tuning.</p>
<h2>A. 5 Adversarial Filtering of Hypotheses Choices</h2>
<p>Given an observation pair and sets of plausible and implausible hypotheses $\left\langle O_{1}, O_{2}, \mathcal{H}^{+}, \mathcal{H}^{-}\right\rangle$, our adversarial filtering algorithm selects one plausible and one implausible hypothesis $\left\langle O_{1}, O_{2}, h^{+}\right.$, $h^{-}$} such that $h^{+}$and $h^{-}$are hard to distinguish between. We make three key improvements over the previously proposed Adversarial Filtering (AF) approach in Zellers et al. (2018). First, Instead of a single positive sample, we exploit a pool $\mathcal{H}^{+}$of positive samples to choose from (i.e. plausible hypotheses). Second, Instead of machine generated distractors, the pool $\mathcal{H}^{-}$of negative samples (i.e. implausible hypotheses) is human-generated. Thus, the distractors share stylistic features of the positive samples as well as that of the context (i.e. observations $O_{1}$ and $O_{2}$ ) - making the negative samples harder to distinguish from positive samples. Finally, We use BERT (Devlin et al., 2018) as</p>
<p>the adversary and introduce a temperature parameter that controls the maximum number of instances that can be modified in each iteration of AF. In later iterations, fewer instances get modified resulting in a smoother convergence of the AF algorithm (described in more detail below).
Algorithm 1 provides a formal description of our approach. In each iteration $i$, we train an adversarial model $M_{i}$ on a random subset $\mathcal{T}<em i="i">{i}$ of the data and update the validation set $\mathcal{V}</em>}$ to make it more challenging for $M_{i}$. For a pair $\left(h_{k}^{+}, h_{k}^{-}\right)$of plausible and implausible hypotheses for an instance $k$, we denote $\delta=\Delta_{M_{i}}\left(h_{k}^{+}, h_{k}^{-}\right)$the difference in the model evaluation of $h_{k}^{+}$and $h_{k}^{-}$. A positive value of $\delta$ indicates that the model $M_{i}$ favors the plausible hypothesis $h_{k}^{+}$over the implausible one $h_{k}^{-}$. With probability $t_{i}$, we update instance $k$ that $M_{i}$ gets correct with a pair $\left(h^{+}, h^{-}\right) \in \mathcal{H<em k="k">{k}^{+} \times \mathcal{H}</em>}^{-}$ of hypotheses that reduces the value of $\delta$, where $\mathcal{H<em k="k">{k}^{+}$(resp. $\mathcal{H}</em>$) is the pool of plausible (resp. implausible) hypotheses for instance $k$.}^{-</p>
<p>We ran AF for 50 iterations and the temperature $t_{i}$ follows a sigmoid function, parameterized by the iteration number, between $t_{s}=1.0$ and $t_{e}=0.2$. Our final dataset, $\mathcal{R} \mathcal{T}$, is generated using BERT as the adversary in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">Dual</span><span class="w"> </span><span class="nx">Adversarial</span><span class="w"> </span><span class="nx">Filtering</span>
<span class="nx">input</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nx">dataset</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">plausible</span><span class="w"> </span><span class="err">\</span><span class="o">&amp;</span><span class="w"> </span><span class="nx">implausible</span><span class="w"> </span><span class="nx">hypothesis</span><span class="w"> </span><span class="nx">sets</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">H</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">H</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">iterations</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="err">\</span><span class="p">),</span>
<span class="w">    </span><span class="nx">initial</span><span class="w"> </span><span class="err">\</span><span class="o">&amp;</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="nx">temperatures</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="nx">s</span><span class="p">},</span><span class="w"> </span><span class="nx">t_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="nx">output</span><span class="p">:</span><span class="w"> </span><span class="nx">dataset</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="k">for</span><span class="w"> </span><span class="nx">iteration</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="nx">n</span><span class="o">-</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="nx">i</span><span class="p">}=</span><span class="nx">t_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="nx">t_</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="o">-</span><span class="nx">t_</span><span class="p">{</span><span class="nx">e</span><span class="p">}}{</span><span class="mi">1</span><span class="o">+</span><span class="nx">e</span><span class="o">^</span><span class="p">{</span><span class="m m-Double">0.3</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">t</span><span class="o">-</span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="mi">3</span><span class="w"> </span><span class="nx">n</span><span class="p">}{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)}}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Randomly</span><span class="w"> </span><span class="nx">partition</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">V</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Train</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">S</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}=</span><span class="err">\</span><span class="nx">emptyset</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">hypotheses</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">V</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">h_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">},</span><span class="w"> </span><span class="nx">h_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">V</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Pick</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">uniformly</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r</span><span class="p">&gt;</span><span class="nx">t_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Delta_</span><span class="p">{</span><span class="nx">M_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">h_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">},</span><span class="w"> </span><span class="nx">h_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)&lt;</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="nx">Add</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">h_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">},</span><span class="w"> </span><span class="nx">h_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">S</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Pick</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">h</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">},</span><span class="w"> </span><span class="nx">h</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">H</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">times</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">H</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="nx">s</span><span class="p">.</span><span class="nx">t</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Delta_</span><span class="p">{</span><span class="nx">M_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">h</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">},</span><span class="w"> </span><span class="nx">h</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)&lt;</span><span class="err">\</span><span class="nx">Delta_</span><span class="p">{</span><span class="nx">M_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">h_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">},</span><span class="w"> </span><span class="nx">h_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Add</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">h</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">},</span><span class="w"> </span><span class="nx">h</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">S</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="nx">end</span>
<span class="w">    </span><span class="nx">end</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">}=</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">S</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="nx">end</span>
</code></pre></div>

<h1>A. 6 ATOMIC RELATIONS</h1>
<p>ATOMIC (Sap et al., 2019) represents commonsense knowledge as a graph with events are nodes and the following nine relations as edges:</p>
<ol>
<li>xIntent: Why does X cause an event?</li>
<li>xNeed: What does X need to do before the event?</li>
<li>xAttr: How would X be described?</li>
<li>xEffect: What effects does the event have on X ?</li>
<li>xWant: What would X likely want to do after the event?</li>
<li>xReaction: How does X feel after the event?</li>
<li>oReact: How do others' feel after the event?</li>
<li>oWant: What would others likely want to do after the event?</li>
<li>oEffect: What effects does the event have on others?</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Input Format</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT2-Fixed</td>
<td style="text-align: left;">$w_{1}^{1} \ldots w_{n}^{1} w_{1}^{2} \ldots w_{n}^{2}$ Because,</td>
</tr>
<tr>
<td style="text-align: left;">$O_{1}-O_{2}-O n 1 y$</td>
<td style="text-align: left;">$\langle o 1\rangle w_{1}^{1} \ldots w_{n}^{1}\langle/o 1\rangle\langle o 2\rangle w_{1}^{2} \ldots w_{n}^{2}\langle/o 2\rangle\langle h\rangle$</td>
</tr>
<tr>
<td style="text-align: left;">COMeT-Txt+GPT2</td>
<td style="text-align: left;">$\left\langle p_{1}^{1}\right\rangle T_{1}^{1} \ldots T_{0}^{1}\left\langle p_{0}^{1}\right\rangle\left\langle p_{1}^{2}\right\rangle T_{1}^{2} \ldots T_{0}^{2}\left\langle p_{0}^{2}\right\rangle\langle o 1\rangle w_{1}^{1} \ldots w_{n}^{1}\langle/o 1\rangle\langle o 2\rangle w_{1}^{2} \ldots w_{n}^{2}\langle/o 2\rangle\langle h\rangle$</td>
</tr>
<tr>
<td style="text-align: left;">COMeT-Emb+GPT2</td>
<td style="text-align: left;">$c_{1}^{1} \ldots c_{9}^{1} ; c_{1}^{2} \ldots c_{9}^{2}\langle o 1\rangle w_{1}^{1} \ldots w_{n}^{1}\langle/o 1\rangle\langle o 2\rangle w_{1}^{2} \ldots w_{n}^{2}\langle/ o 2\rangle\langle h\rangle$</td>
</tr>
</tbody>
</table>
<p>Table 8: Input format used to training and generated text from various GPT2 based models. $c_{i}^{j}$ refers to the COMeTembeddings obtained using a separate transformer model for relation $i$ and observation $j$. Similarly, $T_{i}^{j}$ is the textual phrase for relation $i$, observation $j$. Where appropriate, field specific start and end-tags are added to the sequence of inputs.</p>
<h1>A. 7 Generation Models Input Format</h1>
<p>Table 8 describes the format of input to each variation of the generative model evaluated.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ Please see Appendix A. 6 for a full list of the nine relations.
${ }^{10}$ We describe the format of input for each model in Appendix A.7.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
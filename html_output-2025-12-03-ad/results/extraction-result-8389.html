<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8389 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8389</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8389</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-8f490b938586d8e1b892304dd5209b2295c93ed7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8f490b938586d8e1b892304dd5209b2295c93ed7" target="_blank">Transformers Can Achieve Length Generalization But Not Robustly</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper shows for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length, and shows that the success of length generalization is intricately linked to the data format and the type of position encoding.</p>
                <p><strong>Paper Abstract:</strong> Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8389.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8389.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-Addition-Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer models trained for N-digit decimal addition (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study of causal Transformer variants (2M–268M parameters) trained from scratch on reversed-format decimal addition with ablations over positional encodings, randomized PEs, data-format interventions, and regularization; documents mechanisms that enable digit-by-digit algorithmic computation and the fragility of length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal Transformer (this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer architectures trained from scratch: primary base model is 25M parameters (6 blocks, hidden size 512, FF dim 2048, 8 attention heads, GeGLU, RMSNorm, Pre/PostNorm). Additional variants: 2M, 5M and 268M parameter models with correspondingly fewer/more blocks and hidden dims. Trained with AdamW, causal LM objective, greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit decimal addition (reversed-format with index hints), training lengths sampled uniformly from 1 to T where T in {10,20,30,40}, evaluation up to 100-digit addition.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Digit-by-digit algorithmic computation: model learns local (Markovian) step that depends on two corresponding operand digits and previous carry; operand identification (precise positional access) is critical and enabled by positional encodings and index-hint tokens (enabling indexing via induction-head-like behavior). Attention biases (additive RPEs) shape pre-softmax logits to represent relative positions for operand matching.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Empirical ablations and controlled interventions: compare positional encodings (FIRE, KerpleLog, RoPE, NoPE), randomized positional encoding sampling, random-space augmentation, reversed vs standard data formats, presence/absence of index hints, different training length curricula, weight initialization seeds, training data order seeds, model sizes, weight decay and dropout values; repeat trials (multiple seeds) and analyze training curves, test EM accuracy and error distributions. No explicit neuron-level probing (e.g., linear probes or activation patching) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best configuration (FIRE + randomized PE + reversed format + index hints) trained on lengths 1–40 achieves near-perfect generalization to 100-digit addition (>98% exact-match reported in abstract/figures). Generalization ratios: trained to 40 -> generalize to 100 (2.5×); trained to 30 -> generalize to 45 (1.5×); trained to 20 -> generalize to 25 (1.25×); trained to 10 -> no OOD generalization (1.0×). 5M parameter model achieved ~80% accuracy on 100-digit addition when trained on 1–40; some trials (with different seeds) produce near-zero OOD accuracy. In-distribution training loss typically reaches near-zero by ~10k steps across seeds while OOD accuracy varies widely.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Common failure modes: (1) positional misalignment — model sometimes adds operands adjacent to correct ones (off-by-one index matching); (2) single-digit mistakes dominate (over 90% of incorrect examples are single-digit errors, following an exponential distribution); (3) some PEs lead to premature termination / failure to identify start/end of addition; (4) random/unstructured errors possibly due to attention glitches; (5) large variance across random seeds and training-data order (fragility of the learned algorithm); carry propagation itself is not the primary failure cause (errors appear almost equally with and without carry).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation evidence: (a) reversed format simplifies learning to local digit-step and yields sharp 'grokking'-like transition and better OOD generalization; (b) index hints are critical — removing them causes generalization failure across PEs (Fig. 4); (c) FIRE additive RPE combined with randomized PE gives the best OOD performance (near-perfect on 100-digit) vs RoPE/NoPE/KerpleLog (Fig. 3,7,8); (d) error-distribution analysis shows mostly single-digit errors and nearly uniform position error distribution (Fig. C.10), consistent with a digit-by-digit local algorithm with occasional indexing mistakes or attention glitches; (e) training-length ablations quantify how increasing training max length increases OOD reach (Fig. 13).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Length generalization is highly non-robust: high variance across different weight initializations and training data orders (some seeds achieve near-perfect OOD accuracy, others near-zero despite similar in-distribution loss); randomized PE and random-space augmentation effects are PE-dependent (random spaces help RoPE/KerpleLog but hurt FIRE/NoPE); larger model size does not reliably improve OOD generalization (268M sometimes worse than 25M for FIRE/NoPE); strong dropout destroys generalization; these findings challenge simple claims that improved in-distribution loss or scaling alone ensures robust algorithmic extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers Can Achieve Length Generalization But Not Robustly', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8389.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8389.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FIRE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FIRE (Functional Interpolation for Relative Positions) positional encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An additive relative positional encoding that uses a learnable MLP f_theta over a normalized function of index differences to produce attention biases; shown in this paper to be the most effective PE for length generalization in decimal addition when combined with randomized PE and suitable data formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Functional interpolation for relative positions improves long context transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal Transformer (this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as the additive RPE bias term across attention layers; implemented with a small 2-layer MLP (32 hidden units) and layerwise sharing of attention bias across blocks in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit decimal addition (reversed format with index hints), evaluated up to 100 digits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Encodes relative positions as scalar additive biases for pre-softmax logits (b(i,j) = f_theta(psi(i-j)/psi(max{L,i}))). This richer functional expressivity supports representing relative-index patterns needed for operand identification across longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablation experiments comparing FIRE to RoPE, KerpleLog, and NoPE; combined with randomized PE and random-space augmentation variants to test robustness. Trials across multiple random seeds and model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used with randomized PE, reversed format, and index hints, FIRE attains near-perfect OOD EM accuracy on 100-digit addition after training on 1–40 digits (>98%). Other PEs degrade beyond ~60 digits in best trials.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>FIRE runs that fail tend to produce essentially random single-digit mistakes rather than systematic termination errors; random space augmentation harms FIRE's performance; some FIRE trials suffer near-zero OOD accuracy despite low training loss (seed variance).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical superiority in Fig. 3 (best-of-10 trials), and positive interaction with randomized PE (Fig. 8). Training curves show steady OOD accuracy growth for FIRE compared to more volatile behavior with other PEs (Fig. 12).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>FIRE is sensitive to random-space augmentation (adversarial effect) and to random seeds/training data order; successful FIRE-based generalization is not guaranteed across seeds, indicating the encoding alone is insufficient for robust extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers Can Achieve Length Generalization But Not Robustly', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8389.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8389.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IndexHints</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Index hints (digit index tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Special index tokens inserted alongside digits in both questions and answers to provide explicit positional markers, facilitating reliable operand matching and enabling indexing behaviors (e.g., via induction heads).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What algorithms can transformers learn? a study in length generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal Transformer (this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Index hint tokens drawn from a pre-defined ordered set of 102 symbols are randomly sampled during training and inference to provide positional cues; used in reversed and standard formats in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit decimal addition (reversed/standard formats).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Provides explicit positional labels that the Transformer can attend to, enabling the model to implement indexing logic (cited connection to induction-head-like indexing). This eases operand identification subtask, reducing reliance on implicit positional encodings alone.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablation: compare training runs with vs without index hints across multiple PEs and seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>With index hints, models (especially with FIRE) generalize well to OOD lengths; without index hints, all PEs fail to generalize and some even show poor in-distribution generalization for larger training lengths (e.g., 40 digits) — demonstrated in Fig. 4.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Without index hints models struggle to reliably locate operands (misalignment, premature termination), leading to systematic failure modes. With hints, residual errors are mostly single-digit and seemingly random rather than systematic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Fig. 4 demonstrates large performance difference with/without hints across PEs; discussion links hints to enabling indexing via known mechanisms (Olsson et al. 2022 induction heads).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Index hints are effective but not sufficient: even with hints, success depends on PE choice, randomized PE, data order, and random seed; robustness is not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers Can Achieve Length Generalization But Not Robustly', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8389.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8389.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RandomizedPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Randomized positional encoding (Ruoss et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategy that samples positional encodings from a distribution that covers lengths longer than training sequences, aiming to expose the model to larger positional values and reduce overfitting to training-time positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Randomized positional encodings boost length generalization of transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal Transformer (this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as an augmentation on top of base PE (e.g., FIRE, KerpleLog) with PE-dependent hyperparameters to better cover OOD positions during training.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit decimal addition (reversed format with index hints).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Augments position bias exposure so the learned attention bias functions must accommodate a wider range of position values and thus better extrapolate to longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Applied randomized PE vs non-randomized PE across PEs and measured OOD EM accuracy; interactions with random-space augmentation also tested.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Randomized PE substantially improves FIRE's OOD performance (boosting 100-digit accuracy) but degrades KerpleLog in these experiments (Fig. 8). Effects are PE-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>For some PEs (KerpleLog), randomized PE can degrade performance; the intervention does not eliminate seed/data-order variance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Fig. 8 shows randomized PE's positive effect on FIRE and negative effect on KerpleLog. The paper hypothesizes randomized PE prevents overfitting to specific training positions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Randomized PE is not universally beneficial — its effectiveness depends on the underlying PE parameterization and interacts with other augmentations (random spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers Can Achieve Length Generalization But Not Robustly', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8389.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8389.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReversedFormat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reversed digit order data format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training and evaluation format where digits are listed from least significant digit (LSD) to most significant (MSD), aligning autoregressive generation with elementary-school addition order and making the summation subtask local.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal Transformer (this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied uniformly as default data format in many experiments; compared to standard printed format in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit decimal addition (LSD-first / reversed), used for training up to 40 digits and evaluation up to 100 digits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Enables a local, stepwise ('Markovian') algorithmic representation where each output digit depends mainly on two input digits and a single carry token from the previous step, simplifying learning and enabling grokking-like sharp transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablation comparing reversed vs standard formats, track training dynamics (loss, next-token accuracy) and OOD EM accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reversed format consistently outperforms standard format across PEs for OOD generalization; reversed format yields sharp grokking-like transitions and better long-length extrapolation (Fig. 5, Fig. 6).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Standard format shows gradual training improvement but limited OOD generalization and more carry-propagation difficulties for longer sequences; reformatting to pairwise grouping of digits did not fix adjacent-index misalignment errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Training curves show 'Eureka moment' grokking transition in reversed format (Fig. 6 and C.3); ablations show superior OOD EM accuracy in reversed format vs standard (Fig. 5).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Even in reversed format, success requires the right PE and index hints; reversed format alone is insufficient for robust generalization across seeds and training orders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers Can Achieve Length Generalization But Not Robustly', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8389.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8389.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ErrorAnalysis-Addition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Error analysis and failure modes for decimal addition in Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Characterization of error types: mostly single-digit mistakes, uniform positional distribution of errors, misalignment/off-by-one operand selection, and sensitivity to seeds and data order; carry propagation is not the dominant error source.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal Transformer (this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Analysis primarily on FIRE-best models and on other PEs for comparison; uses 100-digit evaluation and per-position error statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit decimal addition (evaluated up to 100 digits).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Findings are consistent with a learned local digit-step algorithm plus an indexing mechanism that sometimes misroutes attention; random/attention glitches cause isolated single-digit errors.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Statistical analysis of incorrect examples, per-position error histograms (Fig. C.10), classification of errors with/without carry (Fig. 9), and counting number of incorrect digits per example.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Across most models, >90% of errors in incorrect examples are single-digit mistakes (exponential distribution of number of incorrect digits). Error distribution across positions is approximately uniform, with occasional model-specific hotspots.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Detailed: (1) single-digit errors dominate (>90%); (2) off-by-one misalignment where model adds adjacent operands; (3) premature termination in some PEs; (4) near-random single-digit errors in otherwise high-performing FIRE runs; (5) large variability across checkpoints and seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Fig. 9 shows similar error rates with and without carry (carry not primary cause); Fig. C.10 shows near-uniform error distribution and predominance of single-digit mistakes; ablations that change formatting/PE alter error modes (e.g., reformatting to pairwise grouping moves errors but doesn't eliminate misalignment).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Some runs show non-uniform position errors (specific checkpoints), and a minority of runs fail catastrophically (near-zero OOD accuracy) despite low train loss, highlighting unpredictability and fragility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers Can Achieve Length Generalization But Not Robustly', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Randomized positional encodings boost length generalization of transformers <em>(Rating: 2)</em></li>
                <li>Functional interpolation for relative positions improves long context transformers <em>(Rating: 2)</em></li>
                <li>What algorithms can transformers learn? a study in length generalization <em>(Rating: 2)</em></li>
                <li>Positional description matters for transformers arithmetic <em>(Rating: 2)</em></li>
                <li>The impact of positional encoding on length generalization in transformers <em>(Rating: 2)</em></li>
                <li>Train short, test long: Attention with linear biases enables input length extrapolation <em>(Rating: 1)</em></li>
                <li>Groking: Generalization beyond overfitting on small algorithmic datasets <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8389",
    "paper_id": "paper-8f490b938586d8e1b892304dd5209b2295c93ed7",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Transformer-Addition-Study",
            "name_full": "Transformer models trained for N-digit decimal addition (this paper)",
            "brief_description": "Empirical study of causal Transformer variants (2M–268M parameters) trained from scratch on reversed-format decimal addition with ablations over positional encodings, randomized PEs, data-format interventions, and regularization; documents mechanisms that enable digit-by-digit algorithmic computation and the fragility of length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Causal Transformer (this paper's experiments)",
            "model_description": "Autoregressive Transformer architectures trained from scratch: primary base model is 25M parameters (6 blocks, hidden size 512, FF dim 2048, 8 attention heads, GeGLU, RMSNorm, Pre/PostNorm). Additional variants: 2M, 5M and 268M parameter models with correspondingly fewer/more blocks and hidden dims. Trained with AdamW, causal LM objective, greedy decoding.",
            "arithmetic_task_type": "Multi-digit decimal addition (reversed-format with index hints), training lengths sampled uniformly from 1 to T where T in {10,20,30,40}, evaluation up to 100-digit addition.",
            "mechanism_or_representation": "Digit-by-digit algorithmic computation: model learns local (Markovian) step that depends on two corresponding operand digits and previous carry; operand identification (precise positional access) is critical and enabled by positional encodings and index-hint tokens (enabling indexing via induction-head-like behavior). Attention biases (additive RPEs) shape pre-softmax logits to represent relative positions for operand matching.",
            "probing_or_intervention_method": "Empirical ablations and controlled interventions: compare positional encodings (FIRE, KerpleLog, RoPE, NoPE), randomized positional encoding sampling, random-space augmentation, reversed vs standard data formats, presence/absence of index hints, different training length curricula, weight initialization seeds, training data order seeds, model sizes, weight decay and dropout values; repeat trials (multiple seeds) and analyze training curves, test EM accuracy and error distributions. No explicit neuron-level probing (e.g., linear probes or activation patching) reported.",
            "performance_metrics": "Best configuration (FIRE + randomized PE + reversed format + index hints) trained on lengths 1–40 achieves near-perfect generalization to 100-digit addition (&gt;98% exact-match reported in abstract/figures). Generalization ratios: trained to 40 -&gt; generalize to 100 (2.5×); trained to 30 -&gt; generalize to 45 (1.5×); trained to 20 -&gt; generalize to 25 (1.25×); trained to 10 -&gt; no OOD generalization (1.0×). 5M parameter model achieved ~80% accuracy on 100-digit addition when trained on 1–40; some trials (with different seeds) produce near-zero OOD accuracy. In-distribution training loss typically reaches near-zero by ~10k steps across seeds while OOD accuracy varies widely.",
            "error_types_or_failure_modes": "Common failure modes: (1) positional misalignment — model sometimes adds operands adjacent to correct ones (off-by-one index matching); (2) single-digit mistakes dominate (over 90% of incorrect examples are single-digit errors, following an exponential distribution); (3) some PEs lead to premature termination / failure to identify start/end of addition; (4) random/unstructured errors possibly due to attention glitches; (5) large variance across random seeds and training-data order (fragility of the learned algorithm); carry propagation itself is not the primary failure cause (errors appear almost equally with and without carry).",
            "evidence_for_mechanism": "Ablation evidence: (a) reversed format simplifies learning to local digit-step and yields sharp 'grokking'-like transition and better OOD generalization; (b) index hints are critical — removing them causes generalization failure across PEs (Fig. 4); (c) FIRE additive RPE combined with randomized PE gives the best OOD performance (near-perfect on 100-digit) vs RoPE/NoPE/KerpleLog (Fig. 3,7,8); (d) error-distribution analysis shows mostly single-digit errors and nearly uniform position error distribution (Fig. C.10), consistent with a digit-by-digit local algorithm with occasional indexing mistakes or attention glitches; (e) training-length ablations quantify how increasing training max length increases OOD reach (Fig. 13).",
            "counterexamples_or_challenges": "Length generalization is highly non-robust: high variance across different weight initializations and training data orders (some seeds achieve near-perfect OOD accuracy, others near-zero despite similar in-distribution loss); randomized PE and random-space augmentation effects are PE-dependent (random spaces help RoPE/KerpleLog but hurt FIRE/NoPE); larger model size does not reliably improve OOD generalization (268M sometimes worse than 25M for FIRE/NoPE); strong dropout destroys generalization; these findings challenge simple claims that improved in-distribution loss or scaling alone ensures robust algorithmic extrapolation.",
            "uuid": "e8389.0",
            "source_info": {
                "paper_title": "Transformers Can Achieve Length Generalization But Not Robustly",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "FIRE",
            "name_full": "FIRE (Functional Interpolation for Relative Positions) positional encoding",
            "brief_description": "An additive relative positional encoding that uses a learnable MLP f_theta over a normalized function of index differences to produce attention biases; shown in this paper to be the most effective PE for length generalization in decimal addition when combined with randomized PE and suitable data formatting.",
            "citation_title": "Functional interpolation for relative positions improves long context transformers",
            "mention_or_use": "use",
            "model_name": "Causal Transformer (this paper's experiments)",
            "model_description": "Applied as the additive RPE bias term across attention layers; implemented with a small 2-layer MLP (32 hidden units) and layerwise sharing of attention bias across blocks in experiments.",
            "arithmetic_task_type": "Multi-digit decimal addition (reversed format with index hints), evaluated up to 100 digits.",
            "mechanism_or_representation": "Encodes relative positions as scalar additive biases for pre-softmax logits (b(i,j) = f_theta(psi(i-j)/psi(max{L,i}))). This richer functional expressivity supports representing relative-index patterns needed for operand identification across longer sequences.",
            "probing_or_intervention_method": "Ablation experiments comparing FIRE to RoPE, KerpleLog, and NoPE; combined with randomized PE and random-space augmentation variants to test robustness. Trials across multiple random seeds and model sizes.",
            "performance_metrics": "When used with randomized PE, reversed format, and index hints, FIRE attains near-perfect OOD EM accuracy on 100-digit addition after training on 1–40 digits (&gt;98%). Other PEs degrade beyond ~60 digits in best trials.",
            "error_types_or_failure_modes": "FIRE runs that fail tend to produce essentially random single-digit mistakes rather than systematic termination errors; random space augmentation harms FIRE's performance; some FIRE trials suffer near-zero OOD accuracy despite low training loss (seed variance).",
            "evidence_for_mechanism": "Empirical superiority in Fig. 3 (best-of-10 trials), and positive interaction with randomized PE (Fig. 8). Training curves show steady OOD accuracy growth for FIRE compared to more volatile behavior with other PEs (Fig. 12).",
            "counterexamples_or_challenges": "FIRE is sensitive to random-space augmentation (adversarial effect) and to random seeds/training data order; successful FIRE-based generalization is not guaranteed across seeds, indicating the encoding alone is insufficient for robust extrapolation.",
            "uuid": "e8389.1",
            "source_info": {
                "paper_title": "Transformers Can Achieve Length Generalization But Not Robustly",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "IndexHints",
            "name_full": "Index hints (digit index tokens)",
            "brief_description": "Special index tokens inserted alongside digits in both questions and answers to provide explicit positional markers, facilitating reliable operand matching and enabling indexing behaviors (e.g., via induction heads).",
            "citation_title": "What algorithms can transformers learn? a study in length generalization",
            "mention_or_use": "use",
            "model_name": "Causal Transformer (this paper's experiments)",
            "model_description": "Index hint tokens drawn from a pre-defined ordered set of 102 symbols are randomly sampled during training and inference to provide positional cues; used in reversed and standard formats in ablations.",
            "arithmetic_task_type": "Multi-digit decimal addition (reversed/standard formats).",
            "mechanism_or_representation": "Provides explicit positional labels that the Transformer can attend to, enabling the model to implement indexing logic (cited connection to induction-head-like indexing). This eases operand identification subtask, reducing reliance on implicit positional encodings alone.",
            "probing_or_intervention_method": "Ablation: compare training runs with vs without index hints across multiple PEs and seeds.",
            "performance_metrics": "With index hints, models (especially with FIRE) generalize well to OOD lengths; without index hints, all PEs fail to generalize and some even show poor in-distribution generalization for larger training lengths (e.g., 40 digits) — demonstrated in Fig. 4.",
            "error_types_or_failure_modes": "Without index hints models struggle to reliably locate operands (misalignment, premature termination), leading to systematic failure modes. With hints, residual errors are mostly single-digit and seemingly random rather than systematic.",
            "evidence_for_mechanism": "Fig. 4 demonstrates large performance difference with/without hints across PEs; discussion links hints to enabling indexing via known mechanisms (Olsson et al. 2022 induction heads).",
            "counterexamples_or_challenges": "Index hints are effective but not sufficient: even with hints, success depends on PE choice, randomized PE, data order, and random seed; robustness is not guaranteed.",
            "uuid": "e8389.2",
            "source_info": {
                "paper_title": "Transformers Can Achieve Length Generalization But Not Robustly",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RandomizedPE",
            "name_full": "Randomized positional encoding (Ruoss et al., 2023)",
            "brief_description": "A strategy that samples positional encodings from a distribution that covers lengths longer than training sequences, aiming to expose the model to larger positional values and reduce overfitting to training-time positions.",
            "citation_title": "Randomized positional encodings boost length generalization of transformers",
            "mention_or_use": "use",
            "model_name": "Causal Transformer (this paper's experiments)",
            "model_description": "Applied as an augmentation on top of base PE (e.g., FIRE, KerpleLog) with PE-dependent hyperparameters to better cover OOD positions during training.",
            "arithmetic_task_type": "Multi-digit decimal addition (reversed format with index hints).",
            "mechanism_or_representation": "Augments position bias exposure so the learned attention bias functions must accommodate a wider range of position values and thus better extrapolate to longer sequences.",
            "probing_or_intervention_method": "Applied randomized PE vs non-randomized PE across PEs and measured OOD EM accuracy; interactions with random-space augmentation also tested.",
            "performance_metrics": "Randomized PE substantially improves FIRE's OOD performance (boosting 100-digit accuracy) but degrades KerpleLog in these experiments (Fig. 8). Effects are PE-dependent.",
            "error_types_or_failure_modes": "For some PEs (KerpleLog), randomized PE can degrade performance; the intervention does not eliminate seed/data-order variance.",
            "evidence_for_mechanism": "Fig. 8 shows randomized PE's positive effect on FIRE and negative effect on KerpleLog. The paper hypothesizes randomized PE prevents overfitting to specific training positions.",
            "counterexamples_or_challenges": "Randomized PE is not universally beneficial — its effectiveness depends on the underlying PE parameterization and interacts with other augmentations (random spaces).",
            "uuid": "e8389.3",
            "source_info": {
                "paper_title": "Transformers Can Achieve Length Generalization But Not Robustly",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReversedFormat",
            "name_full": "Reversed digit order data format",
            "brief_description": "Training and evaluation format where digits are listed from least significant digit (LSD) to most significant (MSD), aligning autoregressive generation with elementary-school addition order and making the summation subtask local.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Causal Transformer (this paper's experiments)",
            "model_description": "Applied uniformly as default data format in many experiments; compared to standard printed format in ablations.",
            "arithmetic_task_type": "Multi-digit decimal addition (LSD-first / reversed), used for training up to 40 digits and evaluation up to 100 digits.",
            "mechanism_or_representation": "Enables a local, stepwise ('Markovian') algorithmic representation where each output digit depends mainly on two input digits and a single carry token from the previous step, simplifying learning and enabling grokking-like sharp transitions.",
            "probing_or_intervention_method": "Ablation comparing reversed vs standard formats, track training dynamics (loss, next-token accuracy) and OOD EM accuracy.",
            "performance_metrics": "Reversed format consistently outperforms standard format across PEs for OOD generalization; reversed format yields sharp grokking-like transitions and better long-length extrapolation (Fig. 5, Fig. 6).",
            "error_types_or_failure_modes": "Standard format shows gradual training improvement but limited OOD generalization and more carry-propagation difficulties for longer sequences; reformatting to pairwise grouping of digits did not fix adjacent-index misalignment errors.",
            "evidence_for_mechanism": "Training curves show 'Eureka moment' grokking transition in reversed format (Fig. 6 and C.3); ablations show superior OOD EM accuracy in reversed format vs standard (Fig. 5).",
            "counterexamples_or_challenges": "Even in reversed format, success requires the right PE and index hints; reversed format alone is insufficient for robust generalization across seeds and training orders.",
            "uuid": "e8389.4",
            "source_info": {
                "paper_title": "Transformers Can Achieve Length Generalization But Not Robustly",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ErrorAnalysis-Addition",
            "name_full": "Error analysis and failure modes for decimal addition in Transformers",
            "brief_description": "Characterization of error types: mostly single-digit mistakes, uniform positional distribution of errors, misalignment/off-by-one operand selection, and sensitivity to seeds and data order; carry propagation is not the dominant error source.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Causal Transformer (this paper's experiments)",
            "model_description": "Analysis primarily on FIRE-best models and on other PEs for comparison; uses 100-digit evaluation and per-position error statistics.",
            "arithmetic_task_type": "Multi-digit decimal addition (evaluated up to 100 digits).",
            "mechanism_or_representation": "Findings are consistent with a learned local digit-step algorithm plus an indexing mechanism that sometimes misroutes attention; random/attention glitches cause isolated single-digit errors.",
            "probing_or_intervention_method": "Statistical analysis of incorrect examples, per-position error histograms (Fig. C.10), classification of errors with/without carry (Fig. 9), and counting number of incorrect digits per example.",
            "performance_metrics": "Across most models, &gt;90% of errors in incorrect examples are single-digit mistakes (exponential distribution of number of incorrect digits). Error distribution across positions is approximately uniform, with occasional model-specific hotspots.",
            "error_types_or_failure_modes": "Detailed: (1) single-digit errors dominate (&gt;90%); (2) off-by-one misalignment where model adds adjacent operands; (3) premature termination in some PEs; (4) near-random single-digit errors in otherwise high-performing FIRE runs; (5) large variability across checkpoints and seeds.",
            "evidence_for_mechanism": "Fig. 9 shows similar error rates with and without carry (carry not primary cause); Fig. C.10 shows near-uniform error distribution and predominance of single-digit mistakes; ablations that change formatting/PE alter error modes (e.g., reformatting to pairwise grouping moves errors but doesn't eliminate misalignment).",
            "counterexamples_or_challenges": "Some runs show non-uniform position errors (specific checkpoints), and a minority of runs fail catastrophically (near-zero OOD accuracy) despite low train loss, highlighting unpredictability and fragility.",
            "uuid": "e8389.5",
            "source_info": {
                "paper_title": "Transformers Can Achieve Length Generalization But Not Robustly",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Randomized positional encodings boost length generalization of transformers",
            "rating": 2
        },
        {
            "paper_title": "Functional interpolation for relative positions improves long context transformers",
            "rating": 2
        },
        {
            "paper_title": "What algorithms can transformers learn? a study in length generalization",
            "rating": 2
        },
        {
            "paper_title": "Positional description matters for transformers arithmetic",
            "rating": 2
        },
        {
            "paper_title": "The impact of positional encoding on length generalization in transformers",
            "rating": 2
        },
        {
            "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "rating": 1
        },
        {
            "paper_title": "Groking: Generalization beyond overfitting on small algorithmic datasets",
            "rating": 1
        }
    ],
    "cost": 0.0160685,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transformers Can Achieve Length Generalization But Not Robustly</h1>
<p>Yongchao Zhou ${ }^{1,2}$, Uri Alon ${ }^{1}$, Xinyun Chen ${ }^{1}$, Xuezhi Wang ${ }^{1}$, Rishabh Agarwal ${ }^{1}$ and Denny Zhou ${ }^{1}$<br>${ }^{1}$ Google DeepMind, ${ }^{2}$ University of Toronto</p>
<h4>Abstract</h4>
<p>Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is $2.5 \times$ the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.</p>
<h2>1. Introduction</h2>
<p>Transformer-based models have revolutionized natural language understanding and generation across diverse applications (Gemini et al., 2023; OpenAI, 2023). Despite their impressive abilities in mathematical reasoning (Lewkowycz et al., 2022), code synthesis (Li et al., 2022), and theorem proving (Wu et al., 2022), Transformers often struggle with length generalization, an ability that requires the model to generalize to longer sequences than seen during training (Abbe et al., 2023; Anil et al., 2022; Zhou et al., 2023). This limitation raises an essential question: do Transformers genuinely grasp the correct underlying algorithms for a given task, or are they merely resorting to superficial memorization or shortcuts that fail to scale to more complex problems (Liu et al., 2023b)?
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Using an appropriate position encoding and data formatting, we demonstrate that Transformers can generalize to 100-digit decimal addition tasks with more than $98 \%$ of accuracy when trained up to 40-digit addition, resulting in a length extension ratio of $2.5 \times$, which is much more than the ratio of Lee et al. (2023) (1.0×), Kazemnejad et al. (2023) (1.125×), Shen et al. (2023) (1.1×), and Zhou et al. (2023) (1.5×). Unfilled markers ( $\cdot$ ) denote in-distribution test results, filled markers $(\checkmark)$ denote out-of-distribution results. In Zhou et al. (2023) and Our Work, each curve is the best out of 10 trials. For the other three methods, we report the value from their corresponding paper.</p>
<p>Recent work has scrutinized Transformers' shortcomings in length generalization across formal language learning (Deletang et al., 2023) and algorithmic reasoning tasks (Anil et al., 2022; Dziri et al., 2023; Veličković et al., 2022; Zhang et al., 2022). These investigations consistently indicate a notable deficiency in length generalization capabilities. This recurring issue raises a crucial question: Is there an inherent limitation in Transformers' design preventing effective length generalization?</p>
<p>In this paper, we systematically examine the Transformer's capability of length generalization, specifically focusing on the $N$-digit decimal addition problem. We view the addition problem as a form of synthetic language learning, which despite its relative simplicity compared to natural language, provides valuable insights into the Transformer's ability to internalize fundamental algorithms. Notwithstanding its simplicity, recent work has demonstrated that Transformers exhibit limited length generalization in this task (Kazemnejad et al., 2023; Lee et al., 2023; Shen et al., 2023).</p>
<p>Previous attempts to improve Transformer's length generalization ability primarily focus on two areas: refining position encodings (Press et al., 2022; Shen et al., 2023) and optimizing data formats (Lee et al., 2023; Zhou et al., 2023). Therefore, we perform an extensive empirical evaluation of combinations of widely used position encoding and various data formats, resulting in a recipe for successful length generalization. Our final recipe consists of: FIRE position encodings (Li et al., 2023), with randomized positions (Ruoss et al., 2023), in reversed format, with index hints (Zhou et al., 2023).</p>
<p>As shown in Figure 1, when trained on only 40 digits, our model successfully extrapolates to sequences of up to 100 digits, exceeding the input length by $2.5 \times$. To the best of our knowledge, this is the strongest known generalization result for text-based Transformers on addition. Nevertheless, we observe that the robustness of this length generalization is fragile, significantly swayed by variables such as random initialization and the training data order.</p>
<p>Our key contributions are summarized as follows:
(i) We demonstrate that the success in length generalization is markedly influenced by position encoding and data format. Through careful selection of these factors, we achieved extrapolation to lengths that are $2.5 \times$ longer than those seen during training.
(ii) Our exploration of established data formatting and augmentation techniques indicates that their effectiveness in length generalization is primarily contingent on the choice of position encoding.
(iii) Despite remarkable generalization to lengths $2.5 \times$ longer than training, we found this generalization to be fragile and heavily relying on factors like random weight initialization and training data order.</p>
<h1>2. Position Encoding and Data Formats</h1>
<p>Recently proposed improvements in architectural design, notably in position encoding (Kazemnejad et al., 2023; Ruoss et al., 2023; Shen et al., 2023) and attention mechanisms (Duan and Shi, 2023; Dubois et al., 2019), aim to address the challenge of length generalization in arithmetic computations with Transformers. However, the effectiveness of such modifications is often constrained, either due to their overly ad-hoc nature or their poor performance on longer sequences. Although scaling the size of models and datasets has been recognized as a generally effective strategy to improve performance, prior research (Anil et al., 2022; Brown et al., 2020) suggests that relying solely on scale might not be sufficient for handling test sequences that are longer than training. Concurrently, with the rising focus on data-centric AI (Motamedi et al., 2021), recent work has investigated refining the data format to enhance the learning efficacy of existing Transformer models. In this section, we review some of the most common position encodings (Section 2.1) and relevant data formats (Section 2.2)</p>
<h1>2.1. Position Encoding for Length Generalization</h1>
<p>The inability of transformers to extrapolate to longer sequences has been primarily attributed to position encoding (PE; Shaw et al., 2018). In this section, we review existing positional encoding approaches with an emphasis on their length generalization abilities.</p>
<p>Absolute Positional Encoding (APE). APE enhances Transformer models with positional information by attaching a positional vector $\boldsymbol{p}<em i="i">{i}$ to each position $i$. This is achieved through a predefined sinusoidal function (Vaswani et al., 2017) or a learnable approach (Devlin et al., 2018). Then, the vector $\boldsymbol{p}</em>$ before entering the transformer's first layer. Although straightforward, APE often struggles with generalizing to longer sequences, as observed in both NLP (Press et al., 2022) and algorithmic tasks (Kazemnejad et al., 2023).}$ is combined with the token embedding $\boldsymbol{e}_{i</p>
<p>Additive Relative Positional Encoding (RPE). Shaw et al. (2018) pioneered the additive RPEs, diverging from standard input-level integration by modifying keys and, optionally, values in each attention layer. This concept was advanced by T5, which employed scalar biases to directly affect pre-softmax attention logits, a method noted for its simplicity yet criticized for limited efficiency and positional differentiation in long sequences (Press et al., 2022; Raffel et al., 2020). Later approaches such as Alibi (Press et al., 2022), Kerple (Chi et al., 2022) and FIRE (Li et al., 2023) build on the idea of learned additive bias, proposing different functions to model the scalar bias as a function of the key- and query-indices. Most pre-softmax attention logits of additive RPEs can be generally written as (Li et al., 2023):</p>
<p>$$
\boldsymbol{A}<em Q="Q">{\mathrm{RPE}}(\boldsymbol{X})=\boldsymbol{X} \boldsymbol{W}</em>
$$}\left(\boldsymbol{X} \boldsymbol{W}_{K}\right)^{\top}+\boldsymbol{B</p>
<p>where $\boldsymbol{X}, \boldsymbol{W}<em K="K">{Q}, \boldsymbol{W}</em>$, with its $(i, j)$-th entry defined as $b(i, j)$. Instances of $b(i, j)$ include:}$ denote the input and weight matrices for queries and keys. The bias matrix $\boldsymbol{B} \in \mathbb{R}^{n \times n}$ is induced by the position encoding function $b: \mathbb{N}^{* 2} \rightarrow \mathbb{R</p>
<ul>
<li>T5 (Raffel et al., 2020): $b(i, j)=r_{\min }{i-j, K}$, where $K$ is a hyperparameter and $r_{i}$ are learned scalars.</li>
<li>Alibi (Press et al., 2022): $b(i, j)=-r|i-j|$, where $r&gt;0$ is a hyperparameter.</li>
<li>KerpleLog (Chi et al., 2022): $b(i, j)=-r_{1} \log \left(1+r_{2}|i-j|\right)$, where $r_{1}, r_{2}&gt;0$ are learnable scalars.</li>
<li>FIRE (Li et al., 2023): $b(i, j)=f_{\theta}\left(\frac{\psi(i-j)}{\psi(\max {L, i})}\right)$, where $f_{\theta}: \mathbb{R} \rightarrow \mathbb{R}$ is a learnable MLP parameterized by $\theta, \psi: \mathbb{N} \rightarrow \mathbb{R}_{+}$is $\psi(x))=\log (c x+1)$ and $c&gt;0, L&gt;0$ are learnable scalars.</li>
</ul>
<p>Additional background on additive RPEs is provided in Appendix A. 1
Rotary Positional Encoding (RoPE). RoPE (Su et al., 2024) encodes position information in attention logits through rotational encoding of query and key vectors based on their relative positions. Despite being simple and effective, RoPE exhibits limited length generalization (Kazemnejad et al., 2023; Press et al., 2022). While extensions like Position Interpolation Chen et al. (2023); Peng et al. (2023); Su (2023) enhance RoPE's context length, they do not necessarily improve length generalization on algorithmic tasks where learning the underlying algorithm is crucial.</p>
<p>No Positional Encoding (NoPE). While encoder-only Transformers (e.g., BERT (Devlin et al., 2018)) are permutation equivariant without positional encodings, decoder-only counterparts with causal attention, as shown by Haviv et al. (2022), acquire positional understanding autonomously, even without explicit PE. Interestingly, recent findings by Kazemnejad et al. (2023) further reveal that a model without PE outperforms those with specialized PEs on simple algorithmic tasks.</p>
<p>Randomized Position Encoding. Ruoss et al. (2023) introduced Randomized PE to enhance existing PEs by randomly sampling encodings from a range exceeding test-time lengths while pre-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Comparative overview of PEs and data formats: While most related studies focus on APE or NoPE, our approach integrates FIRE (Li et al., 2023) and Randomized PE (Ruoss et al., 2023). All studies utilize a reversed format. Shen et al. (2023) enhance this with random space augmentation, and both Zhou et al. (2023) and Our Work incorporate index hints.
serving the order. Transformers trained this way adapt to larger positional encodings, effectively eliminating OOD position encodings during testing.</p>
<h1>2.2. Data Formats</h1>
<p>Data format plays a pivotal role in enhancing Transformers' length generalization capabilities, primarily by transforming the data into a format that could be more easily learned. We give an overview of the existing techniques below.</p>
<p>Reversed Format. Computing addition in an algorithmic way (as taught in elementary school) requires starting with the least significant digit (LSD) and proceeds to the most significant digit (MSD). This sequence contrasts with the standard printed format $\left(A_{3} A_{2} A_{1}+B_{3} B_{2} B_{1}=C_{3} C_{2} C_{1}\right.$, where $A_{1}$ and $B_{1}$ are the LSDs, which is not ideally suited for autoregressive models due to their outputting the MSD first. However, the reversed format $\left(A_{1} A_{2} A_{3}+B_{1} B_{2} B_{3}=C_{1} C_{2} C_{3}\right)$ aligns better with these the natural order of computing the digits. It simplifies the learning task to a function that depends only on the two corresponding operand digits and the carry from the previous step (Lee et al., 2023; Shen et al., 2023; Zhou et al., 2023).</p>
<p>Index Hints. Zhou et al. (2023) introduced "index hints" in both the query and response of arithmetic tasks. For example, $42+39=81$ is represented as $a 4 b 2+a 3 b 9=a 8 b 1$ during training and inference, enabling transformers to execute indexing via induction heads (Olsson et al., 2022).</p>
<p>Random Space Augmentation. Shen et al. (2023) explored the impact of random spacing between digits in addition, aiming to disrupt the model's reliance on absolute positional information. Their results show successful generalization from 10-digit to 11-digit addition, but falters with longer sequences.</p>
<p>Figure 2 lists the position encodings and data formats used in some of the most related work to ours.</p>
<h2>3. A Recipe for Length Generalization in Decimal Addition</h2>
<p>The task of decimal addition is composed of two critical subtasks: (a) the identification of the right operands to add; and (b) the summation of these operands with the preceding carry. While the</p>
<p>summation step ((b)) is relatively easier because it has a finite set of possible inputs, the primary generalization challenge lies in the operand identification ((a)), where precise positional access is crucial.</p>
<p>Our best model, which leads to the results in Figure 1, uses the following combination:</p>
<ol>
<li>FIRE position encodings (Li et al., 2023): We believe that FIRE position encodings are helpful for length generalization because they are more expressive than other PEs, as shown by Li et al. (2023).</li>
<li>Randomized position encodings (Ruoss et al., 2023): We believe that randomized position encodings are crucial to avoid overfitting on the position indices and index differences that were seen during training.</li>
<li>Reversed format: The reversed format makes it easier for the model to decompose the long computation to local, "markovian", steps that depend only on the single previous step.</li>
<li>Index hints (Zhou et al., 2023): We believe that index hints are useful because they ease the task of operand identification (discussed in (b)), of matching the right operands to add at a certain step.</li>
</ol>
<p>We ablate each of these decisions and some other alternative choices in Section 4.</p>
<h1>4. Experiments</h1>
<h3>4.1. Setup</h3>
<p>Data. As shown in Figure 2, we adopt the reversed format with index hints as our default data format. During training, we randomly sample consecutive index hints from a pre-defined ordered set of hints with 102 symbols, thereby enhancing the learning of hint sequences and their order. We generated a dataset comprising 30 M examples on input lengths $1-40$ for training and 1,000 examples per input length for testing.</p>
<p>Model. Our base model, following Zhou et al. (2023), is a 25M parameter Transformer featuring 6 blocks, a 512 hidden size, and a feedforward layer with a hidden dimension of 2048. We also adopt RMSNorm, integrating both PreNorm and PostNorm layers, following the Primer architecture (So et al., 2021). We use the AdamW optimizer (Loshchilov and Hutter, 2017) to train the model with a weight decay value of 0.1 and no dropout, for 50,000 steps. The learning rate schedule incorporates an initial 500-step linear warm-up, followed by a cosine decay, starting at 3e-4. The hyperparameters are chosen based on Appendix C.10.</p>
<p>Randomized PE and Random Space Augmentation. As will be demonstrated in Figures 7 and 8 , the success of these techniques is markedly PE-dependent. Hence, we tailor the default hyperparameter choice to best suit each PE. Further, instead of using random spaces, we use another special token to prevent automatic merging by the tokenizer.</p>
<p>Due to the high variance (which we discuss in the next section), we repeat each experiment five times unless mentioned otherwise. More implementation details are provided in Appendix B.</p>
<h3>4.2. Results</h3>
<p>FIRE enables significantly better length generalization. Figure 3 compares the length generalization capabilities of four positional encodings in the best of 10 trials (See Appendix C. 1 for all trials). Trained exclusively on sequences of lengths 1-40, the best trial of FIRE exhibit near-perfect</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | EM accuracy (best of 10 trials), trained exclusively on sequences of lengths 1 to 40 , the best trials involving FIRE exhibit near-perfect generalization on 100-digit addition.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure $4 \mid$ EM accuracy of models trained with and without index hints (best of 10 trials): Without index hints, all PE methods fail in generalization, both within and beyond trained lengths.
generalization to sequences up to the length of 100. In contrast, other PEs show a visible degradation in generalization accuracy beyond the sequence length of 60 . This finding counters the findings of Kazemnejad et al. (2023) that no positional encoding (NoPE) surpasses complex PE techniques for length generalization. Our findings suggest that a well-designed PE, such as FIRE, is essential for optimal length generalization.</p>
<p>Index hints are crucial. We compare models trained with and without index hints. As shown in Figure 4, index hints significantly enhance length generalization across various PEs, corroborating the findings of Zhou et al. (2023). Notably, without index hints, NoPE and FIRE demonstrate poor in-distribution generalization for 40-digit additions, a marked deviation from their reasonable performance when trained on 10-digits, as shown in Figure C.8(a). Figure D. 1 shows that this phenomenon occurs across all random seeds. Conversely, RoPE and KerpleLog exhibit moderate in-distribution generalization but falter in out-of-distribution scenarios. Appendices D. 1 and D. 2 shows the training loss and test accuracy of these runs.</p>
<p>Analyzing errors in 11-digit additions from models trained on 10-digits revealed a common misalignment issue: the Transformer often adds operands adjacent to the correct ones. An attempt to rectify this by reformatting addition ( $A_{1} B_{1}, A_{2} B_{2}, A_{3} B_{3}=C_{1} C_{2} C_{3}$, with 1 as the least significant bit) failed to improve length generalization, merely shifting the error to adjacent output positions. This highlights the Transformer's inherent limitations in precise position identification.</p>
<p>Standard format vs reversed format. As shown in Figure 5, standard formatting shows limited length generalization in all PEs compared to the reversed format. FIRE excels in length generalization even with the standard format, even matching RoPE in reverse format. However, FIRE's performance (with standard format) declines beyond 60-digit additions, likely due to increased carry propagation challenges exceeding the model's capacity.</p>
<p>Looking at the training loss and training next-token accuracy in both formats also shows interesting differences. As shown in Figures 6 and C.3, the standard format training leads to gradual improvement, whereas reverse format yields a sharp performance transition. This transition, which is a reminiscent of "grokking" phenomenon Power et al. (2022), shows in this case the "Eureka moment" in which the Transformer learns the right addition algorithm.</p>
<p>Random space augmentation and randomized position encoding. Figure 7 reveals divergent impacts of random space augmentation on four PEs. The augmentation's efficacy is notably contingent upon the chosen PE. While Random Spaces marginally enhances RoPE and KerpleLog's performance,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 | EM accuracy of the standard vs. the reversed format: Consistently with prior studies, the reversed format excels over the standard format across all PEs.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7 | Effects of Random Space Augmentation (RS Aug): Random space augmentation is beneficial for RoPE and KerpleLog; adverse for NoPE and FIRE.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6 | The reversed format shows distinct grokking during training, unlike the gradual enhancement in the standard format. This phenomenon is observed across all PEs (Figure C.3)
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8 | Effects of Randomized PE: Randomized PE enhances FIRE but degrades KerpleLog
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9 | Error Distribution: Errors appear almost equally with and without carry.
it markedly deteriorates NoPE and FIRE. A similar PE-specific pattern is evident in Randomized PE, as Figure 8 demonstrates. Randomized PE significantly degrades KerpleLog's effectiveness, yet it substantially boosts FIRE. See Appendices D. 4 and D. 5 for training loss and EM accuracy for all trials in each setting.</p>
<p>Length generalization is not robust to neither weight initialization nor training data order. Figure 10 illustrates the varying performance of 10 FIRE trials using identical training data order but distinct weight initializations. Notably, while all trials achieve similar close-to-zero training losses after 10K training steps (Figure C.2) and exhibit perfect in-distribution generalization, their out-of-distribution (OOD) length generalization shows significant variance. Moreover, the length generalization performance fluctuates significantly across training steps (Appendix C.3). This observation contrasts with earlier studies suggesting in-distribution loss as a reliable OOD generalization predictor (Nagarajan et al., 2020).</p>
<p>We further examine 15 unique combinations, resulting from 3 weight initialization seeds and 5 data input orders. As shown in Figure 11, there is significant variance across training data orders even when the weight initialization is constant. Intriguingly, certain weight initializations demonstrate remarkable resilience to changes in data input order. This observation is reminiscent of the Lottery Ticket Hypothesis (Frankle and Carbin, 2018), which posits the existence of a sparse, equally effective sub-network within a larger neural network. Our findings suggest the presence of "fortunate" weight</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10 | Exact match across 10 trials using FIRE. While transformers can achieve nearperfect accuracy in 100-digit addition, the variance across different random seeds is high.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11 | Effects of weight initialization and data input order: 15 models trained on a combination of three weight initialization seeds and five data input order seeds.
configurations that exhibit robust length generalization, akin to a "lucky weight ticket."
While Anil et al. (2022) also noticed similar in-distribution accuracy but marked differences in OOD behavior on parity tasks, their OOD performance was quite poor across all runs. Moreover, contrary to the findings of Anil et al. (2022) on the impact of hyperparameter variation, our experiments reveal considerable performance fluctuations even with different random seeds. This inconsistency appears unrelated to position encoding (refer to Figure C. 1 for different PEs), and is more likely due to variations in random weight initialization and data order.</p>
<h1>5. Analysis</h1>
<p>Error analysis. In examining Transformers' error characteristics, we classified erroneous predictions into two categories: those with and without carry. Figure 9 shows no significant difference between these categories, thus carry propagation does not majorly impede length generalization.</p>
<p>Additionally, we analyzed the error distribution in 100-digit addition using FIRE, illustrated in Figure C.10. As shown, Figure C. 10 indicates an overall uniform error distribution across all indices, despite some individual model checkpoints showing errors at specific positions. Excluding two nearzero accuracy runs, over $90 \%$ of errors in incorrect examples are single-digit mistakes, following an exponential distribution. Additional results are shown in Figures C. 11 and C.12.</p>
<p>Despite the imperfect calculation, the FIRE model does not show any systematic error. Random errors may stem from phenomena such as attention glitches Liu et al. (2023a). Conversely, other PEs systematically fail to identify the start or end of addition, leading to premature termination.</p>
<p>Performance evolution during training. Figure 12 shows that while transformers achieve near-perfect in-distribution accuracy early in training, they explore different extrapolation strategies. This ability is remarkable considering the inherent unpredictability and architecture-dependent nature of OOD accuracy. Notably, transformers with FIRE exhibit a generally steady increase in OOD accuracy during training, suggesting that FIRE's inductive bias may be helpful in finding solutions that generalize to different lengths. In contrast, other PE methods display more volatile OOD performance. Interestingly, some methods exhibit a "grokking-like" phenomenon, where there is a sudden surge in the OOD accuracy despite no change in in-distribution accuracy.</p>
<p>Sequence length during training. We trained separate models for addition involving up to 10,</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12 | Comparison of In-Distribution (30digit addition) and Out-of-Distribution Generalization ( 90 -digit addition, except for RoPE at 70-digit addition).
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13 | Different training lengths: Increasing the training length significantly improves length generalization in FIRE, achieving near-perfect accuracy at length 100 .
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14 | Scaling model size inconsistently affects length generalization performance. While consistently enhancing performance in shorter length regimes (1-10, 1-20) across four PEs, this trend does not hold for larger regimes (1-30, 1-40). For instance, larger models outperform smaller ones with RoPE and KerpleLog (Figure C.14), but underperform with NoPE and FIRE. Moreover, increasing model size doesn't noticeably decrease performance variance, suggesting size scaling isn't vital for length generalization.</p>
<p>20, 30, and 40 digits, and evaluated them on addition of up to 100 digits. As depicted in Figures 13 and C.13, training length crucially improves performance in longer length generalizations across different PEs. Notably, not only that models that were trained on 40 digits generalize better than models that were trained on shorter sequences, the generalization factor is also increasing: the model that was trained on 40 digits generalizes to 100 digits $(2.5 \times)$, while the model that was trained on up to 30 digits generalizes to 45 digits $(1.5 \times)$, the model that was trained on up to 20 digits generalizes to 25 digits $(1.25 \times)$, and the model that was trained on up to 10 digits does not generalize beyond training lengths $(1.0 \times)$.</p>
<p>Scaling model size. The scaling of model size is crucial for improving large language models (Chowdhery et al., 2023; Thoppilan et al., 2022). To assess its effect on length generalization, we contrasted models with 25 M and 268 M parameters. We find that model size variation has a minor effect on length generalization. Figure 14 shows that larger models slightly improve generalization in short digit regimes ( 1 to 10 and 1 to 20 digit additions) but yield mixed results in longer regimes. While RoPE and KerpleLog show improvements, NoPE and FIRE experience performance degradation with a larger model, indicating model size may not be the primary factor in length generalization.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15 | Effect of different model sizes with FIRE as the position encoding.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16 | Effect of weight decay with FIRE as the position encoding.</p>
<p>The efficacy of length generalization in the 25 M model prompted us to explore the capabilities of smaller models. Specifically, we trained models with 2 M and 5 M parameters. As Figures 15 and C. 15 illustrate, the 2 M model's performance deteriorates with longer sequences, indicating limited model capacity as a potential performance bottleneck. Intriguingly, this model outperforms its larger counterparts ( 5 M and 25 M models) in tasks involving 1 to 10 digit addition. Furthermore, the 5 M model remarkably achieves $80 \%$ accuracy in 100 digit addition, trained only on 1 to 40 digit tasks, surpassing the 268 M model's performance.</p>
<p>Does stronger regularization reduce variance? To mitigate performance variance, we investigated standard regularization techniques, including weight decay and dropout. As depicted in Figure 16, higher weight decay values (e.g., $0.1,0.3$ ) slightly enhance the likelihood of achieving effective length generalization. Nonetheless, non-trivial length generalization remains attainable with either very low (e.g., 1e-6) or high (e.g., 1.0) weight decay values, evidenced by approximately $80 \%$ accuracy in 100 digit addition trained on 40-digit sequences. Conversely, Figure C. 17 shows that substantial dropout values (e.g., 0.2 ) severely impair length generalization. Dropout rates of 0.0 or 0.1 , however, do not show statistically significant improvements over their counterparts. Overall, while regularization can modestly decrease performance variability, it falls short in ensuring robust length generalization. The variance in performance is still significantly influenced by the randomness of weights initialization and the training data order (Figures 10 and 11).</p>
<h1>6. Related Work</h1>
<p>Length generalization remains a significant challenge in neural networks, underscored by substantial research (Deletang et al., 2023; Dziri et al., 2023; Graves et al., 2016; Hupkes et al., 2020; Schwarzschild et al., 2021; Zhang et al., 2022). Despite their advanced reasoning capabilities, Transformer-based large language models (LLMs) (Chowdhery et al., 2023; Thoppilan et al., 2022) struggle with processing sequences beyond their training scope Anil et al. (2022). Enhancements in length generalization, especially in the addition task, primarily focus on two areas: refining positional encoding and optimizing data format.</p>
<p>Position Encoding for Length Generalization The inability of Transformers to extrapolate to longer sequences has been primarily attributed to Position Encoding (PE) Shaw et al. (2018). Various studies have suggested alternatives, such as relative positional encodings, which focus on the relative distances between tokens (Dai et al., 2019), the implementation of randomized position encoding (Ruoss et al., 2023), or the adoption of weighted attention mechanisms in place of position embeddings (Chi et al., 2022; Li et al., 2023; Press et al., 2022; Raffel et al., 2020). These approaches have shown promise in natural language processing (NLP). However, Kazemnejad et al. (2023) found</p>
<p>that omitting position encoding entirely yields better results for algorithmic tasks. In contrast, our experiments indicate that an effectively designed PE, such as the FIRE, is crucial for achieving optimal length generalization (Figure 3). Moreover, we show that a synergistic approach to consider both PE and data design markedly enhances length generalization capabilities.</p>
<p>Data format for Length Generalization A range of heuristic-based data formatting methods have been introduced, particularly for pretrained LLMs. These methods, including the use of scratchpads and the chain of thoughts approach, aim to facilitate arithmetic learning either through in-context learning or fine-tuning Anil et al. (2022); Zhou et al. (2022). Conversely, there is a body of research focused on Transformers trained from scratch. This research indicates that employing techniques such as reversed formatting and scratch pads can significantly boost length generalization performance Lee et al. (2023); Shen et al. (2023). Furthermore, it has been observed that both the data distribution and the sampling strategies can profoundly influence generalization Lee et al. (2023). Awasthi and Gupta (2023) further demonstrates the benefits of incorporating a simpler auxiliary task (e.g., identifying the successor element) in supporting the primary task (e.g., sorting). In contrast, Jelassi et al. (2023) finds that train set priming enables length generalization for a encoder-only Transformer model. In contrast, our good length generalization performance achieved with naive random sampling approach suggesting that sophisticated data sampling might be redundant.</p>
<h1>7. Conclusion</h1>
<p>Length generalization in Transformers has been a long-standing challenge. We evaluate the ability of Transformers to generalize to longer test sequences using the decimal addition task. Through extensive experiments, we find that there is no inherent limitation in Transformers' design preventing effective length generalization. Instead, the missing ingredient is the right combination of data format and position encoding. We demonstrate that Transformers can achieve almost perfect generalization on sequences up to $2.5 \times$ the training length, given appropriate data formatting and position encoding.</p>
<p>Our thorough empirical analysis of common length generalization techniques reveals a significant dependency between the type of position encoding and the data format. This underscores the importance of synergizing data format with model architecture for optimal generalization. Despite these advancements, robust length generalization in Transformers remains elusive, even with meticulously finetuned regularization hyperparameters.</p>
<h2>References</h2>
<p>E. Abbe, S. Bengio, A. Lotfi, and K. Rizk. Generalization on the unseen, logic reasoning and degree curriculum. arXiv preprint arXiv:2301.13105, 2023.
C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz, V. Misra, V. Ramasesh, A. Slone, G. Gur-Ari, E. Dyer, and B. Neyshabur. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546-38556, 2022.
P. Awasthi and A. Gupta. Improving length-generalization in transformers via task hinting. arXiv preprint arXiv:2310.00726, 2023.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.</p>
<p>T.-C. Chi, T.-H. Fan, P. J. Ramadge, and A. Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:8386-8399, 2022.
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023.
Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.
G. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter, S. Legg, J. Veness, and P. A. Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=WbxHAzkeQcn.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
S. Duan and Y. Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023.
Y. Dubois, G. Dagan, D. Hupkes, and E. Bruni. Location attention for extrapolation to longer sequences. arXiv preprint arXiv:1911.03872, 2019.
N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jian, B. Y. Lin, P. West, C. Bhagavatula, R. L. Bras, J. D. Hwang, et al. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023.
J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.</p>
<p>Gemini, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwinska, S. G. Colmenarejo, E. Grefenstette, T. Ramalho, J. P. Agapiou, A. P. Badia, K. M. Hermann, Y. Zwols, G. Ostrovski, A. Cain, H. King, C. Summerfield, P. Blunsom, K. Kavukcuoglu, and D. Hassabis. Hybrid computing using a neural network with dynamic external memory. Nat., 538(7626):471-476, 2016. doi: 10.1038/NATURE20101. URL https://doi.org/10.1038/nature20101.
A. Haviv, O. Ram, O. Press, P. Izsak, and O. Levy. Transformer language models without positional encodings still learn positional information. arXiv preprint arXiv:2203.16634, 2022.
D. Hupkes, V. Dankers, M. Mul, and E. Bruni. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research, 67:757-795, 2020.
S. Jelassi, S. d'Ascoli, C. Domingo-Enrich, Y. Wu, Y. Li, and F. Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023.
A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy. The impact of positional encoding on length generalization in transformers. arXiv preprint arXiv:2305.19466, 2023.
N. Lee, K. Sreenivasan, J. D. Lee, K. Lee, and D. Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.</p>
<p>A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.
S. Li, C. You, G. Guruganesh, J. Ainslie, S. Ontanon, M. Zaheer, S. Sanghai, Y. Yang, S. Kumar, and S. Bhojanapalli. Functional interpolation for relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023.
Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022.
B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Exposing attention glitches with flip-flop language modeling. arXiv preprint arXiv:2306.00946, 2023a.
B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata. In The Eleventh International Conference on Learning Representations, 2023b. URL https:// openreview.net/forum?id=De4FYqjFueZ.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
M. Motamedi, N. Sakharnykh, and T. Kaldewey. A data-centric approach for training deep neural networks with less data. arXiv preprint arXiv:2110.03613, 2021.
V. Nagarajan, A. Andreassen, and B. Neyshabur. Understanding the failure modes of out-of-distribution generalization. arXiv preprint arXiv:2010.15775, 2020.
C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api. semanticscholar.org/CorpusID:257532815.
B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.
A. Power, Y. Burda, H. Edwards, I. Babuschkin, and V. Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
O. Press, N. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.
A. Ruoss, G. Delétang, T. Genewein, J. Grau-Moya, R. Csordás, M. Bennani, S. Legg, and J. Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023.
A. Schwarzschild, E. Borgnia, A. Gupta, F. Huang, U. Vishkin, M. Goldblum, and T. Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34:6695-6706, 2021.</p>
<p>P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.
N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
R. Shen, S. Bubeck, R. Eldan, Y. T. Lee, Y. Li, and Y. Zhang. Positional description matters for transformers arithmetic. arXiv preprint arXiv:2311.14737, 2023.
D. R. So, W. Mańke, H. Liu, Z. Dai, N. Shazeer, and Q. V. Le. Primer: Searching for efficient transformers for language modeling. arXiv preprint arXiv:2109.08668, 2021.
J. Su. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023.
J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
P. Veličković, A. P. Badia, D. Budden, R. Pascanu, A. Banino, M. Dashevskiy, R. Hadsell, and C. Blundell. The clrs algorithmic reasoning benchmark. In International Conference on Machine Learning, pages 22084-22102. PMLR, 2022.
Y. Wu, A. Q. Jiang, W. Li, M. Rabe, C. Staats, M. Jamnik, and C. Szegedy. Autoformalization with large language models. Advances in Neural Information Processing Systems, 35:32353-32368, 2022.
Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling transformers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.
H. Zhou, A. Nova, H. Larochelle, A. Courville, B. Neyshabur, and H. Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.
H. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. Susskind, S. Bengio, and P. Nakkiran. What algorithms can transformers learn? a study in length generalization. arXiv preprint arXiv:2310.16028, 2023.</p>
<h1>A. Positional Encoding</h1>
<h2>A.1. Additive Relative Positional Encoding (RPE)</h2>
<p>Shaw et al. (2018) pioneered additive RPE by integrating position encodings into the attention layer's key, and optionally the value, rather than the input. This concept was further simplified in T5 (Raffel et al., 2020), where the vector representations of relative positions are simplified to scalar biases added to pre-softmax attention logits. Subsequent advancements in additive RPE, aimed at enhancing length generalization and computational efficiency, include notable methods like Alibi (Press et al., 2022), Kerple (Chi et al., 2022), and FIRE (Li et al., 2023). A commonality among these methods is the unified computation formula for pre-softmax attention logits, as outlined by Li et al. (2023):</p>
<p>$$
A_{\mathrm{RPE}}(X)=X W_{Q}\left(X W_{K}\right)^{\top}+B
$$</p>
<p>where the bias matrix $B \in \mathbb{R}^{n \times n}$ is induced by the position encoding function $b: \mathbb{N}^{* 2} \rightarrow \mathbb{R}$, has its $(i, j)$-th entry defined as $b(i, j)$. Variations in $b$ 's formulations and parameterizations give rise to diverse RPE variants.</p>
<ul>
<li>T5 (Raffel et al., 2020): T5's RPE segments relative distances into distinct buckets with a logarithmic scale, each associated with a unique parameter. With $K+1$ buckets and a predefined distance $L_{1}$, the attention bias is calculated as (assuming $K+1$ is even)</li>
</ul>
<p>$$
b(i, j)= \begin{cases}r_{i-j} &amp; 0 \leq i-j&lt;\frac{K+1}{2} \ r_{\frac{K+1}{2}+\left\lfloor\frac{K+1}{2} \log \left(\frac{2(i-j)}{K+1}\right) / \log \left(\frac{2 i-1}{K+1}\right)\right\rfloor} &amp; \frac{K+1}{2} \leq i-j&lt;L_{1} \ r_{K} &amp; i-j \geq L_{1}\end{cases}
$$</p>
<ul>
<li>Alibi (Press et al., 2022): $b(i, j)=-r|i-j|$, where $r&gt;0$ is a hyper-parameter.</li>
<li>Kerple (Chi et al., 2022): $b(i, j)=-r_{1} \log \left(1+r_{2}|i-j|\right)$ (logarithmic variant) or $-r_{1}|i-j|^{r_{2}}$ (power variant), where $r_{1}, r_{2}&gt;0$ are learnable scalars.</li>
<li>FIRE (Li et al., 2023): $b(i, j)=f_{\theta}\left(\frac{\psi(i-j)}{\psi(\max {L, i})}\right)$, where $f_{\theta}: \mathbb{R} \rightarrow \mathbb{R}$ is a learnable MLP parameterized by $\theta, \psi: \mathbb{N} \rightarrow \mathbb{R}_{+}$is monotonically increasing and $L&gt;0$ is a learnable scalar.</li>
</ul>
<h1>B. Implementation Details</h1>
<h2>B.1. Data Generation</h2>
<p>As shown in Figure 2, we adopt the reversed format with index hints as our default data format. During training, we randomly sample a consecutive index hints from a pre-defined ordered index set with 102 distinct symbols, thereby enhancing the learning of hint sequences and their order. At inference, the same hint sampling strategy is applied to questions, prompting the model for answers.</p>
<p>To generate addition examples, we opt for a naive random sampling approach instead of structured data sampling Lee et al. (2023), as our analysis indicates that carry operations are not a major hindrance to length generalization (See Figure 9). Our approach involves uniformly selecting the number's length from 1 to the maximum training length, followed by independent sampling of two operands based on this length, with an additional zero padding to accommodate potential carryinduced extra digits. For training, datasets comprising 30M, 40M, 60M, and 120M examples are generated for number lengths 1-40, 1-30, 1-20, and 1-10, respectively. In contrast, the test set consists of 1,000 examples per digit length.</p>
<h2>B.2. Training Details</h2>
<p>Our base model, following Zhou et al. (2023), is a 25M parameter Transformer featuring 6 blocks, a 512 hidden size, a feedforward layer with a hidden dimension of 2048 using GeGLU activation (Shazeer, 2020), and an 8 -head attention mechanism. We also adopt RMSNorm, integrating both PreNorm and PostNorm layers, following the Primer architecture (So et al., 2021). Additionally, our preliminary investigations underscore the significance of employing causal language modeling when applying the index hint technique. Conversely, attempts to leverage prefix language modeling paired with bidirectional attention in model inputs consistently falter in length generalization. Our three other model variants with size [2M, 5M, 268M] consist of [2, 4, 16] blocks, a [256, 256, 1024] hidden size, a feedforward layer with a hidden dimension of [1024, 1024, 4096], and a [4, 4, 16]-head attention mechanism, respectively.</p>
<p>In our implementation of FIRE Li et al. (2023), we employ layerwise sharing of attention bias across all attention blocks to enhance training efficiency. The paraterization of FIRE consists of a 2-layer MLP with a 32-unit hidden layer, utilizing ReLU activation.</p>
<p>We use the AdamW optimizer (Loshchilov and Hutter, 2017) to train the model with a weight decay value of 0.1 and dropout rate of 0.0 . The learning rate schedule incorporates an initial 500-step linear warm-up, followed by a cosine decay, starting at $3 \mathrm{e}-4$. We train the model with sequence packing, a batch size of 128, and a sequence length of 2048, over 50,000 steps. We use greedy decoding to generate the model output during evaluation. We summarize the hyperparameters in Table B.1.</p>
<p>Table B. 1 | Hyperparameters Summary for Length Generalization</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Language Model Type</td>
<td style="text-align: left;">Causal</td>
</tr>
<tr>
<td style="text-align: left;">Activation Functions</td>
<td style="text-align: left;">GeGLU</td>
</tr>
<tr>
<td style="text-align: left;">Normalization Layer</td>
<td style="text-align: left;">RMSNorm</td>
</tr>
<tr>
<td style="text-align: left;">Normalization Type</td>
<td style="text-align: left;">PreNorm and PostNorm</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: left;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">Training Steps</td>
<td style="text-align: left;">50,000</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: left;">128</td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate (LR)</td>
<td style="text-align: left;">0.0003</td>
</tr>
<tr>
<td style="text-align: left;">LR Warmup Steps</td>
<td style="text-align: left;">500</td>
</tr>
<tr>
<td style="text-align: left;">LR Cooldown (Begin, End)</td>
<td style="text-align: left;">(500, 50,000)</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Schedule</td>
<td style="text-align: left;">Linear (from 0 to LR)</td>
</tr>
<tr>
<td style="text-align: left;">Cooldown Schedule</td>
<td style="text-align: left;">Cosine Decay (from LR to 0.1LR)</td>
</tr>
<tr>
<td style="text-align: left;">Training Sequence Length</td>
<td style="text-align: left;">2048</td>
</tr>
<tr>
<td style="text-align: left;">Evaluation</td>
<td style="text-align: left;">Greedy</td>
</tr>
</tbody>
</table>
<h1>C. Additional Results</h1>
<h2>C.1. Training Loss and Sequence Exact Match Accuracy of Reverse Format with Index Hint trained up to 40</h2>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure C. 1 | Exact match accuracy on 20 to 100 digit addition of all 10 trials trained on up to 40-digit addition with index hint and reverse format using four different position encodings.
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure C. 2 | Training loss over 10 trials in reverse formats. Despite similar nearly 0 log perplexity losses across runs after 10K training steps, different runs exhibit very different length generalization.</p>
<h1>C.2. Training Loss and Next-token Prediction Accuracy of Standard and Reverse Format</h1>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure C. 3 | Training log perplexity and next-token prediction accuracy over 10 trials in standard versus reverse formats using RoPE, KerpleLog, NoPE and FIRE. Reverse format shows distinct grokking during training, unlike the gradual enhancement in standard format.</p>
<h1>C.3. The evolution of EM Accuracy during training in reverse format using 4 PEs</h1>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure C. 4 | Exact match accuracy on $[30,50,70,90]$ digit addition of all 10 trials trained on up to 40-digit addition with index hint and reverse format using RoPE.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure C. 5 | Exact match accuracy on $[30,50,70,90]$ digit addition of all 10 trials trained on up to 40-digit addition with index hint and reverse format using KerpleLog.</p>
<p><img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure C. 6 | Exact match accuracy on $[30,50,70,90]$ digit addition of all 10 trials trained on up to 40-digit addition with index hint and reverse format using NoPE.
<img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Figure C. 7 | Exact match accuracy on $[30,50,70,90]$ digit addition of all 10 trials trained on up to 40-digit addition with index hint and reverse format using FIRE.</p>            </div>
        </div>

    </div>
</body>
</html>
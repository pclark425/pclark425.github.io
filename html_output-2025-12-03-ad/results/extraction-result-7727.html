<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7727 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7727</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7727</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-277349293</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.21248v2.pdf" target="_blank">ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as"research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7727.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7727.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale benchmark and agentic pipeline that decomposes scientific hypothesis formulation into three sub-tasks (inspiration retrieval, hypothesis composition, hypothesis ranking) and uses LLM-based agents to extract components (research question, background, inspirations, hypotheses) from 1,386 papers across 12 disciplines for evaluating LLMs' discovery capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ResearchBench LLM-based agentic framework</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An LLM-driven multi-module pipeline that (1) extracts a paper's research question and background via iterative self-refine summarization of introduction/related work, (2) decomposes and retrieves inspiration papers (titles + abstracts) by iteratively proposing referenced works and verifying necessity/sufficiency via LLM checks and external metadata (Semantic Scholar, Crossref), and (3) generates hypotheses via an 'evolutionary unit' process (mutate/refine/recombine) and ranks hypotheses pairwise using LLM judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full paper passages (introduction/related work/methods), titles and abstracts of candidate inspiration papers, citation metadata (Crossref, Semantic Scholar), constructed negative-candidate pools</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured decomposition: research question (text), background survey (text summary), list of inspiration papers (title+abstract), generated hypotheses (text), ranked hypotheses (pairwise ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Iterative self-refine extraction, agentic multi-step prompting, evolutionary prompts (mutate/refine/recombine), iterative selection rounds for retrieval, pairwise ranking prompts; negative-sample construction for retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLM APIs used for extraction and evaluation (GPT-4o, GPT-4o Mini, Gemini 2.0 Flash/FT, Claude 3.5 Haiku/Sonnet, Llama-3 variants, Qwen, DeepSeek-V3, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Various (API models plus Llama-3.1-8B, Llama-3.1-70B reported among others)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>A curated corpus of 1,386 papers published in 2024 across 12 disciplines (collected from top venues); candidate negative pools drawn from Crossref, Semantic Scholar, Web of Science (~2000 papers per discipline)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Inspiration retrieval: Hit Ratio (inclusion of ground-truth inspirations in top-k selection after iterative rounds); Hypothesis composition: normalized matched score (6-point Likert -> normalized to [0,1]); Hypothesis ranking: pairwise accuracy (averaged over reversed-order comparisons); analysis of position bias</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLMs perform well at inspiration retrieval (e.g., GPT-4o included a ground-truth inspiration in top 4% with probability ~45.7%), moderate performance in hypothesis composition (normalized scores varying across models; best models ~0.56 normalized), and mixed ranking results influenced by position bias; inspiration retrieval shows rapid gains up to ~8B parameter scale then plateaus around ~70B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Decomposition extraction not perfect (expert-checked accuracy ~91.9% major-issue level, 82.3% including minor issues); possible residual data contamination despite 2024-only cutoff; inspiration retrieval identified as bottleneck with plateauing returns from scale; LLM position bias in pairwise ranking; reliance on external metadata/APIs for inspirations; potential hallucination or incorrect necessity/sufficiency judgements by LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7727.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentic Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based agentic framework for automated decomposition and extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular agentic pipeline using LLMs to iteratively extract research questions, background surveys, inspirations (referenced papers), and main hypotheses from full-text scholarly articles, augmented by external metadata lookups and necessity/sufficiency checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-based agentic extraction framework</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses carefully designed prompts and iterative self-refine to summarize background and question; an 'inspiration decomposition' module proposes candidate inspiration paper titles which are resolved to abstracts via Semantic Scholar/Crossref, followed by LLM 'necessary' and 'sufficient' checkers to prune and validate inspirations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-paper passages (introduction, related work, references), title/abstract retrieval via Semantic Scholar/Crossref</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured components: research question, background survey, list of necessary/sufficient inspiration paper summaries, ground-truth hypothesis extraction</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Iterative self-refine prompting, multi-step agentic prompts (decompose → fetch metadata → necessary checker → sufficient checker)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Not a single model — pipeline implemented using LLM APIs (models used for experiments included GPT-4o, Gemini 2.0, Claude 3.5, Llama variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>1,386 2024 papers across 12 disciplines; Semantic Scholar/Crossref for referenced-paper metadata</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human expert validation of decomposition accuracy; downstream retrieval/composition/ranking metrics (Hit Ratio, matched-score, pairwise accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>High extraction accuracy by the pipeline (expert-validated ~91.9% major-issue accuracy), enabling scalable benchmark creation and contamination-resistant dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extraction errors remain (~8–18% depending on strictness); depends on availability and correctness of external metadata; performance tied to LLM pretraining coverage; potential for subtle data leakage or hallucinated reference mapping</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7727.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evolutionary Unit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary unit (mutate, recombine, refine) for hypothesis composition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedural method to generate research hypotheses by iteratively mutating ways to combine background and inspiration knowledge, refining candidates based on feedback, and recombining strengths of multiple candidates to produce final hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Evolutionary unit (mutate/refine/recombine)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Operates on pairs of background and single inspiration inputs where the LLM 'mutates' to create diverse combination strategies, 'refines' selected candidates given extra knowledge and feedback, and 'recombines' the best parts of different candidates to generate robust hypothesis proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Background survey text and one inspiration paper's abstract/content (ground-truth inspirations used during evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated research hypotheses (textual proposals), multiple candidates per input combination</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Custom prompts for mutate, refine, and recombine stages (iterative LLM prompting; no external fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Applied over benchmarked papers and ground-truth inspirations from ResearchBench; originally motivated/cited from MOOSE/MOOSE-Chem work</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Normalized matched score (6-point Likert scale normalized to [0,1]) comparing generated hypotheses to ground-truth key points; downstream ranking experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Enables hypothesis generation that captures subsets of ground-truth innovations; performance varies by model with top systems achieving moderate normalized scores (~0.55–0.57 for best models)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Hypothesis composition remains challenging — no model consistently achieves high coverage of ground-truth key points; quality sensitive to choice of inspirations and LLM capabilities; potential to generate plausible but incorrect or untestable methods</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7727.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/interaction technique where an LLM iteratively critiques and refines its own outputs to improve extraction or generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses an LLM to generate an initial output then asks the model to critique and iteratively refine that output, improving accuracy of summaries or structured extraction without external fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text passages (e.g., paper introduction, related work)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved/refined textual summaries or extracted components</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Iterative self-critique and regeneration using LLM prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as enabling more accurate extraction in the ResearchBench pipeline (used to extract research question/background/hypothesis), original paper shows iterative refinement improves output quality</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Can converge to locally consistent but incorrect outputs; depends on model's internal calibration and critique ability; may not fully eliminate hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7727.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoLM (Generative inductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method investigating generative inductive reasoning by proposing (commonsense) hypotheses from observations using language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yang et al. (cited as Yang et al., 2024a)</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CoLM (generative inductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to perform inductive reasoning to propose hypotheses from observed data/observations (focus on generating hypotheses rather than retrieval/composition pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Observations/structured inputs (as described in CoLM; exact inputs not specified in ResearchBench text)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated hypotheses (text)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7727.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON (Scientific Inspiration Machines / literature-based discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-based discovery system showing how retrieved concepts from literature can aid hypothesis composition, positioned in related work as an example of LLMs applied to literature-driven discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Wang et al. (cited as Wang et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciMON (literature-based discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Retrieves and composes literature-derived concepts to support hypothesis generation; emphasizes using retrieved concepts as inspirations for composing hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Literature corpus (titles/abstracts/full text depending on implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Concepts/inspiration candidates and composed hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7727.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE / MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE / MOOSE-Chem: LLMs for rediscovering hypotheses (social science / chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Works that characterize many hypotheses as emerging from a research background plus inspirations and demonstrate LLM utility in rediscovering or proposing domain hypotheses (MOOSE for social science, MOOSE-Chem for chemistry).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zonglin Yang et al. (cited as Yang et al., 2024b / 2024c)</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MOOSE / MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Demonstrates that many hypotheses can be modeled as combinations of background and inspirations and uses LLMs to rediscover or propose hypotheses, motivating the inspiration+background decomposition used in ResearchBench.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Domain-specific literature and background summaries (chemistry/social science corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated hypotheses or rediscovered known hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7727.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IdeaBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IdeaBench: Benchmarking large language models for research idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical-domain benchmark designed for idea/hypothesis generation from background knowledge; criticized in ResearchBench for focusing only on generation from background and relying on rule-based reference extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ideabench: Benchmarking large language models for research idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Sikun Guo et al. (cited as Guo et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IdeaBench</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Evaluates LLMs' ability to generate biomedical research ideas/hypotheses from background information; does not include explicit inspiration retrieval/integration sub-tasks and uses rule-based reference extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Biomedical background knowledge and possibly prior methods descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated research ideas/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Biomedical literature (domain-specific corpora as per IdeaBench)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human evaluation of idea quality (as reported by IdeaBench; ResearchBench criticizes limited sub-task coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Domain-restricted (biomedical), limited to hypothesis generation without inspiration retrieval task, rule-based extraction less accurate than LLM-based extraction</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7727.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoveryBench: Towards data-driven discovery with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior effort selecting specific discovery-relevant tasks from a small set of papers to evaluate LLMs in data-driven discovery, but not decomposing the discovery process into the full inspiration-composition-ranking pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discoverybench: Towards data-driven discovery with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Prasad Bodhisattwa et al. (cited as Majumder et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Selects and evaluates specific discovery-related tasks drawn from a handful of papers to study LLMs' data-driven discovery capabilities (e.g., code writing); does not adopt the inspiration+background decomposition used by ResearchBench.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Selected paper tasks and related artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Task-specific outputs (e.g., code, explanations, small task solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Small curated set of discovery-relevant papers (20 papers cited in ResearchBench)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Small scope and task selection; lacks a unified decomposition of scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7727.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScienceAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ScienceAgentBench: Rigorous assessment of language agents for data-driven scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that identifies specific discovery-relevant tasks from a set of papers to evaluate language agents, but does not analyze the fundamental decomposition of the scientific discovery process into inspiration retrieval, composition, and ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ziru Chen et al. (cited as Chen et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ScienceAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Evaluates language agents on curated data-driven scientific tasks extracted from papers (e.g., write code, analyze data) to assess agentic scientific capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Selected discovery task materials from a set of papers (44 papers mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Task-specific agent outputs</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>44 selected papers and associated task artifacts (per ScienceAgentBench)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not target the full inspiration-composition-ranking decomposition highlighted by ResearchBench</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7727.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for generating novel scientific inspirations from literature, optimizing for novelty as a driver of creative hypothesis formation (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Retrieves and recommends literature-based inspirations optimized for novelty to encourage creative hypothesis composition.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Literature corpus metadata and content</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Novel inspiration candidates (papers/concepts) and possibly ranked suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7727.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yang et al. 2024b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for automated open-domain scientific hypotheses discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that explores automated open-domain hypothesis discovery with LLMs, motivating ResearchBench's decomposition and evaluation of hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for automated open-domain scientific hypotheses discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Automated open-domain hypothesis discovery (Yang et al., 2024b)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to discover hypotheses in an open-domain setting, supporting the notion that LLMs can propose valid scientific hypotheses from literature/background inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7727.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7727.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language Models as Inductive Reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models as inductive reasoners (LLMs for inductive/analogical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A line of work framing LLMs as inductive reasoners to generalize from instances and propose hypotheses or explanations; cited as related work informing ResearchBench's decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as inductive reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Inductive reasoning with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Treats LLMs as models that can induce general hypotheses from observed facts or text evidence, enabling hypothesis proposal and reasoning about unseen associations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hypotheses/explanations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery. <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty. <em>(Rating: 2)</em></li>
                <li>Ideabench: Benchmarking large language models for research idea generation. <em>(Rating: 2)</em></li>
                <li>Discoverybench: Towards data-driven discovery with large language models. <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. <em>(Rating: 1)</em></li>
                <li>Language models as inductive reasoners. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7727",
    "paper_id": "paper-277349293",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "ResearchBench",
            "name_full": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "brief_description": "A large-scale benchmark and agentic pipeline that decomposes scientific hypothesis formulation into three sub-tasks (inspiration retrieval, hypothesis composition, hypothesis ranking) and uses LLM-based agents to extract components (research question, background, inspirations, hypotheses) from 1,386 papers across 12 disciplines for evaluating LLMs' discovery capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou",
            "year": 2025,
            "method_name": "ResearchBench LLM-based agentic framework",
            "method_description": "An LLM-driven multi-module pipeline that (1) extracts a paper's research question and background via iterative self-refine summarization of introduction/related work, (2) decomposes and retrieves inspiration papers (titles + abstracts) by iteratively proposing referenced works and verifying necessity/sufficiency via LLM checks and external metadata (Semantic Scholar, Crossref), and (3) generates hypotheses via an 'evolutionary unit' process (mutate/refine/recombine) and ranks hypotheses pairwise using LLM judgments.",
            "input_type": "Full paper passages (introduction/related work/methods), titles and abstracts of candidate inspiration papers, citation metadata (Crossref, Semantic Scholar), constructed negative-candidate pools",
            "output_type": "Structured decomposition: research question (text), background survey (text summary), list of inspiration papers (title+abstract), generated hypotheses (text), ranked hypotheses (pairwise ordering)",
            "prompting_technique": "Iterative self-refine extraction, agentic multi-step prompting, evolutionary prompts (mutate/refine/recombine), iterative selection rounds for retrieval, pairwise ranking prompts; negative-sample construction for retrieval evaluation",
            "model_name": "Various LLM APIs used for extraction and evaluation (GPT-4o, GPT-4o Mini, Gemini 2.0 Flash/FT, Claude 3.5 Haiku/Sonnet, Llama-3 variants, Qwen, DeepSeek-V3, etc.)",
            "model_size": "Various (API models plus Llama-3.1-8B, Llama-3.1-70B reported among others)",
            "datasets_used": "A curated corpus of 1,386 papers published in 2024 across 12 disciplines (collected from top venues); candidate negative pools drawn from Crossref, Semantic Scholar, Web of Science (~2000 papers per discipline)",
            "evaluation_metric": "Inspiration retrieval: Hit Ratio (inclusion of ground-truth inspirations in top-k selection after iterative rounds); Hypothesis composition: normalized matched score (6-point Likert -&gt; normalized to [0,1]); Hypothesis ranking: pairwise accuracy (averaged over reversed-order comparisons); analysis of position bias",
            "reported_results": "LLMs perform well at inspiration retrieval (e.g., GPT-4o included a ground-truth inspiration in top 4% with probability ~45.7%), moderate performance in hypothesis composition (normalized scores varying across models; best models ~0.56 normalized), and mixed ranking results influenced by position bias; inspiration retrieval shows rapid gains up to ~8B parameter scale then plateaus around ~70B.",
            "limitations": "Decomposition extraction not perfect (expert-checked accuracy ~91.9% major-issue level, 82.3% including minor issues); possible residual data contamination despite 2024-only cutoff; inspiration retrieval identified as bottleneck with plateauing returns from scale; LLM position bias in pairwise ranking; reliance on external metadata/APIs for inspirations; potential hallucination or incorrect necessity/sufficiency judgements by LLM agents.",
            "counterpoint": true,
            "uuid": "e7727.0",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Agentic Framework",
            "name_full": "LLM-based agentic framework for automated decomposition and extraction",
            "brief_description": "A modular agentic pipeline using LLMs to iteratively extract research questions, background surveys, inspirations (referenced papers), and main hypotheses from full-text scholarly articles, augmented by external metadata lookups and necessity/sufficiency checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou",
            "year": 2025,
            "method_name": "LLM-based agentic extraction framework",
            "method_description": "Uses carefully designed prompts and iterative self-refine to summarize background and question; an 'inspiration decomposition' module proposes candidate inspiration paper titles which are resolved to abstracts via Semantic Scholar/Crossref, followed by LLM 'necessary' and 'sufficient' checkers to prune and validate inspirations.",
            "input_type": "Full-paper passages (introduction, related work, references), title/abstract retrieval via Semantic Scholar/Crossref",
            "output_type": "Structured components: research question, background survey, list of necessary/sufficient inspiration paper summaries, ground-truth hypothesis extraction",
            "prompting_technique": "Iterative self-refine prompting, multi-step agentic prompts (decompose → fetch metadata → necessary checker → sufficient checker)",
            "model_name": "Not a single model — pipeline implemented using LLM APIs (models used for experiments included GPT-4o, Gemini 2.0, Claude 3.5, Llama variants)",
            "model_size": null,
            "datasets_used": "1,386 2024 papers across 12 disciplines; Semantic Scholar/Crossref for referenced-paper metadata",
            "evaluation_metric": "Human expert validation of decomposition accuracy; downstream retrieval/composition/ranking metrics (Hit Ratio, matched-score, pairwise accuracy)",
            "reported_results": "High extraction accuracy by the pipeline (expert-validated ~91.9% major-issue accuracy), enabling scalable benchmark creation and contamination-resistant dataset construction",
            "limitations": "Extraction errors remain (~8–18% depending on strictness); depends on availability and correctness of external metadata; performance tied to LLM pretraining coverage; potential for subtle data leakage or hallucinated reference mapping",
            "counterpoint": true,
            "uuid": "e7727.1",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Evolutionary Unit",
            "name_full": "Evolutionary unit (mutate, recombine, refine) for hypothesis composition",
            "brief_description": "A procedural method to generate research hypotheses by iteratively mutating ways to combine background and inspiration knowledge, refining candidates based on feedback, and recombining strengths of multiple candidates to produce final hypotheses.",
            "citation_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "mention_or_use": "use",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou",
            "year": 2025,
            "method_name": "Evolutionary unit (mutate/refine/recombine)",
            "method_description": "Operates on pairs of background and single inspiration inputs where the LLM 'mutates' to create diverse combination strategies, 'refines' selected candidates given extra knowledge and feedback, and 'recombines' the best parts of different candidates to generate robust hypothesis proposals.",
            "input_type": "Background survey text and one inspiration paper's abstract/content (ground-truth inspirations used during evaluation)",
            "output_type": "Generated research hypotheses (textual proposals), multiple candidates per input combination",
            "prompting_technique": "Custom prompts for mutate, refine, and recombine stages (iterative LLM prompting; no external fine-tuning)",
            "model_name": null,
            "model_size": null,
            "datasets_used": "Applied over benchmarked papers and ground-truth inspirations from ResearchBench; originally motivated/cited from MOOSE/MOOSE-Chem work",
            "evaluation_metric": "Normalized matched score (6-point Likert scale normalized to [0,1]) comparing generated hypotheses to ground-truth key points; downstream ranking experiments",
            "reported_results": "Enables hypothesis generation that captures subsets of ground-truth innovations; performance varies by model with top systems achieving moderate normalized scores (~0.55–0.57 for best models)",
            "limitations": "Hypothesis composition remains challenging — no model consistently achieves high coverage of ground-truth key points; quality sensitive to choice of inspirations and LLM capabilities; potential to generate plausible but incorrect or untestable methods",
            "counterpoint": true,
            "uuid": "e7727.2",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "A prompting/interaction technique where an LLM iteratively critiques and refines its own outputs to improve extraction or generation quality.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark",
            "year": 2023,
            "method_name": "Self-refine (iterative self-feedback)",
            "method_description": "Uses an LLM to generate an initial output then asks the model to critique and iteratively refine that output, improving accuracy of summaries or structured extraction without external fine-tuning.",
            "input_type": "Text passages (e.g., paper introduction, related work)",
            "output_type": "Improved/refined textual summaries or extracted components",
            "prompting_technique": "Iterative self-critique and regeneration using LLM prompts",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as enabling more accurate extraction in the ResearchBench pipeline (used to extract research question/background/hypothesis), original paper shows iterative refinement improves output quality",
            "limitations": "Can converge to locally consistent but incorrect outputs; depends on model's internal calibration and critique ability; may not fully eliminate hallucinations",
            "counterpoint": null,
            "uuid": "e7727.3",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CoLM",
            "name_full": "CoLM (Generative inductive reasoning)",
            "brief_description": "A method investigating generative inductive reasoning by proposing (commonsense) hypotheses from observations using language models.",
            "citation_title": null,
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Yang et al. (cited as Yang et al., 2024a)",
            "year": 2024,
            "method_name": "CoLM (generative inductive reasoning)",
            "method_description": "Uses LLMs to perform inductive reasoning to propose hypotheses from observed data/observations (focus on generating hypotheses rather than retrieval/composition pipeline).",
            "input_type": "Observations/structured inputs (as described in CoLM; exact inputs not specified in ResearchBench text)",
            "output_type": "Generated hypotheses (text)",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7727.4",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SciMON",
            "name_full": "SciMON (Scientific Inspiration Machines / literature-based discovery)",
            "brief_description": "A literature-based discovery system showing how retrieved concepts from literature can aid hypothesis composition, positioned in related work as an example of LLMs applied to literature-driven discovery.",
            "citation_title": null,
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Wang et al. (cited as Wang et al., 2024)",
            "year": 2024,
            "method_name": "SciMON (literature-based discovery)",
            "method_description": "Retrieves and composes literature-derived concepts to support hypothesis generation; emphasizes using retrieved concepts as inspirations for composing hypotheses.",
            "input_type": "Literature corpus (titles/abstracts/full text depending on implementation)",
            "output_type": "Concepts/inspiration candidates and composed hypotheses",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7727.5",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MOOSE / MOOSE-Chem",
            "name_full": "MOOSE / MOOSE-Chem: LLMs for rediscovering hypotheses (social science / chemistry)",
            "brief_description": "Works that characterize many hypotheses as emerging from a research background plus inspirations and demonstrate LLM utility in rediscovering or proposing domain hypotheses (MOOSE for social science, MOOSE-Chem for chemistry).",
            "citation_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Zonglin Yang et al. (cited as Yang et al., 2024b / 2024c)",
            "year": 2024,
            "method_name": "MOOSE / MOOSE-Chem",
            "method_description": "Demonstrates that many hypotheses can be modeled as combinations of background and inspirations and uses LLMs to rediscover or propose hypotheses, motivating the inspiration+background decomposition used in ResearchBench.",
            "input_type": "Domain-specific literature and background summaries (chemistry/social science corpora)",
            "output_type": "Generated hypotheses or rediscovered known hypotheses",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7727.6",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "IdeaBench",
            "name_full": "IdeaBench: Benchmarking large language models for research idea generation",
            "brief_description": "A biomedical-domain benchmark designed for idea/hypothesis generation from background knowledge; criticized in ResearchBench for focusing only on generation from background and relying on rule-based reference extraction.",
            "citation_title": "Ideabench: Benchmarking large language models for research idea generation.",
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Sikun Guo et al. (cited as Guo et al., 2024)",
            "year": 2024,
            "method_name": "IdeaBench",
            "method_description": "Evaluates LLMs' ability to generate biomedical research ideas/hypotheses from background information; does not include explicit inspiration retrieval/integration sub-tasks and uses rule-based reference extraction.",
            "input_type": "Biomedical background knowledge and possibly prior methods descriptions",
            "output_type": "Generated research ideas/hypotheses",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": "Biomedical literature (domain-specific corpora as per IdeaBench)",
            "evaluation_metric": "Human evaluation of idea quality (as reported by IdeaBench; ResearchBench criticizes limited sub-task coverage)",
            "reported_results": null,
            "limitations": "Domain-restricted (biomedical), limited to hypothesis generation without inspiration retrieval task, rule-based extraction less accurate than LLM-based extraction",
            "counterpoint": null,
            "uuid": "e7727.7",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DiscoveryBench",
            "name_full": "DiscoveryBench: Towards data-driven discovery with large language models",
            "brief_description": "A prior effort selecting specific discovery-relevant tasks from a small set of papers to evaluate LLMs in data-driven discovery, but not decomposing the discovery process into the full inspiration-composition-ranking pipeline.",
            "citation_title": "Discoverybench: Towards data-driven discovery with large language models.",
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Prasad Bodhisattwa et al. (cited as Majumder et al., 2024)",
            "year": 2024,
            "method_name": "DiscoveryBench",
            "method_description": "Selects and evaluates specific discovery-related tasks drawn from a handful of papers to study LLMs' data-driven discovery capabilities (e.g., code writing); does not adopt the inspiration+background decomposition used by ResearchBench.",
            "input_type": "Selected paper tasks and related artifacts",
            "output_type": "Task-specific outputs (e.g., code, explanations, small task solutions)",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": "Small curated set of discovery-relevant papers (20 papers cited in ResearchBench)",
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": "Small scope and task selection; lacks a unified decomposition of scientific discovery",
            "counterpoint": null,
            "uuid": "e7727.8",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ScienceAgentBench",
            "name_full": "ScienceAgentBench: Rigorous assessment of language agents for data-driven scientific discovery",
            "brief_description": "A benchmark that identifies specific discovery-relevant tasks from a set of papers to evaluate language agents, but does not analyze the fundamental decomposition of the scientific discovery process into inspiration retrieval, composition, and ranking.",
            "citation_title": "Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery.",
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Ziru Chen et al. (cited as Chen et al., 2024)",
            "year": 2024,
            "method_name": "ScienceAgentBench",
            "method_description": "Evaluates language agents on curated data-driven scientific tasks extracted from papers (e.g., write code, analyze data) to assess agentic scientific capabilities.",
            "input_type": "Selected discovery task materials from a set of papers (44 papers mentioned)",
            "output_type": "Task-specific agent outputs",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": "44 selected papers and associated task artifacts (per ScienceAgentBench)",
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": "Does not target the full inspiration-composition-ranking decomposition highlighted by ResearchBench",
            "counterpoint": null,
            "uuid": "e7727.9",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Scimon",
            "name_full": "Scimon: Scientific inspiration machines optimized for novelty",
            "brief_description": "A system for generating novel scientific inspirations from literature, optimizing for novelty as a driver of creative hypothesis formation (cited in related work).",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty.",
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Qingyun Wang, Doug Downey, Heng Ji, Tom Hope",
            "year": 2024,
            "method_name": "Scimon",
            "method_description": "Retrieves and recommends literature-based inspirations optimized for novelty to encourage creative hypothesis composition.",
            "input_type": "Literature corpus metadata and content",
            "output_type": "Novel inspiration candidates (papers/concepts) and possibly ranked suggestions",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7727.10",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Yang et al. 2024b",
            "name_full": "Large language models for automated open-domain scientific hypotheses discovery",
            "brief_description": "A cited work that explores automated open-domain hypothesis discovery with LLMs, motivating ResearchBench's decomposition and evaluation of hypothesis generation.",
            "citation_title": "Large language models for automated open-domain scientific hypotheses discovery.",
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria",
            "year": 2024,
            "method_name": "Automated open-domain hypothesis discovery (Yang et al., 2024b)",
            "method_description": "Uses LLMs to discover hypotheses in an open-domain setting, supporting the notion that LLMs can propose valid scientific hypotheses from literature/background inputs.",
            "input_type": null,
            "output_type": "Generated hypotheses",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7727.11",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Language Models as Inductive Reasoners",
            "name_full": "Language models as inductive reasoners (LLMs for inductive/analogical reasoning)",
            "brief_description": "A line of work framing LLMs as inductive reasoners to generalize from instances and propose hypotheses or explanations; cited as related work informing ResearchBench's decomposition.",
            "citation_title": "Language models as inductive reasoners.",
            "mention_or_use": "mention",
            "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
            "authors": "Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei",
            "year": 2024,
            "method_name": "Inductive reasoning with LLMs",
            "method_description": "Treats LLMs as models that can induce general hypotheses from observed facts or text evidence, enabling hypothesis proposal and reasoning about unseen associations.",
            "input_type": null,
            "output_type": "Hypotheses/explanations",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7727.12",
            "source_info": {
                "paper_title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "rating": 2,
            "sanitized_title": "moosechem_large_language_models_for_rediscovering_unseen_chemistry_scientific_hypotheses"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery.",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty.",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "Ideabench: Benchmarking large language models for research idea generation.",
            "rating": 2,
            "sanitized_title": "ideabench_benchmarking_large_language_models_for_research_idea_generation"
        },
        {
            "paper_title": "Discoverybench: Towards data-driven discovery with large language models.",
            "rating": 2,
            "sanitized_title": "discoverybench_towards_datadriven_discovery_with_large_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery.",
            "rating": 1,
            "sanitized_title": "scienceagentbench_toward_rigorous_assessment_of_language_agents_for_datadriven_scientific_discovery"
        },
        {
            "paper_title": "Language models as inductive reasoners.",
            "rating": 1,
            "sanitized_title": "language_models_as_inductive_reasoners"
        }
    ],
    "cost": 0.019969999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition
1 Jul 2025</p>
<p>Yujie Liu liuyujie.cs@gmail.com 
Shanghai Artificial Intelligence Laboratory</p>
<p>Zonglin Yang zonglin.yang@ntu.edu.sg 
Shanghai Artificial Intelligence Laboratory</p>
<p>Nanyang Technological University</p>
<p>Tong Xie 
University of New
South Wales</p>
<p>Jinjie Ni 
National University of Singapore</p>
<p>Ben Gao 
Shanghai Artificial Intelligence Laboratory</p>
<p>Wuhan University</p>
<p>Yuqiang Li 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shixiang Tang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Wanli Ouyang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Erik Cambria cambria@ntu.edu.sg 
Nanyang Technological University</p>
<p>Dongzhan Zhou zhoudongzhan@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Cell Chem ETS MS Phys EGS EVS BL BS Law Math AT Overall</p>
<p>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition
1 Jul 20254E78EA743CF79F4FD60A5333439910A4arXiv:2503.21248v2[cs.CL]Paper Number 152 113 114 116 132 117 116 115 115 97 113 86 1386
Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability in discovering high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark.To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a sufficient set of sub-tasks of scientific discovery, which are inspiration retrieval, hypothesis composition, and hypothesis ranking.We develop an automated framework that extracts critical components-research questions, background surveys, inspirations, and hypotheses-from scientific papers across 12 disciplines, with expert validation confirming its accuracy.To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data.Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations.This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown their potential to assist scientist's research as a copilot (Luo et al., 2025).One of the most challenging copilot tasks is to help scientists discover new valid research hypotheses, where a typical setting is to only provide a research question and a small background survey as input.Understanding how LLMs perform on this task is crucial for selecting the appropriate models and evaluating how different training strategies influence their effectiveness in scientific discovery.</p>
<p>However, although there are efforts benchmarking LLM's performance to the general task, such as Chatbot Arena (Chiang et al., 2024) and MixEval (Ni et al., 2024), it still lacks understanding of the scientific discovery ability of each LLM.</p>
<p>One of the main reasons for this vacancy is the lack of understanding of the scientific discovery process, i.e., how each research hypothesis is formulated.Recently, Yang et al. (2024c) decomposed this research hypothesis formulation process into a sufficient set of sub-tasks, which are (1) retrieving inspirations based on the research question, (2) properly mixing the research background information with the retrieved inspirations to compose research hypotheses, and (3) ranking the composed hypotheses to provide one with the highest confidence.This decomposition is viable because of a fundamental assumption that a majority of hypotheses can originate from a research background and several inspirations.This fundamental assumption is supported by cognitive science findings that creative ideas often result from the cohesive association of two (or more) seemingly unrelated pieces of knowledge (Koestler, 1964;Benedek et al., 2012;Lee &amp; Chung, 2024).The cognitive science findings are disciplineindependent and widely applicable.For example, the proposal of backpropagation is a research hypothesis.In this case, the research background is about multi-layer logistic regression, and the inspiration is the chain rule in calculus.</p>
<p>This research aims at filling the research gap, by providing a benchmark specifically designed to evaluate LLM's performance in terms of the three decomposed tasks of scientific discovery.This benchmark covers 12 disciplines, selecting papers published on top venues such as Nature, Science, or journals of a similar level.The statistics of the benchmark is shown in Table 1.</p>
<p>To construct the benchmark, we download 1386 papers and develop an automated LLM-based agentic framework to analyze each paper into research question, background survey, inspirations, and main hypothesis.We invited five experts in Physics, Chemistry, Astronomy, and Material Science disciplines to check whether the decomposition is accurate.Among the randomly sampled 62 papers checked by the experts, the decomposition accuracy was 91.9% considering only major issues and 82.3% when including both major and minor issues.It shows that the automated framework can accurately extract these components from a paper.</p>
<p>To prevent the data contamination problem, we only select those papers published in 2024.The advantage of our LLM agentic framework for the extraction is that as the LLM's pretraining data cutoff date moves forward, the framework can automatically extract more recent papers to avoid overlapping.</p>
<p>Based on the benchmark, we systematically compare popular LLMs across the three decomposed tasks.We find that current LLMs perform well in retrieving inspirations across disciplines, despite the inclusion of carefully crafted challenging negative inspiration examples.For example, when we ask GPT-4o to select the top 4% of inspiration candidates, the probability that a ground truth inspiration will be included in the top 4% is 45.7%.It is surprising because the inspiration retrieval task is actually an out-of-distribution (OOD) task since inspiration is supposed to be not known to be related to the research question but in fact can assist it.Otherwise, the resulting hypothesis won't be novel.In addition, we find that LLMs also have a good performance on the hypothesis composition and hypothesis ranking task.This suggests that LLMs could be leveraged as research hypothesis mines, where higher-performing LLMs on the three fundamental tasks of scientific discovery act as richer mines, and more inference compute corresponds to more miners.</p>
<p>Overall, the contributions of this paper are:</p>
<p>• We introduce the first large-scale benchmark for evaluating LLMs' capabilities in scientific discovery with a sufficient set of sub-tasks: inspiration retrieval, hypothesis composition, and hypothesis ranking.</p>
<p>• We develop an automated agentic framework to extract essential components-research questions, background surveys, inspirations, and hypotheses-from scientific papers, enabling the scalable and contamination-resistant construction of benchmarks.</p>
<p>• We conduct a comprehensive analysis of LLM performance using our benchmark, presenting the first large-scale study on out-of-distribution (OOD) inspiration retrieval.Our findings demonstrate that LLMs can effectively retrieve inspirations beyond established associations, positioning LLMs as "research hypothesis mines" capable of generating novel scientific insights at scale with minimal human involvement.</p>
<p>2 Related Work</p>
<p>LLMs for Scientific Discovery</p>
<p>CoLM (Yang et al., 2024a) investigates generative inductive reasoning, which is to propose (commonsense) hypothesis from observations.SciMON (Wang et al., 2024) introduces literature-based discovery, showing how retrieved concepts aid hypothesis composition.MOOSE (Yang et al., 2024b) and MOOSE-Chem (Yang et al., 2024c) find that most hypotheses in social science and chemistry can be seen as emerging from a research background and several inspirations.In addition, many researchers have the belief that "An idea is nothing more nor less than a new combination of old elements" (Young, 1975;Kumar et al., 2024).Yang et al. (2024c) further decompose the hypothesis formulation process into a sufficient set of sub-tasks: inspiration retrieval, hypothesis composition, and hypothesis ranking.</p>
<p>Benchmarking LLMs</p>
<p>Most existing benchmarks assess the general intelligence of LLMs (Chiang et al., 2024;Ni et al., 2024).IdeaBench (Guo et al., 2024) is designed for biomedical idea generation and does not evaluate LLMs based on the full set of sub-tasks in scientific discovery.Their focus is limited to generating hypotheses from background knowledge rather than retrieving and integrating inspirations.Additionally, their reference extraction relies on rule-based methods, which are less accurate than the LLM-based agentic framework.Moreover, their benchmark is restricted to the biomedical domain, whereas ours spans 12 disciplines.DiscoveryBench (Majumder et al., 2024) and ScienceAgentBench (Chen et al., 2024) identify and pick specific discovery-relevant tasks (e.g., write a specific code) from 20 papers and 44 papers correspondingly.They do not analyze the fundamental decomposition of the scientific discovery task itself.</p>
<p>3 Benchmark Construction They propose this assumption based on extensive discussions with domain experts, and the cognitive science findings that creative ideas often result from the cohesive association of two (or more) seemingly unrelated pieces of knowledge (Koestler, 1964;Benedek et al., 2012;Lee &amp; Chung, 2024).Denoting background knowledge as b, inspiration knowledge as i, and hypothesis as h, this assumption can be represented in Equation 1:
h = f (b, i 1 , . . . , i k )(1)
Based on this assumption, they decompose the research hypothesis formulation process into a sufficient set of sub-tasks, which are (1) retrieving inspirations based on the research background, (2) properly mixing the research background information with the retrieved inspirations to compose research hypotheses, and (3) ranking the composed hypotheses to provide one with the highest confidence.This process can be represented in Equation 2 and Equation 3.Here I represents the full literature corpus to retrieve inspiration.
P (h | b) ≈ k j=1 P (i j | b, h j−1 , I) • P (h j | b, i j , h j−1 )(2)H = {h 1 , h 2 , . . . , hn | R(h i ) &gt; R(h i+1 ) for all i}(3)
The cognitive science findings are not limited to any single discipline.For example, Yang et al. (2024b) shows that this assumption can also be leveraged in social science disciplines to generate  high-quality research hypothesis.After extensive discussions with researchers in other disciplines such as Physics, Biology, Earth Science, Astronomy, and Math, we find that this assumption is largely and widely true across disciplines.</p>
<p>Based on this observation, we construct this benchmark collecting papers across 12 disciplines in top-ranked venues, develop an agentic framework to automatically extract each paper's research background, inspirations, and hypothesis ( § 3.2), discuss how negative inspiration papers are selected to evaluate LLMs' performance on inspiration retrieval ( § 3.3), and present expert evaluation on the extracted information to illustrate the quality of the benchmark ( § 3.4).</p>
<p>Different from directly assigning a score to each hypothesis and ranking the hypotheses based on their scores (Equation 3), this benchmark adopts a pairwise evaluation for ranking (Equation 4), since pairwise evaluation is widely reported as more robust and reliable (Si et al., 2024).R(h i , h i+1 ) = h i represents h i is selected as a better hypothesis.
H = {h 1 , h 2 , . . . , hn | R(h i , h i+1 ) = h i for all i} (4)</p>
<p>LLM-Based Agentic Framework</p>
<p>We develop a LLM-based agentic framework to automatedly extract the research question, background survey, inspirations, and hypothesis.</p>
<p>The extraction of research question, background survey, and hypothesis is relatively straightforward for LLMs.Specifically, we carefully design prompts and adopt iterative self-refine (Madaan et al., 2023) to extract them.The background survey is summarized based on the information in the introduction section and the related work section.</p>
<p>The extraction of inspirations is not so straightforward compared to the other components.Figure 1 shows the inspiration extraction framework.We have simplified its design as much as possible, retaining only the essential components to ensure efficiency and accuracy.</p>
<p>The inspirations are mostly described in the introduction section, usually starting with "Motivated by", and can be also described in the related work and methodology sections.As shown in Figure 1, given the full passage of a research paper, the "inspiration decomposition" module first iteratively extracts several potential inspirations.Here each potential inspiration is represented by the title and abstract of a referenced research paper.Therefore after the "inspiration decomposition" module suggests the title of a referred paper as inspiration, we use Semantic Scholar and Crossref to find the abstract of the referred paper to compose an inspiration.Then the "necessary checker" module examines whether each extracted inspiration is needed to formulate the hypothesis and not redundant, and the "sufficient checker" is to check whether all necessary inspirations have been extracted to be enough to be possible to formulate the hypothesis.Here by "enough", we mean the information in the research question, background survey, and the inspirations can cover the information scope of the hypothesis.</p>
<p>To prevent data contamination, we apply the agentic framework only to papers published in 2024 or later, thereby minimizing overlap with the pretraining data of LLMs.The cutoff dates for each model's pretraining data are summarized in Table 8.</p>
<p>Negative Inspiration Selection</p>
<p>Although ground-truth inspirations can be extracted, we need negative inspirations to calculate the performance of LLMs on inspiration retrieval.Here our goal is to provide an in-depth analysis on the inspiration retrieval ability by carefully composing a negative inspiration paper set for each paper in the benchmark.</p>
<p>Specifically, for each paper in the benchmark, we collect three levels of negative inspiration papers, based on their distance to the benchmark paper.The first-level is papers that are cited and referred to by the benchmark paper, or papers that have high semantic similar titles to the benchmark paper.</p>
<p>For each benchmark paper, we collect 100 citation-adjacent papers with Crossref API, and 50 semantic adjacent papers with Semantic Scholar API.The second-level are papers that are in the same discipline with the benchmark paper, and the third-level are papers that belong to completely different disciplines (randomly selected).We randomly collect 2000 papers for each discipline by Web of Science, which can be used for both the second-level and the third-level.During experiment, for each benchmark paper, we randomly select 25 negative inspiration papers from each of the distance level to compose the negative inspiration set.</p>
<p>The purpose of the three-level design is two-fold.Firstly, real inspiration can come from each of the levels.By the splitting, LLM's preference in terms of distance can be analyzed.Secondly, the incorporation of closely related papers makes the negative inspiration papers non-trivial: we find that LLMs tend to select papers that are close to the benchmark paper.If the negative papers are only from irrelevant disciplines, then the retrieval success rate will be very high and less meaningful.</p>
<p>Expert Evaluation</p>
<p>We invited five PhD students from diverse disciplines-Physics (1), Chemistry (2), Materials Science (1), and Astronomy (1)-to evaluate the accuracy of our decomposition framework.Specifically, we randomly sampled 62 papers from the benchmark dataset, each accompanied by its extracted research question, background survey, inspirations, and hypothesis, and presented them in the form of a questionnaire.An example of the questionnaire is provided in Appendix A.2.</p>
<p>For each paper, the experts first read the full text of the original research paper and then assessed the accuracy of the extracted components.Each inspiration was evaluated for its necessity, while the entire set of inspirations was assessed for its sufficiency in supporting the research hypothesis.</p>
<p>Overall, the evaluation identified five cases with issues in inspiration identification (three) or hypothesis extraction (two), and six minor issues in research question extraction.The decomposition accuracy was 91.9% considering only major issues and 82.3% when including all issues.</p>
<p>4 Experiments</p>
<p>Inspiration Retrieval</p>
<p>For each benchmark paper, with the extracted research question, groundtruth inspirations, and the negative inspirations, we can calculate the accuracy of an LLM to retrieve the groundtruth inspiration with the research question.</p>
<p>During the experiment, for each benchmark paper, we prepare an inspiration candidate set consisting of 75 papers, including 2-3 groundtruth inspiration papers and about 25 negative inspiration papers from each distance level.Each paper is represented by its title and abstract.The retrieval is performed in several rounds, where in each round, the inspiration candidate set is randomly split into several groups, where each group contains 15 papers.Then LLM is instructed to select the top 3 of each group that it thinks can best serve as inspirations for the background question.In a new round, the selected papers are combined into a new inspiration candidate set and split into groups again.</p>
<p>Following this iterative selection process, only 20% (15 out of 75) of the papers are retained after the first round, and only 4% (3 out of 75) remain after the second round.Thus, the LLM is tasked with selecting 3 papers from an initial pool of 75.We use Hit Ratio as the evaluation metric, it is calculated as the number of groundtruth inspiration papers selected by the LLM divided by the total number of inspiration candidates.</p>
<p>(a) The accuracy (%) of LLMs in retrieving the groundtruth inspiration while only 20% of inspiration candidates are selected.Furthermore, we analyze the Hit Ratio of negative inspirations across the three distance levels.The results are presented in Table 3.It indicates that regardless of the percentage of the papers selected, the closer an inspiration is to the benchmark paper, the higher the probability it is selected as an inspiration.We attribute it to two reasons.Firstly, statistically closer papers objectively have a better chance of being an inspiration; Secondly, if certain papers often appear together in the training data, the LLM may see them as more possibly contributing to each other.</p>
<p>Hypothesis Composition</p>
<p>With the retrieved inspirations, the next step is to associate them with the research background to compose research hypothesis.Figure 2 shows the framework we use for the hypothesis generation process.This framework strictly follows Equation 2, and is designed to be as simple as possible, avoiding unnecessary components.We use the evolutionary unit (Yang et al., 2024c) to associate the research background (b) and inspiration (i), shown in the bottom-left rectangle in Figure 2. Specifically, "mutate" means creating different ways to combine b and i together and "recombine" tries to keep the merits of different combination ways to compose a final hypothesis.The prompts for mutate, refine, and recombine are provided in Appendix A.5.In this step, we only measure the LLM's ability on hypothesis composing.For the LLM-generated hypotheses to be comparable with the groundtruth hypothesis for evaluation, here I represents the groundtruth inspiration papers (usually 2 to 3 papers), while "inspiration retriever" module each time retrieve only one inspiration, and will not retrieve the same inspiration again.</p>
<p>The detailed scoring criteria is shown in Appendix A.3, where we use a 6-point Likert scale (from 0 to 5) to measure whether the generated hypothesis has covered the key points in the groundtruth  hypothesis.To compute the generation accuracy, we normalize the average score by dividing it by the maximum possible score (5).The final results are summarized in Table 4. Table 4 shows that (a) all LLMs preserve a certain kind of ability to associate the research background and inspirations to compose hypothesis; (b) the hypothesis composition task remains challenging, as none of the models achieve consistently high performance.</p>
<p>Hypothesis Ranking</p>
<p>In this section, we evaluate the ability of LLMs to rank hypotheses pairwisely.Specifically, based on the hypothesis generation method in § 4.2, we use the top-ranked negative inspirations and background question to construct a set of negative hypotheses.From this set, we randomly sample 5 negative hypotheses.Additionally, with subsets of groundtruth inspirations and the research question, we use the hypothesis composition framework to generate another set of negative hypotheses.In this set, we randomly sample 10 as negative hypotheses for ranking.As a result, for each benchmark paper, we compose a set of 16 hypotheses, including one groundtruth one and 15 negative ones for ranking.To evaluate ranking performance, we use the groundtruth hypothesis to pairwisely compare with each of the 15 negative ones.The prompt for pairwise ranking is provided in the Appendix A.4.</p>
<p>Accuracy is used as the evaluation metric, which is calculated as the proportion of correct pairwise rankings out of 15 comparisons.During the pairwise evaluation, we find that many LLMs have strong position bias: they largely prefer the first hypothesis than the second.To avoid this bias, for each hypothesis pair, we compare them twice with reverse positions, and the results are averaged.Table 6: Analysis of position bias in hypothesis ranking task.Each hypothesis pair is compared twice, with three possible outcomes: both wrongly ranked (✗ ✗); one right one wrong (✓ ✗); both rightly ranked (✓ ✓).Numbers are averaged percentages (%).
Model ✗ ✗ ✓ ✗ ✓ ✓ GPT-
Table 5 presents the ranking accuracy of each LLM.This ranking results indicate a different scaling law with the scaling law we find in the inspiration retrieval task: more parameters and better pretraining strategies can significantly improve over the hypothesis ranking task, while might lead to less improvements in the inspiration retrieval task.</p>
<p>Table 6 analyzes the position bias problem in the hypothesis ranking task.Specifically, each hypothesis pair is compared twice, with three possible outcomes, and the table shows the averaged percentage of each outcome.It shows that many LLMs are hugely influenced by position bias (e.g., Llama-3.1-8B has 91.67% of the time reaching self-contradictory results), and some are less influenced (e.g., Claude 3.5 Sonnet only 19.17%).The large proportion of self-contradictory results might be one of the main reasons that many LLMs in Table 5 reach a ranking accuracy around 50%.</p>
<p>Analysis</p>
<p>LLMs as Research Hypothesis Mines</p>
<p>The results show that (1) LLMs can already capture many unknown association of knowledge so to retrieve inspirations relatively accurately; (2) Given groundtruth inspirations, many LLMs can compose a hypothesis that capture at least a subset of main innovations in the groundtruth one; (3) with improved scale and better training strategies, LLMs' performance on hypothesis ranking can grow very quickly, and we have not seen the limit.</p>
<p>Also considering that the three tasks of inspiration retrieval, hypothesis composing, and hypothesis ranking are a sufficient set of sub-tasks of scientific discovery, it might indicate that given only a research background, LLMs can already discover hypothesis autonomously: it can screen lots of inspiration candidates to choose the good ones autonomously, associating the research background with the good inspirations autonomously, and autonomously rank those composed hypotheses to provide the scientists with the best ones.</p>
<p>In short, the only input scientists need to provide such a copilot is the research background and enough papers to serve as inspiration.In this view, LLMs can be regarded as research hypothesis mines: models with stronger performance on the three fundamental tasks of scientific discovery represent richer mines, while more inference compute corresponds to deploying more miners.</p>
<p>The Bottleneck Towards Automated Discovery</p>
<p>Across the three sub-tasks, the inspiration retrieval task appears to be the most challenging.Although performance improves rapidly even with relatively small models (e.g., 8B parameters), it quickly plateaus.Scaling up model size or enhancing pretraining strategies yields only marginal gains in retrieval performance.</p>
<p>We attribute this to the nature of the task: inspiration retrieval fundamentally requires deep domain understanding, which is primarily acquired during the pretraining phase as the model ingests millions of papers.In other words, success in this task may rely more on the "intuition" developed through large-scale pretraining rather than the enhanced reasoning abilities typically refined during posttraining.Understanding the fundamental mechanisms behind how LLMs retrieve inspirations may help address a key bottleneck in advancing toward fully automated scientific discovery.</p>
<p>Conclusion</p>
<p>We introduced the first large-scale benchmark for evaluating LLMs in scientific discovery in terms of a sufficient set of sub-tasks, covering inspiration retrieval, hypothesis composition, and hypothesis ranking.Our benchmark, ResearchBench, spans 12 scientific disciplines and utilizes an automated, LLM-based agentic framework, significantly contributing to scalable and contamination-resistant dataset construction.</p>
<p>Our evaluation shows that LLMs achieve promising results in inspiration retrieval, effectively surfacing novel, out-of-distribution knowledge associations.They also demonstrate moderate capabilities in hypothesis composition and ranking tasks; however, performance in these two tasks indicates considerable room for improvement.Notably, we identify inspiration retrieval as a key bottleneck, where accuracy quickly plateaus with increasing model scale, underscoring the need for deeper domain-specific understanding primarily acquired during pretraining rather than fine-tuning.</p>
<p>These findings point toward a promising pathway, positioning LLMs as potential "research hypothesis mines".By systematically addressing the bottlenecks identified, LLMs hold great promise for becoming powerful tools capable of autonomously discovering high-quality scientific hypotheses, ultimately facilitating a paradigm shift towards fully automated scientific exploration.</p>
<p>A Appendix</p>
<p>A.1 Prompt for Retrieving Inspirations</p>
<p>You are helping with the scientific hypotheses generation process.Given a research question, the background and some of the existing methods for this research question, and several top-tier publications (including their title and abstract), try to identify which publication can potentially serve as an inspiration for the background research question so that combining the research question and the inspiration in some way, a novel, valid, and significant research hypothesis can be formed.The inspiration does not need to be similar to the research question.In fact, probably only those inspirations that are distinct with the background research question, combined with the background research question, can lead to a impactful research hypothesis.The reason is that if the inspiration and the background research question are semantically similar enough, they are probably the same, and the inspiration might not provide any additional information to the system, which might lead to a result very similar to a situation that no inspiratrions are found.An example is the backpropagation of neural networks.In backpropagation, the research question is how to use data to automatically improve the parameters of a multi-layer logistic regression, the inspiration is the chain rule in mathematics, and the research hypothesis is the backpropagation itself.In their paper, the authors have conducted experiments to verify their hypothesis.Now try to select inspirations based on background research question.The background research question is: ", "The introduction of the previous methods is:", "The potential inspiration candidates are: ", "Now you have seen the background research question, and many potential inspiration candidates.Please try to identify which three literature candidates are the most possible to serve as the inspiration to the background research question?Please name the title of the literature candidate, and also try to give your reasons.</p>
<p>A.2 Guideline Format for Expert Checking</p>
<p>Titile:</p>
<p>Background question decomposed by automated framework:</p>
<p>Whether the background question correct?</p>
<p><Reply fill in here.Required a detailed analysis> ground-truth hypothesis decomposed by automated framework:</p>
<p>Whether the ground-truth hypothesis accurately reflect the main proposal of the paper?</p>
<p><Reply fill in here.Required a detailed analysis> Inspiration paper 1 title: Relation between the inspiration 1 paper and the decomposed paper:</p>
<p>Whether the collected inspiration paper 1 compose of a set of necessary conditions to reach to the ground-truth hypothesis?</p>
<p><Reply fill in here.Required a detailed analysis> Inspiration paper 2 title: Relation between the inspiration 2 paper and the decomposed paper:</p>
<p>Whether the collected inspiration paper 2 compose of a set of necessary conditions to reach to the ground-truth hypothesis?</p>
<p><Reply fill in here.Required a detailed analysis> Inspiration paper 3 title: Relation between the inspiration 3 paper and the decomposed paper:</p>
<p>Whether the collected inspiration paper 3 compose of a set of necessary conditions to reach to the ground-truth hypothesis?</p>
<p><Reply fill in here.Required a detailed analysis> Whether the collected inspirations paper compose of a set of sufficient conditions to reach to the coarse-grained hypothesis?</p>
<p><Reply fill in here.Required a detailed analysis></p>
<p>A.3 Prompt for Evaluating Generated Hypothesis</p>
<p>You are helping to evaluate the quality of a proposed research hypothesis by a phd student.The groundtruth hypothesis will also be provided to compare.Here we mainly focus on whether the proposed hypothesis has covered the key points of the ground-truth hypothesis.You will also be given a summary of the key points in the ground-truth hypothesis for reference.The evaluation criteria is called 'Matched score', which is in a 6-point Likert scale (from 5 to 0).Particularly, 5 points mean that the proposed hypothesis (1) covers three key points (or covers all the key points) in the ground-truth hypothesis, where every key point is leveraged nearly identically as in the ground-truth hypothesis, and ( 2) does not contain any extra key point(s) that is redundant, unnecessary, unhelpful, or harmful; 4 points mean that the proposed hypothesis (1) covers three key points (or covers all the key points) in the ground-truth hypothesis, where every key point is leveraged nearly identically as in the ground-truth hypothesis, and ( 2) but also contain extra key point(s) that is redundant, unnecessary, unhelpful, or harmful; 3 points mean that the proposed hypothesis (1) covers two key points in the ground-truth hypothesis, where every key point is leveraged nearly identically as in the ground-truth hypothesis, (2) but does not cover all key points in the ground-truth hypothesis, and (3) might or might not contain extra key points; 2 points mean that the proposed hypothesis (1) covers one key point in the ground-truth hypothesis, and leverage it nearly identically as in the ground-truth hypothesis, (2) but does not cover all key points in the ground-truth hypothesis, and (3) might or might not contain extra key points; 1 point means that the proposed hypothesis (1) covers at least one key point in the ground-truth hypothesis, but all the covered key point(s) are used differently as in the ground-truth hypothesis, and (2) might or might not contain extra key points; 0 point means that the proposed hypothesis does not cover any key point in the ground-truth hypothesis at all.Usually total the number of key points a ground-truth hypothesis contain is less than or equal to three.Please note that the total number of key points in the ground-truth hypothesis might be less than three, so that multiple points can be given.E.g., there's only one key point in the ground-truth hypothesis, and the proposed hypothesis covers the one key point nearly identically, it's possible to give 2 points, 4 points, and 5 points.In this case, we should choose score from 4 points and 5 points, depending on the existence and quality of extra key points.'Leveraging a key point nearly identically as in the ground-truth hypothesis means that in the proposed hypothesis, the same (or very related) concept (key point) is used in a very similar way with a very similar goal compared to the ground-truth hypothesis.When judging whether an extra key point has apparent flaws, you should use your own knowledge and understanding of that discipline to judge, rather than only relying on the count number of pieces of extra key point to judge.Importantly, we should focus on whether the fundamental key points match, rather than being influenced by how complex, sophisticated, or advanced the proposed methods appear.A hypothesis that introduces high-level techniques or intricate methodologies does not necessarily mean it is a disadvantage with the ground-truth hypothesis.The core concern is whether the essential key points are correctly captured and utilized.Please evaluate the proposed hypothesis based on the ground-truth hypothesis.The proposed hypothesis is: ", "The ground-truth hypothesis is: ", "The key points in the ground-truth hypothesis are: "</p>
<p>A.4 Prompt for Pairwise Ranking</p>
<p>You are assisting scientists with their research.Given a research question and two research hypothesis candidates proposed by large language models, your task is to predict which hypothesis is a better research hypothesis.By 'better', we mean the hypothesis is more valid and effective for the research question.Please note the following:</p>
<p>(1) Neither hypothesis has been tested experimentally.However, some large language model generated hypothesis might contain expected performance of the hypothesis.Well, just do not believe any of the descriptions of the expected performance or the effect of the hypothesis.Instead, only focus on the technical contents and predict which hypothesis you think will be more effective for the research question if tested in real experiments.</p>
<p>(2) You should remember that, here, we only focus on whether the general direction or major components of the hypothesis are more effective.Providing additional details or making the content more comprehensive is neither an advantage nor a disadvantage.More detailed and multifaceted strategies, additional complexity, and potential challenges are neither advantages nor disadvantages.What truly matters is the fundamental, intrinsic core idea.The research question is: &lt;the background coverage and validation to more domains would further strengthen its generality.Lastly, although we use 2024 publications to reduce pretraining overlap, we cannot fully guarantee the absence of indirect data leakage via related works.We leave these challenges to future work, including more refined validation pipelines and expansion into underrepresented scientific fields.</p>
<p>A.8 Experiment Compute Resources</p>
<p>The construction of our benchmark and subsequent evaluation of LLMs were conducted using publicly accessible APIs, including Gemini 2.0 Flash and GPT-4o-mini.All experiments were run using pay-per-use endpoints without private fine-tuning or proprietary infrastructure.The end-to-end cost of benchmark construction and large-scale LLM evaluation across the three sub-tasks totaled approximately $4,000 USD.This highlights the feasibility of replicating or extending our study within a reasonable compute and budget envelope, making our framework accessible to a broad range of academic and industrial researchers.</p>
<p>A.9 Experiment Statistical Significance While we do not conduct formal statistical significance testing for individual comparisons, our results are aggregated over a large and diverse benchmark comprising 1,386 scientific papers across 12 disciplines.This large-scale evaluation mitigates the impact of outlier cases and provides a stable estimate of model performance.The consistency of trends observed across tasks and domains suggests that the reported differences are meaningful and generalizable.Future work may incorporate bootstrap resampling or hypothesis testing to provide stronger statistical guarantees for fine-grained comparisons.</p>
<p>A.10 Broader Impact</p>
<p>This work introduces a scalable and transparent benchmark for evaluating LLMs in scientific hypothesis generation, supporting more systematic and interpretable assessment of their research capabilities.By covering 12 disciplines and relying on publicly available data, our framework promotes reproducibility and broad accessibility.</p>
<p>The agentic extraction pipeline also enables future extensions to newer literature, making the benchmark adaptable over time.While LLMs should not replace expert judgment, our structured decomposition encourages human-in-the-loop use, where models serve as tools for amplifying creativity rather than replacing it.</p>
<p>We hope this work fosters responsible development of LLM-based tools that support scientific innovation across domains.</p>
<p>Figure 1 :
1
Figure 1: Overview of the inspiration retrieval framework.</p>
<p>Table 1 :
1
Disciplines and paper number distribution.Chem=Chemistry, ETS=Earth Science, MS=Material Science, Phys=Physics, EGS=Energy Science, EVS=Environmental Science, BL=Biology, BS=Business, AT=Astronomy.</p>
<p>Table 2 :
2
Performance of LLMs in hypothesis retrieve task.Gemini 2.0 FT=Gemini 2.0 Flash Thinking; Chem=Chemistry, ETS=Earth Science, MS=Material Science, Phys=Physics, EGS=Energy Science, EVS=Environmental Science, BL=Biology, BS=Business, A=Astronomy.
ModelCell Chem ETSMSPhys EGS EVSBLBSLaw MathAOverallLlama-3.2-1B34.65 34.80 32.57 30.26 30.25 34.75 35.43 33.21 41.09 29.74 36.22 30.1033.68Llama-3.1-8B74.08 78.00 79.69 74.54 76.75 84.56 75.20 75.81 80.00 65.95 75.59 68.3775.92Qwen Turbo74.37 77.20 80.08 72.69 75.80 88.03 78.35 74.01 82.18 67.24 74.80 66.8476.17GPT-4o Mini76.06 83.20 82.76 77.49 81.53 89.96 79.92 70.76 84.00 70.69 74.80 71.9478.74Gemini 2.0 FT74.65 79.60 80.84 73.43 78.34 90.35 76.77 75.09 85.09 80.17 76.38 77.5578.89Gemini 2.0 Flash75.77 76.40 85.82 75.28 79.94 91.89 75.98 75.09 86.91 78.02 76.77 71.9479.24Qwen Plus79.15 82.00 82.76 75.28 80.57 91.12 81.10 76.53 84.73 75.00 79.53 73.9880.27DeepSeek-V380.00 83.60 85.44 76.01 79.94 91.51 79.53 76.90 86.91 75.86 77.56 73.9880.74Claude 3.5 Haiku80.56 85.20 85.06 77.86 79.94 90.35 83.07 75.81 87.27 70.69 77.56 75.5180.89Llama-3.1-70B78.31 84.00 84.67 80.07 80.25 89.58 81.10 79.42 86.91 75.43 77.95 75.5181.18Claude 3.5 Sonnet 78.31 78.40 85.06 76.75 81.53 91.51 85.04 77.62 88.00 77.59 79.53 77.5581.43GPT-4o80.00 87.20 89.27 80.81 84.39 93.05 81.89 77.98 87.64 79.74 83.07 75.0083.43(b) The accuracy (%) of LLMs in retrieving the groundtruth inspiration while only 4% of inspiration candidatesare selected.ModelCell Chem ETSMSPhys EGS EVSBLBSLaw MathAOverallLlama-3.2-1B10.70 11.60 12.26 9.59 11.15 8.49 14.57 13.00 17.09 12.50 11.42 10.7111.91Llama-3.1-8B32.39 38.00 40.61 31.37 32.80 59.85 36.61 28.52 55.64 28.88 36.22 34.6937.87Gemini 2.0 FT31.27 41.20 40.61 30.63 32.48 71.04 39.37 33.57 59.64 37.07 34.65 33.1640.18GPT-4o Mini30.42 43.60 41.00 34.69 33.44 66.80 40.16 28.88 64.73 32.76 37.80 35.7140.59Qwen Turbo35.49 42.40 42.15 33.95 35.03 66.80 43.31 33.21 61.45 29.74 36.61 34.6941.21Gemini 2.0 Flash31.55 38.80 44.06 34.32 34.39 74.52 37.40 32.49 64.00 37.50 37.80 32.6541.46Claude 3.5 Sonnet 36.34 41.20 42.91 30.63 36.31 67.57 40.55 34.30 63.64 34.91 37.40 33.6741.62Qwen Plus36.06 47.20 45.21 33.58 34.39 72.97 43.31 35.38 64.36 34.91 39.37 36.2243.43Claude 3.5 Haiku41.13 48.40 45.98 34.69 33.44 69.88 44.09 34.30 64.00 37.93 38.19 41.3344.28DeepSeek-V338.87 46.00 44.06 36.90 36.62 75.29 41.73 40.07 65.45 36.64 38.58 37.7644.78Llama-3.1-70B41.41 44.00 47.51 36.90 34.39 70.66 45.28 37.18 65.45 39.22 38.19 39.2944.87GPT-4o39.44 46.40 47.13 38.38 35.35 75.29 44.88 38.63 65.82 39.22 40.16 38.7845.65ModelDistance Level 1Distance Level 2Distance Level 3(top 20%) (top 4%) (top 20%) (top 4%) (top 20%) (top 4%)Llama-3.2-1B23.57%6.33%15.52%2.93%14.46%2.85%Qwen Turbo52.72%12.05%9.45%1.11%4.46%0.34%Claude 3.5 Sonnet53.96%10.15%10.16%0.70%2.40%0.13%Llama-3.1-8B53.69%11.17%10.65%0.77%2.94%0.14%Gemini 2.0 Flash Thinking 54.49%10.59%10.34%0.58%2.24%0.11%GPT-4o54.90%10.02%9.84%0.47%2.09%0.09%Llama-3.1-70B55.32%10.04%9.82%0.55%2.16%0.09%DeepSeek-V355.74%10.22%9.80%0.43%1.79%0.07%GPT-4o Mini55.90%10.67%9.54%0.47%2.12%0.09%Claude 3.5 Haiku55.70%10.19%9.51%0.49%2.00%0.07%Gemini 2.0 Flash55.91%10.63%9.63%0.42%2.03%0.09%Qwen Plus56.11%10.57%9.52%0.50%2.16%0.16%</p>
<p>Table 3 :
3
Analysis of negative inspiration retrieval in the inspiration retrieval task.Each value represents the average percentage of negative inspirations retrieved across three distance levels, under two settings where only 20% and 4% of the candidate inspirations are selected, respectively.The Hit Ratio results are presented in Table2.The values in "Overall" column represent averages across 12 disciplines.Overall, LLMs demonstrate surprisingly high retrieval accuracy.Figure2: Overview of the hypothesis composition process.thinkthishighretrieval accuracy stems from LLMs' pretraining process, during which they may have already captured latent knowledge associations that are not yet recognized by scientists.Table2also indicate the scaling law of LLMs for inspiration retrieval: the inspiration retrieval ability grows up very fast before and during 8B parameters, while stuck in a bottleneck at around 70B parameters.No matter how the LLMs are trained with different strategies, they seem to be bottlenecked at the same performance.
InspirationInspirationInspirationRetrieverRetrieverRetrieverHypothesis ComposerHypothesis Composer...Hypothesis Composer...: background : inspiration ( , if)+: hypothesis...: hypothesis mutationmutaterefinerecombine: ground truth inspiration papers ()
In the first selection round, where 20% of the papers are retained, most LLMs successfully identify around 80% of the groundtruth inspiration papers.Even in the final round, where only 3 papers are selected from the initial set of 75, most LLMs maintain an accuracy exceeding 40%, with GPT-4o remaining the best model at 45.65%.These findings show that LLMs can identify papers that were not known as relevant but have the potential to contribute to solving the background question.We</p>
<p>Table 4 :
4ModelCell Chem ETSMSPhys EGS EVSBLBSLaw MathAOverallClaude 3.5 Haiku 40.42 40.87 38.71 46.75 45.00 45.34 48.00 46.15 35.14 37.85 43.59 34.2942.56Llama-3.1-8B44.58 47.83 42.78 46.04 45.05 44.30 46.47 47.37 44.21 47.58 48.21 45.1445.68Gemini 2.0 FT45.67 39.79 48.48 47.22 48.77 49.24 48.57 48.02 41.47 47.03 42.81 40.0046.30Gemini 2.0 Flash 46.25 45.63 48.64 51.63 47.97 51.47 49.41 48.77 47.03 55.91 56.24 49.7150.15Llama-3.1-70B46.67 49.86 50.83 51.53 50.60 50.61 52.10 54.36 49.47 53.94 51.11 49.1450.92GPT-4o Mini46.67 49.42 50.91 52.63 53.82 53.33 54.86 54.36 46.92 56.97 52.48 53.1452.47Qwen Turbo52.92 51.45 49.55 51.06 52.64 50.97 52.57 56.92 53.16 55.76 55.38 53.1452.71GPT-4o55.00 53.04 54.09 53.95 53.82 52.97 53.14 55.38 46.15 53.99 54.53 52.5753.37DeepSeek-V352.78 52.27 53.18 54.25 54.91 53.91 53.71 56.32 50.27 55.15 52.14 53.7153.79Qwen Plus60.00 53.72 57.27 56.63 58.14 56.63 60.57 58.97 51.05 62.19 55.90 56.5757.46
Performance of LLMs in hypothesis composition task.Each number represents the normalized performance of LLMs in composing hypothesis.Gemini 2.0 FT=Gemini 2.0 Flash Thinking; Chem=Chemistry, ETS=Earth Science, MS=Material Science, Phys=Physics, EGS=Energy Science, EVS=Environmental Science, BL=Biology, BS=Business, A=Astronomy.</p>
<p>Table 5 :
5
Performance of LLMs in hypothesis ranking task.Each number represents the accuracy (%) of LLMs in ranking ground-truth hypothesis among negative hypothesis.Chem=Chemistry, ETS=Earth Science, MS=Material Science, Phys=Physics, EGS=Energy Science, EVS=Environmental Science, BL=Biology, BS=Business, A=Astronomy.</p>
<p>ScoreCriteria 5 Points (1) Covers three key points (or all key points) in the ground-truth hypothesis, with each key point leveraged nearly identically to the ground-truth hypothesis.(2) Does not contain any extra key point that is redundant, unnecessary, unhelpful, or harmful.4 Points (1) Covers three key points (or all key points) in the ground-truth hypothesis, with each key point leveraged nearly identically to the ground-truth hypothesis.(2) However, it also contains extra key point(s) that are redundant, unnecessary, unhelpful, or harmful.3 Points (1) Covers two key points in the ground-truth hypothesis, with each key point leveraged nearly identically to the ground-truth hypothesis.(2) Does not cover all key points in the ground-truth hypothesis.(3) May or may not contain extra key points.2 Points (1) Covers one key point in the ground-truth hypothesis and leverages it nearly identically to the ground-truth hypothesis.(2) Does not cover all key points in the ground-truth hypothesis.(3) May or may not contain extra key points.1 Point (1) Covers at least one key point in the ground-truth hypothesis, but all the covered key points are used differently from the ground-truth hypothesis.(2) May or may not contain extra key points.0 Points The proposed hypothesis does not cover any key point in the ground-truth hypothesis.Prompt for mutation: You are helping with the scientific hypotheses generation process.We in general split the period of research hypothesis proposal into three steps.Firstly it's about the research background, including finding a good and specific background research question, and an introduction of the previous methods under the same topic; Secondly its about finding inspirations (mostly from literatures), which combined with the background research question, can lead to a impactful research hypothesis; Finally it's hypothesis generation based on the background research question and found inspirations.Take backpropagation as an example, the research question is how to use data to automatically improve the parameters of a multi-layer logistic regression with data, the inspiration is the chain rule in mathematics, and the research hypothesis is the backpropagation itself.nNow we have identified a good research question, an introduction of previous methods, and a core inspiration in a literature for this research question.The experts know that a proper mixture of these components will definitely lead to a valid, novel, and meaningful research hypothesis.In fact, they already have tried to mix them to compose some research hypotheses (that are supposed to be distinct from each other).Please try to explore a new meaningful way to combine the inspiration with the research background to generate a new research hypothesis that is distinct with all the previous hypotheses in terms of their main method.The new research hypothesis should ideally be novel, valid, ideally significant, and be enough specific in its methodology.Please note that here we are trying to explore a new meaningful way to leverage the inspiration along with the previous methods (inside or outside the introduction) to better answer the background research question, therefore the new research hypothesis should try to leverage or contain the key information or the key reasoning process in the inspiration, trying to better address the background research question.It means the new research hypothesis to be generated should at least not be completely irrelevant to the inspiration or background research question.In addition, by generating distinct hypothesis, please do not achieve it by simply introducing new concept(s) into the previous hypothesis to make the difference, but please focus on the difference on the methodology of integrating or leveraging the inspiration to give a better answer to the research question (in terms of the difference on the methodology, concepts can be introduced or deleted).Prompt for refine: You are helping with the scientific hypotheses generation process.We in general split the period of research hypothesis proposal into four steps.Firstly it's about finding a good and specific background research question, and an introduction of the previous methods under the same topic; Secondly its about finding inspirations (mostly from literatures), which combined with the background research question, can lead to a impactful research hypothesis; Thirdly it's about finding extra knowledge that work along with the inspiration can lead to a more complete hypothesis.Finally it's hypothesis generation based on the background research question, the found inspirations, and the extra knowledge.Now we have identified a good research question, a core inspiration in a literature for this research question, and extra knowledge.With them, we have already generated a preliminary research hypothesis.We have also obtain feedbacks on the hypothesis from domain experts in terms of novalty, validity, significance, and clarity.With these feedbacks, please try your best to refine the hypothesis.Please note that during refinement, do not improve a hypothesis's significance by adding expectation of the performance gain of the method or adding description of its potential impact, but you should work on improving the method itself (e.g., by adding or changing details of the methodology).Prompt for recombine: You are helping with the scientific hypotheses generation process.We in general split the period of research hypothesis proposal into three steps.Firstly it's about the research background, including finding a good and specific background research question, and an introduction of the previous methods under the same topic; Secondly its about finding inspirations (mostly from literatures), which combined with the background research question, can lead to a impactful research hypothesis; Finally it's hypothesis generation based on the background research question and found inspirations.Now we have identified a good research question, an introduction of previous methods, and a core inspiration in a literature for this research question.In addition, several experts have already come out of several different hypotheses on how to leverage the inspiration to generate a novel, valid, and significant research hypothesis for the background research question.Please find the bright parts in these hypotheses, leverage the bright parts from them, modify and combine the good parts of them to generate a better research hypothesis in terms of clarity, novelty, validness, and significance (ideally than any of the given hypotheses).It is not necessary to include methods from every given hypothesis, especially when it is not a good hypothesis.But in general you should try your best to benefit from every given hypothesis.In fact, a researcher has already tried to propose hypothesis based on these information, and we have obtained the feedback to his hypothesis, from another respectful researcher.Please try to leverage the feedback to improve the hypothesis, you can leverage all these provided information as your reference.AA.7 LimitationWhile our benchmark and framework offer a scalable and contamination-resistant means of evaluating LLMs in scientific hypothesis generation, several limitations remain.First, the decomposition accuracy, though high, is not perfect, and minor errors in component extraction may affect downstream evaluations.Second, our expert validation is limited to select disciplines; extending the benchmark's
Associative abilities underlying creativity. Mathias Benedek, Tanja Könen, Aljoscha C Neubauer, Psychology of Aesthetics, Creativity, and the Arts. 632732012</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, 10.48550/arXiv.2410.050802024</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I Jordan, Joseph E Gonzalez, Ion Stoica, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, AustriaJuly 21-27, 2024. OpenReview.net, 2024</p>
<p>Ideabench: Benchmarking large language models for research idea generation. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Eric Huang, Stefan Xie, Aidong Bekiranov, Zhang, 10.48550/arXiv.2411.024292024</p>
<p>The act of creation. Arthur Koestler, 1964HutchinsonLondon</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, 10.48550/arXiv.2409.061852024</p>
<p>An empirical investigation of the impact of chatgpt on creativity. Byung Cheol, Lee , Jaeyeon Chung, Nature Human Behaviour. 2024</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.04306Llm4sr: A survey on large language models for scientific research. 2025arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. December 10 -16, 2023. 2023</p>
<p>Discoverybench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, 10.48550/arXiv.2407.01725arXiv:2406.06565Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, 2024. 2024arXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 202412024ACL 2024</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024a1</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, 10.18653/v1/2024.findings-acl.804Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024b</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, ICLR 20252024c</p>
<p>A technique for producing ideas. James Webb, Young , 1975</p>            </div>
        </div>

    </div>
</body>
</html>
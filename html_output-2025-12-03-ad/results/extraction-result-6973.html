<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6973 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6973</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6973</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-3aaf6a2cbad5850ad81ab5c163599cb3d523436f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3aaf6a2cbad5850ad81ab5c163599cb3d523436f" target="_blank">Self-Refine: Iterative Refinement with Self-Feedback</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach.</p>
                <p><strong>Paper Abstract:</strong> Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6973.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6973.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-3.5 Sentiment Rev.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Sentiment Reversal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative generate-feedback-refine procedure where the same LLM (GPT-3.5 / text-davinci-003) generates an initial output, produces actionable natural-language feedback on that output, and then refines the output using that feedback; repeated up to a small fixed number of iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large language model from OpenAI used as generator, feedback provider, and refiner via few-shot prompts (no additional training).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial output; prompt same model to produce specific, actionable NL feedback about the output; prompt same model to produce a refined output conditioned on the feedback and history; iterate feedback→refine cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment Reversal</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Rewrite review text to reverse its sentiment (e.g., positive→negative).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B; GPT-4 used as proxy evaluator in some analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>8.8% (preference rate / baseline GPT-3.5 direct generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>30.4% (preference rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires high-quality, actionable NL feedback; without specific feedback performance is much lower. Overall method depends on model's ability to produce useful feedback; weaker models may fail to produce feedback or adhere to refinement prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6973.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (ChatGPT Sentiment Rev.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with ChatGPT / gpt-3.5-turbo on Sentiment Reversal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same SELF-REFINE pipeline instantiated with ChatGPT (gpt-3.5-turbo) using few-shot prompts to produce feedback and refinements iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational instruction-following LLM from OpenAI used for generation, feedback, and refinement via few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Alternate FEEDBACK and REFINE steps: the model generates feedback on its own output and then produces a revised output conditioned on that feedback; history is appended to prompts across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment Reversal</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Rewrite review text to reverse its sentiment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B; GPT-4 used as proxy evaluator in some analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>11.4% (preference rate / baseline ChatGPT direct generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>43.2% (preference rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Highly sensitive to feedback quality; without specific actionable feedback the task can fail (paper shows 0% success when no feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6973.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-4 Sentiment Rev.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-4 on Sentiment Reversal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE applied to GPT-4: iterative NL feedback and refinement cycles produced by the same model, improving human/GPT-4 preference rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art instruction-following LLM from OpenAI used as generator, feedback provider, and refiner via few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Few-shot prompted FEEDBACK step to produce actionable, specific critique and a REFINE step that conditions on the critique and full history to produce iterative improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment Reversal</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Rewrite review text to reverse its sentiment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B; GPT-4 used as proxy evaluator in some analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>3.8% (preference rate / baseline GPT-4 direct generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>36.2% (preference rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even strong models can depend on feedback quality; gains rely on model's ability to localize and specify required changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6973.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-3.5 Dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Dialogue Response Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative self-feedback pipeline yields more engaging and relevant dialogue responses by having the model critique and then refine its replies using few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM used as generator, feedback provider, and refiner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model generates an initial dialogue response, then produces feedback such as 'Make response relevant, engaging, safe', then refines the response conditioned on that feedback iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue Response Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce rich conversational responses given dialogue context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B and GPT-4-pref proxy; blind A/B evaluation used for subset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>36.4% (preference rate / baseline GPT-3.5 direct generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>63.6% (preference rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Diminishing returns across iterations; some aspects can improve while others degrade (multi-aspect trade-offs); requires multi-aspect scoring to select balanced outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6973.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (ChatGPT Dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with ChatGPT on Dialogue Response Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE with ChatGPT improves human/GPT-4 preference rates for dialogue responses via iterative self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational LLM used in an identical FEEDBACK→REFINE loop via few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Alternating feedback and refinement steps, retaining history of outputs and feedback to inform further refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue Response Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce rich conversational responses given dialogue context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B and GPT-4-pref proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>40.1% (preference rate / baseline ChatGPT direct generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>59.9% (preference rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-critique may sometimes declare 'everything looks good' especially for reasoning tasks; this can prevent corrections and limit gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6973.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-4 Dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-4 on Dialogue Response Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying SELF-REFINE to GPT-4 drastically increases preference scores for dialogue responses by iteratively generating and incorporating feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advanced instruction-following LLM used as generator, feedback provider and refiner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Few-shot prompted feedback generation followed by refinement conditioned on past outputs and feedback, repeated until stop or max iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue Response Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce rich conversational responses given dialogue context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B and GPT-4-pref proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>25.4% (preference rate / baseline GPT-4 direct generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>74.6% (preference rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large increases observed but method depends on model's ability to generate actionable critique; iterative gains show diminishing returns after early iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6973.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-3.5 Code Opt.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Code Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE applied to Python code: model diagnoses performance/readability issues in NL feedback and rewrites code to improve time complexity or readability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM used to generate code, provide optimization feedback, and produce optimized code in refinement steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates code, then produces actionable feedback (e.g., 'avoid repeated calculations; use DP or NumPy'); refines code accordingly; history appended across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transform inefficient Python programs to more optimized implementations (improve runtime/complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>% programs optimized (OPT %)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>14.8% (%OPT baseline GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>23.0% (%OPT with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some failures due to incorrect feedback localization or inappropriate fixes; qualitative analysis found many failures originate from erroneous feedback (33% mis-localization, 61% inappropriate fixes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6973.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (ChatGPT Code Opt.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with ChatGPT on Code Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE with ChatGPT yields modest improvements in percent of programs optimized by generating actionable feedback and refactoring code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational instruction-following model used as generator, feedback provider, and refiner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>FEEDBACK step produces concrete optimization suggestions; REFINE step rewrites code accordingly; history included in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve efficiency of Python programs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>% programs optimized (OPT %)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>23.9% (%OPT baseline ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>27.5% (%OPT with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gains smaller than other tasks; feedback quality still critical. Some model generations simply alter superficial aspects rather than fixing algorithmic complexity unless feedback is precise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6973.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-4 Code Opt.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-4 on Code Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying SELF-REFINE to GPT-4 improves the fraction of programs that are optimized by the model by iteratively producing actionable feedback and revised implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity instruction-following LLM used to generate and refine code via few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates feedback diagnosing inefficiencies and then rewrites code; history retained across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve efficiency of Python programs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>% programs optimized (OPT %)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>27.3% (%OPT baseline GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>36.0% (%OPT with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Algorithmic improvements require feedback that pinpoints complexity issues; some initial outputs are deceptively correct-looking and feedback may not detect subtle errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6973.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-3.5 Code Readability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Code Readability Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Refinement pipeline focuses on improving variable naming, comments and structure by producing NL feedback and refactoring code accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM used to refactor code for readability using FEEDBACK→REFINE few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial code; produce specific feedback (e.g., improve variable naming); refine code conditioned on feedback and history.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Readability Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Refactor Python code to improve readability (variable names, comments, clarity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Fraction / percentage of appropriately named variables (GPT-4-pref scoring used in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>37.4% (baseline GPT-3.5 readability metric)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>51.3% (readability metric with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements depend on the model's ability to propose concrete renamings; some refinements may change style but not uniformly improve all readability aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6973.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (ChatGPT Code Readability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with ChatGPT on Code Readability Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying SELF-REFINE with ChatGPT substantially increased measured readability scores by producing targeted NL feedback and refactorings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational instruction-following model used for generation, critique, and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates actionable feedback and then refines code; few-shot examples guide both feedback and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Readability Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Refactor Python code to improve readability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-4-based preference/scoring metric quantifying variable naming etc. (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>27.7% (baseline ChatGPT readability metric)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>63.1% (readability metric with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Substantial gains but dependent on evaluator; human evaluation may differ. Some refinements may overfit to the few-shot examples' style.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6973.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-4 Code Readability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-4 on Code Readability Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE with GPT-4 improves measured readability scores by iteratively producing and incorporating actionable feedback on code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity instruction-following LLM used as generator/critic/refiner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use few-shot prompts demonstrating input-output-feedback-refined quadruples to elicit targeted feedback and refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Readability Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Refactor Python code for readability improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-4-based fraction of properly named variables / readability % score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>27.4% (baseline GPT-4 readability metric)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>56.2% (readability metric with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Effectiveness depends on how readability is operationalized (e.g., variable naming fraction); may not capture all dimensions of readability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6973.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-3.5 Math Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Math Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Apply SELF-REFINE to math word problems: model generates solution, critiques it, and attempts to refine the solution iteratively; limited gains observed due to difficulty of detecting nuanced math errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following LLM used to produce chain-of-thought-like solutions, self-feedback, and refinements via few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model produces step-by-step solution then feedback about errors; uses feedback to refine the solution. FEEDBACK may include a stop indicator.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math Reasoning (GSM8K-like / math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve grade-school math / math word problems; metric is solve rate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>% solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>64.1% (solve rate baseline GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>64.1% (solve rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Little or no improvement because the model often fails to detect subtle mathematical errors ('everything looks good' behaviour); external verifier or oracle to detect correctness improves gains (paper notes larger gains if external error detector used).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e6973.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (ChatGPT Math Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with ChatGPT on Math Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE with ChatGPT yields negligible improvements on math solve-rate tasks because the model often cannot accurately identify its math errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational LLM used for chain-of-thought style solutions, self-feedback and iterative refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>FEEDBACK step prompts model to check solution; REFINE produces corrected solution if errors are found; repeated up to 4 times.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve math word problems; metric is solve rate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>% solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>74.8% (solve rate baseline ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>75.0% (solve rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Minimal improvement due to inability to reliably detect fine-grained math mistakes; feedback often states 'everything looks good' causing no refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e6973.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-4 Math Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-4 on Math Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE applied to GPT-4 yields marginal improvements on math solve-rate because GPT-4 often fails to detect subtle mistakes in its own chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity instruction-following LLM used for generating solutions, self-feedback, and refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative critique and revision; prompts include examples of feedback and refinements for math problems; history augments prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve math word problems; metric is solve rate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>% solve rate (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>92.9% (solve rate baseline GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>93.1% (solve rate with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Marginal gains; self-feedback often fails to identify small arithmetic or line-level errors. Paper notes larger gains (>5%) possible if an external verifier flags incorrect answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e6973.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-3.5 Acronym Gen.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Acronym Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE used to produce acronyms from titles: model proposes an acronym, generates feedback on pronunciation/relation, and refines to produce better acronyms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM used as generator and critic in few-shot prompting settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model produces an acronym, feedback suggests improvements (ease of pronunciation, relation to title), then refines the acronym; multiple aspects can be scored numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Acronym Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate context-relevant, pronounceable acronyms for given titles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B; GPT-4 used as proxy in some analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>41.6% (preference/baseline GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>56.4% (preference with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Multi-aspect nature can cause trade-offs across iterations (improving one aspect may hurt another); paper uses numeric multi-aspect scores to select balanced outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e6973.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (ChatGPT Acronym Gen.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with ChatGPT on Acronym Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE with ChatGPT improves acronym quality by iteratively applying specific feedback about pronunciation, spelling, and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational LLM used to generate and refine acronyms using few-shot prompts producing actionable feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates acronyms and feedback examples in the prompt; refiner uses feedback and history to produce improved acronyms.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Acronym Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate context-relevant, pronounceable acronyms for given titles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B; GPT-4 used as proxy in some analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>27.2% (baseline ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>37.2% (with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvement but still sensitive to prompt examples and multi-aspect scoring to avoid regressions in other criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e6973.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-4 Acronym Gen.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-4 on Acronym Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE with GPT-4 yields large gains in acronym generation by applying iterative actionable feedback and refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity instruction-following LLM used to produce and refine acronym candidates via few-shot feedback/refine prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Alternating NL feedback and refinement steps; few-shot examples include desirable acronyms and associated feedback to guide the model.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Acronym Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate context-relevant, pronounceable acronyms for given titles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference rate (%) (human A/B; GPT-4 used as proxy in some analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>30.4% (baseline GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>56.0% (with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large gains, but success depends on the model reliably producing targeted feedback and balancing multi-aspect criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e6973.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-3.5 Constrained Gen.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Constrained Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SELF-REFINE helps include many required keywords (20–30) in generated sentences by iteratively checking coverage and refining to add missed constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM used to generate constrained outputs and iteratively incorporate feedback about missed keywords.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model receives feedback indicating which keywords are missing or coherence issues and refines to increase coverage; history retained across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Constrained Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate sentences containing a large set (20–30) of required keywords while maintaining coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Coverage % (fraction of required keywords included)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>28.0% (coverage baseline GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>37.0% (coverage with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Task benefits strongly from iteration because many keywords are missed initially; diminishing returns after early iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e6973.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (ChatGPT Constrained Gen.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with ChatGPT on Constrained Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT with SELF-REFINE achieves higher coverage of required keywords by iteratively addressing missed constraints via feedback and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational instruction-following LLM used for iterative constraint fulfillment via FEEDBACK→REFINE cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model checks which keywords are missing or coherence problems and produces refinements; few-shot prompts guide feedback specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Constrained Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate sentences containing many required keywords while maintaining coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Coverage % (fraction of required keywords included)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>44.0% (coverage baseline ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>67.0% (coverage with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Significant gains but success depends on ability to explicitly list and act on missing concepts; early iterations contribute most gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e6973.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (GPT-4 Constrained Gen.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE iterative self-feedback (applied with GPT-4 on Constrained Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying SELF-REFINE to GPT-4 greatly improves coverage of many required keywords via iterative feedback and refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity instruction-following LLM used to iteratively include missing keywords by generating feedback and applying refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model produces feedback identifying missing keywords and coherence issues and then rewrites the sentence to include missing terms; repeated up to 4 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Constrained Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate sentences containing many required keywords while maintaining coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Coverage % (fraction of required keywords included)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>15.0% (coverage baseline GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>45.0% (coverage with SELF-REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large relative gains when baseline is weak; still constrained by prompt length and complexity as many keywords increase the difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6973.21">
                <h3 class="extraction-instance">Extracted Data Instance 21 (e6973.21)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE (Vicuna-13B eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-REFINE evaluation with Vicuna-13B (failure case and mixed-refine experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper reports SELF-REFINE struggles when instantiated with weaker or conversationally-tuned models (Vicuna-13B), which often fail to generate feedback in required format or to adhere to refinement prompts; mixed setups (Vicuna init + ChatGPT feedback/refine) show promise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13B (LLaMA-13B fine-tuned on conversations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter LLaMA variant fine-tuned on web conversation data (reported in paper as Vicuna-13B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-REFINE (attempted)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Applied same few-shot FEEDBACK and REFINE prompts as for other models; Vicuna often produced empty or assistant-like feedback and failed to refine correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (FEEDBACK→REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (e.g., Sentiment Transfer, Math Reasoning) — diagnostic analysis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tests whether weaker conversational models can follow feedback/refinement prompts across different tasks; observed failure modes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Qualitative evaluation and task-specific metrics where applicable; report of failures rather than numeric gains in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Vicuna initial baselines varied (e.g., ~24.18% solve rate on Math Reasoning in paper example)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Vicuna often failed to improve; in mixed setting (Vicuna init + ChatGPT FEEDBACK/REFINE) math accuracy improved to 40.5% from 24.18% (reported mixed-refine example)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Vicuna-13B frequently failed to produce feedback in required format (empty or assistant-like responses), repeated outputs, or hallucinated; indicates SELF-REFINE requires base models with strong instruction-following and few-shot generalization capabilities. Mixed-refinement (weaker init + stronger refiner) can mitigate some issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refine: Iterative Refinement with Self-Feedback', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Correction (Generating sequences by learning to self-correct) <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback <em>(Rating: 1)</em></li>
                <li>Re3: Generating longer stories with recursive reprompting and revision <em>(Rating: 1)</em></li>
                <li>PEER: A collaborative language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6973",
    "paper_id": "paper-3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "SELF-REFINE (GPT-3.5 Sentiment Rev.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Sentiment Reversal)",
            "brief_description": "Iterative generate-feedback-refine procedure where the same LLM (GPT-3.5 / text-davinci-003) generates an initial output, produces actionable natural-language feedback on that output, and then refines the output using that feedback; repeated up to a small fixed number of iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "Instruction-tuned large language model from OpenAI used as generator, feedback provider, and refiner via few-shot prompts (no additional training).",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Generate initial output; prompt same model to produce specific, actionable NL feedback about the output; prompt same model to produce a refined output conditioned on the feedback and history; iterate feedback→refine cycles.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Sentiment Reversal",
            "task_description": "Rewrite review text to reverse its sentiment (e.g., positive→negative).",
            "evaluation_metric": "Preference rate (%) (human A/B; GPT-4 used as proxy evaluator in some analyses)",
            "performance_before_reflection": "8.8% (preference rate / baseline GPT-3.5 direct generation)",
            "performance_after_reflection": "30.4% (preference rate with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Requires high-quality, actionable NL feedback; without specific feedback performance is much lower. Overall method depends on model's ability to produce useful feedback; weaker models may fail to produce feedback or adhere to refinement prompts.",
            "uuid": "e6973.0",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (ChatGPT Sentiment Rev.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with ChatGPT / gpt-3.5-turbo on Sentiment Reversal)",
            "brief_description": "Same SELF-REFINE pipeline instantiated with ChatGPT (gpt-3.5-turbo) using few-shot prompts to produce feedback and refinements iteratively.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Conversational instruction-following LLM from OpenAI used for generation, feedback, and refinement via few-shot prompting.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Alternate FEEDBACK and REFINE steps: the model generates feedback on its own output and then produces a revised output conditioned on that feedback; history is appended to prompts across iterations.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Sentiment Reversal",
            "task_description": "Rewrite review text to reverse its sentiment.",
            "evaluation_metric": "Preference rate (%) (human A/B; GPT-4 used as proxy evaluator in some analyses)",
            "performance_before_reflection": "11.4% (preference rate / baseline ChatGPT direct generation)",
            "performance_after_reflection": "43.2% (preference rate with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Highly sensitive to feedback quality; without specific actionable feedback the task can fail (paper shows 0% success when no feedback).",
            "uuid": "e6973.1",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-4 Sentiment Rev.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-4 on Sentiment Reversal)",
            "brief_description": "SELF-REFINE applied to GPT-4: iterative NL feedback and refinement cycles produced by the same model, improving human/GPT-4 preference rates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art instruction-following LLM from OpenAI used as generator, feedback provider, and refiner via few-shot prompts.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Few-shot prompted FEEDBACK step to produce actionable, specific critique and a REFINE step that conditions on the critique and full history to produce iterative improvements.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Sentiment Reversal",
            "task_description": "Rewrite review text to reverse its sentiment.",
            "evaluation_metric": "Preference rate (%) (human A/B; GPT-4 used as proxy evaluator in some analyses)",
            "performance_before_reflection": "3.8% (preference rate / baseline GPT-4 direct generation)",
            "performance_after_reflection": "36.2% (preference rate with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Even strong models can depend on feedback quality; gains rely on model's ability to localize and specify required changes.",
            "uuid": "e6973.2",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-3.5 Dialogue)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Dialogue Response Generation)",
            "brief_description": "Iterative self-feedback pipeline yields more engaging and relevant dialogue responses by having the model critique and then refine its replies using few-shot prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "Instruction-tuned LLM used as generator, feedback provider, and refiner.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "The model generates an initial dialogue response, then produces feedback such as 'Make response relevant, engaging, safe', then refines the response conditioned on that feedback iteratively.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Dialogue Response Generation",
            "task_description": "Produce rich conversational responses given dialogue context.",
            "evaluation_metric": "Preference rate (%) (human A/B and GPT-4-pref proxy; blind A/B evaluation used for subset)",
            "performance_before_reflection": "36.4% (preference rate / baseline GPT-3.5 direct generation)",
            "performance_after_reflection": "63.6% (preference rate with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Diminishing returns across iterations; some aspects can improve while others degrade (multi-aspect trade-offs); requires multi-aspect scoring to select balanced outputs.",
            "uuid": "e6973.3",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (ChatGPT Dialogue)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with ChatGPT on Dialogue Response Generation)",
            "brief_description": "SELF-REFINE with ChatGPT improves human/GPT-4 preference rates for dialogue responses via iterative self-feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Conversational LLM used in an identical FEEDBACK→REFINE loop via few-shot prompts.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Alternating feedback and refinement steps, retaining history of outputs and feedback to inform further refinements.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Dialogue Response Generation",
            "task_description": "Produce rich conversational responses given dialogue context.",
            "evaluation_metric": "Preference rate (%) (human A/B and GPT-4-pref proxy)",
            "performance_before_reflection": "40.1% (preference rate / baseline ChatGPT direct generation)",
            "performance_after_reflection": "59.9% (preference rate with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Self-critique may sometimes declare 'everything looks good' especially for reasoning tasks; this can prevent corrections and limit gains.",
            "uuid": "e6973.4",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-4 Dialogue)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-4 on Dialogue Response Generation)",
            "brief_description": "Applying SELF-REFINE to GPT-4 drastically increases preference scores for dialogue responses by iteratively generating and incorporating feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Advanced instruction-following LLM used as generator, feedback provider and refiner.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Few-shot prompted feedback generation followed by refinement conditioned on past outputs and feedback, repeated until stop or max iterations.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Dialogue Response Generation",
            "task_description": "Produce rich conversational responses given dialogue context.",
            "evaluation_metric": "Preference rate (%) (human A/B and GPT-4-pref proxy)",
            "performance_before_reflection": "25.4% (preference rate / baseline GPT-4 direct generation)",
            "performance_after_reflection": "74.6% (preference rate with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Large increases observed but method depends on model's ability to generate actionable critique; iterative gains show diminishing returns after early iterations.",
            "uuid": "e6973.5",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-3.5 Code Opt.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Code Optimization)",
            "brief_description": "SELF-REFINE applied to Python code: model diagnoses performance/readability issues in NL feedback and rewrites code to improve time complexity or readability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "Instruction-tuned LLM used to generate code, provide optimization feedback, and produce optimized code in refinement steps.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model generates code, then produces actionable feedback (e.g., 'avoid repeated calculations; use DP or NumPy'); refines code accordingly; history appended across iterations.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Code Optimization",
            "task_description": "Transform inefficient Python programs to more optimized implementations (improve runtime/complexity).",
            "evaluation_metric": "% programs optimized (OPT %)",
            "performance_before_reflection": "14.8% (%OPT baseline GPT-3.5)",
            "performance_after_reflection": "23.0% (%OPT with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Some failures due to incorrect feedback localization or inappropriate fixes; qualitative analysis found many failures originate from erroneous feedback (33% mis-localization, 61% inappropriate fixes).",
            "uuid": "e6973.6",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (ChatGPT Code Opt.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with ChatGPT on Code Optimization)",
            "brief_description": "SELF-REFINE with ChatGPT yields modest improvements in percent of programs optimized by generating actionable feedback and refactoring code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Conversational instruction-following model used as generator, feedback provider, and refiner.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "FEEDBACK step produces concrete optimization suggestions; REFINE step rewrites code accordingly; history included in prompts.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Code Optimization",
            "task_description": "Improve efficiency of Python programs.",
            "evaluation_metric": "% programs optimized (OPT %)",
            "performance_before_reflection": "23.9% (%OPT baseline ChatGPT)",
            "performance_after_reflection": "27.5% (%OPT with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Gains smaller than other tasks; feedback quality still critical. Some model generations simply alter superficial aspects rather than fixing algorithmic complexity unless feedback is precise.",
            "uuid": "e6973.7",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-4 Code Opt.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-4 on Code Optimization)",
            "brief_description": "Applying SELF-REFINE to GPT-4 improves the fraction of programs that are optimized by the model by iteratively producing actionable feedback and revised implementations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "High-capacity instruction-following LLM used to generate and refine code via few-shot prompts.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model generates feedback diagnosing inefficiencies and then rewrites code; history retained across iterations.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Code Optimization",
            "task_description": "Improve efficiency of Python programs.",
            "evaluation_metric": "% programs optimized (OPT %)",
            "performance_before_reflection": "27.3% (%OPT baseline GPT-4)",
            "performance_after_reflection": "36.0% (%OPT with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Algorithmic improvements require feedback that pinpoints complexity issues; some initial outputs are deceptively correct-looking and feedback may not detect subtle errors.",
            "uuid": "e6973.8",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-3.5 Code Readability)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Code Readability Improvement)",
            "brief_description": "Refinement pipeline focuses on improving variable naming, comments and structure by producing NL feedback and refactoring code accordingly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "Instruction-tuned LLM used to refactor code for readability using FEEDBACK→REFINE few-shot prompts.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Generate initial code; produce specific feedback (e.g., improve variable naming); refine code conditioned on feedback and history.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Code Readability Improvement",
            "task_description": "Refactor Python code to improve readability (variable names, comments, clarity).",
            "evaluation_metric": "Fraction / percentage of appropriately named variables (GPT-4-pref scoring used in evaluation)",
            "performance_before_reflection": "37.4% (baseline GPT-3.5 readability metric)",
            "performance_after_reflection": "51.3% (readability metric with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvements depend on the model's ability to propose concrete renamings; some refinements may change style but not uniformly improve all readability aspects.",
            "uuid": "e6973.9",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (ChatGPT Code Readability)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with ChatGPT on Code Readability Improvement)",
            "brief_description": "Applying SELF-REFINE with ChatGPT substantially increased measured readability scores by producing targeted NL feedback and refactorings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Conversational instruction-following model used for generation, critique, and iterative refinement.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model generates actionable feedback and then refines code; few-shot examples guide both feedback and refinement.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Code Readability Improvement",
            "task_description": "Refactor Python code to improve readability.",
            "evaluation_metric": "GPT-4-based preference/scoring metric quantifying variable naming etc. (percentage)",
            "performance_before_reflection": "27.7% (baseline ChatGPT readability metric)",
            "performance_after_reflection": "63.1% (readability metric with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Substantial gains but dependent on evaluator; human evaluation may differ. Some refinements may overfit to the few-shot examples' style.",
            "uuid": "e6973.10",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-4 Code Readability)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-4 on Code Readability Improvement)",
            "brief_description": "SELF-REFINE with GPT-4 improves measured readability scores by iteratively producing and incorporating actionable feedback on code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "High-capacity instruction-following LLM used as generator/critic/refiner.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Use few-shot prompts demonstrating input-output-feedback-refined quadruples to elicit targeted feedback and refinements.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Code Readability Improvement",
            "task_description": "Refactor Python code for readability improvements.",
            "evaluation_metric": "GPT-4-based fraction of properly named variables / readability % score",
            "performance_before_reflection": "27.4% (baseline GPT-4 readability metric)",
            "performance_after_reflection": "56.2% (readability metric with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Effectiveness depends on how readability is operationalized (e.g., variable naming fraction); may not capture all dimensions of readability.",
            "uuid": "e6973.11",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-3.5 Math Reasoning)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Math Reasoning)",
            "brief_description": "Apply SELF-REFINE to math word problems: model generates solution, critiques it, and attempts to refine the solution iteratively; limited gains observed due to difficulty of detecting nuanced math errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "Instruction-following LLM used to produce chain-of-thought-like solutions, self-feedback, and refinements via few-shot prompts.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model produces step-by-step solution then feedback about errors; uses feedback to refine the solution. FEEDBACK may include a stop indicator.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Math Reasoning (GSM8K-like / math word problems)",
            "task_description": "Solve grade-school math / math word problems; metric is solve rate.",
            "evaluation_metric": "% solve rate (accuracy)",
            "performance_before_reflection": "64.1% (solve rate baseline GPT-3.5)",
            "performance_after_reflection": "64.1% (solve rate with SELF-REFINE)",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Little or no improvement because the model often fails to detect subtle mathematical errors ('everything looks good' behaviour); external verifier or oracle to detect correctness improves gains (paper notes larger gains if external error detector used).",
            "uuid": "e6973.12",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (ChatGPT Math Reasoning)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with ChatGPT on Math Reasoning)",
            "brief_description": "SELF-REFINE with ChatGPT yields negligible improvements on math solve-rate tasks because the model often cannot accurately identify its math errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Conversational LLM used for chain-of-thought style solutions, self-feedback and iterative refinements.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "FEEDBACK step prompts model to check solution; REFINE produces corrected solution if errors are found; repeated up to 4 times.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Math Reasoning",
            "task_description": "Solve math word problems; metric is solve rate.",
            "evaluation_metric": "% solve rate (accuracy)",
            "performance_before_reflection": "74.8% (solve rate baseline ChatGPT)",
            "performance_after_reflection": "75.0% (solve rate with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Minimal improvement due to inability to reliably detect fine-grained math mistakes; feedback often states 'everything looks good' causing no refinement.",
            "uuid": "e6973.13",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-4 Math Reasoning)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-4 on Math Reasoning)",
            "brief_description": "SELF-REFINE applied to GPT-4 yields marginal improvements on math solve-rate because GPT-4 often fails to detect subtle mistakes in its own chain-of-thought.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "High-capacity instruction-following LLM used for generating solutions, self-feedback, and refinements.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Iterative critique and revision; prompts include examples of feedback and refinements for math problems; history augments prompts.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Math Reasoning",
            "task_description": "Solve math word problems; metric is solve rate.",
            "evaluation_metric": "% solve rate (accuracy)",
            "performance_before_reflection": "92.9% (solve rate baseline GPT-4)",
            "performance_after_reflection": "93.1% (solve rate with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Marginal gains; self-feedback often fails to identify small arithmetic or line-level errors. Paper notes larger gains (&gt;5%) possible if an external verifier flags incorrect answers.",
            "uuid": "e6973.14",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-3.5 Acronym Gen.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Acronym Generation)",
            "brief_description": "SELF-REFINE used to produce acronyms from titles: model proposes an acronym, generates feedback on pronunciation/relation, and refines to produce better acronyms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "Instruction-tuned LLM used as generator and critic in few-shot prompting settings.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model produces an acronym, feedback suggests improvements (ease of pronunciation, relation to title), then refines the acronym; multiple aspects can be scored numerically.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Acronym Generation",
            "task_description": "Generate context-relevant, pronounceable acronyms for given titles.",
            "evaluation_metric": "Preference rate (%) (human A/B; GPT-4 used as proxy in some analyses)",
            "performance_before_reflection": "41.6% (preference/baseline GPT-3.5)",
            "performance_after_reflection": "56.4% (preference with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Multi-aspect nature can cause trade-offs across iterations (improving one aspect may hurt another); paper uses numeric multi-aspect scores to select balanced outputs.",
            "uuid": "e6973.15",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (ChatGPT Acronym Gen.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with ChatGPT on Acronym Generation)",
            "brief_description": "SELF-REFINE with ChatGPT improves acronym quality by iteratively applying specific feedback about pronunciation, spelling, and relevance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Conversational LLM used to generate and refine acronyms using few-shot prompts producing actionable feedback.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model generates acronyms and feedback examples in the prompt; refiner uses feedback and history to produce improved acronyms.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Acronym Generation",
            "task_description": "Generate context-relevant, pronounceable acronyms for given titles.",
            "evaluation_metric": "Preference rate (%) (human A/B; GPT-4 used as proxy in some analyses)",
            "performance_before_reflection": "27.2% (baseline ChatGPT)",
            "performance_after_reflection": "37.2% (with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvement but still sensitive to prompt examples and multi-aspect scoring to avoid regressions in other criteria.",
            "uuid": "e6973.16",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-4 Acronym Gen.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-4 on Acronym Generation)",
            "brief_description": "SELF-REFINE with GPT-4 yields large gains in acronym generation by applying iterative actionable feedback and refinements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "High-capacity instruction-following LLM used to produce and refine acronym candidates via few-shot feedback/refine prompting.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Alternating NL feedback and refinement steps; few-shot examples include desirable acronyms and associated feedback to guide the model.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Acronym Generation",
            "task_description": "Generate context-relevant, pronounceable acronyms for given titles.",
            "evaluation_metric": "Preference rate (%) (human A/B; GPT-4 used as proxy in some analyses)",
            "performance_before_reflection": "30.4% (baseline GPT-4)",
            "performance_after_reflection": "56.0% (with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Large gains, but success depends on the model reliably producing targeted feedback and balancing multi-aspect criteria.",
            "uuid": "e6973.17",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-3.5 Constrained Gen.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-3.5 on Constrained Generation)",
            "brief_description": "SELF-REFINE helps include many required keywords (20–30) in generated sentences by iteratively checking coverage and refining to add missed constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "Instruction-tuned LLM used to generate constrained outputs and iteratively incorporate feedback about missed keywords.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model receives feedback indicating which keywords are missing or coherence issues and refines to increase coverage; history retained across iterations.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Constrained Generation",
            "task_description": "Generate sentences containing a large set (20–30) of required keywords while maintaining coherence.",
            "evaluation_metric": "Coverage % (fraction of required keywords included)",
            "performance_before_reflection": "28.0% (coverage baseline GPT-3.5)",
            "performance_after_reflection": "37.0% (coverage with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Task benefits strongly from iteration because many keywords are missed initially; diminishing returns after early iterations.",
            "uuid": "e6973.18",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (ChatGPT Constrained Gen.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with ChatGPT on Constrained Generation)",
            "brief_description": "ChatGPT with SELF-REFINE achieves higher coverage of required keywords by iteratively addressing missed constraints via feedback and refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Conversational instruction-following LLM used for iterative constraint fulfillment via FEEDBACK→REFINE cycles.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model checks which keywords are missing or coherence problems and produces refinements; few-shot prompts guide feedback specificity.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Constrained Generation",
            "task_description": "Generate sentences containing many required keywords while maintaining coherence.",
            "evaluation_metric": "Coverage % (fraction of required keywords included)",
            "performance_before_reflection": "44.0% (coverage baseline ChatGPT)",
            "performance_after_reflection": "67.0% (coverage with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Significant gains but success depends on ability to explicitly list and act on missing concepts; early iterations contribute most gains.",
            "uuid": "e6973.19",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (GPT-4 Constrained Gen.)",
            "name_full": "SELF-REFINE iterative self-feedback (applied with GPT-4 on Constrained Generation)",
            "brief_description": "Applying SELF-REFINE to GPT-4 greatly improves coverage of many required keywords via iterative feedback and refinements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "High-capacity instruction-following LLM used to iteratively include missing keywords by generating feedback and applying refinements.",
            "model_size": null,
            "reflection_method_name": "SELF-REFINE",
            "reflection_method_description": "Model produces feedback identifying missing keywords and coherence issues and then rewrites the sentence to include missing terms; repeated up to 4 iterations.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Constrained Generation",
            "task_description": "Generate sentences containing many required keywords while maintaining coherence.",
            "evaluation_metric": "Coverage % (fraction of required keywords included)",
            "performance_before_reflection": "15.0% (coverage baseline GPT-4)",
            "performance_after_reflection": "45.0% (coverage with SELF-REFINE)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Large relative gains when baseline is weak; still constrained by prompt length and complexity as many keywords increase the difficulty.",
            "uuid": "e6973.20",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "SELF-REFINE (Vicuna-13B eval)",
            "name_full": "SELF-REFINE evaluation with Vicuna-13B (failure case and mixed-refine experiment)",
            "brief_description": "Paper reports SELF-REFINE struggles when instantiated with weaker or conversationally-tuned models (Vicuna-13B), which often fail to generate feedback in required format or to adhere to refinement prompts; mixed setups (Vicuna init + ChatGPT feedback/refine) show promise.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-13B (LLaMA-13B fine-tuned on conversations)",
            "model_description": "13B-parameter LLaMA variant fine-tuned on web conversation data (reported in paper as Vicuna-13B).",
            "model_size": "13B",
            "reflection_method_name": "SELF-REFINE (attempted)",
            "reflection_method_description": "Applied same few-shot FEEDBACK and REFINE prompts as for other models; Vicuna often produced empty or assistant-like feedback and failed to refine correctly.",
            "iteration_type": "generate-then-reflect (FEEDBACK→REFINE)",
            "num_iterations": 4,
            "task_name": "Various (e.g., Sentiment Transfer, Math Reasoning) — diagnostic analysis",
            "task_description": "Tests whether weaker conversational models can follow feedback/refinement prompts across different tasks; observed failure modes reported.",
            "evaluation_metric": "Qualitative evaluation and task-specific metrics where applicable; report of failures rather than numeric gains in main text.",
            "performance_before_reflection": "Vicuna initial baselines varied (e.g., ~24.18% solve rate on Math Reasoning in paper example)",
            "performance_after_reflection": "Vicuna often failed to improve; in mixed setting (Vicuna init + ChatGPT FEEDBACK/REFINE) math accuracy improved to 40.5% from 24.18% (reported mixed-refine example)",
            "improvement_observed": null,
            "limitations_or_failure_cases": "Vicuna-13B frequently failed to produce feedback in required format (empty or assistant-like responses), repeated outputs, or hallucinated; indicates SELF-REFINE requires base models with strong instruction-following and few-shot generalization capabilities. Mixed-refinement (weaker init + stronger refiner) can mitigate some issues.",
            "uuid": "e6973.21",
            "source_info": {
                "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Correction (Generating sequences by learning to self-correct)",
            "rating": 2,
            "sanitized_title": "selfcorrection_generating_sequences_by_learning_to_selfcorrect"
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "rating": 1,
            "sanitized_title": "check_your_facts_and_try_again_improving_large_language_models_with_external_knowledge_and_automated_feedback"
        },
        {
            "paper_title": "Re3: Generating longer stories with recursive reprompting and revision",
            "rating": 1,
            "sanitized_title": "re3_generating_longer_stories_with_recursive_reprompting_and_revision"
        },
        {
            "paper_title": "PEER: A collaborative language model",
            "rating": 1,
            "sanitized_title": "peer_a_collaborative_language_model"
        }
    ],
    "cost": 0.025784249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Refine: <br> Iterative Refinement with Self-Feedback</h1>
<p>Aman Madaan ${ }^{1}$, Niket Tandon ${ }^{2}$, Prakhar Gupta ${ }^{1}$, Skyler Hallinan ${ }^{3}$, Luyu Gao ${ }^{1}$, Sarah Wiegreffe ${ }^{2}$, Uri Alon ${ }^{1}$, Nouha Dziri ${ }^{2}$, Shrimai Prabhumoye ${ }^{4}$, Yiming Yang ${ }^{1}$, Shashank Gupta ${ }^{2}$, Bodhisattwa Prasad Majumder ${ }^{5}$, Katherine Hermann ${ }^{6}$, Sean Welleck ${ }^{2,3}$, Amir Yazdanbakhsh ${ }^{6}$, Peter Clark ${ }^{2}$<br>${ }^{1}$ Language Technologies Institute, Carnegie Mellon University<br>${ }^{2}$ Allen Institute for Artificial Intelligence<br>${ }^{3}$ University of Washington ${ }^{4}$ NVIDIA ${ }^{5}$ UC San Diego ${ }^{6}$ Google Research, Brain Team<br>amadaan@cs.cmu.edu, nikett@allenai.org</p>
<h4>Abstract</h4>
<p>Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by $\sim 20 \%$ absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach. ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Although large language models (LLMs) can generate coherent outputs, they often fall short in addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may benefit from further iterative refinement-i.e., iteratively mapping a candidate output to an improved one-to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022), which may not always be feasible to obtain. These limitations underscore the need for an effective refinement approach that can be applied to various tasks without requiring extensive supervision.</p>
<p>Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962; Flower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, when</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Preprint. Under review.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given an input (⓪), SELF-REFINE starts by generating an output and passing it back to the same model $\mathcal{M}$ to get feedback (⓫). The feedback is passed back to $\mathcal{M}$, which refines the previously generated output (⓵). Steps (⓶) and (⓷) iterate until a stopping condition is met. SELF-REFINE is instantiated with a language model such as GPT-3.5 and does not involve human assistance.
drafting an email to request a document from a colleague, an individual may initially write a direct request such as "Send me the data ASAP". Upon reflection, however, the writer recognizes the potential impoliteness of the phrasing and revises it to "Hi Ashley, could you please send me the data at your earliest convenience?". When writing code, a programmer may implement an initial "quick and dirty" implementation, and then, upon reflection, refactor their code to a solution that is more efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement without additional training, leading to higher-quality outputs on a wide range of tasks.
We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps-FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model $\mathcal{M}$, we pass it back to the same model $\mathcal{M}$ to get feedback. Then, the feedback is passed back to the same model to refine the previously-generated draft. This process is repeated either for a specified number of iterations or until $\mathcal{M}$ determines that no further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide $\mathcal{M}$ to both generate feedback and incorporate the feedback into an improved draft. Figure 1 illustrates the high-level idea, that SELF-REFINE uses the same underlying language model to generate feedback and refine its outputs.
We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural language and source-code generation. We show that SELF-REFINE outperforms direct generation from strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) by 5-40\% absolute improvement. In code-generation tasks, SELF-REFINE improves the initial generation by up to absolute $13 \%$ when applied to strong code models such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which is easily extensible to other LLMs. In essence, our results show that even when an LLM cannot generate an optimal output on its first try, the LLM can often provide useful feedback and improve its own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs from a single model without any additional training, via iterative (self-)feedback and refinement.</p>
<h1>2 Iterative Refinement with SELF-REFINE</h1>
<p>Given an input sequence, SELF-REFINE generates an initial output, provides feedback on the output, and refines the output according to the feedback. SELF-REFINE iterates between feedback and refinement until a desired condition is met. SELF-REFINE relies on a suitable language model and three prompts (for initial generation, feedback, and refinement), and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail.</p>
<p>Initial generation Given an input $x$, prompt $p_{\text {gen }}$, and model $\mathcal{M}$, SELF-REFINE generates an initial output $y_{0}$ :</p>
<p>$$
y_{0}=\mathcal{M}\left(p_{\text {gen }} | x\right)
$$</p>
<p>(a) Dialogue: $x, \mathbf{y}<em _mathbf_t="\mathbf{t">{\mathbf{t}}$
(b) FEEDBACK fb
(c) REFINE $\mathbf{y}</em>$
}+1<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of SELF-REFINE: an initial output $\square$ generated by the base LLM and then passed back to the same LLM to receive feedback $\square$ to the same LLM to refine the output $\square$. The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback.</p>
<p>Algorithm 1 SELF-REFINE algorithm
Require: input $x$, model $\mathcal{M}$, prompts $\left{p_{\text {gen }}, p_{\text {fb }}, p_{\text {refine }}\right}$, stop condition stop $(-)$
$1: y_{0}=\mathcal{M}\left(p_{\text {gen }} | x\right) \quad \triangleright$ Initial generation (Eqn. 1)
2: for iteration $\mathrm{t} \in 0,1, \ldots$ do
3: $\quad f b_{t}=\mathcal{M}\left(p_{\mathrm{fb}} | x | y_{t}\right) \quad \triangleright$ Feedback (Eqn. 2)
4: if $\operatorname{stop}\left(f b_{t}, t\right)$ then $\triangleright$ Stop condition
5: break
6: else
7: $\quad y_{t+1}=\mathcal{M}\left(p_{\text {refine }} | x | y_{0} | f b_{0} | \ldots | y_{t} | f b_{t}\right) \quad \triangleright$ Refine (Eqn. 4)
8: end if
9: end for
10: return $y_{t}$</p>
<p>Figure 3: The SELF-REFINE algorithm. See (§2) for a discussion of each component.</p>
<p>For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, $p_{\text {gen }}$ is a task-specific few-shot prompt (or instruction) for an initial generation, and $|$ denotes concatenation. The few-shot prompt contains input-output pairs $\left\langle x^{(k)}, y^{(k)}\right\rangle$ for the task. ${ }^{2}$</p>
<p>FEEDBACK Next, SELF-REFINE uses the same model $\mathcal{M}$ to provide feedback $f b_{t}$ on its own output, given a task-specific prompt $p_{\mathrm{fb}}$ for generating feedback:</p>
<p>$$
f b_{t}=\mathcal{M}\left(p_{\mathrm{fb}} | x | y_{t}\right)
$$</p>
<p>Intuitively, the feedback may address multiple aspects of the output. For example, in code optimization, the feedback might address the efficiency, readability, and overall quality of the code.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Here, the prompt $p_{\mathrm{fh}}$ provides examples of feedback in the form of input-output-feedback triples $\left\langle x^{(k)}, y^{(k)}, f b^{(k)}\right\rangle$. We prompt the model to write feedback that is actionable and specific via $f b^{(k)}$. By 'actionable', we mean the feedback should contain a concrete action that would likely improve the output. By 'specific', we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is "This code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... $(\boldsymbol{n}(\boldsymbol{n}+\mathbf{1})) / 2$ ". This feedback is actionable, since it suggests the action 'use the formula...'. The feedback is specific since it mentions the 'for loop'.</p>
<p>REFINE Next, SElf-REFINE uses $\mathcal{M}$ to refine its most recent output, given its own feedback:</p>
<p>$$
y_{t+1}=\mathcal{M}\left(p_{\text {refine }} | x | y_{t} | f b_{t}\right)
$$</p>
<p>For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt $p_{\text {refine }}$ provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples $\left\langle x^{(k)}, y_{t}^{(k)}, f b_{t}^{(k)}, y_{t+1}^{(k)}\right\rangle$.
Iterating Self-Refine Self-Refine alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition $\operatorname{stop}\left(f b_{t}, t\right)$ either stops at a specified timestep $t$, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in $p_{\mathrm{fh}}$, and the condition is determined per-task.</p>
<p>To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as:</p>
<p>$$
y_{t+1}=\mathcal{M}\left(p_{\text {refine }} | x | y_{0} | f b_{0} | \ldots | y_{t} | f b_{t}\right)
$$</p>
<p>Finally, we use the last refinement $y_{t}$ as the output of SElf-REFINE.
Algorithm 1 summarizes SElf-REFINE, and Figure 2 shows an example of SElf-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the $p_{\text {gen }}, p_{\mathrm{fh}}, p_{\text {refine }}$ prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples.</p>
<h1>3 Evaluation</h1>
<p>We evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R)
Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A).</p>
<h3>3.1 Instantiating SELF-REFINE</h3>
<p>We instantiate SELF-REFINE following the high-level description in Section 2. The FEEDBACKREFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4.</p>
<p>Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art. ${ }^{3}$ We used the same prompts from previous work when</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">GPT-3.5</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">ChatGPT</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">GPT-4</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: right;">Base</td>
<td style="text-align: right;">+SELF-REFINE</td>
<td style="text-align: right;">Base</td>
<td style="text-align: right;">+SELF-REFINE</td>
<td style="text-align: right;">Base</td>
<td style="text-align: right;">+SELF-REFINE</td>
</tr>
<tr>
<td style="text-align: left;">Sentiment Reversal</td>
<td style="text-align: right;">8.8</td>
<td style="text-align: right;">$\mathbf{3 0 . 4}(\dagger 21.6)$</td>
<td style="text-align: right;">11.4</td>
<td style="text-align: right;">$\mathbf{4 3 . 2}(\dagger 31.8)$</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">$\mathbf{3 6 . 2}(\dagger 32.4)$</td>
</tr>
<tr>
<td style="text-align: left;">Dialogue Response</td>
<td style="text-align: right;">36.4</td>
<td style="text-align: right;">$\mathbf{6 3 . 6}(\dagger 27.2)$</td>
<td style="text-align: right;">40.1</td>
<td style="text-align: right;">$\mathbf{5 9 . 9}(\dagger 19.8)$</td>
<td style="text-align: right;">25.4</td>
<td style="text-align: right;">$\mathbf{7 4 . 6}(\dagger 49.2)$</td>
</tr>
<tr>
<td style="text-align: left;">Code Optimization</td>
<td style="text-align: right;">14.8</td>
<td style="text-align: right;">$\mathbf{2 3 . 0}(\dagger 8.2)$</td>
<td style="text-align: right;">23.9</td>
<td style="text-align: right;">$\mathbf{2 7 . 5}(\dagger 3.6)$</td>
<td style="text-align: right;">27.3</td>
<td style="text-align: right;">$\mathbf{3 6 . 0}(\dagger 8.7)$</td>
</tr>
<tr>
<td style="text-align: left;">Code Readability</td>
<td style="text-align: right;">37.4</td>
<td style="text-align: right;">$\mathbf{5 1 . 3}(\dagger 13.9)$</td>
<td style="text-align: right;">27.7</td>
<td style="text-align: right;">$\mathbf{6 3 . 1}(\dagger 35.4)$</td>
<td style="text-align: right;">27.4</td>
<td style="text-align: right;">$\mathbf{5 6 . 2}(\dagger 28.8)$</td>
</tr>
<tr>
<td style="text-align: left;">Math Reasoning</td>
<td style="text-align: right;">$\mathbf{6 4 . 1}$</td>
<td style="text-align: right;">$\mathbf{6 4 . 1}(0)$</td>
<td style="text-align: right;">74.8</td>
<td style="text-align: right;">$\mathbf{7 5 . 0}(\dagger 0.2)$</td>
<td style="text-align: right;">92.9</td>
<td style="text-align: right;">$\mathbf{9 3 . 1}(\dagger 0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">Acronym Generation</td>
<td style="text-align: right;">41.6</td>
<td style="text-align: right;">$\mathbf{5 6 . 4}(\dagger 14.8)$</td>
<td style="text-align: right;">27.2</td>
<td style="text-align: right;">$\mathbf{3 7 . 2}(\dagger 10.0)$</td>
<td style="text-align: right;">30.4</td>
<td style="text-align: right;">$\mathbf{5 6 . 0}(\dagger 25.6)$</td>
</tr>
<tr>
<td style="text-align: left;">Constrained Generation</td>
<td style="text-align: right;">28.0</td>
<td style="text-align: right;">$\mathbf{3 7 . 0}(\dagger 9.0)$</td>
<td style="text-align: right;">44.0</td>
<td style="text-align: right;">$\mathbf{6 7 . 0}(\dagger 23.0)$</td>
<td style="text-align: right;">15.0</td>
<td style="text-align: right;">$\mathbf{4 5 . 0}(\dagger 30.0)$</td>
</tr>
</tbody>
</table>
<p>Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2.
available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups.</p>
<h1>3.2 Metrics</h1>
<p>We report three types of metrics:</p>
<ul>
<li>Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: $\%$ solve rate; Code Optimization: \% programs optimized; Constrained Gen: coverage \%)</li>
<li>Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C.</li>
<li>GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation ( $82 \%$ for Sentiment Reversal, $68 \%$ for Acronym Generation, and $71 \%$ for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT4 to calculate fraction of the variables that are appropriately named given the context (e.g., $\mathrm{x}=[] \rightarrow$ input_buffer $=[]$ ). Additional details are provided in Appendix D.</li>
</ul>
<h3>3.3 Results</h3>
<p>Table 1 shows our main results:
SELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by $8.7 \%$ (absolute) in Code Optimization, increasing optimization percentage from $27.3 \%$ to $36.0 \%$. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F.</p>
<p>One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs.</p>
<p>In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by $49.2 \%$ - from $25.4 \%$ to $74.6 \%$. Similarly, we see remarkable improvements in the other preference-based tasks across all models.</p>
<p>The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to</p>
<p>think that "everything looks good" (e.g., ChatGPT feedback for 94\% instances is 'everything looks good'). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5\%+) if an external source can identify if the current math answer is incorrect.</p>
<p>Improvement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F.</p>
<h1>4 Analysis</h1>
<p>The three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">SELF-REFINE feedback</th>
<th style="text-align: center;">Generic feedback</th>
<th style="text-align: center;">No feedback</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Code Optimization</td>
<td style="text-align: center;">$\mathbf{2 7 . 5}$</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">24.8</td>
</tr>
<tr>
<td style="text-align: left;">Sentiment Reversal</td>
<td style="text-align: center;">$\mathbf{4 3 . 2}$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Acronym Generation</td>
<td style="text-align: center;">$\mathbf{5 6 . 4}$</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">48.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2.</p>
<p>The impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback's clear influence.</p>
<p>In Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance - specific, actionable feedback yields superior results.</p>
<p>This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0 , even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback.</p>
<p>How important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output $\left(y_{0}\right)$ has a score of 22.0 , which improves to 28.8 after three iterations $\left(y_{3}\right)$. Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations.</p>
<p>The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>$y_{0}$</th>
<th>$y_{1}$</th>
<th>$y_{2}$</th>
<th>$y_{3}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Code Opt.</td>
<td>22.0</td>
<td>27.0</td>
<td>27.9</td>
<td>$\mathbf{2 8 . 8}$</td>
</tr>
<tr>
<td>Sentiment Rev.</td>
<td>33.9</td>
<td>34.9</td>
<td>36.1</td>
<td>$\mathbf{3 6 . 8}$</td>
</tr>
<tr>
<td>Constrained Gen.</td>
<td>29.0</td>
<td>40.3</td>
<td>46.7</td>
<td>$\mathbf{4 9 . 7}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains( $\Delta$ ) are in the initial iterations for both Code Opt. and Sentiment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation).</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Slower code
def solve(amount):
    best_price = (amount + 199) // 200 *
    ~ 380
    # First loop
    for a in range(amount // 200 + 1):
        # ... 4 nested loops ...
        for c1 in range(amount // 1500 +
        ~ 1):
            if a*200 + b*300 == amount:
                price = a*380 + b*550
                    if price &lt; best_price:
                    best_price = price
    return best_price
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="n">Faster</span><span class="w"> </span><span class="n">code</span>
<span class="n">def</span><span class="w"> </span><span class="n">solve</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">coins</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">200, 300</span><span class="o">]</span>
<span class="w">    </span><span class="n">prices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">380, 550</span><span class="o">]</span>
<span class="w">    </span><span class="n">dp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">float(&#39;inf&#39;)</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">amount</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="n">dp</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">coins</span><span class="p">))</span><span class="err">:</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">coins</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">amount</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="err">:</span>
<span class="w">            </span><span class="n">dp</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">dp</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">dp</span><span class="o">[</span><span class="n">j -</span>
<span class="n">            coins[i</span><span class="o">]</span><span class="err">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">prices</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">dp</span><span class="p">(</span><span class="n">amount</span><span class="err">]</span>
</code></pre></div>

<p>Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to $\mathcal{O}($ amount $*$ coins $)$. The full example is provided in Appendix H</p>
<p>Can we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates $k=4$ samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these $k$ initial outputs in a 1 vs. $k$ evaluation. In other words, we assess whether SELF-REFINE can outperform all $k$ initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. $k$ setting, the outputs of SELF-REFINE are still preferred by humans over all $k$ initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs.</p>
<p>Does SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a</p>
<p>less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G.</p>
<p>Qualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total ( 35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code Optimization, we found that the feedback was predominantly actionable, with the majority identifying problematic aspects of the original generation and suggesting ways to rectify them.
When SELF-REFINE failed to improve the original generation, the majority of issues were due to erroneous feedback rather than faulty refinements. Specifically, $33 \%$ of unsuccessful cases were due to feedback inaccurately pinpointing the error's location, while $61 \%$ were a result of feedback suggesting an inappropriate fix. Only $6 \%$ of failures were due to the refiner incorrectly implementing good feedback. These observations highlight the vital role of accurate feedback plays in SELF-REFINE.
In successful cases, the refiner was guided by accurate and useful feedback to make precise fixes to the original generation in $61 \%$ of the cases. Interestingly, the refiner was capable of rectifying issues even when the feedback was partially incorrect, which was the situation in $33 \%$ of successful cases. This suggests resilience to sub-optimal feedback. Future research could focus on examining the refiner's robustness to various types of feedback errors and exploring ways to enhance this resilience. In Figure 5, we illustrate how SELF-REFINE significantly improves program efficiency by transforming a brute force approach into a dynamic programming solution, as a result of insightful feedback. Additional analysis on other datasets such as Dialogue Response Generation is provided in Appendix H.</p>
<p>Going Beyond Benchmarks While our evaluation focuses on benchmark tasks, SELF-REFINE is designed with broader applicability in mind. We explore this in a real-world use case of website generation, where the user provides a high-level goal and SELF-REFINE assists in iteratively developing the website. Starting from a rudimentary initial design, SELF-REFINE refines HTML, CSS, and JS to evolve the website in terms of both usability and aesthetics. This demonstrates the potential of SELF-REFINE in real-world, complex, and creative tasks. See Appendix I for examples and further discussion, including broader, societal impact of our work.</p>
<h1>5 Related work</h1>
<p>Leveraging human- and machine-generated natural language (NL) feedback for refining outputs has been effective for a variety of tasks, including summarization (Scheurer et al., 2022), script generation (Tandon et al., 2021), program synthesis (Le et al., 2022a; Yasunaga and Liang, 2020), and other tasks (Bai et al., 2022a; Schick et al., 2022b; Saunders et al., 2022a; Bai et al., 2022b; Welleck et al., 2022). Refinement methods differ in the source and format of feedback, and the way that a refiner is obtained. Table 3 summarizes some related approaches; see Appendix B for an additional discussion.</p>
<p>Source of feedback. Humans have been an effective source of feedback (Tandon et al., 2021; Elgohary et al., 2021; Tandon et al., 2022; Bai et al., 2022a). Since human feedback is costly, several approaches use a scalar reward function as a surrogate of (or alternative to) human feedback (e.g., (Bai et al., 2022a; Liu et al., 2022; Lu et al., 2022; Le et al., 2022a; Welleck et al., 2022)). Alternative sources such as compilers (Yasunaga and Liang, 2020) or Wikipedia edits (Schick et al., 2022b) can provide domain-specific feedback. Recently, LLMs have been used to generate feedback for general domains (Fu et al., 2023; Peng et al., 2023; Yang et al., 2022), However, ours is the only method that generates feedback using an LLM on its own output, for the purpose of refining with the same LLM.</p>
<p>Representation of feedback. The form of feedback can be generally divided into natural language (NL) and non-NL feedback. Non-NL feedback can come in human-provided example pairs (Dasgupta</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Supervision- <br> free refiner</th>
<th style="text-align: center;">Supervision- <br> free feedback</th>
<th style="text-align: center;">Multi-aspect <br> feedback</th>
<th style="text-align: center;">Iterative</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Learned refiners: PEER (Schick et al., <br> 2022b), Self-critique (Saunders et al., 2022b), <br> CodeRL (Le et al., 2022b), Self-correction <br> (Welleck et al., 2022).</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">or $\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">or $\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: left;">Prompted refiners: Augmenter (Peng et al., <br> 2023), Re ${ }^{\mathrm{x}}$ (Yang et al., 2022), Reflexion <br> (Shinn et al., 2023).</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">or $\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SELF-REFINE (this work)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: A comparison of SELF-REFINE to closely related prior refinement approaches.
et al., 2019) or scalar rewards (Liu et al., 2022; Le et al., 2022b). In this work, we use NL feedback, since this allows the model to easily provide self-feedback using the same LM that generated the output, while leveraging existing pretrained LLMs such as GPT-4.</p>
<p>Types of refiners. Pairs of feedback and refinement have been used to learn supervised refiners (Schick et al., 2022b; Du et al., 2022; Yasunaga and Liang, 2020; Madaan et al., 2021). Since gathering supervised data is costly, some methods learn refiners using model generations (Welleck et al., 2022; Peng et al., 2023). However, the refiners are trained for each new domain. Finally, (Yang et al., 2022) use prompted feedback and refinement specifically tailored for story generation. In this work, we avoid training a separate refiner, and show that the same model can be used as both the refiner and the source of feedback across multiple domains.</p>
<p>Non-refinement reinforcement learning (RL) approaches. Rather than having explicit refinement, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022a)). These methods differ from SELF-REFINE in that the model does not access feedback on an intermediate generation. Second, these RL methods require updating the model's parameters, unlike SELF-REFINE.</p>
<h1>6 Limitations and Discussion</h1>
<p>The main limitation of our approach is that the base models need to have sufficient few-shot modeling or instruction-following abilities, in order to learn to provide feedback and to refine in an in-context fashion, without having to train supervised models and rely on supervised data.
Further, the experiments in this work were performed with language models that are not open-sourced, namely GPT-3.5, ChatGPT, GPT-4, and CODEX. Existing literature (Ouyang et al., 2022) does not fully describe the details of these models, such as the pretraining corpus, model sizes, and model biases. Further, these models are not free to use, and using them for research requires some funding. Nonetheless, we release our code and model outputs to ensure the reproducibility of our work.
Another limitation of our work is that we exclusively experiment with datasets in English. In other languages, the current models may not provide the same benefits.
Finally, there is a possibility for bad actors to use prompting techniques to steer a model to generate more toxic or harmful text. Our approach does not explicitly guard against this.</p>
<h2>7 Conclusion</h2>
<p>We present SELF-REFINE: a novel approach that allows large language models to iteratively provide self-feedback and refine their own outputs. SELF-REFINE operates within a single LLM, requiring neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of use of SELF-REFINE across a wide variety of tasks. By showcasing the potential of SELF-REFINE in diverse tasks, our research contributes to the ongoing exploration and development of large language models, with the aim of reducing the cost of human creative processes in real-world settings. We</p>
<p>hope that our iterative approach will help drive further research in this area. To this end, we make all our code, data and prompts anonymously available at https://selfrefine.info/.</p>
<h1>References</h1>
<p>Teresa M. Amabile. 1983. A Theoretical Framework. In The Social Psychology of Creativity, pages 65-96. Springer New York, New York, NY.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv:2204.05862.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Emery D Berger, Sam Stern, and Juan Altmayer Pizzorno. 2022. Triangulating Python Performance Issues with SCALENE. ArXiv preprint, abs/2212.07597.</p>
<p>Lawrence D Brown, T Tony Cai, and Anirban DasGupta. 2001. Interval estimation for a binomial proportion. Statistical science, 16(2):101-133.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901, Online. Curran Associates, Inc.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. 2019. Teaching a black-box learner. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages $1547-1555$. PMLR.</p>
<p>Wanyu Du, Zae Myung Kim, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang. 2022. Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. In Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022), pages 96-108, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural language interaction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5599-5610, Online. Association for Computational Linguistics.</p>
<p>Linda Flower and John R Hayes. 1981. A cognitive process theory of writing. College composition and communication, 32(4):365-387.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post.</p>
<p>Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022a. CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning.</p>
<p>Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022b. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. ArXiv, abs/2207.01780.</p>
<p>Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865-1874, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823-1840, Online. Association for Computational Linguistics.</p>
<p>Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. ArXiv, abs/2205.13636.</p>
<p>Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, and Amir Yazdanbakhsh. 2023. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867.</p>
<p>Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, and Eduard Hovy. 2021. Think about it! improving defeasible reasoning by first modeling the question scenario. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6291-6310, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020. Unsupervised evaluation of interactive dialog with DialoGPT. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225-235, 1st virtual meeting. Association for Computational Linguistics.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. ArXiv preprint, abs/2203.13474.</p>
<p>OpenAI. Model index for researchers. https://platform.openai.com/docs/ model-index-for-researchers. Accessed: May 14, 2023.</p>
<p>OpenAI. 2022. Model index for researchers. Blogpost.
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv:2203.02155.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback.</p>
<p>Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style transfer through back-translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866-876, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.</p>
<p>Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. 2021. Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655.</p>
<p>Machel Reid and Graham Neubig. 2022. Learning to model editing processes. arXiv preprint arXiv:2205.12374.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022a. Self-critiquing models for assisting human evaluators.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022b. Self-critiquing models for assisting human evaluators. ArXiv:2206.05802.</p>
<p>Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022. Training language models with natural language feedback. ArXiv:2204.14146.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022a. Peer: A collaborative language model.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022b. Peer: A collaborative language model. ArXiv, abs/2208.11663.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection.</p>
<p>Herbert A. Simon. 1962. The architecture of complexity. Proceedings of the American Philosophical Society, 106(6):467-482.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.</p>
<p>Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. 2021. Interscript: A dataset for interactive learning of scripts through error feedback. arXiv preprint arXiv:2112.07867.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 339-352.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053.</p>
<p>Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Michihiro Yasunaga and Percy Liang. 2020. Graph-based, self-supervised program repair from diagnostic feedback. 37th Int. Conf. Mach. Learn. ICML 2020, PartF168147-14:10730-10739.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28.</p>
<h1>A Evaluation Tasks</h1>
<p>Table 4 lists the tasks in our evaluation, and examples from each task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task and Description</th>
<th style="text-align: center;">Sample one iteration of FEEDBACK-REFINE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Sentiment Reversal</td>
<td style="text-align: center;">$x$ : The food was fantastic..."</td>
</tr>
<tr>
<td style="text-align: center;">Rewrite reviews to reverse sentiment.</td>
<td style="text-align: center;">$y_{t}$ : The food was disappointing..."</td>
</tr>
<tr>
<td style="text-align: center;">Dataset: (Zhang et al., 2015) 1000 review passages</td>
<td style="text-align: center;">$f b$ : Increase negative sentiment $y_{t+1}$ : The food was utterly terrible..."</td>
</tr>
<tr>
<td style="text-align: center;">Dialogue Response Generation</td>
<td style="text-align: center;">$x$ : What's the best way to cook pasta?"</td>
</tr>
<tr>
<td style="text-align: center;">Produce rich conversational responses.</td>
<td style="text-align: center;">$y_{t}$ : The best way to cook pasta is to..."</td>
</tr>
<tr>
<td style="text-align: center;">Dataset: (Mehri and Eskenazi, 2020) 372 conv.</td>
<td style="text-align: center;">$f b$ : Make response relevant, engaging, safe $y_{t+1}$ : Boil water, add salt, and cook pasta..."</td>
</tr>
<tr>
<td style="text-align: center;">Code Optimization</td>
<td style="text-align: center;">$x$ : Nested loop for matrix product</td>
</tr>
<tr>
<td style="text-align: center;">Enhance Python code efficiency</td>
<td style="text-align: center;">$y_{t}$ : NumPy dot product function</td>
</tr>
<tr>
<td style="text-align: center;">Dataset: (Madaan et al., 2023): 1000 programs</td>
<td style="text-align: center;">$f b$ : Improve time complexity $y_{t+1}$ : Use NumPy's optimized matmul function</td>
</tr>
<tr>
<td style="text-align: center;">Code Readability Improvement</td>
<td style="text-align: center;">$x$ : Unclear variable names, no comments</td>
</tr>
<tr>
<td style="text-align: center;">Refactor Python code for readability.</td>
<td style="text-align: center;">$y_{t}$ : Descriptive names, comments</td>
</tr>
<tr>
<td style="text-align: center;">Dataset: (Puri et al., 2021) 300 programs*</td>
<td style="text-align: center;">$f b$ : Enhance variable naming; add comments $y_{t+1}$ : Clear variables, meaningful comments</td>
</tr>
<tr>
<td style="text-align: center;">Math Reasoning</td>
<td style="text-align: center;">$x$ : Olivia has $\$ 23$, buys 5 bagels at $\$ 3$ each"</td>
</tr>
<tr>
<td style="text-align: center;">Solve math reasoning problems.</td>
<td style="text-align: center;">$y_{t}$ : Solution in Python</td>
</tr>
<tr>
<td style="text-align: center;">Dataset: (Cobbe et al., 2021) 1319 questions</td>
<td style="text-align: center;">$f b$ : Show step-by-step solution $y_{t+1}$ : Solution with detailed explanation</td>
</tr>
<tr>
<td style="text-align: center;">Acronym Generation</td>
<td style="text-align: center;">$x$ : Radio Detecting and Ranging"</td>
</tr>
<tr>
<td style="text-align: center;">Generate acronyms for a given title</td>
<td style="text-align: center;">$y_{t}$ : RDR</td>
</tr>
<tr>
<td style="text-align: center;">Dataset: (Appendix Q) 250 acronyms</td>
<td style="text-align: center;">$f b$ : be context relevant; easy pronunciation $y_{t+1}:$ RADAR"</td>
</tr>
<tr>
<td style="text-align: center;">Constrained Generation</td>
<td style="text-align: center;">$x$ : beach, vacation, relaxation</td>
</tr>
<tr>
<td style="text-align: center;">Generate sentences with given keywords.</td>
<td style="text-align: center;">$y_{t}$ : During our beach vacation...</td>
</tr>
<tr>
<td style="text-align: center;">Dataset: (Lin et al., 2020) 200 samples</td>
<td style="text-align: center;">$f b$ : Include keywords; maintain coherence $y_{t+1}:$.. beach vacation was filled with relaxation</td>
</tr>
</tbody>
</table>
<p>Table 4: An overview of the tasks which we evaluate SELF-REFINE on, along with their associated datasets and sizes. For every task, we demonstrate a single iteration of refinement of input $x$, the previously generated output $y_{t}$, the feedback generated $f b_{t}$, and the refinement $y_{t+1}$. Few-shot prompts used for FEEDBACK and REFINE are provided in Appendix S.</p>
<h1>B Broader Related Work</h1>
<p>Compared to a concurrent work, Reflexion (Shinn et al., 2023), our approach involves correction using feedback, whereas their setup involves finding the next best solution in planning using ReAct. While ReAct and Reflexion provide a free-form reflection on whether a step was executed correctly and potential improvements, our approach is more granular and structured, with multi-dimensional feedback and scores. This distinction allows our method to offer more precise and actionable feedback, making it suitable for a wider range of natural language generation tasks, including those that may not necessarily involve step-by-step planning such as open-ended dialogue generation.</p>
<p>Comparison with Welleck et al. (2022) The closest work to ours may be Self-Correction (Welleck et al., 2022); however, Self-Correction has several disadvantages compared to SELF-REFINE:</p>
<ol>
<li>Self-Correction does not train their model to generate explicit feedback; instead, Welleck et al. (2022) trained their models to refine only. As we show in Section 4 and Table 2, having the model generate explicit feedback results in significantly better refined outputs.</li>
<li>Self-Correction trains a separate refiner (or "corrector") for each task. In contrast, SELFREFINE uses instructions and few-shot prompting, and thus does not require training a separate refiner for each task.</li>
<li>Empirically, we evaluated SELF-REFINE using the same base model of GPT-3 as SelfCorrection, and with the same settings on the GSM8K benchmark. Self-Correction achieved $45.9 \%$ accuracy while SELF-REFINE (this work) achieved $\mathbf{5 5 . 7 \%}$ ( $\mathbf{1 9 . 8}$ ).</li>
</ol>
<p>Comparison with non-refinement reinforcement learning (RL) approaches. Rather than having an explicit refinement module, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022a)). These methods differ from SELF-REFINE (and more generally, refinement-based approaches) in that the model cannot access feedback on an intermediate generation. Second, these reinforcement learning methods require updating the model's parameters, unlike SELF-REFINE.
See Table 5 for an additional detailed comparison of related work.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Primary Novelty</th>
<th style="text-align: center;">rere/few shot improvement</th>
<th style="text-align: center;">multi aspect critics</th>
<th style="text-align: center;">NL feedback with error localization</th>
<th style="text-align: center;">iterative framework</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BLHF (Stiennon et al., 2020)</td>
<td style="text-align: center;">optimize for human preference</td>
<td style="text-align: center;">Xtrained on feedback</td>
<td style="text-align: center;">X single (human)</td>
<td style="text-align: center;">Gnot self gen.)</td>
<td style="text-align: center;">X</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RL to generate knowledge</td>
<td style="text-align: center;">Xtrained on end task</td>
<td style="text-align: center;">X single(accuracy)</td>
<td style="text-align: center;">X (neveri. only)</td>
<td style="text-align: center;">X</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">quantization to edit generations</td>
<td style="text-align: center;">X trained on end task</td>
<td style="text-align: center;">X single(scalar score)</td>
<td style="text-align: center;">X (dense signal)</td>
<td style="text-align: center;">(train time iter.)</td>
</tr>
<tr>
<td style="text-align: center;">Qi-vent RL (Lu et al., 2022)</td>
<td style="text-align: center;">actor critic RL for code improvement</td>
<td style="text-align: center;">X trained on end task</td>
<td style="text-align: center;">X single(unit tests)</td>
<td style="text-align: center;">X (dense signal)</td>
<td style="text-align: center;">X</td>
</tr>
<tr>
<td style="text-align: center;">Delleguie (Yasumga and Liang, 2020)</td>
<td style="text-align: center;">Complete feedback to iteratively repair</td>
<td style="text-align: center;">X trained semi sap.</td>
<td style="text-align: center;">X single(compiler msg)</td>
<td style="text-align: center;">Gnot self gen.)</td>
<td style="text-align: center;">园</td>
</tr>
<tr>
<td style="text-align: center;">PEER (Schick et al., 2022b)</td>
<td style="text-align: center;">doc edit trained on wiki edits</td>
<td style="text-align: center;">Xtrained on edits</td>
<td style="text-align: center;">X single(accuracy)</td>
<td style="text-align: center;">Gnot self gen.)</td>
<td style="text-align: center;">园</td>
</tr>
<tr>
<td style="text-align: center;">Self critique (Samslavi et al., 2022a)</td>
<td style="text-align: center;">few shot critique generation</td>
<td style="text-align: center;">X feedback training</td>
<td style="text-align: center;">X single(human)</td>
<td style="text-align: center;">G self gen.)</td>
<td style="text-align: center;">园</td>
</tr>
<tr>
<td style="text-align: center;">Self-correct (Welleck et al., 2022)</td>
<td style="text-align: center;">novel training of a corrector train RL4F on automat (critique, revision) pair</td>
<td style="text-align: center;">X trained on end task</td>
<td style="text-align: center;">X single (task specific)</td>
<td style="text-align: center;">G (limited setting)</td>
<td style="text-align: center;">(limited setting)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">X critique training</td>
<td style="text-align: center;">G (fixed set)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Self-ask (Press et al., 2022)</td>
<td style="text-align: center;">ask followup ques when interim ans correct,final wrong</td>
<td style="text-align: center;">G few shot</td>
<td style="text-align: center;">X none</td>
<td style="text-align: center;">X(none)</td>
<td style="text-align: center;">X</td>
</tr>
<tr>
<td style="text-align: center;">GPT3 score (Fu et al., 2023)</td>
<td style="text-align: center;">GPT can score generations with instruction</td>
<td style="text-align: center;">G few shot</td>
<td style="text-align: center;">X single(single utility fn)</td>
<td style="text-align: center;">X(none)</td>
<td style="text-align: center;">X</td>
</tr>
<tr>
<td style="text-align: center;">Augmenter (Peng et al., 2023)</td>
<td style="text-align: center;">factuality feedback from external KBs</td>
<td style="text-align: center;">G few shot</td>
<td style="text-align: center;">X single(factuality)</td>
<td style="text-align: center;">G self gen.)</td>
<td style="text-align: center;">园</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{Re}^{\mathrm{D}}$ (Yang et al., 2022)</td>
<td style="text-align: center;">$\sim$ ours but one domain, trained critics</td>
<td style="text-align: center;">G few shot</td>
<td style="text-align: center;">G (trained critics)</td>
<td style="text-align: center;">Gnot self gen.)</td>
<td style="text-align: center;">园</td>
</tr>
<tr>
<td style="text-align: center;">SELF-REFINE</td>
<td style="text-align: center;">fewshot iterative multi aspect NL fb</td>
<td style="text-align: center;">G few shot</td>
<td style="text-align: center;">G multiple(few shot critics)</td>
<td style="text-align: center;">G self gen.)</td>
<td style="text-align: center;">园</td>
</tr>
</tbody>
</table>
<p>Table 5: Summary of related approaches. Reinforcement learning approaches are shown in purple
, trained corrector approaches are shown in orange, and few-shot corrector approaches are shown in green.</p>
<h1>C Human Evaluation</h1>
<p>The A/B evaluation in our study was conducted by the authors, where a human judge was presented with an input, task instruction, and two candidate outputs generated by the baseline method and Self-Refine. The setup was blind, i.e., the judges did not know which outputs were generated by which method. The judge was then asked to select the output that is better aligned with the task instruction. For tasks that involve A/B evaluation, we calculate the relative improvement as the percentage increase in preference rate. The preference rate represents the proportion of times annotators selected the output produced by SELF-REFINE over the output from the baseline method. Table 6 shows the results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">SELF-REFINE (\%)</th>
<th style="text-align: center;">Direct (\%)</th>
<th style="text-align: center;">Either (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sentiment Transfer</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">21.43</td>
<td style="text-align: center;">3.57</td>
</tr>
<tr>
<td style="text-align: left;">Acronym Generation</td>
<td style="text-align: center;">44.59</td>
<td style="text-align: center;">12.16</td>
<td style="text-align: center;">43.24</td>
</tr>
<tr>
<td style="text-align: left;">Response Generation</td>
<td style="text-align: center;">47.58</td>
<td style="text-align: center;">19.66</td>
<td style="text-align: center;">32.76</td>
</tr>
</tbody>
</table>
<p>Table 6: Relative improvement of SELF-REFINE in A/B evaluations across different tasks. The values represent normalized preferences, which correspond to the proportion of times the output generated by SELF-REFINE was selected as better aligned with the task instruction over the baseline method. The evaluation was conducted for 150 examples for each dataset. The judges were not aware of the method that generated each sample.</p>
<h1>D GPT-4 Evaluation</h1>
<p>In light of the impressive achievements of GPT-4 in assessing and providing reasoning for complex tasks, we leverage its abilities for evaluation in SELF-REFINE. The approach involves presenting tasks to GPT-4 in a structured manner, promoting the model's deliberation on the task and generating a rationale for its decision. This methodology is demonstrated in Listings 1 to 3:</p>
<p>Listing 1 Prompt for GPT-4 evaluation of Sentiment Reversal.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">f</span><span class="s">&quot;&quot;</span><span class="err">&quot;</span><span class="nx">Which</span><span class="w"> </span><span class="nx">review</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">aligned</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">sentiment</span><span class="w"> </span><span class="p">{</span><span class="nx">target_sentiment</span><span class="p">}?</span>
<span class="nx">Review</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">review_a</span><span class="p">}</span>
<span class="nx">Review</span><span class="w"> </span><span class="nx">B</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">review_b</span><span class="p">}.</span>
<span class="nx">Pick</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="p">[</span><span class="err">&#39;</span><span class="nx">Review</span><span class="w"> </span><span class="nx">A</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">Review</span><span class="w"> </span><span class="nx">B</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">both</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">neither</span><span class="err">&#39;</span><span class="p">].</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">a</span>
<span class="o">~</span><span class="w"> </span><span class="nx">short</span><span class="w"> </span><span class="nx">explanation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="kd">choice</span><span class="w"> </span><span class="nx">first</span><span class="p">.</span><span class="w"> </span><span class="nx">Then</span><span class="p">,</span><span class="w"> </span><span class="nx">generate</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">The</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">aligned</span>
<span class="o">~</span><span class="w"> </span><span class="nx">review</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">A</span><span class="err">&#39;</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">The</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">aligned</span><span class="w"> </span><span class="nx">review</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">B</span><span class="err">&#39;</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">The</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">aligned</span><span class="w"> </span><span class="nx">review</span><span class="w"> </span><span class="k">is</span>
<span class="o">~</span><span class="w"> </span><span class="nx">both</span><span class="err">&#39;</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">The</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">aligned</span><span class="w"> </span><span class="nx">review</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">neither</span><span class="err">&#39;</span><span class="p">.</span>
<span class="nx">Format</span><span class="p">:</span><span class="w"> </span><span class="p">&lt;</span><span class="nx">explanation</span><span class="p">&gt;</span><span class="w"> </span><span class="p">&lt;</span><span class="nx">answer</span><span class="p">&gt;</span><span class="w"> </span><span class="nx">STOP</span>
</code></pre></div>

<p>Listing 2 Prompt for GPT-4 evaluation of Acronym Generation.</p>
<div class="codehilite"><pre><span></span><code>f&quot;&quot;&quot;Title: {title}
Acronym A: {acronym_a}
Acronym B: {acronym_b}
Pick the better acronym for the given title. The acronyms should be compared based
~ on the following criteria:
<span class="gs">* Ease of pronunciation.</span>
<span class="gs">*</span> Ease of spelling.
<span class="gs">* Relation to title.</span>
<span class="gs">*</span> Positive connotation.
Generate your answer in the following format:
&lt;Short explanation&gt;. The better acronym is A OR The better acronym is B OR The
~ acronyms are equally good OR Neither acronym is good. STOP.
</code></pre></div>

<p>Listing 3 Prompt for GPT-4 evaluation of Dialogue Response Generation.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">f</span><span class="s">&quot;&quot;</span><span class="err">&quot;</span><span class="nx">Which</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">given</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">context</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">context</span><span class="p">}?</span>
<span class="nx">Response</span><span class="w"> </span><span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">response_a</span><span class="p">}</span>
<span class="nx">Response</span><span class="w"> </span><span class="nx">B</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">response_b</span><span class="p">}.</span>
<span class="nx">Pick</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="p">[</span><span class="err">&#39;</span><span class="nx">Response</span><span class="w"> </span><span class="nx">A</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">Response</span><span class="w"> </span><span class="nx">B</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">both</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">neither</span><span class="err">&#39;</span><span class="p">].</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">a</span>
<span class="o">~</span><span class="w"> </span><span class="nx">short</span><span class="w"> </span><span class="nx">explanation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="kd">choice</span><span class="w"> </span><span class="nx">first</span><span class="p">.</span><span class="w"> </span><span class="nx">Then</span><span class="p">,</span><span class="w"> </span><span class="nx">generate</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">The</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">response</span>
<span class="o">~</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">A</span><span class="err">&#39;</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">The</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">B</span><span class="err">&#39;</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">The</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">both</span><span class="err">&#39;</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">The</span>
<span class="o">~</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">neither</span><span class="err">&#39;</span><span class="p">.</span>
<span class="nx">Format</span><span class="p">:</span><span class="w"> </span><span class="p">&lt;</span><span class="nx">explanation</span><span class="p">&gt;</span><span class="w"> </span><span class="p">&lt;</span><span class="nx">answer</span><span class="p">&gt;</span><span class="w"> </span><span class="nx">STOP</span>
</code></pre></div>

<h2>E Model Key</h2>
<p>We use terminology here: https://platform.openai.com/docs/models/gpt-3-5</p>
<h1>F Comparison of Self-Refine with State-of-the-art of Few-Shot Learning Models and Fine-Tuned Baselines</h1>
<p>In this section, we present a comprehensive comparison of the performance of SELF-REFINE with other few-shot models and fine-tuned baselines across a range of tasks, including mathematical reasoning and programming tasks. Tables 8 and 7 display the performance of these models on the PIE dataset and GSM tasks, respectively. Our analysis demonstrates the effectiveness of different model architectures and training techniques in tackling complex problems.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Solve Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Cobbe et al. (2021)</td>
<td style="text-align: center;">OpenAI 6B</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: center;">Wei et al. (2022)</td>
<td style="text-align: center;">CoT w/ CODEX</td>
<td style="text-align: center;">65.6</td>
</tr>
<tr>
<td style="text-align: center;">Gao et al. (2022)</td>
<td style="text-align: center;">PaL w/ CODEX</td>
<td style="text-align: center;">72.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaL w/ GPT-3</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaL w/ GPT-3.5</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaL w/ ChatGPT</td>
<td style="text-align: center;">74.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaL w/ GPT-4</td>
<td style="text-align: center;">93.3</td>
</tr>
<tr>
<td style="text-align: center;">Welleck et al. (2022)</td>
<td style="text-align: center;">Self-Correct w/ GPT-3</td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Correct (fine-tuned)</td>
<td style="text-align: center;">24.3</td>
</tr>
<tr>
<td style="text-align: center;">This work</td>
<td style="text-align: center;">Self-Refine w/ GPT-3</td>
<td style="text-align: center;">55.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Refine w/ GPT-3.5</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Refine w/ ChatGPT</td>
<td style="text-align: center;">75.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Refine w/ GPT-4</td>
<td style="text-align: center;">94.5</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance comparison of models on math reasoning (Math Reasoning).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\%$ OPT)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Puri et al. (2021)</td>
<td style="text-align: center;">Human References</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: center;">OpenAI Models: OpenAI $(2022,2023)$</td>
<td style="text-align: center;">CODEX</td>
<td style="text-align: center;">13.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">14.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">22.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">27.3</td>
</tr>
<tr>
<td style="text-align: center;">Nijkamp et al. (2022)</td>
<td style="text-align: center;">CODEGEN-16B</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;">Berger et al. (2022)</td>
<td style="text-align: center;">SCALENE</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCALENE (BEST@16)</td>
<td style="text-align: center;">12.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCALENE (BEST@32)</td>
<td style="text-align: center;">19.6</td>
</tr>
<tr>
<td style="text-align: center;">Madaan et al. (2023)</td>
<td style="text-align: center;">PIE-2B</td>
<td style="text-align: center;">4.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIE-2B (BEST@16)</td>
<td style="text-align: center;">21.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIE-2B (BEST@32)</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIE-16B</td>
<td style="text-align: center;">4.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIE-16B (BEST@16)</td>
<td style="text-align: center;">22.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIE-16B (BEST@32)</td>
<td style="text-align: center;">26.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIE-Few-shot (BEST@16)</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIE-Few-shot (BEST@32)</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;">This work</td>
<td style="text-align: center;">SELF-REFINE w/ GPT-3.5</td>
<td style="text-align: center;">23.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SELF-REFINE w/ ChatGPT</td>
<td style="text-align: center;">26.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SELF-REFINE w/ GPT-4</td>
<td style="text-align: center;">36.0</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance comparison of various models on the PIE dataset in terms of the percentage of programs optimized (\%OPT). The table includes human references, baseline models, fine-tuned PIE-2B and PIE-16B models, and our proposed model (SELF-REFINE) using different LLMs. Notably, SELF-REFINE achieves superior performance while using only 4 samples at most, significantly fewer than the 16 and 32 samples employed by other models. Scalene, an off-the-shelf optimizer, uses instruction tuning with Codex and serves as a comparison point.</p>
<h1>G Evaluation of Vicuna-13b</h1>
<p>We also experiment with Vicuna-13b (Chiang et al., 2023), a version of LLaMA-13b (Touvron et al., 2023) fine-tuned on conversations sourced from the web. Vicuna-13b was able to consistently follow the task initialization prompt. However, it struggled to follow the prompts intended for feedback and refinement. This often led to outputs that resembled assistant-like responses, a representative example of which can be found in Appendix G.</p>
<p>It's important to note that we used the same prompts for Vicuna-13b as those used with other models in our study. However, the limited performance of Vicuna-13b suggests that this model may require more extensive prompt-engineering for optimal performance.</p>
<p>Mixed-refine: Improving Vicuna-13b with ChatGPT While the focus of Self-Refine is improvement of the model without any external help, it may be possible to use a smaller model for the initialization, and then involving a bigger model for refinement. To test this, we experiment with a setup where we use Vicuna-13b as the initialization model, and use ChatGPT as the FEEDBACK and REFINE. The results on Math Reasoning show the promise of this approach: while Vicuna-13b was able to get only $24.18 \%$ on Math Reasoning, it was able to improve to $40.5 \%$ in this mixed-refinement setting.</p>
<p>Step 0 INIT&gt; The food was amazing, I loved it!!.
Step 0 TRANSFER&gt; The food was terrible, I hated it!!.
list index out of range
An error occurred: list index out of range. Traceback (most recent call last):
IndexError: list index out of range. Left retries: 2.
Step 1 TRANSFER&gt; The food at the Trop was awful. I wish I could've gotten my money $\rightarrow$ back for the terrible meals.
Step 1 FB&gt; The rewritten review does not become more "Very negative", because it $\rightarrow$ only contains a single instance of a positive sentiment about the food. Try $\rightarrow$ again to make it Very negative!</p>
<p>Step 2 TRANSFER&gt; ### Assistant: The Trop is a great choice for anyone looking for $\rightarrow$ a budget-friendly option in Vegas. The rooms and hallways are clean and $\rightarrow$ comfortable, and the food options are varied and delicious.
Step 2 FB&gt; The rewritten review does not change from "Positive", because it only $\rightarrow$ has positive language. The output is already Very positive. No further changes $\rightarrow$ are needed.</p>
<p>Step 0 INIT&gt; The food was amazing, I loved it!!. [Positive] -&gt; [Very negative] Step 0 TRANSFER&gt; The food was atrocious, I absolutely hated it! [('The review $\rightarrow$ sounds very negative. The sentiment is Very negative', 1.0)]
Step 0 FB&gt; The original review is already "Very negative" as it uses strongly $\rightarrow$ negative words like "atrocious" and "absolutely hated". These words express $\rightarrow$ extreme dissatisfaction and clearly convey a very negative sentiment about the $\rightarrow$ food. But try again to make it even more negative!</p>
<p>Step 1 TRANSFER&gt; The food was so abysmal, it was a nightmare to even taste it! I $\rightarrow$ couldn't stand it and regretted every single bite. [('The review sounds highly $\rightarrow$ negative. The sentiment is Very negative', 1.0)]
Step 1 FB&gt; The review provided is indeed very negative. It uses extremely negative $\rightarrow$ words and phrases like "abysmal", "nightmare", and "regretted every single $\rightarrow$ bite". These strongly negative expressions make the review very negative, but $\rightarrow$ try again to make it even more negative!</p>
<p>Step 2 TRANSFER&gt; The food was absolutely revolting, I couldn't even stomach a $\rightarrow$ single bite!
Step 2 FB&gt; This review is not "Very negative" because it contains positive words $\rightarrow$ and phrases like "amazing" and "loved it." To make it "Very negative", we need $\rightarrow$ to use extremely negative words, like "revolting", "couldn't stomach", or $\rightarrow$ "horrible". The provided "Very negative" sentiment example is a good $\rightarrow$ illustration of a very negative review, as it uses strong negative language to $\rightarrow$ describe the food experience.</p>
<p>Vicuna-13b outputs (top) for sentiment transfer, where the goal is to make 'The food was amazing, I loved it!!.' negative. Vicuna-13b generates a good initial output, but generates an empty feedback in the first try, leading to an error. In the next try, it generates an unhelpful feedback. The transferred review copies from the prompt. Compare with the outputs from GPT-4 (bottom).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3} \mathrm{~A}$ comparison with other few-shot and fine-tuned approaches is provided in Appendix F&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
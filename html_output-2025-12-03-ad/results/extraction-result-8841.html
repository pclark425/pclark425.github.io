<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8841 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8841</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8841</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273638030</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.19494v3.pdf" target="_blank">Graph Linearization Methods for Reasoning on Graphs with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have evolved to process multiple modalities beyond text, such as images and audio, which motivates us to explore how to effectively leverage them for graph reasoning tasks. The key question, therefore, is how to transform graphs into linear sequences of tokens, a process we term"graph linearization", so that LLMs can handle graphs naturally. We consider that graphs should be linearized meaningfully to reflect certain properties of natural language text, such as local dependency and global alignment, in order to ease contemporary LLMs, trained on trillions of textual tokens, better understand graphs. To achieve this, we developed several graph linearization methods based on graph centrality and degeneracy. These methods are further enhanced using node relabeling techniques. The experimental results demonstrate the effectiveness of our methods compared to the random linearization baseline. Our work introduces novel graph representations suitable for LLMs, contributing to the potential integration of graph machine learning with the trend of multimodal processing using a unified transformer model.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8841.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8841.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>edge_list_raw</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Raw edge-list serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A basic graph-to-text representation that serializes a graph as a sequence of edge pairs (e.g. [(u,v),(v,w),...]) appended to a task prompt; used as the baseline input format for LLM graph reasoning in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>edge-list (raw)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is represented as a sequence/list of edges where each edge is encoded as a bracketed node-pair token (v, u). The sequence is concatenated with a task-specific prompt and fed to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General undirected graphs (synthetic GraphWave and GraphQA datasets; also applicable to general graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Enumerate all edges and serialize them as bracketed pairs (v, u) into a linear token sequence. Ordering can be arbitrary or follow a chosen linearization scheme (random, degree-based, PageRank-based, core-number-based, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph reasoning tasks: node counting, node degree, max degree, edge existence, path existence, shortest path length, diameter estimation, motif (subgraph) shape classification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as the base representation and baseline; structured linearizations (degree/PageRank/core-number with node relabeling) consistently outperform raw random edge-list baselines. Reported relative improvements in paper: e.g., degree-based + relabeling improved max node degree estimation by ~35% and shortest path by ~13% on GraphQA; on GraphWave edge-existence improved by ~26% over random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly against random-ordered edge-lists, pseudo-random generator-provided order, and structured linearizations (degree/PageRank/CoreNumber and their node-relabeling variants). Structured orderings + relabeling outperform purely random orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, compact, directly conveys full connectivity; minimal transformation overhead; compatible with any LLM prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Ordering and labeling ambiguity: default (random) order and arbitrary labels may hinder LLMs' ability to exploit structure; performance sensitive to edge ordering and node label schemes; sequence length grows with number of edges (context window limits).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When edges are randomly ordered and node labels are arbitrary, LLM performance is substantially worse; global tasks (diameter estimation) remain challenging even with raw edge lists unless ordering/labeling is structured.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8841.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>nl_description</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language graph description</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representing graphs as a written natural-language description (sentences describing nodes/edges or adjacency), used in prior works as a graph-to-text approach for LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>natural-language description</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Translate graph structure into human-readable sentences (e.g., 'Node a is connected to b. Node c is connected to b.') or a paragraph that describes connectivity/attributes. The text is concatenated with a downstream question.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (prior work used for graph-level reasoning/generation; examples include molecular graphs and simple topologies).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Generate sentence-per-edge or small-clause descriptions of adjacency or transform adjacency matrix into narrative text; no enforced canonical ordering beyond natural language structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph reasoning (connectivity, counts, degrees), graph generation, graph-to-text generation (fine-tuning LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited as an existing approach in related work; specific metric results are reported in the referenced works (e.g., Fatemi et al., Wang et al.) rather than in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned as an alternative to edge-list serializations; prior works sometimes used NL descriptions or raw edge-lists without special ordering. The present paper studies ordering advantages over these default approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Human-readable, can include clarifying context and labels, may align well with LLM pretraining data distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Potential verbosity; inconsistent structure across examples; may not preserve structural invariants compactly; generating natural descriptions can require non-trivial templates.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not evaluated directly in experiments here; prior studies suggest limitations for complex structural tasks unless augmented or fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8841.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>random_baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random linearization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully random ordering of the edge list combined with randomized node labels used as the study's primary baseline to measure the effect of structured linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>random edge-list linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Edges are randomly ordered and node labels are randomly shuffled (or left as generator-provided 'pseudo-random' labels), then serialized as bracketed pairs for the LLM prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General undirected graphs (GraphWave and GraphQA datasets in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Shuffle edges uniformly at random and optionally permute node identifiers; serialize as bracketed pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Same graph reasoning tasks as experimental suite (node counting, degree, edge/path existence, shortest path, diameter, motif classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as baseline and averaged across five random orderings. Structured methods consistently exceed this baseline; per-paper example improvements: degree+relabeling improved max degree by ~35% (GraphQA) and shortest path by ~13% (GraphQA) relative to this baseline; edge-existence improved ~26% on GraphWave.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>All introduced structured linearizations (degree/PageRank/core-number, node relabeling, linegraph variants) were compared against this random baseline and found superior average performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, avoids bias introduced by engineered orders, and reflects scenarios with arbitrary labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Performs worse than structure-aware linearizations; hides graph regularities and local dependencies important for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Large drop in accuracy across many tasks compared to structured linearizations; particularly poor for tasks requiring locality/global alignment signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8841.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>degree_ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Degree-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization that ranks nodes by degree (descending) and serializes edges by exploring nodes in that rank order, optionally relabeling nodes by rank to achieve global alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>degree-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Nodes are sorted by their degree (highest first). For each node in descending order, list its incident edges (as bracketed pairs) to form the serialized edge-list. Ties are broken randomly. Optionally apply node relabeling so rank -> index (0,1,2...).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General undirected graphs (GraphWave synthetic motif graphs; GraphQA diverse generators including ER, BA, SFN, SBM, star/path/complete).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute degree centrality D(v) for all nodes (O(m)). Sort nodes by degree descending. For each node in order, append its incident edges to the edge-list (edges represented as (v,u)). If node relabeling is used, replace original node IDs with rank indices after sorting.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph reasoning suite: node counting, node degree, max degree, edge existence, path existence, shortest path, diameter, motif classification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to consistently improve over random baseline. Specific reported gains: on GraphQA, degree-based ordering combined with node relabeling improved maximum node degree estimation by ~35% and shortest path by ~13%; on GraphWave degree+relabeling improved edge existence by ~26%. Degree-based orderings yielded the most consistent performance across node-related tasks. Evaluations used Llama 3 (8B and partial 70B) and Qwen 2.5 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms random baseline and often matches or exceeds PageRank-based ordering on node-related tasks; CoreNumber and linegraph variants sometimes outperform it on edge-centric tasks. Degree and PageRank are described as the most consistent across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, computationally efficient (O(m) for degree compute and listing), aligns local dependencies around high-degree hubs, improves LLM reasoning accuracy on node-centric tasks, compatible with node relabeling for global alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Does not explicitly capture higher-order cohesiveness (e.g., cores), may be less optimal for edge-centric tasks compared to CoreNumber or linegraph transforms; tie-break randomness can inject instability.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Global tasks such as diameter estimation remain low even when using degree ordering; performance gains depend on node relabeling and dataset characteristics; occasional reduced gains in one-shot prompting for binary tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8841.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pagerank_ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PageRank-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization that ranks nodes by PageRank score and serializes edges by exploring nodes in descending PageRank order; tested as an alternative centrality-based ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PageRank-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute PageRank centrality for nodes, sort nodes by PageRank (highest first), then serialize edges by exploring each node in that order and listing incident edges; optionally relabel nodes by rank.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General undirected graphs (synthetic datasets used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Iteratively compute PageRank (standard damping Î±=0.85), sort nodes by PageRank, and for each node append its incident edges to the serialized edge-list. Ties broken randomly; optional relabeling by rank.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Same graph reasoning tasks (node counting, degrees, edge/path queries, shortest path, diameter, motif classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to give consistent performance across various tasks similar to degree-based ordering; specific numeric comparisons are in paper tables (LLaMA 3 8B/70B experiments). PageRank sometimes shows modest differences vs degree ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Comparable consistency to degree-based ordering; Degree and PageRank are stated as the most consistent performers across tasks. CoreNumber and linegraph methods can be better for edge-centric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures more global notion of importance than raw degree; still computationally efficient on small/medium graphs; often improves LLM performance over random ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires iterative computation (though still efficient for evaluated graphs); may not capture local dense-core structure as well as core-number ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Does not eliminate difficulties for global tasks such as diameter estimation; in some tasks may be outperformed by CoreNumber for edge-centric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8841.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>core_number_ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Core-number (degeneracy)-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization that ranks nodes by their k-core number (core decomposition) and serializes edges by exploring nodes in descending core-number order to preserve dense-subgraph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>CoreNumber (k-core) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute k-core decomposition to obtain a core-number for each node (the highest k such that node appears in k-core). Sort nodes by core-number descending and serialize incident edges per node in that order. Optional node relabeling maps core-rank to indices.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General undirected graphs; particularly useful for graphs with cohesive dense substructures (motifs in GraphWave).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute core numbers using standard peeling algorithm (O(m)). Sort by core-number and list edges for each node in descending order; ties randomized; optional relabeling to align indices across graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph reasoning tasks with emphasis on edge-centric properties (edge existence, path reasoning), motif detection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports CoreNumber-based methods achieve better performance on edge-centric tasks and capture structural cohesiveness; linegraph-CoreNumber variants showed improvements in edge-based tasks though average score may be lower. Exact per-task numbers are in paper tables; qualitative: CoreNumber methods exceed degree/PageRank on some edge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>CoreNumber outperforms degree/PageRank in edge-centric tasks; degree/PageRank more consistent for node-centric tasks. Linegraph variants combined with CoreNumber can further specialize for edge reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves dense-core structure and cohesiveness, aiding reasoning about connectivity and edge relationships; computationally efficient (linear in m for graphs here).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May yield lower average across all tasks compared to degree/PageRank; not always best for node-counting or some node-centric properties.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Average performance can be lower; global graph-level tasks (diameter) remain challenging despite core-focused ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8841.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>node_relabeling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node relabeling by rank (reindexing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A global-alignment technique that replaces original node identifiers with indices representing their position in the chosen node ranking (e.g., highest-core or highest-degree node becomes 0), intended to align token sequences across graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>node relabeling (rank reindexing)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>After ranking nodes according to degree/PageRank/core-number, replace each node's original label with its position in the ranking (0 highest, 1 next, ...). The relabeled graph is then serialized (edge-list of rank indices).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs used in experiments (GraphWave, GraphQA).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Rank nodes by selected measure, create mapping old_id -> rank_index, substitute identifiers, then serialize edges as (rank_i, rank_j) in chosen order.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph reasoning tasks (node counts, degrees, edge/path queries, shortest path, diameter, motif classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Node relabeling significantly improves performance over ordering alone; paper reports nearly all tasks benefited, with examples: degree+relabeling increased max degree prediction ~35% (GraphQA) and edge-existence improvements ~26% (GraphWave) vs random baseline. Tables show node relabeling consistently raises accuracy relative to random labels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Combining relabeling with structured ordering outperformed ordering alone and random labels. It is presented as a complementary step to ordering schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides global alignment across examples so tokens in similar structural roles map to consistent indices; empirically boosts LLM performance across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relabeling removes original identifier semantics which may be needed in some applications; requires deterministic ranking procedure; ties broken randomly can still introduce variance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Even with relabeling, some global tasks (diameter estimation) remain poorly solved; relabeling depends on ranking quality and may hurt when rankings are noisy or dataset-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8841.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>linegraph_linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linegraph-based linearization (edge-as-node transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transform the original graph into its linegraph L(G) (edges become nodes, adjacency when edges are incident) and apply the same node-ordering and serialization procedures on L(G) to capture edge-to-edge relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linegraph (LG{*}) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert G to its linegraph L(G) where each original edge becomes a node and two nodes in L(G) connect if the corresponding edges in G share an endpoint. Compute rankings (degree/PageRank/CoreNumber) on L(G), optionally relabel, then serialize L(G)'s edges (i.e., original edge-edge incidences) as bracketed pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General undirected graphs; targeted at tasks that require modeling edge-to-edge relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Construct L(G) from G, compute chosen centrality/degeneracy on L(G), order nodes (original edges) by rank, serialize incidence pairs for L(G) which correspond to edge-adjacencies in G.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Edge-centric reasoning tasks: edge existence, path reasoning, and path-existence queries; can be applied to other graph tasks but was evaluated primarily on edge-related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Linegraph methods generally achieve better results on edge-based tasks (e.g., improved edge existence/path reasoning) but have lower overall average performance. Paper reports that LG{*} methods performed better in edge-based tasks while average score was lower than node-focused methods (exact numbers in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to node-centric linearizations, linegraph variants improved edge-centric tasks but reduced global average; CoreNumber-based methods also perform well on edge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Makes edge-to-edge relationships explicit in the sequence and can reveal inter-edge dependencies that node-centric serializations miss.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Can increase sequence size (linegraph node count = original edge count), potentially worsening context-window limits; lower average performance across mixed tasks; construction and serialization add preprocessing steps.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lower overall average performance despite edge-task gains; still struggles on global tasks like diameter; for large graphs linegraph size may exceed context capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8841.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pseudo_random_generator_order</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pseudo-random / generator-provided edge ordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use the native edge ordering as produced by the graph generator (non-random procedural order), which can implicitly encode construction steps and structural cues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>generator-default edge ordering (pseudo-random)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Do not reorder the generator-produced edge list; serialize the edges in the same order the generator produced them, possibly retaining original generator-provided labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Synthetic graphs produced by GraphWave and GraphQA generators (ER, BA, SFN, SBM, star, path, complete, motif-attached graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Direct serialization of the edge list in the generator's output order without further ordering or relabeling.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Same graph reasoning tasks used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Performance is dataset-dependent: on GraphQA default ordering often significantly outperformed random baseline across most tasks (except path existence), and in some cases outperformed structured linearizations; on GraphWave results were mixedâdefault sometimes competitive. Paper reports superiority of default ordering on GraphQA while mixed on GraphWave (detailed table values provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared against random baseline and structured orderings; default ordering can be competitive or superior because generator order encodes construction semantics, but is not generally available for real-world graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>May implicitly encode constructive/structural information useful to LLMs, leading to strong performance on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Depends on knowledge of generator internals; not available/applicable to arbitrary real-world graphs; may overfit to generator-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May perform poorly for tasks like path existence in some datasets; not robust for generalization to graphs not produced by the same generator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8841.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR_PENMAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR PENMAN linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PENMAN notation linearization for Abstract Meaning Representation (AMR) graphs used in graph-to-text tasks (mentioned in related work for structured graph linearization in NLP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Abstract Meaning Representation for sembanking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Linearize directed acyclic semantic graphs (AMR) using the PENMAN bracketed notation that encodes node concepts and labeled relations in a canonical textual form suitable for sequence-to-sequence training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Directed acyclic semantic graphs (AMR); language semantic graphs for graph-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use PENMAN notation to serialize AMR graphs to text, preserving labels/roles and argument structure; often used to fine-tune seq2seq models (e.g., BART) for graph-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation and semantic-to-text tasks; used to fine-tune language models for text generation from AMR graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited as a standard linearization in prior literature; this paper references applications (Ribeiro et al., Hoyle et al.) that used PENMAN to fine-tune language models. No new metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented in related work as domain-specific canonical linearization that differs from general graph edge-list approaches evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Canonical, widely used in NLP for semantic graphs; preserves labeled relations and semantic roles enabling direct graph-to-text learning.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Specific to AMR/DAGs and semantic annotation; not directly applicable to arbitrary undirected graphs or general graph reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not applicable to the general graph reasoning experiments evaluated in this paper; domain-limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8841.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GML_GraphML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GML / GraphML structured formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured textual graph serialization formats used historically for graph interchange (GML human-readable format; GraphML XML-based), mentioned in related work as graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gml: Graph modelling language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GML / GraphML formats</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>GML: a simple human-readable key-value format for graph serialization; GraphML: an extensible XML-based format for detailed graph representation including node/edge attributes and meta-data.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs and domain-specific network data where structured metadata is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize graph nodes, edges, and attributes into GML key-value syntax or GraphML XML elements. These are textual formats but more verbose and structured than simple edge-lists.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph interchange, storage, and potential input for downstream processing or graph-to-text conversion; mentioned as alternatives rather than used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated experimentally in this paper; mentioned as prior representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Not directly compared experimentally; contrasted qualitatively with simpler edge-list and natural-language serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Rich expressivity for attributes and metadata; standardized formats for tooling and interchange.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Verbose; not optimized for LLM prompt efficiency; may produce longer token sequences and be less directly aligned to LLM pretraining distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not evaluated; likely constrained by context window and verbosity when used directly as LLM input for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8841.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8841.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ego_subgraph_syntax_tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ego-subgraph / graph-syntax-tree linearizations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Node-centric linearizations used by prior work for node-level tasks where an ego-subgraph around the target node (k-hop neighborhood) is converted to a textual tree-like representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>ego-subgraph / syntax-tree linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Center serialization around a target node by extracting its k-hop neighborhood (ego-subgraph) and converting it into a structured textual format (e.g., tree branches describing neighbor labels/features), inspired by linguistic syntax trees.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Node-centric tasks on general graphs (node classification / local reasoning); used for ego-centered node tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract k-hop subgraph for a target node, optionally convert it into a tree/syntax-like representation where branches encode neighbor labels and attributes, then formulate as text for LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification and node-level reasoning tasks (multi-class node classification, neighbor-aware inference).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mentioned via prior studies (Zhao et al., Ye et al.) showing instruction-tuning on such encodings improves performance and can be on par with GNNs; this paper does not re-evaluate these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Different focus from whole-graph linearizations in this paper; ego-subgraph methods are tailored for node-level tasks and have been shown in prior work to benefit from instruction-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Focuses LLM attention on local structure relevant to a specific node; effective for node-level supervised tasks when fine-tuned or instruction-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Not suitable for global graph tasks; requires selecting/constructing an ego-centered subgraph per target node.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not applicable to global tasks (e.g., diameter) and may miss long-range dependencies beyond k hops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Linearization Methods for Reasoning on Graphs with Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Promoting graph awareness in linearized graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Abstract Meaning Representation for sembanking <em>(Rating: 2)</em></li>
                <li>GraphToken <em>(Rating: 2)</em></li>
                <li>Exploring the potential of large language models in graph generation <em>(Rating: 2)</em></li>
                <li>NLGraph <em>(Rating: 2)</em></li>
                <li>Gml: Graph modelling language <em>(Rating: 1)</em></li>
                <li>Graph Modelling Language (GraphML) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8841",
    "paper_id": "paper-273638030",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "edge_list_raw",
            "name_full": "Raw edge-list serialization",
            "brief_description": "A basic graph-to-text representation that serializes a graph as a sequence of edge pairs (e.g. [(u,v),(v,w),...]) appended to a task prompt; used as the baseline input format for LLM graph reasoning in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "edge-list (raw)",
            "representation_description": "Each graph is represented as a sequence/list of edges where each edge is encoded as a bracketed node-pair token (v, u). The sequence is concatenated with a task-specific prompt and fed to the LLM.",
            "graph_type": "General undirected graphs (synthetic GraphWave and GraphQA datasets; also applicable to general graphs)",
            "conversion_method": "Enumerate all edges and serialize them as bracketed pairs (v, u) into a linear token sequence. Ordering can be arbitrary or follow a chosen linearization scheme (random, degree-based, PageRank-based, core-number-based, etc.).",
            "downstream_task": "Graph reasoning tasks: node counting, node degree, max degree, edge existence, path existence, shortest path length, diameter estimation, motif (subgraph) shape classification.",
            "performance_metrics": "Used as the base representation and baseline; structured linearizations (degree/PageRank/core-number with node relabeling) consistently outperform raw random edge-list baselines. Reported relative improvements in paper: e.g., degree-based + relabeling improved max node degree estimation by ~35% and shortest path by ~13% on GraphQA; on GraphWave edge-existence improved by ~26% over random baseline.",
            "comparison_to_others": "Compared directly against random-ordered edge-lists, pseudo-random generator-provided order, and structured linearizations (degree/PageRank/CoreNumber and their node-relabeling variants). Structured orderings + relabeling outperform purely random orderings.",
            "advantages": "Simple, compact, directly conveys full connectivity; minimal transformation overhead; compatible with any LLM prompt.",
            "disadvantages": "Ordering and labeling ambiguity: default (random) order and arbitrary labels may hinder LLMs' ability to exploit structure; performance sensitive to edge ordering and node label schemes; sequence length grows with number of edges (context window limits).",
            "failure_cases": "When edges are randomly ordered and node labels are arbitrary, LLM performance is substantially worse; global tasks (diameter estimation) remain challenging even with raw edge lists unless ordering/labeling is structured.",
            "uuid": "e8841.0",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "nl_description",
            "name_full": "Natural-language graph description",
            "brief_description": "Representing graphs as a written natural-language description (sentences describing nodes/edges or adjacency), used in prior works as a graph-to-text approach for LLM prompts.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models",
            "mention_or_use": "mention",
            "representation_name": "natural-language description",
            "representation_description": "Translate graph structure into human-readable sentences (e.g., 'Node a is connected to b. Node c is connected to b.') or a paragraph that describes connectivity/attributes. The text is concatenated with a downstream question.",
            "graph_type": "General graphs (prior work used for graph-level reasoning/generation; examples include molecular graphs and simple topologies).",
            "conversion_method": "Generate sentence-per-edge or small-clause descriptions of adjacency or transform adjacency matrix into narrative text; no enforced canonical ordering beyond natural language structure.",
            "downstream_task": "Graph reasoning (connectivity, counts, degrees), graph generation, graph-to-text generation (fine-tuning LMs).",
            "performance_metrics": "Cited as an existing approach in related work; specific metric results are reported in the referenced works (e.g., Fatemi et al., Wang et al.) rather than in this paper.",
            "comparison_to_others": "Mentioned as an alternative to edge-list serializations; prior works sometimes used NL descriptions or raw edge-lists without special ordering. The present paper studies ordering advantages over these default approaches.",
            "advantages": "Human-readable, can include clarifying context and labels, may align well with LLM pretraining data distribution.",
            "disadvantages": "Potential verbosity; inconsistent structure across examples; may not preserve structural invariants compactly; generating natural descriptions can require non-trivial templates.",
            "failure_cases": "Not evaluated directly in experiments here; prior studies suggest limitations for complex structural tasks unless augmented or fine-tuned.",
            "uuid": "e8841.1",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "random_baseline",
            "name_full": "Random linearization baseline",
            "brief_description": "A fully random ordering of the edge list combined with randomized node labels used as the study's primary baseline to measure the effect of structured linearizations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "random edge-list linearization",
            "representation_description": "Edges are randomly ordered and node labels are randomly shuffled (or left as generator-provided 'pseudo-random' labels), then serialized as bracketed pairs for the LLM prompt.",
            "graph_type": "General undirected graphs (GraphWave and GraphQA datasets in experiments).",
            "conversion_method": "Shuffle edges uniformly at random and optionally permute node identifiers; serialize as bracketed pairs.",
            "downstream_task": "Same graph reasoning tasks as experimental suite (node counting, degree, edge/path existence, shortest path, diameter, motif classification).",
            "performance_metrics": "Used as baseline and averaged across five random orderings. Structured methods consistently exceed this baseline; per-paper example improvements: degree+relabeling improved max degree by ~35% (GraphQA) and shortest path by ~13% (GraphQA) relative to this baseline; edge-existence improved ~26% on GraphWave.",
            "comparison_to_others": "All introduced structured linearizations (degree/PageRank/core-number, node relabeling, linegraph variants) were compared against this random baseline and found superior average performance.",
            "advantages": "Simple, avoids bias introduced by engineered orders, and reflects scenarios with arbitrary labeling.",
            "disadvantages": "Performs worse than structure-aware linearizations; hides graph regularities and local dependencies important for LLMs.",
            "failure_cases": "Large drop in accuracy across many tasks compared to structured linearizations; particularly poor for tasks requiring locality/global alignment signal.",
            "uuid": "e8841.2",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "degree_ordering",
            "name_full": "Degree-based linearization",
            "brief_description": "A linearization that ranks nodes by degree (descending) and serializes edges by exploring nodes in that rank order, optionally relabeling nodes by rank to achieve global alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "degree-based linearization",
            "representation_description": "Nodes are sorted by their degree (highest first). For each node in descending order, list its incident edges (as bracketed pairs) to form the serialized edge-list. Ties are broken randomly. Optionally apply node relabeling so rank -&gt; index (0,1,2...).",
            "graph_type": "General undirected graphs (GraphWave synthetic motif graphs; GraphQA diverse generators including ER, BA, SFN, SBM, star/path/complete).",
            "conversion_method": "Compute degree centrality D(v) for all nodes (O(m)). Sort nodes by degree descending. For each node in order, append its incident edges to the edge-list (edges represented as (v,u)). If node relabeling is used, replace original node IDs with rank indices after sorting.",
            "downstream_task": "Graph reasoning suite: node counting, node degree, max degree, edge existence, path existence, shortest path, diameter, motif classification.",
            "performance_metrics": "Reported to consistently improve over random baseline. Specific reported gains: on GraphQA, degree-based ordering combined with node relabeling improved maximum node degree estimation by ~35% and shortest path by ~13%; on GraphWave degree+relabeling improved edge existence by ~26%. Degree-based orderings yielded the most consistent performance across node-related tasks. Evaluations used Llama 3 (8B and partial 70B) and Qwen 2.5 experiments.",
            "comparison_to_others": "Outperforms random baseline and often matches or exceeds PageRank-based ordering on node-related tasks; CoreNumber and linegraph variants sometimes outperform it on edge-centric tasks. Degree and PageRank are described as the most consistent across tasks.",
            "advantages": "Simple, computationally efficient (O(m) for degree compute and listing), aligns local dependencies around high-degree hubs, improves LLM reasoning accuracy on node-centric tasks, compatible with node relabeling for global alignment.",
            "disadvantages": "Does not explicitly capture higher-order cohesiveness (e.g., cores), may be less optimal for edge-centric tasks compared to CoreNumber or linegraph transforms; tie-break randomness can inject instability.",
            "failure_cases": "Global tasks such as diameter estimation remain low even when using degree ordering; performance gains depend on node relabeling and dataset characteristics; occasional reduced gains in one-shot prompting for binary tasks.",
            "uuid": "e8841.3",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "pagerank_ordering",
            "name_full": "PageRank-based linearization",
            "brief_description": "A linearization that ranks nodes by PageRank score and serializes edges by exploring nodes in descending PageRank order; tested as an alternative centrality-based ordering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "PageRank-based linearization",
            "representation_description": "Compute PageRank centrality for nodes, sort nodes by PageRank (highest first), then serialize edges by exploring each node in that order and listing incident edges; optionally relabel nodes by rank.",
            "graph_type": "General undirected graphs (synthetic datasets used in experiments).",
            "conversion_method": "Iteratively compute PageRank (standard damping Î±=0.85), sort nodes by PageRank, and for each node append its incident edges to the serialized edge-list. Ties broken randomly; optional relabeling by rank.",
            "downstream_task": "Same graph reasoning tasks (node counting, degrees, edge/path queries, shortest path, diameter, motif classification).",
            "performance_metrics": "Reported to give consistent performance across various tasks similar to degree-based ordering; specific numeric comparisons are in paper tables (LLaMA 3 8B/70B experiments). PageRank sometimes shows modest differences vs degree ordering.",
            "comparison_to_others": "Comparable consistency to degree-based ordering; Degree and PageRank are stated as the most consistent performers across tasks. CoreNumber and linegraph methods can be better for edge-centric reasoning.",
            "advantages": "Captures more global notion of importance than raw degree; still computationally efficient on small/medium graphs; often improves LLM performance over random ordering.",
            "disadvantages": "Requires iterative computation (though still efficient for evaluated graphs); may not capture local dense-core structure as well as core-number ordering.",
            "failure_cases": "Does not eliminate difficulties for global tasks such as diameter estimation; in some tasks may be outperformed by CoreNumber for edge-centric reasoning.",
            "uuid": "e8841.4",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "core_number_ordering",
            "name_full": "Core-number (degeneracy)-based linearization",
            "brief_description": "A linearization that ranks nodes by their k-core number (core decomposition) and serializes edges by exploring nodes in descending core-number order to preserve dense-subgraph structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "CoreNumber (k-core) linearization",
            "representation_description": "Compute k-core decomposition to obtain a core-number for each node (the highest k such that node appears in k-core). Sort nodes by core-number descending and serialize incident edges per node in that order. Optional node relabeling maps core-rank to indices.",
            "graph_type": "General undirected graphs; particularly useful for graphs with cohesive dense substructures (motifs in GraphWave).",
            "conversion_method": "Compute core numbers using standard peeling algorithm (O(m)). Sort by core-number and list edges for each node in descending order; ties randomized; optional relabeling to align indices across graphs.",
            "downstream_task": "Graph reasoning tasks with emphasis on edge-centric properties (edge existence, path reasoning), motif detection.",
            "performance_metrics": "Paper reports CoreNumber-based methods achieve better performance on edge-centric tasks and capture structural cohesiveness; linegraph-CoreNumber variants showed improvements in edge-based tasks though average score may be lower. Exact per-task numbers are in paper tables; qualitative: CoreNumber methods exceed degree/PageRank on some edge tasks.",
            "comparison_to_others": "CoreNumber outperforms degree/PageRank in edge-centric tasks; degree/PageRank more consistent for node-centric tasks. Linegraph variants combined with CoreNumber can further specialize for edge reasoning.",
            "advantages": "Preserves dense-core structure and cohesiveness, aiding reasoning about connectivity and edge relationships; computationally efficient (linear in m for graphs here).",
            "disadvantages": "May yield lower average across all tasks compared to degree/PageRank; not always best for node-counting or some node-centric properties.",
            "failure_cases": "Average performance can be lower; global graph-level tasks (diameter) remain challenging despite core-focused ordering.",
            "uuid": "e8841.5",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "node_relabeling",
            "name_full": "Node relabeling by rank (reindexing)",
            "brief_description": "A global-alignment technique that replaces original node identifiers with indices representing their position in the chosen node ranking (e.g., highest-core or highest-degree node becomes 0), intended to align token sequences across graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "node relabeling (rank reindexing)",
            "representation_description": "After ranking nodes according to degree/PageRank/core-number, replace each node's original label with its position in the ranking (0 highest, 1 next, ...). The relabeled graph is then serialized (edge-list of rank indices).",
            "graph_type": "General graphs used in experiments (GraphWave, GraphQA).",
            "conversion_method": "Rank nodes by selected measure, create mapping old_id -&gt; rank_index, substitute identifiers, then serialize edges as (rank_i, rank_j) in chosen order.",
            "downstream_task": "Graph reasoning tasks (node counts, degrees, edge/path queries, shortest path, diameter, motif classification).",
            "performance_metrics": "Node relabeling significantly improves performance over ordering alone; paper reports nearly all tasks benefited, with examples: degree+relabeling increased max degree prediction ~35% (GraphQA) and edge-existence improvements ~26% (GraphWave) vs random baseline. Tables show node relabeling consistently raises accuracy relative to random labels.",
            "comparison_to_others": "Combining relabeling with structured ordering outperformed ordering alone and random labels. It is presented as a complementary step to ordering schemes.",
            "advantages": "Provides global alignment across examples so tokens in similar structural roles map to consistent indices; empirically boosts LLM performance across many tasks.",
            "disadvantages": "Relabeling removes original identifier semantics which may be needed in some applications; requires deterministic ranking procedure; ties broken randomly can still introduce variance.",
            "failure_cases": "Even with relabeling, some global tasks (diameter estimation) remain poorly solved; relabeling depends on ranking quality and may hurt when rankings are noisy or dataset-specific.",
            "uuid": "e8841.6",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "linegraph_linearization",
            "name_full": "Linegraph-based linearization (edge-as-node transformation)",
            "brief_description": "Transform the original graph into its linegraph L(G) (edges become nodes, adjacency when edges are incident) and apply the same node-ordering and serialization procedures on L(G) to capture edge-to-edge relationships.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linegraph (LG{*}) linearization",
            "representation_description": "Convert G to its linegraph L(G) where each original edge becomes a node and two nodes in L(G) connect if the corresponding edges in G share an endpoint. Compute rankings (degree/PageRank/CoreNumber) on L(G), optionally relabel, then serialize L(G)'s edges (i.e., original edge-edge incidences) as bracketed pairs.",
            "graph_type": "General undirected graphs; targeted at tasks that require modeling edge-to-edge relationships.",
            "conversion_method": "Construct L(G) from G, compute chosen centrality/degeneracy on L(G), order nodes (original edges) by rank, serialize incidence pairs for L(G) which correspond to edge-adjacencies in G.",
            "downstream_task": "Edge-centric reasoning tasks: edge existence, path reasoning, and path-existence queries; can be applied to other graph tasks but was evaluated primarily on edge-related tasks.",
            "performance_metrics": "Linegraph methods generally achieve better results on edge-based tasks (e.g., improved edge existence/path reasoning) but have lower overall average performance. Paper reports that LG{*} methods performed better in edge-based tasks while average score was lower than node-focused methods (exact numbers in tables).",
            "comparison_to_others": "Compared to node-centric linearizations, linegraph variants improved edge-centric tasks but reduced global average; CoreNumber-based methods also perform well on edge tasks.",
            "advantages": "Makes edge-to-edge relationships explicit in the sequence and can reveal inter-edge dependencies that node-centric serializations miss.",
            "disadvantages": "Can increase sequence size (linegraph node count = original edge count), potentially worsening context-window limits; lower average performance across mixed tasks; construction and serialization add preprocessing steps.",
            "failure_cases": "Lower overall average performance despite edge-task gains; still struggles on global tasks like diameter; for large graphs linegraph size may exceed context capacity.",
            "uuid": "e8841.7",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "pseudo_random_generator_order",
            "name_full": "Pseudo-random / generator-provided edge ordering",
            "brief_description": "Use the native edge ordering as produced by the graph generator (non-random procedural order), which can implicitly encode construction steps and structural cues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "generator-default edge ordering (pseudo-random)",
            "representation_description": "Do not reorder the generator-produced edge list; serialize the edges in the same order the generator produced them, possibly retaining original generator-provided labeling.",
            "graph_type": "Synthetic graphs produced by GraphWave and GraphQA generators (ER, BA, SFN, SBM, star, path, complete, motif-attached graphs).",
            "conversion_method": "Direct serialization of the edge list in the generator's output order without further ordering or relabeling.",
            "downstream_task": "Same graph reasoning tasks used in experiments.",
            "performance_metrics": "Performance is dataset-dependent: on GraphQA default ordering often significantly outperformed random baseline across most tasks (except path existence), and in some cases outperformed structured linearizations; on GraphWave results were mixedâdefault sometimes competitive. Paper reports superiority of default ordering on GraphQA while mixed on GraphWave (detailed table values provided).",
            "comparison_to_others": "Compared against random baseline and structured orderings; default ordering can be competitive or superior because generator order encodes construction semantics, but is not generally available for real-world graphs.",
            "advantages": "May implicitly encode constructive/structural information useful to LLMs, leading to strong performance on some datasets.",
            "disadvantages": "Depends on knowledge of generator internals; not available/applicable to arbitrary real-world graphs; may overfit to generator-specific biases.",
            "failure_cases": "May perform poorly for tasks like path existence in some datasets; not robust for generalization to graphs not produced by the same generator.",
            "uuid": "e8841.8",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AMR_PENMAN",
            "name_full": "AMR PENMAN linearization",
            "brief_description": "PENMAN notation linearization for Abstract Meaning Representation (AMR) graphs used in graph-to-text tasks (mentioned in related work for structured graph linearization in NLP).",
            "citation_title": "Abstract Meaning Representation for sembanking",
            "mention_or_use": "mention",
            "representation_name": "PENMAN AMR linearization",
            "representation_description": "Linearize directed acyclic semantic graphs (AMR) using the PENMAN bracketed notation that encodes node concepts and labeled relations in a canonical textual form suitable for sequence-to-sequence training.",
            "graph_type": "Directed acyclic semantic graphs (AMR); language semantic graphs for graph-to-text generation.",
            "conversion_method": "Use PENMAN notation to serialize AMR graphs to text, preserving labels/roles and argument structure; often used to fine-tune seq2seq models (e.g., BART) for graph-to-text tasks.",
            "downstream_task": "Graph-to-text generation and semantic-to-text tasks; used to fine-tune language models for text generation from AMR graphs.",
            "performance_metrics": "Cited as a standard linearization in prior literature; this paper references applications (Ribeiro et al., Hoyle et al.) that used PENMAN to fine-tune language models. No new metrics in this paper.",
            "comparison_to_others": "Presented in related work as domain-specific canonical linearization that differs from general graph edge-list approaches evaluated in this paper.",
            "advantages": "Canonical, widely used in NLP for semantic graphs; preserves labeled relations and semantic roles enabling direct graph-to-text learning.",
            "disadvantages": "Specific to AMR/DAGs and semantic annotation; not directly applicable to arbitrary undirected graphs or general graph reasoning tasks.",
            "failure_cases": "Not applicable to the general graph reasoning experiments evaluated in this paper; domain-limited.",
            "uuid": "e8841.9",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GML_GraphML",
            "name_full": "GML / GraphML structured formats",
            "brief_description": "Structured textual graph serialization formats used historically for graph interchange (GML human-readable format; GraphML XML-based), mentioned in related work as graph representations.",
            "citation_title": "Gml: Graph modelling language",
            "mention_or_use": "mention",
            "representation_name": "GML / GraphML formats",
            "representation_description": "GML: a simple human-readable key-value format for graph serialization; GraphML: an extensible XML-based format for detailed graph representation including node/edge attributes and meta-data.",
            "graph_type": "General graphs and domain-specific network data where structured metadata is needed.",
            "conversion_method": "Serialize graph nodes, edges, and attributes into GML key-value syntax or GraphML XML elements. These are textual formats but more verbose and structured than simple edge-lists.",
            "downstream_task": "Graph interchange, storage, and potential input for downstream processing or graph-to-text conversion; mentioned as alternatives rather than used in experiments.",
            "performance_metrics": "Not evaluated experimentally in this paper; mentioned as prior representations.",
            "comparison_to_others": "Not directly compared experimentally; contrasted qualitatively with simpler edge-list and natural-language serializations.",
            "advantages": "Rich expressivity for attributes and metadata; standardized formats for tooling and interchange.",
            "disadvantages": "Verbose; not optimized for LLM prompt efficiency; may produce longer token sequences and be less directly aligned to LLM pretraining distributions.",
            "failure_cases": "Not evaluated; likely constrained by context window and verbosity when used directly as LLM input for reasoning tasks.",
            "uuid": "e8841.10",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ego_subgraph_syntax_tree",
            "name_full": "Ego-subgraph / graph-syntax-tree linearizations",
            "brief_description": "Node-centric linearizations used by prior work for node-level tasks where an ego-subgraph around the target node (k-hop neighborhood) is converted to a textual tree-like representation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "ego-subgraph / syntax-tree linearization",
            "representation_description": "Center serialization around a target node by extracting its k-hop neighborhood (ego-subgraph) and converting it into a structured textual format (e.g., tree branches describing neighbor labels/features), inspired by linguistic syntax trees.",
            "graph_type": "Node-centric tasks on general graphs (node classification / local reasoning); used for ego-centered node tasks.",
            "conversion_method": "Extract k-hop subgraph for a target node, optionally convert it into a tree/syntax-like representation where branches encode neighbor labels and attributes, then formulate as text for LLM prompts.",
            "downstream_task": "Node classification and node-level reasoning tasks (multi-class node classification, neighbor-aware inference).",
            "performance_metrics": "Mentioned via prior studies (Zhao et al., Ye et al.) showing instruction-tuning on such encodings improves performance and can be on par with GNNs; this paper does not re-evaluate these metrics.",
            "comparison_to_others": "Different focus from whole-graph linearizations in this paper; ego-subgraph methods are tailored for node-level tasks and have been shown in prior work to benefit from instruction-tuning.",
            "advantages": "Focuses LLM attention on local structure relevant to a specific node; effective for node-level supervised tasks when fine-tuned or instruction-tuned.",
            "disadvantages": "Not suitable for global graph tasks; requires selecting/constructing an ego-centered subgraph per target node.",
            "failure_cases": "Not applicable to global tasks (e.g., diameter) and may miss long-range dependencies beyond k hops.",
            "uuid": "e8841.11",
            "source_info": {
                "paper_title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Promoting graph awareness in linearized graph-to-text generation",
            "rating": 2,
            "sanitized_title": "promoting_graph_awareness_in_linearized_graphtotext_generation"
        },
        {
            "paper_title": "Abstract Meaning Representation for sembanking",
            "rating": 2,
            "sanitized_title": "abstract_meaning_representation_for_sembanking"
        },
        {
            "paper_title": "GraphToken",
            "rating": 2,
            "sanitized_title": "graphtoken"
        },
        {
            "paper_title": "Exploring the potential of large language models in graph generation",
            "rating": 2,
            "sanitized_title": "exploring_the_potential_of_large_language_models_in_graph_generation"
        },
        {
            "paper_title": "NLGraph",
            "rating": 2
        },
        {
            "paper_title": "Gml: Graph modelling language",
            "rating": 1,
            "sanitized_title": "gml_graph_modelling_language"
        },
        {
            "paper_title": "Graph Modelling Language (GraphML)",
            "rating": 1,
            "sanitized_title": "graph_modelling_language_graphml"
        }
    ],
    "cost": 0.0205725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph Linearization Methods for Reasoning on Graphs with Large Language Models
25 Jun 2025</p>
<p>Christos Xypolopoulos 
Ecole Polytechnique</p>
<p>NTUA</p>
<p>Guokan Shang guokan.shang@mbzuai.ac.ae 
MBZUAI</p>
<p>Xiao Fei 
Ecole Polytechnique</p>
<p>Giannis Nikolentzos 
University of Peloponnese</p>
<p>Hadi Abdine 
MBZUAI</p>
<p>Iakovos Evdaimon 
Ecole Polytechnique</p>
<p>Michail Chatzianastasis 
Ecole Polytechnique</p>
<p>Giorgos Stamou 
NTUA</p>
<p>Michalis Vazirgiannis 
Ecole Polytechnique</p>
<p>MBZUAI</p>
<p>Graph Linearization Methods for Reasoning on Graphs with Large Language Models
25 Jun 20252CC22D4344FF04552A8C2B94CC202D66arXiv:2410.19494v3[cs.CL]
Large language models have evolved to process multiple modalities beyond text, such as images and audio, which motivates us to explore how to effectively leverage them for graph reasoning tasks.The key question, therefore, is how to transform graphs into linear sequences of tokens-a process we term "graph linearization"-so that LLMs can handle graphs naturally.We consider that graphs should be linearized meaningfully to reflect certain properties of natural language text, such as local dependency and global alignment, in order to ease contemporary LLMs, trained on trillions of textual tokens, better understand graphs.To achieve this, we developed several graph linearization methods based on graph centrality and degeneracy.These methods are further enhanced using node relabeling techniques.The experimental results demonstrate the effectiveness of our methods compared to the random linearization baseline.Our work introduces novel graph representations suitable for LLMs, contributing to the potential integration of graph machine learning with the trend of multimodal processing using a unified transformer model.</p>
<p>Introduction</p>
<p>Transformer-based large pre-trained models have revolutionized machine learning research, demonstrating unprecedented performance across diverse data modalities and even a mixture of modalities, including image, audio, and text domains (Xu et al., 2023;Yin et al., 2023).In particular, large language models (LLMs) have shown promising results in arithmetic, symbolic, and logical reasoning tasks (Hendrycks et al., 2020).Despite their success, the adaptation for processing graphs-an ubiquitous data structure that encapsulates rich structural and relational information-remains a comparably emerging and underdeveloped research direction, even if it has recently been gaining attention (Ye et al., 2023;Fatemi et al., 2023;Wang et al., 2024).This asymmetry is due in large part to the inherent challenge of representing graphs as sequential tokens, in a manner conducive to the language modeling objectives typical of transformers, a challenge not encountered when dealing with the other modalities.This unique problem has encouraged us to investigate a key question: How can we represent graphs as linear sequences of tokens for transformers in a suitable way?We refer to this research endeavor as Graph Linearization.</p>
<p>Existing methods of using LLMs for graph machine learning tasks, such as graph reasoning and graph generation, represent entire graphs as either raw edge lists or natural language descriptions that adhere to adjacency matrices without any special treatment (Fatemi et al., 2023;Wang et al., 2024;Yao et al., 2024).For example, a natural language description of the star graph S might be: "An undirected graph with nodes a, b, c, and d.Node b is connected to a. Node c is connected to b. Node b is connected to d.", its equivalent edge list representation is: "[(b, a), (c, b), (b, d)]".Either of the linearized representations is then appended with a task-specific question to form an LLM query prompt, e.g., "Is there a cycle in this graph?".Other studies focus solely on node-level tasks (Zhao et al., 2023;Ye et al., 2023), centering the linearization around an ego-subgraph for a target node (Hamilton et al., 2017), where the neighboring graph structure and node features up to k-hop are described in the prompt.However, there is a lack of studies investigating how to maintain the integrity of graph structures while efficiently transforming them into sequences suitable for LLMs.</p>
<p>Our research addresses this limitation.By relying on edge list representations as exemplified above, we study the performance impact on LLMs of various methods to order the edges in the list and rename interchangeable node labels, as shown in Figure 1.We argue that if the linearization of  {4: 3, 7: 3, 6: 3, 5: 3, 0: 2, 1: 2, 3: 2, 2: 2} (4, 5), (4,6), (4, 7), (4, 3), (5, 7), (5,6), (6,7), (3,2) ,  (3, 0),  (2, 1),  (1, 0 Given an input graph G, we rank its nodes based on their degree and then explore the edges in that order.The resulting linearized graph is then combined with a task-specific prompt and passed into a LLM.</p>
<p>graphs is conducted in a meaningful way, capturing properties similar to those found in natural language such as local dependency and global alignment, it will benefit contemporary LLMs by enhancing their ability to understand graphs, as they are trained on trillions of textual tokens.We define local dependency as the ability to predict the next (missing) token based on the previous (surrounding) context, as seen when a sequence of tokens naturally progresses in text, analogously to the distributional hypothesis of language (Joos, 1950;Harris, 1954;Firth, 1957).Global alignment involves starting sequences from tokens with similar characteristics, aligning the sequences to mimic how text samples begin or end with common words like "The" or "In conclusion" respectively.Addressing this question helps identify LLM-suitable representations for graphs that potentially align with natural languages, unlocking new insights and applications in fields where graphs naturally represent data.Graph linearization paves the way for extending transformer models' capabilities to graph data and unifying various graph tasks across domains, serving as a fundamental step toward building successful large graph models.This also facilitates the integration of graph learning with the multi-modal processing trend and the development of cohesive AI systems using a unified transformer model.</p>
<p>In this work, we developed multiple general graph linearization methods that leverage graph centrality and degeneracy, enriched by node relabeling techniques to achieve the aforementioned natural language properties.We conducted a series of inference experiments on various graph reasoning tasks, which form the basis for a deeper understanding of graph structures.Experimental results using Llama 3 models (Dubey et al., 2024) on synthetic datasets demonstrate the effectiveness of our methods compared to the random linearization baseline.Our key findings are as follows:</p>
<p>â¢ Edge ordering matters -Structured graph linearization improves the graph reasoning capabilities of LLMs.</p>
<p>â¢ Node labels contribute significantly -Node labels based on graph features further improve performance.</p>
<p>â¢ Task-specific linearizations -Node-based and edge-based linearizations perform better in respective tasks.</p>
<p>Related Work</p>
<p>In this section, we first introduce previous efforts to use transformers for graph machine learning tasks.We then discuss the current trend of using modern LLMs for graph reasoning and generation.Finally, we explore linearization methods for graphs especially from specific domains.</p>
<p>Transformers for graph machine learning</p>
<p>Transformers (Vaswani et al., 2017) have been successfully applied in various domains beyond text, demonstrating both versatility and effectiveness (Devlin et al., 2018).For instance, the Vision Transformer (Dosovitskiy et al., 2020) has achieved remarkable performance in image classification tasks by treating images as sequences of patches, marking a shift from traditional CNN-based approaches.Similarly, transformers have been used in speech recognition.Models like the Speech Transformer (Dong et al., 2018) apply self-attention mechanisms to process audio data as sequences, outperforming traditional RNN-based methods.</p>
<p>These successes have spurred interest in using modified transformers to replace the de facto GNNbased approaches for graph machine learning tasks.Notably, Graphormer (Ying et al., 2021) enables transformer to effectively capture the dependencies and relationships within a graph.It achieves this by integrating node centrality encoding and attention biases that account for the spatial distance between nodes.Graph Transformer (GT) (Dwivedi and Bresson, 2020) generalizes the transformer architecture for graph representation learning.GT introduces the concept of relative positional encodings to account for the pairwise distances between nodes in a graph.This approach allows the model to learn rich node representations that capture both local and global graph structures.</p>
<p>In contrast to the above works, Kim et al. (2022) show that by treating all nodes and edges as independent tokens and inputting them into a standard Transformer encoder without any graph-specific modifications, notable outcomes can be achieved both theoretically and practically.Results on molecular graphs for quantum chemical property prediction show that this approach outperforms all GNN baselines and achieves competitive performance compared to graph Transformer variants.Despite not applying any structural alterations to the Transformer, this approach still requires sophisticated token-wise node and edge embeddings to explicitly represent the connectivity structure.</p>
<p>LLMs for graph reasoning</p>
<p>Following the recent success of LLMs in tasks beyond language processing (Hendrycks et al., 2020), several studies have explored the capacity of offthe-shelf LLMs for graph reasoning.While there is no clear consensus on the specific tasks, models are tested on understanding basic topological properties, such as graph size, node degree and connectivity, which form the foundation for a deeper understanding of graph structures (Zhang et al., 2023b).Using various prompting methods, these studies show that LLMs, even without fine-tuning, demonstrate preliminary graph reasoning abilities.</p>
<p>Several studies have evaluated LLMs for graph reasoning at both the node and graph levels.For example, NLGraph (Wang et al., 2024) covers eight graph reasoning tasks of varying complexity, ranging from simple tasks like connectivity and shortest path to complex problems like maximum flow and simulating graph neural networks.This work also proposes two graph-specific prompting methods that achieve notable performance improvements.GraphQA (Fatemi et al., 2023) focuses on relatively simple tasks to measure the performance of pre-trained LLMs in edge existence, node degree, node count, edge count, connected nodes, and cycle checks.It shows that larger models generally perform better on graph reasoning, with graphs generated synthetically using various graph generators.Similar works include various studies that explore graph reasoning using different LLMs, prompting techniques, graph tasks, domains, and evaluation approaches (Chen et al., 2023;Guo et al., 2023;Zhang et al., 2023a;Hu et al., 2023;Huang et al., 2024;Liu and Wu, 2023;Das et al., 2023;Yuan et al., 2024;Wu et al., 2024;Skianis et al., 2024).</p>
<p>Another line of research criticizes the above approach, arguing that solely using prompt engineering or in-context learning with frozen LLMs hinders achieving top performance in downstream graph tasks.Therefore, instruction-tuning or finetuning is necessary.The work of Ye et al. (2023) preliminarily confirms this on the multi-class node classification task.A prompt template is designed to describe both the neighbor graph structure and node features centered around a target node up to the 3-hop level.Similarly, Zhao et al. (2023) draw inspiration from linguistic syntax trees.For a target node, the work converts its ego-subgraph into a graph syntax tree with branches describing the neighborhood's "label" and "feature", which are then encoded as text in the prompt.Results show that instruction-tuning performs much better than in-context learning, and is on par with GNN-based models.Similar conclusions can be observed in recent studies on graph-level reasoning settings (Luo et al., 2024).Finally, Perozzi et al. (2024) introduce GraphToken, which trains an encoder to create continuous representations, rather than converting graphs into text tokens.</p>
<p>LLMs for graph generation</p>
<p>Graph generation involves creating graphs with specific properties, a process that holds significant real-world value in areas like drug discovery.This task is more challenging than graph reasoning.</p>
<p>To the best of our knowledge, Yao et al. (2024) was the first to show the preliminary abilities of LLMs in graph generation.The work experimented with graph generation in three settings: 1) Rulebased: generating graphs of basic structure types, given rules describing the desired structures, e.g., trees, cycles, etc.; 2) Distribution-based: generating graphs following a structural type distribution p, given a set of example graphs with the same distribution; 3) Property-based: generating molecule structures with specific properties, given example molecules in SMILES format.Results show that LLMs have reasonably good abilities across the three tasks, and advanced prompting techniques do not necessarily lead to better performance.comment Graph generation is critical for applications like drug discovery but presents greater challenges than graph reasoning.(Yao et al., 2024) provide early insights into LLM-driven graph generation, evaluating rule-based, distribution-based, and property-based approaches.Their findings suggest that while LLMs can generate structurally meaningful graphs, advanced prompting does not always yield better performance, highlighting the need for more robust generation strategies.</p>
<p>Linearization for specific graphs</p>
<p>In deriving an ordering for graphs, topological sorting in graph theory examines the linear ordering of directed acyclic graphs, such that for every directed edge (u, v), u precedes v in the ordering.However, such graph traversal is node-centric, making edge information not encoded.</p>
<p>In other domains involving specific types of graphs, such as discourse graphs-a directed weakly connected graph reflecting discourse structure-nodes represent utterances, and edges represent discourse relations (e.g., elaboration, clarification, completion) within a conversation (Rennard et al., 2024).The work of Chernyavskiy et al. (2024) proposes a linearization method for discourse graphs that arranges utterances chronologically, assigning unique identifiers to speakers, utterances, and addressees.It incorporates discourse relations and sentiment tokens to generate a structured sequence, using special tokens for clarity.This structured sequence is then used to train a BART (Lewis et al., 2020) for dialogue generation.Similarly, Abstract Meaning Representation (AMR) uses directed acyclic graphs to provide a structured semantic representation of language, incorporating semantic roles with annotated arguments and values where nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013).AMR corpora are usually linearized using the PENMAN-based notation (Patten, 1993) as in the work of (Ribeiro et al., 2021) and (Hoyle et al., 2021) to fine-tune pre-trained language models to perform graph-to-text generation.For citation networks, Guo et al. ( 2023) have explored the Graph Modelling Language (GML) and Graph Markup Language (GraphML) for graph representation (Himsolt, 1997;Brandes et al., 2013).GML is a simple, human-readable format, while GraphML is XML-based and offers extensibility for complex applications.</p>
<p>The scope of our work.Unlike the above linearization methods limited to specific types of graphs, where linearization can be naturally derived to some extent, we focus on general graphs.Furthermore, unlike previous works using LLMs for reasoning and generation tasks, where edge lists are directly leveraged without special treatment, we introduce various linearization methods for ordering the edges in the list and renaming interchangeable node labels to make them suitable for LLMs.Although our work involves only graph reasoning experiments, our graph linearization methods are general and applicable to various scenarios.This allows for the effective transformation of graph structures into sequences suitable for language models and has the potential to improve performance in both reasoning and generation tasks, with or without fine-tuning.</p>
<p>Graph Linearization Methods</p>
<p>This section describes the graph linearization approach, emphasizing the use of graph features to enhance graph reasoning with LLMs.</p>
<p>Generally speaking, we define graph linearization as the process of representing graphs as linear sequences of tokens.In this work, we aim to identify the linearization approaches that will benefit LLMs by enhancing their ability to understand graphs.We argue that linearized graphs, represented as sequences of tokens, should capture properties similar to those in natural language, given the fact that LLMs are pre-trained on trillions of textual tokens.Such properties should include local dependency and global alignment.</p>
<p>Local dependency refers to the ability to predict the next (missing) token based on the previous (surrounding) context, within the token sequence of a single linearized graph.This property is analogous to the fundamental distributional hypothesis of language (Joos, 1950;Harris, 1954;Firth, 1957), which states that words that occur in similar contexts tend to have similar meanings (or functions).This hypothesis suggests that given a new word, one should be able to figure out its meaning based on the contexts in which it is used.In fact, the masked and casual language modeling for training encoder-only (Devlin et al., 2019) and decoder-only (Radford et al., 2019) language models, can be seen as instantiations of this hypothesis.</p>
<p>Global alignment refers to the alignment across the token sequences of different linearized graphs, ensuring that the corresponding tokens of both sequences match across their full length.This property takes into account the overall structure of the sequences, reflecting the typical flow of text, where common words like "The" or "In conclusion" are used at the start or end of a sequence, guiding the alignment.For example, linearization should always start from the node with the highest degree, and such nodes are all relabeled as index 0 for different graphs.</p>
<p>By relying on edge list, which defines a graph in terms of its individual connections, we study the performance impact on LLMs of various methods for ordering the edges in the list and renaming interchangeable node labels, as shown in Figure 1.More specifically, we leverage the advances in graph degeneracy and centrality as detailed in below to meet the local dependency property.</p>
<p>Graph Degeneracy (Seidman, 1983).Let G(V, E) be an undirected graph with n = |V | nodes and m = |E| edges.A k-core of G is a maximal subgraph of G in which every node v has at least degree k.The k-core decomposition of G forms a hierarchy of nested subgraphs whose cohesiveness and size increase and decrease, respectively, with k.Higher-level cores can be viewed as filtered versions of the graph that capture the most significant structural information.The Core Number of a node is the highest order of a core that contains the node.</p>
<p>Graph Centrality.Graph centrality is a key concept in network analysis used to determine the influence or importance of nodes based on the structure of the graph.We consider two types of centrality, one based on the Degree of nodes and one based on PageRank.Degree centrality is one of the simplest measures of node importance (Freeman, 1978).It is defined as the number of edges incident to a node, making it a measure of local centrality that reflects the node's immediate connectivity within the graph.For an undirected graph G with n = |V | nodes and m = |E| edges, the degree centrality D(v) of a vertex v â V is calculated as D(v) = uâV A vu where A is the adjacency matrix of the graph, and A vu = 1 if there is an edge between vertices v and u, and 0 otherwise.</p>
<p>PageRank is a more sophisticated centrality measure, originally developed by Brin and Page (1998) for ranking web pages.It extends the concept of degree centrality by considering not only the number of links a node has but also the importance of the nodes linking to it.PageRank effectively captures the notion that connections from highlyranked nodes contribute more to the ranking of a given node than connections from low-ranked nodes (Page et al., 1999).PageRank, although designed for directed graphs, can also be adapted for undirected ones by treating all edges as bidirectional.The PageRank centrality P R(v) of a node v â V is computed iteratively using the formula:
P R(v) = 1âÎ± |V | + Î± uâN (v) P R(u) deg + (u)
where Î± is a damping factor typically set to 0.85, N (v) represents the set of nodes linking to v, and deg + (u) is the out-degree of node u.</p>
<p>Graph Linearization Implementation.Our approach to capitalizing on the local dependency property involves the following steps.Given a graph G, we initially rank the nodes by the centrality and degeneracy measures described previously.Then, we begin exploring the nodes by descending order and list the edges connected to it, arranging them in a random order.Each edge is represented as a node pair.In the case where two or more nodes share an equal value, the order is selected randomly.After the ordering process has concluded, each edge list constitutes a sequence of tokens following a descending order of node importance.</p>
<p>In addition to linearization methods, node relabeling is employed as a means to attempt the attainment of the global alignment property.Specifically, node relabeling introduces an additional step to our procedure.After ranking the nodes, their original labels are replaced with their respective positions in the ranking.Consequently, the node with index 0 corresponds to the one with the highest core number, and so forth.This approach may prove advantageous for the LLM by ensuring a consistent association between node indices and their respective importance properties.</p>
<p>Finally, we conducted experiments in which the edges were ordered directly, rather than the nodes.This allows our linearization to directly capture relationships between edges, which can be essential for understanding complex graph structures.To achieve this, each graph was transformed into its corresponding linegraph representation.A linegraph L(G) of a graph G is the graph where each edge of G is replaced by a node, and where two edges of G are connected in L(G) if they are incident in G. Subsequently, the previously described processes were applied directly to L(G).</p>
<p>Experimental Setup</p>
<p>In this section, we provide a detailed overview of the experimental setup, including the methodologies, resources, and evaluation frameworks used in our experiments.</p>
<p>Datasets</p>
<p>To better explore the capabilities of LLMs on graph reasoning tasks, we utilized two synthetic datasets.</p>
<p>GraphWave.First, we constructed a synthetic graph dataset using GraphWave (Donnat et al., 2018).This graph generator was originally developed for controlled experimentation and evaluation of node embedding techniques and measurement of structural equivalence on graphs with known network motifs.Motifs, in the context of network science, are sub-graphs that repeat themselves either within the same graph or across different graphs.For our analysis, these structurally similar sub-graphs enable us to construct structurerelated tasks, that allow us to assess the ability of language models to analyze and infer structural features within these graphs.</p>
<p>The generator operates by sequentially constructing a base graph, that follows either a cycle or chain structure, and then attaching a number of motifs that follow predetermined shapes-cliques, stars, fans, diamonds, and trees.To cover a wider variety of structural complexities, we also include all combinations of two shapes, along with all combinations of three shapes with unique shapes per triplet.Example graphs can be found in Figure 2.</p>
<p>For each combination of shapes, we generated 100 graphs, leading to a total of 3000 graphs.This ensures sufficient variance in graph sizes and in the combinations of base and motif sub-graphs.The number of nodes in each graph shape is selected randomly, with the following constraints: base (3-21 nodes); clique, fan, and star (4-11 nodes); diamond (6 nodes); and tree (perfect binary trees with 3-6 levels).The generated dataset contains an average of 32.33 nodes and 43.72 edges per graph.</p>
<p>GraphQA.Furthermore, we included GraphQA (Fatemi et al., 2023), which is widely used to evaluate the graph reasoning capabilities of LLMs.Similar to GraphWave, the dataset consists of randomly generated graphs derived from various graph gen-erators, including ErdÅs-RÃ©nyi graphs (ErdÅs and RÃ©nyi, 1959), scale-free networks (SFN) (BarabÃ¡si and Albert, 1999), the BarabÃ¡si-Albert model (BA) (Albert and BarabÃ¡si, 2002), and the stochastic block model (SBM) (Holland et al., 1983), along with star, path, and complete graph generators.A total of 500 graphs were sampled for ER, BA, SFN, and SBM models, whereas 100 graphs were sampled for path, complete, and star graphs due to their lower structural variability.All generated graphs contained between 5 and 20 nodes.</p>
<p>Tasks</p>
<p>Our experiments encompass a series of graph reasoning tasks, which can be broadly categorized into classification-based and numerical tasks.Numerical tasks, ranging in difficulty, require the model to produce a numerical output, either through structural computation or counting-based inference.This combination of tasks allows us to comprehensively evaluate LLMs' understanding of structural features and examine how different edge list orderings affect their performance.</p>
<p>A fundamental task is Node Counting, where the LLM estimates the number of nodes.In Node Degree calculation, the LLM determines the degree of a node.A more advanced variant, Maximum Degree calculation, requires the LLM to internally calculate the degree of all nodes and then identify the maximum among them.</p>
<p>Beyond node-related tasks, we assess the ability to infer relational properties.In Edge Existence and Path Existence tasks the LLM is given a randomly selected pair of nodes and must determine whether an edge or a connecting path exists between them, respectively.In the Shortest Path task, the model must compute the length of the shortest path between two given nodes, requiring a deeper understanding of graph connectivity.The Diameter Estimation task requires the model to determine the longest shortest path in the graph, showcasing global graph structure understanding.</p>
<p>Table 2: Accuracy scores for a subset of tasks on the GraphWave dataset using Llama 3 70B as an ablation study.Notations remain the same as in Table 1.</p>
<p>Finally, we evaluated Motifs' Shape classification, a dataset-specific task leveraging Graph-Wave's embedded structures, where the LLM is given definitions of the five motif types and asked is to identify which is present.</p>
<p>In every prompt, a node v is represented by an incremental integer, while an edge between nodes v and u is denoted by the bracketed pair (v, u).An edge list is expressed as a sequence of edges, sorted according to the scheme used in each linearization method.We tested both zero-shot and one-shot approaches, where a randomly selected graph from the dataset was used consistently as the one-shot example across all experiments.The prompt templates are provided in Appendix D.</p>
<p>LLMs</p>
<p>We used the 8B parameter Llama 3 Instruct (Dubey et al., 2024) with a temperature of 1eâ3 and a sampling parameter of 1eâ1 for more deterministic outputs to assess sensitivity to our linearization methods.Experiments were conducted on an NVIDIA A5000.Additionally, we conducted partial experiments with the 70B-parameter model to evaluate its impact on graph reasoning capabilities and to verify the consistency of our approach.Finally, to explore the impact of model family differences, we also include experiments over Qwen 2.5 14B-1M (Yang et al., 2025) in the Appendix A.</p>
<p>Baselines</p>
<p>For our comparisons, we consider only a random baseline.This baseline involves a fully random ordering of the edge list, where edges are arranged without following any inherent scheme.To further eliminate structural biases, we also randomly shuffle the node labels.This baseline is founded on the fact that we are working with general graphs, where default labels or ordering are neither predetermined nor necessarily provide meaningful information in real-world applications.In addition, to mitigate the risk of skewed results, we applied five different random orderings and averaged their performance.Our random ordering can be considered comparable to prior studies, which tend to preserve the inherent structure of the generator.</p>
<p>Evaluation</p>
<p>We use exact accuracy to compare our methods, measuring the ratio of correct predictions as 1 n n i=1 I (y i = Å·i ), where n is the number of graphs, y i the correct answer, and Å·i the LLM's response.For numerical tasks, we consider a result Table 3: Accuracy scores for all tasks on the GraphQA dataset using Llama 3 8B, , including the overall average.Notations remain the same as in Table 1.</p>
<p>accurate only if it is an exact match.For the motifs' shape classification task, accuracy reflects the total across all shapes, requiring the predicted shape to appear at least once in the graph.</p>
<p>Experimental Results Analysis</p>
<p>In this section, we review the performance of our linearization methods across various tasks.</p>
<p>Tables 1 and 3 present the performance of our methods on the GraphWave and GraphQA datasets with the Llama 3 8B model.Additionally, to evaluate model scale, we report results for selected GraphWave tasks using the 70B model in Table 2.The results are organized into three groups: one where node labels in the linearized graphs are randomly assigned, ensuring a fair comparison since, in real-world graphs, labels might be arbitrary; another where node labels are reindexed according to each method, as described in Section 3; and finally, a comparison against the random baseline.The performance related to the pseudo-random (default) node labels originally provided by synthetic graph generators is discussed in Appendix A.</p>
<p>Overall, across both datasets, our linearization methods consistently outperform the random baseline, highlighting the critical role of graph linearization, as evident in the average performance and across multiple individual tasks.Notably, on the GraphQA dataset, the combination of degreebased ordering and node relabeling improves performance by approximately 35% on the maximum node degree estimation task and by around 13% on the shortest path task.Similarly, on the GraphWave dataset, the combination of degree-based ordering and node relabeling enhances edge existence per-formance by roughly 26%.Comparing random and node relabeling reveals that ordering alone offers significant improvements over the baseline, while the additional information from structured relabeling further enhances accuracy on nearly all tasks.Among all tasks, diameter estimation is the most challenging across both datasets, with consistently low performance, indicating LLMs struggle to infer global graph properties at this level of complexity.</p>
<p>Linegraph-based methods (LG{*})-where edges are reinterpreted as nodes-highlight the importance of edge-to-edge relationships.While their overall average score is lower, they generally perform better in edge-based tasks, such as edge existence and path reasoning, by capturing interdependencies that might be less evident in traditional node-focused representations.These findings suggest that a more suitable linearization approach may be necessary to fully exploit the benefits of the linegraph transformation.</p>
<p>Similarly, CoreNumber-based methods achieve better performance in edge-centric tasks, which can be attributed to its ability to capture the structural cohesiveness of a graph.By emphasizing nodes embedded in densely connected subgraphs, core number ordering effectively preserves key connectivity patterns, making it particularly advantageous for reasoning about edge relationships.In contrast, while Degree-and PageRank-based orderings demonstrate the most consistent performance across various tasks, their strengths are more pronounced in node-related tasks.</p>
<p>When moving from zero-shot to one-shot setting, we notice a performance loss on binary classification tasks like edge and path existence.This decline may result from the model's reliance on a single graph example, which does not fully capture the complexity and diversity of the dataset.However, despite this drop, one-shot prompting remains effective for more complex tasks.</p>
<p>Finally, when comparing the results between the 8B and 70B versions of Llama, we observe a significant performance boost in the node counting and motif shape classification tasks with the larger model.In contrast, the node degree task shows only a modest improvement.Meanwhile, diameter estimation continues to exhibit very low accuracy, indicating that this task remains particularly challenging regardless of model size.</p>
<p>Conclusion</p>
<p>In this study, we investigated different graph linearization techniques for LLMs.The core challenge lies in converting graphs into linear token sequences while maintaining important structural features-like local dependencies and global coherence-akin to those in natural language, to help LLMs more effectively interpret graph-based data.To this end, we developed and evaluated several linearization methods based on graph centrality and degeneracy.Our experiments showed that graph linearization notably enhances the performance of LLMs on graph reasoning tasks, especially when paired with the node relabeling technique.Our work presents novel graph representations tailored for LLMS, paving the way for integrating graph machine learning with the growing trend of multimodal processing through a unified transformer.</p>
<p>Limitations</p>
<p>Our study has several limitations that future research could address.First, we considered only a limited set of structural features.Key graph properties, such as community structures and connected components, were not incorporated, yet they could enhance the model's ability to capture complex structural patterns.Second, the diversity of datasets is limited.We conducted our experiments exclusively on synthetic graphs, and future work should explore a broader range of graph datasets to improve real-world generalization.Third, the range of LLMs evaluated was narrow.We primarily focused on LLaMA 3, and future studies could investigate a wider variety of models, including those fine-tuned for coding or mathematical reasoning, to assess their impact on graph-based tasks.Fourth, our method inherits a fundamental limitation from the underlying LLMs: the input sequence is bounded by the model's context window.This imposes an upper limit on the number of edges that can be represented per graph.In Appendix C, we analyze how this constraint translates to a maximum graph size under different edge densities and token budgets.Lastly, our study does not investigate the impact of additional training to adapt models to our linearization methods, which could potentially enhance their graph reasoning capabilities.</p>
<p>Marinka Zitnik and Jure Leskovec.2017.Predicting multicellular function through multi-layer tissue networks.arXiv preprint arXiv:1707.04638.</p>
<p>A Additional Experiments</p>
<p>Pseudo-random linearization.We also investigated the performance of utilizing the edge ordering directly provided by the graph generator.Most non-random graph generators create graphs procedurally, inherently embedding structural information within the edge list order.For instance, in GraphWave, the process begins with a base graph and subsequently attaches motifs, making it easier to distinguish between different structures.We hypothesize that this embedded structural knowledge will enhance task performance and boost the LLM's capabilities.However, generating such structure-aware edge lists requires an understanding of the graph construction process, which may not be feasible for real-world applications involving larger and more complex graphs.The results of both datasets are presented in Table 4.For the GraphWave dataset, the default edge ordering shows mixed performance compared to the random ordering baseline.When combined with structured edge ordering (Table 1), accuracy improves consistently across tasks, except for path existence.When comparing default labeling with structured node labeling, performance generally improves, though default labeling remains competitive in certain tasks.For the GraphQA dataset, the default edge ordering performs significantly better than the random ordering across all tasks except path existence.In this case, the default ordering proves to be particularly robust, making it challenging for structured edge ordering to achieve higher accuracy.Even compared to structured edge ordering (Table 3), default ordering often maintains a performance advantage, highlighting its effectiveness in this dataset.LLM Family Variation with Extended Context.To further examine the influence of model architecture and extended context capacity, we evaluated the performance of Qwen 2.5 14B-1M (Yang et al., 2025), a large-context language model from a distinct model family capable of processing input sequences of up to 1 million tokens.This evaluation allows us to assess whether architectural differences impact performance on graph-based reasoning tasks.Table 5 reports the results obtained for the same selection of tasks previously used in Table 2, enabling a direct comparison across models with varying capacities.</p>
<p>Although scaling from LLaMA 8B to 70B yields substantial gains in tasks such as node counting and motif shape classification, Table 5 demonstrates that Qwen 2.5 14B-despite having fewer parameters than LLaMA 70B-achieves competitive, and in some cases superior, performance across several tasks.This is particularly evident in motif shape classification and diameter estimation, where Qwen's results rival or exceed those of the larger model.Nonetheless, in line with trends observed across LLaMA variants, diameter estimation remains a consistently challenging task, with overall accuracy remaining low regardless of model architecture or scale.</p>
<p>B Performance and Complexity</p>
<p>All considered graph measures, such as core number, degree, and PageRank, are computationally efficient.For graphs where the number of edges exceeds the number of nodes, the computational complexity scales linearly with the number of edges, that is, O(m), where m denotes the number of edges.Given that the graphs in the evaluated datasets are relatively small, the computation time required for these measures is negligible compared to the response generation time of the LLMs.</p>
<p>To illustrate this, we report on Table 6 the number of tokens generated per task along with the average time taken to produce a single response using LLama 3 8b.Numerical tasks, LLM responses are concise and rapidly converge to a final answer.In contrast, more complex tasks elicit longer responses that often involve intermediate reasoning steps.</p>
<p>C Graph Size Limitations</p>
<p>In our approach, the entire graph is linearized into a token sequence and embedded directly into the model's input prompt.As a result, the maximum size of the graph that can be processed in a single prompt is constrained by the model's context window.Since the sequence length is primarily determined by the number of edges, we estimate the maximum number of edges that can be encoded per prompt for each model considered in this study.</p>
<p>To compute these estimates, we assume that each edge requires approximately 5 tokens to represent, and that the accompanying task description consumes an average of 100 tokens.Under these assumptions, Table 7 reports the estimated edge ca-Table 4: Accuracy scores for all tasks on the GraphWave (top) and GraphQA (bottom) datasets using Llama 3 8B.For each task, we compare the default labeling scheme, as provided by the corresponding graph generator, against the default order the edges have been generated.Notations remain the same as in Table 1.Table 5: Accuracy scores for a subset of tasks on the GraphWave dataset using Qwen 2.5 14B -1M as an ablation study.Notations remain the same as in Table 1.</p>
<p>pacity corresponding to the context window of each model.</p>
<p>In Table 8, we present statistics for several widely used graph datasets.Many of these graphs are sufficiently small to fit within the context window of contemporary LLMs, with notable exceptions such as large-scale social and e-commerce networks (e.g., Reddit, Amazon).This indicates that a substantial portion of benchmark graph datasets can be fully serialized and input to an LLM in a single prompt.Nevertheless, in practical applications, even graph neural networks often rely on sampling strategies rather than processing entire graphs at once.A similar strategy may be necessary when using LLMs for real-world graph tasks, depending on the application and scale.</p>
<p>Although our study primarily investigates the  ability of LLMs to understand and reason over complete graph structures, we recognize that some of the tasks examined-such as node counting-are primarily diagnostic and may have limited practical relevance.These tasks are intended to serve as controlled benchmarks to assess the reasoning capabilities of LLMs, rather than to reflect typical graph processing workloads.</p>
<p>D Prompt templates</p>
<p>Node Counting</p>
<p>In an undirected graph G, (i, j) means that node i and node j are connected with an undirected edge.Q: How many nodes are in G?  (Yang et al., 2018) 12,752 491,722 OGBN-Proteins (Hu et al., 2020) 132,534 39,561,252 Reddit (Hamilton et al., 2017) 232,965 114,615,892 Amazon Products (Chiang et al., 2019) 1,569,960 264,339,468 Table 8: Node and edge counts for commonly used graph datasets.</p>
<p>G: {linearized graph}</p>
<p>Max Degree</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.The degree of a node is the number of edges connected to the node.Given a graph G and its list of edges, respond to the following question: Q: Without any justification, what is the maximum node degree in the following graph G? G: {linearized graph}</p>
<p>Node Degree</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.The degree of a node is the number of edges connected to the node.Given a graph G and its list of edges, respond to the following question: Q: Without any justification, what is the degree of node {node} in the following graph G? G: {linearized graph}</p>
<p>Edge Existence</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.Given a graph G and its list of edges, respond to the following question: Q: Does an undirected edge ({node1}, {node2}) exist in the following graph G?. G: {linearized graph}</p>
<p>Diameter</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.The diameter of a graph is the length of the shortest path between the most distanced nodes.Given a graph G and its list of edges, respond to the following question: Q: Without any justification, what is the diameter of the following graph G? G: {linearized graph}</p>
<p>Shortest Path</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.Given a graph G and its list of edges, respond to the following question: Q: Without any justification, what is the length of the shortest path from node {node1} to node {node2}?If no path exists, the response is '0'.G: {linearized graph}</p>
<p>Path Existence</p>
<p>In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.Given a graph G and its list of edges, respond to the following question: Q: Does a path that connects node {node1} and {node2} exist in the following graph G? G: {linearized graph} Motifs' Shape Classification: In an undirected graph, (i, j) means that node i and node j are connected with an undirected edge.The graph contains a motif graph with strictly one of the following structures.{structure}: {definition} Q: Which of the defined structures is included in the following graph?graph: {linearized graph}</p>
<p>Figure 1 :
1
Figure 1: An example of degree-based graph linearization method.Given an input graph G, we rank its nodes based on their degree and then explore the edges in that order.The resulting linearized graph is then combined with a task-specific prompt and passed into a LLM.</p>
<p>Figure 2 :
2
Figure 2: Overview of GraphWave synthetic dataset.</p>
<p>Table 6 :
6
Tokens generated and response time for a single graph using LLaMA 3 8B across tasks.
TaskNumber of Tokens Inference Time (sec)Node Counting160.6Max Degree160.6Node Degree160.6Edge Existence1284.8Diameter1284.8Shortest Path1284.8Path Existence1284.8Motif's Shape160.6LLMContext Length (tokens) Max Number of EdgesLLama 3 8b8,1921,618LLama 3 70b8,1921,618Qwen 2.5 14B 1M1,010,000199,980</p>
<p>Table 7 :
7
Estimated Maximum Number of Edges considering Context Window Size.</p>
<p>Node Counting Max Degree Node Degree Edge Existence Diameter Shortest Path Path Existence Motifs' Shape Average Random Labeling CoreNumber. 25.97 / 34.98 17.47 / 17.37 58.53 / 56.79</p>
<p>28.81 / 39.18 24.10 / 23.57 59.4 / 56.42PageRank. </p>
<p>. 28.65 / 36.05 14.57 / 16.17 58.3 / 61.32Node Relabeling CoreNumber. </p>
<p>34.35 / 38.01 26.07 / 25.24 65.37 / 56.92 50.83 / 46.08 10.47 / 11.17 32.33 / 17.67PageRank. </p>
<p>. 32.46 / 34.6 15.13 / 12.94 39.36 / 45.99Baseline. </p>
<p>Each task compares two node labeling schemes in zero-shot / one-shot prompting against the random Baseline. The linearization names represent the node ordering methods used (CoreNumber, Degree, or PageRank), while 'LG{*}' indicates graphs were transformed to the corresponding linegraph beforehand. Underlined scores denote the best-performing linearization method for each task, labeling, and X-shot combination; bold indicates task-wise best. 76.47 / 75.29 72.47 / 63.75 3.97 / 4.07Node Counting Node Degree Diameter Motifs' Shape Random Labels CoreNumber. Table 1: Accuracy scores for all tasks on the GraphWave dataset using Llama 3 8B, including the overall average</p>
<p>84.77 / 81.13 73.93 / 67.59 8.63 / 12.84 58.1 / 47.4PageRank. </p>
<p>76.27 / 76.93 75.47 / 65.32 8.77 / 12.84LG{CoreNumber}. </p>
<p>. 83.24 / 83.04 69.69 / 59.81 4.52 / 7.72 41.57 / 42.5Baseline. </p>
<p>60.77 / 24.28 15.88 / 18.67 54.44 / 37.23 70.68 / 61.59 3.28 / 18.53 49.21 / 57.25Node Counting Max Degree Node Degree Edge Existence Diameter Shortest Path Path Existence Average Random Labels CoreNumber. </p>
<p>62.12 / 28.31 26.03 / 28.94 56.23 / 28.05PageRank. </p>
<p>61.14 / 23.94 10.81 / 25.89 49.09 / 36.65LG{CoreNumber}. </p>
<p>64.71 / 44.97 40.72 / 43.15 58.77 / 28.65 69.97 / 70.83 3.57 / 14.13 26.17 / 49.54PageRank. </p>
<p>. 66.28 / 27.38 9.28 / 20.37 49.78 / 35.86Baseline. </p>
<p>Statistical mechanics of complex networks. Reviews of Modern Physics. References RÃ©ka Albert and Albert-LÃ¡szlÃ³ BarabÃ¡si7412002</p>
<p>Abstract Meaning Representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseSofia, BulgariaAssociation for Computational Linguistics2013</p>
<p>Emergence of scaling in random networks. Albert-LÃ¡szlÃ³ BarabÃ¡si, RÃ©ka Albert, Science. 28654391999</p>
<p>Graph markup language (graphml). Ulrik Brandes, Markus Eiglsperger, JÃ¼rgen Lerner, Christian Pich, Handbook of graph drawing visualization, Discrete mathematics and its applications. Roberto Tamassia, Boca RatonCRC Press2013u.a.</p>
<p>The anatomy of a large-scale hypertextual web search engine. Sergey Brin, Lawrence Page, Computer Networks and ISDN Systems. Elsevier199830</p>
<p>Exploring the potential of large language models (llms) in learning on graph. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, NeurIPS 2023 Workshop: New Frontiers in Graph Learning. 2023</p>
<p>GroundHog: Dialogue generation using multi-grained linguistic input. Alexander Chernyavskiy, Lidiia Ostyakova, Dmitry Ilvovsky, Proceedings of the 5th Workshop on Computational Approaches to Discourse (CODI 2024). the 5th Workshop on Computational Approaches to Discourse (CODI 2024)St. Julians, Malta. Association for Computational Linguistics2024</p>
<p>Clustergcn: An efficient algorithm for training deep and large graph convolutional networks. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, Cho-Jui Hsieh, arXiv:1907.049312019arXiv preprint</p>
<p>Which modality should i use-text, motif, or image?. Debarati Das, Ishaan Gupta, Jaideep Srivastava, Dongyeop Kang, arXiv:2311.09862Understanding graphs with large language models. 2023arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Speechtransformer: a no-recurrence sequence-to-sequence model for speech recognition. Linhao Dong, Shuang Xu, Bo Xu, 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE2018</p>
<p>Learning structural node embeddings via diffusion wavelets. Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec ; Alexey, Lucas Dosovitskiy, Alexander Beyer, Dirk Kolesnikov, Xiaohua Weissenborn, Thomas Zhai, Mostafa Unterthiner, Matthias Dehghani, Georg Minderer, Sylvain Heigold, Gelly, arXiv:2010.11929Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018. 2020arXiv preprintAn image is worth 16x16 words: Transformers for image recognition at scale</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>A generalization of transformer networks to graphs. Vijay Prakash, Dwivedi , Xavier Bresson, arXiv:2012.096992020arXiv preprint</p>
<p>On random graphs. Paul ErdÅs, AlfrÃ©d RÃ©nyi, Publicationes Mathematicae. 61959</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>A synopsis of linguistic theory, 1930-1955. John R Firth, Studies in linguistic analysis. 1957</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Linton C Freeman ; Jiayan, Lun Guo, Hengyu Du, Mengyu Liu, Xinyi Zhou, Shi He, Han, arXiv:2305.15066Social networks. 131978. 2023arXiv preprintCentrality in social networks conceptual clarification</p>
<p>Inductive representation learning on large graphs. Advances in neural information processing systems. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730</p>
<p>S Zellig, Harris, Distributional structure. Word. 195410</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Gml: Graph modelling language. Michael Himsolt, 1997University of Passau</p>
<p>Paul W Holland, Kathryn B Laskey, Samuel Leinhardt, Stochastic blockmodels: First steps. 19835</p>
<p>Promoting graph awareness in linearized graph-to-text generation. Alexander Miserlis Hoyle, Ana MarasoviÄ, Noah A Smith, 10.18653/v1/2021.findings-acl.82Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in Neural Information Processing Systems. 202033</p>
<p>Yuntong Hu, Zheng Zhang, Liang Zhao, arXiv:2310.04944Beyond text: A deep dive into large language models' ability on understanding graph data. 2023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information through prompts, and why?. Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma, Transactions on Machine Learning Research. 2024</p>
<p>Description of language design. Martin Joos, The Journal of the Acoustical Society of America. 2261950</p>
<p>Pure transformers are powerful graph learners. Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong, Advances in Neural Information Processing Systems. 202235</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1603.088612016arXiv preprint</p>
<p>. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 2020</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBARTOnline. Association for Computational Linguistics</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. Chang Liu, Bo Wu, arXiv:2308.112242023arXiv preprint</p>
<p>Graphinstruct: Empowering large language models with graph understanding and reasoning capability. Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin, arXiv:2403.044832024arXiv preprint</p>
<p>The pagerank citation ranking: Bringing order to the web. Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, 1999Stanford InfoLabTechnical report</p>
<p>Book reviews: Text generation and systemic-functional linguistics: Experiences from English and Japanese. Terry Patten, Computational Linguistics. 1911993</p>
<p>Let your graph do the talking: Encoding structured data for llms. Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow, arXiv:2402.058622024arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Leveraging discourse structure for extractive meeting summarization. Virgile Rennard, Guokan Shang, Michalis Vazirgiannis, Julie Hunter, arXiv:2405.110552024Preprint</p>
<p>Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, 10.18653/v1/2021.nlp4convai-1.20Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI. the 3rd Workshop on Natural Language Processing for Conversational AIOnline. Association for Computational Linguistics2021Hinrich SchÃ¼tze, and Iryna Gurevych</p>
<p>Network structure and minimum degree. Stephen B Seidman, 10.1016/0378-8733(83)90028-XSocial networks. 531983</p>
<p>Graph reasoning with large language models via pseudo-code prompting. Konstantinos Skianis, Giannis Nikolentzos, Michalis Vazirgiannis, arXiv:2409.179062024arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, Advances in Neural Information Processing Systems. 202436</p>
<p>Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, Ambuj K Singh, arXiv:2406.16176Grapheval2000: Benchmarking and improving large language models on graph datasets. 2024arXiv preprint</p>
<p>Multimodal learning with transformers: A survey. Peng Xu, Xiatian Zhu, David A Clifton, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, arXiv:2501.15383Qwen2.5-1m technical report. 2025arXiv preprint</p>
<p>Zhilin Yang, William W Cohen, Ruslan Salakhutdinov, arXiv:1811.05868Benchmarking graph neural networks. 2018arXiv preprint</p>
<p>Exploring the potential of large language models in graph generation. Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei, arXiv:2403.143582024arXiv preprint</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023arXiv preprint</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen, arXiv:2306.13549A survey on multimodal large language models. 2023arXiv preprint</p>
<p>Do transformers really perform badly for graph representation?. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Gracore: Benchmarking graph comprehension and complex reasoning in large language models. Zike Yuan, Ming Liu, Hui Wang, Bing Qin, arXiv:2407.029362024arXiv preprint</p>
<p>Llm4dyg: Can large language models solve problems on dynamic graphs?. Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, Wenwu Zhu, arXiv:2310.17110Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. 2023a. 2023barXiv preprintNeurIPS 2023 Workshop: New Frontiers in Graph Learning</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2310.01089Graphtext: Graph reasoning in text space. 2023arXiv preprint</p>
<p>Node Counting Max Degree Node Degree Edge Existence Diameter Shortest Path Path Existence Motifs' Shape Default Labels CoreNumber. 25.06 / 37.41 17.87 / 16.67 60.9 / 62.45</p>
<p>35.77 / 49.38 24.40 / 26.88 63.43 / 56.42PageRank. </p>
<p>24.6 / 32.21 23.23 / 17.84 49.33 / 41.35 56.4 / 52.72 10.67 / 11.04 29.33 / 19.47LG{Degree}. </p>
<p>Node Counting Max Degree Node Degree Edge Existence Diameter Shortest Path Path Existence Default Labels CoreNumber. 61.23 / 38.52 22.4 / 34.35 52.72 / 44.51</p>
<p>67.73 / 38.38 22.66 / 30.09 47.63 / 33.75 71.87 / 61.71 1.27 / 11.68 40.44 / 47.07LG{CoreNumber}. </p>            </div>
        </div>

    </div>
</body>
</html>
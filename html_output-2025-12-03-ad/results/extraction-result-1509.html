<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1509 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1509</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1509</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-1f2adde68261616afb057abea3e19a28da4993e2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1f2adde68261616afb057abea3e19a28da4993e2" target="_blank">Bayesian Analysis of Multifidelity Computer Models With Local Features and Nonnested Experimental Designs: Application to the WRF Model</a></p>
                <p><strong>Paper Venue:</strong> Technometrics</p>
                <p><strong>Paper TL;DR:</strong> An efficient imputation mechanism is introduced which allows the practical implementation of co-kriging when the experimental design is nonhierarchically nested by enabling the specification of semiconjugate priors.</p>
                <p><strong>Paper Abstract:</strong> ABSTRACT Motivated by a multi-fidelity Weather Research and Forecasting (WRF) climate model application where the available simulations are not generated based on hierarchically nested experimental design, we develop a new co-kriging procedure called augmented Bayesian treed co-kriging. The proposed procedure extends the scope of co-kriging in two major ways. We introduce a binary treed partition latent process in the multifidelity setting to account for nonstationary and potential discontinuities in the model outputs at different fidelity levels. Moreover, we introduce an efficient imputation mechanism which allows the practical implementation of co-kriging when the experimental design is nonhierarchically nested by enabling the specification of semiconjugate priors. Our imputation strategy allows the design of an efficient reversible jump Markov chain Monte Carlo implementation that involves collapsed blocks and direct simulation from conditional distributions. We develop the Monte Carlo recursive emulator which provides a Monte Carlo proxy for the full predictive distribution of the model output at each fidelity level, in a computationally feasible manner. The performance of our method is demonstrated on benchmark examples and used for the analysis of a large-scale climate modeling application which involves the WRF model.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1509.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1509.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WRF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weather Research and Forecasting (WRF) regional climate model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art, physics-based regional atmospheric/climate simulator used here to generate multi-resolution runs (two grid spacings) for emulation; used as the expensive, high‑fidelity computer model whose outputs (average precipitation) are emulated by the proposed Bayesian multi‑fidelity emulator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>WRF (Weather Research and Forecasting regional climate model)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Physics-based regional atmospheric/climate simulator; in this paper WRF is run with the Rapid Radiative Transfer Model for GCM radiation package and the Kain–Fritsch convective parametrisation, producing expensive deterministic simulations of precipitation at different spatial resolutions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>climate / fluid dynamics (atmospheric physics, thermodynamics-related)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Two fidelity levels by spatial resolution: coarser (25 km grid spacing) and finer (12.5 km grid spacing); higher fidelity corresponds to finer grid spacing (12.5 km).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Fidelity differences arise from grid spacing (spatial resolution) and possibly representation of sub-grid physics via parameterizations (uses Rapid Radiative Transfer Model, Kain–Fritsch convection); higher fidelity (finer grid) resolves smaller-scale features; underlying physics models are the same but discretization and effective resolution differ.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Augmented Bayesian Treed co-Kriging (ABTCK) emulator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A fully Bayesian multi‑fidelity emulator combining autoregressive co‑kriging, Gaussian process components, and a binary treed partition (nonstationary local GP pieces) with an augmentation/imputation mechanism to handle non‑nested designs; inference via RJ‑MCMC and Monte Carlo recursive emulation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulate/predict WRF outputs (average precipitation) across fidelity levels and learn discrepancy functions between lower and higher fidelity simulations as a function of input parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Prediction of higher-fidelity simulator outputs (finer-grid WRF runs) from combined lower- and higher-fidelity simulation data; no transfer to physical experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The paper uses two WRF resolutions (12.5 km and 25 km) and explicitly models the discrepancies due to grid spacing; it emphasizes that fidelity increases with finer grid spacing and that discrepancies may depend on input parameters of the convective parametrisation, but no numeric transfer/performance metrics for the WRF case are reported in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper states that fidelity increases with finer grid spacing (12.5 km is higher fidelity than 25 km) and that modeling discrepancies across fidelity levels is necessary; it does not provide a quantitative statement about the minimum fidelity required for successful transfer to real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases are reported for WRF in the provided text; the paper highlights practical issues (high cost of high-fidelity runs, and non‑nested experimental designs) motivating the augmentation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Analysis of Multifidelity Computer Models With Local Features and Nonnested Experimental Designs: Application to the WRF Model', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1509.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1509.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heated-block FEM models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three FEM-based heat-transfer computer models for a heated metal block (multi-fidelity PDE simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark consisting of three elliptic PDE-based simulators of steady-state temperature in a metal block, constructed at three fidelity levels by varying the spatially dependent thermal conductivity function; outputs are produced via a finite-element method (FEM) solver on a 24119-node mesh.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Custom FEM heat-transfer simulators (three fidelity variants)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Set of deterministic PDE solvers (FEM) for the steady-state heat equation -∇·(c^(j)(x) ∇u^(j)(x)) = f(x) on a 2D block with mixed Dirichlet/Neumann BCs; three variants differ in the spatial thermal conductivity c^(j)(x) to represent increasing model fidelity. The FEM domain discretization used has 24,119 nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>thermodynamics / heat transfer (partial differential equations, computational mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Three discrete fidelity levels: low-fidelity model with constant conductivity c^(1)(x)=1; medium-fidelity c^(2)(x)=exp(1.5 sin(3.33 π x2)) applied only for x2<1.8 (piecewise); high-fidelity c^(3)(x)=exp(1.5 sin(3.33 π x2)) applied everywhere. Higher fidelity models include more spatial variability in thermal conductivity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>High-fidelity model includes full spatial dependence of conductivity (captures higher-frequency variations in x2); medium fidelity includes spatial variation only in part of the domain (piecewise approximation); low fidelity uses constant conductivity (omits spatial heterogeneity). All are solved with the same FEM solver/resolution (24119 nodes) but differ in modeled physics (c(x) specification) leading to discontinuities in solution at material boundaries (e.g. x1=0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Augmented Bayesian Treed co-Kriging (ABTCK) / co-kriging emulators</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ABTCK emulators combining local GP pieces (treed partition) and autoregressive co‑kriging across fidelity levels with augmentation for non‑nested designs; inference by RJ‑MCMC and Monte Carlo recursive predictions (Student‑T predictive processes).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulate the highest-fidelity steady-state temperature field using outputs from lower- and higher-fidelity PDE solvers; capture discontinuities and nonstationarity across the domain and across fidelity levels.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Mapping information from lower-fidelity PDE models (and intermediate fidelity) to predict high-fidelity PDE outputs (i.e., transfer from simpler conductivity models to the more realistic conductivity model).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The benchmark explicitly constructs and compares three fidelity levels (c^(1), c^(2), c^(3)) to exercise the multifidelity emulator; the paper emphasizes discontinuities and nonstationarity that require the treed/local modeling but does not report numeric transfer metrics in the provided excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit minimum-fidelity requirement is stated; the example is used to demonstrate the need to model localized nonstationarity and discontinuities when fusing models of differing fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>The presence of a sharp discontinuity (at x1=0.5) and piecewise conductivity in the medium-fidelity model is noted as a challenge motivating the treed/local approach; no explicit numeric failure is reported in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Analysis of Multifidelity Computer Models With Local Features and Nonnested Experimental Designs: Application to the WRF Model', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1509.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1509.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic two-level functions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic low- and high-fidelity functions y1(x) and y2(x) used as a two‑level simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constructed synthetic two-fidelity test problem where y1(x) and y2(x) are deterministic analytic functions (domain [-2,6]^2) used to mimic lower- and higher-fidelity simulators; used to compare ABTCK and competing co‑kriging approaches under non‑nested designs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Synthetic analytic two-level simulator (functions y1 and y2)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Closed-form analytic functions defining a low-fidelity output y1(x) and a more accurate but expensive high-fidelity output y2(x). They differ by scale and additive components and exhibit input-dependent discrepancies including localized sinusoidal features; used to generate training sets via Latin Hypercube Sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>methodological benchmark (function approximation / surrogate modeling) — representative of model discrepancy problems in physical sciences (e.g., thermodynamics-like behavior in 2D inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Two fidelity levels: y1 is lower fidelity (faster/cheaper surrogate), y2 is higher fidelity (more accurate); differences include multiplicative scale changes and distinct small-scale sinusoidal features.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Differences in amplitude and small-scale oscillatory structure; the discrepancy between levels is nonstationary across the input domain and changes value spatially (so cannot be captured by a simple constant scalar relationship).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Augmented Bayesian Treed co-Kriging (ABTCK) and baseline emulators (ABCK, NBCK, K&O, ZBCK, HFGP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ABTCK: treed autoregressive co‑kriging with augmentation for non‑nested designs; ABCK: same without treed partitioning; competing methods include standard high-fidelity GP (HFGP), nested Bayesian co-kriging (NBCK), Kennedy & O'Hagan co-kriging (K&O), and Zertuche's approach (ZBCK).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn to predict the high-fidelity function y2(x) using a large set of low-fidelity evaluations (n1=120) and a smaller non-nested set of high-fidelity evaluations (n2=30); capture input-dependent discrepancies between fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Reported quantitative predictive performance (MSPE on a 100x100 grid): ABTCK MSPE = 0.0006; ABCK = 0.0221; ZBCK = 0.0339; NBCK = 0.0602; K&O = 0.0550; HFGP = 0.107 (these are mean squared predictive error values reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Transfer/learn mapping from low-fidelity to high-fidelity analytic function (i.e., predict y2 across domain using mixed-fidelity training data); the 'transfer' here is multi-fidelity learning to the higher-fidelity simulator function.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>ABTCK achieved MSPE = 0.0006 on the held-out grid (best among compared methods), indicating very strong transfer/emulation performance for this synthetic benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Two fidelity levels were compared (y1 vs y2). ABTCK (which models input-dependent discrepancies and nonstationarity) substantially outperformed the other methods (orders-of-magnitude lower MSPE), indicating that modeling spatially varying discrepancy and using treed partitioning improved transfer to the high-fidelity target.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The authors note that non-nested designs may lead to more accurate emulations in some cases (possibly because they allow learning from different input locations at different fidelities), but they explicitly state that a detailed theoretical investigation is out of scope; no strict minimal fidelity specification is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Baseline methods that do not model nonstationary, input-dependent discrepancies (e.g., HFGP, NBCK, K&O, ZBCK) perform substantially worse on this benchmark; ZBCK introduced bias on non-nested designs and had higher MSPE. No absolute failure (model collapse) is reported, but predictive accuracy degrades substantially for less flexible methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Analysis of Multifidelity Computer Models With Local Features and Nonnested Experimental Designs: Application to the WRF Model', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Predicting the output from a complex computer code when fast approximations are available <em>(Rating: 2)</em></li>
                <li>Multi-fidelity Gaussian process regression for computer experiments <em>(Rating: 2)</em></li>
                <li>A description of the Advanced Research WRF Version 3 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1509",
    "paper_id": "paper-1f2adde68261616afb057abea3e19a28da4993e2",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "WRF",
            "name_full": "Weather Research and Forecasting (WRF) regional climate model",
            "brief_description": "A state-of-the-art, physics-based regional atmospheric/climate simulator used here to generate multi-resolution runs (two grid spacings) for emulation; used as the expensive, high‑fidelity computer model whose outputs (average precipitation) are emulated by the proposed Bayesian multi‑fidelity emulator.",
            "citation_title": "Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model",
            "mention_or_use": "use",
            "simulator_name": "WRF (Weather Research and Forecasting regional climate model)",
            "simulator_description": "Physics-based regional atmospheric/climate simulator; in this paper WRF is run with the Rapid Radiative Transfer Model for GCM radiation package and the Kain–Fritsch convective parametrisation, producing expensive deterministic simulations of precipitation at different spatial resolutions.",
            "scientific_domain": "climate / fluid dynamics (atmospheric physics, thermodynamics-related)",
            "fidelity_level": "Two fidelity levels by spatial resolution: coarser (25 km grid spacing) and finer (12.5 km grid spacing); higher fidelity corresponds to finer grid spacing (12.5 km).",
            "fidelity_characteristics": "Fidelity differences arise from grid spacing (spatial resolution) and possibly representation of sub-grid physics via parameterizations (uses Rapid Radiative Transfer Model, Kain–Fritsch convection); higher fidelity (finer grid) resolves smaller-scale features; underlying physics models are the same but discretization and effective resolution differ.",
            "model_or_agent_name": "Augmented Bayesian Treed co-Kriging (ABTCK) emulator",
            "model_description": "A fully Bayesian multi‑fidelity emulator combining autoregressive co‑kriging, Gaussian process components, and a binary treed partition (nonstationary local GP pieces) with an augmentation/imputation mechanism to handle non‑nested designs; inference via RJ‑MCMC and Monte Carlo recursive emulation.",
            "reasoning_task": "Emulate/predict WRF outputs (average precipitation) across fidelity levels and learn discrepancy functions between lower and higher fidelity simulations as a function of input parameters.",
            "training_performance": null,
            "transfer_target": "Prediction of higher-fidelity simulator outputs (finer-grid WRF runs) from combined lower- and higher-fidelity simulation data; no transfer to physical experiments reported.",
            "transfer_performance": null,
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "The paper uses two WRF resolutions (12.5 km and 25 km) and explicitly models the discrepancies due to grid spacing; it emphasizes that fidelity increases with finer grid spacing and that discrepancies may depend on input parameters of the convective parametrisation, but no numeric transfer/performance metrics for the WRF case are reported in the provided text.",
            "minimal_fidelity_discussion": "The paper states that fidelity increases with finer grid spacing (12.5 km is higher fidelity than 25 km) and that modeling discrepancies across fidelity levels is necessary; it does not provide a quantitative statement about the minimum fidelity required for successful transfer to real experiments.",
            "failure_cases": "No explicit failure cases are reported for WRF in the provided text; the paper highlights practical issues (high cost of high-fidelity runs, and non‑nested experimental designs) motivating the augmentation approach.",
            "uuid": "e1509.0",
            "source_info": {
                "paper_title": "Bayesian Analysis of Multifidelity Computer Models With Local Features and Nonnested Experimental Designs: Application to the WRF Model",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Heated-block FEM models",
            "name_full": "Three FEM-based heat-transfer computer models for a heated metal block (multi-fidelity PDE simulators)",
            "brief_description": "A benchmark consisting of three elliptic PDE-based simulators of steady-state temperature in a metal block, constructed at three fidelity levels by varying the spatially dependent thermal conductivity function; outputs are produced via a finite-element method (FEM) solver on a 24119-node mesh.",
            "citation_title": "Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model",
            "mention_or_use": "use",
            "simulator_name": "Custom FEM heat-transfer simulators (three fidelity variants)",
            "simulator_description": "Set of deterministic PDE solvers (FEM) for the steady-state heat equation -∇·(c^(j)(x) ∇u^(j)(x)) = f(x) on a 2D block with mixed Dirichlet/Neumann BCs; three variants differ in the spatial thermal conductivity c^(j)(x) to represent increasing model fidelity. The FEM domain discretization used has 24,119 nodes.",
            "scientific_domain": "thermodynamics / heat transfer (partial differential equations, computational mechanics)",
            "fidelity_level": "Three discrete fidelity levels: low-fidelity model with constant conductivity c^(1)(x)=1; medium-fidelity c^(2)(x)=exp(1.5 sin(3.33 π x2)) applied only for x2&lt;1.8 (piecewise); high-fidelity c^(3)(x)=exp(1.5 sin(3.33 π x2)) applied everywhere. Higher fidelity models include more spatial variability in thermal conductivity.",
            "fidelity_characteristics": "High-fidelity model includes full spatial dependence of conductivity (captures higher-frequency variations in x2); medium fidelity includes spatial variation only in part of the domain (piecewise approximation); low fidelity uses constant conductivity (omits spatial heterogeneity). All are solved with the same FEM solver/resolution (24119 nodes) but differ in modeled physics (c(x) specification) leading to discontinuities in solution at material boundaries (e.g. x1=0.5).",
            "model_or_agent_name": "Augmented Bayesian Treed co-Kriging (ABTCK) / co-kriging emulators",
            "model_description": "ABTCK emulators combining local GP pieces (treed partition) and autoregressive co‑kriging across fidelity levels with augmentation for non‑nested designs; inference by RJ‑MCMC and Monte Carlo recursive predictions (Student‑T predictive processes).",
            "reasoning_task": "Emulate the highest-fidelity steady-state temperature field using outputs from lower- and higher-fidelity PDE solvers; capture discontinuities and nonstationarity across the domain and across fidelity levels.",
            "training_performance": null,
            "transfer_target": "Mapping information from lower-fidelity PDE models (and intermediate fidelity) to predict high-fidelity PDE outputs (i.e., transfer from simpler conductivity models to the more realistic conductivity model).",
            "transfer_performance": null,
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "The benchmark explicitly constructs and compares three fidelity levels (c^(1), c^(2), c^(3)) to exercise the multifidelity emulator; the paper emphasizes discontinuities and nonstationarity that require the treed/local modeling but does not report numeric transfer metrics in the provided excerpt.",
            "minimal_fidelity_discussion": "No explicit minimum-fidelity requirement is stated; the example is used to demonstrate the need to model localized nonstationarity and discontinuities when fusing models of differing fidelity.",
            "failure_cases": "The presence of a sharp discontinuity (at x1=0.5) and piecewise conductivity in the medium-fidelity model is noted as a challenge motivating the treed/local approach; no explicit numeric failure is reported in the provided text.",
            "uuid": "e1509.1",
            "source_info": {
                "paper_title": "Bayesian Analysis of Multifidelity Computer Models With Local Features and Nonnested Experimental Designs: Application to the WRF Model",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Synthetic two-level functions",
            "name_full": "Synthetic low- and high-fidelity functions y1(x) and y2(x) used as a two‑level simulator",
            "brief_description": "A constructed synthetic two-fidelity test problem where y1(x) and y2(x) are deterministic analytic functions (domain [-2,6]^2) used to mimic lower- and higher-fidelity simulators; used to compare ABTCK and competing co‑kriging approaches under non‑nested designs.",
            "citation_title": "Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model",
            "mention_or_use": "use",
            "simulator_name": "Synthetic analytic two-level simulator (functions y1 and y2)",
            "simulator_description": "Closed-form analytic functions defining a low-fidelity output y1(x) and a more accurate but expensive high-fidelity output y2(x). They differ by scale and additive components and exhibit input-dependent discrepancies including localized sinusoidal features; used to generate training sets via Latin Hypercube Sampling.",
            "scientific_domain": "methodological benchmark (function approximation / surrogate modeling) — representative of model discrepancy problems in physical sciences (e.g., thermodynamics-like behavior in 2D inputs)",
            "fidelity_level": "Two fidelity levels: y1 is lower fidelity (faster/cheaper surrogate), y2 is higher fidelity (more accurate); differences include multiplicative scale changes and distinct small-scale sinusoidal features.",
            "fidelity_characteristics": "Differences in amplitude and small-scale oscillatory structure; the discrepancy between levels is nonstationary across the input domain and changes value spatially (so cannot be captured by a simple constant scalar relationship).",
            "model_or_agent_name": "Augmented Bayesian Treed co-Kriging (ABTCK) and baseline emulators (ABCK, NBCK, K&O, ZBCK, HFGP)",
            "model_description": "ABTCK: treed autoregressive co‑kriging with augmentation for non‑nested designs; ABCK: same without treed partitioning; competing methods include standard high-fidelity GP (HFGP), nested Bayesian co-kriging (NBCK), Kennedy & O'Hagan co-kriging (K&O), and Zertuche's approach (ZBCK).",
            "reasoning_task": "Learn to predict the high-fidelity function y2(x) using a large set of low-fidelity evaluations (n1=120) and a smaller non-nested set of high-fidelity evaluations (n2=30); capture input-dependent discrepancies between fidelities.",
            "training_performance": "Reported quantitative predictive performance (MSPE on a 100x100 grid): ABTCK MSPE = 0.0006; ABCK = 0.0221; ZBCK = 0.0339; NBCK = 0.0602; K&O = 0.0550; HFGP = 0.107 (these are mean squared predictive error values reported in Table 1).",
            "transfer_target": "Transfer/learn mapping from low-fidelity to high-fidelity analytic function (i.e., predict y2 across domain using mixed-fidelity training data); the 'transfer' here is multi-fidelity learning to the higher-fidelity simulator function.",
            "transfer_performance": "ABTCK achieved MSPE = 0.0006 on the held-out grid (best among compared methods), indicating very strong transfer/emulation performance for this synthetic benchmark.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Two fidelity levels were compared (y1 vs y2). ABTCK (which models input-dependent discrepancies and nonstationarity) substantially outperformed the other methods (orders-of-magnitude lower MSPE), indicating that modeling spatially varying discrepancy and using treed partitioning improved transfer to the high-fidelity target.",
            "minimal_fidelity_discussion": "The authors note that non-nested designs may lead to more accurate emulations in some cases (possibly because they allow learning from different input locations at different fidelities), but they explicitly state that a detailed theoretical investigation is out of scope; no strict minimal fidelity specification is provided.",
            "failure_cases": "Baseline methods that do not model nonstationary, input-dependent discrepancies (e.g., HFGP, NBCK, K&O, ZBCK) perform substantially worse on this benchmark; ZBCK introduced bias on non-nested designs and had higher MSPE. No absolute failure (model collapse) is reported, but predictive accuracy degrades substantially for less flexible methods.",
            "uuid": "e1509.2",
            "source_info": {
                "paper_title": "Bayesian Analysis of Multifidelity Computer Models With Local Features and Nonnested Experimental Designs: Application to the WRF Model",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Predicting the output from a complex computer code when fast approximations are available",
            "rating": 2
        },
        {
            "paper_title": "Multi-fidelity Gaussian process regression for computer experiments",
            "rating": 2
        },
        {
            "paper_title": "A description of the Advanced Research WRF Version 3",
            "rating": 2
        }
    ],
    "cost": 0.015986499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model</h1>
<p>Bledar A. Konomi <em><br>Department of Mathematical Sciences, University of Cincinnati, USA and<br>Georgios Karagiannis </em><br>Department of Mathematical Sciences, Durham University, UK</p>
<p>October 2, 2020</p>
<h4>Abstract</h4>
<p>Motivated by a multi-fidelity Weather Research and Forecasting (WRF) climate model application where the available simulations are not generated based on hierarchically nested experimental design, we develop a new ko-criging procedure called Augmented Bayesian Treed Co-Kriging. The proposed procedure extends the scope of co-kriging in two major ways. We introduce a binary treed partition latent process in the multifidelity setting to account for non-stationary and potential discontinuities in the model outputs at different fidelity levels. Moreover, we introduce an efficient imputation mechanism which allows the practical implementation of co-kriging when the experimental design is non-hierarchically nested by enabling the specification of semi-conjugate priors. Our imputation strategy allows the design of an efficient RJMCMC implementation that involves collapsed blocks and direct simulation from conditional distributions. We develop the Monte Carlo recursive emulator which provides a Monte Carlo proxy for the full predictive distribution of the model output at each fidelity level, in a computationally feasible manner. The performance of our method is demonstrated on benchmark examples and used for the analysis of a large-scale climate modeling application which involves the WRF model. Supplementary materials are available online.</p>
<p>Keywords: Augmented hierarchically nested design, Binary treed partition, Gaussian process, Collapsed MCMC</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Understanding the behavior as well as the underlying mechanisms of real systems such as physical procedures is central to many applications such as weather forecasting. Direct investigation of the real system is often impossible due to limited resources, and hence it is simulated by computer models aiming at reproducing the real system's behavior with high accuracy. Our case study involves an expensive computer model which requires a significant amount of resources to perform a single run; and hence, only a limited number of simulations can be performed. Gaussian process (GP) regression models (Sacks et al., 1989) are statistical models that allow the emulation of the computer model output by using only a few runs of the computer model.</p>
<p>Computer models are often able to run at different levels of fidelity, sophistication, or resolution. As high fidelity runs are usually more expensive, collecting data by simulating the model at different fidelity levels is preferred for a given budget of resources. Statistical inference is preferable to be made against the whole simulated data set, and thus account for across fidelity level dependence, rather than against simulated data sets associated with individual fidelity levels (Kennedy and O'Hagan, 2000). Assume there are available $S$ deterministic computer models $\left{\mathfrak{C}<em t="1">{t}\right}</em>$. Autoregressive co-kriging assumes an autoregressive model}^{S}$ aiming at simulating the same real system. The models are ordered by ascending fidelity level $t$. Let $y_{t}(\boldsymbol{x}): \mathcal{X} \rightarrow \mathbb{R}$ denote the output function of the computer model $\mathscr{C}_{t}$ with respect to a $m$-dimensional input $x \in \mathcal{X</p>
<p>$$
y_{t}(\boldsymbol{x})=\xi_{t-1}(\boldsymbol{x}) y_{t-1}(\boldsymbol{x})+\delta_{t}(\boldsymbol{x}) \quad \text { for } x \in \mathcal{X}, t=2, \ldots, S
$$</p>
<p>where $y_{t-1}(\boldsymbol{x}), \delta_{t}(\boldsymbol{x}), \xi_{t-1}(\boldsymbol{x})$ are independent unknown functions a priori modeled as Gaussian processes. Here, $\delta_{t}(\cdot)$ is the location discrepancy function (representing a local adjustment from $\mathfrak{C}<em t="t">{t-1}$ to $\mathfrak{C}</em>(\cdot)$ is the scale discrepancy (representing a scale change from}$ ), and $\xi_{t</p>
<p>$\mathfrak{C}<em t="t">{t-1}$ to $\mathfrak{C}</em>}$ for $t=1, \ldots, S$ ). Discrepancy terms, $\left{\delta_{t}(\cdot)\right}$ and $\left{\xi_{t}(\cdot)\right}$, can be thought of as accounting for 'missing' or 'misrepresented' physical properties in the lower fidelity computer model $\mathfrak{C<em t="t">{t-1}$ with respect to the higher one $\mathfrak{C}</em>$ in (1) to impose stationarity throughout $t$.}$. Model (1) is induced by the Markovian condition $\operatorname{cov}\left(y_{t}(\boldsymbol{x}), y_{t-1}\left(\boldsymbol{x}^{\prime}\right) \mid y_{t-1}(\boldsymbol{x})\right)=0$ of the heuristic 'there is nothing more to learn about $y_{t}(\boldsymbol{x})$ from $y_{t-1}\left(\boldsymbol{x}^{\prime}\right)$ for any $\boldsymbol{x}^{\prime} \neq \boldsymbol{x}$ given $y_{t-1}(\boldsymbol{x})$ is known ' which is broadly accepted in computer experiments (OṶagan, 1998). Originally, Kennedy and O'Hagan (2000) considered a constant $\xi_{t-1}=\xi_{t-1}(\boldsymbol{x})$ for all $\boldsymbol{x} \in \mathcal{X</p>
<p>A number of important variations of the autoregressive co-kriging have been proposed. Qian et al. (2005) used $\xi_{t-1}(x)$ as a polynomial expansion to model more general autoregressive dependencies. Based on this, Qian and Wu (2008) considered the scale discrepancy as a function of the input space by casting it as a GP. In practice, their approach is applicable to problems with only two fidelity levels, as the computational overhead caused by using more fidelity levels is increased dramatically. Le Gratiet (2013); Le Gratiet and Garnier (2014) modeled the scale discrepancy as an expansion of bases defined on the inputs, and presented conditional conjugate priors which lead to standard conditional posterior distributions for the unknown coefficients of the expansion. However, casting the scale discrepancy as a basis expansion may require an undesirably large number of bases in order to explain small scale discrepancies; while it cannot represent discontinuities and sudden changes. Furthermore, this may aggravate non-identifiability between the scale and additive discrepancies. Perdikaris et al. (2015) proposed a machine learning framework, which uses sparse precision matrices of Gaussian-Markov random fields introduced by Lindgren et al. (2011). This facilitates computations that leverage on the sparsity of the resulting discrete operators. Perdikaris et al. (2017) relaxed the auto-regressive structure by using deep learning ideas, however the computational demands to train the model are significantly increased. The aforementioned developments require hierarchically nested experimental designs for computational reasons, otherwise the computational demands become impractical. This constraint</p>
<p>prevents their practical implementation on a number of important real world problems where the available data set is not based on such nested designs.</p>
<p>We are motivated by a real world application that involves the Weather Research and Forecasting (WRF) regional climate model (Skamarock et al., 2008). WRF is an expensive computer model that allows the use of different resolutions leading to different fidelity levels. We consider the WRF with the Rapid Radiative Transfer Model for General Circulation Model (Pincus et al., 2003), with the Kain-Fritsch convective parametrisation scheme (KF CPS) (Kain, 2004), and with five input parameters, while we are interested in the average precipitation as an output. The available simulations were generated by running WRF at two resolution levels, 12.5 km and 25 km grid spacing. The fidelity of the simulations increases when the grid spacing gets finer. The available simulations have not been generated based on a hierarchically nested design, while it is not possible to re-run the expensive computer model in our facilities and generate simulations based on such a design due to the high computational cost required. The aforesaid co-kriging methods cannot be implemented directly due to the lack of nested design, and hence new developments are required. We are interested in designing an accurate emulator that aggregates all the available simulations as well as represents features of the WRF. Previous research in (Yan et al., 2014; Yang et al., 2012) suggested that discrepancies between the two levels may depend on the five inputs of the KF CPS. Interest also lies in better understanding how different grid spacing affects the discrepancies in WRF with respect to the input parameters. Existing co-kriging methods do not model/account for such behaviors, thus suitable extensions must be introduced.</p>
<p>We propose the Augmented Bayesian Treed co-kriging (ABTCK); a fully Bayesian method for building multifidelity emulators of computer models that extends the scope of co-kriging mainly in two ways. The proposed method is able to address applications where the available training data set has not necessarily been generated according to a hierarchically nested experimental design. To achieve this, we introduce a suitable imputation mechanism that</p>
<p>augments the original data set with uncertain quantities which can be thought of as missing data from a complete data set generated based on an hierarchically nested design. The proposed imputation allows the specification of conditional conjugate priors, and analytic integration of a large number of dimensions from the posterior. A different remedy for nonhierarchically nested designs is studied in the Thesis of Zertuche (2015), however, unlike our approach, that approach leads to an approximation of the posterior distribution while it is concentrated to only two fidelity levels. Moreover, our method is able to account for nonstationary, and possible discontinuities. This is achieved by suitably specifying the statistical model as a combination of computationally convenient and GP regression models by using a binary treed partition which a priori follows a process similar to (Chipman et al., 1998; Gramacy and Lee, 2008). The additional flexibility of the proposed model aims at producing more accurate predictions as well as providing an insight of the model discrepancies. To facilitate inference, we propose a reversible jump Markov chain Monte Carlo (RJ-MCMC) implementation, tailored to the proposed model, that involves an efficient MCMC sampler which operates on the joint space of the missing data and the parameters, and consists of collapsed blocks. Due to the augmentation, the MCMC loop consists of local RJ updates operating on a lower dimensional state space and producing more acceptable proposals, and a block simulating the missing data directly from the conditionals. Finally, we propose the Monte Carlo recursive emulator, as an alternative to those in (Kennedy and O'Hagan, 2000; Le Gratiet and Garnier, 2014; Le Gratiet, 2013), which is able to provide fully Bayesian posterior predictive inference even with non-nested designs while keeping the computational cost lower than the others.</p>
<p>The rest of the paper is organized as follows. In Section 2, we present the proposed procedure; in Section 3, we provide numerical comparisons with other methods; and in Section 4, we implement our procedure for the analysis of the WRF model. Conclusions are summarised in Section 5.</p>
<h1>2 The Augmented Bayesian Treed co-Kriging</h1>
<p>We describe the development of our Augmented Bayesian treed co-kriging model (ABTCK) which extends the scope of co-kriging to applications with non-nested designs and/or nonstationary model outputs. A schematic is available in Supplementary Section S.1.</p>
<h3>2.1 Treed auto-regressive co-kriging model</h3>
<p>To account for non-stationarity we consider an unknown partition $\left{\mathcal{X}<em k="1">{k}\right}</em>$, whose sub-regions are assumed to be homogeneous in the sense that a co-kriging model (1) can be defined independently at each sub-region, i.e.}^{K}$ of the input space $\mathcal{X</p>
<p>$$
y_{k, t}(\boldsymbol{x})=\xi_{k, t-1}(\boldsymbol{x}) y_{k, t-1}(\boldsymbol{x})+\delta_{k, t}(\boldsymbol{x}) \quad \text { for } \boldsymbol{x} \in \mathcal{X}_{k}, t=2, \ldots, S
$$</p>
<p>such that input dependencies are represented accurately enough by parameterizing the unknown scale discrepancies $\left{\xi_{k, t}(\boldsymbol{x})\right}$, location discrepancies $\left{\delta_{k, t}(\boldsymbol{x})\right}$, and output functions $\left{y_{k, 1}(\boldsymbol{x})\right}$ with computationally convenient forms.</p>
<p>We cast $\left{\mathcal{X}<em k="1">{k}\right}</em>}^{K}$ as a binary tree partition with rectangular sub-regions $\mathcal{X<em k="k">{k}:=\mathcal{X}</em>$, we use the binary tree process prior of Chipman et al. (1998) specified as}(\mathcal{T})$, for $k=1, \ldots, K(\mathcal{T})$, determined by a binary tree $\mathcal{T}$. This specification adds structure to the model for the sake of computational convenience, however it can still provide a reasonable approximation to the reality. Binary treed partitioning has been successfully used in other problems (Denison et al., 1998; Chipman et al., 1998; Gramacy and Lee, 2008; Pratola et al., 2017; Konomi et al., 2017; Karagiannis et al., 2017). To account for the uncertainty about $\mathcal{T</p>
<p>$$
\pi(\mathcal{T})=P_{\text {rule }}(\rho \mid v, \mathcal{T}) \prod_{v_{i} \in \mathcal{I}} P_{\text {split }}\left(v_{i}, \mathcal{T}\right) \prod_{v_{j} \in \mathcal{E}}\left(1-P_{\text {split }}\left(v_{j}, \mathcal{T}\right)\right)
$$</p>
<p>where $\mathcal{E}$ denotes the set of external nodes corresponding to sub-regions of the partition</p>
<p>$\left{\mathcal{X}<em _split="{split" _text="\text">{k}(\mathcal{T})\right}$ and $\mathcal{I}$ denotes the internal nodes. It describes a process where input space is recursively sub-divided by spiting a sub-region, one at a time, into two regions according to a probability low. In particular here, prior tree process $\mathcal{T}$ has origin denoting the whole input space $\mathcal{X}$, while each node $v \in \mathcal{T}$ represents a sub-region of the input space. Each node splits with probability $P</em>) \propto 1$ specifying that the dimension and the location of the split are drawn independently and randomly.}}(v, \mathcal{T})=\zeta\left(1+u_{v}\right)^{-d}$ where $u_{v}$ is the depth of $v \in \mathcal{T}, \zeta$ controls the balance of the shape of the tree, and $d$ controls the size of the tree. The splits are performed based on a random splitting rule $\rho$ which follows a distribution $P_{\text {rule }}(\rho \mid v, \mathcal{T</p>
<p>We specify mutually independent Gaussian processes (GP) priors for $y_{k, 1}(\cdot)$, and $\delta_{k, t}(\cdot)$</p>
<p>$$
\begin{aligned}
&amp; y_{k, 1}(\cdot) \mid \mathcal{T} \sim \operatorname{GP}\left(\mu_{1}\left(\cdot \mid \boldsymbol{\beta}<em 1="1" k_="k,">{k, 1}\right), \sigma</em>}^{2} R_{1}\left(\cdot, \cdot \mid \boldsymbol{\phi<em k_="k," t="t">{k, 1}\right)\right) \
&amp; \delta</em>}(\cdot) \mid \mathcal{T} \sim \operatorname{GP}\left(\mu_{t}\left(\cdot \mid \boldsymbol{\beta<em k_="k," t="t">{k, t}\right), \sigma</em> t=2, \ldots, S
\end{aligned}
$$}^{2} R_{t}\left(\cdot, \cdot \mid \boldsymbol{\phi}_{k, t}\right)\right), \text { for </p>
<p>for $k=1, \ldots, K$, to account for their uncertainty. Given a suitable partition $\left{\mathcal{X}<em k="1">{k}\right}</em>}^{K}$ for the model (2), we can use simple and computationally convenient functions to model $\mu_{t}\left(\cdot \mid \boldsymbol{\beta<em t="t">{k, t}\right), R</em>}\left(\cdot, \cdot \mid \boldsymbol{\phi<em k_="k," t="t">{k, t}\right)$, and $\xi</em>}(\boldsymbol{x})$. The mean functions are parametrized as basis expansions $\mu_{t}\left(\cdot \mid \boldsymbol{\beta<em t="t">{k, t}\right)=\boldsymbol{h}</em>}(\cdot)^{T} \boldsymbol{\beta<em t="t">{k, t}$, where $\boldsymbol{h}</em>}(\cdot)$ is a vector of basis functions and $\beta_{k, t}$ are vectors of coefficients, at fidelity level $t$, and sub-region $\mathcal{X<em t="t">{k}$. The GP mean parameterized as an expansion of basis functions allows modeling long scale variations along $x$ which facilitates the use of computationally convenient correlation functions modeling low scale variations in a similar manner to the standard GP regression as in (Ba and Joseph, 2012; Sang and Huang, 2012). We consider a the family of square exponential correlation functions in separable form $R</em>}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime} \mid \boldsymbol{\phi<em k_="k," t="t">{k, t}\right)=\exp \left(-\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{x}^{\prime}\right)^{\top} \operatorname{diag}\left(\boldsymbol{\phi}</em>}\right)\left(\boldsymbol{x}-\boldsymbol{x}^{\prime}\right)\right)$; more sophisticated ones can be used (Williams and Rasmussen, 2006). The unknown functions $\left{\xi_{k, t}(\boldsymbol{x})\right}$ are modeled as low degree basis expansions $\xi_{k, t}\left(\boldsymbol{x} \mid \boldsymbol{\gamma<em t="t">{k, t}\right)=\boldsymbol{w}</em>}(\boldsymbol{x})^{T} \boldsymbol{\gamma<em t="t">{k, t}$ where $\left{\boldsymbol{w}</em>}(\boldsymbol{x})\right}$ are polynomial bases and $\left{\boldsymbol{\gamma<em t="t">{k, t}\right}$ are uncertain coefficients. Modeling $\boldsymbol{\mu}</em>}\left(\cdot \mid \boldsymbol{\beta<em k_="k," t="t">{k, t}\right)$, and $\xi</em>)$ as basis expansions facilitates the specification of conjugate priors and leads to computational savings given a}(\boldsymbol{x</p>
<p>suitable treatment in the likelihood to be introduced in Section 2.2.</p>
<h1>2.2 Conditional conjugacy via augmentation</h1>
<p>We do not require the available experimental design to be hierarchically nested, unlike existing co-kriging methods (Kennedy and O'Hagan, 2000; Le Gratiet, 2013). Namely, if $\left{\boldsymbol{y}<em t="t">{t}, \mathscr{D}</em>}\right}$ denotes the available training data set with output values $y_{t} \in \mathbb{R}^{n_{t}}$ at the experimental design $\mathscr{D<em t="t">{t}$ of size $n</em>}$ at fidelity level $t=1, . ., S$, it may be $\mathscr{D<em t="t">{t+1} \nsubseteq \mathscr{D}</em>}$ for some $t$. This realistic generalization prevents the direct specification of priors conjugate to the Gaussian likelihood $f\left(\boldsymbol{y<em 1:="1:" S="S">{1: S} \mid \mathcal{T}, \boldsymbol{\sigma}</em>}^{2}, \boldsymbol{\phi<em 1:="1:" S="S">{1: S}, \boldsymbol{\beta}</em>}, \boldsymbol{\gamma<em t="t">{1: S-1}\right)$, and hence makes the Bayesian computations prohibitively expensive. In such cases, direct implementation of existing co-kriging methods would require the inversion of large covariance matrices with size $\sum</em>} n_{t} \times \sum_{t} n_{t}$ for the computation of the likelihood, and possibly the use of Metropolis-Hastings operations in high-dimensional state spaces which would lead to practically infeasible computations. The introduction of the binary partition exacerbates this issue as it increases the dimensionality of the posterior by introducing additional unknown parameters $\boldsymbol{\beta<em k_="k," t="t">{k, t}, \boldsymbol{\gamma}</em>$ associated to each sub-region; this necessitates the specification of conjugate priors.}, \sigma_{k, t}^{2}, \boldsymbol{\phi}_{k, t</p>
<p>We address this issue by properly imputing the observed data with uncertain quantities, that can be thought of as missing data of a hierarchically nested experimental design, able to induce a conditional independence that enables the specification of conjugate priors, facilitates tractability of posterior marginals and conditionals, and allows the design of efficient MCMC implementations, while it leads to the same Bayesian inference as if we had considered the original data set only.</p>
<p>Augmentation Let $\left{\boldsymbol{y}<em k_="k," t="t">{k, t}, \mathscr{D}</em>}\right}$ be the observed data set with output values $\boldsymbol{y<em t="t">{k, t}=y</em>}\left(\mathscr{D<em k_="k," t="t">{k, t}\right)$ and design $\mathscr{D}</em>}$ at sub-region $\mathcal{X<em k_="k," t="t">{k}$ and fidelity level $t$. Assume sets of points $\tilde{\mathscr{D}}</em>}$ and $\hat{\mathscr{D}<em S="S" k_="k,">{k, t}$ such that $\tilde{\mathscr{D}}</em>}=\mathscr{D<em S="S" k_="k,">{k, S}$ with $\hat{\mathscr{D}}</em>}=\emptyset$, and $\tilde{\mathscr{D}<em k_="k," t="t">{k, t}=\mathscr{D}</em>} \cup \hat{\mathscr{D}<em k_="k," t="t">{k, t}$ where $\hat{\mathscr{D}}</em>}=\tilde{\mathscr{D}<em k_="k," t="t">{k, t+1}-\mathscr{D}</em>}$ is defined as the relative complement of $\mathscr{D<em k_="k," t_1="t+1">{k, t}$ in $\tilde{\mathscr{D}}</em>$ for $t=S-1, \ldots, 1$. It is easy to</p>
<p>check that $\tilde{\mathscr{D}}<em j="t">{k, t}=\cup</em>}^{S} \mathscr{D<em k_="k," t="t">{k, j}$, and that $\left{\tilde{\mathscr{D}}</em>\right}<em k_="k," t="t">{t=1}^{S}$ is hierarchically nested; i.e. $\tilde{\mathscr{D}}</em>} \subseteq \tilde{\mathscr{D}<em k_="k," t="t">{k, t-1}$. By construction, $\left{\hat{\mathscr{D}}</em>}\right}$ is the smallest collection of sets of input points required to be added to the original design $\left{\mathscr{D<em k_="k," t="t">{k, t}\right}$ in order to obtain a hierarchically nested experimental design $\left{\hat{\mathscr{D}}</em>}\right}$. Let $\hat{\boldsymbol{y}<em t="t">{k, t}=y</em>}\left(\hat{\mathscr{D}<em k_="k," t="t">{k, t}\right)$ be the missing output values of the computer model at the corresponding input points in $\hat{\mathscr{D}}</em>}$. We refer to $\left{\hat{\boldsymbol{y}<em k_="k," t="t">{k, t}, \hat{\mathscr{D}}</em>}\right}$ as missing data set, and $\left{\hat{\boldsymbol{y}<em k_="k," t="t">{k, t}, \hat{\mathscr{D}}</em>}\right}$ as complete data set, where $\hat{\mathscr{D}<em k_="k," t="t">{k, t}$ is the complete experimental design, and $\hat{\boldsymbol{y}}</em>}=y_{t}\left(\hat{\mathscr{D}<em k_="k," t="t">{k, t}\right)$ are the output model values at input points in $\hat{\mathscr{D}}</em>}$. The number of input points at sub-region $\mathcal{X<em k_="k," t="t">{k}$ and fidelity level $t$ after augmentation is denoted as $\tilde{n}</em>\right|$.}=\left|\tilde{\mathscr{D}}_{k, t</p>
<p>The joint distribution of $\hat{\boldsymbol{y}}=\left(\tilde{y}_{k, t}\right)$ given the parameters $\left(\mathcal{T}, \boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}, \boldsymbol{\phi}\right)$ is</p>
<p>$$
f\left(\tilde{y} \mid \mathcal{T}, \boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}, \boldsymbol{\phi}\right)=\prod_{k=1}^{K} f_{k}\left(\hat{\boldsymbol{y}}<em 1="1" k_="k,">{k, 1} \mid \boldsymbol{\beta}</em>}, \sigma_{k, 1}^{2}, \boldsymbol{\phi<em t="2">{k, 1}\right) \prod</em>}^{S} f_{k}\left(\hat{\boldsymbol{y}<em k_="k," t-1="t-1">{k, t} \mid \hat{\boldsymbol{y}}</em>}, \boldsymbol{\beta<em k_="k," t-1="t-1">{k, t}, \boldsymbol{\gamma}</em>\right)
$$}, \sigma_{k, t}^{2}, \boldsymbol{\phi}_{k, t</p>
<p>where each conditional $f_{k}\left(\hat{\boldsymbol{y}}<em t-1="t-1">{k, t} \mid \ldots\right)$ is a Gaussian distribution with mean $\xi</em>}\left(\hat{\mathscr{D}<em k_="k," t-1="t-1">{k, t} \mid \boldsymbol{\gamma}</em>}\right) \circ$ $y_{k, t-1}\left(\hat{\mathscr{D}<em t="t">{k, t}\right)+\mu</em>}\left(\hat{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\beta}</em>}\right)$, and covariance $\sigma_{k, t}^{2} R_{t}\left(\hat{\mathscr{D}<em k_="k," t="t">{k, t}, \hat{\mathscr{D}}</em>} \mid \boldsymbol{\phi<em k_="k," t="t">{k, t}\right)$. Here, $\circ$ denotes the Hadamard product. The joint distribution of $\hat{\boldsymbol{y}}$ can be factorized as in (6) because the proposed augmentation artificially creates a hierarchically nested design which due to the Markovian condition of (2) induces the required conditional independence. The computation of the augmented likelihood (6) is broken down into that of $S$ Gaussian densities requiring the inversion of $\tilde{n}</em>} \times \tilde{n<em t="t">{k, t}$ covariance matrices. Otherwise, we would be unable to factorize (6) and we would be required to invert a larger covariance matrices with sizes $\sum</em>$.} n_{t} \times \sum_{t} n_{t</p>
<p>Priors To account for the uncertainty about unknowns $\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}, \boldsymbol{\phi}$, we specify a prior factorized as</p>
<p>$$
\pi\left(\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}, \boldsymbol{\phi} \mid \mathcal{T}\right)=\prod_{k=1}^{K} \pi\left(\boldsymbol{\beta}<em 1="1" k_="k,">{k, 1}, \sigma</em>}^{2} \mid \mathcal{T}\right) \pi\left(\boldsymbol{\phi<em t="2">{k, 1} \mid \mathcal{T}\right) \prod</em>}^{S} \pi\left(\boldsymbol{\beta<em k_="k," t-1="t-1">{k, t}, \boldsymbol{\gamma}</em>\right)
$$}, \sigma_{k, t}^{2} \mid \mathcal{T}\right) \pi\left(\boldsymbol{\phi}_{k, t} \mid \mathcal{T</p>
<p>We assign Normal-inverse-Gamma prior distributions on $\left(\beta, \gamma, \sigma^{2}\right)$ such as</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{\beta}<em 1="1" k_="k,">{k, 1} \mid \mathcal{T}, \sigma</em>}^{2} \sim \mathrm{~N<em 1="1">{p</em>}}\left(\boldsymbol{b<em 1="1" k_="k,">{1}, \sigma</em>}^{2} \boldsymbol{B<em 1="1" k_="k,">{1}\right) ; \quad \sigma</em>\right) ; \
&amp; \boldsymbol{\beta}}^{2} \mid \mathcal{T} \sim \operatorname{IG}\left(\lambda_{1}, \chi_{1<em k_="k," t-1="t-1">{k, t}, \boldsymbol{\gamma}</em>} \mid \mathcal{T}, \sigma_{k, t}^{2} \sim \mathrm{~N<em t="t">{p</em>}+q_{t-1}}\left(\left[\boldsymbol{b<em t-1="t-1">{t}, \boldsymbol{g}</em>}\right]^{\top}, \sigma_{k, t}^{2} \operatorname{diag}\left(\boldsymbol{B<em t-1="t-1">{t}, \boldsymbol{G}</em>\right) ;
\end{aligned}
$$}\right)^{\top}\right) ; \quad \sigma_{k, t}^{2} \mid \mathcal{T} \sim \operatorname{IG}\left(\lambda_{t}, \chi_{t</p>
<p>which are conjugate to the conditionals $f_{k}\left(\overline{\boldsymbol{y}}<em t="t">{k, t} \mid \ldots\right)$ in augmented likelihood (6). This allows the analytic marginalization of the posterior and leads to important computational benefits discussed in Section 2.3. Without augmentation, we would be unable to specify conjugate priors for the actual likelihood, and computations for learning $\left(\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}\right)$ would be impractical. Elicitation of the priors is performed according to (Oakley, 2002; Brynjarsdóttir and O'Hagan, 2014). Weakly informative priors are obtained by adjusting $\boldsymbol{b}</em>}, \boldsymbol{g<em t="t">{t-1}, \boldsymbol{B}</em>}^{-1}$ and $\boldsymbol{G<em t="t">{t}^{-1}$ to be close to zero as they place equal amount of prior mass above and below zero in $\mu</em>\right)\right}$ are proper priors chosen by the researcher.}(\cdot)$ and $\xi_{t}(\cdot)$, and $\lambda_{t} \rightarrow 1+\left(p_{t}+q_{t-1}\right) / 2$ for $t=2, \ldots, S$, and $\lambda_{1} \rightarrow 1+p_{1} / 2$. Here, $\left{\pi\left(\boldsymbol{\phi}_{k, t} \mid \mathcal{T</p>
<p>The posterior distribution of ABTCK model is</p>
<p>$$
\pi\left(\mathcal{T}, \boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{2}, \boldsymbol{\phi}, \hat{\boldsymbol{y}} \mid \boldsymbol{y}\right) \propto f\left(\hat{\boldsymbol{y}} \mid \boldsymbol{y}, \mathcal{T}, \boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{2}, \boldsymbol{\phi}\right) f\left(\boldsymbol{y} \mid \mathcal{T}, \boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{2}, \boldsymbol{\phi}\right) \pi\left(\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{2}, \boldsymbol{\phi} \mid \mathcal{T}\right) \pi(\mathcal{T})
$$</p>
<p>admits the posterior of interest $\pi\left(\mathcal{T}, \boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{2}, \boldsymbol{\phi} \mid \boldsymbol{y}\right)$ as marginal by construction, and hence leads to the same Bayesian analysis.</p>
<h1>2.3 Bayesian inference and computations</h1>
<p>We design a RJMCMC sampler, targeting the augmented posterior (8), that involves a random permutation scan of blocks updating $[\hat{\boldsymbol{y}} \mid \boldsymbol{y}, \boldsymbol{\phi}, \boldsymbol{\sigma}^{2}, \boldsymbol{\gamma}, \mathcal{T}],[\boldsymbol{\phi}, \mathcal{T} \mid \overline{\boldsymbol{y}}]$, and $\left[\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{2}, \boldsymbol{\phi} \mid \overline{\boldsymbol{y}}, \mathcal{T}\right]$. The blocks are collapsed to avoid undesired high Monte Carlo (MC) standard errors due to the originally high-dimensional sampling space (Liu, 1994). The sampler is computationally efficient as it breaks down the inversion of covariance matrices and involves parallel sam-</p>
<p>pling at different sub-regions $k$ and fidelity levels $t$. Details regarding the MCMC blocks are explained below.</p>
<p>Update $[\hat{\boldsymbol{y}} \mid \boldsymbol{y}, \boldsymbol{\phi}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}, \mathcal{T}] \quad$ The full conditional posterior of $\hat{\boldsymbol{y}}_{k, t}$, after integrating out $\boldsymbol{\beta}$ 's from the joint posterior (8), is a Normal distribution with mean and covariance matrix</p>
<p>$$
\begin{aligned}
&amp; \hat{\boldsymbol{\mu}}<em k_="k," t="t">{k, t}=\hat{\boldsymbol{\Sigma}}</em>}\left[\frac{\hat{R<em k_="k," t="t">{t}^{-1}\left(\phi</em>} \mid \hat{\mathscr{D}<em k_="k," t="t">{k, t} ; \mathscr{D}</em>}\right)}{\sigma_{k, t}^{2}} \hat{\mu<em k_="k," t="t">{(t-1) \rightarrow t}\left(\boldsymbol{\phi}</em>}, \boldsymbol{\gamma<em k_="k," t="t">{k, t-1} \mid \hat{\mathscr{D}}</em>} ; \mathscr{D<em t="t">{k, t}\right)\right. \
&amp; \left.+\Xi</em>}\left(\hat{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\gamma}</em>}\right) \frac{\hat{R<em k_="k," t_1="t+1">{t+1}^{-1}\left(\boldsymbol{\phi}</em>} \mid \hat{\mathscr{D}<em k_="k," t_1="t+1">{k, t} ; \mathscr{D}</em>}^{\otimes}\right)}{\sigma_{k, t+1}^{2}} \hat{\mu<em k_="k," t_1="t+1">{(t+1) \rightarrow t}\left(\boldsymbol{\phi}</em>}, \boldsymbol{\gamma<em k_="k," t="t">{k, t} \mid \hat{\mathscr{D}}</em>} ; \mathscr{D<em k_="k," t="t">{k, t+1}^{\otimes}\right)\right] \
&amp; \hat{\boldsymbol{\Sigma}}</em>}=\left[\frac{\hat{R<em k_="k," t="t">{t}^{-1}\left(\boldsymbol{\phi}</em>} \mid \hat{\mathscr{D}<em k_="k," t="t">{k, t} ; \mathscr{D}</em>}\right)}{\sigma_{k, t}^{2}}+\boldsymbol{\Xi<em k_="k," t="t">{t}\left(\hat{\mathscr{D}}</em>} \mid \boldsymbol{\gamma<em t_1="t+1">{k, t}\right) \frac{\hat{R}</em>}^{-1}\left(\boldsymbol{\phi<em k_="k," t="t">{k, t+1} \mid \hat{\mathscr{D}}</em>} ; \mathscr{D<em k_="k," t_1="t+1">{k, t+1}^{\otimes}\right)}{\sigma</em>}^{2}} \boldsymbol{\Xi<em k_="k," t="t">{t}\left(\hat{\mathscr{D}}</em>
\end{aligned}
$$} \mid \boldsymbol{\gamma}_{k, t}\right)\right]^{-1</p>
<p>where $\mathscr{D}<em k_="k," t_1="t+1">{k, t+1}^{\otimes}:=\tilde{\mathscr{D}}</em>}-\hat{\mathscr{D}<em k_="k," t="t">{k, t}$ is the relative complement of $\hat{\mathscr{D}}</em>}$ in $\tilde{\mathscr{D}<em t="t">{k, t+1}, \boldsymbol{\Xi}</em>}\left(\hat{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\gamma}</em>}\right)=$ $\operatorname{diag}\left(\xi_{t}\left(\hat{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\gamma}</em>}\right)\right)$, for $k=1, \ldots, K$ and $t=1, \ldots, S-1$. The functions $\hat{R<em _rightarrow="\rightarrow" _t-1_="(t-1)" t="t">{t}, \hat{\mu}</em>}$, and $\hat{\mu<em k_="k," t="t">{(t+1) \rightarrow t}$ are given in Appendix A. We observe that updating missing data $\hat{y}</em>}$ takes into account information from the lower level $t-1$, same level $t$, and higher level $t+1$ by interpolating the associated moments. For instance, $\hat{\mu<em _rightarrow="\rightarrow" _t_1_="(t+1)" t="t">{(t-1) \rightarrow t}$ (and $\hat{\mu}</em>}$ ) provide information about the location of $\hat{\boldsymbol{y}<em _rightarrow="\rightarrow" _t-1_="(t-1)" t="t">{k, t}$ from levels $t-1, t$ (and levels $t+1, t$ ). It is worth mentioning that (9) can be re-written as a matrix-weighted average of $\hat{\mu}</em>$; i.e.}$, and scaled $\hat{\mu}_{(t+1) \rightarrow t</p>
<p>$$
\begin{aligned}
&amp; \hat{\boldsymbol{\mu}}<em k_="k," t="t">{k, t}=\hat{\Omega}</em>}\left(\phi_{k, t}, \boldsymbol{\gamma<em k_="k," t="t">{k, t}, \sigma</em>}^{2}, \sigma_{k, t+1}^{2}\right) \hat{\mu<em k_="k," t="t">{(t-1) \rightarrow t}\left(\boldsymbol{\phi}</em>}, \boldsymbol{\gamma<em k_="k," t="t">{k, t-1} \mid \hat{\mathscr{D}}</em>} ; \mathscr{D<em k_="k," t="t">{k, t}\right) \
&amp; \quad+\left(I-\hat{\Omega}</em>}\left(\phi_{k, t}, \boldsymbol{\gamma<em k_="k," t="t">{k, t}, \sigma</em>}^{2}, \sigma_{k, t+1}^{2}\right)\right) \boldsymbol{\Xi<em k_="k," t="t">{t}^{-1}\left(\hat{\mathscr{D}}</em>} \mid \boldsymbol{\gamma<em _rightarrow="\rightarrow" _t_1_="(t+1)" t="t">{k, t}\right) \hat{\mu}</em>}\left(\boldsymbol{\phi<em k_="k," t="t">{k, t+1}, \boldsymbol{\gamma}</em>} \mid \hat{\mathscr{D}<em k_="k," t_1="t+1">{k, t} ; \mathscr{D}</em>\right) . \
&amp; \hat{\Omega}}^{\otimes<em k_="k," t="t">{k, t}\left(\phi</em>}, \boldsymbol{\gamma<em k_="k," t="t">{k, t}, \sigma</em>}^{2}, \sigma_{k, t+1}^{2}\right)=\hat{\boldsymbol{\Sigma}<em t="t">{k, t} \frac{\hat{R}</em>}^{-1}\left(\boldsymbol{\phi<em k_="k," t="t">{k, t} \mid \hat{\mathscr{D}}</em>} ; \mathscr{D<em k_="k," t="t">{k, t}\right)}{\sigma</em>
\end{aligned}
$$}^{2}</p>
<p>Hence, each update interpolates both across the input space at an individual fidelity level and across the fidelity levels. Simulating $\left[\hat{\boldsymbol{y}}<em k_="k," t="t">{k, t} \mid \boldsymbol{y}, \boldsymbol{\phi}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}, \mathcal{T}\right]$ can be performed in parallel for $k$ which is a computational benefit, and it can be suppressed if $\hat{\mathscr{D}}</em>=\emptyset$.</p>
<p>Elaborating further into specific cases of the above imputation, if levels $t$ and $t+1$ do not share any design points at all, at sub-region $\mathcal{X}<em k_="k," t_1="t+1">{k}$, i.e., $\mathscr{D}</em>}^{\otimes}=\emptyset$, then $\hat{R<em k_="k," t_1="t+1">{t+1}^{-1}\left(\boldsymbol{\phi}</em>} \mid \hat{\mathscr{D}<em t_1="t+1">{k, t} ; \emptyset\right)=$ $R</em>}^{-1}\left(\hat{\mathscr{D}<em k_="k," t="t">{k, t}, \hat{\mathscr{D}}</em>} \mid \boldsymbol{\phi<em _rightarrow="\rightarrow" _t_1_="(t+1)" t="t">{k, t+1}\right)$, and $\hat{\mu}</em>}\left(\boldsymbol{\phi<em k_="k," t="t">{k, t+1}, \boldsymbol{\gamma}</em>} \mid \hat{\mathscr{D}<em k_="k," t_1="t+1">{k, t} ; \emptyset\right)=y</em>}\left(\hat{\mathscr{D}<em t_1="t+1">{k, t}\right)-\boldsymbol{H}</em>}\left(\hat{\mathscr{D}<em t_1="t+1">{k, t}\right) \boldsymbol{b}</em>}$. This implies that, given weak priors on $\delta_{k, t+1}(\cdot)$ are specified, i.e. $\boldsymbol{b<em k_="k," t="t">{t+1} \rightarrow 0$, the update of missing $\hat{\boldsymbol{y}}</em>}$ obtains information from the upper level $t+1$ which entirely relies on the observed output $\boldsymbol{y<em k_="k," t_1="t+1">{k, t+1}$ and not from the discrepancy terms $\delta</em>}(\cdot)$ and $\xi_{k, t}(\cdot)$ of the two levels. If levels $t$ and $t+1$ share design points, $\mathscr{D<em _rightarrow="\rightarrow" _t_1_="(t+1)" t="t">{k, t+1}^{\otimes} \neq \emptyset$, the extra structure of the equations of $\hat{\mu}</em>}$ and $\hat{R<em k="k">{t+1}^{-1}$ in (27) and (26) (see Appendix A) can be interpreted as the factor quantifying the discrepancy between levels $t$ and $t+1$. Finally, we can see that when the correlation between the two levels $t$ and $t+1$, at sub-region $\mathcal{X}</em>}$, is weak, e.g. $\boldsymbol{\Xi<em k_="k," t="t">{t}\left(\hat{\mathscr{D}}</em>\right) \rightarrow 0$, the missing data update resembles the prediction relying only on the information from the current level $t$. Based on these observations, it may be preferable to consider designs with some overlap at adjacent levels not only for computational convenience but also for modeling reasons. However, a theoretical proof of this statement is out of scope.} \mid \boldsymbol{\gamma}_{k, t</p>
<p>Update $[\mathcal{T}, \boldsymbol{\phi} \mid \tilde{\boldsymbol{y}}] \quad$ To update $[\mathcal{T}, \boldsymbol{\phi} \mid \tilde{\boldsymbol{y}}]$, we propose a mixture of the Markov transitions targeting the augmented marginal posterior $\pi(\mathcal{T}, \boldsymbol{\phi} \mid \tilde{\boldsymbol{y}})$ whose density is proportional to</p>
<p>$$
\begin{aligned}
\pi(\tilde{\boldsymbol{y}}, \mathcal{T}, \boldsymbol{\phi}) &amp; =\pi(\mathcal{T}) \prod_{k=1}^{K} \pi\left(\tilde{\boldsymbol{y}}<em 1="1" k_="k,">{k, 1}, \boldsymbol{\phi}</em>} \mid \mathcal{T}\right) \prod_{t=2}^{S} \pi\left(\tilde{\boldsymbol{y}<em k_="k," t="t">{k, t}, \boldsymbol{\phi}</em>} \mid \tilde{\boldsymbol{y}<em k_="k," t="t">{k, t-1}, \mathcal{T}\right) \
\pi\left(\tilde{\boldsymbol{y}}</em>}, \boldsymbol{\phi<em k_="k," t-1="t-1">{k, t} \mid \tilde{\boldsymbol{y}}</em>}, \mathcal{T}\right) &amp; =\pi\left(\boldsymbol{\phi<em k_="k," t="t">{k, t}\right) \frac{\left|\hat{\boldsymbol{A}}</em>}\left(\phi_{k, t}\right)\right|^{\frac{1}{2}}}{\left|\boldsymbol{B<em t="t">{t}\right|^{\frac{1}{2}}}\left|\boldsymbol{G}</em>}\right|^{\frac{1}{2}} \pi^{\frac{\tilde{n<em t="t">{k, t}}{2}} \frac{\Gamma\left(\lambda</em>}+\frac{\tilde{n<em t="t">{k, t}}{2}\right)}{\Gamma\left(\lambda</em>}\right)}\left(\operatorname{SSE<em k_="k," t="t">{k, t}\left(\boldsymbol{\phi}</em>
\end{aligned}
$$}\right)\right)^{-\lambda_{t}-\frac{\tilde{n}_{k, t}}{2}</p>
<p>where $\operatorname{SSE}<em k_="k," t="t">{k, t}\left(\boldsymbol{\phi}</em>}\right)=\left(\tilde{n<em t="t">{k, t}+2 \lambda</em>}-2\right) \hat{\sigma<em k_="k," t="t">{k, t}^{2}\left(\boldsymbol{\phi}</em>}\right)$. Functions $\hat{\sigma<em k_="k," t="t">{k, t}^{2}$ and $\hat{\boldsymbol{A}}</em>$ are given in (23) and (24) in Appendix A. The Markov transitions are based on the operations change, swap, rotate, and grow \&amp; prune, introduced by (Chipman et al., 1998; Gramacy and Lee, 2008). The first three operations are Metropolis-Hastings algorithms (Hastings, 1970) whose implementation is straightforward. The grow \&amp; prune operations are local reversible jump</p>
<p>(RJ) transitions and further specification is required.
The grow operation performing a transition from state $(\mathcal{T}, \boldsymbol{\phi})$ to $\left(\mathcal{T}^{<em>}, \boldsymbol{\phi}^{</em>}\right)$ works as follows. We randomly select an external node $\omega_{j_{0}}$ and assume it corresponds to a sub-region $\mathcal{X}<em 0="0">{j</em>}}$, data set $\left{\widehat{\mathscr{D}<em 0="0">{j</em>}}, \widehat{\boldsymbol{y}<em 0="0">{j</em>}}\right}$, and parameters $\boldsymbol{\phi<em 0="0">{j</em>^{}, t}$ though the augmented statistical model. We propose node $\omega_{j_{0}}$ to split into two new child nodes $\omega_{j_{1}}$ and $\omega_{j_{2}}$ according to the splitting rule $P_{\text {rule }}$ in prior (7), and we denote the proposed tree as $\mathcal{T<em>}$. Nodes $\omega_{j_{1}}$ and $\omega_{j_{2}}$ correspond to disjoint sub-regions $\mathcal{X}<em 1="1">{j</em>}}$ and $\mathcal{X<em 2="2">{j</em>}}$ (with $\mathcal{X<em 0="0">{j</em>}}=\mathcal{X<em 0="0">{j</em>}} \cup \mathcal{X<em 1="1">{j</em>}}$ ), data sets $\left{\widehat{\mathscr{D}<em 1="1">{j</em>}, t}, \widehat{\boldsymbol{y}<em 1="1">{j</em>}, t}\right}$ and $\left{\widehat{\mathscr{D}<em 2="2">{j</em>}, t}, \widehat{\boldsymbol{y}<em 2="2">{j</em>}, t}\right}$, and parameters $\boldsymbol{\phi<em 1="1">{j</em>^{}, t</em>}$ and $\boldsymbol{\phi}<em 2="2">{j</em>^{}, t<em>}$, respectively. Randomly, one of the parameters $\boldsymbol{\phi}<em 1="1">{j</em>^{}, t</em>}$ or $\boldsymbol{\phi}<em 2="2">{j</em>^{}, t<em>}$ inherits the values from the parent ones; e.g., $\boldsymbol{\phi}<em 1="1">{j</em>^{}, t</em>}=\boldsymbol{\phi}<em 0="0">{j</em>}, t}$. The values of the other parameter are proposed by simulating from a probability distribution; e.g., $\boldsymbol{\phi<em 2="2">{j</em>^{}, t<em>} \sim Q_{t}(\cdot)$, such as the corresponding priors. The rest elements of $\boldsymbol{\phi}_{t}^{</em>}$ inherit their values from $\boldsymbol{\phi}_{t}$. The proposed transition is accepted with probability $\min (1, \Delta)$ where</p>
<p>$$
\begin{aligned}
&amp; \Delta=\frac{\zeta\left(1+u_{\omega_{j_{0}}}\right)^{-d}\left(1-\zeta\left(2+u_{\omega_{j_{0}}}\right)^{-d}\right)^{2}}{1-\zeta\left(1+u_{\omega_{j_{0}}}\right)^{-d}} \frac{|\mathcal{G}|}{\left|\mathcal{P}^{<em>}\right|} \prod_{t=2}^{S} \frac{\pi\left(\hat{\boldsymbol{y}}<em 1="1">{j</em>}, t}, \boldsymbol{\phi<em 1="1">{j</em>^{}</em>}\right| \hat{\boldsymbol{y}}<em 1="1">{j</em>^{}, t-1}, \mathcal{T<em>}\right) \pi\left(\hat{\boldsymbol{y}}<em 2="2">{j</em>}, t}, \boldsymbol{\phi<em 2="2">{j</em>^{}, t</em>} \mid \hat{\boldsymbol{y}}<em 2="2">{j</em>^{}, t-1}, \mathcal{T<em>}\right)}{\pi\left(\hat{\boldsymbol{y}}<em 0="0">{j</em>}, t}, \boldsymbol{\phi<em 1="1">{j</em>^{}, t</em>} \mid \hat{\boldsymbol{y}}<em 0="0">{j</em>}, t-1}, \mathcal{T}\right) Q_{t}\left(\boldsymbol{\phi<em 2="2">{j</em>^{}, t<em>}\right)} \
&amp; \times \frac{\pi\left(\hat{\boldsymbol{y}}<em 1="1">{j</em>}, 1}, \boldsymbol{\phi<em 1="1">{j</em>^{}, 1</em>} \mid \mathcal{T}^{<em>}\right) \pi\left(\hat{\boldsymbol{y}}<em 2="2">{j</em>}, 1}, \boldsymbol{\phi<em 2="2">{j</em>^{}, 1</em>} \mid \mathcal{T}^{<em>}\right)}{\pi\left(\hat{\boldsymbol{y}}<em 0="0">{j</em>}, 1}, \boldsymbol{\phi<em 1="1">{j</em>^{}, 1</em>} \mid \mathcal{T}\right) Q_{t}\left(\boldsymbol{\phi}<em 2="2">{j</em>
\end{aligned}
$$}, 1}^{*}\right)</p>
<p>$\mathcal{G}$ is the set of growable nodes in tree $\mathcal{T}$, and $\mathcal{P}^{<em>}$ is the set of prounable nodes in tree $\mathcal{T}^{</em>}$. The prune operation, performing a transition from state $\left(\mathcal{T}^{<em>}, \boldsymbol{\phi}^{</em>}\right)$ to $(\mathcal{T}, \boldsymbol{\phi})$, is fully defined as the reverse operation of the grow operation, and is accepted with probability $\min (1,1 / \Delta)$.</p>
<p>Due to the proposed augmentation in Section 2.2, we are able to analytically integrate out a potentially high-dimensional parameter vector $\left(\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}\right)$ from the joint density (8), and hence design local RJ moves targeting the marginal $\pi(\mathcal{T}, \boldsymbol{\phi} \mid \hat{\boldsymbol{y}})$. The benefit from this collapsed update is that the proposed RJ algorithm operates on a lower dimensional state space, which allows for shorter and more acceptable jumps in practice. If necessary, grow and prune operations can be further improved by using the annealing mechanism of Karagiannis and Andrieu (2013).</p>
<p>Update $\left[\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}, \boldsymbol{\phi} \mid \tilde{\boldsymbol{y}}, \mathcal{T}\right]$ The conditional posterior $\pi\left(\beta, \gamma, \sigma^{2}, \phi \mid \tilde{y}, \mathcal{T}\right)$ has the form</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{\beta}<em k_="k," t="t">{k, t} \mid \tilde{\boldsymbol{y}}</em>}, \tilde{\boldsymbol{y}<em k_="k," t-1="t-1">{k, t-1}, \boldsymbol{\gamma}</em>}, \sigma_{k, t}^{2}, \boldsymbol{\phi<em k_="k," t="t">{k, t} \sim \mathrm{~N}\left(\tilde{\boldsymbol{\beta}}</em>}\left(\boldsymbol{\phi<em k_="k," t="t">{k, t}\right), \tilde{\boldsymbol{B}}</em>}\left(\boldsymbol{\phi<em k_="k," t="t">{k, t}\right) \sigma</em> t=2, \ldots S \
&amp; \boldsymbol{\beta}}^{2}\right), \text { for <em 1="1" k_="k,">{k, 1} \mid \tilde{\boldsymbol{y}}</em>}, \sigma_{k, 1}^{2}, \boldsymbol{\phi<em 1="1" k_="k,">{k, 1} \sim \mathrm{~N}\left(\tilde{\boldsymbol{\beta}}</em>}\left(\boldsymbol{\phi<em 1="1" k_="k,">{k, 1}\right), \tilde{\boldsymbol{B}}</em>}\left(\boldsymbol{\phi<em 1="1" k_="k,">{k, 1}\right) \sigma</em>\right), \
&amp; \boldsymbol{\gamma}}^{2<em k_="k," t="t">{k, t-1} \mid \tilde{\boldsymbol{y}}</em>}, \tilde{\boldsymbol{y}<em k_="k," t="t">{k, t-1}, \sigma</em>}^{2}, \boldsymbol{\phi<em k_="k," t-1="t-1">{k, t} \sim \mathrm{~N}\left(\tilde{\boldsymbol{\gamma}}</em>}\left(\boldsymbol{\phi<em k_="k," t-1="t-1">{k, t}\right), \tilde{\boldsymbol{G}}</em>}\left(\boldsymbol{\phi<em k_="k," t="t">{k, t}\right) \sigma</em> t=2, \ldots S \
&amp; \sigma_{k, t}^{2} \mid \tilde{\boldsymbol{y}}}^{2}\right), \text { for <em k_="k," t-1="t-1">{k, t}, \tilde{\boldsymbol{y}}</em>}, \boldsymbol{\phi<em k_="k," t="t">{k, t} \sim \mathrm{IG}\left(\hat{\lambda}</em>}, \hat{\chi<em k_="k," t="t">{k, t}\left(\boldsymbol{\phi}</em> t=2, \ldots S \
&amp; \sigma_{k, 1}^{2} \mid \tilde{\boldsymbol{y}}}\right)\right), \text { for <em 1="1" k_="k,">{k, 1}, \boldsymbol{\phi}</em>} \sim \mathrm{IG}\left(\hat{\lambda<em 1="1" k_="k,">{k, 1}, \hat{\chi}</em>}\left(\boldsymbol{\phi<em k_="k," t="t">{k, 1}\right)\right), \
&amp; \boldsymbol{\phi}</em>\right),
\end{aligned}
$$} \mid \tilde{\boldsymbol{y}}, \mathcal{T} \sim \pi\left(\boldsymbol{\phi}_{k, t} \mid \tilde{\boldsymbol{y}}, \mathcal{T</p>
<p>where the hatted quantities are given in (21)-(23) of Appendix A.
Conditional distributions (13)-(16) can be sampled directly, and in parallel for different $(k, t)$. Sampling from the full conditional of $\boldsymbol{\beta}$ 's (13) and (14) is not necessary and can be ignored from the MCMC sweep if prediction is the only concern of the analysis. This is because $\boldsymbol{\beta}$ 's can be analytically integrated out from the proposed emulator in Section 2.4. Alternatively, $\boldsymbol{\beta}$ 's can be sampled outside the MCMC sweep (13) and (14) by conditioning.</p>
<p>Updating $\phi$ by simulating from $\pi(\boldsymbol{\phi} \mid \tilde{\boldsymbol{y}}, \mathcal{T})$ is not necessary in theory, as it is updated in block $[\mathcal{T}, \boldsymbol{\phi} \mid \tilde{\boldsymbol{y}}]$, however it improves mixing in practice. The marginal posterior (17) cannot be sampled directly. Conditional independence in (10) implies that $\left{\boldsymbol{\phi}_{k, t}\right}$ can be simulated by running in parallel $K \times S$ Metropolis-Hastings algorithms each of them targeting distributions with densities proportional to (11).</p>
<h1>2.4 Posterior analysis and emulation</h1>
<p>Assume there is available a MCMC sample $\mathcal{S}^{N}=\left(\hat{\boldsymbol{y}}^{(j)}, \mathcal{T}^{(j)}, \boldsymbol{\gamma}^{(j)}, \boldsymbol{\sigma}^{\mathbf{2},(j)}, \boldsymbol{\phi}^{(j)}\right)<em k="k">{j=1}^{N}$ generated from the RJMCMC sampler in Section 2.3, and let $\left{\mathcal{X}</em>$. Central Limit Theorem can be applied to facilitate inference as the proposed sampler is aperiodic, irreducible, and reversible (Roberts et al., 2004).}^{(j)}\right}_{k=1}^{K^{(j)}}$ denote the partition corresponding to tree $\mathcal{T}^{(j)</p>
<p>The proposed procedure ABTCK allows inference to be performed for the missing output values $\hat{\boldsymbol{y}}<em t="t">{t}=\boldsymbol{y}</em>}\left(\hat{\mathscr{D}<em t="t">{t}\right)$ at input points in $\hat{\mathscr{D}}</em>}=\bigcup_{\forall k} \hat{\mathscr{D}<em t="t">{k, t}$. Inference on $\hat{\boldsymbol{y}}</em>}$ can be particularly useful when the computer model has been unable to generate simulations at these input points due to numerical crash or limitations. The marginal posterior distribution of $\hat{\boldsymbol{y}<em t="t">{t}$, along with its expectations, can be approximated via standard Monte Carlo (MC) using the generated samples $\left{\hat{\boldsymbol{y}}</em>}^{(j)}\right}$ at each level $t$. Alternatively, point estimates of $\hat{\boldsymbol{y}<em k_="k," t="t">{k, t}$ at $\hat{\mathscr{D}}</em>}$ can be approximated by the more accurate Rao-Blackwell MC estimator $\mathrm{E}\left(\hat{\boldsymbol{y}<em 1:="1:" S="S">{k, t} \mid \boldsymbol{y}</em>}\right) \approx \frac{1}{N} \sum_{j=1}^{N} \hat{\boldsymbol{\mu}<em k_="k," t="t">{k, t}^{(j)}$, where $\left{\hat{\boldsymbol{\mu}}</em>\right}$ is the $j$-th MCMC realization of (9).}^{(j)</p>
<p>A Monte Carlo recursive emulator able to facilitate fully Bayesian predictive inference on the output $y_{t}\left(\mathscr{D}^{<em>}\right)$ at untried input points $\mathscr{D}^{</em>}$ at every fidelity level $t=1, \ldots, S$ can be derived. The conditional distribution $\left[\boldsymbol{y}<em 1:="1:" S="S">{1: S}(\cdot) \mid \boldsymbol{y}</em>}, \hat{\boldsymbol{y}<em 1:="1:" S="S">{1: S}, \boldsymbol{\beta}</em>}, \boldsymbol{\gamma<em 1:="1:" S="S">{1: S}, \boldsymbol{\sigma}</em>}^{2}, \boldsymbol{\phi<em 1:="1:" S="S">{1: S}\right]$ inherits a conditional independence similar to (6) due to the augmentation of the data with $\hat{\boldsymbol{y}}</em>}$ that allows it to be analytically integrated out with respect to (13)-(16). Hence the distribution of $\left[\boldsymbol{y<em 1:="1:" S="S" k_="k,">{1: S}(\cdot) \mid \boldsymbol{y}</em>}, \hat{\boldsymbol{y}<em 1:="1:" S="S" k_="k,">{k, 1: S}, \boldsymbol{\phi}</em>$, is calculated as}, \mathcal{T}\right]$, at sub-region $\mathcal{X}_{k</p>
<p>$$
\begin{gathered}
y_{1}(\cdot) \mid \hat{\boldsymbol{y}}<em 1="1">{1}, \boldsymbol{\phi}</em>}, \mathcal{T} \sim \operatorname{STP}\left(\boldsymbol{\mu<em 1="1" k_="k,">{k, 1}^{<em>}\left(\cdot \mid \hat{\boldsymbol{y}}<em 1="1" k_="k,">{k, 1}, \boldsymbol{\phi}</em>}\right), \hat{\sigma<em 1="1" k_="k,">{k, 1}^{2} R</em>^{</em>}\left(\cdot, \cdot \mid \hat{\boldsymbol{y}}</em>}, \boldsymbol{\phi<em 1="1">{k, 1}\right), 2 \lambda</em>}+\tilde{n<em t="t">{k, 1}\right) \
y</em>}(\cdot) \mid y_{t-1}(\cdot), \hat{\boldsymbol{y}<em k_="k," t="t">{t: t-1}, \boldsymbol{\phi}</em>^{}, \mathcal{T} \sim \operatorname{STP}\left(\mu_{k, t<em>}\left(\cdot \mid \hat{\boldsymbol{y}}<em k_="k," t="t">{k, t}, \boldsymbol{\phi}</em>}\right), \hat{\sigma<em k_="k," t="t">{k, t}^{2} R</em>^{</em>}\left(\cdot, \cdot \mid \hat{\boldsymbol{y}}<em k_="k," t="t">{k, t}, \boldsymbol{\phi}</em>\right)
\end{gathered}
$$}\right), 2 \lambda_{t}+\tilde{n}_{k, t</p>
<p>where the conditionals are Student-T processes (STP) with</p>
<p>$$
\begin{aligned}
\mu_{t}^{<em>}\left(x \mid \hat{\boldsymbol{y}}<em k_="k," t="t">{k, t}, \boldsymbol{\phi}</em>}\right)= &amp; \boldsymbol{L<em t="t">{t}\left(\boldsymbol{x} ; \boldsymbol{y}</em>}\right) \hat{\boldsymbol{a}<em t="t">{t}+\boldsymbol{R}</em>}\left(x, \tilde{\mathscr{D}<em k_="k," t="t">{t} \mid \boldsymbol{\phi}</em>}\right) \boldsymbol{R<em k_="k," t="t">{t}^{-1}\left(\tilde{\mathscr{D}}</em>}, \tilde{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\phi}</em>}\right)\left[\boldsymbol{L<em k_="k," t="t">{t}\left(\tilde{\mathscr{D}}</em>} ; \boldsymbol{y<em t="t">{t}\right) \hat{\boldsymbol{a}}</em>}-\boldsymbol{y<em k_="k," t="t">{t}\left(\tilde{\mathscr{D}}</em>\right)\right] \
R_{t}^{</em>}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime} \mid \boldsymbol{\phi}<em t="t">{k, t}\right)= &amp; R</em>}\left(x, x^{\prime} \mid \boldsymbol{\phi<em t="t">{k, t}\right)-\boldsymbol{R}</em>}\left(\boldsymbol{x}, \tilde{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\phi}</em>}\right) \boldsymbol{R<em k_="k," t="t">{t}^{-1}\left(\tilde{\mathscr{D}}</em>}, \tilde{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\phi}</em>}\right) R_{t}^{\top}\left(\boldsymbol{x}^{\prime}, \tilde{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\phi}</em>\right) \
&amp; +\left[\boldsymbol{L}<em t="t">{t}\left(\boldsymbol{x} ; \boldsymbol{y}</em>}\right)-\boldsymbol{R<em k_="k," t="t">{t}\left(\boldsymbol{x}, \tilde{\mathscr{D}}</em>} \mid \boldsymbol{\phi<em t="t">{k, t}\right) \boldsymbol{R}</em>}^{-1}\left(\tilde{\mathscr{D}<em k_="k," t="t">{k, t}, \tilde{\mathscr{D}}</em>} \mid \boldsymbol{\phi<em t="t">{k, t}\right) \boldsymbol{L}</em>}\left(\tilde{\mathscr{D}<em t="t">{k, t} ; \boldsymbol{y}</em>}\right)\right] \hat{\boldsymbol{A}<em t="t">{t} \
&amp; \times\left[\boldsymbol{L}</em>}\left(\boldsymbol{x}^{\prime} ; \boldsymbol{y<em t="t">{t}\right)-\boldsymbol{R}</em>}\left(\boldsymbol{x}^{\prime}, \tilde{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\phi}</em>}\right) \boldsymbol{R<em k_="k," t="t">{t}^{-1}\left(\tilde{\mathscr{D}}</em>}, \tilde{\mathscr{D}<em k_="k," t="t">{k, t} \mid \boldsymbol{\phi}</em>}\right) \boldsymbol{L<em k_="k," t="t">{t}\left(\tilde{\mathscr{D}}</em>
\end{aligned}
$$} ; \boldsymbol{y}_{t}\right)\right]^{\top</p>
<p>for $\boldsymbol{x}, \boldsymbol{x}^{\prime} \in \mathcal{X}<em t="t">{k}$, and $\boldsymbol{L}</em>}\left(\mathfrak{Z} ; \boldsymbol{y<em t="t">{t-1}\right)=\left[\boldsymbol{H}</em>}(\mathfrak{Z}), \operatorname{diag}\left(\boldsymbol{y<em t-1="t-1">{t-1}(\mathfrak{Z}) \boldsymbol{W}</em>)\right)\right]$ for $t=2, \ldots, S$ and}(\mathfrak{Z</p>
<p>$\boldsymbol{L}<em 1="1">{1}(\mathfrak{Z} ; \cdot)=\boldsymbol{H}</em>}(\mathfrak{Z})$ for a set $\mathfrak{Z}$. An MCMC sample from the predictive distribution of $\left[\boldsymbol{y<em 1="1" S="S" _=";">{1 ; S}(\cdot) \mid \boldsymbol{y}</em>\right]$, and its moments, at any fidelity level $t$. The conditional independence in the predictive distribution (18) and (19) results because of our imputation strategy.}\right]$, at $\boldsymbol{x} \in \mathscr{D}^{*}$, can be obtained by simulating (18)-(19) given the sample values $\mathcal{S}^{N}=\left{\hat{\boldsymbol{y}}^{(j)}, \boldsymbol{\phi}^{(j)}, \mathcal{T}^{(j)}\right}$. This allows the computation of a Monte Carlo approximation of the emulator of $\left[y_{t}(\cdot) \mid \boldsymbol{y}_{1 ; S</p>
<p>The proposed emulator accounts for non-stationariy and discontinuity by aggregating simpler GP emulators in a Bayesian model averaging manner, while it integrates uncertainty regarding the unknown 'missing data' $\hat{\boldsymbol{y}}$ and parameters. It is computationally preferable compared to existing co-kriging one (Kennedy and O'Hagan, 2000; Le Gratiet, 2013) because it allows the parallel inversion of smaller covariance matrices with sizes $\tilde{n}<em k="k" t_="t,">{t, k} \times \tilde{n}</em>}$ while the others require the inversion of a large co-variance matrix of size $\sum_{t=1}^{S} \tilde{n<em t="1">{t} \times \sum</em>$. Moreover, it is able to recover the whole predictive distribution and its moments, unlike the derivation in Le Gratiet and Garnier (2014) where only the predictive mean and variance are derived recursively. More importantly, it is able to be applied in problems where the training data set is not hierarchically nested, while its competitors cannot.}^{S} \tilde{n}_{t</p>
<h1>2.5 Further details</h1>
<p>Two novel co-kriging procedures can be distinguished as special cases of the proposed ABTCK. In applications where the design is non hierarchically nested, but the computer model outputs can be assumed as stationary, one can consider to drop the partitioning by setting $K=1$ and suppressing the MCMC update $[\mathcal{T}, \boldsymbol{\phi} \mid \hat{\boldsymbol{y}}]$. We will refer to this reduced version of ABTCK as Augmented Bayesian co-kriging (ABCK). Unlike standard co-kriging, our ABCK can be applied with non-nested designs as it makes the computations for training the Bayesian model or computing the emulator practically feasible. In fact, the proposed augmentation strategy separates the posterior into conditionally independent quantities and</p>
<p>allows closed form inference for the majority of the hyper-parameters. Another special case is where the design is hierarchically nested but the model outputs present non-stationarity, the imputation mechanism can be dropped by setting $\left{\hat{\mathscr{D}}_{k, t} \equiv \emptyset\right}$ and suppressing the update $\left[\hat{\boldsymbol{y}} \mid \boldsymbol{y}, \boldsymbol{\phi}, \boldsymbol{\gamma}, \boldsymbol{\sigma}^{\mathbf{2}}, \mathcal{T}\right]$. We will refer to this reduced version of ABTCK as Bayesian treed co-kriging (BTCK). In such a case, BTCK can be preferable to the standard co-kriging as it can model the aforesaid stationarity by properly combining simple stationary GPs.</p>
<p>The computational complexity of the proposed ABTCK compared to existing co-kriging methods is reduced in two ways: a) by breaking the emulation into $K$ parts via the partitioning, and b) by breaking the emulation into $S$ parts via the recursively prediction procedure. In ABTCK the computational complexity of evaluating the augmented likelihood or the predictive distribution is $\mathcal{O}\left(\sum_{t=1}^{S} \sum_{k=1}^{K} \tilde{n}<em t="1">{k, t}^{3}\right)$ in sequential computing environments, while it can be further reduced to $\mathcal{O}\left(\sum</em> \max }^{S<em k_="k," t="t">{k=1, \ldots, K}\left(\tilde{n}</em>}\right)^{3}\right)$ in parallel computing environments since operations at each $k$ can be performed in parallel. Under non-hierarchical designs, our ABCK (assuming the partitioning is dropped) requires $\mathcal{O}\left(\sum_{t=1}^{S} \tilde{n<em t="1">{t}^{3}\right)$ for the evaluation of the augmented likelihood or the Monte Carlo emulator which is smaller than $\mathcal{O}\left(\left(\sum</em>}^{S} n_{t}\right)^{3}\right)$ required by (Kennedy and O'Hagan, 2000; Le Gratiet et al., 2014) for the evaluation of the associated likelihoods since $\tilde{n<em t="t">{t} \leq n</em>$.</p>
<h1>3 Case study</h1>
<p>We examine the predictive ability of the proposed method, refereed to as augmented Bayesian treed co-kriging (ABTCK), its special cases ABCK, and BTCK, as well as we provide comparisons with existing approaches of (Le Gratiet, 2013; Kennedy and O'Hagan, 2000) showing the good performance of the proposed method. Details about the performance measures used can be found in Supplementary Section S.3. For the simulations, we used in MATLAB R2017b on a computer with specifications (IntelCore ${ }^{\text {TM }}$ 7-7700K CPU @ $4.20 \mathrm{GHz} \times 8$, and 62.8 GiBRAM).</p>
<h1>3.1 Numerical example</h1>
<p>Consider functions</p>
<p>$$
\begin{aligned}
&amp; y_{1}(\boldsymbol{x})=2 x_{1} \exp \left(-x_{1}^{2}-x_{2}^{2}\right)+0.5 \exp \left{\sin \left(\left(0.9\left(\frac{x_{1}+2}{8}+0.48\right)^{10}\right)\right)\right}+1.2, \boldsymbol{x} \in[-2,6]^{2} \
&amp; y_{2}(\boldsymbol{x})=4 x_{1} \exp \left(-x_{1}^{2}-x_{2}^{2}\right)+0.2 \exp \left{\sin \left(\left(0.9\left(\frac{x_{1}+2}{8}+0.48\right)^{10}\right)\right)\right}+0.5, \boldsymbol{x} \in[-2,6]^{2}
\end{aligned}
$$</p>
<p>which are assumed to be output functions of computer models $\mathfrak{C}<em 2="2">{1}$ and $\mathfrak{C}</em>}$, with $\mathfrak{C<em 1="1">{2}$ being more accurate but slower to run than $\mathfrak{C}</em>(\cdot)$.}$. By expressing (20) as (1), it can be seen that the discrepancy functions $\delta_{1}(\cdot)$ and $\xi_{1}(\boldsymbol{x})$ change over $\mathcal{X}$. We pretend that equations in (20) are unknown, and we are interested in learning the high fidelity $y_{2</p>
<p>We consider a non-hierarchically nested design $\mathscr{D}=\left{\mathscr{D}<em 2="2">{1}, \mathscr{D}</em>}\right}$, generated as follows. For level $t=1$, the observed data are generated by employing a Latin Hypercube Sampling (LHS) (McKay et al., 1979) to generate a point set $\mathscr{D<em 1="1">{1}$ of size $n</em>}=120$, and computing the corresponding observations $\boldsymbol{y<em 2="2">{1}$ from (20). For level $t=2$, the observations are generated likewise by generating a point set $\mathscr{D}</em>}$ of size $n_{2}=30$ via LHS such that $\mathscr{D<em 1="1">{2} \nsubseteq \mathscr{D}</em>}$. For our comparisons against, we consider a second hierarchically nested experimental design $\mathscr{D}^{\prime}=\left{\mathscr{D<em 2="2">{1}, \mathscr{D}</em>$ is randomly generated via the condition Latin Hypercube Sampling (cLHS) design (Minasny and McBratney, 2006).}^{\prime}\right}$, where the high fidelity point set $\mathscr{D}_{2}^{\prime</p>
<p>To implement our ABTCK, we consider weakly informative priors with hyper-parameters $\boldsymbol{b}<em t="t">{t}=\boldsymbol{g}</em>}=0, \boldsymbol{B<em t="t">{t}=10, \lambda</em>}=2, \chi_{t}=2$ and a mixture prior of Gamma distributions $\boldsymbol{\phi<em t="t">{t} \mid \mathcal{T} \sim 0.5 \mathrm{G}(1,20)+0.5 \mathrm{G}(10,10)$ for $\boldsymbol{\phi}</em>}$ distributing the prior mass on areas of smaller and larger values (Gramacy and Lee, 2008). The scale discrepancy is parametrised as a zero-degree basis expansion $\xi_{k, t}\left(\boldsymbol{x} \mid \boldsymbol{\gamma<em k_="k," t="t">{k, t}\right)=\boldsymbol{\gamma}</em>$. The tree process prior has hyper-parameters $\zeta=0.5$ and $d=2$. The statistical model was trained by MCMC running for 25000 iterations where the first 5000 iterations where discarded as as burn-in. Also, we consider ABCK which is a special case of ABTCK where binary partitioning is suspended. Furthermore, for comparisons involve the existing approaches: the standard GP considering only the high</p>
<p>fidelity data on $\mathscr{D}_{2}$ (HFGP), Zertuche's co-kriging approach (ZBCK) in Zertuche (2015) against non-nested design $\mathscr{D}$, Bayesian co-kriging on nested design $\mathscr{D}^{\prime}$ (NBCK), and the Kenedy \&amp; O'Hagan's CK (K\&amp;O) approach on non-nested design $\mathscr{D}$.</p>
<p>Figure 1 present the relative absolute error (RAE), on a $100 \times 100$ grid of $\mathcal{X}$, produced by ABTCK and ABCK (where partitioning is suppressed). We observe that ABTCK has produced a significantly smaller RAE than ABCK suggesting the benefit in predictions from including the binary partitioning mechanism in our proposed procedure.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Absolute relative error (in log scale) between the real response $y_{2}(\cdot)$ and the predictive mean of the high-level computer model using the augmented Bayesian co-kriging (ABCK) and augmented Bayesian treed co-kriging (ABTCK).</p>
<p>Table 1 presents the mean squared predictive error (MSPE) on a $100 \times 100$ grid of $\mathcal{X}$ produced by the procedures under comparison, as well as the model fitting times. We observe that MSPE produced by ABTCK is around 40 times smaller than that by ABCK suggesting that the partitioning mechanism as implemented in our ABTCK has been able to successfully capture and model the non-stationarity, and hence produce more accurate predictions, in the multifidelity setting. Our ABTCK and ABCK produced smaller MSPE than ZCK which is reasonable as the latter introduced a bias while dealing with non-nested designs. Moreover ABTCK and ABCK on the non-nested design $\mathscr{D}$ produced smaller MSPEs than NBCK and $\mathrm{K} \&amp; \mathrm{O}$ on nested designs $\mathscr{D}^{\prime}$. This suggests the possibility that non-nested designs can lead to more accurate emulations (at the expense of execution time) and hence the limitation of</p>
<p>approaches requiring only nested designs. We believe that this may happen because nonnested designs allow the procedure learn from different input locations at different fidelity levels, however a more detailed examination of this phenomenon is out of the scope of this study. Finally, we observe that ABTCK presented smaller model fitting time than the other co-kriging approaches. This is the result of the partitioning in ABTCK that requires inversion of smaller covariance matrices than other methods. In fact, the computational overhead introduced by the RJ operation is dominated by the computational gain due to the partition and subsequent inversion of smaller matrices.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">HFGP</th>
<th style="text-align: center;">NBCK</th>
<th style="text-align: center;">K\&amp;O</th>
<th style="text-align: center;">ZBCK</th>
<th style="text-align: center;">ABCK</th>
<th style="text-align: center;">ABTCK</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MSPE</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.0602</td>
<td style="text-align: center;">0.0550</td>
<td style="text-align: center;">0.0339</td>
<td style="text-align: center;">0.0221</td>
<td style="text-align: center;">0.0006</td>
</tr>
<tr>
<td style="text-align: center;">Time (sec)</td>
<td style="text-align: center;">71.12</td>
<td style="text-align: center;">314.11</td>
<td style="text-align: center;">738.74</td>
<td style="text-align: center;">484.86</td>
<td style="text-align: center;">500.75</td>
<td style="text-align: center;">220.84</td>
</tr>
</tbody>
</table>
<p>Table 1: The computed MSPE and the corresponding model fitting time in seconds using: the High Fidelity Gaussian process (HFGP), nested Bayesian Co-kriging with nested design (NBCK), Zertuche's Bayesian Co-kriging (ZBCK), Augmented Bayesian Co-kriging (ABCK), Augmented Bayesian Treed Co-kriging (ABTCK).</p>
<p>In Figures 2a and 2b, we can see the that predictive mean of high fidelity response $y_{2}(\boldsymbol{x})$ by ABTCK h predictive mean of high fidelity response $y_{2}(\boldsymbol{x})$ for ABCK and ABTCKas been able to emulate the sinusoidal dependence of $y_{2}(\boldsymbol{x})$ on the left hand side more accurately than ABCK. The Monte Carlo approximation of the posterior mean of the scalar discrepancy $\hat{\xi}(\boldsymbol{x}) \approx \frac{1}{N} \sum_{j=1}^{N} \boldsymbol{w}<em k="1">{t}(\boldsymbol{x})^{T}\left(\sum</em>}^{K^{(i)}} 1(\boldsymbol{x}) \hat{\gamma<em k_="k," t="t">{k, t}\left(\boldsymbol{\phi}</em>)$ changes value. In contrast, ABCK produces a posterior scalar discrepancy which is equal to 0.525 and constant throughout the input space due to the lack of partitioning.}^{(j)}\right)\right)$ produced by the ABTCK is presented in Figure 2c. We observe that ABTCK has recovered a representation of the scalar discrepancy which suggests that $\xi(\boldsymbol{x</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The predictive mean of $y_{2}(\boldsymbol{x})$ produced from ABCK in (a), predictive mean of $y_{2}(\boldsymbol{x})$ produced from ABTCK in (b), and posterior mean of the scalar discrepancy $\xi_{1}(\boldsymbol{x})$ between low and high fidelity computer models from ABTCK in (c). ABCK produced a posterior mean for $\xi_{1}(\boldsymbol{x})$ around 0.525.</p>
<h3>3.2 Heat transfer benchmark example</h3>
<p>We consider the benchmark problem of a heated metal block of size $\mathcal{X}=[0,1] \times[0,3]$ with a rectangular cavity of size $[0.5,0.015] \times[1,2.5]$ where the temperature $u(\boldsymbol{x})$ is modeled as an elliptic partial differential equation. Let us consider 2D elliptic PDEs $-\nabla \cdot c^{(j)}(\boldsymbol{x}) \nabla u^{(j)}(\boldsymbol{x})=$ $f(\boldsymbol{x})$ with $\boldsymbol{x}=\left(x_{1}, x_{2}\right)$ and $\boldsymbol{x} \in \mathcal{X}-\partial \mathcal{X}$, for $j=1,2,3$. The left side of the block is heated to 100 degrees and hence we consider Dirichlet condition $u=100$. At the right side of the metal block, heat is flowing from the block to the surrounding air at a constant rate and we assume Neumann condition $u^{\prime}(\boldsymbol{x})=-20$. The rest boundary conditions are Neumann condition $u^{\prime}(\boldsymbol{x})=0$. The internal heat source is $f(\boldsymbol{x})=1$.</p>
<p>Assume that there are three computer models aiming at describing the steady state of the temperature, and they are arranged in ascending order of fidelity as $\left{\mathfrak{C}^{(t)}\right}<em 2="2">{t=1}^{3}$. The spatial dependent thermal connectivity is denoted as $c^{(j)}(\boldsymbol{x})$; it is $c^{(1)}(\boldsymbol{x})=1$ for the least accurate computer model, $c^{(2)}(\boldsymbol{x})=\exp \left(1.5 \sin \left(3.33 \pi x</em>$ discretized in 24119 nodes. The temperature produced by the three computer models is presented in Figures 3a, 3b, and 3c.}\right)\right) 1\left(x_{2}&lt;1.8\right)$ for more accurate computer model, and $c^{(3)}(\boldsymbol{x})=\exp \left(1.5 \sin \left(3.33 \pi x_{2}\right)\right)$ for most accurate computer model. The PDE is solved via a FEM solver with the domain $\mathcal{X</p>
<p>There is an obvious discontinuity at $x_{1}=0.5$. The accurate model $\mathfrak{C}^{(3)}$ has high frequen-</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*The two authors contributed equally to this work. Corresponding authors: Bledar A. Konomi (alex.konomi@uc.edu) and Georgios Karagiannis (georgios.karagiannis@durham.ac.uk).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9878 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9878</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9878</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-a5524d085ac586d531021dcb1ec156eaf942b109</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a5524d085ac586d531021dcb1ec156eaf942b109" target="_blank">Humanity's Last Exam</a></p>
                <p><strong>Paper Venue:</strong> Robotics</p>
                <p><strong>Paper TL;DR:</strong> HLE is introduced, a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage, to inform research and policymaking upon a clear understanding of model capabilities.</p>
                <p><strong>Paper Abstract:</strong> Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce HUMANITY’S LAST EXAM (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,700 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9878.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9878.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HLE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Humanity's Last Exam</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal, expert-curated benchmark of 2,500 extremely challenging closed-ended academic questions across dozens of subjects designed to probe frontier LLM capabilities; includes multiple-choice and exact-match items, a public and private split, and an iterative LLM+expert review pipeline to ensure difficulty and non-searchability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various frontier LLMs (e.g., GPT-40, Grok 2, Claude 3.5 Sonnet, Gemini 1.5 Pro, Gemini 2.0, o1, DeepSeek-R1, o3-Mini)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Frontier multimodal and text-only large language models evaluated by the paper; specific model versions and modality notes are provided in the paper (Section C.5); some models are non-multimodal and evaluated on the text-only subset.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General academic domains spanning mathematics, natural sciences, humanities, computer science, linguistics, ecology, etc. (closed-ended academic questions across dozens of subjects).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated evaluation of model outputs against ground-truth exact-match or multiple-choice answers using a standardized system prompt (structured reasoning + final answer), with an LLM judge (o3-mini) to verify equivalence of answers and a held-out private test set to detect overfitting; models were also prompted to provide confidence for calibration analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary metrics are top-line accuracy (percentage correct) and RMS calibration error (root-mean-square difference between stated confidence and empirical accuracy); additional measurements include completion token counts (inference cost) and per-category accuracies (text-only subset vs multimodal).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Humanity's Last Exam (HLE): 2,500 expert-written, precise, non-searchable multi-modal questions (approx. 14% require image comprehension), mixing multiple-choice (~24%) and exact-match items; includes public set and private held-out test set.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>State-of-the-art models achieve very low accuracy on HLE (examples from Table 1: GPT-40 2.7%, Grok 2 3.0%, Claude 3.5 Sonnet 4.1%, Gemini 1.5 Pro 4.6%, Gemini 2.0 6.6%, o1 8.0%, DeepSeek-R1 8.5%, o3-Mini 13.4%) and uniformly high RMS calibration error (examples: 73–89%), indicating low accuracy and poor calibration; reasoning models use many more completion tokens than non-reasoning models for modest performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dataset is intentionally filtered to be difficult (questions that frontier LLMs could solve were rejected), so low accuracy is partly by construction; noise in model inference can yield inconsistent non-zero accuracy; evaluation depends on automated judging (o3-mini) and canonical answers which require careful equivalence handling; calibration measurement relies on self-reported confidences, which models may not report reliably; multimodality limits apply to non-multimodal models (evaluated on text-only subset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>HLE is explicitly designed to measure the gap between LLMs and expert human performance on closed-ended academic questions; dataset creation and verification use human subject-matter experts (graduate-level reviewers) to ensure questions are unambiguous and expert-level, and the paper notes that high performance on HLE would indicate expert-level closed-ended capabilities but would not imply autonomous research ability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multi-stage question vetting that includes LLM difficulty checks and expert human review; keep a private held-out test set to detect overfitting/gaming of the public set; design questions to be precise, unambiguous, non-searchable, and automatable (exact-match or multiple-choice) to enable objective scoring; prompt models to provide confidence to measure calibration; use an LLM judge that accounts for equivalent answer formats; prefer short, verifiable answers for exact-match items; iteratively refine questions that models answer correctly (e.g., change number of options) to reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Humanity's Last Exam", 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9878.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9878.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMS Calibration Error</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Root-Mean-Square Calibration Error (RMS calibration error)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalar metric that quantifies calibration by measuring the root-mean-square difference between a model's stated confidence and its empirical accuracy across items; implemented following prior work referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Evaluated across the frontier LLMs listed in the paper (e.g., GPT-40, Grok 2, Claude 3.5 Sonnet, Gemini models, o1, DeepSeek-R1, o3-Mini)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Frontier LLMs prompted to produce both an answer and a confidence (0–100%) for each question; RMS calibration error computed from these pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General evaluation of model reliability across academic question answering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt models to return a confidence score per question and compute RMS between confidence and actual correctness; the paper follows the prompting setup from Wei et al. and an implementation detail attributed to Hendrycks et al.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>RMS calibration error (lower is better); interpretation: a well-calibrated model's stated confidence should match empirical accuracy (e.g., 50% confidence coinciding with 50% accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Computed on HLE (per-model RMS calibration errors reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>All evaluated models show high RMS calibration errors (examples: GPT-40 89%, Grok 2 87%, Claude 3.5 Sonnet 84%, Gemini 1.5 Pro 88%, Gemini 2.0 82%, o1 83%, DeepSeek-R1 73%, o3-Mini 80%), indicating models are overconfident and poorly calibrated on these difficult questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on models providing meaningful, comparable numeric confidences; confidence reporting can be unreliable or adversarially shaped by system prompts; high RMS may reflect both poor epistemic uncertainty estimation and dataset difficulty; calibration computed on intentionally hard questions may be dominated by dataset construction effects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Unlike human experts who may abstain or hedge, models frequently give high-confidence wrong answers; RMS quantifies this mismatch, and the paper emphasizes that well-calibrated uncertainty estimates are important for trustworthy deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prompt models to provide explicit confidences using standardized phrasing and scoring procedures; compute RMS calibration error as part of evaluation; pair calibration metrics with accuracy to understand overconfidence; use established implementations and prior prompting setups (as cited) to ensure reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Humanity's Last Exam", 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9878.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9878.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Difficulty Check</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Difficulty Pre-screening</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset collection method in which candidate questions are first validated against several frontier LLMs and only forwarded to human expert review if models fail (or perform worse than random for multiple-choice), intended to ensure dataset remains challenging for current models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Frontier LLMs used as filters during collection (internal pre-release models; paper logs ~70,000 attempts and identified ~13,000 questions that stumped LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Multiple frontier models (text and multimodal) were used during collection to attempt candidate questions; if models could answer correctly the question was rejected or modified.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dataset curation/benchmark construction for general academic question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated querying of frontier LLMs on candidate questions; use average model performance threshold (e.g., worse than random for MCQ or unable to produce correct exact-match) to decide which questions proceed to human review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pass/fail filter based on model success: questions are accepted for expert review only if frontier LLMs fail to answer accurately (or average performance ≤ random for multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Procedure used in creation of HLE (part of the dataset creation pipeline described in Section 3.1 and Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Over the collection process the team logged >70,000 LLM attempts and forwarded ~13,000 LLM-failed questions to human expert review; this pre-screening contributed to the final 2,500-question HLE public set and private held-out split.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Filtering by existing models can bias dataset toward adversarially hard items that exploit current model weaknesses and may become solvable with small model updates; may inadvertently filter out valid questions that models solve for trivial or spurious reasons; requires maintaining a private set to detect overfitting to the public set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Traditional human-curated exams typically rely on expert judgment alone; LLM difficulty pre-screening adds an automated step to ensure items are frontier-hard relative to current models, complementing (not replacing) human expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Combine automated LLM difficulty checks with multi-stage expert review to ensure question quality and long-term usefulness; maintain private held-out sets for evaluation to reduce overfitting; allow iterative question refinements if models answer correctly due to faulty reasoning or format quirks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Humanity's Last Exam", 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9878.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9878.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert Review Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-stage Expert Human Review and Approval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-round human review process where graduate-level subject experts grade and iteratively refine candidate questions (1–3 reviews each), followed by organizer/expert approval to ensure clarity, closed-endedness, and verifiability; includes a public review period after release.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>N/A (human-centric process supplemented by LLM pre-screening and automated evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Not applicable; human reviewers with graduate-level credentials perform selection and refinement, using standardized rubrics (details in Section C.7).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Benchmark/question curation for cross-disciplinary academic domains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human experts review questions forwarded from LLM difficulty check, provide feedback and refinements in round 1 (1–3 reviews each), and then organizers/expert reviewers approve high-quality questions in round 2; each submission must include solution rationale and metadata for accountability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Rubric-based assessment for precision, unambiguity, non-searchability, suitability for automated grading (exact-match or multiple-choice), and adherence to content restrictions (no WMD-related content); reviewer judgments determine whether a question is accepted, revised, or rejected.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used to curate the HLE public dataset and private held-out set; ensures domain expert verification and long-term dataset integrity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Human review reduced candidate set to high-quality, expert-verified questions; final HLE contains 2,500 items vetted via this pipeline. Disagreement rates and rubric details are discussed in the paper (Section B.3 and C.7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human review is resource-intensive and may have inter-reviewer disagreement; expert judgment can introduce domain-specific biases; maintaining consistent rubrics and training reviewers is necessary to control quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Aligns with traditional exam construction by domain experts but augments the process with LLM pre-screening and explicit requirements for machine-evaluable answers; provides explicit accountability through contributor affiliations and supplied solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Employ standardized rubrics and reviewer training, require detailed rationales/solutions with each submission, combine multiple rounds of review, and publish both public and private splits with a post-release public review period to correct issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Humanity's Last Exam", 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9878.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9878.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini (used as an automated judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight LLM used in this work as an automated judge to verify model outputs against canonical answers while accounting for equivalent answer formats (e.g., decimal vs fraction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Described in the paper as an LLM used as an answer judge; noted as a high-capacity small model variant used to verify answer correctness and equivalence on exact-match questions (details in Sections 4.1 and C.1.1).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automated grading/judging for closed-ended academic question evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>o3-mini is prompted to compare model outputs to ground-truth answers while accounting for equivalent formats and to produce a correctness judgment used for accuracy scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Judgments of equivalence or correctness for exact-match and multiple-choice outputs (used to compute accuracy metrics reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the HLE dataset as the primary automated judge for answer equivalence and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>o3-mini judged model outputs and enabled automated scoring over the 2,500-question HLE set; its use allowed consistent handling of format-equivalent answers and scaling of evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Using an LLM as judge risks propagating judge-model biases and mistakes; judge failure modes could affect measured accuracy if equivalence rules are complex; the paper mitigates this by requiring short, verifiable answers and by human-provided canonical solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated LLM judging scales better than manual human grading for large datasets but requires careful prompt design and equivalence handling; human review remains essential for dataset creation and ambiguous cases.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use an LLM judge that is tuned or prompted to robustly handle answer formatting differences and equivalences, and validate judge decisions on a sample with human checks; keep canonical short answers to minimize judging ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Humanity's Last Exam", 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding <em>(Rating: 2)</em></li>
                <li>Measuring short-form factuality in large language models <em>(Rating: 2)</em></li>
                <li>Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai <em>(Rating: 2)</em></li>
                <li>Omnimath: A universal olympiad level mathematic benchmark for large language models <em>(Rating: 2)</em></li>
                <li>Not all llm reasoners are created equal <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9878",
    "paper_id": "paper-a5524d085ac586d531021dcb1ec156eaf942b109",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "HLE",
            "name_full": "Humanity's Last Exam",
            "brief_description": "A multi-modal, expert-curated benchmark of 2,500 extremely challenging closed-ended academic questions across dozens of subjects designed to probe frontier LLM capabilities; includes multiple-choice and exact-match items, a public and private split, and an iterative LLM+expert review pipeline to ensure difficulty and non-searchability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Various frontier LLMs (e.g., GPT-40, Grok 2, Claude 3.5 Sonnet, Gemini 1.5 Pro, Gemini 2.0, o1, DeepSeek-R1, o3-Mini)",
            "llm_description": "Frontier multimodal and text-only large language models evaluated by the paper; specific model versions and modality notes are provided in the paper (Section C.5); some models are non-multimodal and evaluated on the text-only subset.",
            "scientific_domain": "General academic domains spanning mathematics, natural sciences, humanities, computer science, linguistics, ecology, etc. (closed-ended academic questions across dozens of subjects).",
            "evaluation_method": "Automated evaluation of model outputs against ground-truth exact-match or multiple-choice answers using a standardized system prompt (structured reasoning + final answer), with an LLM judge (o3-mini) to verify equivalence of answers and a held-out private test set to detect overfitting; models were also prompted to provide confidence for calibration analysis.",
            "evaluation_criteria": "Primary metrics are top-line accuracy (percentage correct) and RMS calibration error (root-mean-square difference between stated confidence and empirical accuracy); additional measurements include completion token counts (inference cost) and per-category accuracies (text-only subset vs multimodal).",
            "benchmark_or_dataset": "Humanity's Last Exam (HLE): 2,500 expert-written, precise, non-searchable multi-modal questions (approx. 14% require image comprehension), mixing multiple-choice (~24%) and exact-match items; includes public set and private held-out test set.",
            "results_summary": "State-of-the-art models achieve very low accuracy on HLE (examples from Table 1: GPT-40 2.7%, Grok 2 3.0%, Claude 3.5 Sonnet 4.1%, Gemini 1.5 Pro 4.6%, Gemini 2.0 6.6%, o1 8.0%, DeepSeek-R1 8.5%, o3-Mini 13.4%) and uniformly high RMS calibration error (examples: 73–89%), indicating low accuracy and poor calibration; reasoning models use many more completion tokens than non-reasoning models for modest performance gains.",
            "limitations_or_challenges": "Dataset is intentionally filtered to be difficult (questions that frontier LLMs could solve were rejected), so low accuracy is partly by construction; noise in model inference can yield inconsistent non-zero accuracy; evaluation depends on automated judging (o3-mini) and canonical answers which require careful equivalence handling; calibration measurement relies on self-reported confidences, which models may not report reliably; multimodality limits apply to non-multimodal models (evaluated on text-only subset).",
            "comparison_to_human_or_traditional": "HLE is explicitly designed to measure the gap between LLMs and expert human performance on closed-ended academic questions; dataset creation and verification use human subject-matter experts (graduate-level reviewers) to ensure questions are unambiguous and expert-level, and the paper notes that high performance on HLE would indicate expert-level closed-ended capabilities but would not imply autonomous research ability.",
            "recommendations_or_best_practices": "Use multi-stage question vetting that includes LLM difficulty checks and expert human review; keep a private held-out test set to detect overfitting/gaming of the public set; design questions to be precise, unambiguous, non-searchable, and automatable (exact-match or multiple-choice) to enable objective scoring; prompt models to provide confidence to measure calibration; use an LLM judge that accounts for equivalent answer formats; prefer short, verifiable answers for exact-match items; iteratively refine questions that models answer correctly (e.g., change number of options) to reduce false positives.",
            "uuid": "e9878.0",
            "source_info": {
                "paper_title": "Humanity's Last Exam",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "RMS Calibration Error",
            "name_full": "Root-Mean-Square Calibration Error (RMS calibration error)",
            "brief_description": "A scalar metric that quantifies calibration by measuring the root-mean-square difference between a model's stated confidence and its empirical accuracy across items; implemented following prior work referenced in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Evaluated across the frontier LLMs listed in the paper (e.g., GPT-40, Grok 2, Claude 3.5 Sonnet, Gemini models, o1, DeepSeek-R1, o3-Mini)",
            "llm_description": "Frontier LLMs prompted to produce both an answer and a confidence (0–100%) for each question; RMS calibration error computed from these pairs.",
            "scientific_domain": "General evaluation of model reliability across academic question answering tasks.",
            "evaluation_method": "Prompt models to return a confidence score per question and compute RMS between confidence and actual correctness; the paper follows the prompting setup from Wei et al. and an implementation detail attributed to Hendrycks et al.",
            "evaluation_criteria": "RMS calibration error (lower is better); interpretation: a well-calibrated model's stated confidence should match empirical accuracy (e.g., 50% confidence coinciding with 50% accuracy).",
            "benchmark_or_dataset": "Computed on HLE (per-model RMS calibration errors reported in Table 1).",
            "results_summary": "All evaluated models show high RMS calibration errors (examples: GPT-40 89%, Grok 2 87%, Claude 3.5 Sonnet 84%, Gemini 1.5 Pro 88%, Gemini 2.0 82%, o1 83%, DeepSeek-R1 73%, o3-Mini 80%), indicating models are overconfident and poorly calibrated on these difficult questions.",
            "limitations_or_challenges": "Relies on models providing meaningful, comparable numeric confidences; confidence reporting can be unreliable or adversarially shaped by system prompts; high RMS may reflect both poor epistemic uncertainty estimation and dataset difficulty; calibration computed on intentionally hard questions may be dominated by dataset construction effects.",
            "comparison_to_human_or_traditional": "Unlike human experts who may abstain or hedge, models frequently give high-confidence wrong answers; RMS quantifies this mismatch, and the paper emphasizes that well-calibrated uncertainty estimates are important for trustworthy deployment.",
            "recommendations_or_best_practices": "Prompt models to provide explicit confidences using standardized phrasing and scoring procedures; compute RMS calibration error as part of evaluation; pair calibration metrics with accuracy to understand overconfidence; use established implementations and prior prompting setups (as cited) to ensure reproducibility.",
            "uuid": "e9878.1",
            "source_info": {
                "paper_title": "Humanity's Last Exam",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM Difficulty Check",
            "name_full": "LLM Difficulty Pre-screening",
            "brief_description": "A dataset collection method in which candidate questions are first validated against several frontier LLMs and only forwarded to human expert review if models fail (or perform worse than random for multiple-choice), intended to ensure dataset remains challenging for current models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Frontier LLMs used as filters during collection (internal pre-release models; paper logs ~70,000 attempts and identified ~13,000 questions that stumped LLMs).",
            "llm_description": "Multiple frontier models (text and multimodal) were used during collection to attempt candidate questions; if models could answer correctly the question was rejected or modified.",
            "scientific_domain": "Dataset curation/benchmark construction for general academic question answering.",
            "evaluation_method": "Automated querying of frontier LLMs on candidate questions; use average model performance threshold (e.g., worse than random for MCQ or unable to produce correct exact-match) to decide which questions proceed to human review.",
            "evaluation_criteria": "Pass/fail filter based on model success: questions are accepted for expert review only if frontier LLMs fail to answer accurately (or average performance ≤ random for multiple-choice).",
            "benchmark_or_dataset": "Procedure used in creation of HLE (part of the dataset creation pipeline described in Section 3.1 and Figure 4).",
            "results_summary": "Over the collection process the team logged &gt;70,000 LLM attempts and forwarded ~13,000 LLM-failed questions to human expert review; this pre-screening contributed to the final 2,500-question HLE public set and private held-out split.",
            "limitations_or_challenges": "Filtering by existing models can bias dataset toward adversarially hard items that exploit current model weaknesses and may become solvable with small model updates; may inadvertently filter out valid questions that models solve for trivial or spurious reasons; requires maintaining a private set to detect overfitting to the public set.",
            "comparison_to_human_or_traditional": "Traditional human-curated exams typically rely on expert judgment alone; LLM difficulty pre-screening adds an automated step to ensure items are frontier-hard relative to current models, complementing (not replacing) human expert review.",
            "recommendations_or_best_practices": "Combine automated LLM difficulty checks with multi-stage expert review to ensure question quality and long-term usefulness; maintain private held-out sets for evaluation to reduce overfitting; allow iterative question refinements if models answer correctly due to faulty reasoning or format quirks.",
            "uuid": "e9878.2",
            "source_info": {
                "paper_title": "Humanity's Last Exam",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Expert Review Pipeline",
            "name_full": "Multi-stage Expert Human Review and Approval",
            "brief_description": "A two-round human review process where graduate-level subject experts grade and iteratively refine candidate questions (1–3 reviews each), followed by organizer/expert approval to ensure clarity, closed-endedness, and verifiability; includes a public review period after release.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "N/A (human-centric process supplemented by LLM pre-screening and automated evaluation)",
            "llm_description": "Not applicable; human reviewers with graduate-level credentials perform selection and refinement, using standardized rubrics (details in Section C.7).",
            "scientific_domain": "Benchmark/question curation for cross-disciplinary academic domains.",
            "evaluation_method": "Human experts review questions forwarded from LLM difficulty check, provide feedback and refinements in round 1 (1–3 reviews each), and then organizers/expert reviewers approve high-quality questions in round 2; each submission must include solution rationale and metadata for accountability.",
            "evaluation_criteria": "Rubric-based assessment for precision, unambiguity, non-searchability, suitability for automated grading (exact-match or multiple-choice), and adherence to content restrictions (no WMD-related content); reviewer judgments determine whether a question is accepted, revised, or rejected.",
            "benchmark_or_dataset": "Used to curate the HLE public dataset and private held-out set; ensures domain expert verification and long-term dataset integrity.",
            "results_summary": "Human review reduced candidate set to high-quality, expert-verified questions; final HLE contains 2,500 items vetted via this pipeline. Disagreement rates and rubric details are discussed in the paper (Section B.3 and C.7).",
            "limitations_or_challenges": "Human review is resource-intensive and may have inter-reviewer disagreement; expert judgment can introduce domain-specific biases; maintaining consistent rubrics and training reviewers is necessary to control quality.",
            "comparison_to_human_or_traditional": "Aligns with traditional exam construction by domain experts but augments the process with LLM pre-screening and explicit requirements for machine-evaluable answers; provides explicit accountability through contributor affiliations and supplied solutions.",
            "recommendations_or_best_practices": "Employ standardized rubrics and reviewer training, require detailed rationales/solutions with each submission, combine multiple rounds of review, and publish both public and private splits with a post-release public review period to correct issues.",
            "uuid": "e9878.3",
            "source_info": {
                "paper_title": "Humanity's Last Exam",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "o3-mini judge",
            "name_full": "o3-mini (used as an automated judge)",
            "brief_description": "A lightweight LLM used in this work as an automated judge to verify model outputs against canonical answers while accounting for equivalent answer formats (e.g., decimal vs fraction).",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "o3-mini",
            "llm_description": "Described in the paper as an LLM used as an answer judge; noted as a high-capacity small model variant used to verify answer correctness and equivalence on exact-match questions (details in Sections 4.1 and C.1.1).",
            "scientific_domain": "Automated grading/judging for closed-ended academic question evaluation.",
            "evaluation_method": "o3-mini is prompted to compare model outputs to ground-truth answers while accounting for equivalent formats and to produce a correctness judgment used for accuracy scoring.",
            "evaluation_criteria": "Judgments of equivalence or correctness for exact-match and multiple-choice outputs (used to compute accuracy metrics reported in the paper).",
            "benchmark_or_dataset": "Applied to the HLE dataset as the primary automated judge for answer equivalence and scoring.",
            "results_summary": "o3-mini judged model outputs and enabled automated scoring over the 2,500-question HLE set; its use allowed consistent handling of format-equivalent answers and scaling of evaluation.",
            "limitations_or_challenges": "Using an LLM as judge risks propagating judge-model biases and mistakes; judge failure modes could affect measured accuracy if equivalence rules are complex; the paper mitigates this by requiring short, verifiable answers and by human-provided canonical solutions.",
            "comparison_to_human_or_traditional": "Automated LLM judging scales better than manual human grading for large datasets but requires careful prompt design and equivalence handling; human review remains essential for dataset creation and ambiguous cases.",
            "recommendations_or_best_practices": "Use an LLM judge that is tuned or prompted to robustly handle answer formatting differences and equivalences, and validate judge decisions on a sample with human checks; keep canonical short answers to minimize judging ambiguity.",
            "uuid": "e9878.4",
            "source_info": {
                "paper_title": "Humanity's Last Exam",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 2
        },
        {
            "paper_title": "Measuring short-form factuality in large language models",
            "rating": 2
        },
        {
            "paper_title": "Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai",
            "rating": 2
        },
        {
            "paper_title": "Omnimath: A universal olympiad level mathematic benchmark for large language models",
            "rating": 2
        },
        {
            "paper_title": "Not all llm reasoners are created equal",
            "rating": 1
        }
    ],
    "cost": 0.0199065,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Humanity's Last Exam</h1>
<h2>Organizing Team</h2>
<p>Long Phan ${ }^{<em> 1}$, Alice Gatti ${ }^{</em> 1}$, Ziwen Han<em>2, Nathaniel Li ${ }^{</em> 1}$,
Josephina $\mathrm{Hu}^{2}$, Hugh Zhang ${ }^{1}$, Chen Bo Calvin Zhang ${ }^{2}$, Mohamed Shaaban ${ }^{2}$, John Ling ${ }^{2}$, Sean Shi ${ }^{2}$, Michael Choi ${ }^{2}$, Anish Agrawal ${ }^{2}$, Arnav Chopra ${ }^{2}$, Adam Khoja ${ }^{1}$, Ryan Kim ${ }^{1}$, Richard Ren ${ }^{1}$, Jason Hausenloy ${ }^{1}$, Oliver Zhang ${ }^{1}$, Mantas Mazeika ${ }^{1}$,
Summer Yue ${ }^{<em> * 2}$, Alexandr Wang ${ }^{</em> * 2}$, Dan Hendrycks ${ }^{* * 1}$
${ }^{1}$ Center for AI Safety, ${ }^{2}$ Scale AI</p>
<h2>Dataset Contributors</h2>
<p>Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav Kazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G. Willcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward Vendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow, Natanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod, Gözdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang, Paolo Rissone, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin Imperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad Hogg, Carlo Bosio, Brian P Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov, Philippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Kelsey Van den Houte, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust, Bikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Ariel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Joseph M Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan, Sergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes, Alexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Zakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias Kreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger, Kaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov, Václav Rozhoň, Vincent Ginis, Christian Stump, Niv Cohen, Rafał Poświata, Josef Tkadlec, Alan Goldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan Stendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek Shukla, Alan Givré, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark H Inlow, Hao He, Ling Zhang, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Harrison K Wang, Kalyan Ramakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik Kirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Yuzhou Nie, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Shreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, Jérémy Andréoletti, Raghav Singhal, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Marco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Khánh, Paul Rosu, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William Alley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob Loader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida Bosio, Fereshteh Kazemi, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Jiaqi Wang, Maria Inês S. Nunes, Jeremiah Milbauer, M Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff, Lynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D.O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee, Robin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Jiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota, Ronak Pradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâcâ, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K. Zhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan, Andrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Claudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David Stap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo Rodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna Samuele, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Pefiaflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez, Daniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon Park, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin Yong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo Albani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yiğit Yalın, Gbenga Daniel Obikoya, Rai (Michael Pokorny), Filippo Bigi, M.C. Boscá, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C.H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu (Quinn) Liu, Stefano Cavalleri, Olle Häggström, Emil Verkama, Joshua Newbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Ting Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder de Witt, Pablo Hernández-Cámara, Emanuele Rodolà, Jules Robins, Dominic Williamson, Vincent Cheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D.P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth Anderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro José Moyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier, Omid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh Ducey, Matheus Piza, Maja Somrak, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Antoine Jallon, I.M.J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah, Marc Carauleanu, Pascal Lauer, Tran Duc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark, Assaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel Poesia Reis e Silva, Long (Tony) Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam, Juan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume, Wiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo, Jakub Łucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa Gonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo Jiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria,</p>
<p>Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels Mündler, Sören Möller, Luca Arnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony Fruhauff, Glen Sherman, Mátyás Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan, Innocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John Lai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P V, Michael Richmond, Joseph McGowan, Tejal Patwardhan
Late Contributors Hao-Yu Sun, Ting Sun, Nikola Zubić, Samuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella, Alex Meiburg, Tilen Medved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang, Jason Poulos, Vilém Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha, Yinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng Wu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Mickaël Noyé, Michał Perełkiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa Nguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario Abbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del Rio-Chanona, Dániel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny Reddish, Jakob Hauser, Francisco-Javier Rodrigo-Ginés, Suchandra Datta, Maxwell Shepherd, Thom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt, Satyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar Shridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David (Quod) Soler Bartomeu, Tony CY Pang, Adam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena, Xing Han Lü, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming Yin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez, Costin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin Briański, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-Orallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen, Alexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan Todoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther Yap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqj Wang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira Arrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam Bouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas Subramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim, Yushun Chen, Sara Vera Marjanović, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen, Dawn Song, Cedegao E. Zhang, Zhun Wang, Gaël Gendron, Yunze Xiao, Leo Smucker, Erica Weng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony Gitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin, Philipp D. Siedler, Jichao Fang, Ritwik Mishra, Károly Zsolnai-Fehér, Xilin Jiang, Shadab Khan, Jun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang, Isha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao Zheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen, Deepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V. Gonçalves, Wei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline Geirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Yücel, Brandon Christof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac Park, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng, Zhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang, Bruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang, Marc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying Liu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe, Hongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset, Zishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia Chernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park, Hieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui Pan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W. Bartlett, Christopher R. Scotese, Phuong M. Cao, Ben Wu, Jacek Karwowski, Davide Scaramuzza</p>
<p>Auditors Aakaash Nattanmai, Gordon McKellips, Anish Cheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay Paek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent Cheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath, Violet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin Wang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Xiangwan Sun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert Yang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David Sun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan Zhou, Aidan Wu, Jason Luo, Anwith Telluri</p>
<h1>Abstract</h1>
<p>Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over $90 \%$ accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.</p>
<h2>1 Introduction</h2>
<p>The capabilities of large language models (LLMs) have progressed dramatically, exceeding human performance across a diverse array of tasks. To systematically measure these capabilities, LLMs are evaluated upon benchmarks: collections of questions which assess model performance on tasks such as math, programming, or biology. However, state-of-the-art LLMs [3, 15, 17, 35, 38, 51, 58] now achieve over $90 \%$ accuracy on popular benchmarks such as MMLU [22], which were once challenging frontiers for LLMs. The saturation of existing benchmarks, as shown in Figure 1, limits our ability to precisely measure AI capabilities and calls for more challenging evaluations that can meaningfully assess the rapid improvements in LLM capabilities at the frontiers of human knowledge.
To address this gap, we introduce Humanity's Last Exam (HLE), a benchmark of 2,500 extremely challenging questions from dozens of subject areas, designed to be the final closed-ended benchmark of broad academic capabilities. HLE is developed by academics and domain experts, providing a precise measure of capabilities as LLMs continue to improve (Section 3.1). HLE is multi-modal, featuring questions that are either text-only or accompanied by an image reference, and includes both multiple-choice and exact-match questions for automated answer verification. Questions are original, precise, unambiguous, and resistant to simple internet lookup or database retrieval. Amongst the diversity of questions in the benchmark, HLE emphasizes world-class mathematics problems aimed at testing deep reasoning skills broadly applicable across multiple academic areas.
We employ a multi-stage review process to thoroughly ensure question difficulty and quality (Section 3.2). Before submission, each question is tested against state-of-the-art LLMs to verify its difficulty - questions are rejected if LLMs can answer them correctly. Questions submitted then proceed through a two-stage reviewing process: (1) an initial feedback round with multiple graduatelevel reviewers and (2) organizer and expert reviewer approval, ensuring quality and adherence to our submission criteria. Following release, we conducted a public review period, welcoming community feedback to correct any points of concern in the dataset.
Frontier LLMs consistently demonstrate low accuracy across all models, highlighting a significant gap between current capabilities and expert-level academic performance (Section 4). Models also provide incorrect answers with high confidence rather than acknowledging uncertainty on these challenging questions, with RMS calibration errors above $70 \%$ across all models.
As AI systems approach human expert performance in many domains, precise measurement of their capabilities and limitations is essential for informing research, governance, and the broader public. High performance on HLE would suggest expert-level capabilities on closed-ended academic questions. To establish a common reference point for assessing these capabilities, we publicly release a large number of 2,500 questions from HLE to enable this precise measurement, while maintaining a private test set to assess potential model overfitting.</p>
<h1>Accuracy of LLMs Across Benchmarks</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Compared against the saturation of some existing benchmarks, Humanity's Last Exam accuracy remains low across several frontier models, demonstrating its effectiveness for measuring advanced, closed-ended, academic capabilities. The sources for our evaluation metrics are detailed in Section C.6. We further evaluate more frontier models on HLE in Table 1.</p>
<h2>2 Related Work</h2>
<p>LLM Benchmarks. Benchmarks are important tools for tracking the rapid advancement of LLM capabilities, including scientific $[11,13,22,30,31,45,49,55,63]$ and mathematical reasoning $[14,18-$ $20,23,32,46,52]$, code generation $[7,10-12,21,27,62]$, and general-purpose human assistance $[1,8,9,26,41,43,44,49,56]$. Due to their objectivity and ease of automated scoring at scale, evaluations commonly include multiple-choice and short-answer questions [16, 43, 53, 54, 60], with benchmarks such as MMLU [22] also spanning a broad range of academic disciplines and levels of complexity.</p>
<p>Saturation and Frontier Benchmark Design. However, state-of-the-art models now achieve nearly perfect scores on many existing evaluations [3, 15, 17, 35, 38, 51, 58], obscuring the full extent of current and future frontier AI capabilities [28, 33, 39, 40]. This has motivated the development of more challenging benchmarks which test for multi-modal capabilities [2, 11, 27, 29, 32, 48, $50,55,59,61]$, strengthen existing benchmarks [25, 44, 46, 50, 55], filter questions over multiple stages of review [19, 28, 31, 34, 45], and employ experts to write tests for advanced academic knowledge [5, 19, 31, 35, 42, 45]. HLE combines these approaches: the questions are developed by subject-matter experts and undergo multiple rounds of review, while preserving the broad subjectmatter coverage of MMLU. As a result, HLE provides a clear measurement of the gap between current AI capabilities and human expertise on closed-ended academic tasks, complementing other assessments of advanced capabilities in open-ended domains [11, 36, 37, 57].</p>
<h2>3 Dataset</h2>
<p>Humanity's Last Exam (HLE) consists of 2,500 challenging questions across over a hundred subjects. A high level summary is provided in Figure 3. We publicly release these questions, while maintaining a private test set of held out questions to assess model overfitting.</p>
<h3>3.1 Collection</h3>
<p>HLE is a global collaborative effort, with questions from nearly 1000 subject expert contributors affiliated with over 500 institutions across 50 countries - comprised mostly of professors, researchers, and graduate degree holders.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Here is a representation of a Roman inscription, orginally found on a tombstone. Provide a translation for the Palmyrene script. A transformation of the text is provided: RGYNr BT HRY BR $\cdot$T HBL</p>
<p>Hens $1^{1}$
2) Merton College, Oxford</p>
<h2>$\triangle$ Mathematics</h2>
<h2>Question:</h2>
<p>The set of natural transformations between two functors $F, G: C \rightarrow D$ can be expressed as the end</p>
<p>$$
\operatorname{Nat}(F, G) \cong \int_{F}^{G} \operatorname{Hom}_{D}(F(A), G(A))
$$</p>
<p>Define set of natural cotransformations from $F$ to $G$ to be the coend</p>
<p>$$
\operatorname{CoNat}(F, G) \cong \int^{F} \operatorname{Hom}_{D}(F(A), G(A))
$$</p>
<p>Let:
$-F=B_{\bullet}\left(\Sigma_{k}\right)<em k="k">{0} \cdot$ be the under $\infty$-category of the nerve of the delooping of the symmetric group $\Sigma</em>$ on 4 letters under the unique 0 -simplex $<em>$ of $B_{\bullet} \Sigma_{k}$.
$-G=B_{\bullet}\left(\Sigma_{7}\right)<em 7="7">{0} \cdot$ be the under $\infty$-category nerve of the delooping of the symmetric group $\Sigma</em>$ on 7 letters under the unique 0 -simplex $</em>$ of $B_{\bullet} \Sigma_{7}$.</p>
<p>How many natural cotransformations are there between $F$ and $G$ ?
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>The reaction shown is a thermal pericyclic cascade that converts the starting heptaene into endiandric acid B methyl ester. The cascade involves three steps: two electrocyclizations followed by a cycloaddition. What types of electrocyclizations are involved in step 1 and step 2, and what type of cycloaddition is involved in step 3?</p>
<p>Provide your answer for the electrocyclizations in the form of [ $\mathrm{n} \pi$ ]con or [ $\mathrm{n} \pi$ ]-dis (where n is the number of $\pi$ electrons involved, and whether it is conrotatory or disrotatory), and your answer for the cycloaddition in the form of [ $\mathrm{m} * \mathrm{n}$ ] (where m and n are the number of atoms on each component).</p>
<p>Noat 8
2) Stanford University</p>
<h2>$\sim$ Ecology</h2>
<h2>Question:</h2>
<p>Hummingbirds within Apodiformes uniquely have a bilaterally paired oval bone, a sesamoid embedded in the caudolateral portion of the expanded, cruciale aponeurosis of insertion of $m$, depressor caudae. How many paired tendons are supported by this sesamoid bone? Answer with a number.
5. Edward V
2) Massachusetts Institute of Technology</p>
<h2>Computer Science</h2>
<h2>Question:</h2>
<p>Let $G$ be a graph. An edge-indicator of $G$ is a function $a:{0,1} \rightarrow$ $V(G)$ such that ${a(0), a(1)} \in E(G)$.</p>
<p>Consider the following Markov Chain $M=M(G)$ :
The statespace of $M$ is the set of all edge-indicators of $G$, and the transitions are defined as follows:</p>
<p>Assume $M_{t}=a$.</p>
<ol>
<li>pick $b \in{0,1}$ u.a.r.</li>
<li>pick $v \in N(a(1-b))$ u.a.r. (here $N(v)$ denotes the open neighbourhood of $v$ )</li>
<li>set $a^{\prime}(b)=v$ and $a^{\prime}(1-b)=a(1-b)$</li>
<li>Set $M_{t+1}=a^{\prime}$</li>
</ol>
<p>We call a class of graphs $\mathcal{G}$ well-behaved if, for each $G \in \mathcal{G}$ the Markov chain $M(G)$ converges to a unique stationary distribution, and the unique stationary distribution is the uniform distribution.</p>
<p>Which of the following graph classes is well-behaved?</p>
<h2>Answer Choices:</h2>
<p>A. The class of all non-bipartite regular graphs
B. The class of all connected cubic graphs
C. The class of all connected graphs
D. The class of all connected non-bipartite graphs
E. The class of all connected bipartite graphs.
5. Hans 9
2) Queen Mary University of London</p>
<h2>$\nearrow$ Linguistics</h2>
<h2>Question:</h2>
<p>I am providing the standardized Biblical Hebrew source text from the Biblia Hebraica Stuttgartensia (Psalms 104:7). Your task is to distinguish between closed and open syllables. Please identify and list all closed syllables (ending in a consonant sound) based on the latest research on the Tiberian pronunciation tradition of Biblical Hebrew by scholars such as Geoffrey Khan, Aaron D. Hornkohl, Kim Phillips, and Benjamin Suchard. Medieval sources, such as the Karate transcription manuscripts, have enabled modern researchers to better understand specific aspects of Biblical Hebrew pronunciation in the Tiberian tradition, including the qualities and functions of the shewa and which letters were pronounced as consonants at the ends of syllables.</p>
<p>[חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזקק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק: [חזק</p>
<p>Question Style. HLE contains two question formats: exact-match questions (models provide an exact string as output) and multiple-choice questions (the model selects one of five or more answer choices). HLE is a multi-modal benchmark, with around 14% of questions requiring comprehending both text and an image. 24% of questions are multiple-choice with the remainder being exact-match.</p>
<p>Each question submission includes several required components: the question text itself, answer specifications (either an an exact-match answer, or multiple-choice options with the correct answer marked), detailed rationale explaining the solution, academic subject, and contributor name and institutional affiliation to maintain accountability and accuracy.</p>
<p>Submission Format. To ensure question quality and integrity, we enforce strict submission criteria. Questions should be precise, unambiguous, solvable, and non-searchable, ensuring models cannot rely on memorization or simple retrieval methods. All submissions must be original work or non-trivial syntheses of published information, though contributions from unpublished research are acceptable. Questions typically require graduate-level expertise or test knowledge of highly specific topics (e.g., precise historical details, trivia, local customs) and have specific, unambiguous answers accepted by domain experts. When LLMs provide correct answers with faulty reasoning, authors are encouraged to modify question parameters, such as the number of answer choices, to discourage false positives. We require clear English with precise technical terminology, supporting $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ notation wherever necessary. Answers are kept short and easily verifiable for exact-match questions to support automatic grading. We prohibit open-ended questions, subjective interpretations, and content related to weapons of mass destruction. Finally, every question is accompanied by a detailed solution to verify accuracy.</p>
<p>Prize Pool. To attract high-quality submissions, we establish a $\$ 500,000$ USD prize pool, with prizes of $\$ 5,000$ USD for each of the top 50 questions and $\$ 500$ USD for each of the next 500 questions, as determined by organizers. This incentive structure, combined with the opportunity for paper co-authorship for anyone with an accepted question in HLE, draws participation from qualified experts, particularly those with advanced degrees or significant technical experience in their fields.</p>
<h1>3.2 Review</h1>
<p>LLM Difficulty Check To ensure question difficulty, each question is first validated against several frontier LLMs prior to submission (Section B.1). If the LLMs cannot solve the question (or in the case of multiple choices, if the models on average do worse than random guessing), the question proceeds to the next stage: human expert review. In total, we logged over 70,000 attempts, resulting in approximately 13,000 questions which stumped LLMs that were forwarded to expert human review.</p>
<p>Expert Review Our human reviewers possess a graduate degree (eg. Master's, PhD, JD, etc.) in their fields. Reviewers select submissions in their domain, grading them against standardized rubrics and offering feedback when applicable. There are two rounds of reviews. The first round focuses
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: HLE consists of 2,500 exam questions in over a hundred subjects, grouped into high level categories here. We provide a more detailed list of subjects in Section B.4.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Dataset creation pipeline. We accept questions that make frontier LLMs fail, then iteratively refine them with the help of expert peer reviewers. Each question is then manually approved by organizers or expert reviewers trained by organizers. A private held-out set is kept in addition to the public set to assess model overfitting and gaming on the public benchmark.
on iteratively refining submissions, with each question receiving between 1-3 reviews. The primary goal is to help the question contributors (who are primarily academics and researchers from a wide range of disciplines) better design questions that are closed-ended, robust, and of high quality for AI evaluation. In the second round, good and outstanding questions from the first round are identified and approved by organizers and reviewers to be included in the final HLE dataset. Details, instructions, and rubrics for both rounds can be found in Section C.7. Figure 4 details our full process. We discuss estimated disagreement rates among experts on HLE in Section B.3.</p>
<h1>4 Evaluation</h1>
<p>We evaluate the performance of state-of-the-art LLMs on HLE and analyze their capabilities across different question types and domains. We describe our evaluation setup (Section 4.1) and present several quantitative results on metrics that track model performance (Section 4.2).</p>
<h3>4.1 Setup</h3>
<p>After data collection and review, we evaluated our final HLE dataset on additional frontier multimodal LLMs. We employ a standardized system prompt that structures model responses into explicit reasoning followed by a final answer. As the question-answers are precise and close-ended, we use o3-mini as a judge to verify answer correctness against model predictions while accounting for equivalent formats (e.g., decimals vs. fractions or estimations). Evaluation prompts are detailed in Section C.1.1, and exact model versions are provided in Section C.5.</p>
<h3>4.2 Quantitative Results</h3>
<p>Accuracy. All frontier models achieve low accuracy on HLE (Table 1), highlighting significant room for improvement in narrowing the gap between current LLMs and expert-level academic capabilities on closed-ended questions. These low scores are partially by design - the dataset collection process (Section 3.1) attempts to filter out questions that existing models can answer correctly. Nevertheless, we notice upon evaluation, models exhibit non-zero accuracy. This is due to inherent noise in model inference - models can inconsistently guess the right answer or guess worse than random chance for multiple choice questions. We choose to leave these questions in the dataset as a natural component instead of strongly adversarially filtering. However, we stress the true capability floor of frontier models on the dataset will remain an open question and small inflections close to zero accuracy are not strongly indicative of progress.</p>
<p>Calibration Error. Given low performance on HLE, models should be calibrated, recognizing their uncertainty rather than confidently provide incorrect answers, indicative of confabulation/hallucination. To measure calibration, we prompt models to provide both an answer and their confidence from $0 \%$ to $100 \%$ (Section C.1.1), employing the setup from Wei et al. [56]. The implementation of our RMS calibration error is from Hendrycks et al. [24]. A well-calibrated model's stated confidence should match its actual accuracy - for example, achieving $50 \%$ accuracy on questions where it claims $50 \%$ confidence. Table 1 reveals poor calibration across all models, reflected in high RMS calibration</p>
<table>
<thead>
<tr>
<th>Pre-Release Models</th>
<th>Accuracy (\%) $\uparrow$</th>
<th>Calibration Error (\%) $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-40</td>
<td>2.7</td>
<td>89</td>
</tr>
<tr>
<td>Grok 2</td>
<td>3.0</td>
<td>87</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>4.1</td>
<td>84</td>
</tr>
<tr>
<td>Gemini 1.5 Pro</td>
<td>4.6</td>
<td>88</td>
</tr>
<tr>
<td>Gemini 2.0 Flash Thinking</td>
<td>6.6</td>
<td>82</td>
</tr>
<tr>
<td>o1</td>
<td>8.0</td>
<td>83</td>
</tr>
<tr>
<td>DeepSeek-R1*</td>
<td>8.5</td>
<td>73</td>
</tr>
<tr>
<td>o3-Mini (High)*</td>
<td>13.4</td>
<td>80</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy and RMS calibration error of different models on HLE, demonstrating low accuracy and high calibration error across all models, indicative of hallucination. *Model is not multi-modal, evaluated on text-only subset. We report text-only results on all models in Section C. 2 and accuracy by category in Section C.3.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Average completion token counts of reasoning models tested, including both reasoning and output tokens. We also plot average token counts for non-reasoning models in Section C.4.
error scores. Models frequently provide incorrect answers with high confidence on HLE, failing to recognize when questions exceed their capabilities.</p>
<p>Token Counts. Models with reasoning require substantially more inference time compute. To shed light on this in our evaluation, we analyze the number of completion tokens used across models. As shown in Figure 5, all reasoning models require generating significantly more tokens compared to non-reasoning models for an improvement in performance (Section C.4). We emphasize that future models should not only do better in terms of accuracy, but also strive to be compute-optimal.</p>
<h1>5 Discussion</h1>
<p>Future Model Performance. While current LLMs achieve very low accuracy on HLE, recent history shows benchmarks are quickly saturated - with models dramatically progressing from near-zero to near-perfect performance in a short timeframe [13, 45]. Given the rapid pace of AI development, it is plausible that models could exceed $50 \%$ accuracy on HLE by the end of 2025. High accuracy on HLE would demonstrate expert-level performance on closed-ended, verifiable questions and cutting-edge scientific knowledge, but it would not alone suggest autonomous research capabilities or "artificial general intelligence." HLE tests structured academic problems rather than open-ended research or creative problem-solving abilities, making it a focused measure of technical knowledge and reasoning. HLE may be the last academic exam we need to give to models, but it is far from the last benchmark for AI.</p>
<p>Impact. By providing a clear measure of AI progress, HLE creates a common reference point for scientists and policymakers to assess AI capabilities. This enables more informed discussions about development trajectories, potential risks, and necessary governance measures.</p>
<h1>References</h1>
<p>[1] C. Alberti, K. Lee, and M. Collins. A bert baseline for the natural questions, 2019. URL https://arxiv.org/abs/1901.08634.
[2] M. Andriushchenko, A. Souly, M. Dziemian, D. Duenas, M. Lin, J. Wang, D. Hendrycks, A. Zou, Z. Kolter, M. Fredrikson, E. Winsor, J. Wynne, Y. Gal, and X. Davies. Agentharm: A benchmark for measuring harmfulness of llm agents, 2024. URL https://arxiv.org/abs/ 2410.09024 .
[3] Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. URL https://api. semanticscholar.org/CorpusID:268232499.
[4] Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet, 2024. URL https://assets.anthropic.com/m/1cd9d098ac3e6467/original/ Claude-3-Model-Card-October-Addendum.pdf.
[5] Anthropic. Responsible scaling policy updates, 2024. URL https://www.anthropic.com/ rsp-updates.
[6] R. K. Arora, J. Wei, R. S. Hicks, P. Bowman, J. Quiñonero-Candela, F. Tsimpourlas, M. Sharman, M. Shah, A. Vallone, A. Beutel, J. Heidecke, and K. Singhal. Healthbench: Evaluating large language models towards improved human health, 2025. URL https://arxiv.org/abs/ 2505.08775 .
[7] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton. Program synthesis with large language models, 2021. URL https: //arxiv.org/abs/2108.07732.
[8] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. HatfieldDodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862.
[9] P. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu, R. Majumder, A. McNamara, B. Mitra, T. Nguyen, M. Rosenberg, X. Song, A. Stoica, S. Tiwary, and T. Wang. Ms marco: A human generated machine reading comprehension dataset, 2018. URL https://arxiv.org/ abs/1611.09268.
[10] M. Bhatt, S. Chennabasappa, C. Nikolaidis, S. Wan, I. Evtimov, D. Gabi, D. Song, F. Ahmad, C. Aschermann, L. Fontana, S. Frolov, R. P. Giri, D. Kapil, Y. Kozyrakis, D. LeBlanc, J. Milazzo, A. Straumann, G. Synnaeve, V. Vontimitta, S. Whitman, and J. Saxe. Purple llama cyberseceval: A secure coding benchmark for language models, 2023. URL https://arxiv.org/abs/ 2312.04724 .
[11] J. S. Chan, N. Chowdhury, O. Jaffe, J. Aung, D. Sherburn, E. Mays, G. Starace, K. Liu, L. Maksin, T. Patwardhan, L. Weng, and A. Madry. Mle-bench: Evaluating machine learning agents on machine learning engineering, 2024. URL https://arxiv.org/abs/2410.07095.
[12] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.
[13] F. Chollet, M. Knoop, G. Kamradt, and B. Landers. Arc prize 2024: Technical report, 2024. URL https://arxiv.org/abs/2412.04604.</p>
<p>[14] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168.
[15] DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://github.com/ deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf.
[16] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019. URL https: //arxiv.org/abs/1903.00161.
[17] A. Dubey et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407. 21783.
[18] B. Gao, F. Song, Z. Yang, Z. Cai, Y. Miao, Q. Dong, L. Li, C. Ma, L. Chen, R. Xu, Z. Tang, B. Wang, D. Zan, S. Quan, G. Zhang, L. Sha, Y. Zhang, X. Ren, T. Liu, and B. Chang. Omnimath: A universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985.
[19] E. Glazer, E. Erdil, T. Besiroglu, D. Chicharro, E. Chen, A. Gunning, C. F. Olsson, J.-S. Denain, A. Ho, E. de Oliveira Santos, O. Järviniemi, M. Barnett, R. Sandler, J. Sevilla, Q. Ren, E. Pratt, L. Levine, G. Barkley, N. Stewart, B. Grechuk, T. Grechuk, and S. V. Enugandla. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai, 2024. URL https://arxiv.org/abs/2411.04872.
[20] C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu, L. Qi, Z. Liu, and M. Sun. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/ abs/2402.14008.
[21] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt. Measuring coding challenge competence with apps, 2021. URL https://arxiv.org/abs/2105.09938.
[22] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021. URL https://arxiv.org/abs/2009. 03300 .
[23] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv. org/abs/2103.03874.
[24] D. Hendrycks, A. Zou, M. Mazeika, L. Tang, B. Li, D. Song, and J. Steinhardt. Pixmix: Dreamlike pictures comprehensively improve safety measures, 2022. URL https://arxiv. org/abs/2112.05135.
[25] A. Hosseini, A. Sordoni, D. Toyama, A. Courville, and R. Agarwal. Not all llm reasoners are created equal, 2024. URL https://arxiv.org/abs/2410.01748.
[26] A. Jacovi, A. Wang, C. Alberti, C. Tao, J. Lipovetz, K. Olszewska, L. Haas, M. Liu, N. Keating, A. Bloniarz, C. Saroufim, C. Fry, D. Marcus, D. Kukliansky, G. S. Tomar, J. Swirhun, J. Xing, L. W. andMadhu Gurumurthy, M. Aaron, M. Ambar, R. Fellinger, R. Wang, R. Sims, Z. Zhang, S. Goldshtein, and D. Das. Facts leaderboard. https://kaggle.com/facts-leaderboard, 2024. Google DeepMind, Google Research, Google Cloud, Kaggle.
[27] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/ abs/2310.06770.
[28] D. Kiela, M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, and A. Williams. Dynabench: Rethinking benchmarking in nlp, 2021. URL https://arxiv. org/abs/2104.14337.</p>
<p>[29] P. Kumar, E. Lau, S. Vijayakumar, T. Trinh, S. R. Team, E. Chang, V. Robinson, S. Hendryx, S. Zhou, M. Fredrikson, S. Yue, and Z. Wang. Refusal-trained llms are easily jailbroken as browser agents, 2024. URL https://arxiv.org/abs/2410.13886.
[30] J. M. Laurent, J. D. Janizek, M. Ruzo, M. M. Hinks, M. J. Hammerling, S. Narayanan, M. Ponnapati, A. D. White, and S. G. Rodriques. Lab-bench: Measuring capabilities of language models for biology research, 2024. URL https://arxiv.org/abs/2407.10362.
[31] N. Li, A. Pan, A. Gopal, S. Yue, D. Berrios, A. Gatti, J. D. Li, A.-K. Dombrowski, S. Goel, L. Phan, G. Mukobi, N. Helm-Burger, R. Lababidi, L. Justen, A. B. Liu, M. Chen, I. Barrass, O. Zhang, X. Zhu, R. Tamirisa, B. Bharathi, A. Khoja, Z. Zhao, A. Herbert-Voss, C. B. Breuer, S. Marks, O. Patel, A. Zou, M. Mazeika, Z. Wang, P. Oswal, W. Lin, A. A. Hunt, J. TienkenHarder, K. Y. Shih, K. Talley, J. Guan, R. Kaplan, I. Steneker, D. Campbell, B. Jokubaitis, A. Levinson, J. Wang, W. Qian, K. K. Karmakar, S. Basart, S. Fitz, M. Levine, P. Kumaraguru, U. Tupakula, V. Varadharajan, R. Wang, Y. Shoshitaishvili, J. Ba, K. M. Esvelt, A. Wang, and D. Hendrycks. The wmdp benchmark: Measuring and reducing malicious use with unlearning, 2024. URL https://arxiv.org/abs/2403.03218.
[32] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. URL https://arxiv.org/abs/2310.02255.
[33] T. R. McIntosh, T. Susnjak, N. Arachchilage, T. Liu, P. Watters, and M. N. Halgamuge. Inadequacies of large language model benchmarks in the era of generative artificial intelligence, 2024. URL https://arxiv.org/abs/2402.09880.
[34] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela. Adversarial nli: A new benchmark for natural language understanding, 2020. URL https://arxiv.org/abs/1910. 14599 .
[35] OpenAI. Openai o1 system card, 2024. URL https://cdn.openai.com/ o1-system-card-20240917.pdf.
[36] OpenAI. Openai and los alamos national laboratory announce bioscience research partnership, 2024. URL https://openai.com/index/ openai-and-los-alamos-national-laboratory-work-together/.
[37] OpenAI. Introducing swe-bench verified, 2024. URL https://openai.com/index/ introducing-swe-bench-verified/.
[38] OpenAI et al. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774.
[39] S. Ott, A. Barbosa-Silva, K. Blagec, J. Brauner, and M. Samwald. Mapping global dynamics of benchmark creation and saturation in artificial intelligence. Nature Communications, 13(1): 6793, 2022.
[40] D. Owen. How predictable is language model benchmark performance?, 2024. URL https: //arxiv.org/abs/2401.04757.
[41] E. Perez, S. Ringer, K. Lukošiūtė, K. Nguyen, E. Chen, S. Heiner, C. Pettit, C. Olsson, S. Kundu, S. Kadavath, A. Jones, A. Chen, B. Mann, B. Israel, B. Seethor, C. McKinnon, C. Olah, D. Yan, D. Amodei, D. Amodei, D. Drain, D. Li, E. Tran-Johnson, G. Khundadze, J. Kernion, J. Landis, J. Kerr, J. Mueller, J. Hyun, J. Landau, K. Ndousse, L. Goldberg, L. Lovitt, M. Lucas, M. Sellitto, M. Zhang, N. Kingsland, N. Elhage, N. Joseph, N. Mercado, N. DasSarma, O. Rausch, R. Larson, S. McCandlish, S. Johnston, S. Kravec, S. El Showk, T. Lanham, T. Telleen-Lawton, T. Brown, T. Henighan, T. Hume, Y. Bai, Z. Hatfield-Dodds, J. Clark, S. R. Bowman, A. Askell, R. Grosse, D. Hernandez, D. Ganguli, E. Hubinger, N. Schiefer, and J. Kaplan. Discovering language model behaviors with model-written evaluations, 2022. URL https://arxiv.org/abs/2212.09251.
[42] M. Phuong, M. Aitchison, E. Catt, S. Cogan, A. Kaskasoli, V. Krakovna, D. Lindner, M. Rahtz, Y. Assael, S. Hodkinson, H. Howard, T. Lieberum, R. Kumar, M. A. Raad, A. Webson, L. Ho, S. Lin, S. Farquhar, M. Hutter, G. Deletang, A. Ruoss, S. El-Sayed, S. Brown, A. Dragan,</p>
<p>R. Shah, A. Dafoe, and T. Shevlane. Evaluating frontier models for dangerous capabilities, 2024. URL https://arxiv.org/abs/2403.13793.
[43] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text, 2016. URL https://arxiv.org/abs/1606.05250.
[44] P. Rajpurkar, R. Jia, and P. Liang. Know what you don't know: Unanswerable questions for squad, 2018. URL https://arxiv.org/abs/1806.03822.
[45] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: A graduate-level google-proof q\&amp;a benchmark, 2023. URL https://arxiv. org/abs/2311.12022.
[46] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, et al. Large language models encode clinical knowledge. Nature, 620 (7972):172-180, 2023.
[47] M. Skarlinski, J. Laurent, A. Bou, and A. White. About 30\% of Humanity's Last Exam chemistry/biology answers are likely wrong, July 2025. URL https://www.futurehouse. org/research-announcements/hle-exam.
[48] V. K. Srinivasan, Z. Dong, B. Zhu, B. Yu, H. Mao, D. Mosk-Aoyama, K. Keutzer, J. Jiao, and J. Zhang. Nexusraven: A commercially-permissive language model for function calling. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URL https: //openreview.net/forum?id=5lcPe6DqfI.
[49] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Slone, A. Rahane, A. S. Iyer, A. Andreassen, A. Madotto, A. Santilli, A. Stuhlmüller, A. Dai, A. La, A. Lampinen, A. Zou, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023. URL https:// arxiv.org/abs/2206.04615.
[50] S. A. Taghanaki, A. Khani, and A. Khasahmadi. Mmlu-pro+: Evaluating higher-order reasoning and shortcut learning in llms, 2024. URL https://arxiv.org/abs/2409.02257.
[51] G. Team et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530.
[52] G. Tsoukalas, J. Lee, J. Jennings, J. Xin, M. Ding, M. Jennings, A. Thakur, and S. Chaudhuri. Putnambench: Evaluating neural theorem-provers on the putnam mathematical competition, 2024. URL https://arxiv.org/abs/2407.11214.
[53] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019. URL https: //arxiv.org/abs/1804.07461.
[54] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2020. URL https://arxiv.org/abs/1905.00537.
[55] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark (published at neurips 2024 track datasets and benchmarks), 2024. URL https://arxiv.org/abs/2406.01574.
[56] J. Wei, N. Karina, H. W. Chung, Y. J. Jiao, S. Papay, A. Glaese, J. Schulman, and W. Fedus. Measuring short-form factuality in large language models, 2024. URL https://arxiv.org/ abs/2411.04368.</p>
<p>[57] H. Wijk, T. Lin, J. Becker, S. Jawhar, N. Parikh, T. Broadley, L. Chan, M. Chen, J. Clymer, J. Dhyani, E. Ericheva, K. Garcia, B. Goodrich, N. Jurkovic, M. Kinniment, A. Lajko, S. Nix, L. Sato, W. Saunders, M. Taran, B. West, and E. Barnes. Re-bench: Evaluating frontier ai r\&amp;d capabilities of language model agents against human experts, 2024. URL https : //arxiv.org/abs/2411.15114.
[58] xAI. Grok-2 beta release, 2024. URL https://x.ai/blog/grok-2.
[59] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_ function_calling_leaderboard.html, 2024.
[60] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600.
[61] S. Yao, N. Shinn, P. Razavi, and K. Narasimhan. $\tau$-bench: A benchmark for tool-agent-user interaction in real-world domains, 2024. URL https://arxiv.org/abs/2406.12045.
[62] A. K. Zhang, N. Perry, R. Dulepet, J. Ji, J. W. Lin, E. Jones, C. Menders, G. Hussein, S. Liu, D. Jasper, P. Peetathawatchai, A. Glenn, V. Sivashankar, D. Zamoshchin, L. Glikbarg, D. Askaryar, M. Yang, T. Zhang, R. Alluri, N. Tran, R. Sangpisit, P. Yiorkadjis, K. Osele, G. Raghupathi, D. Boneh, D. E. Ho, and P. Liang. Cybench: A framework for evaluating cybersecurity capabilities and risks of language models, 2024. URL https://arxiv.org/abs/2408.08926.
[63] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. Agieval: A human-centric benchmark for evaluating foundation models, 2023. URL https : //arxiv.org/abs/2304.06364.</p>
<h1>A Authors</h1>
<p>We offered optional co-authorship to all question submitters with an accepted question in HumanITY's LAST EXAM (including both public and private splits). All potential co-authors with an accepted question were contacted directly. Authorship order is ranked based on the number of accepted questions in HumanITY's LAST EXAM. This list only represents a subset of our participating institutions and authors, many chose to remain anonymous.</p>
<h2>A. 1 Data Contributors \&amp; Affiliations</h2>
<p>Dmitry Dodonov, Tung Nguyen ${ }^{121}$, Jaeho Lee ${ }^{45}$, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes ${ }^{310}$, Mobeen Mahmood ${ }^{52}$, Oleksandr Pokutnyi ${ }^{337,338}$, Oleg Iskra ${ }^{10}$, Jessica P. Wang ${ }^{144}$, John-Clark Levin ${ }^{7}$, Mstyslav Kazakov ${ }^{340}$, Fiona Feng ${ }^{423}$, Steven Y. Feng ${ }^{3}$, Haoran Zhao ${ }^{22}$, Michael Yu, Varun Gangal, Chelsea Zou ${ }^{3}$, Zihan Wang ${ }^{33}$, Serguei Popov ${ }^{89}$, Robert Gerbicz ${ }^{200}$, Geoff Galgon ${ }^{272}$, Johannes Schmitt ${ }^{11}$, Will Yeadon ${ }^{51}$, Yongki Lee ${ }^{162}$, Scott Sauers ${ }^{181}$, Alvaro Sanchez, Fabian Giska, Marc Roth ${ }^{83}$, Søren Riis ${ }^{83}$, Saiteja Utpala ${ }^{53}$, Noah Burns ${ }^{3}$, Gashaw M. Goshu, Mohinder Maheshbhai Naiya ${ }^{217}$, Chidozie Agu ${ }^{189}$, Zachary Giboney ${ }^{187}$, Antrell Cheatom ${ }^{361}$, Francesco Fournier-Facio ${ }^{7}$, Sarah-Jane Crowson ${ }^{100}$, Lennart Finke ${ }^{11}$, Zerui Cheng ${ }^{8}$, Jennifer Zampese ${ }^{191}$, Ryan G. Hoerr ${ }^{119}$, Mark Nandor, Hyunwoo Park ${ }^{19}$, Tim Gehrunger ${ }^{11}$, Juaqi Cai ${ }^{5}$, Ben McCarty ${ }^{196}$, Alexis C Garretson ${ }^{163,164}$, Edwin Taylor, Damien Sileo ${ }^{78}$, Qiuyu Ren ${ }^{4}$, Usman Qazi ${ }^{31,204}$, Lianghui Li ${ }^{16}$, Jungbae Nam ${ }^{331}$, John B. Wydallis, Pavel Arkhipov ${ }^{202}$, Jack Wei Lun Shi ${ }^{74}$, Aras Bacho ${ }^{37}$, Chris G. Willcocks ${ }^{51}$, Hangrui Cao ${ }^{10}$, Sumeet Motwani ${ }^{8}$, Emily de Oliveira Santos ${ }^{52}$, Johannes Veith ${ }^{47,158}$, Edward Vendrow ${ }^{5}$, Doru Cojoc ${ }^{24}$, Kengo Zenitani, Joshua Robinson ${ }^{43}$, Longke Tang ${ }^{9}$, Yuqi Li ${ }^{221}$, Joshua Vendrow ${ }^{5}$, Natanael Wildner Fraga, Vladyslav Kuchkin ${ }^{126}$, Andrey Pupasov Maksimov ${ }^{214}$, Pierre Marion ${ }^{16}$, Denis Efremov ${ }^{167}$, Jayson Lynch ${ }^{5}$, Kaiqu Liang ${ }^{9}$, Aleksandar Mikov ${ }^{16}$, Andrew Gritsevskiy ${ }^{120}$, Julien Guillod ${ }^{91,212}$, Gözdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou ${ }^{4}$, Saeed Soori ${ }^{15}$, Ori Press ${ }^{19}$, Henry Tang ${ }^{8}$, Paolo Rissone ${ }^{40}$, Sean R. Green, Lina Brüssel ${ }^{7}$, Moon Twayana ${ }^{72}$, Aymeric Dieuleveut ${ }^{160}$, Joseph Marvin Imperial ${ }^{77,138}$, Ameya Prabhu ${ }^{19}$, Jinzhou Yang ${ }^{177}$, Nick Crispino ${ }^{17}$, Arun Rao ${ }^{39}$, Dimitri Zvonkine ${ }^{81,88}$, Gabriel Loiseau ${ }^{78}$, Mikhail Kalinin ${ }^{190}$, Marco Lukas ${ }^{30}$, Ciprian Manolescu ${ }^{3}$, Nate Stambaugh ${ }^{155}$, Subrata Mishra ${ }^{139}$, Tad Hogg ${ }^{235}$, Carlo Bosio ${ }^{4}$, Brian P Coppola ${ }^{13}$, Julian Salazar ${ }^{49}$, Jaehyesk Jin ${ }^{24}$, Rafael Sayous ${ }^{41}$, Stefan Ivanov ${ }^{7}$, Philippe Schwaller ${ }^{16}$, Shaipranesh Senthilkuma ${ }^{10}$, Andres M Bran ${ }^{16}$, Andres Algaba ${ }^{35}$, Kelsey Van den Houte ${ }^{35,104}$, Lynn Van Der Sypt ${ }^{35,104}$, Brecht Verbeken ${ }^{35}$, David Noever ${ }^{171}$, Alexei Kopylov, Benjamin Myklebust ${ }^{318}$, Bikun Li ${ }^{12}$, Lisa Schut ${ }^{8}$, Evgenii Zheltonozhskii ${ }^{70}$, Qiaochu Yuan, Derek Lim ${ }^{5}$, Richard Stanley ${ }^{5,179}$, Tong Yang ${ }^{10}$, John Maar ${ }^{85}$, Julian Wykowski ${ }^{7}$, Martí Oller ${ }^{7}$, Anmol Sahu, Cesare Giulio Ardito ${ }^{102}$, Yuzheng Hu ${ }^{14}$, Ariel Ghislain Kemogne Kamdoum ${ }^{68}$, Alvin Jin ${ }^{5}$, Tobias Garcia Vilchis ${ }^{198}$, Yuexuan $\mathrm{Zu}^{5}$, Martin Lackner ${ }^{50}$, James Koppel, Gongbo Sun ${ }^{18}$, Daniil S. Antonenko ${ }^{69}$, Steffi Chern ${ }^{10}$, Bingchen Zhao ${ }^{26}$, Pierrot Arsene ${ }^{80}$, Joseph M Cavanagh ${ }^{4}$, Daofeng Li ${ }^{17}$, Jiawei Shen ${ }^{17}$, Donato Crisostomi ${ }^{40}$, Wenjin Zhang ${ }^{17}$, Ali Dehghan, Sergey Ivanov, David Perrella ${ }^{99}$, Nurdin Kaparov ${ }^{250}$, Allen Zang ${ }^{12}$, Ilia Sucholutsky ${ }^{26}$, Arina Kharlamova ${ }^{24}$, Daniil Orel ${ }^{23}$, Vladislav Portiski, Shales Ben-David ${ }^{48}$, Zachary Berger ${ }^{5}$, Parker Whitfill ${ }^{5}$, Michael Foster, Daniel Munro ${ }^{35}$, Linh Ho, Shankar Sivarajan ${ }^{48}$, Dan Bar Hava ${ }^{146}$, Aleksey Kuchkin, David Holmes ${ }^{75}$, Alexandra Rodriguez-Romero, Frank Sommerhage ${ }^{186}$, Anji Zhang ${ }^{5}$, Richard Moat ${ }^{107}$, Keith Schneider, Zakayo Kazibwe ${ }^{211}$, Don Clarke ${ }^{124}$, Dae Hyun Kim ${ }^{142}$, Felipe Meneguitti Dias ${ }^{52}$, Sara Fish ${ }^{6}$, Veit Elser ${ }^{21}$, Tobias Kreiman ${ }^{4}$, Victor Efren Guadarrama Vilchis ${ }^{231}$, Immo Klose ${ }^{24}$, Ujjwala Anantheswaran ${ }^{36}$, Adam Zweiger ${ }^{5}$, Kaivalya Rawal ${ }^{8}$, Jeffery Li ${ }^{7}$, Jeremy Nguyen ${ }^{182}$, Nicolas Daans ${ }^{132}$, Haline Heidinger ${ }^{192,193}$, Maksim Radionov ${ }^{157}$, Václav Rozholt ${ }^{80}$, Vincent Ginis ${ }^{5,35}$, Christian Stump ${ }^{132}$, Niv Cohen ${ }^{28}$, Rafał Poświata ${ }^{228}$, Josef Tkadlec ${ }^{56}$, Alan Goldfarb ${ }^{4}$, Chenguang Wang ${ }^{17}$, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery ${ }^{17}$, Ryan Stendall ${ }^{220}$, Jamie Tucker-Foltz ${ }^{6}$, Jack Stade ${ }^{108}$, T. Ryan Rogers ${ }^{179}$, Tom Goertzen ${ }^{46}$, Declan Grabb ${ }^{3}$, Abhishek Shukla ${ }^{73}$, Alan Givré ${ }^{134}$, John Arnold Ambay ${ }^{218}$, Archan Sen ${ }^{4}$, Muhammad Fayez Aziz ${ }^{14}$, Mark H Inlow ${ }^{256}$, Hao $\mathrm{He}^{166}$, Ling Zhang ${ }^{106}$, Younesse Kaddar ${ }^{8}$, Ivar Ångquist ${ }^{27}$, Yanxu Chen ${ }^{54}$, Harrison K Wang ${ }^{6}$, Kalyan Ramakrishnan ${ }^{8}$, Elliott Thornley ${ }^{312}$, Antonio Terpin ${ }^{11}$, Hailey Schoelkopf, Eric Zheng ${ }^{10}$, Avishy Carmi ${ }^{208}$, Ethan D. L. Brown ${ }^{255}$, Kelin Zhu ${ }^{38}$, Max Bartolo ${ }^{242}$, Richard Wheeler ${ }^{26}$, Martin Stehberger, Peter Bradshaw ${ }^{14}$, JP Heimonen ${ }^{359}$, Kaustubh Sridhar ${ }^{30}$, Ido Akov ${ }^{298}$, Jennifer Sandlin ${ }^{36}$, Yury Makarychev ${ }^{352}$, Joanna Tam ${ }^{67}$, Hieu Hoang ${ }^{253}$, David M. Cunningham ${ }^{323}$, Vladimir Goryachev, Demosthenes Patramanis ${ }^{8}$, Michael Krause ${ }^{133}$, Andrew Redenti ${ }^{14}$, David Aldous ${ }^{4}$, Jesyin Lai ${ }^{224}$, Shannon Coleman ${ }^{31}$, Jiangnan Xu ${ }^{239}$, Sangwon Lee, Ilias Magoulas ${ }^{36}$, Sandy Zhao, Ning Tang ${ }^{4}$, Michael K. Cohen ${ }^{4}$, Orr Paradise ${ }^{4}$, Jan Hendrik Kirchner ${ }^{65}$, Maksym Ovchynnikov ${ }^{185}$, Jason O. Matos ${ }^{67}$, Adithya Shenoy, Michael Wang ${ }^{4}$, Yuzhou Nie ${ }^{34}$, Anna Sztyber-Betley ${ }^{206}$, Paolo Faraboschi ${ }^{353}$, Robin Riblet ${ }^{80}$, Jonathan Crozier ${ }^{84}$, Shiv Halasyamani ${ }^{260}$, Shreyas Verma ${ }^{234}$, Prashant Joshi ${ }^{130}$, Eli Meril ${ }^{341}$, Ziqiao Ma ${ }^{13}$, Jérémy Andréoletti ${ }^{91}$, Raghav Singhal ${ }^{23}$, Jacob Platnick ${ }^{29}$, Volodymyr Nevirkovets ${ }^{44}$, Luke Basler ${ }^{328}$, Alexander Ivanov ${ }^{311}$, Seri Khoury ${ }^{96}$, Nils Gustafsson ${ }^{97}$, Marco Piccardo ${ }^{147}$, Hamid Mostaghimi ${ }^{98}$, Qijia Chen ${ }^{6}$, Virendra Singh ${ }^{342}$, Tran Quoc Khánh ${ }^{291}$, Paul Rosu ${ }^{42}$, Hannah Szlyk ${ }^{17}$, Zachary Brown ${ }^{7}$, Himanshu Narayan, Aline Menezes, Jonathan Roberts ${ }^{7}$, William Alley, Kunyang Sun ${ }^{4}$, Arkil Patel ${ }^{92,66}$, Max Lamparth ${ }^{9}$, Anka Reuel ${ }^{9}$, Linwei Xin ${ }^{12}$, Hanmeng Xu ${ }^{69}$, Jacob Loader ${ }^{7}$, Freddie Martin, Zixuan Wang ${ }^{9}$, Andrea Achilleos ${ }^{41}$, Thomas Preu ${ }^{325}$, Tomek Korbak ${ }^{321}$, Ida Bosio ${ }^{310}$, Fereshteh Kazemi, Ziye Chen ${ }^{45}$, Biró Bálint, Eve J. Y. Lo ${ }^{137}$, Jiaqi Wang ${ }^{22}$, Maria Inês S. Nunes ${ }^{362}$, Jeremiah Milbauer ${ }^{39}$, M Saiful Bari ${ }^{166}$, Zihao Wang ${ }^{12}$, Behzad Ansarinejad, Yewen Sun ${ }^{71}$,</p>
<p>Stephane Durand ${ }^{270}$, Hossam Elgnainy ${ }^{143}$, Guillaume Douville, Daniel Tordera ${ }^{215}$, George Balabanian ${ }^{30}$, Hew Wolff, Lynna Kvistad ${ }^{140}$, Hsiaoyun Milliron ${ }^{305}$, Ahmad Sakor ${ }^{90}$, Murat Eron ${ }^{334}$, Andrew Favre D.O. ${ }^{313}$, Shailesh Shah ${ }^{305}$, Xiaoxiang Zhou ${ }^{47}$, Firuz Kamalov ${ }^{281}$, Sherwin Abdoli ${ }^{19}$, Tim Santens ${ }^{7}$, Shaul Barkan ${ }^{55}$, Allison Tee ${ }^{3}$, Robin Zhang ${ }^{5}$, Alessandro Tomasiello ${ }^{183}$, G. Bruno De Luca ${ }^{3}$, Shi-Zhuo Looi ${ }^{37}$, Vinh-Kha Le ${ }^{4}$, Noam Kolt ${ }^{55}$, Jiayi Pan ${ }^{4}$, Emma Rodman ${ }^{258}$, Jacob Drori, Carl J Fossum ${ }^{319}$, Niklas Muennghoff ${ }^{3}$, Milind Jagota ${ }^{4}$, Ronak Pradeep ${ }^{48}$, Honglu Fan ${ }^{151}$, Jonathan Eicher ${ }^{172}$, Michael Chen ${ }^{37}$, Kushal Thaman ${ }^{3}$, William Merrill ${ }^{28}$, Moritz Firsching ${ }^{356}$, Carter Harris ${ }^{237}$, Stefan Ciobăcă ${ }^{350}$, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri ${ }^{54}$, Pavel Zhelnov ${ }^{15}$, Mohammadreza Molayezi ${ }^{11}$, Alexander Piperski ${ }^{146}$, David K. Zhang ${ }^{3}$, Kostiantyn Dobarskyi, Roman Leventov ${ }^{226}$, Ignat Soroko ${ }^{72}$, Joshua Duerseh ${ }^{334}$, Vage Taamazyan ${ }^{375}$, Andrew $\mathrm{Ho}^{236}$, Wenjie $\mathrm{Ma}^{4}$, William Held ${ }^{3,29}$, Ruicheng Xian ${ }^{14}$, Armel Randy Zebaze ${ }^{311}$, Mohanad Mohamed ${ }^{307}$, Julian Noah Leser ${ }^{50}$, Michelle X Yuan, Laila Yacar ${ }^{241}$, Johannes Lengler ${ }^{11}$, Katarzyna Olszewska, Claudio Di Fratta ${ }^{364}$, Edson Oliveira ${ }^{123}$, Joseph W. Jackson ${ }^{180}$, Andy Zou ${ }^{10,258}$, Muthu Chidambaram ${ }^{42}$, Timothy Manik, Hector Haffenden, Dashiell Stander ${ }^{247}$, Ali Dasouqi ${ }^{20}$, Alexander Shen ${ }^{300}$, Bita Golshani, David Stap ${ }^{54}$, Egor Kretov ${ }^{308}$, Mikalai Uzhou ${ }^{310}$, Alina Borisovna Zhidkovskaya ${ }^{24}$, Nick Winter, Miguel Orbegoso Rodriguez ${ }^{11}$, Robert Lauff ${ }^{85}$, Dustin Wehr, Colin Tang ${ }^{10}$, Zaki Hossain ${ }^{248}$, Shaun Phillips, Fortuna Samuele ${ }^{358}$, Fredrik Ekström, Angela Hammon, Oam Patel ${ }^{6}$, Faraz Farhidi ${ }^{249}$, George Medley, Forough Mohammadzadeh, Madellene Peñaflor ${ }^{154}$, Haile Kassahun ${ }^{32}$, Alena Friedrich ${ }^{322}$, Rayner Hernandez Perez ${ }^{103}$, Daniel Pyda ${ }^{233}$, Taom Sakal ${ }^{34}$, Omkar Dhamane ${ }^{232}$, Ali Khajegili Mirabadi ${ }^{31}$, Eric Hallman, Kenchi Okutsu ${ }^{364}$, Mike Battaglia, Mohammad Maghsoudimehrabani ${ }^{103}$, Alon Amit ${ }^{128}$, Dave Hulbert, Roberto Pereira ${ }^{309}$, Simon Weber ${ }^{11}$, Handoko, Anton Peristyy, Stephen Malina ${ }^{161}$, Mustafa Mehkary ${ }^{15,100}$, Rami Aly ${ }^{7}$, Frank Reidegeld, Anna-Katharina Dick ${ }^{19}$, Cary Friday ${ }^{173}$, Mukhwinder Singh ${ }^{129}$, Hassan Shapourian ${ }^{343}$, Wanyoung Kim ${ }^{159}$, Mariana Costa, Hubeyb Gurdogan ${ }^{39}$, Harsh Kumar ${ }^{280}$, Chiara Ceconello, Chao Zhuang, Haon Park ${ }^{278,279}$, Micah Carroll ${ }^{4}$, Andrew R. Tawfeek ${ }^{22}$, Stefan Steinerberger ${ }^{22}$, Daattavya Aggarwal ${ }^{7}$, Michael Kirchhof ${ }^{19}$, Linjie Dai ${ }^{5}$, Evan Kim ${ }^{5}$, Johan Ferret ${ }^{40}$, Jainam Shah ${ }^{131}$, Yuzhou Wang ${ }^{29}$, Minghao Yan ${ }^{18}$, Krzysztof Burdzy ${ }^{25}$, Lixin Zhang, Antonio Franca ${ }^{7}$, Diana T. Pham ${ }^{125}$, Kang Yong Loh ${ }^{3}$, Joshua Robinson ${ }^{150}$, Abram Jackson, Paolo Giordano ${ }^{82}$, Philipp Petersen ${ }^{82}$, Adrian Cosma ${ }^{302}$, Jesus Colino, Colin White ${ }^{195}$, Jacob Votava ${ }^{6}$, Vladimir Vinnikov, Ethan Delaney ${ }^{101}$, Petr Spelda ${ }^{56}$, Vit Stritecky ${ }^{56}$, Syed M. Shahid ${ }^{199}$, Jean-Christophe Mourrat ${ }^{88,201}$, Lavr Vetoshkin ${ }^{254}$, Koen Sponselee ${ }^{355}$, Renas Bacho ${ }^{301}$, Zheng-Xin Yong ${ }^{45}$, Florencia de la Rosa ${ }^{263}$, Nathan Cho ${ }^{3}$, Xiuyu Li ${ }^{4}$, Guillaume Malod ${ }^{169}$, Orion Weller ${ }^{20}$, Guglielmo Albani ${ }^{168}$, Leon Lang ${ }^{54}$, Julien Laurendeau ${ }^{16}$, Dmitry Kazakov ${ }^{6}$, Fatimah Adesanya, Julien Portier ${ }^{7}$, Lawrence Hollom ${ }^{7}$, Victor Souza ${ }^{7}$, Yuchen Anna Zhou ${ }^{165}$, Julien Degorre ${ }^{360}$, Yiğit Yalın ${ }^{209}$, Gbenga Daniel Obikoya, Rai (Michael Pokorny) ${ }^{87}$, Filippo Bigi ${ }^{16}$, M.C. Boscă ${ }^{351}$, Oleg Shumar, Kaniuar Bacho ${ }^{26}$, Gabriel Recchia ${ }^{303}$, Mara Popescu ${ }^{76}$, Nikita Shulga ${ }^{277}$, Ngefor Mildred Tanwie ${ }^{227}$, Thomas C.H. Lux ${ }^{225}$, Ben Rank, Colin Ni ${ }^{26}$, Matthew Brooks, Alesia Yakimchyk ${ }^{203}$, Huanxu (Quinn) Liu ${ }^{262}$, Stefano Cavalleri ${ }^{197}$, Olle Häggström ${ }^{203}$, Emil Verkama ${ }^{57}$, Joshua Newbould ${ }^{15}$, Hans Gundlach ${ }^{5}$, Leonor Brito-Santana ${ }^{144}$, Brian Amaro ${ }^{3}$, Vivek Vajipey ${ }^{3}$, Rynaa Grover ${ }^{20}$, Ting Wang ${ }^{17}$, Yosi Kratish ${ }^{44}$, Wen-Ding Li ${ }^{21}$, Sivakanth Gopi ${ }^{53}$, Andrea Caciolai ${ }^{40}$, Christian Schroeder de Witt ${ }^{8}$, Pablo Hernández-Cámara ${ }^{294}$, Emanuele Rodolà ${ }^{40}$, Jules Robins, Dominic Williamson ${ }^{46}$, Vincent Cheng ${ }^{33}$, Brad Raynor ${ }^{357}$, Hao Qi ${ }^{27}$, Ben Segev ${ }^{24}$, Jingxuan Fan ${ }^{6}$, Sarah Martinson ${ }^{6}$, Erik Y. Wang ${ }^{6}$, Kaylie Hausknecht ${ }^{6}$, Michael P. Brenner ${ }^{6}$, Mao Mao ${ }^{27}$, Christoph Demian ${ }^{47}$, Peyman Kassani ${ }^{330}$, Xinyu Zhang ${ }^{27}$, David Avagian ${ }^{64}$, Eshawn Jessica Scipio ${ }^{261}$, Alon Ragoler ${ }^{136}$, Justin Tan ${ }^{7}$, Blake Sims, Rebeka Plecnik, Aaron Kirtland ${ }^{45}$, Omer Faruk Bodur, D.P. Shinde, Yan Carlos Leyva Labrador ${ }^{346}$, Zahra Adoul ${ }^{332}$, Mohamed Zekry ${ }^{326}$, Ali Karakoc ${ }^{194}$, Tania C. B. Santos, Samir Shamseldeen ${ }^{313}$, Loukmane Karim ${ }^{100}$, Anna Liakhovitskaia ${ }^{305}$, Nate Resman ${ }^{95}$, Nicholas Farina, Juan Carlos Gonzalez ${ }^{178}$, Gabe Maayan ${ }^{27}$, Earth Anderson ${ }^{77}$, Rodrigo De Oliveira Pena ${ }^{268}$, Elizabeth Kelley ${ }^{56}$, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu ${ }^{31}$, Ross Finocchio, Ismail Alarab ${ }^{240}$, Joshua Cole ${ }^{269}$, Danyelle Ferreira, Bryan Johnson ${ }^{338}$, Mohammad Safdari ${ }^{304}$, Liangti Dai ${ }^{8}$, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro José Moyano ${ }^{213}$, Alexey Pronin ${ }^{274}$, Jing Fan ${ }^{76}$, Angel Ramirez-Trinidad, Yana Malysheva ${ }^{17}$, Daphiny Pottmaier ${ }^{299}$, Omid Taheri ${ }^{34}$, Stanley Stepanic ${ }^{271}$, Samuel Perry, Luke Askew ${ }^{292}$, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi ${ }^{105}$, Ricardo Lorena ${ }^{97}$, Krishnamurthy Iyer ${ }^{96}$, Arshad Anil Fasiludeen ${ }^{7}$, Ronald Clark ${ }^{8}$, Josh Ducey ${ }^{324}$, Matheus Piza ${ }^{303}$, Maja Somrak, Eric Vergo, Juehang Qin ${ }^{264}$, Benjámin Borbás ${ }^{288}$, Eric Chu ${ }^{49}$, Jack Lindsey ${ }^{65}$, Antoine Jallon, I.M.J. McInnis, Evan Chen ${ }^{5}$, Avi Semler ${ }^{8}$, Luk Gloor, Tej Shah ${ }^{122}$, Marc Carauleanu ${ }^{309}$, Pascal Lauer ${ }^{289,290}$, Tran Duc Huy ${ }^{285}$, Hossein Shahrtash ${ }^{222}$, Emilien Duc ${ }^{11}$, Lukas Lewark ${ }^{11}$, Assaf Brown ${ }^{55}$, Samuel Albanie, Brian Weber ${ }^{251}$, Warren S. Vaz, Pierre Clavier ${ }^{327}$, Yiyang Fan, Gabriel Poesia Reis e Silva ${ }^{4}$, Long (Tony) Lian ${ }^{4}$, Marcus Abramovitch, Xi Jiang ${ }^{12}$, Sandra Mendoza ${ }^{175,176}$, Murat Islam ${ }^{252}$, Juan Gonzalez, Vasilios Mavroudis ${ }^{52}$, Justin Xu ${ }^{6}$, Pawan Kumar ${ }^{127}$, Laxman Prasad Goswami ${ }^{73}$, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong ${ }^{135}$, Yhorben Jansen ${ }^{141}$, Antonella Pinto ${ }^{79}$, Archimedes Apronti ${ }^{149}$, Abdallah Galal ${ }^{152}$, Ng Ze-An ${ }^{153}$, Ankit Singh ${ }^{156}$, Tong Jiang ${ }^{6}$, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani ${ }^{174}$, Gang Zhang, Zhehang Du ${ }^{30}$, Benedito Alves de Oliveira Junior ${ }^{52}$, Dmitry Malishev, Nicolas Remy ${ }^{207}$, Taylor D. Hartman ${ }^{210}$, Tim Tarver ${ }^{216}$, Stephen Mensah ${ }^{273}$, Gautier Abou Loume ${ }^{229,230}$, Wiktor Morak, Farzad Habibi ${ }^{58}$, Sarah Hoback ${ }^{6}$, Will Cai ${ }^{4}$, Javier Gimenez, Roselynn Grace Montecillo ${ }^{243}$, Jakub Lucki ${ }^{14}$, Russell Campbell ${ }^{245}$, Asankhaya Sharma ${ }^{249}$, Khalida Meer, Shreen Gul ${ }^{257}$, Daniel Espinosa Gonzalez ${ }^{44}$, Xavier Alapont, Alex Hoover ${ }^{14}$, Gunjan Chhabiani ${ }^{29}$, Freddie Vargus ${ }^{266}$, Arunim Agarwal ${ }^{267}$, Yibo Jiang ${ }^{12}$, Deepakkumar Patil ${ }^{273}$, David Outevsky ${ }^{276}$, Kevin Joseph Scaria ${ }^{36}$, Rajat Maheshwari ${ }^{282}$, Abdelkader Dendane, Priti Shukla ${ }^{283}$, Ashley Cartwright ${ }^{284}$, Sergei Bogdanov ${ }^{286}$, Niels Mündler ${ }^{11}$, Sören Möller ${ }^{287}$, Luca Arnaboldi ${ }^{16}$, Kunvar Thaman ${ }^{283}$, Muhammad Rehan Siddiqi ${ }^{295,296}$, Prajvi Saxena ${ }^{297}$, Himanshu Gupta ${ }^{36}$, Tony Fruhauff, Glen Sherman, Mátyás Vincze ${ }^{38,317}$,</p>
<p>Siranut Usawasutsakorn ${ }^{320}$, Dylan Ler, Anil Radhakrishnan ${ }^{84}$, Innocent Enyekwe, Sk Md Salauddin ${ }^{329}$, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi ${ }^{3}$, Mohsen Bahaloohoreh, Claire Sparrow ${ }^{12}$, Jasdeep Sidhu, Sam Ali ${ }^{44}$, Song Bian ${ }^{38}$, John Lai, Eric Singer ${ }^{330}$, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy ${ }^{344}$, Darling Duclosel ${ }^{345}$, Dario Bezzi ${ }^{347}$, Yashaswini Jain ${ }^{348}$, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Imad Ali Shah ${ }^{101}$, Jun Jin, Scott Creighton, Denis Peskoff ${ }^{5}$, Zienab EL-Wasif ${ }^{105}$, Ragavendran P V, Michael Richmond, Joseph McGowan ${ }^{15}$, Tejal Patwardhan ${ }^{87}$
Late Contributors Hao-Yu Sun ${ }^{371}$, Ting Sun ${ }^{14}$, Nikola Zubić ${ }^{63}$, Samuele Sala ${ }^{402}$, Stephen Ebert ${ }^{39}$, Jean Kaddour ${ }^{41}$, Manuel Schottdorf ${ }^{384}$, Dianzhuo Wang ${ }^{6}$, Gerol Petruzella ${ }^{385}$, Alex Meiburg ${ }^{48,428}$, Tilen Medved ${ }^{390}$, Ali ElSheikh ${ }^{44}$, S Ashwin Hebbar ${ }^{9}$, Lorenzo Vaquero ${ }^{98}$, Xianjun Yang ${ }^{34}$, Jason Poulos ${ }^{399}$, Vilém Zouhar ${ }^{11}$, Sergey Bogdanik, Mingfang Zhang ${ }^{403}$, Jorge Sanz-Ros ${ }^{3}$, David Anugraha ${ }^{12}$, Yinwei Dai ${ }^{6}$, Anh N. Nhu ${ }^{30}$, Xue Wang ${ }^{90}$, Ali Anil Demircali ${ }^{62}$, Zhibai Jia ${ }^{21}$, Yuyin Zhou ${ }^{61}$, Juncheng Wu ${ }^{61}$, Mike He ${ }^{9}$, Nitin Chandok, Aarush Sinha ${ }^{400}$, Gaoxiang Luo ${ }^{96}$, Long Le ${ }^{43}$, Mickaël Noyé ${ }^{409}$, Michał Perełkiewicz ${ }^{228}$, Ioannis Pantidis ${ }^{408}$, Tianbo Qi ${ }^{115}$, Soham Sachin Purohit ${ }^{13}$, Letitia Parcalabescu ${ }^{117}$, Thai-Hoa Nguyen ${ }^{365}$, Genta Indra Winata, Edoardo M. Ponti ${ }^{26}$, Hanchen Li ${ }^{12}$, Kaustubh Dhole ${ }^{58}$, Jongee Park ${ }^{412}$, Dario Abbondanza ${ }^{420}$, Yuanli Wang ${ }^{27}$, Anupam Nayak ${ }^{10}$, Diogo M. Caetano ${ }^{97}$, Antonio A. W. L. Wong ${ }^{41}$, Maria del Rio-Chanona ${ }^{25,41}$, Dâmiel Kondor ${ }^{25}$, Pieter Francois ${ }^{5,92}$, Ed Chalstrey ${ }^{41}$, Jakob Zsambok ${ }^{43}$, Dan Hoyer ${ }^{48}$, Jenny Reddish ${ }^{25}$, Jakob Hauser ${ }^{25}$, Francisco-Javier Rodrigo-Ginés ${ }^{417}$, Suchandra Datta, Maxwell Shepherd ${ }^{20}$, Thom Kamphuis ${ }^{411}$, Qizheng Zhang ${ }^{3}$, Hyunjun Kim ${ }^{60}$, Ruiji Sun ${ }^{4}$, Jianzhu Yao ${ }^{9}$, Franck Dernoncourt ${ }^{380}$, Satyapriya Krishna ${ }^{6}$, Sina Rismanchian ${ }^{59}$, Bonan Pu, Francesco Pinto ${ }^{12}$, Yingheng Wang ${ }^{21}$, Kumar Shridhar ${ }^{11}$, Kalon J. Overholt ${ }^{5}$, Glib Briia ${ }^{387}$, Hieu Nguyen ${ }^{64}$, David (Quod) Soler Bartomeu ${ }^{420}$, Tony CY Pang ${ }^{46,398}$, Adam Wecker, Yifan Xiong ${ }^{52}$, Fanfei Li ${ }^{369}$, Lukas S. Huber ${ }^{19,118}$, Joshua Jaeger ${ }^{118}$, Romano De Maddalena ${ }^{431}$, Xing Han Lü ${ }^{32}$, Yuhui Zhang ${ }^{4}$, Claas Beger ${ }^{21}$, Patrick Tser Jern Kon ${ }^{13}$, Sean Li ${ }^{99}$, Vivek Sanker ${ }^{3}$, Ming Yin ${ }^{9}$, Yihao Liang ${ }^{9}$, Xinlu Zhang ${ }^{34}$, Ankit Agrawal ${ }^{418}$, Li S. Yifei ${ }^{30}$, Zechen Zhang ${ }^{6}$, Mu Cai ${ }^{18}$, Yasin Sonmez ${ }^{4}$, Costin Cozianu ${ }^{386}$, Changhao $\mathrm{Li}^{5}$, Alex Slen ${ }^{30}$, Shoubin Yu ${ }^{113}$, Hyun Kyu Park ${ }^{429}$, Gabriele Sarti ${ }^{376}$, Marcin Briański ${ }^{369}$, Alessandro Stolfo ${ }^{11}$, Truong An Nguyen ${ }^{368}$, Mike Zhang ${ }^{415}$, Yotam Perlitz ${ }^{382}$, Jose Hernandez-Orallo ${ }^{389}$, Runjia Li ${ }^{8}$, Amin Shabani ${ }^{373}$, Felix Juefei-Xu, Shikhar Dhingra ${ }^{384}$, Orr Zohar ${ }^{9}$, My Chiffon Nguyen, Alexander Pondaven ${ }^{6}$, Abdurrahim Yilmaz ${ }^{62}$, Xuandong Zhao ${ }^{4}$, Chuanyang Jin ${ }^{20}$, Muyan Jiang ${ }^{4}$, Stefan Todoran ${ }^{32}$, Xinyao Han ${ }^{5}$, Jules Kreuer ${ }^{19}$, Brian Rabern ${ }^{26}$, Anna Plassart ${ }^{107}$, Martino Maggetti ${ }^{388}$, Luther Yap ${ }^{9}$, Robert Geirhos ${ }^{10}$, Jonathon Kean ${ }^{394}$, Dingsu Wang, Sina Mollaei ${ }^{3}$, Chenkai Sun ${ }^{14}$, Yifan Yin ${ }^{20}$, Shiqi Wang ${ }^{115}$, Rui Li ${ }^{3}$, Yaowen Chang ${ }^{14}$, Anjiang Wei ${ }^{5}$, Alice Bizeul ${ }^{11}$, Xiaohan Wang ${ }^{5}$, Alexandre Oliveira Arrais ${ }^{433}$, Kushin Mukherjee ${ }^{5}$, Jorge Chamorro-Padial ${ }^{370}$, Jiachen Liu ${ }^{13}$, Xingyu Qu ${ }^{43}$, Junyi Guan ${ }^{25}$, Adam Bouyamourn ${ }^{4}$, Shuyu Wu ${ }^{13}$, Martyna Plomecka ${ }^{63}$, Junda Chen ${ }^{33}$, Mengze Tang ${ }^{18}$, Jiaqi Deng ${ }^{29}$, Shreyas Subramanian ${ }^{378}$, Haocheng Xi ${ }^{4}$, Haoxuan Chen ${ }^{3}$, Weizhi Zhang ${ }^{112}$, Yinuo Ren ${ }^{3}$, Haoqin Tu ${ }^{61}$, Sejong Kim ${ }^{60}$, Yushun Chen ${ }^{116}$, Sara Vera Marjanović ${ }^{108}$, Junwoo $\mathrm{Ha}^{396}$, Grzegorz Luczyna, Jeff J. Ma ${ }^{13}$, Zewen Shen ${ }^{15}$, Dawn Song ${ }^{4}$, Cedegao E. Zhang ${ }^{5}$, Zhun Wang ${ }^{4}$, Gaël Gendron ${ }^{365}$, Yunze Xiao ${ }^{10}$, Leo Smucker ${ }^{15}$, Erica Weng ${ }^{10}$, Kwok Hao Lee ${ }^{74}$, Zhe Ye ${ }^{4}$, Stefano Ermon ${ }^{3}$, Ignacio D. López-Miguel ${ }^{60}$, Theo Knights ${ }^{101}$, Anthony Gittin ${ }^{10,421}$, Namkyu Park ${ }^{414}$, Boyi Wei ${ }^{5}$, Hongzheng Chen ${ }^{21}$, Kunal Pai ${ }^{111}$, Ahmed Elkhanany ${ }^{374}$, Han Lin ${ }^{368}$, Philipp D. Siedler ${ }^{117}$, Jichao Fang ${ }^{422}$, Ritwik Mishra ${ }^{406}$, Károly Zsolnai-Fehér ${ }^{410}$, Xilin Jiang ${ }^{24}$, Shadab Khan ${ }^{375}$, Jun Yuan ${ }^{419}$, Rishab Kumar Jain ${ }^{6}$, Xi Lin ${ }^{13}$, Mike Peterson, Zhe Wang ${ }^{397}$, Aditya Malusare ${ }^{109}$, Maosen Tang ${ }^{21}$, Isha Gupta ${ }^{58}$, Ivan Fosin, Timothy Kang, Barbara Dworakowska ${ }^{62}$, Kazuki Matsumoto ${ }^{434}$, Guangyao Zheng ${ }^{20}$, Gerben Sewuster ${ }^{377}$, Jorge Pretel Villanueva ${ }^{325}$, Ivan Rannev ${ }^{392}$, Igor Chernyavsky ${ }^{102}$, Jiale Chen ${ }^{75}$, Deepayan Banik ${ }^{15}$, Ben Racz ${ }^{10}$, Wenchao Dong ${ }^{427}$, Jianxin Wang ${ }^{20}$, Laila Bashmal, Duarte V. Gonçalves ${ }^{69}$, Wei Hu ${ }^{14}$, Kaushik Bar ${ }^{325}$, Ondrej Bohdal ${ }^{25}$, Atharv Singh Patlan ${ }^{9}$, Shehzaad Dhuliawala ${ }^{11}$, Caroline Geirhos ${ }^{426}$, Julien Wist ${ }^{401}$, Yuval Kansal ${ }^{9}$, Bingsen Chen ${ }^{28}$, Kutay Tire ${ }^{114}$, Atak Talay Yücel ${ }^{114}$, Brandon Christof ${ }^{372}$, Veerupaksh Singla ${ }^{109}$, Zijian Song ${ }^{111}$, Sanxing Chen ${ }^{42}$, Jiaxin $\mathrm{Ge}^{4}$, Kaustubh Ponkshe ${ }^{23}$, Isaac Park ${ }^{28}$, Tianneng Shi ${ }^{4}$, Martin Q. Ma ${ }^{10}$, Joshua Mak ${ }^{307}$, Sherwin Lai ${ }^{3}$, Antoine Moulin ${ }^{381}$, Zhuo Cheng ${ }^{10}$, Zhauda Zhu ${ }^{15}$, Ziyi Zhang ${ }^{14}$, Vaidehi Patil ${ }^{113}$, Ketan Jha ${ }^{416}$, Qiatong Men ${ }^{28}$, Jiaxuan Wu ${ }^{18}$, Tianchi Zhang ${ }^{12}$, Bruno Hebling Vieira ${ }^{63}$, Alham Fikri Aji ${ }^{33}$, JaeWon Chung ${ }^{13}$, Mohammed Mahfoud ${ }^{66}$, Ha Thi Hoang ${ }^{404}$, Marc Sperzel, Wei Hao ${ }^{24}$, Kristof Meding ${ }^{19}$, Sihan $\mathrm{Xu}^{13}$, Vassilis Kostakos ${ }^{379}$, Davide Manini ${ }^{70}$, Yueying Liu ${ }^{14}$, Christopher Toukmaji ${ }^{59}$, Jay Paek ${ }^{33}$, Eunmi Yu ${ }^{424}$, Arif Engin Demircali ${ }^{413}$, Zhiyi Sun ${ }^{13}$, Ivan Dewerpe ${ }^{64}$, Hongsen Qin ${ }^{37}$, Roman Pflugfelder ${ }^{435,436}$, James Bailey ${ }^{381}$, Johnathan Morris ${ }^{16}$, Ville Heilala ${ }^{424}$, Sybille Rosset ${ }^{433}$, Zishun Yu ${ }^{112}$, Peter E. Chen ${ }^{45}$, Woongyeong Yeo ${ }^{60}$, Eeshaan Jain ${ }^{16}$, Ryan Yang ${ }^{5}$, Sreekar Chigurupati ${ }^{110}$, Julia Chernyavsky, Sai Prajwal Reddy ${ }^{110}$, Subhashini Venugopalan ${ }^{64}$, Hunar Batra ${ }^{6}$, Core Francisco Park ${ }^{6}$, Hieu Tran ${ }^{38}$, Guilherme Maximiano, Genghan Zhang ${ }^{9}$, Yizhuo Liang ${ }^{43}$, Hu Shiyu ${ }^{407}$, Rongwu Xu ${ }^{22}$, Rui Pan ${ }^{9}$, Siddharth Suresh ${ }^{18}$, Ziqi Liu ${ }^{18}$, Samaksh Gulati ${ }^{116}$, Songyang Zhang ${ }^{42}$, Peter Turchin ${ }^{25}$, Christopher W. Bartlett ${ }^{71}$, Christopher R. Scotese ${ }^{44}$, Phuong M. Cao ${ }^{14}$, Ben Wu ${ }^{437}$, Jacek Karwowski ${ }^{8}$, Davide Scaramuzza ${ }^{63}$</p>
<h1>Auditors $\ddagger$ All auditor work conducted while at Scale AI.</h1>
<p>Aakaash Nattanmai, Gordon McKellips, Anish Cheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay Paek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent Cheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath, Violet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin Wang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Xiangwan Sun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert Yang, Timothy Wu, Spandan Patel,</p>
<p>Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David Sun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan Zhou, Aidan Wu, Jason Luo, Anwith Telluri</p>
<h1>Affiliations</h1>
<ol>
<li>Stanford University</li>
<li>University of California, Berkeley</li>
<li>Massachusetts Institute of Technology</li>
<li>Harvard University</li>
<li>University of Cambridge</li>
<li>University of Oxford</li>
<li>Princeton University</li>
<li>Carnegie Mellon University</li>
<li>ETH Zürich</li>
<li>University of Chicago</li>
<li>University of Michigan</li>
<li>University of Illinois Urbana-Champaign</li>
<li>University of Toronto</li>
<li>École Polytechnique Fédérale de Lausanne</li>
<li>Washington University</li>
<li>University of Wisconsin-Madison</li>
<li>University of Tübingen</li>
<li>Johns Hopkins University</li>
<li>Cornell University</li>
<li>University of Washington</li>
<li>Mohamed bin Zayed University of Artificial Intelligence</li>
<li>Columbia University</li>
<li>Complexity Science Hub</li>
<li>University of Edinburgh</li>
<li>Boston University</li>
<li>New York University</li>
<li>Georgia Institute of Technology</li>
<li>University of Pennsylvania</li>
<li>University of British Columbia</li>
<li>McGill University</li>
<li>University of California, San Diego</li>
<li>University of California, Santa Barbara</li>
<li>Vrije Universiteit Brussel</li>
<li>Arizona State University</li>
<li>California Institute of Technology</li>
<li>University of Maryland</li>
<li>University of California, Los Angeles</li>
<li>Sapienza University of Rome</li>
<li>University College London</li>
<li>Duke University</li>
<li>University of Southern California</li>
<li>Northwestern University</li>
<li>Brown University</li>
<li>The University of Sydney</li>
<li>Humboldt-Universität zu Berlin</li>
<li>University of Waterloo</li>
<li>Google DeepMind</li>
<li>TU Wien</li>
<li>Durham University</li>
<li>University of Sao Paulo</li>
<li>Microsoft Research</li>
<li>University of Amsterdam</li>
<li>The Hebrew University of Jerusalem</li>
<li>Charles University</li>
<li>KTH Royal Institute of Technology</li>
<li>Emory University</li>
<li>University of California, Irvine</li>
<li>Korea Advanced Institute of Science and Technology</li>
<li>University of California, Santa Cruz</li>
<li>Imperial College London</li>
<li>University of Zurich</li>
<li>Google</li>
<li>Anthropic</li>
<li>Mila - Québec AI Institute</li>
<li>Northeastern University</li>
<li>University of Calgary</li>
<li>Yale University</li>
<li>Technion - Israel Institute of Technology</li>
<li>The Ohio State University</li>
<li>University of North Texas</li>
<li>Indian Institute of Technology Delhi</li>
<li>National University of Singapore</li>
<li>Universiteit Leiden</li>
<li>Heidelberg University</li>
<li>University of Arkansas</li>
<li>Inria</li>
<li>Independent researcher</li>
<li>École Normale Supérieure Paris-Saclay</li>
<li>Université Paris-Saclay</li>
<li>University of Vienna</li>
<li>Queen Mary University of London</li>
<li>North Carolina State University</li>
<li>Technische Universität Berlin</li>
<li>INSAIT</li>
<li>
<p>OpenAI</p>
</li>
<li>
<p>CNRS</p>
</li>
<li>University of Porto</li>
<li>Leibniz University Hannover</li>
<li>École Normale Supérieure</li>
<li>Alan Turing Institute</li>
<li>University of Mannheim</li>
<li>Materials Platform for Data Science LLC</li>
<li>University of Oklahoma</li>
<li>University of Minnesota</li>
<li>INESC Microsistemas e Nanotecnologias</li>
<li>Fondazione Bruno Kessler</li>
<li>University of Western Australia</li>
<li>The Hospital for Sick Children</li>
<li>University of Galway</li>
<li>University of Manchester</li>
<li>The University of Chicago</li>
<li>UZ Brussel</li>
<li>Cairo University</li>
<li>The Australian National University</li>
<li>The Open University</li>
<li>University of Copenhagen</li>
<li>Purdue University</li>
<li>Indiana University</li>
<li>University of California, Davis</li>
<li>University of Illinois Chicago</li>
<li>University of North Carolina at Chapel Hill</li>
<li>Bilkent University</li>
<li>Scripps Research</li>
<li>Dell Technologies</li>
<li>Aleph Alpha</li>
<li>University of Bern</li>
<li>Metropolitan State University of Denver</li>
<li>Contramont Research</li>
<li>Texas A\&amp;M University</li>
<li>Rutgers University</li>
<li>CERo Therapeutics Holdings, Inc.</li>
<li>Sanford Burnham Preybs</li>
<li>The University of Texas at Arlington</li>
<li>University of Luxembourg</li>
<li>Pondicherry Engineering College</li>
<li>Intuit</li>
<li>Saint Mary's University</li>
<li>All India Institute of Medical Sciences</li>
<li>blurrylogic</li>
<li>Ruhr University Bochum</li>
<li>University of Windsor</li>
<li>University of Buenos Aires</li>
<li>Mănuka Honey and Beekeeping Consultancy Ltd</li>
<li>Eastlake High School</li>
<li>Royal Veterinary College</li>
<li>National University Philippines</li>
<li>Indian Institute of Technology Bombay</li>
<li>Monash University</li>
<li>Leibniz Institute for Science and Mathematics Education</li>
<li>Yonsei University</li>
<li>Cairo University Specialized Pediatric Hospital</li>
<li>Unidade Local de Saúde de Lisboa Ocidental</li>
<li>KU Leuven</li>
<li>Manhattan School of Music</li>
<li>Universidade de Lisboa,</li>
<li>Stockholm University</li>
<li>Royal Holloway, University of London</li>
<li>The Hartree Centre</li>
<li>University of Geneva</li>
<li>Tanta University</li>
<li>University of Malaya</li>
<li>Polytechnic University of the Philippines</li>
<li>Diverging Mathematics</li>
<li>Hemwati Nandan Bahuguna Garhwal University</li>
<li>Brandenburg University of Technology</li>
<li>Charité - Universitätsmedizin</li>
<li>Fyaora Labs</li>
<li>Institut Polytechnique de Paris</li>
<li>Dyno Therapeutics</li>
<li>Georgia Southern University</li>
<li>Tufts University</li>
<li>The Jackson Laboratory</li>
<li>The New School</li>
<li>SDAIA</li>
<li>Rockwell Automation</li>
<li>Politecnico di Milano</li>
<li>Université Paris Cité and Sorbonne Université</li>
<li>University of Miami</li>
<li>PeopleTec, Inc.</li>
<li>MolMind</li>
<li>Lewis Katz School of Medicine</li>
<li>University Mohammed I</li>
<li>CONICET</li>
<li>Universidad Tecnológica Nacional</li>
<li>Maastricht University</li>
<li>Jala University</li>
<li>
<p>TRR Designs</p>
</li>
<li>
<p>The Univeirsty of Tennessee</p>
</li>
<li>University of Minnesota Twin Cities</li>
<li>Swinburne University of Technology</li>
<li>Università di Milano-Bicocca</li>
<li>RWTH Aachen University</li>
<li>CERN</li>
<li>Synbionix</li>
<li>ZG Law</li>
<li>Sheffield Hallam University</li>
<li>Alberta Health Services</li>
<li>Martin-Luther-University Halle-Wittenberg</li>
<li>University of Canterbury</li>
<li>St. Petersburg College</li>
<li>La Molina National Agrarian University</li>
<li>Bogazici University</li>
<li>Abacus.AI</li>
<li>Accenture Labs</li>
<li>Clearhorse Ltd</li>
<li>Universidad Iberoamericana</li>
<li>Eastern Institute of Technology (EIT)</li>
<li>ELTE</li>
<li>ENS Lyon</li>
<li>Institute of Science and Technology Austria</li>
<li>Chalmers University of Technology</li>
<li>RUSM</li>
<li>University of Innsbruck</li>
<li>Warsaw University of Technology</li>
<li>LGM</li>
<li>Ben-Gurion University</li>
<li>Max Planck Institute for Software Systems</li>
<li>Northern Illinois University</li>
<li>Corteva Agriscience</li>
<li>Sorbonne Université</li>
<li>OncoPrecision</li>
<li>Universidade Federal de Juiz de Fora</li>
<li>Universidad de Valencia</li>
<li>Bethune-Cookman University</li>
<li>Auckland University of Technology</li>
<li>University of Technology Sydney</li>
<li>National University</li>
<li>Cranfield University</li>
<li>C. N. Yang institute for Theoretical Physics</li>
<li>Pennsylvania College of Technology</li>
<li>Queen's University</li>
<li>St. Jude Children's Research Hospital</li>
<li>Lux Labs</li>
<li>Gaia Lab</li>
<li>University of Yaoundé I</li>
<li>National Information Processing Institute</li>
<li>Université de Yaoundé I</li>
<li>Ecole Nationale Supérieure Polytechnique de Yaoundé</li>
<li>University of Leeds</li>
<li>University of Mumbai</li>
<li>Drexel University</li>
<li>Simplr AI, Asurion</li>
<li>Institute for Molecular Manufacturing</li>
<li>Ivy Natal</li>
<li>Cal Poly San Luis Obispo</li>
<li>University of Alabama Huntsville</li>
<li>Rochester Institute of Technology</li>
<li>Bournemouth University</li>
<li>Universidad de Buenos Aires</li>
<li>Cohere</li>
<li>Central Mindanao University</li>
<li>College of Eastern Idaho</li>
<li>University of the Fraser Valley</li>
<li>Patched Codes, Inc</li>
<li>EleutherAI</li>
<li>Cambridge University</li>
<li>Georgia State University</li>
<li>Snorkel AI</li>
<li>Intelligent Geometries</li>
<li>John Crane UK Ltd</li>
<li>Case Wester Reserve University</li>
<li>Czech Technical University in Prague</li>
<li>Donald and Barbara Zucker School of Medicine</li>
<li>Indiana State University</li>
<li>Missouri University of Science and Technology</li>
<li>University of Massachusetts Lowell</li>
<li>Gray Swan AI</li>
<li>University of Houston</li>
<li>The Future Paralegals of America</li>
<li>Nabu Technologies Inc</li>
<li>Universidad de Morón</li>
<li>Rice University</li>
<li>The University of Texas at Dallas</li>
<li>Quotient AI</li>
<li>Center for AI Safety</li>
<li>Florida Atlantic University</li>
<li>University of Warwick</li>
<li>University of Montreal</li>
<li>University of Virginia</li>
<li>
<p>Nimbus AI</p>
</li>
<li>
<p>CSMSS Chh. Shahu College of Engineering</p>
</li>
<li>Central College</li>
<li>Intrinsic Innovation LLC</li>
<li>Outevsky Bespoke Dance Education</li>
<li>La Trobe University</li>
<li>AIM Intelligence</li>
<li>Seoul National University</li>
<li>Indian Institute of Technology (BHU)</li>
<li>Canadian University Dubai</li>
<li>Genomia Diagnostics Research Pvt Ltd</li>
<li>EF Polymers Pvt Ltd</li>
<li>Sheffield Teaching Hospitals NHS Foundation Trust</li>
<li>HUTECH</li>
<li>Ecole polytechnique</li>
<li>Forschungszentrum Jülich</li>
<li>HUN-REN</li>
<li>Australian National University</li>
<li>Saarland University</li>
<li>Posts and Telecommunications Institute of Technology</li>
<li>Dartmouth College</li>
<li>Standard Intelligence</li>
<li>Image Processing Lab, Universitat de Valencia</li>
<li>RMIT University</li>
<li>Universal Higher Education</li>
<li>German Research Center for Artificial Intelligence</li>
<li>Aalto University</li>
<li>Nottingham Trent University</li>
<li>University of Montpellier</li>
<li>CISPA Helmholtz Center for Information Security</li>
<li>POLITEHNICA Bucharest National University of Science and Technology</li>
<li>Modulo Research</li>
<li>University of Hertfordshire</li>
<li>Univerisity of Bristol</li>
<li>CTTC / CERCA</li>
<li>King Saud University</li>
<li>Fraunhofer IMTE</li>
<li>AE Studio</li>
<li>University of Padua</li>
<li>INRIA</li>
<li>Oxford University</li>
<li>Mansoura University</li>
<li>Ruhr-Universität Bochum</li>
<li>Larkin Community Hospital</li>
<li>HomeEquity Bank</li>
<li>University of Trento</li>
<li>Ecco IT</li>
<li>Virginia Tech</li>
<li>Chulalongkorn University</li>
<li>UK AI Safety Institute</li>
<li>University of Oregon</li>
<li>EHC Investments LLC</li>
<li>James Madison University</li>
<li>Universität Zürich</li>
<li>Beni Suef University</li>
<li>École Polytechnique</li>
<li>University of Arizona</li>
<li>Aligarh Muslim University</li>
<li>Children's Hospital of Orange County</li>
<li>CICMA</li>
<li>University of Bradford</li>
<li>University of Guelph</li>
<li>IEEE Life Member</li>
<li>Van Andel Institute</li>
<li>Hereford College of Arts</li>
<li>Institute of Mathematics of NAS of Ukraine</li>
<li>Kiev School of Economics</li>
<li>Happy Technologies LLC</li>
<li>Kyiv Polytechnic Institute</li>
<li>Tel Aviv University</li>
<li>Indian Institute of Technology Kharagpur</li>
<li>Cisco</li>
<li>Menoufia University</li>
<li>Instituto Politécnico Nacional</li>
<li>Center for Scientific Research and Higher Education at Ensenada (CICESE)</li>
<li>University of Bologna</li>
<li>Manipal University Jaipur</li>
<li>Gift Horse Mouth Inspections</li>
<li>Alexandru Ioan Cuza University</li>
<li>Universidad de Granada</li>
<li>Toyota Technological Institute at Chicago</li>
<li>Hewlett Packard Enterprise</li>
<li>Gakushuin University</li>
<li>University of Hamburg</li>
<li>Google Research</li>
<li>Bison Fellers LLC</li>
<li>University of Pisa</li>
<li>Siili Solutions Oyj</li>
<li>Creative Choice LLC</li>
<li>University of Illinois</li>
<li>Instituto Superior Técnico</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Co-first Authors. ** Senior Authors. ${ }^{1}$ Work conducted while at Center for AI Safety. ${ }^{1}$ Work conducted while at Scale AI. Complete list of author affiliations in Section A. Correspondence to agibenchmark@safe.ai.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
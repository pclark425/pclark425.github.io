<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8739 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8739</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8739</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279251695</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.06923v1.pdf" target="_blank">Boosting LLM Reasoning via Spontaneous Self-Correction</a></p>
                <p><strong>Paper Abstract:</strong> While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one. One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes. However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass. To address this, we propose SPOC, a spontaneous self-correction approach that enables LLMs to generate interleaved solutions and verifications in a single inference pass, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute. SPOC considers a multi-agent perspective by assigning dual roles -- solution proposer and verifier -- to the same model. We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration. We further improve its solution proposal and verification accuracy through online reinforcement learning. Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8739.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8739.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPOC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spontaneous Self-Correction (SPOC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-loop single-pass self-correction method that interleaves solution proposals and self-verifications from the same LLM, dynamically terminating generation when the verifier approves; trained with synthetic multi-turn data and message-wise online RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct; Llama-3.1-70B-Instruct; Llama-3.3-70B-Instruct; DeepSeek-R1-Distill-Llama (8B & 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruct-tuned Llama-3 family models (8B and 70B variants) and DeepSeek-R1-distilled Llama variants used as base models; finetuned on synthetic Pair-SFT multi-turn data and further optimized with online message-wise RL (RAFT or RLOO) in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SPOC (spontaneous self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model alternates roles as proposer and verifier within a single generated trajectory: after each full-solution message the model emits a concise verification (rationale + Yes/No). If verification is 'No', the model generates another solution attempt; training uses Pair-SFT synthetic multi-turn SFT to enable this format, followed by message-wise online RL optimizing correctness rewards for both solution and verification messages (RAFT or RLOO). Termination is dynamic when verification accepts the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH500; AMC23; AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning benchmarks with verifiable final outputs: MATH500 (500 curated math problems), AMC23 (40 American Mathematics Contest problems), AIME24 (30 AIME problems).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>SPOC (best reported configurations) - Llama-3.1-8B-Instruct: pass@1 = 77.6% (MATH500), 70.0% (AMC23), 23.3% (AIME24). Llama-3.1-70B-Instruct: pass@1 = 89.9% (MATH500), 85.0% (AMC23), 53.3% (AIME24). Using the RLOO optimizer, SPOC further improves to: Llama-3.1-8B: 87.2%/87.5%/50.0% (MATH500/AMC23/AIME24); Llama-3.1-70B: 94.6%/92.5%/76.7% (MATH500/AMC23/AIME24).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no SPOC) pass@1 reported for same base models: Llama-3.1-8B-Instruct: 52.2% (MATH500), 22.5% (AMC23), 3.3% (AIME24). Llama-3.1-70B-Instruct: 65.8% (MATH500), 32.5% (AMC23), 16.7% (AIME24).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>In-model interleaved verification messages produced by the same LLM (no external oracle). Data-stage: Pair-SFT synthetic multi-turn SFT to teach the turn-taking format; RL-stage: message-wise online RL (RAFT by default; RLOO variant also used) with binary correctness rewards derived from a rule-based final-answer checker and parsing verification Yes/No outputs. Uses special message termination tokens to represent turns and dynamic termination logic conditioned on verifier outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: large pass@1 gains across benchmarks and model sizes (e.g., +8.8% and +11.6% absolute on MATH500 for 8B and 70B respectively under SPOC compared to base). Further improvements when using RLOO vs RAFT. Ablations (Table 4) show the Corr joint reward setting yields superior results versus Last/All variants, and iterative training (second iteration) gives additional gains on harder benchmarks (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires tasks with verifiable final outputs (rule-based checker); verifier reliability is imperfect (confusion matrices show non-zero FP/FN rates, with small models depending heavily on TN); naive prompting-based self-correction can degrade performance and thus explicit fine-tuning is needed; strong chain-of-thought verifiers require additional training and are out of scope; for stronger models most corrections happen rarely (fewer turns) so marginal gains may be smaller; long-chain reasoning remains a challenge (future work suggested to apply step-level partial solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to prompting-based self-refinement and prior finetuning approaches, SPOC interleaves verification during generation without external triggers. SPOC (Pair-SFT + RL) outperforms Pair-SFT alone and prompting baselines; RLOO outperforms RAFT in reported experiments. The paper situates SPOC relative to Self-Refine, Reflexion and other multi-agent/debate frameworks, arguing SPOC's open-loop single-pass dynamic termination is more flexible for deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablation on reward configurations (Corr vs Last vs All) shows Corr outperforms Last and All (e.g., on Llama-3.1-8B MATH500: Corr 61.0% vs Last 59.8% vs All 58.4%). Iterative training (second iterate of PairSFT+RL) produces additional improvements (Table 3). Data-balancing during Pair-SFT is emphasized as important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting LLM Reasoning via Spontaneous Self-Correction', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8739.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8739.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pair-SFT (variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pair Supervised Fine-Tuning (Pair-SFT) variant for multi-turn data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic data construction and SFT initialization that creates paired solution+verification multi-turn examples from base model rollouts by sampling correct/incorrect responses and prompting the base model to generate concise verification messages; used to teach the interleaved proposer/verifier response format before RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Base Llama-3 Instruct models used to rollout single-turn responses</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses the initial policy/model (π_{θ0}) to sample K single-turn solutions per question, label them with a rule-based checker, and prompt the same base model to write verification messages by comparing to a correct sample; finetune with masking of incorrect messages and reweighting to balance correct/incorrect subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Pair-SFT multi-turn SFT</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Collect K single-turn rollouts per question, label each solution as correct/incorrect, generate verification messages by prompting the base model to compare a given response with a correct sample and produce a concise error explanation + Yes/No verdict; assemble (question, solution, verification, correct-solution) tuples and SFT-finetune the model to produce interleaved multi-turn trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as pretraining step for same math benchmarks (MATH500/AMC23/AIME24)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic multi-turn dataset generation for math reasoning tasks to enable in-model verifier behavior and multi-turn generation format.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Pair-SFT alone yields modest gains over base: example Llama-3.1-8B-Instruct PairSFT (iter1) reported MATH500 53.8% vs base 52.2% (Table 3); gains are small compared to full SPOC+RL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base Llama-3.1-8B-Instruct MATH500 = 52.2% (pass@1) as reported (Table 3); Pair-SFT yields small improvements before RL.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompting the base model to generate verification messages and SFT finetuning on formed multi-turn examples; masking tokens of incorrect messages during SFT and reweighting correct/incorrect subsets for balance.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical: Pair-SFT provides initial capability to adhere to multi-turn format and slightly increases verification/solution accuracy prior to RL; paper reports Pair-SFT iter1 yields small absolute improvements and is important as a bootstrap for stable RL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Pair-SFT alone gives only modest improvement; synthetic verifications are concise and not full chain-of-thought, so verifier strength is limited; depends on the quality of base model rollouts and on reweighting to avoid verification bias.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to distillation from stronger teachers or human-annotated refinement data, Pair-SFT uses self-generated synthetic data (no stronger teacher) and is positioned as a simple, effective bootstrap before RL.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper emphasizes that balancing the number of correct vs incorrect examples in Pair-SFT improves downstream verifier accuracy and RL stability; quantitative ablation details discussed qualitatively (data balancing important).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting LLM Reasoning via Spontaneous Self-Correction', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8739.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8739.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAFT (as used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAFT (Reward-ranked fine-tuning) policy optimizer (applied message-wise)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A policy optimization / reward-ranking technique that samples multiple responses per prompt and uses best-of-N response selections to perform SFT-style updates; used here as the default message-wise RL optimizer for SPOC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to same Llama-3 Instruct base models during online RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Policy optimizer that prompts current policy to generate multiple responses and uses best-of-N ranking to form training updates; implemented under CGPO framework for SPOC's message-wise RL.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RAFT message-wise online RL</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate K trajectories per question, choose best-of-N responses based on message-level correctness rewards and perform a one-step SFT/RL update on the policy to increase probability of high-reward solution/verification messages (KL-regularized objective).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH500; AMC23; AIME24 (used during online RL stage)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same math reasoning benchmarks; RAFT used to optimize proposer/verifier shared policy with per-message correctness rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>SPOC using RAFT produced substantial gains over base; example reported (interpreted as RAFT-stage results): Llama-3.1-8B-Instruct SPOC (RAFT) reported pass@1 ~77.6% (MATH500), 70.0% (AMC23), 23.3% (AIME24) in one summary; exact partitioning of RAFT vs RLOO numbers is provided in the paper (RAFT used as default).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no RAFT RL) reported as lower (e.g., Llama-3.1-8B base MATH500 52.2%); Pair-SFT alone yields modest uplift but RAFT provides larger gains when combined with SPOC format.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>RL/policy optimization using trajectory sampling and best-of-N selection under a KL-regularized objective, applied message-wise to both solution and verification turns; uses per-message binary rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper reports consistent improvements after online RL with RAFT versus SFT-only and base; RAFT is baseline optimizer for SPOC training and yields large absolute gains when combined with the multi-turn format.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>RAFT uses best-of-N which is less sample-efficient than RLOO; paper reports RLOO often yields stronger results. RAFT updates require filtering prompts where no sampled response contains correct solution/verification (CGPO filter).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>RAFT compared against RLOO in this paper; RLOO gave stronger empirical performance and better sample efficiency in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>The paper compares RAFT vs RLOO empirically and finds RLOO augmented with process rewards often outperforms RAFT; no further RAFT-specific ablations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting LLM Reasoning via Spontaneous Self-Correction', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8739.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8739.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLOO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RLOO (Reward Leave-One-Out variant used as message-wise optimizer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative message-wise policy optimization variant that subtracts the mean reward and divides by standard deviation across messages (a leave-one-out style advantage normalization), allowing optimization across all generated responses with better sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to SPOC training on Llama-3 Instruct models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RLOO replaces the BoN selection of RAFT by normalizing rewards across samples (mean subtraction and division by std) and using all generated responses for policy updates, improving sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RLOO message-wise online RL</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Compute per-message advantages by normalizing message-level rewards across K rollouts (mean subtraction and division by std), use all sampled trajectories in the learning batch, and perform gradient updates for both proposer and verifier objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH500; AMC23; AIME24 (used during online RL stage)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same math reasoning benchmarks; RLOO used as an alternative policy optimizer during SPOC training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>RLOO yields stronger results than RAFT in this work: reported SPOC+RLOO results - Llama-3.1-8B-Instruct: pass@1 = 87.2% (MATH500), 87.5% (AMC23), 50.0% (AIME24). Llama-3.1-70B-Instruct: pass@1 = 94.6% (MATH500), 92.5% (AMC23), 76.7% (AIME24).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to same-model base (no RLOO RL) numbers: Llama-3.1-8B base MATH500 52.2% etc; RLOO enables much larger improvements than SFT-only or RAFT in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Message-wise RL with normalized advantages across rollouts (mean-subtraction and std-division), uses all sampled trajectories rather than best-of-N, and applies updates to shared proposer/verifier policy under KL regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Direct quantitative comparison in the paper: RLOO-trained SPOC substantially outperforms RAFT-trained SPOC in reported pass@1 on all evaluated benchmarks and model sizes (numerical values given above).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>None specific beyond general RL training challenges; requires reward signals (rule-based checker/parsing) and stable value of verifier; may be sensitive to reward design and process rewards (authors report process rewards help).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper reports RLOO is more sample-efficient and achieves stronger final performance than RAFT in SPOC training; RLOO uses all trajectories rather than best-of-N and normalizes message-level rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Authors report that RLOO augmented with process rewards yields the strongest results; ablations on reward configurations (Corr/Last/All) were also conducted in conjunction with RL choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting LLM Reasoning via Spontaneous Self-Correction', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8739.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8739.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback (Madaan et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior iterative refinement method where a model generates an initial solution then self-critique and attempts corrections, typically triggered by explicit prompts; mentioned in related work as a prompting-based self-correction approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (prompted iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate a solution, then prompt the model to identify/correct errors and re-generate refined solutions; typically requires extra system prompts to trigger reflection and may rely on oracle labels for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to ask the model to critique and rewrite its own answers (closed-loop prompting requiring external triggers), not spontaneous single-pass interleaving.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites prior observations that prompting-based self-refinement can lead to minimal improvements or even degradation without strong assumptions or external feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior works (cited) report limited gains or degraded performance in zero-oracle settings; prompting requires additional system design to trigger corrections at inference time, making deployment and dynamic compute scaling harder.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with SPOC which trains the model to spontaneously interleave proposals and verifications without external triggers; Self-Refine typically relies on explicit prompts during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting LLM Reasoning via Spontaneous Self-Correction', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8739.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8739.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning (Shinn et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that trains language agents to iteratively self-reflect and improve through verbal RL loops; cited as example of generate-then-reflect / verbal RL family of methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents generate solutions, receive verbal feedback or self-generated reflections, then learn to use that feedback to improve future outputs; typically involves repeated cycles and may use external memory of reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Verbal reinforcement learning with iterative self-reflection cycles; often uses external memory and separate feedback mechanisms (prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as related work demonstrating benefits of verbal RL/self-reflection in some tasks; SPOC differs by enabling spontaneous single-pass interleaving inside generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior methods may need external memory/prompting and do not perform single-pass spontaneous correction; cited concerns around needing oracles or external feedback for strong gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting LLM Reasoning via Spontaneous Self-Correction', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Training language models to self-correct via reinforcement learning <em>(Rating: 2)</em></li>
                <li>S2r: Teaching llms to self-verify and self-correct via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-rewarding correction for mathematical reasoning <em>(Rating: 2)</em></li>
                <li>Generative verifiers: Reward modeling as next-token prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8739",
    "paper_id": "paper-279251695",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "SPOC",
            "name_full": "Spontaneous Self-Correction (SPOC)",
            "brief_description": "An open-loop single-pass self-correction method that interleaves solution proposals and self-verifications from the same LLM, dynamically terminating generation when the verifier approves; trained with synthetic multi-turn data and message-wise online RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct; Llama-3.1-70B-Instruct; Llama-3.3-70B-Instruct; DeepSeek-R1-Distill-Llama (8B & 70B)",
            "model_description": "Instruct-tuned Llama-3 family models (8B and 70B variants) and DeepSeek-R1-distilled Llama variants used as base models; finetuned on synthetic Pair-SFT multi-turn data and further optimized with online message-wise RL (RAFT or RLOO) in this work.",
            "reflection_method_name": "SPOC (spontaneous self-correction)",
            "reflection_method_description": "The model alternates roles as proposer and verifier within a single generated trajectory: after each full-solution message the model emits a concise verification (rationale + Yes/No). If verification is 'No', the model generates another solution attempt; training uses Pair-SFT synthetic multi-turn SFT to enable this format, followed by message-wise online RL optimizing correctness rewards for both solution and verification messages (RAFT or RLOO). Termination is dynamic when verification accepts the solution.",
            "task_name": "MATH500; AMC23; AIME24",
            "task_description": "Mathematical reasoning benchmarks with verifiable final outputs: MATH500 (500 curated math problems), AMC23 (40 American Mathematics Contest problems), AIME24 (30 AIME problems).",
            "performance_with_reflection": "SPOC (best reported configurations) - Llama-3.1-8B-Instruct: pass@1 = 77.6% (MATH500), 70.0% (AMC23), 23.3% (AIME24). Llama-3.1-70B-Instruct: pass@1 = 89.9% (MATH500), 85.0% (AMC23), 53.3% (AIME24). Using the RLOO optimizer, SPOC further improves to: Llama-3.1-8B: 87.2%/87.5%/50.0% (MATH500/AMC23/AIME24); Llama-3.1-70B: 94.6%/92.5%/76.7% (MATH500/AMC23/AIME24).",
            "performance_without_reflection": "Base (no SPOC) pass@1 reported for same base models: Llama-3.1-8B-Instruct: 52.2% (MATH500), 22.5% (AMC23), 3.3% (AIME24). Llama-3.1-70B-Instruct: 65.8% (MATH500), 32.5% (AMC23), 16.7% (AIME24).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "In-model interleaved verification messages produced by the same LLM (no external oracle). Data-stage: Pair-SFT synthetic multi-turn SFT to teach the turn-taking format; RL-stage: message-wise online RL (RAFT by default; RLOO variant also used) with binary correctness rewards derived from a rule-based final-answer checker and parsing verification Yes/No outputs. Uses special message termination tokens to represent turns and dynamic termination logic conditioned on verifier outcome.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: large pass@1 gains across benchmarks and model sizes (e.g., +8.8% and +11.6% absolute on MATH500 for 8B and 70B respectively under SPOC compared to base). Further improvements when using RLOO vs RAFT. Ablations (Table 4) show the Corr joint reward setting yields superior results versus Last/All variants, and iterative training (second iteration) gives additional gains on harder benchmarks (Table 3).",
            "limitations_or_failure_cases": "Requires tasks with verifiable final outputs (rule-based checker); verifier reliability is imperfect (confusion matrices show non-zero FP/FN rates, with small models depending heavily on TN); naive prompting-based self-correction can degrade performance and thus explicit fine-tuning is needed; strong chain-of-thought verifiers require additional training and are out of scope; for stronger models most corrections happen rarely (fewer turns) so marginal gains may be smaller; long-chain reasoning remains a challenge (future work suggested to apply step-level partial solutions).",
            "comparison_to_other_methods": "Compared to prompting-based self-refinement and prior finetuning approaches, SPOC interleaves verification during generation without external triggers. SPOC (Pair-SFT + RL) outperforms Pair-SFT alone and prompting baselines; RLOO outperforms RAFT in reported experiments. The paper situates SPOC relative to Self-Refine, Reflexion and other multi-agent/debate frameworks, arguing SPOC's open-loop single-pass dynamic termination is more flexible for deployment.",
            "ablation_study_results": "Ablation on reward configurations (Corr vs Last vs All) shows Corr outperforms Last and All (e.g., on Llama-3.1-8B MATH500: Corr 61.0% vs Last 59.8% vs All 58.4%). Iterative training (second iterate of PairSFT+RL) produces additional improvements (Table 3). Data-balancing during Pair-SFT is emphasized as important.",
            "uuid": "e8739.0",
            "source_info": {
                "paper_title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Pair-SFT (variant)",
            "name_full": "Pair Supervised Fine-Tuning (Pair-SFT) variant for multi-turn data",
            "brief_description": "A synthetic data construction and SFT initialization that creates paired solution+verification multi-turn examples from base model rollouts by sampling correct/incorrect responses and prompting the base model to generate concise verification messages; used to teach the interleaved proposer/verifier response format before RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Base Llama-3 Instruct models used to rollout single-turn responses",
            "model_description": "Uses the initial policy/model (π_{θ0}) to sample K single-turn solutions per question, label them with a rule-based checker, and prompt the same base model to write verification messages by comparing to a correct sample; finetune with masking of incorrect messages and reweighting to balance correct/incorrect subsets.",
            "reflection_method_name": "Pair-SFT multi-turn SFT",
            "reflection_method_description": "Collect K single-turn rollouts per question, label each solution as correct/incorrect, generate verification messages by prompting the base model to compare a given response with a correct sample and produce a concise error explanation + Yes/No verdict; assemble (question, solution, verification, correct-solution) tuples and SFT-finetune the model to produce interleaved multi-turn trajectories.",
            "task_name": "Used as pretraining step for same math benchmarks (MATH500/AMC23/AIME24)",
            "task_description": "Synthetic multi-turn dataset generation for math reasoning tasks to enable in-model verifier behavior and multi-turn generation format.",
            "performance_with_reflection": "Pair-SFT alone yields modest gains over base: example Llama-3.1-8B-Instruct PairSFT (iter1) reported MATH500 53.8% vs base 52.2% (Table 3); gains are small compared to full SPOC+RL.",
            "performance_without_reflection": "Base Llama-3.1-8B-Instruct MATH500 = 52.2% (pass@1) as reported (Table 3); Pair-SFT yields small improvements before RL.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompting the base model to generate verification messages and SFT finetuning on formed multi-turn examples; masking tokens of incorrect messages during SFT and reweighting correct/incorrect subsets for balance.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical: Pair-SFT provides initial capability to adhere to multi-turn format and slightly increases verification/solution accuracy prior to RL; paper reports Pair-SFT iter1 yields small absolute improvements and is important as a bootstrap for stable RL.",
            "limitations_or_failure_cases": "Pair-SFT alone gives only modest improvement; synthetic verifications are concise and not full chain-of-thought, so verifier strength is limited; depends on the quality of base model rollouts and on reweighting to avoid verification bias.",
            "comparison_to_other_methods": "Compared to distillation from stronger teachers or human-annotated refinement data, Pair-SFT uses self-generated synthetic data (no stronger teacher) and is positioned as a simple, effective bootstrap before RL.",
            "ablation_study_results": "Paper emphasizes that balancing the number of correct vs incorrect examples in Pair-SFT improves downstream verifier accuracy and RL stability; quantitative ablation details discussed qualitatively (data balancing important).",
            "uuid": "e8739.1",
            "source_info": {
                "paper_title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "RAFT (as used)",
            "name_full": "RAFT (Reward-ranked fine-tuning) policy optimizer (applied message-wise)",
            "brief_description": "A policy optimization / reward-ranking technique that samples multiple responses per prompt and uses best-of-N response selections to perform SFT-style updates; used here as the default message-wise RL optimizer for SPOC.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to same Llama-3 Instruct base models during online RL",
            "model_description": "Policy optimizer that prompts current policy to generate multiple responses and uses best-of-N ranking to form training updates; implemented under CGPO framework for SPOC's message-wise RL.",
            "reflection_method_name": "RAFT message-wise online RL",
            "reflection_method_description": "Generate K trajectories per question, choose best-of-N responses based on message-level correctness rewards and perform a one-step SFT/RL update on the policy to increase probability of high-reward solution/verification messages (KL-regularized objective).",
            "task_name": "MATH500; AMC23; AIME24 (used during online RL stage)",
            "task_description": "Same math reasoning benchmarks; RAFT used to optimize proposer/verifier shared policy with per-message correctness rewards.",
            "performance_with_reflection": "SPOC using RAFT produced substantial gains over base; example reported (interpreted as RAFT-stage results): Llama-3.1-8B-Instruct SPOC (RAFT) reported pass@1 ~77.6% (MATH500), 70.0% (AMC23), 23.3% (AIME24) in one summary; exact partitioning of RAFT vs RLOO numbers is provided in the paper (RAFT used as default).",
            "performance_without_reflection": "Base (no RAFT RL) reported as lower (e.g., Llama-3.1-8B base MATH500 52.2%); Pair-SFT alone yields modest uplift but RAFT provides larger gains when combined with SPOC format.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "RL/policy optimization using trajectory sampling and best-of-N selection under a KL-regularized objective, applied message-wise to both solution and verification turns; uses per-message binary rewards.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper reports consistent improvements after online RL with RAFT versus SFT-only and base; RAFT is baseline optimizer for SPOC training and yields large absolute gains when combined with the multi-turn format.",
            "limitations_or_failure_cases": "RAFT uses best-of-N which is less sample-efficient than RLOO; paper reports RLOO often yields stronger results. RAFT updates require filtering prompts where no sampled response contains correct solution/verification (CGPO filter).",
            "comparison_to_other_methods": "RAFT compared against RLOO in this paper; RLOO gave stronger empirical performance and better sample efficiency in the reported experiments.",
            "ablation_study_results": "The paper compares RAFT vs RLOO empirically and finds RLOO augmented with process rewards often outperforms RAFT; no further RAFT-specific ablations reported.",
            "uuid": "e8739.2",
            "source_info": {
                "paper_title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "RLOO",
            "name_full": "RLOO (Reward Leave-One-Out variant used as message-wise optimizer)",
            "brief_description": "An alternative message-wise policy optimization variant that subtracts the mean reward and divides by standard deviation across messages (a leave-one-out style advantage normalization), allowing optimization across all generated responses with better sample efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to SPOC training on Llama-3 Instruct models",
            "model_description": "RLOO replaces the BoN selection of RAFT by normalizing rewards across samples (mean subtraction and division by std) and using all generated responses for policy updates, improving sample efficiency.",
            "reflection_method_name": "RLOO message-wise online RL",
            "reflection_method_description": "Compute per-message advantages by normalizing message-level rewards across K rollouts (mean subtraction and division by std), use all sampled trajectories in the learning batch, and perform gradient updates for both proposer and verifier objectives.",
            "task_name": "MATH500; AMC23; AIME24 (used during online RL stage)",
            "task_description": "Same math reasoning benchmarks; RLOO used as an alternative policy optimizer during SPOC training.",
            "performance_with_reflection": "RLOO yields stronger results than RAFT in this work: reported SPOC+RLOO results - Llama-3.1-8B-Instruct: pass@1 = 87.2% (MATH500), 87.5% (AMC23), 50.0% (AIME24). Llama-3.1-70B-Instruct: pass@1 = 94.6% (MATH500), 92.5% (AMC23), 76.7% (AIME24).",
            "performance_without_reflection": "Compared to same-model base (no RLOO RL) numbers: Llama-3.1-8B base MATH500 52.2% etc; RLOO enables much larger improvements than SFT-only or RAFT in reported comparisons.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Message-wise RL with normalized advantages across rollouts (mean-subtraction and std-division), uses all sampled trajectories rather than best-of-N, and applies updates to shared proposer/verifier policy under KL regularization.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Direct quantitative comparison in the paper: RLOO-trained SPOC substantially outperforms RAFT-trained SPOC in reported pass@1 on all evaluated benchmarks and model sizes (numerical values given above).",
            "limitations_or_failure_cases": "None specific beyond general RL training challenges; requires reward signals (rule-based checker/parsing) and stable value of verifier; may be sensitive to reward design and process rewards (authors report process rewards help).",
            "comparison_to_other_methods": "Paper reports RLOO is more sample-efficient and achieves stronger final performance than RAFT in SPOC training; RLOO uses all trajectories rather than best-of-N and normalizes message-level rewards.",
            "ablation_study_results": "Authors report that RLOO augmented with process rewards yields the strongest results; ablations on reward configurations (Corr/Last/All) were also conducted in conjunction with RL choices.",
            "uuid": "e8739.3",
            "source_info": {
                "paper_title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-Refine (prior work)",
            "name_full": "Self-Refine: Iterative refinement with self-feedback (Madaan et al., 2023)",
            "brief_description": "A prior iterative refinement method where a model generates an initial solution then self-critique and attempts corrections, typically triggered by explicit prompts; mentioned in related work as a prompting-based self-correction approach.",
            "citation_title": "Self-Refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-Refine (prompted iterative refinement)",
            "reflection_method_description": "Generate a solution, then prompt the model to identify/correct errors and re-generate refined solutions; typically requires extra system prompts to trigger reflection and may rely on oracle labels for best performance.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Prompt engineering to ask the model to critique and rewrite its own answers (closed-loop prompting requiring external triggers), not spontaneous single-pass interleaving.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites prior observations that prompting-based self-refinement can lead to minimal improvements or even degradation without strong assumptions or external feedback.",
            "limitations_or_failure_cases": "Prior works (cited) report limited gains or degraded performance in zero-oracle settings; prompting requires additional system design to trigger corrections at inference time, making deployment and dynamic compute scaling harder.",
            "comparison_to_other_methods": "Contrasted with SPOC which trains the model to spontaneously interleave proposals and verifications without external triggers; Self-Refine typically relies on explicit prompts during inference.",
            "uuid": "e8739.4",
            "source_info": {
                "paper_title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Reflexion (prior work)",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning (Shinn et al., 2023)",
            "brief_description": "A prior approach that trains language agents to iteratively self-reflect and improve through verbal RL loops; cited as example of generate-then-reflect / verbal RL family of methods.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reflexion / generate-then-reflect",
            "reflection_method_description": "Agents generate solutions, receive verbal feedback or self-generated reflections, then learn to use that feedback to improve future outputs; typically involves repeated cycles and may use external memory of reflections.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Verbal reinforcement learning with iterative self-reflection cycles; often uses external memory and separate feedback mechanisms (prior work).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as related work demonstrating benefits of verbal RL/self-reflection in some tasks; SPOC differs by enabling spontaneous single-pass interleaving inside generation.",
            "limitations_or_failure_cases": "Prior methods may need external memory/prompting and do not perform single-pass spontaneous correction; cited concerns around needing oracles or external feedback for strong gains.",
            "uuid": "e8739.5",
            "source_info": {
                "paper_title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Training language models to self-correct via reinforcement learning",
            "rating": 2,
            "sanitized_title": "training_language_models_to_selfcorrect_via_reinforcement_learning"
        },
        {
            "paper_title": "S2r: Teaching llms to self-verify and self-correct via reinforcement learning",
            "rating": 2,
            "sanitized_title": "s2r_teaching_llms_to_selfverify_and_selfcorrect_via_reinforcement_learning"
        },
        {
            "paper_title": "Self-rewarding correction for mathematical reasoning",
            "rating": 2,
            "sanitized_title": "selfrewarding_correction_for_mathematical_reasoning"
        },
        {
            "paper_title": "Generative verifiers: Reward modeling as next-token prediction",
            "rating": 1,
            "sanitized_title": "generative_verifiers_reward_modeling_as_nexttoken_prediction"
        }
    ],
    "cost": 0.01963175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Boosting LLM Reasoning via Spontaneous Self-Correction
7 Jun 2025</p>
<p>Xutong Zhao xutong.zhao@mila.quebec 
Work done at Meta</p>
<p>Work done at Meta</p>
<p>Tengyu Xu 
Work done at Meta</p>
<p>Xuewei Wang 
Work done at Meta</p>
<p>Zhengxing Chen 
Work done at Meta</p>
<p>Di Jin 
Work done at Meta</p>
<p>Liang Tan 
Work done at Meta</p>
<p>Zishun Yu 
Work done at Meta</p>
<p>Zhuokai Zhao 
Work done at Meta</p>
<p>Yun He 
Work done at Meta</p>
<p>Sinong Wang 
Work done at Meta</p>
<p>Han Fang 
Work done at Meta</p>
<p>Sarath Chandar 
Work done at Meta</p>
<p>Chen Zhu 
Work done at Meta</p>
<p>Metaai 
Work done at Meta</p>
<p>Mila -Quebec 
Work done at Meta</p>
<p>A I Institute 
Work done at Meta</p>
<p>Polytechnique Montréal 
Work done at Meta</p>
<p>Boosting LLM Reasoning via Spontaneous Self-Correction
7 Jun 2025541C7904BCF6D63D4B26B80C90B61A0AarXiv:2506.06923v1[cs.AI]Question Solution 1 Reflection request 1 Reflection: No Revise request Solution 2
While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one.One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes.However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass.To address this, we propose SPOC, a spontaneous self-correction approach that enables LLMs to generate interleaved solutions and verifications in a single inference pass, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute.SPOC considers a multi-agent perspective by assigning dual roles -solution proposer and verifier -to the same model.We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration.We further improve its solution proposal and verification accuracy through online reinforcement learning.Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance.Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have showcased promising results across a broad spectrum of text generation tasks.Among the various domains of LLM applications, mathematical reasoning remains particularly challenging due to its symbolic and structured nature (Shao et al., 2024;Chen et al., 2024).Recent advances in self-correction (Shinn et al., 2023;Madaan et al., 2023) have emerged as a promising paradigm towards self-improvement through iterative critique and refinement of model's own responses.</p>
<p>However, the effectiveness and practicality of existing self-correction approaches remain unclear.Naive prompting methods may lead to minimal improvement or performance degradation without access to external feedback (Huang et al., 2023;Qu et al., 2024).Finetuning-based methods seek to address such issues by post-training the LLM on refinement data collected from oracles (Saunders et al., 2022;Qu et al., 2024) or the learner model itself (Kumar et al., 2024).Nonetheless, these approaches typically rely on a specific prompt after each model response to trigger self-reflection or correction (Figures 1a and 1b), necessitating additional system design to inject these prompts during inference.In other words, existing approaches lack the ability to spontaneously and adaptively self reflect and correct, resulting in ineffective test-time compute scaling and inflexible deployment in practice.</p>
<p>To address these challenges, we introduce SPOC, a spontaneous self-correction approach that enables LLMs to spontaneously generate interleaved solutions and verifications in a single inference pass.SPOC employs an open-loop inference paradigm, which triggers self-correction only when the self-verification identifies errors, and iteratively revises the solution until it passes self-verification, without requiring any external interventions during response generation.It dynamically elicits and terminates generations on-the-fly using solely the model's inherent capabilities, thereby effectively scaling inference time compute.We consider a multi-agent formalism that models the alternating solutions and verifications as the interaction between a solution proposer and a verifier, and adopt a self-play training strategy by assigning dual roles to the same model.We adopt a simple yet effective approach to generate synthetic data from the initial model for supervised fine-tuning (Welleck et al., 2022), enabling the model to adhere to the multi-turn generation style, meanwhile developing capabilities for self-verification and inter-agent collaboration without distilling from a stronger teacher.We further boost the model's accuracy in its solution proposal and verification via online reinforcement learning, using the correctness of solutions and verifications as the reward.</p>
<p>Our main contributions are threefold:</p>
<p>• We demonstrate that generating self-verification and correction trajectories from the initial model's correct and incorrect outputs effectively bootstraps its spontaneous self-verification and correction behavior.We call out the importance of data balancing in achieving high verification accuracy in this stage, which in turn benefits the subsequent RL phase.</p>
<p>• We propose the message-wise online RL framework for SPOC, and present the formulation of RAFT (Dong et al., 2023) and RLOO (Ahmadian et al., 2024) as the RL stage of SPOC for enhancing self-verification and correction accuracies.Our results show that RLOO, augmented with process rewards for each solution or verification step, yields stronger results.</p>
<p>• We achieve significant improvements on math reasoning tasks across model sizes and task difficulties using our pipeline without distilling from stronger models.SPOC boosts the pass@1 accuracy of Llama-3.1-8B and 70B Instruct models-improving performance by 8.8% and 11.6% on MATH500, by 10.0% and 20.0% on AMC23, and by 3.3% and 6.7% on AIME24.</p>
<p>Related work</p>
<p>Self-correction.Given that high-quality external feedback is often unavailable across various realistic circumstances, it is beneficial to enable an LLM to correct its initial responses based on solely on its inherent capabilities.Prior works on such intrinsic self-correction (Huang et al., 2023) or self-refinement can be categorized into two groups based on the problem settings and correction mechanisms: prompting and finetuning.Recent works (Huang et al., 2023;Qu et al., 2024) show that prior prompting methods lead to minimal improvement or degrading performance without strong assumptions on problem settings.For instance, Shinn et al. (2023) rely on oracle labels which are often unavailable in real-world applications; Madaan et al. (2023) use less informative prompts for initial responses, resulting in overestimation of correction performance.</p>
<p>Finetuning methods seek to improve correction performance via finetuning the LLM on refinement data, collected from human annotators (Saunders et al., 2022), stronger models (Qu et al., 2024), or the learner model itself (Kumar et al., 2024).However, these works lack the mechanisms that correct errors while generating solutions in a single inference pass (Ye et al., 2024).Our work is akin to concurrent works on self-correction (Ma et al., 2025;Xiong et al., 2025).Differently, Xiong et al. (2023) re-attempts a solution within the verification instead of evaluating the previous one; moreover, they only apply RAFT in their learning framework, while we also conduct experiments on RLOO.Ma et al. (2025) uses the more complex GRPO as their RL algorithm, while we show that better performance can be achieved in the same setting (Llama 3.1 8B) by using simpler RL algorithms like RAFT for SPOC.</p>
<p>Multi-agent frameworks.By introducing multiple roles into problem-solving, multi-agent formalisms serve as a different perspective to address complex reasoning tasks.AutoGen (Wu et al., 2023) and debatebased frameworks (Du et al., 2023;Liang et al., 2023) solve math problems through customized inter-agent conversations.Despite increased test-time computation, these works lack post-training for different agent roles, which may result in suboptimal performance or distribution shifts at inference time (Xiang et al., 2025).While other works train separate models to perform correction (Motwani et al., 2024;Havrilla et al., 2024;Akyürek et al., 2023;Paul et al., 2023), models do not perform spontaneous corrections during solution generations; instead, they require extra system designs to trigger and stop corrections at deployment.In contrast, our method enables dynamic inference-time scaling by improving the model's own inherent deliberation capabilities.</p>
<p>Method</p>
<p>In this section, we first introduce the multi-turn formalism, in which the agent performs interleaved solution and verification turns.We then discuss how we finetune the agent to ensure it consistently adheres to the multi-turn response style.We finally describe our online reinforcement learning scheme which further boosts the final accuracy of the policy.Figure 2 illustrates the two stages, fine-tuning and online RL, of SPOC.</p>
<p>Multi-turn formalism</p>
<p>Problem setup.Let D ≡ X × Y = {(x i , y * i )} N i=1 be a dataset of N math problems, where each pair (x, y * ) contains a question x i and the corresponding solution y * i with ground-truth final answer.An LLM agent is defined by the policy π θ (•|x), parameterized by θ, that generates the solution y to solve the given problem x.</p>
<p>Alternated-turn generation.Suppose given a question x, the LLM generates a trajectory consisting of L interleaved solutions and verifications τ = (y 1 , v 1 , . . ., y L , v L ), where a solution y l indicating the model's l-th complete solution attempt that reaches a final answer, and a verification v l indicating the l-th self-verification validating correctness of the solution y l .For clarity, message or turn refers to each single solution y l or verification v l , and response or generation τ refers to the entire trajectory until the end.For brevity, we denote previous l turns by: τ l = (y 1:l , v 1:l ) and τ vf l = (y 1:l , v 1:l−1 ).The timestep t ∈ N 0 indicates a single decoding step where the LLM outputs one token from its policy distribution.</p>
<p>Multi-agent formulation.We model the reasoning task as an extensive-form game (EFG) (Osborne, 1994;Shoham and Leyton-Brown, 2008), which generalizes the Markov Decision Process (MDP) (Sutton, 2018) to a turn-taking interaction between solution proposer and verifier.At each turn, the proposer outputs a solution to the given math problem, and the verifier assesses its correctness.In this context, the EFG is a tuple ⟨N , A, S, T , r, I, γ⟩, where N = {1, . . ., n} is the set of n = 2 players (i.e. the proposer and verifier), A is a finite set of actions (i.e. the LLM's token space), S is a finite set of states (i.e. each state is a question and a sequence of reasoning/verification steps in context), T ⊂ S is a subset of terminal states (i.e.complete response trajectories τ = (y 1 , v 1 , . . ., y L , v L )), r : T × N 0 → ∆ n r ⊂ R n is the reward function assigning each player a scalar utility at terminal states (i.e.∆ r = {0, 1} characterizes binary outcome feedback), I : S → N is a player identity function identifing which player acts at s (i.e.I(τ l ) = 1 and I(τ vf l ) = 2), and γ ∈ [0, 1] is the discount factor.Unlike the general definition of EFGs, we do not distinguish between histories and states due to the deterministic dynamics and perfect-information nature in mathematical reasoning (i.e.τ l+1 = τ l ∪ {y l+1 , v l+1 }).We denote the proposer's and the verifier's action spaces as A sl ⊂ A and A vf ⊂ A, representing the set of solution and verification messages, respectively.We define a per-step reward function for a transition as r(s, a) representing a vector of reward to both agents.The return for player i ∈ N is defined as
G t,i = ∞ k=0 γ k r i (s t+k , a t+k ). The corresponding state-action value function under policy π is Q πi (s, a) = E π [G t,i |s t = s, a t = a].
To improve reasoning capabilities by learning from both solution and verification experiences, we adopt the commonly-used self-play strategy with parameter sharing (Albrecht et al., 2024), where the proposer policy π sl : S → ∆(A sl ) and the verifier policy π vf : S → ∆(A vf ) share the same set of parameters θ.The policy π θ outputs alternated solution and verification messages depending on the context1 .</p>
<p>Policy optimization.We optimize the policy π θ by maximizing the KL-regularized learning objective
J(θ) = E s∼ρ,a∼π(•|s) [Q π (s, a)] − η • E s∼ρ <a href="1">KL(π θ (•|s)|π θ0 (•|s))</a>
where ρ indicates the discounted state distribution, η &gt; 0 is the KL-regularization coefficient, and π θ0 is the reference policy parameterized by the initial parameters θ 0 .This objective has a close-form solution for the optimal policy π * (a|s
) = 1 Z(s) π θ0 (a|s) exp( 1 η Q(s, a)), where Z(s) = E a∼π θ 0 (•|s) [exp( 1 η Q(s, a))].
Given our multi-agent formulation, this objective introduces an individual objective for each role, namely
J sl (θ) = E[Q sl π (s, a)] − η sl • E[KL(π sl θ (•|s)|π sl θ0 (•|s))]
(2)
J vf (θ) = E[Q vf π (s, a)] − η vf • E<a href="3">KL(π vf θ (•|s)|π vf θ0 (•|s))</a>
Due to shared parameters across both roles, we jointly optimize both objectives using common generated trajectory experiences.Hence the optimal proposer and verifier policies satisfy π sl * (a|s) ∝ π sl θ0 (a|s) exp( 1 η Q sl (s, a)) and π vf * (a|s) ∝ π vf θ0 (a|s) exp( 1 η Q vf (s, a)), respectively, implying the optimal shared policy increases the probability of outputting high-rewarding solutions/verifications.Note that the optimal policy for the unregularized learning objective (η = 0) results in the maximizer of the action-value function: s, a)], also yielding high probablity of generating high-rewarding messages.
π * (•|s) = arg max π(•|s)∈∆(A) E a∼π(•|s) [Q π (
Reward setting.To obtain a reward signal for each token in each message, we evaluate the outcome correctness of each message.In particular, we assume access to a rule-based checker for the final answer in the solution, and provide a binary outcome reward denoted by r sl (y, y * ) ∈ {0, 1}, where r sl (y, y * ) = 1 when the model answer matches the ground-truth answer.Similarly, we parse the Yes/No conclusion in each verification, and denote the reward function by r vf (v, v * ) ∈ {0, 1}, with v * = r sl (y, y * ) indicating the ground-truth verification.</p>
<p>Figure 3a shows the joint reward setting, denoted by Corr hereafter.To obtain maximal returns against each other role, our reward setting admits one unique Nash equilibrium (Shoham and Leyton-Brown, 2008) with the joint policy (i.e. the shared policy π) generating both correct solutions and correct verifications.</p>
<p>Enabling multi-turn generation</p>
<p>Since off-the-shelf LLMs do not adhere to the response style of interleaved solution and verification turns by default, before conducting RL optimization, we first perform an initial finetuning with multi-turn data to enable such behaviour.To collect such data, we implement a variant of Pair-SFT (Kumar et al., 2024;Welleck et al., 2022) to construct synthetic correction responses.</p>
<p>In particular, we rollout the base policy π θ0 to collect single-turn responses for each question x i ∈ X , denoted by
{y k i } K k=1 ∼ π θ0 (•|x i ).
For each response, we record its binary correctness using the solution reward function
r k i = r sl (y k i , y * i ).
We obtain the verification message of one single-turn response by pairing it with a correct sampled response.To generate verification of one response, either correct or incorrect, we prompt the same base model π θ0 to identify the potential error, briefly explain it, and output a final binary conclusion indicating correctness of the given solution.The entire verification message is denoted as
v i ∼ π θ0 (•|x i , y i , y * i )
, where y * i indicates the correct sample.We denote this synthetic multi-turn correction dataset as the Pair-SFT dataset
D pair = {(x i , y − i , v − i , y * i )} ∪ {(x i , y + i , v + i )}
, where the +/− superscripts indicates correctness of the corresponding solution turn.We perform SFT finetuning on the base model, with tokens in incorrect messages masked out, and denote the finetuned model by π θ sft .In practice, we observe that reweighting the subsets
{(x i , y − i , v − i , y * i )} and {(x i , y + i , v + i )
} to approximately the same scale leads to a π θ sft with higher verification accuracy and more stable RL training afterwards.The complete training data collection procedure is detailed in Algorithm 2.</p>
<p>When generating the verification messages, we adapt the generative critic method (Zhang et al., 2024;Zheng et al., 2024) that prompts the model to respond with rationales before judging solution correctness, except that our variant concisely explains the error rather than performing a chain-of-thought (COT) analysis.Obtaining a strong COT verifier requires explicit training and it is out of scope of this work.Prompt templates for data construction are detailed in Appendix E.</p>
<p>Algorithm 1 SPOC Message-wise Online Reinforcement Learning
1: Inputs: Question-answer dataset D = X × Y = {(x j , y * j )} N j=1
, policy model π θ parameterized by θ, number of questions N , number of steps T , number of rollouts per question K, batch size B, rule-based solution correctness reward function r sl (y, y * ) ∈ {0, 1}, verification correctness reward function
r vf (v, v * ) ∈ {0, 1} 2: for i = 1, . . . , T do 3: Sample a batch D i ⊂ D of size B 4: Sample K trajectories for each x j ∈ X i : {τ k j } K k=1 ∼ π θ (•|x j ), where τ k j = (y j,k 1:L k , v j,k 1:L k ) 5:
Label binary rewards:
r sl j,k,l = r sl (y j,k l , y * j ), r vf j,k,l = r vf (v j,k l , v * j,k,l ), where v * j,k,l = r sl j,k,l 6:
Update policy with any policy optimization algorithm (e.g.Algorithm 3, Algorithm 4)</p>
<p>7: end for 8: return π θ</p>
<p>Online reinforcement learning</p>
<p>With the multi-turn problem formulated and the agent adhering to the multi-turn responses style, we conduct online reinforcement learning to improve the policy performance.The overall message-level RL training procedure is described in Algorithm 1.While SPOC is compatible with any policy optimization method, we apply RAFT (Dong et al., 2023) unless otherwise specified.The RAFT policy optimization algorithm is presented in Algorithm 3.</p>
<p>Besides the RAFT policy optimizer, we also implement an RLOO (Ahmadian et al., 2024) variant, which replaces the leave-one-out procedure with subtraction of the mean reward across all messages, followed by division by the standard deviation.We refer to this approach as RLOO for brevity.Unlike the best-of-N (BoN) response selection strategy in RAFT, RLOO optimizes the policy using all generated responses, enjoying better sample efficiency.The RLOO policy optimization process is detailed in Algorithm 4.</p>
<p>Experiments</p>
<p>In this section we present empirical experiments on math reasoning benchmarks.We first overview the tasks we conduct experiments on.We then describe the experimental setup and evaluation protocols.Finally we discuss the results and provide ablation studies.</p>
<p>Experimental setup</p>
<p>Tasks.We perform experiments on established math reasoning benchmarks.To enable rule-based answer checking, all problems in selected benchmarks require a verifiable final output.We evaluate models on benchmarks: (1) MATH500 (Lightman et al., 2023), a curated dataset of 500 problems selected from the full MATH (Hendrycks et al., 2021) evaluation set;</p>
<p>(2) AMC23 (AI-MO, 2023), a dataset of 40 challenging competition questions;</p>
<p>(3) AIME24 (AI-MO, 2024), a dataset of 30 more difficult competition problems.</p>
<p>Evaluation protocol.Our primary evaluation metric is the final answer accuracy.We additionally report cross-solution correction accuracy serving as a complementary evaluation.</p>
<p>For all experiments, we finetune Llama-3-Instruct models (Dubey et al., 2024) (3.1-8B &amp; 70B, 3.3-70B, DeepSeek-R1-Distill-Llama 8B &amp; 70B) as the base models.We conduct training using the NuminaMath dataset (LI et al., 2024), which consists of training sets from various data sources, covering a wide range of mathematical topics and difficulty levels.We exclude the Orca-Math dataset (Mitra et al., 2024) and synthetic data subset since their correctness are not human-validated despite their large scale.</p>
<p>For evaluations, we report the pass@1 accuracy of the final answer.We use greedy decoding and zero-shot COT prompting unless otherwise specified.As mentioned in previous sections, we do not utilize additional external instructions to prompt the finetuned model to attempt another solution trial; instead the model spontaneously performs self-verification to determine whether another attempt is needed.Our prompt templates for evaluation are included in Appendix E.</p>
<p>Implementation details.All models are prompted with the original Llama tokenizer and chat configs (Dubey et al., 2024) unless otherwise specified.All models except the DeepSeek-R1-Distill-Llama based ones are evaluated using the maximum generation length of 6, 144 tokens, while the DeepSeek-R1-Distill-Llama based models are evaluated using the maximum generation length of 32, 768 tokens, as per Guo et al. (2025).</p>
<p>To support training with multi-message responses, we utilize different special termination tokens for each model message.In particular, in each model response each message starts with assistant header tokens, indicating the source of message is the model.Besides, every assitant message except the last ends with an &lt;|eom_id|&gt; termination token, representing the end of one message.The last assistant message ends with an &lt;|eot_id|&gt; token, which concludes the entire model response.We implement RAFT (Dong et al., 2023) under the CGPO (Xu et al., 2024) framework, which allows for filtering out prompts whose all corresponding sampled responses contain no correct solutions or verifications.Gemini-1.5-Flash(4-shot) * (Team et al., 2024) 54.9 --SCoRe * (Kumar et al., 2024) 64.4 --Llama-3-8B-Instruct (4-shot) * (Meta, 2024) 30.0 --Self-rewarding IFT * (Xiong et al., 2025) 27.9 --Self-rewarding-IFT + Gold RM * 33.9 --DeepSeek-R1-Distill-Llama-8B-R1tok-avg@4 88.9 Baselines that we directly use results from their reports are marked with * .The best performance under each initial model is marked with bold text (omitted prompting-based Self-Refine for fair comparisons)."R1tok" indicates the model is evaluated using the R1 modified tokenizer and chat configs."avg@4" indicates the model is evaluated using sampling, with the temperature of 0.6, the top-p value of 0.95, and 4 responses generated per question to compute the mean pass@1 (Guo et al., 2025).Blue indicates ours, and green indicates other RL based approaches.</p>
<p>Results</p>
<p>Table 1 presents the comprehensive evaluation results, showing the comparisons across different initial models and parameter scales.In general, SPOC consistently outperforms the base models on all initialization models across all benchmark tasks.Notably, SPOC enhances the accuracy of Llama3.1 8B and 70B, reaching gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.This result highlights the effectiveness of SPOC across different parameter scales and task difficulties.</p>
<p>SPOC also achieves consistent enhancement when fine-tuned with strong initial models.Despite marginal improvement on Llama3.3-70Bmodel, SPOC obtains significant overall outperformance compared to the baselines after finetuning the DeepSeek-R1-Distill-Llama models.Table 2 shows performance across the first two solution turns on MATH500.Overall, SPOC achieves consistent improvement on the second solution turns over the first.With the smaller Llama3.1-8Bmodel, SPOC shows more inclination to generate a second solution turn, resulting in a more significant improvement margin.With larger 70B models that achieve higher final accuracy, on the other hand, SPOC tends to get the first solution message correct in the first place, resulting in an already strong turn1 performance and a marginal ∆(t1, t2).Such behaviour is well aligned with our expected Nash equilibrium admitted by the Corr reward setting, where policy optimization encourages the joint policy to generate both correct solutions and correct verifications in the first place.The complete per-turn performance analysis and diagnostics of verifier reliability are presented in Appendix C. We conduct ablation experiments on different reward configurations, as overviewed in Figure 3.We present comparisons with the default Corr reward setting in Table 4, using Llama-3.1-8B-Instruct as the base model.Compared to Corr, the ablation variants Last and All do not yield a unique Nash equilibrium; instead, they promote generating correct solutions regardless of the correctness of verifications.Results show that both variants still improve performance over the baseline; however, they both underperform Corr on two out of three tasks.Last and All obtains only one more correct answer than Corr in AIME24 and AMC23, respectively, while the performance discrepancy on MATH500 dominates the overall gap.The ablation highlights the importance of jointly optimizing the correctness of both solutions and verifications.</p>
<p>Conclusions</p>
<p>In this work, we tackle the mathematical reasoning challenge for Large Language Models by promoting intrinsic self-corrections.We propose SPOC, a novel approach that enables spontaneous, real-time solution proposal and verification within a single inference pass.SPOC frames the reasoning process as a multi-agent collaboration, where the model assumes both the roles of a solution proposer and verifier.SPOC dynamically elicits and terminates reasoning generations based on verification results, which flexibly and efficiently scales inference-time compute while improving accuracy.SPOC leverages synthetic data for fine-tuning and further enhances performance via online reinforcement learning, without requiring human or oracle input.</p>
<p>Comprehensive empirical evaluations on challenging math reasoning benchmarks showcase SPOC's efficacy, yielding substantial performance improvement.</p>
<p>Our results highlight the potential of spontaneous self-correction as an effective strategy for advancing LLM reasoning capabilities.To address the prohibitive length of long CoTs (Marjanović et al., 2025), future work could explore extending SPOC to partial solutions in long reasoning chains, using step-level process rewards to guide RL training and enable dynamic revisions when errors are detected until reaching the final answer.It would also be interesting to adopt SPOC to broader reasoning domains beyond mathematics, further enhancing its applicability.Sample K solutions for each question x i ∈ X :
{y k i } K k=1 ∼ π 0 (•|x i ) 5:
Label binary reward for each solution y k i :
r k i = r sl (y k i , y * i ) 6:
Append to rejection sampling set:
D rjs ← D rjs ∪ {(x i , y k i , r k i )} 7:
// Obtain verifications 8:</p>
<p>Choose the best/worst-of-N samples:
k + = arg max k r k i , k − = arg min k r k i 9: if r k + i = 0 or r k − i = 1 then 10:
continue // All correct or all incorrect solutions 11: else 12:
y * i ← y k + i , c_flag ← false, i_flag ← false 13: for k = 1, . . . , K do 14: if r k i = 0 then 15: v − i ∼ π 0 (•|x i , y k i , y * i ) 16: if f vf (v − i ) = 1 then 17: i_flag ← 1 18: D pair ← D pair ∪ {(x i , y k i , v − i , y * i )} 19:
end if 20:</p>
<p>else if r k i = 1 and k ̸ = k + then 21:
v + i ∼ π 0 (•|x i , y k i , y * i ) 22: if f vf (v + i ) = 1 then 23: c_flag ← 1 24: D pair ← D pair ∪ {(x i , y k i , v + i )}D learn = x j , τ k + j , {r sl j,k + ,l } l∈[L k ] , {r vf j,k + ,l } l∈[L k ] r sl j,k + ,L k = 1 ∨ r vf j,k + ,l = 1 j∈[B] 5:
Perform one gradient update on θ with Equations ( 2) and (3) using D learn Algorithm 4 RLOO Message-wise Policy Optimization
1: Inputs: Question-answer batch D i = X i × Y i = {(x j , y * j )} B j=1
, batch size B, policy model π θ , number of rollouts per question K, generated trajectory {τ k j } K k=1 , solution correctness rewards
{r sl j,k,l } k∈[K],l∈[L k ] , verification correctness rewards {r vf j,k,l } k∈[K],l∈[L k ] 2: // Message-wise advantage 3: for l = 1, . . . , max k L k ; r = r sl , r vf do 4: µ j,l = 1 K k∈[K] r j,k,l 5: σ j,l = 1 K k∈[K] |r j,k,l − µ j,l | 2 1 2 6: A j,k,l = r j,k,l −µ j,l
σ j,l 7: end for 8: Learning batch contains all K samples for each question:
D learn = x j , τ k j , {A sl j,k,l } l∈[L k ] , {A vf j,k,l } l∈[L k ] j∈[B]
,k∈[K] 9: Perform one gradient update on θ with Equations ( 2) and (3) using D learn</p>
<p>B Experimental setup details</p>
<p>Tasks.We evaluate model on test sets as follows:</p>
<p>• MATH500 (Lightman et al., 2023).A dataset of 500 problems selected from the full MATH (Hendrycks et al., 2021) evaluation set.This test set spans five difficulty levels and seven subjects, which promotes a comprehensive evaluation of reasoning capabilities.</p>
<p>• AMC23.A dataset of 40 problems from the American Mathematics Contest 12 (AMC12) 2023 (AI- MO, 2023).This test set consists of challenging competition questions intending to evaluate the model's capability to solve complex reasoning problems.</p>
<p>• AIME24.A dataset of 30 problems from the American Invitational Mathematics Examination (AIME) 2024 (AI-MO, 2024).This test set contains difficult questions, with few at AMC level and others drastically more difficult in comparison, aim to access the model's abiblity to perform more intricate math reasoning.</p>
<p>Implementation details.We use the AdamW optimizer with β 1 = 0.9, β 2 = 0.95, weight decay = 0.1, and a constant learning rate 1.0 × 10 −6 .We conduct all training runs on 32 NVIDIA H100 GPUs.We set the global batch size to 2048, and train for 256 steps.</p>
<p>C Extra results</p>
<p>C.1 Verifier reliability</p>
<p>We provide detailed diagnostics for verifier reliability in Table 5.Each confusion matrix corresponds to a base model and task pair, with the rows and columns indicating the actual and predicted solution correctness, respectively -i.e., diagonal cells represent the true positive (TP) and true negative (TN) rates while the off-diagonal cells represent the false positive (FP) and false negative (FN) rates.We observe the following phenomena:</p>
<p>• On easier tasks, the proposer has higher solution accuracy, and the verifier tends to show higher TP&amp;FP and lower TN&amp;FN.</p>
<p>• Stronger models that reach higher solution accuracy also have higher TP&amp;FP.</p>
<p>• The small model's high verification accuracy attributes largely to its higher TN.</p>
<p>Base Model MATH500 AMC2023 AIME2024</p>
<p>C.2 Per-turn performance analysis</p>
<p>We provide the per-turn performance statistics for AIME24 and AMC23 in Table 6 and Table 7, respectively.The results are consistent with MATH500 analysis in Table 2. SPOC generally improves or maintains performance on the second solution turns.The smaller model has lower final accuracy yet larger turn-wise improvements, while larger models tend to achieve correct solutions sooner at turn1.Moreover, turn-wise corrections occurs less in these two challenging competition benchmarks, as they contain significantly fewer questions than MATH500.We will include both tables in the appendix of our revised manuscript.</p>
<p>Base Model trained w/ SPOC Base.Acc.Verif.Acc.@t1Acc.@t1 Acc.@t2 ∆(t1, t2) ∆ c→i ∆ i→c Llama-3.Base Model trained w/ SPOC Base.Acc.Verif.Acc.@t1Acc.@t1 Acc.@t2 ∆(t1, t2)  Table 2 presents our per-turn performance analysis over turn1 → 2, where the majority of self-correction occurs.In practice, all finetuned models perform multiple rounds of self-reflection.We hereby present the complete results, where the Table 8 shows the turn 2 → 3 performance of all models, and Table 9 shows the all-turn performance of the 8B model (as the other stopped reflection earlier).Results suggest that the 8B model reaches a maximum of 6 turns while the 70B models reach a maximum of 3 turns across all 500 evaluation questions.This observation aligns with our discussion in Section 4.2, where stronger models tend to achieve correct solutions sooner.We also observe that the amount of questions requiring additional solutions drops over turns, aligning with the looping until verified correctness behavior.Overall, SPOC achieves improvement over turns.
∆ c→i ∆ i→c Llama-3.1-8B-
Base Model trained w/ SPOC Base.Acc.Verif.Acc.@t2Acc.@t2 Acc.@t3 ∆(t2, t3)  Turn l Verif.Acc.@tl Acc.@t l Acc.@t l+1 ∆(t l , t l+1 ) ∆c→i ∆i→c  9 Performance across all solution turns on MATH500 for Llama-3.1-8B-Instructbase model.
∆ c→i ∆ i→c Llama-3.1-8B-1</p>
<p>D Preliminaries</p>
<p>CGPO (Xu et al., 2024) is a constrained RL framework that allows for flexible applications of constraints on model generations.Denoting the contraints that the LLM generations need to satisfy as {C 1 , . . ., C M }, the prompt-generation set that satisfies constraint C m is defined as
Σ m = {(x, y) ∈ X × Y : (x, y) satisfies C m }.
The feasible region is defined as the prompt-generation set that satisfies all constraints, i.e., Σ = ∩ M m=1 C m .In the single-task setting, CGPO solves the constrained optimization problem as follows:
max θ E x∼X ,y∼π θ (x) [r(x, y)] s.t. P x∼X ,y∼π θ (x) ((x, y) ∈ Σ) &gt; 0, KL x∼X (π θ (x)∥π ref (x)) ≤ KL max
where r(x, y) is the reward function.CGPO is compatible with a wide spectrum of policy optimizers.The RAFT (Dong et al., 2023) algorithm prompts the current policy to generate multiple responses for each prompt, and the best-of-N (BoN) response is used to perform a one-step SFT update on the policy.</p>
<p>E Prompts</p>
<p>Llama 3.1 COT query template User:</p>
<p>Solve the following math problem efficiently and clearly:</p>
<p>-For simple problems (2 steps or fewer): Provide a concise solution with minimal explanation.</p>
<p>-For complex problems (3 steps or more): Use this step-by-step format: ...</p>
<p>Regardless of the approach, always conclude with:</p>
<p>Therefore, the final answer is: $\boxed{answer}$.I hope it is correct.</p>
<p>Where [answer] is just the final number or expression that solves the problem.</p>
<p>Problem: {{ Question }}</p>
<p>Simple COT query template</p>
<p>User:</p>
<p>Please reason step by step, and put your final answer within \boxed{}.Question: {{ Question }} Figure 5 Simple COT query template (Guo et al., 2025).</p>
<p>Instance reflection query template</p>
<p>User:</p>
<p>Compare a correct solution and a potential solution to the same math question, identify the error in the potential solution, if any, and briefly explain the error in two sentences or less in the <reflection> ... </reflection> tags, and then conclude with:</p>
<p>Verification: Is the previous solution correct?(Yes/No)</p>
<p>The explanation should be "I think the solution is correct."if the potential solution is correct.</p>
<p>DO NOT solve the question.DO NOT include phrases implying the comparison between the two solutions.</p>
<p>SPOC simple COT query template</p>
<p>User:</p>
<p>Please reason step by step, and put your final answer within \boxed{}.</p>
<p>After each solution attempt, reflect on its correctness within <reflection> ... </reflection> tags.</p>
<p>Your reflection should first concisely evaluate the previous solution, and then conclude with:</p>
<p>Verification: Is the previous solution correct?(Yes/No) If the verification is "No", rewrite the solution in a separate attempt, either correcting the error or choosing a different approach altogether.</p>
<p>Question: {{ Question }} ).Thus, ( b = 1 ) is not possible since ( a &lt; b ), implying ( a ) would have to be less than 1, which is not possible for positive integers.We need to find a factor of 2009 that fits the geometric sequence criteria.However, our analysis has been misguided because we overlooked the simplest factorization that allows for a geometric sequence with integer ratio.Let's correct this by directly applying the condition that ( a, b, 2009 ) form a geometric sequence with ( r ) being an integer.The correct approach involves recognizing that ( a ) must be a factor of 2009 such that ( r ) can be an integer, but since 2009 is prime, ( a ) must effectively be 1 for ( r ) to be an integer, given ( r^2 = 1 ) as the only viable integer square factor of 2009.</p>
<p>Therefore, the final answer is: \boxed{1}&lt;|eom_id|&gt; <reflection> The error lies in the incorrect assumption that ( r^2 ) must directly divide 2009, leading to a flawed analysis of possible values for ( r ) and ( a ).This mistake results in an incorrect conclusion that ( a ) must be 1, when in fact, ( a ) should be derived from considering the factors of 2009 and the properties of geometric sequences with integer ratios.</p>
<p>Verification: Is the previous solution correct?No </reflection>&lt;|eom_id|&gt; ## Step 1: Understand the problem and recall the formula for a geometric sequence.A geometric sequence is defined by the formula (a_n = a_1 \cdot r^{(n-1)}), where (a_n) is the nth term of the sequence, (a_1) is the first term, (r) is the common ratio, and (n) is the term number.</p>
<p>Figure 1
1
Figure 1 Multi-turn generation formalisms.(a)&amp;(b) Sample closed-loop paradigms that require extra system designs and prompting to trigger and terminate correction; (c) Sample open-loop paradigm that spontaneously adapts generations.</p>
<p>Figure 2
2
Figure 2 SPOC training overview.Left: PairSFT for initializing multi-turn generation.Right: Online RL for policy optimization.</p>
<p>Figure 3
3
Figure3Reward configurations for policy optimization, where sl, vf, C, I indicate solution, verification, correct, and incorrect, respectively.For Last and All, SPOC optimizes correct solutions (first row in each table) only when the last solution is correct.</p>
<p>Inputs: Question-answer dataset D = X × Y = {(x i , y * i )} N i=1, policy model π 0 , number of questions N , number of single-turn rollouts per question K, rule-based solution correctness reward function r sl (y, y * ) ∈ {0, 1}, single-turn sampling set D rjs ← {}, multi-turn correction set D pair ← {}, verification message validator f vf (v) ∈ {0, 1} 2: for i = 1, . . ., N do</p>
<h1></h1>
<h1>Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations]</h1>
<p>Figure 4
4
Figure4Llama 3.1 COT query template(Dubey et al., 2024).</p>
<p>QuestionFigure 6
6
Figure6Instance reflection query template.</p>
<p>Figure 7
7
Figure 7 SPOC simple COT query template.</p>
<h2>Step 8 :</h2>
<p>8
Correctly apply the geometric sequence formula and the fact that 2009 is a prime number.Since ( 2009 = ar^2 ) and ( r ) is an integer, we look for integer factors of 2009.However, since 2009 is prime, its only factors are 1 and 2009.For ( a, b, 2009 ) to be in a geometric sequence, we must have ( b = ar ) and ( 2009 = ar^2 ).Given ( r ) is an integer and ( a &lt; b &lt; 2009 ), we examine the possibility of ( r = 2009 ) and ( a = 1 ), but this does not fit because ( b ) would then be 2009, violating ( b &lt; 2009 ).The correct approach involves recognizing that since 2009 is prime, the only way ( a, b, 2009 ) can form a geometric sequence with integer ( r ) is if ( r ) itself is a factor of 2009, but since 2009 is prime, ( r ) must effectively be 1 or 2009 for the sequence to hold with integer values.However, ( r = 1 ) does not yield a geometric sequence with distinct terms, and ( r = 2009 ) does not fit the ( a &lt; b &lt; 2009 ) condition.We need to think about the factors of 2009 in a way that allows for a geometric sequence with ( a &lt; b &lt; 2009 ) and ( r ) being an integer.## Step 9: Re-evaluate the conditions for ( a ), ( b ), and 2009 to form a geometric sequence.Given ( b = ar ) and ( 2009 = ar^2 ), and knowing 2009 is prime, for ( r ) to be an integer, ( a ) must be such that ( ar^2 = 2009 ) holds true with ( r ) being an integer.Since 2009 is prime, ( r^2 ) must divide 2009, implying ( r^2 = 1 ) or ( r^2 = 2009 ) since these are the only divisors of 2009.( r^2 = 2009 ) is not possible for integer ( r ) since 2009 is not a perfect square.Thus, ( r^2 = 1 ), giving ( r = 1 ) or ( r = -1 ).However, ( r = 1 ) does not yield a geometric sequence with distinct terms, and ( r = -1 ) would imply ( b = -a ), which does not fit ( a &lt; b &lt; 2009 ) for positive integers.We must consider the nature of geometric sequences and prime numbers to find ( a ).## Step 10: Apply the properties of geometric sequences and prime numbers correctly.For ( a, b, 2009 ) to be a geometric sequence with integer ( r ), and given 2009 is prime, we look for ( a ) and ( r ) such that ( 2009 = ar^2 ) and ( b = ar ).Since ( r ) must be an integer and 2009 is prime, ( r^2 ) must be a factor of 2009, which means ( r^2 = 1 ) (since 2009 is prime and has no other square factors).This implies ( r = 1 ) or ( r = -1 ), but ( r = 1 ) does not give a geometric sequence with distinct terms, and ( r = -1 ) does not fit the condition ( a &lt; b &lt; 2009 ) for positive ( a ) and ( b ).We need to think about the prime factorization of 2009 and how it applies to geometric sequences.Given 2009 is prime, for ( a, b, 2009 ) to form a geometric sequence, ( a ) must be 1, and ( r ) must be such that ( r^2 = 2009 ), but since 2009 is not a perfect square, this approach is flawed.We must reconsider the basic properties of geometric sequences and the given conditions.## Step 11: Correctly identify ( a ) based on the geometric sequence and prime number conditions.Given ( a, b, 2009 ) form a geometric sequence with integer ( r ), and 2009 is prime, we have ( b = ar ) and ( 2009 = ar^2 ).Since ( r ) is an integer and 2009 is prime, ( r^2 ) must divide 2009, implying ( r^2 = 1 ) or ( r^2 = 2009 ).Since ( r^2 = 2009 ) is not possible for integer ( r ) (2009 is not a perfect square), we consider ( r^2 = 1 ), giving ( r = 1 ) or ( r = -1 ).However, ( r = 1 ) does not yield a geometric sequence with distinct terms, and ( r = -1 ) does not fit ( a &lt; b &lt; 2009 ) for positive integers.The error lies in assuming ( r^2 ) must directly divide 2009; instead, we should consider how ( a ) and ( r ) relate through the geometric sequence formula.Since 2009 is prime, and ( a, b, 2009 ) are in a geometric sequence, we look for ( a ) and ( r ) such that ( 2009 = ar^2 ) and ( b = ar ), with ( r ) being an integer.Given 2009's primality, ( a ) must be 1 for ( r^2 ) to be an integer factor of 2009, but since 2009 is not a perfect square, ( r ) cannot be an integer if ( a = 1 ).We need to reconsider the relationship between ( a ), ( b ), and 2009 in the context of geometric sequences and prime numbers.## Step 12: Finalize the correct approach for finding ( a ).Given ( a, b, 2009 ) form a geometric sequence with ( r ) as an integer, and 2009 is prime, we must find ( a ) such that ( b = ar ) and ( 2009 = ar^2 ).Since 2009 is prime, for ( r^2 ) to divide 2009, ( r^2 ) must be 1 (since 2009 is not a perfect square).This implies ( r = 1 ) or ( r = -1 ), but ( r = 1 ) does not yield a geometric sequence with distinct terms, and ( r = -1 ) does not fit ( a &lt; b &lt; 2009 ) for positive ( a ) and ( b ).The key insight is recognizing that since 2009 is prime, ( a ) must be such that ( ar^2 = 2009 ) holds with ( r ) being an integer.Given ( r^2 = 1 ) is the only integer square factor of 2009 (since 2009 is not a perfect square), we must consider how ( a ) and ( r ) relate through the geometric sequence.If ( r = -1 ), then ( b = -a ), which does not fit ( a &lt; b &lt; 2009 ) for positive integers.Thus, we need to think about the factors of 2009 in a way that allows for a geometric sequence.Since 2009 is prime, the only factors are 1 and 2009, implying ( a ) could be 1 if ( r^2 = 2009 ), but ( r ) cannot be an integer in this case.</p>
<p>For the given problem, we have (a_1 = a), (a_2 = b), and (a_3 = 2009).Therefore, we can write the equations (b = ar) and (2009 = ar^2).## Step 2: Express (b) in terms of (a) and (r), and (2009) in terms of (a) and (r).From the geometric sequence formula, we have (b = ar) and (2009 = ar^2).## Step 3: Solve for (r) using the information given.Dividing the equation (2009 = ar^2) by (b = ar), we get (\frac{2009}{b} = r).Since (b = ar), we can substitute (b) in the equation to get (\frac{2009}{ar} = r), which simplifies to (2009 = ar^2).We already know this, so let's directly use the relationship between (a), (b), and (2009) to find (r).Given (b = ar) and (2009 = ar^2), dividing these equations gives (r = \frac{2009}{b}).## Step 4: Determine the possible values of (r) and (a).Since (a), (b), and (2009) form a geometric sequence with (a &lt; b &lt; 2009), and (r) is an integer, we need to find integer factors of (2009) that could serve as (r).The prime factorization of (2009) is (7^2 \cdot 41).Thus, possible values for (r) could be (1), (7), (41), or (49), considering the factors of (2009).However, since (a &lt; b &lt; 2009), (r) must be greater than (1).## Step 5: Test possible values of (r) to find (a).Let's test (r = 7): If (r = 7), then (2009 = a \cdot 7^2), which gives (a = \frac{2009}{49}).Calculating this, (a = 41), which is an integer and satisfies (a &lt; b &lt; 2009).Let's test (r = 41): If (r = 41), then (2009 = a \cdot 41^2), which does not yield an integer (a) since (41^2 = 1681) and (2009 / 1681) is not an integer.Let's test (r = 49): If (r = 49), then (2009 = a \cdot 49^2) does not apply since (49^2 = 2401), which is larger than (2009), making it impossible for (a) to be an integer in this context.## Step 6: Verify the solution.Given (r = 7) and (a = 41), we find (b = ar = 41 \cdot 7 = 287).Thus, the sequence is (41, 287, 2009), which satisfies the given conditions.The final answer is: $\boxed{41}$&lt;|eom_id|&gt; <reflection> I think the solution is correct.\n\nVerification:Is the previous solution correct?Yes </reflection>&lt;|eot_id|&gt;</p>
<p>Respectively on MATH500/AMC23/AIME24, SPOC reaches 77.6%/70.0%/23.3%with the 8B model, and 89.9%/85.0%/53.3%with the 70B model.Furthermore, SPOC achieves more drastic performance improvement using the RLOO policy optimizer, obtaining 87.2%/87.5%/50.0%with the 8B model, and 94.6%/92.5%/76.7%with the 70B model.It is important to note that the gap between our evaluation of DeepSeek-R1-Distill-Llama base models for post-training and their corresponding R1tok results is attributed to different tokenizers and chat configurations.Base Model trained w/ SPOC Base.Acc.Verif.Acc.@t1Acc.@t1 Acc.@t2 ∆(t1, t2) ∆ c→i ∆ i→c
Llama-3.1-8B-Instruct52.280.259.061.02.08/29 18/79Llama-3.1-70B-Instruct65.880.077.077.40.43/105/8Llama-3.3-70B-Instruct75.681.877.877.80.01/41/20</p>
<p>Table 2
2
Performance across first two solution turns on MATH500.∆ c→i &amp;∆ i→c presents (#correct/#all) at the next turn.</p>
<p>Table 3
3
shows the performance of applying multiple iterations of PairSFT-RL training procedure.Results indicate that the second iteration still leads to overall consistent improvement over all models.Although the overall improvement is mainly marginal, the second iteration shows a larger gain in challenging competition benchmarks.For instance, with Llama3.1-70B,iter2 improves over iter1 by 10% and 6.7% on AMC23 and AIME24, respectively.
ApproachMATH500 AMC23 AIME24Llama-3.1-8B-Instruct52.222.53.3PairSFT (iter1)53.822.510.0SPOC (iter1)61.032.56.7PairSFT (iter2)60.835.06.7SPOC (iter2)62.032.510.0Llama-3.1-70B-Instruct65.832.516.7PairSFT (iter1)74.847.523.3SPOC (iter1)77.452.523.3PairSFT (iter2)76.467.520.0SPOC (iter2)77.662.530.0Llama-3.3-70B-Instruct75.657.526.7PairSFT (iter1)75.062.523.3SPOC (iter1)77.870.023.3PairSFT (iter2)79.672.526.7SPOC (iter2)79.870.030.0</p>
<p>Table 3
3
Iterative training performance.The second iteration still leads to overall consistent improvement over all models.
4.3 AblationsModelMATH500 AMC23 AIME24Base52.222.53.3SPOC-Corr61.032.56.7SPOC-Last59.827.510.0SPOC-All58.435.06.7</p>
<p>Table 4
4
Ablation experiments under different reward settings.Experiments are conducted on the Llama-3.1-8B-Instructmodel.</p>
<p>AnYang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren  Zhou, Junyang Lin, et al.Qwen2.5-mathtechnical report: Toward mathematical expert model via self-improvement.arXivpreprint arXiv:2409.12122,2024.Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu.Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems.arXiv preprint arXiv:2408.16293,2024.
Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generativeverifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024.Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, and Le Sun.Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic. arXiv preprintarXiv:2408.16326, 2024.</p>
<p>Choose the best-of-N trajectory for each question x j based on last solution message: k + = arg max k r sl Filter out questions with no correct final solution or no correct verification, i.e. learning batch is
Algorithm 3 RAFT Message-wise Policy Optimization1: Inputs: Question-answer batch D i = X i × Y i = {(x j , y  *  j )} B j=1 , batch size B, policy model π θ , number ofrollouts per question K, generated trajectory {τ k j } K k=1 , solution correctness rewards {r sl j,k,l } k∈[K],l∈[L k ] ,verification correctness rewards {r vf j,k,l } k∈[K],l∈[L k ]2: j,k,L k 3: // Apply constraint4:25:end if26:end if27:if c_flag = 1 and i_flag = 1 then28:break29:end if30:end for31:end if32: end for33: return D pair</p>
<p>Table 5
5
Diagnostics for verifier reliability at the first turn across MATH500, AMC2023, and AIME2024 benchmarks.</p>
<p>Table 6
6
Performance across first two solution turns on AIME2024.∆ c→i &amp;∆ i→c presents (#correct/#all) at the next turn.
1-8B-Instruct3.329/301/302/301/300/11/7Llama-3.1-70B-Instruct16.710/307/307/300/300/10/1Llama-3.3-70B-Instruct26.711/307/307/300/3000/1</p>
<p>Table 7
7
Performance across first two solution turns on AMC2023.∆ c→i &amp;∆ i→c presents (#correct/#all) at the next turn.</p>
<p>Table 8
8
Performance across solution turns 2 → 3 on MATH500.∆ c→i &amp;∆ i→c presents (#correct/#all) at the next turn.</p>
<p>Table
401/50059.061.02.08/2918/79219/2261.061.20.20/31/1836/861.261.0-0.22/21/642/261.061.00.0-0/251/161.061.00.0-0/160/161.0----
Different from the classic self-play in zero-sum games (e.g., AlphaZero(Silver et al.,<br />
)), ours involves non-symmetrical roles in the sense that two policies are different conditioned on the context.
Self-Refine w/o oracle query templateUser:There might be an error in the solution above because of lack of understanding of the question.Please correct the error, if any, and rewrite the solution.Be sure to apply the given format and conclude with: "Therefore, the final answer is: $\boxed{answer}$." Figure8Self-Refine w/o oracle query template(Madaan et al., 2023).Self-Refine w/ oracle query templateUser:There is an error in the solution above because of lack of understanding of the question.Please correct the error and rewrite the solution.Ensure you use the information from past attempts.If you arrive at a solution you have already had, the answer is incorrect once again, so take that into account and retry if necessary.Be sure to apply the given format and conclude with: "Therefore, the final answer is: $\boxed{answer}$."Figure9Self-Refine w/ oracle query template(Madaan et al., 2023).F Example responseWe present example responses of SPOC finetuned on Llama-3.1-70B-Instruct on MATH500.SPOC sample response
Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker, arXiv:2402.147402024arXiv preprint</p>
<p>A I Meta, AI-MO. American invitational mathematics examination. 2024. 2023. 2024AI-MO. American mathematics contestLlama-3.3-70b-instruct</p>
<p>Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, Niket Tandon, arXiv:2305.088442023arXiv preprint</p>
<p>Multi-agent reinforcement learning: Foundations and modern approaches. Filippos Stefano V Albrecht, Lukas Christianos, Schäfer, 2024MIT Press</p>
<p>Autoprm: Automating procedural supervision for multi-step reasoning via controllable question decomposition. Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, Huaxiu Yao, arXiv:2402.114522024arXiv preprint</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang, arXiv:2304.06767Raft: Reward ranked finetuning for generative foundation model alignment. 2023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, Forty-first International Conference on Machine Learning. 2023</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Raileanu, arXiv:2402.10963Glore: When, where, and how to improve llm reasoning via global and local refinements. 2024arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Training language models to self-correct via reinforcement learning. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, arXiv:2409.129172024arXiv preprint</p>
<p>. L I Jia, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Costa Shengyi, Kashif Huang, Longhui Rasul, Albert Yu, Ziju Jiang, Zihan Shen, Bin Qin, Li Dong, Yann Zhou, Guillaume Fleureau, Stanislas Lample, Polu, Numinamath, 2024</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, arXiv:2305.191182023arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>S 2 r: Teaching llms to self-verify and self-correct via reinforcement learning. Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, Jia Li, arXiv:2502.128532025arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>Deepseek-r1 thoughtology: Let's think about llm reasoning. Sara Vera Marjanović, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad Behnamghader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, arXiv:2504.071282025arXiv preprint</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Meta Ai, Meta AI. 2562024</p>
<p>Orca-math: Unlocking the potential of slms in grade school math. Arindam Mitra, Hamed Khanpour, Corby Rosset, Ahmed Awadallah, arXiv:2402.148302024arXiv preprint</p>
<p>Ramesh Sumeet, Chandler Motwani, Smith, Jyoti Rocktim, Rafael Das, Ivan Rafailov, Laptev, Fabio Philip Hs Torr, Ronald Pizzati, Christian Clark, Schroeder De Witt, arXiv:2412.01928Malt: Improving reasoning with multi-agent llm training. 2024arXiv preprint</p>
<p>A course in game theory. Osborne Martin, 1994MIT Press</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, arXiv:2304.01904Refiner: Reasoning feedback on intermediate representations. 2023arXiv preprint</p>
<p>Recursive introspection: Teaching language model agents how to self-improve. Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar, arXiv:2407.182192024arXiv preprint</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, arXiv:2206.058022022arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Yoav Shoham, Kevin Leyton-Brown, 2008Cambridge University Press</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, arXiv:1712.018152017arXiv preprint</p>
<p>Reinforcement learning: An introduction. Richard S Sutton, 2018A Bradford Book</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024arXiv preprint</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, arXiv:2211.000532022arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, arXiv:2308.081552023arXiv preprint</p>
<p>Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, arXiv:2501.046822025arXiv preprint</p>
<p>Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, Tong Zhang, arXiv:2312.114562023arXiv preprint</p>
<p>Self-rewarding correction for mathematical reasoning. Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, Tong Zhang, arXiv:2502.196132025arXiv preprint</p>
<p>The perfect blend: Redefining rlhf with mixture of judges. Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, arXiv:2409.203702024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>